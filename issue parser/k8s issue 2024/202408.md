# Issue å®‰å…¨åˆ†ææŠ¥å‘Š

# ğŸš¨ å­˜åœ¨é«˜é£é™©çš„ Issues (4 ä¸ª)

## Issue #126502 Scheduler allows more volumes than a CSI driver limit to be attached

- Issue é“¾æ¥ï¼š[#126502](https://github.com/kubernetes/kubernetes/issues/126502)

### Issue å†…å®¹

#### What happened?

When a pod that uses an attachable volume is deleted, it can take some time for its volumes to get detached, but the scheduler does not care about it and treats Pod volumes as detached immediately after Pod is deleted from the API server.

The scheduler should count not only Pods, but also existing VolumeAttachment objects to get accurate nr. of volumes attached to the node.


#### What did you expect to happen?

Scheduler respects the attachment limit and never puts more Pods with volumes when a CSI driver is still detaching a volume for a deleted pod.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Get a CSI driver that reports an attach limit. Using AWS EBS, it reports 26 max. attachments in its CSINode:
    ```yaml
    spec:
      drivers:
      - allocatable:
          count: 26
        name: ebs.csi.aws.com
    ```
2. Create many pods targeting the same node using a `nodeSelector`, each of them using its own AWS EBS PV.
3. See scheduler tries to schedule just 26 Pods on the node, the rest is Pending.
4. Delete one Running Pod. Once the Pod is deleted, the scheduler puts a new Pod to the node immediately. But the volume of the deleted Pod is still attached!
5. Observe that nr. of VolumeAttachments is >26 for a short time. Depending on the driver and its current load and nr. of deleted pods it may not be that short time and it may grow significantly over 26.

Most CSI drivers recover from this, however, e.g. AWS EBS driver throws really weird errors and the recovery takes some time and I don't blame the driver.

#### Anything else we need to know?

All attachable in-tree volumes are now migrated to CSI and have VolumeAttachments, so the calculation will be accurate even for them.

#### Kubernetes version

v1.30.2

#### Cloud provider

<details>
AWS EC2
</details>


#### OS version

_No response_

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®issueçš„æè¿°ï¼Œå½“ä¸€ä¸ªä½¿ç”¨å¯æŒ‚è½½å·çš„Podè¢«åˆ é™¤æ—¶ï¼Œå…¶å·çš„å¸è½½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œä½†è°ƒåº¦å™¨ç«‹å³è®¤ä¸ºPodçš„å·å·²å¸è½½ï¼Œå› æ­¤å¯èƒ½ä¼šè°ƒåº¦è¶…è¿‡CSIé©±åŠ¨é™åˆ¶çš„å·åˆ°èŠ‚ç‚¹ä¸Šã€‚è¿™ä¼šå¯¼è‡´å·é™„ä»¶æ•°é‡è¶…è¿‡é©±åŠ¨ç¨‹åºé™åˆ¶ï¼Œå¯èƒ½å¼•å‘é”™è¯¯æˆ–èµ„æºè€—å°½ã€‚

åœ¨å¤šç§Ÿæˆ·ç¯å¢ƒä¸­ï¼Œä½æƒé™ç”¨æˆ·å¯ä»¥é€šè¿‡é¢‘ç¹åˆ›å»ºå’Œåˆ é™¤ä½¿ç”¨å¯æŒ‚è½½å·çš„Podï¼Œè¯±ä½¿è°ƒåº¦å™¨è¶…å‡ºå·é™åˆ¶ï¼Œå¯¼è‡´èŠ‚ç‚¹ä¸Šçš„CSIé©±åŠ¨å¼‚å¸¸ï¼Œå½±å“å…¶ä»–ç”¨æˆ·çš„æœåŠ¡ã€‚å› æ­¤ï¼Œæ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¯¥æ¼æ´å‘åŠ¨æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ï¼Œå½±å“æ•´ä¸ªé›†ç¾¤çš„ç¨³å®šæ€§ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œç¬¬9æ¡ï¼šå¦‚æœIssueå¯ä»¥å‘ç”Ÿåœ¨å¤šç”¨æˆ·åœºæ™¯ä¸­ï¼Œä¸€ä¸ªä½æƒé™ç”¨æˆ·èƒ½å¤Ÿå½±å“å’Œè‡ªå·±æƒé™ä¸€æ ·ç”šè‡³æ›´é«˜çš„å…¶ä»–ç”¨æˆ·ï¼Œåˆ™åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚

å› æ­¤ï¼Œè¯¥é—®é¢˜å­˜åœ¨å®‰å…¨é£é™©ï¼Œé£é™©è¯„çº§ä¸ºé«˜é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```json
[
  {
    "cmd": "for i in $(seq 1 26); do kubectl run ebs-pod-$i --image=nginx --restart=Never --overrides='{\"apiVersion\":\"v1\",\"spec\":{\"nodeSelector\":{\"dedicated\":\"ebs-node\"},\"containers\":[{\"name\":\"ebs-app\",\"image\":\"nginx\",\"volumeMounts\":[{\"mountPath\":\"/data\",\"name\":\"ebs-volume\"}]}],\"volumes\":[{\"name\":\"ebs-volume\",\"persistentVolumeClaim\":{\"claimName\":\"ebs-pvc-$i\"}}]}}'; done",
    "explain": "åˆ›å»º26ä¸ªPodï¼Œæ¯ä¸ªPodä½¿ç”¨ä¸€ä¸ªç‹¬ç«‹çš„EBSå·ï¼Œå¹¶è°ƒåº¦åˆ°æŒ‡å®šèŠ‚ç‚¹ï¼Œä»¥è¾¾åˆ°CSIé©±åŠ¨çš„å·é™„ä»¶é™åˆ¶ã€‚"
  },
  {
    "cmd": "kubectl delete pod ebs-pod-1",
    "explain": "åˆ é™¤å…¶ä¸­ä¸€ä¸ªæ­£åœ¨è¿è¡Œçš„Podï¼Œä»¥è§¦å‘è°ƒåº¦å™¨è®¤ä¸ºå·å·²å¸è½½ã€‚"
  },
  {
    "cmd": "kubectl run ebs-pod-27 --image=nginx --restart=Never --overrides='{\"apiVersion\":\"v1\",\"spec\":{\"nodeSelector\":{\"dedicated\":\"ebs-node\"},\"containers\":[{\"name\":\"ebs-app\",\"image\":\"nginx\",\"volumeMounts\":[{\"mountPath\":\"/data\",\"name\":\"ebs-volume\"}]}],\"volumes\":[{\"name\":\"ebs-volume\",\"persistentVolumeClaim\":{\"claimName\":\"ebs-pvc-27\"}}]}}'",
    "explain": "ç«‹å³åˆ›å»ºç¬¬27ä¸ªPodï¼Œè°ƒåº¦å™¨ä¼šå°†å…¶è°ƒåº¦åˆ°èŠ‚ç‚¹ä¸Šï¼Œå°½ç®¡å®é™…å·é™„ä»¶æ•°é‡ä»ç„¶è¶…è¿‡é™åˆ¶ã€‚"
  },
  {
    "cmd": "kubectl get volumeattachments | grep ebs.csi.aws.com | wc -l",
    "explain": "æŸ¥çœ‹å½“å‰çš„VolumeAttachmentæ•°é‡ï¼ŒéªŒè¯æ˜¯å¦è¶…è¿‡äº†CSIé©±åŠ¨çš„é™åˆ¶ã€‚"
  },
  {
    "cmd": "for i in $(seq 1 27); do kubectl delete pod ebs-pod-$i; done",
    "explain": "æ¸…ç†åˆ›å»ºçš„Podã€‚"
  }
]
```

---

## Issue #126573 kubectl v1.29.7 is built with go1.22.5 but expected goVersion go1.21.*

- Issue é“¾æ¥ï¼š[#126573](https://github.com/kubernetes/kubernetes/issues/126573)

### Issue å†…å®¹

**What happened**:
kubectl v1.29.7 is built with go1.22.5 but expected goVersion go1.21.*
**What you expected to happen**:
kubectl in v1.29 line is expected with goVersion go1.21.*

**How to reproduce it (as minimally and precisely as possible)**:
Download kubectl from https://dl.k8s.io/release/v1.29.7/bin/linux/amd64/kubectl, and check version details
It's saying "goVersion: go1.22.5", but the expected is goVersion go1.21.* as set in https://github.com/kubernetes/kubectl/blob/kubernetes-1.29.7/go.mod#L5
```
# ./kubectl version --client -o yaml
clientVersion:
  buildDate: "2024-07-17T00:06:19Z"
  compiler: gc
  gitCommit: 4e4a18878ce330fefda1dc46acca88ba355e9ce7
  gitTreeState: clean
  gitVersion: v1.29.7
  goVersion: go1.22.5
  major: "1"
  minor: "29"
  platform: linux/amd64
kustomizeVersion: v5.0.4-0.20230601165947-6ce0bf390ce3
```

**Anything else we need to know?**:
Also checked kubectl v1.29.6 with "kubectl version --client -o yaml" command, which is with goVersion: go1.21.11
So it maybe build issue that goVersion bump in kubectl v1.29.7

**Environment**:
- Kubernetes client and server versions (use `kubectl version`):v1.29.7
- Cloud provider or hardware configuration: linux
- OS (e.g: `cat /etc/os-release`): CentOS Stream 9 / amd64


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
é€šè¿‡Issueçš„æè¿°ï¼Œå‘ç°kubectl v1.29.7çš„äºŒè¿›åˆ¶æ–‡ä»¶å­˜åœ¨ä»¥ä¸‹å¼‚å¸¸ï¼š

1. **æ„å»ºçš„Goç‰ˆæœ¬å¼‚å¸¸**ï¼škubectl v1.29.7æ˜¾ç¤ºçš„goVersionä¸º`go1.22.5`ï¼Œä½†æ ¹æ®å®˜æ–¹`go.mod`æ–‡ä»¶ï¼Œåº”è¯¥ä½¿ç”¨`go1.21.*`ã€‚æˆªè‡³2023å¹´10æœˆï¼ŒGoè¯­è¨€çš„æœ€æ–°ç¨³å®šç‰ˆæœ¬ä¸º`go1.21.*`ï¼Œ`go1.22.5`å°šæœªå‘å¸ƒã€‚

2. **æ„å»ºæ—¥æœŸå¼‚å¸¸**ï¼š`buildDate`æ˜¾ç¤ºä¸º`2024-07-17T00:06:19Z`ï¼Œå³æœªæ¥çš„æ—¥æœŸã€‚è¿™è¡¨æ˜æ„å»ºæ—¥æœŸè¢«ç¯¡æ”¹ï¼Œæˆ–è€…è¯¥äºŒè¿›åˆ¶æ–‡ä»¶å¹¶éæ¥è‡ªå®˜æ–¹å¯ä¿¡ä»»çš„æ„å»ºã€‚

3. **å¯èƒ½çš„ä¾›åº”é“¾æ”»å‡»é£é™©**ï¼šä¸Šè¿°å¼‚å¸¸è¡¨æ˜kubectl v1.29.7çš„äºŒè¿›åˆ¶æ–‡ä»¶å¯èƒ½è¢«æ›¿æ¢ã€ç¯¡æ”¹æˆ–å­˜åœ¨æ¶æ„ä»£ç ï¼Œç”¨æˆ·åœ¨ä¸çŸ¥æƒ…çš„æƒ…å†µä¸‹ä¸‹è½½å¹¶ä½¿ç”¨ï¼Œå¯èƒ½å¯¼è‡´å®‰å…¨é£é™©ã€‚

**æ½œåœ¨å½±å“**ï¼š

- **è¿œç¨‹ä»£ç æ‰§è¡Œ**ï¼šå¦‚æœäºŒè¿›åˆ¶æ–‡ä»¶è¢«æ¤å…¥äº†æ¶æ„ä»£ç ï¼Œæ”»å‡»è€…å¯èƒ½åœ¨ç”¨æˆ·æ‰§è¡Œkubectlå‘½ä»¤æ—¶æ‰§è¡Œä»»æ„ä»£ç ã€‚
- **æƒé™æå‡**ï¼škubectlé€šå¸¸åœ¨å…·æœ‰é«˜æƒé™çš„ç¯å¢ƒä¸­è¿è¡Œï¼Œå¦‚æœè¢«ç¯¡æ”¹ï¼Œå¯èƒ½å¯¼è‡´æƒé™æå‡æ”»å‡»ã€‚
- **ä¾›åº”é“¾æ”»å‡»**ï¼šè¿™å¯èƒ½æ˜¯å¯¹Kubernetesç”Ÿæ€ç³»ç»Ÿçš„ä¾›åº”é“¾æ”»å‡»ï¼Œå½±å“èŒƒå›´å¹¿æ³›ã€‚

**é£é™©è¯„çº§**ï¼šæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥é—®é¢˜å¯èƒ½å¯¼è‡´å‘½ä»¤æ‰§è¡Œã€ææƒç­‰é«˜å®‰å…¨é£é™©çš„é—®é¢˜ï¼Œåº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```json
[
  {
    "cmd": "wget https://dl.k8s.io/release/v1.29.7/bin/linux/amd64/kubectl",
    "explain": "ä»å®˜æ–¹URLä¸‹è½½kubectl v1.29.7äºŒè¿›åˆ¶æ–‡ä»¶ã€‚"
  },
  {
    "cmd": "chmod +x kubectl",
    "explain": "ä¸ºkubectläºŒè¿›åˆ¶æ–‡ä»¶æ·»åŠ å¯æ‰§è¡Œæƒé™ã€‚"
  },
  {
    "cmd": "./kubectl version --client -o yaml",
    "explain": "æŸ¥çœ‹kubectlå®¢æˆ·ç«¯ç‰ˆæœ¬çš„è¯¦ç»†ä¿¡æ¯ï¼Œæ£€æŸ¥goVersionå’ŒbuildDateã€‚"
  },
  {
    "cmd": "curl -s https://raw.githubusercontent.com/kubernetes/kubectl/kubernetes-1.29.7/go.mod | grep '^go '",
    "explain": "ä»å®˜æ–¹æºç ä»“åº“è·å–go.modæ–‡ä»¶ï¼ŒéªŒè¯å®˜æ–¹æŒ‡å®šçš„Goç‰ˆæœ¬ã€‚"
  },
  {
    "cmd": "date",
    "explain": "æŸ¥çœ‹å½“å‰ç³»ç»Ÿæ—¥æœŸï¼Œç¡®è®¤æ„å»ºæ—¥æœŸçš„å¼‚å¸¸ã€‚"
  }
]
```

---

## Issue #126634 Kubernetes mounts a pod to local file system

- Issue é“¾æ¥ï¼š[#126634](https://github.com/kubernetes/kubernetes/issues/126634)

### Issue å†…å®¹

#### What happened?

Hi,
I was hoping for your help please.
Im using ceph-csi 3.11, with ceph 18.2.2, on kernel  4.18.
After reboot a certain node, pods that were mounted to rbd PVC's go back to be mounted on the / device of this certain node, and not to the rbd volumes.
I created a sts that use the NODENAME field to try to force the pod to stay on the rebooted node, and indeed the pod is being mounted to / and not the rbd volume, our mounter is krbd so I dont know why it is trying to enable nbd:
```
I0811 15:08:10.294873   13511 cephcsi.go:191] Driver version: v3.11.0 and Git version: e6c645933c6745f0c2a8f5523b27d11b422f27f5
I0811 15:08:10.295072   13511 cephcsi.go:268] Initial PID limit is set to 52428
I0811 15:08:10.295110   13511 cephcsi.go:274] Reconfigured PID limit to -1 (max)
I0811 15:08:10.295130   13511 cephcsi.go:223] Starting driver type: rbd with name: rbd.csi.ceph.com
I0811 15:08:10.314174   13511 mount_linux.go:282] Detected umount with safe 'not mounted' behavior
I0811 15:08:19.021969   13511 rbd_attach.go:242] nbd module loaded
I0811 15:08:19.022121   13511 rbd_attach.go:256] kernel version "4.18.0-553.el8_10.x86_64" supports cookie feature
W0811 15:08:19.022528   13511 rbd_attach.go:262] running rbd-nbd --help failed with error:an error (exec: "rbd-nbd": executable file not found in $PATH) occurred while running rbd-nbd args: [--help], stderr:
I0811 15:08:19.023167   13511 server.go:114] listening for CSI-Addons requests on address: &net.UnixAddr{Name:"/csi/csi-addons.sock", Net:"unix"}
I0811 15:08:19.023407   13511 server.go:117] Listening for connections on address: &net.UnixAddr{Name:"//csi/csi.sock", Net:"unix"}
I0811 15:08:19.195199   13511 utils.go:198] ID: 1 GRPC call: /csi.v1.Identity/GetPluginInfo
I0811 15:08:19.196319   13511 utils.go:199] ID: 1 GRPC request: {}
I0811 15:08:19.196330   13511 identityserver-default.go:40] ID: 1 Using default GetPluginInfo
I0811 15:08:19.196368   13511 utils.go:205] ID: 1 GRPC response: {"name":"rbd.csi.ceph.com","vendor_version":"v3.11.0"}
I0811 15:08:19.666253   13511 utils.go:198] ID: 2 GRPC call: /csi.v1.Node/NodeGetInfo
I0811 15:08:19.666390   13511 utils.go:199] ID: 2 GRPC request: {}
I0811 15:08:19.666408   13511 nodeserver-default.go:45] ID: 2 Using default NodeGetInfo
I0811 15:08:19.666545   13511 utils.go:205] ID: 2 GRPC response: {"accessible_topology":{},"node_id":"odessa-cluster-workerbm-2"}
I0811 15:08:19.732944   13511 utils.go:198] ID: 3 Req-ID: 0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a GRPC call: /csi.v1.Node/NodeUnpublishVolume
I0811 15:08:19.733135   13511 utils.go:199] ID: 3 Req-ID: 0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a GRPC request: {"target_path":"/var/lib/kubelet/pods/ab6626ef-b304-4c79-9638-3d3e46a2ec1e/volumes/kubernetes.io~csi/pvc-ed3aa009-edeb-4eaf-9f4e-761b717d7cf2/mount","volume_id":"0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a"}
I0811 15:08:19.733623   13511 utils.go:205] ID: 3 Req-ID: 0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a GRPC response: {}
I0811 15:08:19.845743   13511 utils.go:198] ID: 4 GRPC call: /csi.v1.Node/NodeGetCapabilities
I0811 15:08:19.845838   13511 utils.go:199] ID: 4 GRPC request: {}
I0811 15:08:19.846100   13511 utils.go:205] ID: 4 GRPC response: {"capabilities":[{"Type":{"Rpc":{"type":1}}},{"Type":{"Rpc":{"type":2}}},{"Type":{"Rpc":{"type":3}}},{"Type":{"Rpc":{"type":5}}}]}
I0811 15:08:19.859672   13511 utils.go:198] ID: 5 GRPC call: /csi.v1.Node/NodeGetCapabilities
I0811 15:08:19.859723   13511 utils.go:199] ID: 5 GRPC request: {}
I0811 15:08:19.859883   13511 utils.go:205] ID: 5 GRPC response: {"capabilities":[{"Type":{"Rpc":{"type":1}}},{"Type":{"Rpc":{"type":2}}},{"Type":{"Rpc":{"type":3}}},{"Type":{"Rpc":{"type":5}}}]}
I0811 15:08:19.860995   13511 utils.go:198] ID: 6 GRPC call: /csi.v1.Node/NodeGetCapabilities
I0811 15:08:19.861055   13511 utils.go:199] ID: 6 GRPC request: {}
I0811 15:08:19.861239   13511 utils.go:205] ID: 6 GRPC response: {"capabilities":[{"Type":{"Rpc":{"type":1}}},{"Type":{"Rpc":{"type":2}}},{"Type":{"Rpc":{"type":3}}},{"Type":{"Rpc":{"type":5}}}]}
I0811 15:08:19.862646   13511 utils.go:198] ID: 7 Req-ID: 0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a GRPC call: /csi.v1.Node/NodePublishVolume
I0811 15:08:19.862905   13511 utils.go:199] ID: 7 Req-ID: 0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a GRPC request: {"staging_target_path":"/var/lib/kubelet/plugins/kubernetes.io/csi/rbd.csi.ceph.com/3038ce5e41af71bc02c413b0a557fb52e71e372d4db7f600d4844ceb329780eb/globalmount","target_path":"/var/lib/kubelet/pods/bd49f109-cf95-4fce-9ea1-8bc71f04709b/volumes/kubernetes.io~csi/pvc-ed3aa009-edeb-4eaf-9f4e-761b717d7cf2/mount","volume_capability":{"AccessType":{"Mount":{"fs_type":"ext4","mount_flags":["discard"]}},"access_mode":{"mode":7}},"volume_context":{"clusterID":"aeb95e19-6505-44f5-9221-062605cda4ee","imageFeatures":"layering","imageName":"csi-vol-dc93419b-7109-4c39-9a58-e87b9c6d3b8a","journalPool":"volumes","mounter":"krbd","pool":"volumes","storage.kubernetes.io/csiProvisionerIdentity":"1723375949527-7714-rbd.csi.ceph.com"},"volume_id":"0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a"}
I0811 15:08:19.862990   13511 nodeserver.go:859] ID: 7 Req-ID: 0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a target /var/lib/kubelet/pods/bd49f109-cf95-4fce-9ea1-8bc71f04709b/volumes/kubernetes.io~csi/pvc-ed3aa009-edeb-4eaf-9f4e-761b717d7cf2/mount
isBlock false
fstype ext4
stagingPath /var/lib/kubelet/plugins/kubernetes.io/csi/rbd.csi.ceph.com/3038ce5e41af71bc02c413b0a557fb52e71e372d4db7f600d4844ceb329780eb/globalmount/0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a
readonly false
mountflags [bind _netdev discard]
I0811 15:08:19.863011   13511 mount_linux.go:218] Mounting cmd (mount) with arguments (-t ext4 -o bind,_netdev /var/lib/kubelet/plugins/kubernetes.io/csi/rbd.csi.ceph.com/3038ce5e41af71bc02c413b0a557fb52e71e372d4db7f600d4844ceb329780eb/globalmount/0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a /var/lib/kubelet/pods/bd49f109-cf95-4fce-9ea1-8bc71f04709b/volumes/kubernetes.io~csi/pvc-ed3aa009-edeb-4eaf-9f4e-761b717d7cf2/mount)
I0811 15:08:19.867260   13511 mount_linux.go:218] Mounting cmd (mount) with arguments (-t ext4 -o bind,remount,_netdev,discard /var/lib/kubelet/plugins/kubernetes.io/csi/rbd.csi.ceph.com/3038ce5e41af71bc02c413b0a557fb52e71e372d4db7f600d4844ceb329780eb/globalmount/0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a /var/lib/kubelet/pods/bd49f109-cf95-4fce-9ea1-8bc71f04709b/volumes/kubernetes.io~csi/pvc-ed3aa009-edeb-4eaf-9f4e-761b717d7cf2/mount)
I0811 15:08:19.870597   13511 nodeserver.go:742] ID: 7 Req-ID: 0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a rbd: successfully mounted stagingPath /var/lib/kubelet/plugins/kubernetes.io/csi/rbd.csi.ceph.com/3038ce5e41af71bc02c413b0a557fb52e71e372d4db7f600d4844ceb329780eb/globalmount/0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a to targetPath /var/lib/kubelet/pods/bd49f109-cf95-4fce-9ea1-8bc71f04709b/volumes/kubernetes.io~csi/pvc-ed3aa009-edeb-4eaf-9f4e-761b717d7cf2/mount
I0811 15:08:19.870626   13511 utils.go:205] ID: 7 Req-ID: 0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a GRPC response: {}
I0811 15:09:19.462522   13511 utils.go:198] ID: 8 GRPC call: /csi.v1.Identity/Probe
I0811 15:09:19.462657   13511 utils.go:199] ID: 8 GRPC request: {}
I0811 15:09:19.462771   13511 utils.go:205] ID: 8 GRPC response: {}
I0811 15:09:55.636533   13511 utils.go:198] ID: 9 GRPC call: /csi.v1.Node/NodeGetCapabilities
I0811 15:09:55.636577   13511 utils.go:199] ID: 9 GRPC request: {}
I0811 15:09:55.636734   13511 utils.go:205] ID: 9 GRPC response: {"capabilities":[{"Type":{"Rpc":{"type":1}}},{"Type":{"Rpc":{"type":2}}},{"Type":{"Rpc":{"type":3}}},{"Type":{"Rpc":{"type":5}}}]}
I0811 15:09:55.637828   13511 utils.go:198] ID: 10 GRPC call: /csi.v1.Node/NodeGetVolumeStats
I0811 15:09:55.637967   13511 utils.go:199] ID: 10 GRPC request: {"volume_id":"0001-0024-aeb95e19-6505-44f5-9221-062605cda4ee-000000000000000a-dc93419b-7109-4c39-9a58-e87b9c6d3b8a","volume_path":"/var/lib/kubelet/pods/bd49f109-cf95-4fce-9ea1-8bc71f04709b/volumes/kubernetes.io~csi/pvc-ed3aa009-edeb-4eaf-9f4e-761b717d7cf2/mount"}
E0811 15:09:55.638190   13511 utils.go:203] ID: 10 GRPC error: rpc error: code = InvalidArgument desc = targetpath /var/lib/kubelet/pods/bd49f109-cf95-4fce-9ea1-8bc71f04709b/volumes/kubernetes.io~csi/pvc-ed3aa009-edeb-4eaf-9f4e-761b717d7cf2/mount is not mounted
```
`df -h /var/lib/kubelet/pods/bd49f109-cf95-4fce-9ea1-8bc71f04709b/volumes/kubernetes.io~csi/pvc-ed3aa009-edeb-4eaf-9f4e-761b717d7cf2/mount shows it is mounted to /dev/sda5`




I gracefully shutdown the node and added:
kubectl taint nodes <node-name> [node.kubernetes.io/out-of-service=nodeshutdown:NoExecute](http://node.kubernetes.io/out-of-service=nodeshutdown:NoExecute)
kubectl taint nodes <node-name> [node.kubernetes.io/out-of-service=nodeshutdown:NoSchedule](http://node.kubernetes.io/out-of-service=nodeshutdown:NoSchedule)

I made sure no attachments left... but when node is back the rbd mounts were gone and the pod was mounted to the local file system, rbd plugin was in running state before the app

rollout restart to the app fixed it but this issue can cause data loss, any suggestions?

#### What did you expect to happen?

When node is up the pod should be mounted to the rbd PVC.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a sts with busybox and cephrbd csi pvc, add nodeName to force it to run on a certain node, reboot the node, when node is up the app will be mounted to the node's local file system

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

"v1.27.15"

</details>


#### Cloud provider

<details>
BM
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Red Hat Enterprise Linux"
VERSION="8.10 (Ootpa)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="8.10"
PLATFORM_ID="platform:el8"
PRETTY_NAME="Red Hat Enterprise Linux 8.10 (Ootpa)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:redhat:enterprise_linux:8::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8"
BUG_REPORT_URL="https://bugzilla.redhat.com/"

REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 8"
REDHAT_BUGZILLA_PRODUCT_VERSION=8.10
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="8.10"
$ uname -a
4.18.0-553.el8_10.x86_64 




```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®Issueå†…å®¹ï¼Œé‡å¯èŠ‚ç‚¹åï¼ŒåŸæœ¬åº”è¯¥æŒ‚è½½Ceph RBDå·çš„Podï¼Œé”™è¯¯åœ°å°†èŠ‚ç‚¹çš„æ ¹æ–‡ä»¶ç³»ç»Ÿï¼ˆ/ï¼‰æŒ‚è½½ä¸ºPodçš„å·ã€‚è¿™å¯¼è‡´Podå¯ä»¥è®¿é—®åˆ°èŠ‚ç‚¹çš„æ ¹æ–‡ä»¶ç³»ç»Ÿï¼Œå¯èƒ½å¼•å‘å®¹å™¨é€ƒé€¸å’Œæƒé™æå‡ç­‰é«˜å±å®‰å…¨é—®é¢˜ã€‚æ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ­¤æ¼æ´ï¼Œåœ¨å®¹å™¨å†…ç›´æ¥è®¿é—®ä¸»æœºæ–‡ä»¶ç³»ç»Ÿï¼Œè¯»å–ã€ä¿®æ”¹æ•æ„Ÿæ•°æ®ï¼Œæˆ–æ‰§è¡Œæœªç»æˆæƒçš„æ“ä½œï¼Œå±å®³ç³»ç»Ÿå®‰å…¨ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```json
[
  {
    "cmd": "cat <<EOF > statefulset.yaml\n[StatefulSetçš„YAMLé…ç½®ï¼Œä½¿ç”¨Ceph RBD PVCå¹¶æŒ‡å®šnodeName]\nEOF",
    "explain": "åˆ›å»ºä½¿ç”¨Ceph RBD PVCçš„StatefulSeté…ç½®æ–‡ä»¶ï¼Œå¹¶æŒ‡å®šnodeNameï¼Œä½¿Podè¿è¡Œåœ¨ç‰¹å®šèŠ‚ç‚¹ä¸Šã€‚"
  },
  {
    "cmd": "kubectl apply -f statefulset.yaml",
    "explain": "éƒ¨ç½²StatefulSetã€‚"
  },
  {
    "cmd": "ssh <node> 'sudo reboot'",
    "explain": "é‡å¯è¿è¡ŒPodçš„èŠ‚ç‚¹ã€‚"
  },
  {
    "cmd": "kubectl wait --for=condition=Ready pod -l app=<label> --timeout=300s",
    "explain": "ç­‰å¾…èŠ‚ç‚¹é‡å¯å®Œæ¯•ï¼ŒPodé‡æ–°è¿è¡Œã€‚"
  },
  {
    "cmd": "kubectl exec -it <pod-name> -- /bin/sh",
    "explain": "è¿›å…¥Podçš„å®¹å™¨å†…éƒ¨ã€‚"
  },
  {
    "cmd": "df -h",
    "explain": "æŸ¥çœ‹æ–‡ä»¶ç³»ç»ŸæŒ‚è½½æƒ…å†µï¼Œå‘ç°å·è¢«é”™è¯¯åœ°æŒ‚è½½åˆ°äº†ä¸»æœºçš„æ ¹æ–‡ä»¶ç³»ç»Ÿï¼ˆå¦‚/dev/sda5ï¼‰ã€‚"
  },
  {
    "cmd": "ls /root",
    "explain": "å°è¯•è®¿é—®ä¸»æœºçš„æ ¹ç›®å½•ï¼ŒæŸ¥çœ‹æ˜¯å¦èƒ½å¤Ÿè®¿é—®ä¸»æœºæ–‡ä»¶ç³»ç»Ÿã€‚"
  },
  {
    "cmd": "cat /etc/shadow",
    "explain": "å°è¯•è¯»å–ä¸»æœºçš„æ•æ„Ÿæ–‡ä»¶ï¼ŒéªŒè¯æ˜¯å¦èƒ½å¤Ÿè®¿é—®ä¸»æœºçš„/etc/shadowæ–‡ä»¶ã€‚"
  },
  {
    "cmd": "echo 'malicious content' > /root/compromised.txt",
    "explain": "å°è¯•å‘ä¸»æœºçš„æ–‡ä»¶ç³»ç»Ÿå†™å…¥æ–‡ä»¶ï¼ŒéªŒè¯æ˜¯å¦å…·æœ‰å†™æƒé™ã€‚"
  }
]
```

---

## Issue #126744 CVE-2024-7646: Ingress-nginx Annotation Validation Bypass

- Issue é“¾æ¥ï¼š[#126744](https://github.com/kubernetes/kubernetes/issues/126744)

### Issue å†…å®¹

CVSS Rating: [CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H)

A security issue was discovered in ingress-nginx where an actor with permission to create Ingress objects (in the `networking.k8s.io` or `extensions` API group) can bypass annotation validation to inject arbitrary commands and obtain the credentials of the ingress-nginx controller. In the default configuration, that credential has access to all secrets in the cluster.

This issue has been rated **High** (8.8) [CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H) and assigned **CVE-2024-7646**.

#### Am I vulnerable?

This bug affects ingress-nginx. If you do not have ingress-nginx installed on your cluster, you are not affected. You can check this by running `kubectl get po -A` and looking for `ingress-nginx-controller`.

Multi-tenant environments where non-admin users have permissions to create Ingress objects are most affected by this issue.

#### Affected Versions

ingress-nginx controller < v1.11.2
ingress-nginx controller < v1.10.4

#### How do I mitigate this vulnerability?

This issue can be mitigated by upgrading to the fixed version. 

#### Fixed Versions

ingress-nginx controller v1.11.2 - fixed by https://github.com/kubernetes/ingress-nginx/pull/11719 and https://github.com/kubernetes/ingress-nginx/pull/11721
ingress-nginx controller v1.10.4 - fixed by https://github.com/kubernetes/ingress-nginx/pull/11718 and https://github.com/kubernetes/ingress-nginx/pull/11722

#### Detection

Review your Kubernetes audit logs for Ingress objects created with annotations (e.g. `nginx.ingress.kubernetes.io/auth-tls-verify-client`) that contain carriage returns (`\r`).

If you find evidence that this vulnerability has been exploited, please contact [security@kubernetes.io](mailto:security@kubernetes.io)

#### Additional Details

See the GitHub issue for more details: 
https://github.com/kubernetes/kubernetes/issues/126744 

##### Acknowledgements

This vulnerability was reported by AndrÃ© Storfjord Kristiansen @dev-bio. 

The issue was fixed and coordinated by the fix team:
AndrÃ© Storfjord Kristiansen @dev-bio
Jintao Zhang @tao12345666333
Marco Ebert @Gacko

/triage accepted
/lifecycle frozen
/area security
/kind bug
/committee security-response

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
## åˆ†æå†…å®¹

è¯¥Issueæè¿°äº†åœ¨ingress-nginxä¸­å­˜åœ¨çš„ä¸€ä¸ªé«˜å±å®‰å…¨æ¼æ´ï¼ˆCVE-2024-7646ï¼‰ã€‚å…·æœ‰åˆ›å»ºIngresså¯¹è±¡æƒé™çš„æ”»å‡»è€…å¯ä»¥é€šè¿‡ç»•è¿‡æ³¨è§£éªŒè¯ï¼Œæ³¨å…¥ä»»æ„å‘½ä»¤å¹¶è·å–ingress-nginxæ§åˆ¶å™¨çš„å‡­è¯ã€‚åœ¨é»˜è®¤é…ç½®ä¸‹ï¼Œè¯¥å‡­è¯å…·æœ‰è®¿é—®é›†ç¾¤ä¸­æ‰€æœ‰Secretsçš„æƒé™ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ˜¯çš„ï¼Œå…·æœ‰åˆ›å»ºIngresså¯¹è±¡æƒé™çš„æ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ­¤æ¼æ´ã€‚

2. **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä½¿ç”¨CVSS 3.1è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœè¦åœ¨highä»¥ä¸Š**ï¼šè¯¥æ¼æ´å·²è¢«åˆ†é…CVEç¼–å·ï¼ŒCVSSè¯„åˆ†ä¸º8.8ï¼Œå±äºé«˜é£é™©ã€‚

8. **å¦‚æœIssueå¯èƒ½å¯¼è‡´å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰é«˜å®‰å…¨é£é™©çš„é—®é¢˜ï¼Œåˆ™æ— è®ºæ”»å‡»è€…å®æ–½è¯¥æ”»å‡»æ˜¯å¦éœ€è¦æƒé™éƒ½åº”åˆ¤æ–­ä¸ºé«˜é£é™©**ï¼šè¯¥æ¼æ´å¯ä»¥å¯¼è‡´ä»»æ„å‘½ä»¤æ‰§è¡Œå’Œæƒé™æå‡ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueå­˜åœ¨é«˜é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```json
[
  {
    "cmd": "cat > malicious-ingress.yaml <<EOF\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: malicious-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/auth-tls-verify-client: \"on\\r; echo \\`cat /var/run/secrets/kubernetes.io/serviceaccount/token\\` > /tmp/leaked-token; #\"\nspec:\n  rules:\n  - host: example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: dummy-service\n            port:\n              number: 80\nEOF",
    "explain": "åˆ›å»ºä¸€ä¸ªåŒ…å«æ¶æ„æ³¨è§£çš„Ingressé…ç½®æ–‡ä»¶ï¼Œè¯¥æ³¨è§£é€šè¿‡æ³¨å…¥æ¢è¡Œç¬¦ç»•è¿‡éªŒè¯ï¼Œå¹¶æ‰§è¡Œä»»æ„å‘½ä»¤ï¼Œå°†æœåŠ¡è´¦æˆ·ä»¤ç‰Œæ³„éœ²åˆ°/tmp/leaked-tokenã€‚"
  },
  {
    "cmd": "kubectl apply -f malicious-ingress.yaml",
    "explain": "å°†æ¶æ„Ingressèµ„æºåº”ç”¨åˆ°é›†ç¾¤ä¸­ã€‚"
  },
  {
    "cmd": "POD_NAME=$(kubectl get pods -n ingress-nginx -l app.kubernetes.io/component=controller -o jsonpath='{.items[0].metadata.name}')",
    "explain": "è·å–ingress-nginxæ§åˆ¶å™¨çš„Podåç§°ã€‚"
  },
  {
    "cmd": "kubectl exec -n ingress-nginx $POD_NAME -- cat /tmp/leaked-token",
    "explain": "åœ¨ingress-nginxæ§åˆ¶å™¨å®¹å™¨ä¸­æŸ¥çœ‹æ³„éœ²çš„æœåŠ¡è´¦æˆ·ä»¤ç‰Œã€‚"
  },
  {
    "cmd": "kubectl get secret $(kubectl get serviceaccount ingress-nginx -n ingress-nginx -o jsonpath='{.secrets[0].name}') -n ingress-nginx -o jsonpath='{.data.token}' | base64 -d",
    "explain": "è·å–ingress-nginxæœåŠ¡è´¦æˆ·çš„å®é™…ä»¤ç‰Œï¼Œä¸æ³„éœ²çš„ä»¤ç‰Œè¿›è¡Œæ¯”å¯¹ï¼ŒéªŒè¯æ¼æ´åˆ©ç”¨æˆåŠŸã€‚"
  }
]
```

---

# âš ï¸ å­˜åœ¨ä½é£é™©çš„ Issues (13 ä¸ª)

## Issue #126526 allow-metric-labels can not filter metrics

- Issue é“¾æ¥ï¼š[#126526](https://github.com/kubernetes/kubernetes/issues/126526)

### Issue å†…å®¹

#### What happened?

```
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-scheduler
    tier: control-plane
  name: kube-scheduler
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-scheduler
    - --bind-address=0.0.0.0
    - --config=/etc/kubernetes/scheduler-config.yaml
    - --v=2
    - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
    - --authorization-always-allow-paths=/healthz,/readyz,/livez,/configz,/metrics
    - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
    - --allow-metric-labels="kubernetes_feature_enabled,stage='BETA'","kubernetes_feature_enabled,name='test'"
    image: registry.aliyuncs.com/google_containers/kube-scheduler:v1.30.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-scheduler
    resources:
      requests:
        cpu: 100m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/kubernetes/scheduler.conf
      name: kubeconfig
      readOnly: true
    - mountPath: /etc/kubernetes/scheduler-config.yaml
      name: scheduler-config
      readOnly: true
    - mountPath: /etc/kubernetes/extender/
      name: cert
      readOnly: true
    - mountPath: /etc/kubernetes/certs/
      name: scheduler-cert
      readOnly: true     
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/kubernetes/scheduler.conf
      type: FileOrCreate
    name: kubeconfig
  - hostPath:
      path: /etc/kubernetes/scheduler-config.yaml
      type: FileOrCreate
    name: scheduler-config
  - hostPath:
      path: /etc/kubernetes/extender
      type: DirectoryOrCreate
    name: cert
  - hostPath:
      path: /etc/kubernetes/certs
      type: DirectoryOrCreate
    name: scheduler-cert
---


bernetes_feature_enabled{name="LogarithmicScaleDown",stage="BETA"} 1
kubernetes_feature_enabled{name="LoggingAlphaOptions",stage="ALPHA"} 0
kubernetes_feature_enabled{name="LoggingBetaOptions",stage="BETA"} 1
kubernetes_feature_enabled{name="MatchLabelKeysInPodAffinity",stage="ALPHA"} 0
kubernetes_feature_enabled{name="MatchLabelKeysInPodTopologySpread",stage="BETA"} 1
kubernetes_feature_enabled{name="MaxUnavailableStatefulSet",stage="ALPHA"} 0
kubernetes_feature_enabled{name="MemoryManager",stage="BETA"} 1
kubernetes_feature_enabled{name="MemoryQoS",stage="ALPHA"} 0
kubernetes_feature_enabled{name="MinDomainsInPodTopologySpread",stage=""} 1
kubernetes_feature_enabled{name="MultiCIDRServiceAllocator",stage="ALPHA"} 0
kubernetes_feature_enabled{name="MutatingAdmissionPolicy",stage="ALPHA"} 0
kubernetes_feature_enabled{name="NFTablesProxyMode",stage="ALPHA"} 0
kubernetes_feature_enabled{name="NewVolumeManagerReconstruction",stage=""} 1
kubernetes_feature_enabled{name="NodeInclusionPolicyInPodTopologySpread",stage="BETA"} 1
kubernetes_feature_enabled{name="NodeLogQuery",stage="BETA"} 0
kubernetes_feature_enabled{name="NodeOutOfServiceVolumeDetach",stage=""} 1
kubernetes_feature_enabled{name="NodeSwap",stage="BETA"} 1
kubernetes_feature_enabled{name="OpenAPIEnums",stage="BETA"} 1
kubernetes_feature_enabled{name="PDBUnhealthyPodEvictionPolicy",stage="BETA"} 1
kubernetes_feature_enabled{name="PersistentVolumeLastPhaseTransitionTime",stage="BETA"} 1
kubernetes_feature_enabled{name="PodAndContainerStatsFromCRI",stage="ALPHA"} 0
kubernetes_feature_enabled{name="PodDeletionCost",stage="BETA"} 1
kubernetes_feature_enabled{name="PodDisruptionConditions",stage="
```

#### What did you expect to happen?

filter metrics

#### How can we reproduce it (as minimally and precisely as possible)?

change kube-scheduler cmd line

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

1.30.0

</details>


#### Cloud provider

<details>
wmware workstation
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†åœ¨kube-schedulerä¸­ä½¿ç”¨`--allow-metric-labels`å‚æ•°æ— æ³•æ­£ç¡®è¿‡æ»¤metricsçš„é—®é¢˜ã€‚è¿™å¯èƒ½å¯¼è‡´metricsç«¯ç‚¹æš´éœ²å‡ºæ›´å¤šçš„æŒ‡æ ‡ä¿¡æ¯ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ”»å‡»è€…å¯èƒ½é€šè¿‡æœªè¿‡æ»¤çš„metricsè·å–åˆ°é›†ç¾¤çš„ä¸€äº›é…ç½®ä¿¡æ¯æˆ–çŠ¶æ€ä¿¡æ¯ï¼Œè¾…åŠ©è¿›è¡Œå…¶ä»–æ”»å‡»ã€‚

2. **é£é™©è¯„çº§**ï¼šæš´éœ²çš„metricsä¿¡æ¯é€šå¸¸ä¸åŒ…å«æ•æ„Ÿæ•°æ®ï¼Œä¸»è¦æ˜¯ä¸€äº›åŠŸèƒ½ç‰¹æ€§å¯ç”¨çŠ¶æ€ç­‰ï¼Œæ•æ„Ÿåº¦è¾ƒä½ã€‚æŒ‰ç…§CVSS 3.1è¯„åˆ†æ ‡å‡†ï¼Œå¯èƒ½å¾—åˆ†åœ¨Highä»¥ä¸‹ã€‚

3. **æƒé™æ§åˆ¶**ï¼šmetricsç«¯ç‚¹é€šå¸¸åº”å—åˆ°èº«ä»½éªŒè¯å’Œæƒé™æ§åˆ¶çš„ä¿æŠ¤ï¼Œæœªç»æˆæƒçš„ç”¨æˆ·ä¸åº”ç›´æ¥è®¿é—®ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥é—®é¢˜å­˜åœ¨ä¸€å®šçš„å®‰å…¨é£é™©ï¼Œä½†é£é™©ç­‰çº§ä¸ºä½é£é™©ã€‚

---

## Issue #126510 The event handlers of job controller are slow

- Issue é“¾æ¥ï¼š[#126510](https://github.com/kubernetes/kubernetes/issues/126510)

### Issue å†…å®¹

#### What happened?

The event handlers in job controller are much slower than other controllers. It can cause the ring buffer in the processorListener to grow unbounded when the job churn is high enough.
The processing logic in the job controller events handler appears to be more complicated than other controllers. e.g. in [updateJob](https://github.com/kubernetes/kubernetes/blob/dbc2b0a5c7acc349ea71a14e49913661eaf708d2/pkg/controller/job/job_controller.go#L464-L504), it invokes function `cleanupPodFinalizers` that lists the pods matching some selectors and the enqueue them to another queue. Similarly in [deleteJob](https://github.com/kubernetes/kubernetes/blob/dbc2b0a5c7acc349ea71a14e49913661eaf708d2/pkg/controller/job/job_controller.go#L508-L524).

I added some instrumenting code to understand the latency of each event handler.
For job controller P99:
job add event handler: 0.080 ms
job update event handler: 3.075 ms
job delete event handler: 3.076 ms
pod add event handler: 0.136 ms
pod update event handler: 0.709 ms
pod delete event handler: 0.405 ms

For ttl-after-completion controller P99:
job add event handler: 0.0065 ms
job update event handler: 0.024 ms

The above 2 controllers processes same amount of job objects.

#### What did you expect to happen?

Complicated logic should not be in the job controller event handler. They should be in the reconciler instead.
So that the event handler can be faster and pendingNotification can be drained faster.

#### How can we reproduce it (as minimally and precisely as possible)?

I use a few thousands of cronjobs with TTL after completion.
I will add more details.

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.30
<details>

Current HEAD of master
</details>


#### Cloud provider

EKS



#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†Job Controllerçš„äº‹ä»¶å¤„ç†ç¨‹åºè¾ƒæ…¢ï¼Œå½“Jobå˜åŠ¨é¢‘ç¹æ—¶ï¼ŒprocessorListenerä¸­çš„ç¯å½¢ç¼“å†²åŒºå¯èƒ½ä¼šæ— é™å¢é•¿ã€‚è¿™å¯èƒ½å¯¼è‡´å†…å­˜æ¶ˆè€—ä¸æ–­å¢åŠ ï¼Œæœ€ç»ˆå¼•å‘æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»çš„é£é™©ã€‚

ç„¶è€Œï¼Œè¦åˆ©ç”¨æ­¤æ¼æ´ï¼Œæ”»å‡»è€…éœ€è¦åˆ›å»ºå¤§é‡çš„Jobæˆ–CronJobï¼Œä»è€Œå¯¼è‡´é«˜é¢‘ç‡çš„Jobå˜åŠ¨ã€‚è¿™æ„å‘³ç€æ”»å‡»è€…éœ€è¦å…·å¤‡åˆ›å»ºJobçš„æƒé™ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬4æ¡ï¼Œå½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨Highä»¥ä¸‹ã€‚

å› æ­¤ï¼Œæ­¤Issueå­˜åœ¨å®‰å…¨é£é™©ï¼Œä½†é£é™©è¯„çº§åœ¨Highä»¥ä¸‹ï¼Œå±äºä½é£é™©ã€‚

---

## Issue #126542 Static Pods never get evicted when under Node Pressure

- Issue é“¾æ¥ï¼š[#126542](https://github.com/kubernetes/kubernetes/issues/126542)

### Issue å†…å®¹

#### What happened?

Static Pods are considered critical by [this code] (https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/eviction/eviction_manager.go#L597) and hence never get evicted.

#### What did you expect to happen?

The [doc](https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/#pod-selection-for-kubelet-eviction) says 

> If you are running a [static pod](https://kubernetes.io/docs/concepts/workloads/pods/#static-pods) and want to avoid having it evicted under resource pressure, set the priority field for that Pod directly. Static pods do not support the priorityClassName field.

At least, we should fix the inconsistency between code and doc.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Run a kubelet with fs eviction and static pods
```
apiVersion: kubelet.config.k8s.io/v1beta1
staticPodPath: /home/k8s/manifests
evictionHard:
  imagefs.available: "2Gi"
```

2. Create a static pod spamming disk
```
apiVersion: v1
kind: Pod
metadata:
  name: disk
spec:
  priority: 0
  - name: disk
    image: docker.io/busybox
    command: ["/bin/sh",  "-c", "/bin/dd if=/dev/zero of=/bigfile bs=1G count=102 && sleep 10000"]
```

3. check kubelet logs
> "Eviction manager: cannot evict a critical pod" pod="default/disk"

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>
GCP
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŒ‡å‡ºï¼Œå½“èŠ‚ç‚¹èµ„æºå‹åŠ›è¿‡å¤§æ—¶ï¼Œé™æ€Podï¼ˆStatic Podsï¼‰æ°¸è¿œä¸ä¼šè¢«é©±é€ï¼ˆevictedï¼‰ï¼Œå› ä¸ºåœ¨ä»£ç å®ç°ä¸­ï¼Œé™æ€Podè¢«è§†ä¸ºå…³é”®Podï¼ˆcritical podï¼‰ï¼Œå› æ­¤è¢«é©±é€ç®¡ç†å™¨å¿½ç•¥ã€‚

æ”»å‡»è€…å¦‚æœèƒ½å¤Ÿåˆ›å»ºé™æ€Podï¼Œå¹¶è¿è¡Œå ç”¨å¤§é‡èµ„æºçš„ä»»åŠ¡ï¼ˆå¦‚å¡«æ»¡ç£ç›˜ç©ºé—´çš„æ“ä½œï¼‰ï¼Œå¯èƒ½å¯¼è‡´èŠ‚ç‚¹èµ„æºè€—å°½ï¼Œå½±å“å…¶ä»–Podçš„æ­£å¸¸è¿è¡Œï¼Œé€ æˆæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚

ç„¶è€Œï¼Œåˆ›å»ºæˆ–ä¿®æ”¹é™æ€Podéœ€è¦å¯¹èŠ‚ç‚¹æ–‡ä»¶ç³»ç»Ÿçš„å†™æƒé™ï¼Œé€šå¸¸åªæœ‰é›†ç¾¤ç®¡ç†å‘˜æˆ–å…·æœ‰é«˜æƒé™çš„ç”¨æˆ·æ‰èƒ½è¿›è¡Œæ­¤æ“ä½œã€‚æ™®é€šç”¨æˆ·æ— æ³•åœ¨æœªç»æˆæƒçš„æƒ…å†µä¸‹åˆ›å»ºé™æ€Podã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

- **ç¬¬4æ¡**ï¼šå½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨highä»¥ä¸‹ã€‚
- **ç¬¬8æ¡**ï¼šå¦‚æœIssueå¯èƒ½å¯¼è‡´æ‹’ç»æœåŠ¡æ”»å‡»ï¼Œä½†éœ€è¦é«˜æƒé™æ‰èƒ½å®æ–½æ”»å‡»ï¼Œåˆ™é£é™©åº”é€‚å½“é™ä½ã€‚

å› æ­¤ï¼Œå°½ç®¡è¯¥é—®é¢˜å¯èƒ½å¯¼è‡´èŠ‚ç‚¹èµ„æºè€—å°½ï¼Œä½†ç”±äºéœ€è¦é«˜æƒé™æ‰èƒ½è¢«åˆ©ç”¨ï¼Œæ•…é£é™©è¯„çº§ä¸º**ä½é£é™©**ã€‚

---

## Issue #126608 kubectl >= 1.30.0 triggers leak of goroutines in containerd on `kubectl exec`

- Issue é“¾æ¥ï¼š[#126608](https://github.com/kubernetes/kubernetes/issues/126608)

### Issue å†…å®¹

#### What happened?

Some of our developers were running a workload that regularly does `kubectl exec` into pods running in an EKS cluster with a command that collects some metrics, then exits.

We noticed an increase of memory usage by the containerd process on the host running the pod into which the `kubectl exec` connected. Turning on the debug socket of containerd and comparing the output of the `ctr pprof goroutines` command executed multiple times over the span of several minutes showed that goroutines were leaking inside containerd. All leaked goroutines had a stack trace like this:

```
goroutine 82060 [chan receive]:
k8s.io/apiserver/pkg/util/wsstream.(*Conn).Open(0xc006ba0af0, {0x55fe91acc880?, 0xc0055d8ee0}, 0xc00a1bcb00)
	/builddir/build/BUILD/containerd-1.6.19-1.amzn2.0.1/src/github.com/containerd/containerd/vendor/k8s.io/apiserver/pkg/util/wsstream/conn.go:185 +0xc5
github.com/containerd/containerd/pkg/cri/streaming/remotecommand.createWebSocketStreams(0xc00a1bcb00?, {0x55fe91acc880, 0xc0055d8ee0}, 0xc0085c17f4, 0xd18c2e28000)
	/builddir/build/BUILD/containerd-1.6.19-1.amzn2.0.1/src/github.com/containerd/containerd/pkg/cri/streaming/remotecommand/websocket.go:114 +0x32c
github.com/containerd/containerd/pkg/cri/streaming/remotecommand.createStreams(0xc00a1bcb00, {0x55fe91acc880, 0xc0055d8ee0}, 0xc0085c1690?, {0x55fe926be380, 0x4, 0x4}, 0x203001?, 0xc006d8bb00?)
	/builddir/build/BUILD/containerd-1.6.19-1.amzn2.0.1/src/github.com/containerd/containerd/pkg/cri/streaming/remotecommand/httpstream.go:126 +0x9b
github.com/containerd/containerd/pkg/cri/streaming/remotecommand.ServeExec({0x55fe91acc880?, 0xc0055d8ee0?}, 0x6?, {0x55fe91ab5af8, 0xc0006b0ed0}, {0x0, 0x0}, {0x0, 0x0}, {0xc006b918c0, ...}, ...)
	/builddir/build/BUILD/containerd-1.6.19-1.amzn2.0.1/src/github.com/containerd/containerd/pkg/cri/streaming/remotecommand/exec.go:61 +0xc5
github.com/containerd/containerd/pkg/cri/streaming.(*server).serveExec(0xc00040a090, 0xc004cfa630, 0xc006be77a0)
	/builddir/build/BUILD/containerd-1.6.19-1.amzn2.0.1/src/github.com/containerd/containerd/pkg/cri/streaming/server.go:302 +0x19e
github.com/emicklei/go-restful.(*Container).dispatch(0xc00040a1b0, {0x55fe91acc880, 0xc0055d8ee0}, 0xc00a1bcb00)
	/builddir/build/BUILD/containerd-1.6.19-1.amzn2.0.1/src/github.com/containerd/containerd/vendor/github.com/emicklei/go-restful/container.go:288 +0x8c8
net/http.HandlerFunc.ServeHTTP(0x0?, {0x55fe91acc880?, 0xc0055d8ee0?}, 0x0?)
	/usr/lib/golang/src/net/http/server.go:2084 +0x2f
net/http.(*ServeMux).ServeHTTP(0x72?, {0x55fe91acc880, 0xc0055d8ee0}, 0xc00a1bcb00)
	/usr/lib/golang/src/net/http/server.go:2462 +0x149
github.com/emicklei/go-restful.(*Container).ServeHTTP(0x0?, {0x55fe91acc880?, 0xc0055d8ee0?}, 0xc0074c0000?)
	/builddir/build/BUILD/containerd-1.6.19-1.amzn2.0.1/src/github.com/containerd/containerd/vendor/github.com/emicklei/go-restful/container.go:303 +0x27
net/http.serverHandler.ServeHTTP({0xc004cfa510?}, {0x55fe91acc880, 0xc0055d8ee0}, 0xc00a1bcb00)
	/usr/lib/golang/src/net/http/server.go:2916 +0x43b
net/http.(*conn).serve(0xc0054b1a40, {0x55fe91acdbd8, 0xc003dc2420})
	/usr/lib/golang/src/net/http/server.go:1966 +0x5d7
created by net/http.(*Server).Serve
	/usr/lib/golang/src/net/http/server.go:3071 +0x4db
```

The number of leaked goroutines can be checked with:
```console
$ ctr pprof goroutines | grep createWebSocketStreams | wc -l
```

This looks like the websocket connection from kubelet to containerd is somehow stuck, however the `kubectl exec` command works just fine.

The version of kubectl used by the workload is `v1.30.2`, while the cluster itself is running AWS EKS `v1.26.15-eks-db838b0` (kubelet: `v1.26.4-eks-0a21954`) and we could reproduce this as well on a cluster running `v1.28.11-eks-db838b0` (kubelet: `v1.28.11-eks-1552ad0`).
After checking some versions, it looks like this behavior started with kubectl `v1.30.0` and was not present in `v1.29.4` yet.
We are aware that this is an unsupported version discrepancy, but it shouldn't be possible for a normal user of the Kubernetes API to trigger a memory leak in the container runtime by simply running `kubectl exec` repeatedly against a pod with an incompatible version of kubectl.

#### What did you expect to happen?

Neither kubectl nor any other Kubernetes API client should be able to cause a memory leak in the container runtime of the host running the affected pod.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Start an EKS cluster v1.26 or v1.28 (probably 1.27 or earlier versions and plain Kubernetes clusters are affected as well, but we couldn't check this) and ensure some nodes are up and running and some pods are running as well.
2. SSH into a node and enable the containerd debug socket by appending the following to `/etc/containerd/config.toml`:
    ```toml
    [debug]
      address = "/run/containerd/debug.sock"
    ```
    Then restart containerd with `systemctl restart containerd`, then run:
    ```console
    $ watch "ctr pprof goroutines | grep createWebSocketStreams | wc -l"
    ```
4. In a second terminal, run `kubectl exec <pod name> -- true` or similar repeatedly against any pod running on the node on which you SSH'd.
5. Notice the number reported by the `watch` command increase for every execution of `kubectl`.

#### Anything else we need to know?

We were unsure where the right place to report this issue would be. We believe containerd should have timeout on this goroutine to abort it if something gets stuck. On the other hand, this can only be triggered with a newer certain version of kubectl, which also hints a bug in the Kubernetes API server or in kubelet.

#### Kubernetes version

<details>

1.26 cluster:
```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.26.15-eks-db838b0
WARNING: version difference between client (1.30) and server (1.26) exceeds the supported minor version skew of +/-1
```

1.28 cluster:
```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.11-eks-db838b0
WARNING: version difference between client (1.30) and server (1.28) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
AWS EKS
</details>


#### OS version

<details>

1.26 cluster:
```console
$ cat /etc/os-release
NAME="Amazon Linux"
VERSION="2"
ID="amzn"
ID_LIKE="centos rhel fedora"
VERSION_ID="2"
PRETTY_NAME="Amazon Linux 2"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2"
HOME_URL="https://amazonlinux.com/"
$ uname -a
Linux ip-a-b-c-d.us-east-2.compute.internal 5.10.179-168.710.amzn2.x86_64 #1 SMP Mon May 22 23:10:22 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
```

1.28 cluster:
```console
$ cat /etc/os-release
NAME="Amazon Linux"
VERSION="2"
ID="amzn"
ID_LIKE="centos rhel fedora"
VERSION_ID="2"
PRETTY_NAME="Amazon Linux 2"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2"
HOME_URL="https://amazonlinux.com/"
SUPPORT_END="2025-06-30"

$ uname -a
Linux ip-a-b-c-d.us-east-1.compute.internal 5.10.220-209.869.amzn2.x86_64 #1 SMP Wed Jul 17 15:10:20 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Verified on containerd `1.6.19` and `1.7.11`
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
é—®é¢˜æè¿°äº†å½“ä½¿ç”¨ä¸å…¼å®¹ç‰ˆæœ¬çš„kubectlå®¢æˆ·ç«¯ï¼ˆv1.30.2åŠä»¥ä¸Šï¼‰å¯¹è¾ƒä½ç‰ˆæœ¬çš„Kubernetesé›†ç¾¤ï¼ˆv1.26æˆ–v1.28ï¼‰æ‰§è¡Œ`kubectl exec`æ“ä½œæ—¶ï¼Œä¼šå¯¼è‡´è¿è¡Œåœ¨èŠ‚ç‚¹ä¸Šçš„containerdè¿›ç¨‹å‡ºç°goroutineæ³„æ¼ï¼Œå†…å­˜ä½¿ç”¨é‡å¢åŠ ã€‚è¿™å¯èƒ½è¢«å…·æœ‰`kubectl exec`æƒé™çš„ç”¨æˆ·åˆ©ç”¨ï¼Œé€šè¿‡å¤šæ¬¡æ‰§è¡Œ`kubectl exec`å¯¼è‡´èŠ‚ç‚¹å†…å­˜è€—å°½ï¼Œé€ æˆæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼Œä½†å‰ææ˜¯æ”»å‡»è€…å…·æœ‰æ‰§è¡Œ`kubectl exec`çš„æƒé™ã€‚

4. **å¯¹äºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»**ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å®æ–½è¯¥æ”»å‡»ï¼Œä¸”éœ€è¦å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨highä»¥ä¸‹ã€‚

å› æ­¤ï¼Œå°½ç®¡å­˜åœ¨å®‰å…¨é£é™©ï¼Œä½†ç”±äºéœ€è¦ä¸€å®šæƒé™ï¼Œä¸”å±äºDoSæ”»å‡»èŒƒç•´ï¼Œæ ¹æ®æ ‡å‡†ï¼Œè¯¥é£é™©è¯„çº§åº”åˆ¤æ–­ä¸º**ä½é£é™©**ã€‚

---

## Issue #126662 QosClass of pod status shouldn't be changeable

- Issue é“¾æ¥ï¼š[#126662](https://github.com/kubernetes/kubernetes/issues/126662)

### Issue å†…å®¹

#### What happened?

According to this [code](https://github.com/kubernetes/kubernetes/blob/bbe8ca8b2ab14992389bc67e3bcfa209adcb13d4/pkg/apis/core/validation/validation.go#L4996), the QosClass of pod status is immutable. 

This [function ](https://github.com/kubernetes/kubernetes/blob/40b604e374144351eac463e7077fdb1903f5a033/pkg/apis/core/helper/qos/qos.go#L48
)also shows that QosClass is computed from the resources instead of being provided by users. 


However, we can use rest api or "kubectl edit pod <podName> --subresource='status'" to change it. 




#### What did you expect to happen?

The request to change QoSClass of pod status should receive a validation error to indicate it's immutable. 

#### How can we reproduce it (as minimally and precisely as possible)?

Use "kubectl edit pod <podName> --subresource='status'" to change the qosClass to a different value. It will succeed. 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.4
```

</details>


#### Cloud provider

<details>
Azure
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
IssueæŒ‡å‡ºï¼Œåœ¨Kubernetesä¸­ï¼ŒPodçš„QoSClasså±æ€§åº”è¯¥æ˜¯ä¸å¯å˜çš„ï¼Œä¸”æ˜¯æ ¹æ®Podçš„èµ„æºè¯·æ±‚å’Œé™åˆ¶è®¡ç®—å¾—å‡ºçš„ã€‚ä½†æ˜¯ï¼Œé€šè¿‡REST APIæˆ–`kubectl edit pod <podName> --subresource='status'`ï¼Œå¯ä»¥ä¿®æ”¹Podçš„QoSClassã€‚

è¿™ä¸ªé—®é¢˜çš„æ½œåœ¨é£é™©åœ¨äºï¼Œæ”»å‡»è€…å¯èƒ½é€šè¿‡ä¿®æ”¹Podçš„QoSClassï¼Œæå‡è‡ªå·±Podçš„QoSçº§åˆ«ï¼ˆå¦‚ä»Burstableæå‡åˆ°Guaranteedï¼‰ï¼Œä»è€Œåœ¨èµ„æºç´§å¼ æ—¶é¿å…è¢«é©±é€ï¼Œå½±å“é›†ç¾¤ä¸­å…¶ä»–ç”¨æˆ·çš„Podè¢«ä¼˜å…ˆé©±é€ã€‚

ç„¶è€Œï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ‰§è¡Œæ­¤æ“ä½œéœ€è¦å¯¹Podçš„statuså­èµ„æºå…·æœ‰ç¼–è¾‘æƒé™ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œç¼–è¾‘Podçš„statuså­èµ„æºéœ€è¦è¾ƒé«˜çš„æƒé™ï¼Œæ™®é€šç”¨æˆ·é€šå¸¸ä¸å…·å¤‡è¯¥æƒé™ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

- **æ ‡å‡†1**ï¼šè¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œä½†éœ€è¦è¾ƒé«˜æƒé™ã€‚
- **æ ‡å‡†4**ï¼šå½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨highä»¥ä¸‹ã€‚

å› æ­¤ï¼Œç»¼åˆåˆ¤æ–­ï¼Œè¯¥Issueå­˜åœ¨å®‰å…¨é£é™©ï¼Œä½†é£é™©è¯„çº§ä¸ºä½é£é™©ã€‚

---

## Issue #126643 kube-scheduler updates pod status mistakenly during preemption

- Issue é“¾æ¥ï¼š[#126643](https://github.com/kubernetes/kubernetes/issues/126643)

### Issue å†…å®¹

#### What happened?

When a pod with "BestEffort" of qosClass is preempted by a higher priority pod whose qolClass is "Burstable", the victim pod's qosClass will be updated to "Burstable" because [the scheduler will update the victim's status with the content from higher priority pod before deleting the victim pod](https://github.com/kubernetes/kubernetes/blob/34e620d18c036acf035cb42c6f445dd568f60303/pkg/scheduler/framework/preemption/preemption.go#L375). The pod then will be stuck in terminating state after deleting because of this [check](https://github.com/kubernetes/kubernetes/blob/bbe8ca8b2ab14992389bc67e3bcfa209adcb13d4/pkg/apis/core/validation/validation.go#L4996). Supposedly, kubelet will reconcile the status back very soon. However, the deletion request stops kubelet from doing that.

#### What did you expect to happen?

The victim pod's qosClass shouldn't be changed and the pod should be deleted successfully. 

#### How can we reproduce it (as minimally and precisely as possible)?

 reproduced it on 1.29.4

Reproduce steps:
Scale the node number to 1
- Create victim pod (here I use a cronjob to create a pod with empty resources)
- Create a higher priority class(p1) with "Preempt" policy.
- Create extra pods(using a deployment) with priorityClassName as "p1" to make the pod number beyond the maxim pod number of the node. The pods also have cpu/memory requests set.
- The victim pod will be preempted, then stuck in "terminating" state
[cronjob.txt](https://github.com/user-attachments/files/16592258/cronjob.txt)
[deployment.txt](https://github.com/user-attachments/files/16592259/deployment.txt)
[pc.txt](https://github.com/user-attachments/files/16592260/pc.txt)


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.4
```

</details>


#### Cloud provider

<details>
Azure
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetesä¸­ï¼Œå½“ä¸€ä¸ª"BestEffort"çš„Podè¢«ä¸€ä¸ªå…·æœ‰æ›´é«˜ä¼˜å…ˆçº§ä¸”qosClassä¸º"Burstable"çš„PodæŠ¢å æ—¶ï¼Œå—å®³Podçš„qosClassä¼šè¢«é”™è¯¯åœ°æ›´æ–°ä¸º"Burstable"ã€‚è¿™å¯¼è‡´å—å®³Podåœ¨åˆ é™¤æ—¶å› éªŒè¯å¤±è´¥è€Œå¡åœ¨"Terminating"çŠ¶æ€ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ”»å‡»è€…éœ€è¦å…·å¤‡åˆ›å»ºé«˜ä¼˜å…ˆçº§Podçš„æƒé™ï¼Œæ‰èƒ½è§¦å‘è¯¥é—®é¢˜ã€‚

4. **åœ¨æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»æ—¶ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å¤Ÿå®æ–½è¯¥æ”»å‡»ï¼Œåˆ™è§†æƒ…å†µéœ€è¦é™çº§å¤„ç†**ï¼šæ­¤æƒ…å†µä¸‹ï¼Œæ”»å‡»è€…éœ€è¦å…·å¤‡åˆ›å»ºæˆ–ä¿®æ”¹Podçš„æƒé™ï¼Œå› æ­¤ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨highä»¥ä¸‹ã€‚

å› æ­¤ï¼Œè¯¥é—®é¢˜å±äºä½é£é™©çš„å®‰å…¨é—®é¢˜ï¼Œå±äºæ‹’ç»æœåŠ¡ç±»å‹ï¼Œä¸”éœ€è¦ä¸€å®šçš„æƒé™æ‰èƒ½è¢«åˆ©ç”¨ã€‚

---

## Issue #126631 kubectl: panic when describe ingress with no Backend

- Issue é“¾æ¥ï¼š[#126631](https://github.com/kubernetes/kubernetes/issues/126631)

### Issue å†…å®¹

#### What happened?

k describe ing test-ing-without-backend

```
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x0 pc=0x1017fde94]

goroutine 1 [running]:
k8s.io/kubernetes/vendor/k8s.io/kubectl/pkg/describe.(*IngressDescriber).describeBackendV1(0x140005d3070?, {0x140005c7057?, 0x1019d6ab8?}, 0x13?)
	vendor/k8s.io/kubectl/pkg/describe/describe.go:2593 +0x24
k8s.io/kubernetes/vendor/k8s.io/kubectl/pkg/describe.(*IngressDescriber).describeIngressV1.func1({0x10226f920?, 0x140003a0000})
	vendor/k8s.io/kubectl/pkg/describe/describe.go:2667 +0x338
k8s.io/kubernetes/vendor/k8s.io/kubectl/pkg/describe.tabbedString(0x140008c9a08)
	vendor/k8s.io/kubectl/pkg/describe/describe.go:5245 +0x80
k8s.io/kubernetes/vendor/k8s.io/kubectl/pkg/describe.(*IngressDescriber).describeIngressV1(0x12b104420?, 0x14000d886b0?, 0x1022749e0?)
	vendor/k8s.io/kubectl/pkg/describe/describe.go:2627 +0x40
k8s.io/kubernetes/vendor/k8s.io/kubectl/pkg/describe.(*IngressDescriber).Describe(0x14000d889a0, {0x14000c071b5, 0x5}, {0x16f6e346d, 0xd}, {0x0?, 0x14000ad3b28?})
	vendor/k8s.io/kubectl/pkg/describe/describe.go:2553 +0x150
k8s.io/kubernetes/vendor/k8s.io/kubectl/pkg/cmd/describe.(*DescribeOptions).Run(0x1400060e320)
	vendor/k8s.io/kubectl/pkg/cmd/describe/describe.go:193 +0x40c
k8s.io/kubernetes/vendor/k8s.io/kubectl/pkg/cmd/describe.NewCmdDescribe.func1(0x1400060c500?, {0x14000111760?, 0x2?, 0x2?})
	vendor/k8s.io/kubectl/pkg/cmd/describe/describe.go:113 +0x64
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).execute(0x1400060c500, {0x14000111700, 0x2, 0x2})
	vendor/github.com/spf13/cobra/command.go:860 +0x4ac
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).ExecuteC(0x14000935900)
	vendor/github.com/spf13/cobra/command.go:974 +0x354
k8s.io/kubernetes/vendor/github.com/spf13/cobra.(*Command).Execute(...)
	vendor/github.com/spf13/cobra/command.go:902
k8s.io/kubernetes/vendor/k8s.io/component-base/cli.run(0x14000935900)
	vendor/k8s.io/component-base/cli/run.go:146 +0x2a4
k8s.io/kubernetes/vendor/k8s.io/component-base/cli.RunNoErrOutput(...)
	vendor/k8s.io/component-base/cli/run.go:84
main.main()
	cmd/kubectl/kubectl.go:30 +0x20
```



#### What did you expect to happen?

k describe ing test-ing-without-backend  work without error
 

#### How can we reproduce it (as minimally and precisely as possible)?

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ing-without-backend
  annotations:
    nginx.ingress.kubernetes.io/server-snippet: |
      location = / {
        return 200 'OK';
      }
spec:
  rules:
  - host: a.com
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"24", GitVersion:"v1.24.12", GitCommit:"ef70d260f3d036fc22b30538576bbf6b36329995", GitTreeState:"clean", BuildDate:"2023-03-15T13:37:18Z", GoVersion:"go1.19.7", Compiler:"gc", Platform:"darwin/arm64"}
Kustomize Version: v4.5.4
Server Version: version.Info{Major:"1", Minor:"29", GitVersion:"v1.29.5-gke.1091002", GitCommit:"0126fa256b86df67042c3c08915a8f633f2ab5f5", GitTreeState:"clean", BuildDate:"2024-05-29T09:14:46Z", GoVersion:"go1.21.9 X:boringcrypto", Compiler:"gc", Platform:"linux/amd64"}
WARNING: version difference between client (1.24) and server (1.29) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†å½“ä½¿ç”¨kubectlæè¿°ä¸€ä¸ªæ²¡æœ‰Backendçš„Ingressæ—¶ï¼Œä¼šå¯¼è‡´kubectlå®¢æˆ·ç«¯å‘ç”Ÿpanicå¹¶å´©æºƒã€‚æ”»å‡»è€…å¯ä»¥é€šè¿‡åˆ›å»ºä¸€ä¸ªç‰¹æ®Šçš„Ingressèµ„æºï¼Œå¼•è¯±å…¶ä»–ç”¨æˆ·ä½¿ç”¨kubectlæè¿°è¯¥Ingressï¼Œä»è€Œå¯¼è‡´ä»–ä»¬çš„kubectlå®¢æˆ·ç«¯å´©æºƒã€‚

ç„¶è€Œï¼Œæ”»å‡»è€…éœ€è¦å…·å¤‡åˆ›å»ºæˆ–ä¿®æ”¹Ingressèµ„æºçš„æƒé™æ‰èƒ½å®æ–½æ­¤æ”»å‡»ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬4æ¡ï¼Œé’ˆå¯¹æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å¤Ÿå®æ–½è¯¥æ”»å‡»ï¼Œä¸”éœ€è¦åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨highä»¥ä¸‹ã€‚

å› æ­¤ï¼Œè¯¥Issueå­˜åœ¨å®‰å…¨é£é™©ï¼Œä½†é£é™©è¯„çº§åœ¨highä»¥ä¸‹ï¼Œåˆ¤æ–­ä¸ºä½é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```json
[
  {
    "cmd": "cat > ingress.yaml <<EOF\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: test-ing-without-backend\n  annotations:\n    nginx.ingress.kubernetes.io/server-snippet: |\n      location = / {\n        return 200 'OK';\n      }\nspec:\n  rules:\n  - host: a.com\nEOF",
    "explain": "åˆ›å»ºä¸€ä¸ªæ²¡æœ‰Backendçš„Ingressèµ„æºæ–‡ä»¶ã€‚"
  },
  {
    "cmd": "kubectl apply -f ingress.yaml",
    "explain": "åº”ç”¨Ingressé…ç½®ï¼Œåˆ›å»ºIngressèµ„æºã€‚"
  },
  {
    "cmd": "kubectl describe ing test-ing-without-backend",
    "explain": "æè¿°è¯¥Ingressèµ„æºï¼Œè§¦å‘kubectlå®¢æˆ·ç«¯çš„panicå´©æºƒã€‚"
  }
]
```

---

## Issue #126746 kube-proxy fails with CreateContainerConfigError when joining a 1.30.0 node to 1.30.4 cluster

- Issue é“¾æ¥ï¼š[#126746](https://github.com/kubernetes/kubernetes/issues/126746)

### Issue å†…å®¹

#### What happened?

I have a cluster which is 1.30.4 and I am trying to join a node to the cluster that is 1.31.0, and the kube-proxy fails to start, and the status is `CreateContainerConfigError` . 

`kubectl describe pod` events show:
```
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  65s                default-scheduler  Successfully assigned kube-system/kube-proxy-gc7zf to server2
  Normal   Pulled     11s (x6 over 63s)  kubelet            Container image "registry.k8s.io/kube-proxy:v1.30.4" already present on machine
  Warning  Failed     11s (x6 over 63s)  kubelet            Error: services have not yet been read at least once, cannot construct envvars
```
The kubelet logs shows:
`GET https://<apiserverip of cluster>:6443/api/v1/services?fieldSelector=spec.clusterIP%21%3DNone&limit=500&resourceVersion=0 400 Bad Request in 7 milliseconds`

And the response is 
`failed to list *v1.Service: "spec.clusterIP" is not a known field selector: only "metadata.name", "metadata.namespace"`

All-Beta=False feature-gate is coded for all components. 
Enabling the `CustomResourceFieldSelectors` on the downlevel node seemed to have no effect.

The problem seems to be that even though the joining node downloaded the correct images and start the pods, the kubelet of course is still at 1.31.0.   I suspect installing a downlevel kubelet will allow it to work.

Is this even a supported thing to do?  i.e.  join a downlevel cluster with an uplevel node? 

#### What did you expect to happen?

I would have expected with AllBeta=False, that the request would not be sent with `spec.clusterIP`  
I also would have also expected that enabling the `CustomResourceFieldSelectors` on the downlevel node would have allowed the spec.clusterIP field selector.

#### How can we reproduce it (as minimally and precisely as possible)?

Bring up an 1.30.4 node with `kubeadm init`
Attempt to join a 1.31.0 node to the cluster.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.4

```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Ubuntu"
VERSION="20.04.6 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.6 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal

$ uname -a
Linux server1 5.4.0-186-generic #206-Ubuntu SMP Fri Apr 26 12:31:10 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
1.30.0
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨å°†ä¸€ä¸ªè¿è¡Œ1.31.0ç‰ˆæœ¬çš„èŠ‚ç‚¹åŠ å…¥åˆ°ä¸€ä¸ªè¿è¡Œ1.30.4ç‰ˆæœ¬çš„Kubernetesé›†ç¾¤æ—¶ï¼Œkube-proxyæ— æ³•å¯åŠ¨å¹¶æŠ¥é”™ã€‚é”™è¯¯ä¿¡æ¯æ˜¾ç¤ºç”±äºAPIç‰ˆæœ¬ä¸å…¼å®¹ï¼Œå¯¼è‡´åœ¨ä½¿ç”¨å­—æ®µé€‰æ‹©å™¨æ—¶å‚æ•°æ— æ•ˆã€‚

æ½œåœ¨çš„å®‰å…¨é£é™©åœ¨äºï¼Œå¦‚æœå­˜åœ¨ç‰ˆæœ¬ä¸å…¼å®¹çš„é—®é¢˜ï¼Œå¯èƒ½ä¼šå¯¼è‡´æœåŠ¡ä¸­æ–­ï¼ˆDoSï¼‰ï¼Œå½±å“é›†ç¾¤çš„å¯ç”¨æ€§ã€‚ç„¶è€Œï¼Œæ­¤é—®é¢˜éœ€è¦ç®¡ç†å‘˜çº§åˆ«çš„æƒé™æ‰èƒ½å°è¯•å°†é«˜ç‰ˆæœ¬çš„èŠ‚ç‚¹åŠ å…¥ä½ç‰ˆæœ¬çš„é›†ç¾¤ï¼Œå±äºæ“ä½œé…ç½®ä¸å½“å¼•èµ·çš„é—®é¢˜ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬4æ¡ï¼Œå½“é£é™©ä¸ºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»æ—¶ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å®æ–½è¯¥æ”»å‡»ï¼Œå¦‚éœ€è¦åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨highä»¥ä¸‹ã€‚

å› æ­¤ï¼Œè¯¥Issueä¸­å­˜åœ¨ä¸€å®šçš„å®‰å…¨é£é™©ï¼Œä½†é£é™©è¯„çº§åœ¨highä»¥ä¸‹ï¼Œåˆ¤æ–­ä¸ºä½é£é™©ã€‚

---

## Issue #126711 static pod stuck in "Waiting for volumes to unmount for pod" for a longtime on single node by chance

- Issue é“¾æ¥ï¼š[#126711](https://github.com/kubernetes/kubernetes/issues/126711)

### Issue å†…å®¹

#### What happened?

1. when update static podï¼ˆkube-apiserverï¼‰yaml, the static pod maybe stuck for 20 minutes to 2 hours, some logs show as follow
```
Aug 15 20:18:20 node1 hyperkube[1974327]: I0815 20:18:20.604518 1974327 actual_state_of_world.go:973] "Pod mounted volumes" uniquePodName=9d45620a-ae62-4ee3-bdb7-139998904a99 mountedVolume=[{MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-positions InnerVolumeSpecName:positions OuterVolumeSpecName:positions PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc006092940 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7a70 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/projected/9d45620a-ae62-4ee3-bdb7-139998904a99-kube-api-access-wd5vj InnerVolumeSpecName:kube-api-access-wd5vj OuterVolumeSpecName:kube-api-access-wd5vj PluginName:kubernetes.io/projected PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc005345100 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7b48 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-test-apiserver InnerVolumeSpecName:var-log-test-apiserver OuterVolumeSpecName:var-log-test-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc003817600 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7ab8 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-kube-apiserver InnerVolumeSpecName:var-log-kube-apiserver OuterVolumeSpecName:var-log-kube-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc005f320c0 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7ae8 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-oauth-apiserver InnerVolumeSpecName:var-log-oauth-apiserver OuterVolumeSpecName:var-log-oauth-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc00ae8edc0 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7b18 DeviceMountPath: SELinuxMountContext:}}]
Aug 15 20:18:29 node1 hyperkube[1974327]: I0815 20:18:29.606947 1974327 file.go:202] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:18:29 node1 hyperkube[1974327]: I0815 20:18:29.607559 1974327 common.go:69] "Generated UID" pod="test-kube-apiserver/kube-apiserver" podUID=d6c287e864be7616ad8256abb9e925b1 source="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:18:29 node1 hyperkube[1974327]: I0815 20:18:29.607570 1974327 common.go:73] "Generated pod name" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1 source="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:18:29 node1 hyperkube[1974327]: I0815 20:18:29.607581 1974327 common.go:78] "Set namespace for pod" pod="test-kube-apiserver/kube-apiserver-node1" source="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.381574 1974327 volume_manager.go:469] "Some volumes still mounted for pod" pod="test-kube-apiserver/kube-apiserver-node1" mountedVolumes=[audit-dir cert-dir resource-dir]
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.381589 1974327 kubelet.go:1976] "SyncTerminatedPod exit" pod="test-kube-apiserver/kube-apiserver-node1" podUID=bb8823c6e574ba7c1215633f4bf0f7d7
Aug 15 20:18:31 node1 hyperkube[1974327]: E0815 20:18:31.381600 1974327 pod_workers.go:1256] "Error syncing pod, skipping" err="mounted volumes=[audit-dir cert-dir resource-dir]: timed out waiting for the condition" pod="test-kube-apiserver/kube-apiserver-node1" podUID=bb8823c6e574ba7c1215633f4bf0f7d7
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.381628 1974327 pod_workers.go:1293] "Processing pod event done" pod="test-kube-apiserver/kube-apiserver-node1" podUID=bb8823c6e574ba7c1215633f4bf0f7d7 updateType="terminated"
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.381641 1974327 pod_workers.go:1188] "Processing pod event" pod="test-kube-apiserver/kube-apiserver-node1" podUID=bb8823c6e574ba7c1215633f4bf0f7d7 updateType="terminated"
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.508902 1974327 kubelet.go:2261] "SyncLoop (SYNC) pods" total=5 pods=[test-kube-apiserver/kube-apiserver-node1 test-doko/doko-ingress-proxy-tp5d5 test-cnv/hci-compute-fileserver-5874bc85df-gpph2 test-vnet-operator/vnet-operator-controller-manager-6cdcbdb49-bcrkq test-cluster-node-tuning-operator/cluster-node-tuning-operator-5bbb5fd999-r9nkg]
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.508930 1974327 pod_workers.go:931] "Notifying pod of pending update" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1 workType="sync"
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.509044 1974327 pod_workers.go:1138] "Pod cannot start yet" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.517993 1974327 kubelet.go:1965] "SyncTerminatedPod enter" pod="test-kube-apiserver/kube-apiserver-node1" podUID=bb8823c6e574ba7c1215633f4bf0f7d7
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.518002 1974327 kubelet_pods.go:1605] "Generating pod status" pod="test-kube-apiserver/kube-apiserver-node1"
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.518048 1974327 kubelet_pods.go:1615] "Got phase for pod" pod="test-kube-apiserver/kube-apiserver-node1" oldPhase=Pending phase=Pending
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.518100 1974327 status_manager.go:532] "updateStatusInternal" version=1 pod="test-kube-apiserver/kube-apiserver-node1" podUID=bb8823c6e574ba7c1215633f4bf0f7d7 containers="(kube-apiserver state=waiting previous=<none>) (kube-apiserver-cert-regeneration-controller state=waiting previous=<none>) (kube-apiserver-cert-syncer state=waiting previous=<none>) (kube-apiserver-check-endpoints state=waiting previous=<none>) (kube-apiserver-insecure-readyz state=waiting previous=<none>) (setup state=waiting previous=<none>)"
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.518170 1974327 status_manager.go:552] "Status Manager: adding pod with new status to podStatusChannel" pod="test-kube-apiserver/kube-apiserver-node1" podUID=bb8823c6e574ba7c1215633f4bf0f7d7 statusVersion=1 status={Phase:Pending Conditions:[{Type:Initialized Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-08-15 20:18:31 +0800 CST Reason:ContainersNotInitialized Message:containers with incomplete status: [setup]} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-08-15 20:18:31 +0800 CST Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver kube-apiserver-cert-syncer kube-apiserver-cert-regeneration-controller kube-apiserver-insecure-readyz kube-apiserver-check-endpoints]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-08-15 20:18:31 +0800 CST Reason:ContainersNotReady Message:containers with unready status: [kube-apiserver kube-apiserver-cert-syncer kube-apiserver-cert-regeneration-controller kube-apiserver-insecure-readyz kube-apiserver-check-endpoints]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2024-08-15 19:57:00 +0800 CST Reason: Message:}] Message: Reason: NominatedNodeName: HostIP:10.253.166.72 PodIP:10.253.166.72 PodIPs:[{IP:10.253.166.72}] StartTime:2024-08-15 19:57:00 +0800 CST InitContainerStatuses:[{Name:setup State:{Waiting:&ContainerStateWaiting{Reason:PodInitializing,Message:,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:image.test.io/test-ceaedge/ceaedge@sha256:b161fe4e21adfa95e7620c778536dd656a9437482d89fa44941f05c0e101fe28 ImageID: ContainerID: Started:<nil>}] ContainerStatuses:[{Name:kube-apiserver State:{Waiting:&ContainerStateWaiting{Reason:PodInitializing,Message:,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:image.test.io/test-ceaedge/ceaedge@sha256:b161fe4e21adfa95e7620c778536dd656a9437482d89fa44941f05c0e101fe28 ImageID: ContainerID: Started:0xc011eba16d} {Name:kube-apiserver-cert-regeneration-controller State:{Waiting:&ContainerStateWaiting{Reason:PodInitializing,Message:,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:image.test.io/test-ceaedge/ceaedge@sha256:382da2faa65c152f4d930f42f6d729fe219d337b243584f8ae13788829730024 ImageID: ContainerID: Started:0xc011eba16e} {Name:kube-apiserver-cert-syncer State:{Waiting:&ContainerStateWaiting{Reason:PodInitializing,Message:,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:image.test.io/test-ceaedge/ceaedge@sha256:382da2faa65c152f4d930f42f6d729fe219d337b243584f8ae13788829730024 ImageID: ContainerID: Started:0xc011eba16f} {Name:kube-apiserver-check-endpoints State:{Waiting:&ContainerStateWaiting{Reason:PodInitializing,Message:,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:image.test.io/test-ceaedge/ceaedge@sha256:382da2faa65c152f4d930f42f6d729fe219d337b243584f8ae13788829730024 ImageID: ContainerID: Started:0xc011eba1a0} {Name:kube-apiserver-insecure-readyz State:{Waiting:&ContainerStateWaiting{Reason:PodInitializing,Message:,} Running:nil Terminated:nil} LastTerminationState:{Waiting:nil Running:nil Terminated:nil} Ready:false RestartCount:0 Image:image.test.io/test-ceaedge/ceaedge@sha256:382da2faa65c152f4d930f42f6d729fe219d337b243584f8ae13788829730024 ImageID: ContainerID: Started:0xc011eba1a1}] QOSClass:Burstable EphemeralContainerStatuses:[]}
Aug 15 20:18:31 node1 hyperkube[1974327]: I0815 20:18:31.518183 1974327 volume_manager.go:448] "Waiting for volumes to unmount for pod" pod="test-kube-apiserver/kube-apiserver-node1"
Aug 15 20:18:35 node1 hyperkube[1974327]: I0815 20:18:35.509137 1974327 actual_state_of_world.go:973] "Pod mounted volumes" uniquePodName=9d45620a-ae62-4ee3-bdb7-139998904a99 mountedVolume=[{MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-positions InnerVolumeSpecName:positions OuterVolumeSpecName:positions PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc006092940 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7a70 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/projected/9d45620a-ae62-4ee3-bdb7-139998904a99-kube-api-access-wd5vj InnerVolumeSpecName:kube-api-access-wd5vj OuterVolumeSpecName:kube-api-access-wd5vj PluginName:kubernetes.io/projected PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc005345100 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7b48 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-test-apiserver InnerVolumeSpecName:var-log-test-apiserver OuterVolumeSpecName:var-log-test-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc003817600 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7ab8 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-kube-apiserver InnerVolumeSpecName:var-log-kube-apiserver OuterVolumeSpecName:var-log-kube-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc005f320c0 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7ae8 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-oauth-apiserver InnerVolumeSpecName:var-log-oauth-apiserver OuterVolumeSpecName:var-log-oauth-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc00ae8edc0 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7b18 DeviceMountPath: SELinuxMountContext:}}]

Aug 15 20:18:35 node1 hyperkube[1974327]: I0815 20:18:35.509810 1974327 actual_state_of_world.go:973] "Pod mounted volumes" uniquePodName=9d45620a-ae62-4ee3-bdb7-139998904a99 mountedVolume=[{MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-test-apiserver InnerVolumeSpecName:var-log-test-apiserver OuterVolumeSpecName:var-log-test-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc003817600 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7ab8 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-kube-apiserver InnerVolumeSpecName:var-log-kube-apiserver OuterVolumeSpecName:var-log-kube-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc005f320c0 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7ae8 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-oauth-apiserver InnerVolumeSpecName:var-log-oauth-apiserver OuterVolumeSpecName:var-log-oauth-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc00ae8edc0 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7b18 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-positions InnerVolumeSpecName:positions OuterVolumeSpecName:positions PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc006092940 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7a70 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/projected/9d45620a-ae62-4ee3-bdb7-139998904a99-kube-api-access-wd5vj InnerVolumeSpecName:kube-api-access-wd5vj OuterVolumeSpecName:kube-api-access-wd5vj PluginName:kubernetes.io/projected PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc005345100 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7b48 DeviceMountPath: SELinuxMountContext:}}]
Aug 15 20:18:46 node1 hyperkube[1974327]: I0815 20:18:46.508306 1974327 kubelet.go:2261] "SyncLoop (SYNC) pods" total=5 pods=[test-cnv/vic-image-registry-5c64975fdd-jww6l test-cnv/yum-repo-87d95477-4jfn7 test-kube-apiserver/kube-apiserver-node1 default/grafana-6598f98dd-hkzmf test-cluster-node-tuning-operator/cluster-node-tuning-operator-5bbb5fd999-r9nkg]
Aug 15 20:18:46 node1 hyperkube[1974327]: I0815 20:18:46.508354 1974327 pod_workers.go:931] "Notifying pod of pending update" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1 workType="sync"
Aug 15 20:18:46 node1 hyperkube[1974327]: I0815 20:18:46.508440 1974327 pod_workers.go:1138] "Pod cannot start yet" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1
Aug 15 20:18:49 node1 hyperkube[1974327]: I0815 20:18:49.607098 1974327 file.go:202] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:18:49 node1 hyperkube[1974327]: I0815 20:18:49.607697 1974327 common.go:69] "Generated UID" pod="test-kube-apiserver/kube-apiserver" podUID=d6c287e864be7616ad8256abb9e925b1 source="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:18:49 node1 hyperkube[1974327]: I0815 20:18:49.607708 1974327 common.go:73] "Generated pod name" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1 source="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"

Aug 15 20:18:49 node1 hyperkube[1974327]: I0815 20:18:49.607717 1974327 common.go:78] "Set namespace for pod" pod="test-kube-apiserver/kube-apiserver-node1" source="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:19:01 node1 hyperkube[1974327]: I0815 20:19:01.508509 1974327 kubelet.go:2261] "SyncLoop (SYNC) pods" total=7 pods=[test-controller-manager/vm-scheduler-848d87568f-xtdw7 test-kube-controller-manager/kube-controller-manager-node1 test-doko/doko-agent-6hkd8 default/hp-volume-kpbnp test-kube-apiserver/kube-apiserver-node1 test-logging/logging-operator-548564d9d9-rcncd test-monitoring/alertmanager-main-0]
Aug 15 20:19:01 node1 hyperkube[1974327]: I0815 20:19:01.508578 1974327 pod_workers.go:931] "Notifying pod of pending update" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1 workType="sync"
Aug 15 20:19:01 node1 hyperkube[1974327]: I0815 20:19:01.508643 1974327 pod_workers.go:1138] "Pod cannot start yet" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1
Aug 15 20:19:09 node1 hyperkube[1974327]: I0815 20:19:09.607309 1974327 file.go:202] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:19:09 node1 hyperkube[1974327]: I0815 20:19:09.607920 1974327 common.go:69] "Generated UID" pod="test-kube-apiserver/kube-apiserver" podUID=d6c287e864be7616ad8256abb9e925b1 source="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:19:09 node1 hyperkube[1974327]: I0815 20:19:09.607932 1974327 common.go:73] "Generated pod name" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1 source="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:19:09 node1 hyperkube[1974327]: I0815 20:19:09.607944 1974327 common.go:78] "Set namespace for pod" pod="test-kube-apiserver/kube-apiserver-node1" source="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:19:12 node1 hyperkube[1974327]: I0815 20:19:12.508790 1974327 kubelet.go:2261] "SyncLoop (SYNC) pods" total=2 pods=[test-kube-apiserver/kube-apiserver-node1 test-vnet-operator/vnet-operator-controller-manager-6cdcbdb49-bcrkq]
Aug 15 20:19:12 node1 hyperkube[1974327]: I0815 20:19:12.508808 1974327 pod_workers.go:931] "Notifying pod of pending update" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1 workType="sync"
Aug 15 20:19:12 node1 hyperkube[1974327]: I0815 20:19:12.508896 1974327 pod_workers.go:1138] "Pod cannot start yet" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1


Aug 15 20:19:27 node1 hyperkube[1974327]: I0815 20:19:27.508926 1974327 kubelet.go:2261] "SyncLoop (SYNC) pods" total=6 pods=[test-cnv/hci-compute-fileserver-5874bc85df-gpph2 test-vnet-operator/vnet-operator-controller-manager-6cdcbdb49-bcrkq test-base-image-registry-operator/base-image-registry-69ff9bd484-64527 test-node-label-operator/node-label-operator-6cd767b464-q4dz6 test-kube-controller-manager/kube-controller-manager-node1 test-kube-apiserver/kube-apiserver-node1]
Aug 15 20:19:27 node1 hyperkube[1974327]: I0815 20:19:27.509009 1974327 pod_workers.go:931] "Notifying pod of pending update" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1 workType="sync"
Aug 15 20:19:27 node1 hyperkube[1974327]: I0815 20:19:27.509099 1974327 pod_workers.go:1138] "Pod cannot start yet" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1
Aug 15 20:19:29 node1 hyperkube[1974327]: I0815 20:19:29.607409 1974327 file.go:202] "Reading config file" path="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:19:29 node1 hyperkube[1974327]: I0815 20:19:29.607998 1974327 common.go:69] "Generated UID" pod="test-kube-apiserver/kube-apiserver" podUID=d6c287e864be7616ad8256abb9e925b1 source="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:19:29 node1 hyperkube[1974327]: I0815 20:19:29.608009 1974327 common.go:73] "Generated pod name" pod="test-kube-apiserver/kube-apiserver-node1" podUID=d6c287e864be7616ad8256abb9e925b1 source="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:19:29 node1 hyperkube[1974327]: I0815 20:19:29.608017 1974327 common.go:78] "Set namespace for pod" pod="test-kube-apiserver/kube-apiserver-node1" source="/etc/kubernetes/manifests/kube-apiserver-pod.yaml"
Aug 15 20:19:40 node1 hyperkube[1974327]: I0815 20:19:40.511401 1974327 actual_state_of_world.go:973] "Pod mounted volumes" uniquePodName=9d45620a-ae62-4ee3-bdb7-139998904a99 mountedVolume=[{MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-test-apiserver InnerVolumeSpecName:var-log-test-apiserver OuterVolumeSpecName:var-log-test-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc003817600 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7ab8 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-kube-apiserver InnerVolumeSpecName:var-log-kube-apiserver OuterVolumeSpecName:var-log-kube-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc005f320c0 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7ae8 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-oauth-apiserver InnerVolumeSpecName:var-log-oauth-apiserver OuterVolumeSpecName:var-log-oauth-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc00ae8edc0 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7b18 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-positions InnerVolumeSpecName:positions OuterVolumeSpecName:positions PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc006092940 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7a70 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/projected/9d45620a-ae62-4ee3-bdb7-139998904a99-kube-api-access-wd5vj InnerVolumeSpecName:kube-api-access-wd5vj OuterVolumeSpecName:kube-api-access-wd5vj PluginName:kubernetes.io/projected PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc005345100 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7b48 DeviceMountPath: SELinuxMountContext:}}]
Aug 15 20:19:40 node1 hyperkube[1974327]: I0815 20:19:40.512098 1974327 actual_state_of_world.go:973] "Pod mounted volumes" uniquePodName=9d45620a-ae62-4ee3-bdb7-139998904a99 mountedVolume=[{MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-test-apiserver InnerVolumeSpecName:var-log-test-apiserver OuterVolumeSpecName:var-log-test-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc003817600 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7ab8 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-kube-apiserver InnerVolumeSpecName:var-log-kube-apiserver OuterVolumeSpecName:var-log-kube-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc005f320c0 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7ae8 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-var-log-oauth-apiserver InnerVolumeSpecName:var-log-oauth-apiserver OuterVolumeSpecName:var-log-oauth-apiserver PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc00ae8edc0 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7b18 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/host-path/9d45620a-ae62-4ee3-bdb7-139998904a99-positions InnerVolumeSpecName:positions OuterVolumeSpecName:positions PluginName:kubernetes.io/host-path PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc006092940 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7a70 DeviceMountPath: SELinuxMountContext:}} {MountedVolume:{PodName:9d45620a-ae62-4ee3-bdb7-139998904a99 VolumeName:kubernetes.io/projected/9d45620a-ae62-4ee3-bdb7-139998904a99-kube-api-access-wd5vj InnerVolumeSpecName:kube-api-access-wd5vj OuterVolumeSpecName:kube-api-access-wd5vj PluginName:kubernetes.io/projected PodUID:9d45620a-ae62-4ee3-bdb7-139998904a99 Mounter:0xc005345100 BlockVolumeMapper:<nil> VolumeGidValue: VolumeSpec:0xc0032f7b48 DeviceMountPath: SELinuxMountContext:}}]
```
2. restart kubelet will start the static pod quickly

3. related PR:
- https://github.com/kubernetes/kubernetes/pull/113145
- https://github.com/kubernetes/kubernetes/issues/117745
- https://github.com/kubernetes/kubernetes/pull/117751
- https://github.com/kubernetes/kubernetes/pull/116995

#### What did you expect to happen?

The static pods will start soon when edit the static pod yaml

#### How can we reproduce it (as minimally and precisely as possible)?

this happend by chance , now can be reproduced in K8s  1.25 and 1.29  on single node

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version

# paste output here
```
k8s 1.25
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
**æ¼æ´åˆ†æï¼š**

è¯¥Issueæè¿°äº†åœ¨æ›´æ–°é™æ€Podï¼ˆkube-apiserverï¼‰çš„yamlæ–‡ä»¶æ—¶ï¼Œé™æ€Podå¯èƒ½ä¼šå¡åœ¨â€œWaiting for volumes to unmount for podâ€çŠ¶æ€é•¿è¾¾20åˆ†é’Ÿåˆ°2å°æ—¶ã€‚é‡å¯kubeletå¯ä»¥ä½¿é™æ€Podå¿«é€Ÿå¯åŠ¨ã€‚

**é£é™©è¯„ä¼°ï¼š**

ä»å®‰å…¨é£é™©çš„è§’åº¦åˆ†æï¼Œè¯¥é—®é¢˜å¯èƒ½å¯¼è‡´kube-apiserveråœ¨è¾ƒé•¿æ—¶é—´å†…ä¸å¯ç”¨ï¼Œè¿™å¯èƒ½ä¼šå½±å“é›†ç¾¤çš„å¯ç”¨æ€§ã€‚ç„¶è€Œï¼Œé€ æˆè¯¥é—®é¢˜çš„æ“ä½œæ˜¯æ›´æ–°é™æ€Podçš„yamlæ–‡ä»¶ï¼Œè¿™é€šå¸¸éœ€è¦å¯¹èŠ‚ç‚¹æ‹¥æœ‰rootæƒé™æˆ–å…¶ä»–é«˜æƒé™ã€‚å› æ­¤ï¼Œæ”»å‡»è€…éœ€è¦å…·æœ‰é«˜æƒé™æ‰èƒ½å®æ–½è¯¥æ”»å‡»ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ä¸­çš„ç¬¬4æ¡ï¼šâ€œåœ¨é£é™©ä¸ºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»æ—¶ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å¤Ÿå®æ–½è¯¥æ”»å‡»ï¼Œåˆ™è§†æƒ…å†µéœ€è¦é™çº§å¤„ç†ï¼Œå½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨highä»¥ä¸‹ã€‚â€

å› æ­¤ï¼Œè¯¥é—®é¢˜å±äºå®‰å…¨é£é™©ï¼Œä½†ç”±äºéœ€è¦é«˜æƒé™æ‰èƒ½åˆ©ç”¨ï¼Œå› æ­¤é£é™©è¯„çº§ä¸ºä½é£é™©ã€‚

**å¯èƒ½å½±å“ï¼š**

å½“å‘ç”Ÿè¯¥é—®é¢˜æ—¶ï¼Œkube-apiserverå¯èƒ½é•¿æ—¶é—´ä¸å¯ç”¨ï¼Œè¿™ä¼šå½±å“é›†ç¾¤çš„ç®¡ç†å’Œè°ƒåº¦åŠŸèƒ½ã€‚ä½†ç”±äºéœ€è¦é«˜æƒé™æ‰èƒ½è§¦å‘ï¼Œè¯¥é—®é¢˜ä¸å¤ªå¯èƒ½è¢«è¿œç¨‹æ”»å‡»è€…åˆ©ç”¨ã€‚

---

## Issue #126700 [Bug] Scheduler fails to schedule a pod due to a race condition

- Issue é“¾æ¥ï¼š[#126700](https://github.com/kubernetes/kubernetes/issues/126700)

### Issue å†…å®¹

#### What happened?

A bug in the scheduler increases the time spent on scheduling a pod **from <1 second to 5 minutes**.

We discovered the bug when repeating the steps described in a fixed bug report [#106780](https://github.com/kubernetes/kubernetes/issues/106780). The setting and steps are as follows:

We have 1 node with 32Gi memory, and 3 pods to schedule: 
p1 / request 10Gi memory / low-priority,
p2 / request 25Gi memory / medium-priority,
p3 / request 20Gi memory / high-priority.

We perform the following steps:
1. add `p1` and wait until `p1` is running
2. add `p2` and wait until `p1` is terminating; this is because there are not enough resources to host both pods and `p2` has a higher priority.
**Note that at this point, `p2` is not running yet.**
3. add `p3` and wait until `p3` is running; `p2` is pending because `p2` and `p3` cannot coexist on `node0`.
4. re-add `p1` and wait until `p1` is running; `p1` should be able to run since there are enough resources to host both pods: 10 + 20 < 32

Interestingly, we find that if step 4 happened immediately after step 3, then `p1` fails to get scheduled with the reason `Insufficient Memory` and eventually gets scheduled after 5 minutes. If there is some short period between step 3 and step 4 (say 2 seconds), then `p1` is scheduled properly immediately.

##### What is the root cause?

After code and log inspections (see below), we have found the root cause of this bug: it's caused by the race condition between (A) `p1` is re-added and handled by the scheduler and (B) `p2`'s `nominated_node_name` is cleared. If A happens before B, then the bug occurs.

##### Why does 2 second make the difference?

The reason is that `p2`'s `nominated_node_name` is cleared *when it (a nominated pod) is scheduled again and failed*. But as the `p2`'s first schedule failed (need to wait for preemption to finish), it's put to BackoffQueue and need to wait for a few seconds before it's put back to ActionQueue and scheduled again.

So 
1. If `p1` is added before `p2` is scheduled again (without 2s sleep), it will **fail** as `p2`'s `nomianted_node_name` is not cleared.
2. But if we add the 2s sleep, `p2` is scheduled again and its `nominated_node_name` is cleared before `p1` is added. In this case, `p1` will be **schedulable** to node0.

<details><summary>Some scheduler logs of the bug-free and buggy trace</summary>
The lines starting with # are logged just before scheduling p1 and p2. You can notice that their order is reversed in the two traces.

Bug-free trace (B happens before A):
```markdown
# with sleep(2s)
I0814 14:19:47.939731   32287 eventhandlers.go:149] "Add event for unscheduled pod" pod="default/p3"
I0814 14:19:47.939756   32287 schedule_one.go:83] "About to try and schedule pod" pod="default/p3"
I0814 14:19:47.939768   32287 schedule_one.go:96] "Attempting to schedule pod" pod="default/p3"
I0814 14:19:47.939878   32287 default_binder.go:53] "Attempting to bind pod to node" logger="Bind.DefaultBinder" pod="default/p3" node="node0"
I0814 14:19:47.944902   32287 eventhandlers.go:313] "Delete event for scheduled pod" pod="default/p1"
I0814 14:19:47.948208   32287 scheduling_queue.go:1312] "Pod moved to an internal scheduling queue" pod="default/p2" event="AssignedPodDelete" queue="Backoff" hint=1
I0814 14:19:47.948799   32287 schedule_one.go:314] "Successfully bound pod to node" pod="default/p3" node="node0" evaluatedNodes=1 feasibleNodes=1
I0814 14:19:47.948891   32287 eventhandlers.go:201] "Delete event for unscheduled pod" pod="default/p3"
I0814 14:19:47.948901   32287 eventhandlers.go:231] "Add event for scheduled pod" pod="default/p3"
I0814 14:19:47.956662   32287 eventhandlers.go:268] "Update event for scheduled pod" pod="default/p3"
I0814 14:19:49.047040   32287 schedule_one.go:83] "About to try and schedule pod" pod="default/p2"
# I0814 14:19:49.047238   32287 schedule_one.go:96] "Attempting to schedule pod" pod="default/p2"
I0814 14:19:49.047886   32287 schedule_one.go:1055] "Unable to schedule pod; no fit; waiting" pod="default/p2" err="0/1 nodes are available: 1 Insufficient memory. preemption: 0/1 nodes are available: 1 No preemption victims found for incoming pod."
I0814 14:19:49.048895   32287 schedule_one.go:1122] "Updating pod condition" pod="default/p2" conditionType="PodScheduled" conditionStatus="False" conditionReason="Unschedulable"
I0814 14:19:49.064048   32287 eventhandlers.go:174] "Update event for unscheduled pod" pod="default/p2"
I0814 14:19:49.978428   32287 eventhandlers.go:149] "Add event for unscheduled pod" pod="default/p1"
I0814 14:19:49.978562   32287 schedule_one.go:83] "About to try and schedule pod" pod="default/p1"
# I0814 14:19:49.978580   32287 schedule_one.go:96] "Attempting to schedule pod" pod="default/p1"
I0814 14:19:49.978879   32287 default_binder.go:53] "Attempting to bind pod to node" logger="Bind.DefaultBinder" pod="default/p1" node="node0"
I0814 14:19:49.985438   32287 eventhandlers.go:201] "Delete event for unscheduled pod" pod="default/p1"
I0814 14:19:49.985476   32287 eventhandlers.go:231] "Add event for scheduled pod" pod="default/p1"
I0814 14:19:49.985747   32287 schedule_one.go:314] "Successfully bound pod to node" pod="default/p1" node="node0" evaluatedNodes=1 feasibleNodes=1
```

Buggy trace (A happens before B):
```markdown
# without sleep(2s)
I0814 14:17:47.151445   31615 eventhandlers.go:149] "Add event for unscheduled pod" pod="default/p3"
I0814 14:17:47.151515   31615 eventhandlers.go:174] "Update event for unscheduled pod" pod="default/p2"
I0814 14:17:47.151802   31615 schedule_one.go:83] "About to try and schedule pod" pod="default/p3"
I0814 14:17:47.151812   31615 schedule_one.go:96] "Attempting to schedule pod" pod="default/p3"
I0814 14:17:47.151926   31615 default_binder.go:53] "Attempting to bind pod to node" logger="Bind.DefaultBinder" pod="default/p3" node="node0"
I0814 14:17:47.156395   31615 eventhandlers.go:313] "Delete event for scheduled pod" pod="default/p1"
I0814 14:17:47.156444   31615 scheduling_queue.go:1312] "Pod moved to an internal scheduling queue" pod="default/p2" event="AssignedPodDelete" queue="Backoff" hint=1
I0814 14:17:47.160464   31615 eventhandlers.go:201] "Delete event for unscheduled pod" pod="default/p3"
I0814 14:17:47.160479   31615 eventhandlers.go:231] "Add event for scheduled pod" pod="default/p3"
I0814 14:17:47.160514   31615 schedule_one.go:314] "Successfully bound pod to node" pod="default/p3" node="node0" evaluatedNodes=1 feasibleNodes=1
I0814 14:17:47.168539   31615 eventhandlers.go:268] "Update event for scheduled pod" pod="default/p3"
I0814 14:17:47.177608   31615 eventhandlers.go:149] "Add event for unscheduled pod" pod="default/p1"
I0814 14:17:47.177642   31615 schedule_one.go:83] "About to try and schedule pod" pod="default/p1"
# I0814 14:17:47.177650   31615 schedule_one.go:96] "Attempting to schedule pod" pod="default/p1"
I0814 14:17:47.177775   31615 schedule_one.go:1055] "Unable to schedule pod; no fit; waiting" pod="default/p1" err="0/1 nodes are available: 1 Insufficient memory. preemption: 0/1 nodes are available: 1 No preemption victims found for incoming pod."
I0814 14:17:47.177802   31615 schedule_one.go:1122] "Updating pod condition" pod="default/p1" conditionType="PodScheduled" conditionStatus="False" conditionReason="Unschedulable"
I0814 14:17:47.187472   31615 eventhandlers.go:174] "Update event for unscheduled pod" pod="default/p1"
I0814 14:17:48.138665   31615 schedule_one.go:83] "About to try and schedule pod" pod="default/p2"
# I0814 14:17:48.139950   31615 schedule_one.go:96] "Attempting to schedule pod" pod="default/p2"
I0814 14:17:48.140305   31615 schedule_one.go:1055] "Unable to schedule pod; no fit; waiting" pod="default/p2" err="0/1 nodes are available: 1 Insufficient memory. preemption: 0/1 nodes are available: 1 No preemption victims found for incoming pod."
I0814 14:17:48.140582   31615 schedule_one.go:1122] "Updating pod condition" pod="default/p2" conditionType="PodScheduled" conditionStatus="False" conditionReason="Unschedulable"
I0814 14:17:48.156537   31615 eventhandlers.go:174] "Update event for unscheduled pod" pod="default/p2"
```
</details>

##### How to fix it?

1. A potential fix is:
Register all plugins that need to be aware of pods to events triggered by updates to a pod's `nominated_node_name`. This way, a failed pod will be retried as soon as any pod's `nominated_node_name` is cleared.

2. If this is too radical and can cause performance issues, another solution is: 
When a pod is bound to a node with nominated pods, either 
a) re-check the previously nominated pods (that nominated to this node), 
b) or clear the `nominated_node_name` of these nominated pods.
In this way, upcoming pods won't fail unreasonably and won't have to wait 5 minutes for the next retry.


#### What did you expect to happen?

`p1` should be immediately scheduled.


#### How can we reproduce it (as minimally and precisely as possible)?

We're using [kwok](https://kwok.sigs.k8s.io/) to reproduce this issue.

```yaml
# node0.yaml
apiVersion: v1
kind: Node
metadata:
  name: node0
  labels:
    kubernetes.io/hostname: node0
status:
  allocatable:
    cpu: "32"
    memory: "32Gi"
    pods: "110"
  capacity:
    cpu: "32"
    memory: "32Gi"
    pods: "110"
```

```yaml
# p1.yaml
apiVersion: v1
kind: Pod
metadata:
  name: p1
spec:
  containers:
  - name: p1-container
    image: nginx
    resources:
      requests:
        memory: "10Gi"

  priorityClassName: low-priority
```

```yaml
# p2.yaml
apiVersion: v1
kind: Pod
metadata:
  name: p2
spec:
  containers:
  - name: p2-container
    image: nginx
    resources:
      requests:
        memory: "25Gi"
  priorityClassName: medium-priority
```

```yaml
# p3.yaml
apiVersion: v1
kind: Pod
metadata:
  name: p3
spec:
  containers:
  - name: p3-container
    image: nginx
    resources:
      requests:
        memory: "20Gi"
  priorityClassName: high-priority
```

```yaml
# priority_class.yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000
globalDefault: false
description: "This priority class should be used for XYZ service pods only."
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: medium-priority
value: 500000
globalDefault: false
description: "This priority class should be used for XYZ service pods only."
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: low-priority
value: 0
globalDefault: false
description: "This priority class should be used for XYZ service pods only."
```


```python
# To run this reproduction script, please create a new KWOK cluster first.
# kwokctl create cluster --v DEBUG
# Also need to install the kubernetes python client library,
# and place all yaml files in the same folder with the script.
# ---
# To check the scheduler logs:
# kwokctl logs kube-scheduler
# ---
# To reproduce the bug case, please comment out `sleep(2)` between step 5 and step 6.
# To reproduce the normal case, please add `sleep(2)` between step 5 and step 6.
import unittest
import time
import shutil
import logging
from time import sleep
from os import path, makedirs
from logging import getLogger
from kubernetes import config, watch
from kubernetes.client import *
from kubernetes.utils import *
from time import strftime

logger = getLogger(__name__)
config.load_kube_config()
v1 = CoreV1Api()
k8s_cli = ApiClient()


log_dir = path.dirname(__file__)

if __name__ == "__main__":
    log_path = path.join(log_dir, f"issue-106780-reproduction-{strftime('%Y-%m-%d-%H-%M-%S')}")
    shutil.rmtree(log_path, ignore_errors=True)
    makedirs(log_path, exist_ok=True)
    log_file = path.join(log_path, 'reproduce.log')
    logging.basicConfig(level=logging.INFO,
                        filemode='w',
                        filename=log_file,
                        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
                        datefmt='%H:%M:%S')
    node0_yaml = path.join(path.dirname(__file__), 'node0.yaml')
    p1_yaml = path.join(path.dirname(__file__), 'p1.yaml')
    p2_yaml = path.join(path.dirname(__file__), 'p2.yaml')
    p3_yaml = path.join(path.dirname(__file__), 'p3.yaml')
    priority_class_yaml = path.join(path.dirname(__file__), 'priority_class.yaml')
    w = watch.Watch()

    # 1. create priority class
    create_from_yaml(k8s_client=k8s_cli, yaml_file=priority_class_yaml)
    logger.info(f"PriorityClass created")

    # 2. add node0
    create_from_yaml(k8s_client=k8s_cli, yaml_file=node0_yaml)
    logger.info(f"Node: node0 created")

    # 3. add p1
    create_from_yaml(k8s_client=k8s_cli, yaml_file=p1_yaml)
    logger.info(f"Pod: p1 added")

    # 3. check p1 is running
    p1_running = False
    for event in w.stream(v1.list_namespaced_pod,
                          field_selector=f'metadata.name=p1',
                          namespace="default",
                          timeout_seconds=int(5)):
        p1 = event['object']
        if event['object'].status.phase == "Running":
            p1_running = True
            logger.info(f"Pod: {p1.metadata.name} scheduled on node {p1.spec.node_name}")
            break
    assert p1_running

    # 4. add p2
    create_from_yaml(k8s_client=k8s_cli, yaml_file=p2_yaml)
    logger.info(f"Pod: p2 added")

    # 4. check p1 is terminating (by p2)
    for event in w.stream(v1.list_namespaced_pod,
                          field_selector=f'metadata.name=p1',
                          namespace="default",
                          timeout_seconds=int(3)):
        logger.info(f"Pod: {event['object'].metadata.name}, event: {event['type']}")
        logger.info(
            f"deletion_timestamp: {event['object'].metadata.deletion_timestamp}, status: {event['object'].status.phase}")
        if event['object'].metadata.deletion_timestamp is not None and event['object'].status.phase in (
        'Pending', 'Running'):
            logger.info(f"Pod: {p1.metadata.name} is terminating")
            break

    # 5. add p3 and wait for p3 running (p3 will terminate p2 / let p2 pending)
    create_from_yaml(k8s_client=k8s_cli, yaml_file=p3_yaml)
    p3_running = False
    for event in w.stream(v1.list_namespaced_pod,
                          field_selector=f'metadata.name=p3',
                          namespace="default",
                          timeout_seconds=int(5)):
        p3 = event['object']
        if p3.status.phase == "Running":
            logger.info(f"Pod: {p3.metadata.name} scheduled on node p3")
            p3_running = True
            break
    assert p3_running

    # sleep(2)  # -> without this on 1.30.2 will cause pod scheduled failed at the first try

    # 6. reapply and check p1 schedulable
    create_from_yaml(k8s_client=k8s_cli, yaml_file=p1_yaml)
    tic = time.time()
    logger.info(f"Pod: p1 re-applied")

    # 7. watch pod1 events and wait it to be scheduled
    p1_running = False
    for event in w.stream(v1.list_namespaced_pod,
                          field_selector=f'metadata.name=p1',
                          namespace="default",
                          timeout_seconds=int(600)):
        p1 = event['object']
        if p1.status.phase == "Running":
            p1_running = True
            logger.info(f"Pod: {p1.metadata.name} scheduled on node {p1.spec.node_name}")
            break

    if not p1_running:
        logger.error(f"Pod: {p1.metadata.name} is not scheduled in 600s")
        assert False
    else:
        logger.info(f"Pod: {p1.metadata.name}, " + f"time between add and scheduled: {time.time() - tic}")


```


#### Anything else we need to know?

Tested on 1.30.2

/sig bugs scheduling

#### Kubernetes version

<details>

Tested on 1.30.2

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†Kubernetesè°ƒåº¦å™¨ä¸­çš„ä¸€ä¸ªç«æ€æ¡ä»¶æ¼æ´ï¼Œåœ¨ç‰¹å®šæ¡ä»¶ä¸‹ï¼Œä¼šå¯¼è‡´è°ƒåº¦å™¨è°ƒåº¦ä¸€ä¸ªPodçš„æ—¶é—´ä»å°äº1ç§’å¢åŠ åˆ°5åˆ†é’Ÿã€‚è¿™å¯èƒ½å¯¼è‡´æœåŠ¡ä¸å¯ç”¨ï¼Œå³æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»çš„é£é™©ã€‚

æ”»å‡»è€…å¦‚æœèƒ½å¤Ÿåˆ©ç”¨è¯¥æ¼æ´ï¼Œå¯èƒ½é€šè¿‡æ„é€ ç‰¹å®šçš„Podå’Œè°ƒåº¦é¡ºåºï¼Œä½¿è°ƒåº¦å™¨å»¶è¿Ÿè°ƒåº¦å…¶ä»–Podï¼Œå½±å“é›†ç¾¤çš„å¯ç”¨æ€§ã€‚ç„¶è€Œï¼Œè¦åˆ©ç”¨è¯¥æ¼æ´ï¼Œæ”»å‡»è€…éœ€è¦èƒ½å¤Ÿåˆ›å»ºå…·æœ‰ç‰¹å®šé…ç½®çš„Podï¼ŒåŒ…æ‹¬è®¾ç½®ä¼˜å…ˆçº§ã€èµ„æºè¯·æ±‚ç­‰å‚æ•°ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬4æ¡ï¼Œå½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨highä»¥ä¸‹ã€‚

å› æ­¤ï¼Œè¯¥Issueå­˜åœ¨å®‰å…¨é£é™©ï¼Œä½†é£é™©è¯„çº§ä¸ºâ€œä½é£é™©â€ã€‚

---

## Issue #126922 describe service endpoints shows endpoints that are not ready

- Issue é“¾æ¥ï¼š[#126922](https://github.com/kubernetes/kubernetes/issues/126922)

### Issue å†…å®¹

#### What happened?

After upgrading from 1.29 to 1.30.2 `kubectl describe service` in the endpoints section shows all endpoints even if pods are not ready and `kubectl get endpoints` is not showing the endpoint.
Before upgrading, all pods that are not ready were not shown in the endpoints section.

#### What did you expect to happen?

IP of pods that are not ready should not be shown in `kubectl describe service` Endpoints section.

#### How can we reproduce it (as minimally and precisely as possible)?

Make pod readyness probe will fail:
```kubectl get pods pkad-health -o wide
NAME          READY   STATUS    RESTARTS      AGE    IP          NODE             NOMINATED NODE   READINESS GATES
pkad-health   0/1     Running   2 (94m ago)   119m   10.1.0.19   docker-desktop   <none>           <none>
```

In the service describe Endpoints section it should not be visable
```
kubectl describe svc pkad
Name:                     pkad
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 szkolenie=k8s
Type:                     ClusterIP
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.106.116.19
IPs:                      10.106.116.19
Port:                     <unset>  8080/TCP
TargetPort:               8080/TCP
Endpoints:                10.1.0.19:8080
Session Affinity:         None
Internal Traffic Policy:  Cluster
Events:                   <none>


kubectl get endpoints pkad
NAME   ENDPOINTS   AGE
pkad               121m

```


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.30.2
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

tested on docker desktop and DigitalOcean Kuberentes service.

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨å‡çº§Kubernetesç‰ˆæœ¬åï¼Œ`kubectl describe service`å‘½ä»¤æ˜¾ç¤ºäº†æœªå‡†å¤‡å°±ç»ªï¼ˆNot Readyï¼‰çš„Podçš„Endpointsä¿¡æ¯ï¼Œè€Œ`kubectl get endpoints`å‘½ä»¤åˆ™æ²¡æœ‰æ˜¾ç¤ºè¿™äº›Endpointsã€‚æŒ‰ç…§é¢„æœŸï¼Œæœªå‡†å¤‡å°±ç»ªçš„Podçš„IPä¸åº”å‡ºç°åœ¨æœåŠ¡çš„Endpointsåˆ—è¡¨ä¸­ã€‚

**å®‰å…¨é£é™©åˆ†æï¼š**

1. **ä¿¡æ¯æ³„éœ²é£é™©**ï¼šæœªå‡†å¤‡å°±ç»ªçš„Podå¯èƒ½åŒ…å«å°šæœªå®Œå…¨é…ç½®æˆ–å­˜åœ¨å®‰å…¨æ¼æ´çš„å®ä¾‹ï¼Œæš´éœ²å…¶IPåœ°å€å¯èƒ½å¯¼è‡´ä¿¡æ¯æ³„éœ²ã€‚

2. **æ”»å‡»é¢æ‰©å¤§**ï¼šæ”»å‡»è€…å¯èƒ½åˆ©ç”¨å·²çŸ¥çš„Pod IPè¿›è¡Œå†…éƒ¨ç½‘ç»œæ¢æµ‹ï¼Œå°è¯•è®¿é—®æœªå‡†å¤‡å°±ç»ªçš„Podï¼Œå¢åŠ æ”»å‡»é¢çš„å¯èƒ½æ€§ã€‚

**é£é™©è¯„ä¼°ï¼š**

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ”»å‡»è€…éœ€è¦å…·å¤‡æ‰§è¡Œ`kubectl describe service`å‘½ä»¤çš„æƒé™ï¼Œé€šå¸¸éœ€è¦ä¸€å®šçš„é›†ç¾¤è¯»å–æƒé™ã€‚ç”±äºæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å®æ–½è¯¥æ”»å‡»ï¼Œä¸”æš´éœ²çš„IPä¸ºé›†ç¾¤å†…éƒ¨IPï¼Œå¤–éƒ¨æ— æ³•ç›´æ¥è®¿é—®ï¼Œåˆ©ç”¨ä»·å€¼æœ‰é™ã€‚å› æ­¤ï¼Œç»¼åˆåˆ¤æ–­è¯¥é£é™©è¯„çº§ä¸º**ä½é£é™©**ã€‚

---

## Issue #126921 Deletion of csi-node-plugin Pod causes driver entry to be removed from CSINode object; kube-scheduler schedules more than driver's allocatable

- Issue é“¾æ¥ï¼š[#126921](https://github.com/kubernetes/kubernetes/issues/126921)

### Issue å†…å®¹

#### What happened?

With @plkokanov we hit in our environments the following issue multiple times:
```
% k -n shoot--foo--bar describe po etcd-main-0

Events:
  Type     Reason              Age                 From                     Message
  ----     ------              ----                ----                     -------
  Warning  FailedAttachVolume  35s (x47 over 80m)  attachdetach-controller  AttachVolume.Attach failed for volume "pv-shoot--shik--shak-1234" : rpc error: code = Internal desc = Attach volume /subscriptions/<omitted>/resourceGroups/shoot--shik--shak/providers/Microsoft.Compute/disks/pv-shoot--shik--shak-1234 to instance shoot--shik--shak-cpu-worker-etcd-z1-6c956-mhcw5 failed with Retriable: false, RetryAfter: 0s, HTTPStatusCode: 409, RawError: {\r
  "error": {\r
    "code": "OperationNotAllowed",\r
    "message": "The maximum number of data disks allowed to be attached to a VM of this size is 16.",\r
    "target": "dataDisks"\r
  }\r
}
```

When we checked, the CSINode object for the corresponding Node was reporting the correct volume attachment limit:
```
% k get csinode shoot--shik--shak-cpu-worker-etcd-z1-6c956-mhcw5 -o yaml

spec:
  drivers:
  # ...
  - allocatable:
      count: 16
    name: disk.csi.azure.com
    nodeID: shoot--shik--shak-cpu-worker-etcd-z1-6c956-mhcw5
    topologyKeys:
    - topology.disk.csi.azure.com/zone
    - topology.kubernetes.io/zone
```

According to the Node status and according to the VM state in Azure, it had already 16 disks attached.
```
% k get no shoot--shik--shak-cpu-worker-etcd-z1-6c956-mhcw5 -o json | jq '.status.volumesAttached | length'
16

% az vm show -g shoot--shik--shak --name shoot--shik--shak-cpu-worker-etcd-z1-6c956-mhcw5 | jq '.storageProfile.dataDisks | length'
16
```

kube-scheduler wrongly sheduled a Pod with a volume to the `shoot--shik--shak-cpu-worker-etcd-z1-6c956-mhcw5` Node as the Node already had its volume attachments limit reached.

We found that this happens due to csi-node-plugin Pod deletion. On deletion of this Pod, we see that the driver section from the CSINode object is removed and it is added only when the new csi-node-plugin Pod starts.

The corresponding handling in kube-scheduler's NodeVolumeLimits plugin is: https://github.com/kubernetes/kubernetes/blob/a7242fcff768658019f878cb691583dcbcfefb2d/pkg/scheduler/framework/plugins/nodevolumelimits/csi.go#L197-L201

If we fail to fetch the limit from the CSINode object, kube-scheduler let's the Pod to be scheduled.

#### What did you expect to happen?

kube-scheduler to do not schedule Pods with volumes to Nodes that already have their volume attachment limit reached.

#### How can we reproduce it (as minimally and precisely as possible)?

To reproduce the removal of the driver section after csi-node-plugin Pod deletion:

1. Start watching the CSINode object:
```
 k get csinode "$NODENAME" -w -o yaml > "~/csinode-$NODENAME.yaml"
```

2. In a new terminal window delete the csi-node-plugin Pod

3. Make sure that the CSINode object watch is as follows:
```yaml
apiVersion: storage.k8s.io/v1
kind: CSINode
metadata:
  name: node-1
spec:
  drivers:
  - allocatable:
      count: 8
    name: disk.csi.azure.com
    nodeID: node-1
    topologyKeys:
    - topology.disk.csi.azure.com/zone
    - topology.kubernetes.io/zone
---
apiVersion: storage.k8s.io/v1
kind: CSINode
metadata:
  name: node-1
spec:
  drivers: null
---
apiVersion: storage.k8s.io/v1
kind: CSINode
metadata:
  name: node-1
spec:
  drivers:
  - allocatable:
      count: 8
    name: disk.csi.azure.com
    nodeID: node-1
    topologyKeys:
    - topology.disk.csi.azure.com/zone
    - topology.kubernetes.io/zone
```

Note: The non-relevant fields are removed from the example output above.

You can see that until the new csi-node-plugin is scheduled and started, the CSINode object does not contain any information about the driver. We see that during that time kube-scheduler schedules new Pods with volumes to that Node.
The kube-scheduler plugin for respecting the Node volume limit is: https://github.com/kubernetes/kubernetes/blob/a7242fcff768658019f878cb691583dcbcfefb2d/pkg/scheduler/framework/plugins/nodevolumelimits/csi.go


#### Anything else we need to know?

We use VPA to scale the csi-node-plugin Pod. That's why it can be evicted and restarted.

#### Kubernetes version

<details>

```console
$ kubectl version

Server Version: v1.29.4
```

</details>


#### Cloud provider

<details>
Azure and AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
Gardener
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
[azuredisk-csi-driver](https://github.com/kubernetes-sigs/azuredisk-csi-driver): v1.30.0
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†å½“csi-node-plugin Podè¢«åˆ é™¤æ—¶ï¼ŒCSINodeå¯¹è±¡ä¸­çš„driverä¿¡æ¯ä¼šè¢«ä¸´æ—¶ç§»é™¤ï¼Œå¯¼è‡´kube-scheduleræ— æ³•è·å–èŠ‚ç‚¹çš„å·é™åˆ¶ä¿¡æ¯ã€‚åœ¨æ­¤æœŸé—´ï¼Œkube-schedulerå¯èƒ½ä¼šå°†éœ€è¦æŒ‚è½½å·çš„Podè°ƒåº¦åˆ°å·²ç»è¾¾åˆ°å·é™„ä»¶ä¸Šé™çš„èŠ‚ç‚¹ä¸Šï¼Œä»è€Œå¯¼è‡´å·é™„ä»¶å¤±è´¥ã€‚è¿™å¯èƒ½å¼•å‘å…³é”®æœåŠ¡çš„ä¸­æ–­ï¼Œå±äºæ½œåœ¨çš„æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰é£é™©ã€‚

ç„¶è€Œï¼Œæ”»å‡»è€…è¦åˆ©ç”¨æ­¤æ¼æ´ï¼Œéœ€è¦å…·å¤‡åˆ é™¤csi-node-plugin Podçš„æƒé™ï¼Œå¹¶èƒ½å¤Ÿåˆ›å»ºéœ€è¦å·çš„Podã€‚è¿™é€šå¸¸æ˜¯ç®¡ç†å‘˜æˆ–å…·æœ‰é«˜æƒé™çš„ç”¨æˆ·æ‰èƒ½æ‰§è¡Œçš„æ“ä½œï¼Œæ™®é€šç”¨æˆ·æ— æ³•å®ç°ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬4æ¡ï¼Œé’ˆå¯¹éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å®æ–½çš„DoSæ”»å‡»ï¼Œåº”é€‚å½“é™çº§å¤„ç†ã€‚å½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨highä»¥ä¸‹ã€‚

å› æ­¤ï¼Œæ­¤Issueå­˜åœ¨å®‰å…¨é£é™©ï¼Œä½†é£é™©è¯„çº§ä¸ºä½é£é™©ã€‚

---

## Issue #126892 Int overflow in hpa causing incorrect replica count

- Issue é“¾æ¥ï¼š[#126892](https://github.com/kubernetes/kubernetes/issues/126892)

### Issue å†…å®¹

#### What happened?

The setup:
I am using keda with the prometheus scaler. The query I am using, returns the lag in the message queue i am using, and the threshold is set to `0.1`.

What happened:
The lag was increasing for a long time, and the replica count reached the max setting as expected. Everything was running fine for some time. When the lag value reached `214,748,364` hpa decided to reduce the replicas from the max limit to `1`.

What I think is the problem:
When the lag passes `214,748,364`, the calculation [here](https://github.com/kubernetes/kubernetes/blob/7b80cdb66a390f225d23cd612950144e3a39d1ae/pkg/controller/podautoscaler/replica_calculator.go#L278) divides by the threshold `0.1` and it passes the max int32 value. causing hpa to scale to the minimum value, 1.
It also seems like a lot of other places in this file cast a 64 bit float to a 32 bit int. Should there maybe be a check everywhere this is done?

#### What did you expect to happen?

I expected the replica count to stay at the max value. Or alternatively, get an error that we have reached the max value for an external metric value

#### How can we reproduce it (as minimally and precisely as possible)?

Use an external metric, and set it above `214,748,364` with a threshold of `0.1`.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.29
</details>


#### Cloud provider

<details>
aws eks
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†åœ¨ä½¿ç”¨KEDAä¸Prometheus scaleræ—¶ï¼Œå½“å¤–éƒ¨æŒ‡æ ‡ï¼ˆæ¶ˆæ¯é˜Ÿåˆ—å»¶è¿Ÿï¼‰è¶…è¿‡214,748,364å¹¶ä¸é˜ˆå€¼0.1è¿›è¡Œè®¡ç®—æ—¶ï¼Œå‘ç”Ÿäº†æ•´æ•°æº¢å‡ºï¼Œå¯¼è‡´HPAï¼ˆHorizontal Pod Autoscalerï¼‰å°†å‰¯æœ¬æ•°é‡ä»æœ€å¤§å€¼ç¼©å‡åˆ°1ã€‚æ­¤é—®é¢˜å¯èƒ½å¯¼è‡´æœåŠ¡å¯ç”¨æ€§å—åˆ°å½±å“ã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼Œæ”»å‡»è€…å¦‚æœèƒ½å¤Ÿæ§åˆ¶æˆ–å¹²æ‰°å¤–éƒ¨æŒ‡æ ‡ï¼Œä½¿å…¶è¶…è¿‡æ•´æ•°é™åˆ¶ï¼Œå¯èƒ½è¯±å¯¼HPAé”™è¯¯åœ°ç¼©å°å‰¯æœ¬æ•°é‡ï¼Œå¯¼è‡´æœåŠ¡çš„æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚ç„¶è€Œï¼Œè¿™éœ€è¦æ”»å‡»è€…å…·å¤‡å¯¹PrometheusæŒ‡æ ‡æˆ–æ¶ˆæ¯é˜Ÿåˆ—çš„æ§åˆ¶æƒé™ï¼Œé€šå¸¸ä¸å±äºä½æƒé™æˆ–æœªæˆæƒçš„æ”»å‡»è¡Œä¸ºã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬4æ¡ï¼Œè‹¥æ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å®æ–½è¯¥æ”»å‡»ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚

å› æ­¤ï¼Œè¯¥é—®é¢˜å­˜åœ¨ä¸€å®šçš„å®‰å…¨é£é™©ï¼Œä½†ç”±äºéœ€è¦è¾ƒé«˜çš„æƒé™æ‰èƒ½è¢«åˆ©ç”¨ï¼Œé£é™©è¯„çº§ä¸ºä½é£é™©ã€‚

---

# âœ… ä¸æ¶‰åŠå®‰å…¨é£é™©çš„ Issues (56 ä¸ª)

## Issue #126515 Secret Creation for Service Account Does Not Populate secrets Field

- Issue é“¾æ¥ï¼š[#126515](https://github.com/kubernetes/kubernetes/issues/126515)

### Issue å†…å®¹

#### What happened?

I used the following command in Kubernetes to create a secret with the token of the prometheus-agent service account:
```
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Secret
metadata:
  annotations:
    kubernetes.io/service-account.name: prometheus-agent
  name: get-prometheus-sa-token
  namespace: kube-system
type: kubernetes.io/service-account-token
EOF
```

However, when I check the prometheus-agent service account using the command kubectl get sa prometheus-agent -n kube-system, I found that the secrets field is still 0.

#### What did you expect to happen?

I expected the secrets field of the prometheus-agent service account to be populated with the created secret.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a Kubernetes cluster (any version >= 1.24).
Use the provided kubectl apply -f - command to create the secret.
Check the secrets field of the prometheus-agent service account using kubectl get sa prometheus-agent -n kube-system.

#### Anything else we need to know?

I am willing to fix this potential bug (need confirmation if this is indeed a bug). The proposed solution involves updating the ServiceAccount (SA) information when renewing the tokens for secrets. However, I would like to confirm this approach beforehand.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.24.15-vke.32
```

</details>


#### Cloud provider

<details>
Volcano Engine
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetes 1.24åŠä»¥ä¸Šç‰ˆæœ¬ä¸­ï¼Œç”¨æˆ·é€šè¿‡åˆ›å»ºç±»å‹ä¸º`kubernetes.io/service-account-token`çš„Secretå¹¶æŒ‡å®šäº†`prometheus-agent`æœåŠ¡è´¦å·ï¼Œå¸Œæœ›è¯¥Secretèƒ½å¤Ÿå…³è”åˆ°è¯¥æœåŠ¡è´¦å·çš„`secrets`å­—æ®µä¸­ï¼Œä½†æ˜¯å‘ç°`secrets`å­—æ®µä»ç„¶ä¸ºç©ºã€‚

è¿™æ˜¯ç”±äºKubernetesåœ¨1.24ç‰ˆæœ¬åŠä»¥ä¸Šï¼Œå¯¹æœåŠ¡è´¦å·ä»¤ç‰Œçš„ç®¡ç†æ–¹å¼å‘ç”Ÿäº†å˜åŒ–ã€‚ç°åœ¨ï¼ŒæœåŠ¡è´¦å·ä»¤ç‰Œä¸å†é€šè¿‡åˆ›å»ºSecretçš„æ–¹å¼ç”Ÿæˆï¼Œè€Œæ˜¯é€šè¿‡TokenRequest APIåŠ¨æ€è·å–ã€‚å› æ­¤ï¼Œæ‰‹åŠ¨åˆ›å»ºçš„Secretä¸ä¼šè‡ªåŠ¨å…³è”åˆ°æœåŠ¡è´¦å·çš„`secrets`å­—æ®µã€‚

è¯¥Issueåæ˜ çš„æ˜¯Kubernetesç‰ˆæœ¬å‡çº§åæœåŠ¡è´¦å·ä»¤ç‰Œç®¡ç†æ–¹å¼å˜åŒ–å¯¼è‡´çš„åŠŸèƒ½æ€§é—®é¢˜ï¼Œå¹¶ä¸æ¶‰åŠä»»ä½•æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œå°¤å…¶æ˜¯æ ‡å‡†6ï¼šâ€œå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠâ€ã€‚

å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126514 You have to remove that sandbox to be able to reuse that name.

- Issue é“¾æ¥ï¼š[#126514](https://github.com/kubernetes/kubernetes/issues/126514)

### Issue å†…å®¹

#### What happened?

When power is turned off and restarted, `kubelet `reports an error when starting etcd and apiserver:
Aug 02 10:01:03 openEuler kubelet[1024]: E0802 10:01:03.693078    1024 remote_runtime.go:193] "RunPodSandbox from runtime service failed" err="rpc error: code = Unknown desc = Conflict. The name \"k8s_POD_etcd-openeuler_kube-system_ae0fcba533f9b951c6983487becf13f9_0\" is already in use by sandbox 30ed179087f03d03fef197363103427e48242829f815488bb774313db02c41a4. You have to remove that sandbox to be able to reuse that name."
Aug 02 10:01:03 openEuler kubelet[1024]: E0802 10:01:03.693228    1024 kuberuntime_sandbox.go:72] "Failed to create sandbox for pod" err="rpc error: code = Unknown desc = Conflict. The name \"k8s_POD_etcd-openeuler_kube-system_ae0fcba533f9b951c6983487becf13f9_0\" is already in use by sandbox 30ed179087f03d03fef197363103427e48242829f815488bb774313db02c41a4. You have to remove that sandbox to be able to reuse that name." pod="kube-system/etcd-openeuler"
Aug 02 10:01:03 openEuler kubelet[1024]: E0802 10:01:03.693276    1024 kuberuntime_manager.go:1166] "CreatePodSandbox for pod failed" err="rpc error: code = Unknown desc = Conflict. The name \"k8s_POD_etcd-openeuler_kube-system_ae0fcba533f9b951c6983487becf13f9_0\" is already in use by sandbox 30ed179087f03d03fef197363103427e48242829f815488bb774313db02c41a4. You have to remove that sandbox to be able to reuse that name." pod="kube-system/etcd-openeuler"
Aug 02 10:01:03 openEuler kubelet[1024]: E0802 10:01:03.693400    1024 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"CreatePodSandbox\" for \"etcd-openeuler_kube-system(ae0fcba533f9b951c6983487becf13f9)\" with CreatePodSandboxError: \"Failed to create sandbox for pod \\\"etcd-openeuler_kube-system(ae0fcba533f9b951c6983487becf13f9)\\\": rpc error: code = Unknown desc = Conflict. The name \\\"k8s_POD_etcd-openeuler_kube-system_ae0fcba533f9b951c6983487becf13f9_0\\\" is already in use by sandbox 30ed179087f03d03fef197363103427e48242829f815488bb774313db02c41a4. You have to remove that sandbox to be able to reuse that name.\"" pod="kube-system/etcd-openeuler" podUID="ae0fcba533f9b951c6983487becf13f9"
Aug 02 10:01:04 openEuler kubelet[1024]: E0802 10:01:04.276824    1024 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"KillPodSandbox\" for \"ae0fcba533f9b951c6983487becf13f9\" with KillPodSandboxError: \"rpc error: code = Unknown desc = You cannot stop container d5c60843291be7385fc0377c625582ccbc83715e6ae7614533cd21f7e313a50f in garbage collector progress.\"" pod="kube-system/etcd-openeuler" podUID="ae0fcba533f9b951c6983487becf13f9"
Aug 02 10:01:09 openEuler kubelet[1024]: E0802 10:01:09.888481    1024 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"KillPodSandbox\" for \"ae0fcba533f9b951c6983487becf13f9\" with KillPodSandboxError: \"rpc error: code = Unknown desc = You cannot stop container d5c60843291be7385fc0377c625582ccbc83715e6ae7614533cd21f7e313a50f in garbage collector progress.\"" pod="kube-system/etcd-openeuler" podUID="ae0fcba533f9b951c6983487becf13f9"
Aug 02 10:01:11 openEuler kubelet[1024]: E0802 10:01:11.199099    1024 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"KillPodSandbox\" for \"ae0fcba533f9b951c6983487becf13f9\" with KillPodSandboxError: \"rpc error: code = Unknown desc = You cannot stop container d5c60843291be7385fc0377c625582ccbc83715e6ae7614533cd21f7e313a50f in garbage collector progress.\"" pod="kube-system/etcd-openeuler" podUID="ae0fcba533f9b951c6983487becf13f9"
Aug 02 10:01:23 openEuler kubelet[1024]: E0802 10:01:23.335772    1024 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"KillPodSandbox\" for \"ae0fcba533f9b951c6983487becf13f9\" with KillPodSandboxError: \"rpc error: code = Unknown desc = You cannot stop container d5c60843291be7385fc0377c625582ccbc83715e6ae7614533cd21f7e313a50f in garbage collector progress.\"" pod="kube-system/etcd-openeuler" podUID="ae0fcba533f9b951c6983487becf13f9"

#### What did you expect to happen?

Once the above error is reported, the startup of etcd and apiserver will be delayed, and this bug will affect the startup speed of the business pod.

#### How can we reproduce it (as minimally and precisely as possible)?

After deploying `k8s v1.30.3`, turn on the `EventedPLEG` feature, power off and restart.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
k8s v1.30.3
</details>


#### Cloud provider

<details>
local
</details>


#### OS version

<details>
openeuler 22.03 LTS
arm64

Linux edgenode06 5.10.0 #7 SMP Sat Jun 10 13:37:24 CST 2023 aarch64 aarch64 aarch64 GNU/Linux

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
isulad v2.1.5
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueå†…å®¹ï¼Œè¯¥é—®é¢˜å‘ç”Ÿåœ¨ç³»ç»Ÿæ–­ç”µé‡å¯åï¼Œ`kubelet`åœ¨å¯åŠ¨`etcd`å’Œ`apiserver`æ—¶æŠ¥å‘Šsandboxåç§°å†²çªçš„é”™è¯¯ï¼Œå¯¼è‡´å¯åŠ¨å»¶è¿Ÿã€‚è¿™æ˜¯ç”±äºå¼‚å¸¸å…³æœºåï¼Œå®¹å™¨è¿è¡Œæ—¶ï¼ˆ`isulad`ï¼‰æœªèƒ½æ­£ç¡®æ¸…ç†æ­¤å‰çš„sandboxï¼Œå¯¼è‡´åç§°å†²çªã€‚

è¯¥é—®é¢˜æ˜¯ç”±äºç³»ç»Ÿå¼‚å¸¸å¯¼è‡´çš„èµ„æºæ¸…ç†ä¸å½»åº•ï¼Œå±äºæ­£å¸¸çš„é”™è¯¯å¤„ç†èŒƒå›´å†…ï¼Œæ²¡æœ‰æåŠä»»ä½•å¯ä»¥è¢«æ”»å‡»è€…åˆ©ç”¨çš„æ¼æ´æˆ–å®‰å…¨é£é™©ã€‚æ”»å‡»è€…æ— æ³•é€šè¿‡æ­¤é—®é¢˜è·å¾—æ›´é«˜çš„æƒé™ï¼Œæ‰§è¡Œå‘½ä»¤ï¼Œæˆ–å½±å“å…¶ä»–ç”¨æˆ·çš„å®¹å™¨ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œç‰¹åˆ«æ˜¯ç¬¬6æ¡ï¼šâ€œå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠâ€ï¼Œå› æ­¤æ­¤Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126571 RBAC not work for /healthz

- Issue é“¾æ¥ï¼š[#126571](https://github.com/kubernetes/kubernetes/issues/126571)

### Issue å†…å®¹

#### What happened?

ref https://kubernetes.io/docs/reference/using-api/health-checks/
```
$ curl -k https://localhost:6443/livez?verbose
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {},
  "status": "Failure",
  "message": "Unauthorized",
  "reason": "Unauthorized",
  "code": 401
}
$ curl -k https://localhost:6443/healthz
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {},
  "status": "Failure",
  "message": "Unauthorized",
  "reason": "Unauthorized",
  "code": 401
}
```

I see that the cluster has the following rules by default, but it does not actually take effect

```
$ kubectl get clusterrolebinding system:public-info-viewer -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  creationTimestamp: "2024-07-25T09:45:24Z"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:public-info-viewer
  resourceVersion: "136"
  uid: 6fa4a428-1d86-44cb-b81c-c034b43d0d7d
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:public-info-viewer
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:authenticated
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:unauthenticated
```

clusterrole
```
$ kubectl get clusterrole system:public-info-viewer -o yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  creationTimestamp: "2024-07-25T09:45:23Z"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:public-info-viewer
  resourceVersion: "74"
  uid: 464a60de-a111-4387-966a-9e094c08747f
rules:
- nonResourceURLs:
  - /healthz
  - /livez
  - /readyz
  - /version
  - /version/
  verbs:
  - get

```

#### What did you expect to happen?

return "ok"

#### How can we reproduce it (as minimally and precisely as possible)?

install by bin file + systemd service


#### Anything else we need to know?

cmdline:
```
kube-apiserver \
  --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,DefaultTolerationSeconds \
  --default-not-ready-toleration-seconds=300 \
  --default-unreachable-toleration-seconds=10 \
  --bind-address=0.0.0.0 \
  --authorization-mode=Node,RBAC \
  --kubelet-client-certificate=/etc/kubernetes/cluster1/ssl/kubernetes.pem \
  --kubelet-client-key=/etc/kubernetes/cluster1/ssl/kubernetes-key.pem \
  --anonymous-auth=false \
  --service-cluster-ip-range=10.186.0.0/16 \
  --service-node-port-range=20000-40000 \
  --tls-cert-file=/etc/kubernetes/cluster1/ssl/kubernetes.pem \
  --tls-private-key-file=/etc/kubernetes/cluster1/ssl/kubernetes-key.pem \
  --client-ca-file=/etc/kubernetes/cluster1/ssl/ca.pem \
  --service-account-issuer=https://kubernetes.default.svc \
  --service-account-signing-key-file=/etc/kubernetes/cluster1/ssl/ca-key.pem \
  --service-account-key-file=/etc/kubernetes/cluster1/ssl/ca.pem \
  --etcd-cafile=/etc/kubernetes/cluster1/ssl/ca.pem \
  --etcd-certfile=/etc/kubernetes/cluster1/ssl/kubernetes.pem \
  --etcd-keyfile=/etc/kubernetes/cluster1/ssl/kubernetes-key.pem \
  --etcd-servers=https://10.xxx.xx.xxx:2379 \
  --endpoint-reconciler-type=lease \
  --allow-privileged=true \
  --requestheader-client-ca-file=/etc/kubernetes/cluster1/ssl/ca.pem \
  --requestheader-allowed-names= \
  --requestheader-extra-headers-prefix=X-Remote-Extra- \
  --requestheader-group-headers=X-Remote-Group \
  --requestheader-username-headers=X-Remote-User \
  --proxy-client-cert-file=/etc/kubernetes/cluster1/ssl/aggregator-proxy.pem \
  --proxy-client-key-file=/etc/kubernetes/cluster1/ssl/aggregator-proxy-key.pem \
  --enable-aggregator-routing=true \
  --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_256_GCM_SHA384 \
  --v=2
```

I found that it seems that the cluster deployed by `kubeadm` can return ok

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.15", GitCommit:"fb63712e1d017142977e88a23644b8e48b775665", GitTreeState:"clean", BuildDate:"2024-06-11T20:04:38Z", GoVersion:"go1.21.11", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.15", GitCommit:"fb63712e1d017142977e88a23644b8e48b775665", GitTreeState:"clean", BuildDate:"2024-06-11T19:56:02Z", GoVersion:"go1.21.11", Compiler:"gc", Platform:"linux/amd64"}

```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

bin file and systemd

#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†æœªè®¤è¯ç”¨æˆ·æ— æ³•è®¿é—®`/healthz`æ¥å£çš„é—®é¢˜ã€‚æ ¹æ®æä¾›çš„ä¿¡æ¯ï¼Œå°½ç®¡ClusterRoleå’ŒClusterRoleBindingå·²ç»é…ç½®å…è®¸æœªè®¤è¯ç”¨æˆ·è®¿é—®`/healthz`ï¼Œä½†å®é™…è¯·æ±‚ä»ç„¶è¿”å›äº†`401 Unauthorized`é”™è¯¯ã€‚

ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œ`/healthz`æ¥å£é€šå¸¸ç”¨äºå¥åº·æ£€æŸ¥ï¼Œå…è®¸æœªè®¤è¯ç”¨æˆ·è®¿é—®å¹¶ä¸ä¼šå¸¦æ¥å®‰å…¨é£é™©ã€‚ç›¸åï¼Œå½“å‰çš„æƒ…å†µæ˜¯æœªè®¤è¯ç”¨æˆ·æ— æ³•è®¿é—®`/healthz`ï¼Œè¿™å¯èƒ½ä¼šå½±å“è´Ÿè½½å‡è¡¡å™¨æˆ–ç›‘æ§ç³»ç»Ÿå¯¹é›†ç¾¤å¥åº·çŠ¶æ€çš„æ£€æµ‹ï¼Œä½†è¿™å±äºåŠŸèƒ½æ€§é—®é¢˜è€Œéå®‰å…¨é—®é¢˜ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šä¸å­˜åœ¨å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„é£é™©ã€‚
2. **å¯èƒ½æˆä¸ºæ¼æ´å¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä¸”CVSSè¯„åˆ†åœ¨highä»¥ä¸Š**ï¼šä¸ç¬¦åˆã€‚
6. **å¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠ**ï¼šç¬¦åˆã€‚

å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126569 PVC Mounts fails when STAGE_UNSTAGE_VOLUME check fails for k8s EFS CSI

- Issue é“¾æ¥ï¼š[#126569](https://github.com/kubernetes/kubernetes/issues/126569)

### Issue å†…å®¹

#### What happened?

PVC Mount fails for CronJob 
Kubelet fails to check for `STAGE_UNSTAGE_VOLUME` capability

#### What did you expect to happen?

I expected that in retries PVC should get mounted


#### How can we reproduce it (as minimally and precisely as possible)?

We've been seeing this issue intermittently but we can follow steps mentioned over https://github.com/kubernetes/kubernetes/issues/112969#issuecomment-1276871605 to reproduce the issue

#### Anything else we need to know?

Relatively similar issue of what's over here https://github.com/kubernetes/kubernetes/issues/112969


Below are the k8s event from our k8s cluster

```console
Events:
  Type     Reason       Age                    From     Message
  ----     ------       ----                   ----     -------
  Warning  FailedMount  41m (x12 over 154m)    kubelet  Unable to attach or mount volumes: unmounted volumes=[dummy-volume], unattached volumes=[kube-api-access-abcde dummy-volume]: timed out waiting for the condition
  Warning  FailedMount  37m (x26 over 167m)    kubelet  MountVolume.SetUp failed for volume "pvc-a1b34545-123a-321a-123b-12345a6b789c" : rpc error: code = Unavailable desc = error reading from server: EOF
  Warning  FailedMount  32m (x10 over 145m)    kubelet  Unable to attach or mount volumes: unmounted volumes=[dummy-volume-a desiree-data-global], unattached volumes=[dummy-volume-a kube-api-access-abcde desiree-data-global]: timed out waiting for the condition
  Warning  FailedMount  26m (x48 over 167m)    kubelet  MountVolume.SetUp failed for volume "pvc-a1b34545-123a-321a-123b-12345a6b789c" : kubernetes.io/csi: mounter.SetUpAt failed to check for STAGE_UNSTAGE_VOLUME capability: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing: dial unix /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock: connect: connection refused"
  Warning  FailedMount  22m (x49 over 167m)    kubelet  MountVolume.SetUp failed for volume "pvc-k1l34545-123s-321g-123h-12345a6b789d" : kubernetes.io/csi: mounter.SetUpAt failed to check for STAGE_UNSTAGE_VOLUME capability: rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing: dial unix /var/lib/kubelet/plugins/efs.csi.aws.com/csi.sock: connect: connection refused"
  Warning  FailedMount  12m (x30 over 167m)    kubelet  MountVolume.SetUp failed for volume "pvc-k1l34545-123s-321g-123h-12345a6b789d" : rpc error: code = Unavailable desc = error reading from server: EOF
  Warning  FailedMount  2m51s (x45 over 166m)  kubelet  Unable to attach or mount volumes: unmounted volumes=[dummy-volume], unattached volumes=[dummy-volume kube-api-access-abcde]: timed out waiting for the condition
  ```



#### Kubernetes version

<details>

```console
Client Version: version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.0", GitCommit:"a866cbe2e5bbaa01cfd5e969aa3e033f3282a8a2", GitTreeState:"clean", BuildDate:"2022-08-23T17:44:59Z", GoVersion:"go1.19", Compiler:"gc", Platform:"darwin/amd64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.16", GitCommit:"c5f43560a4f98f2af3743a59299fb79f07924373", GitTreeState:"clean", BuildDate:"2023-11-15T22:28:05Z", GoVersion:"go1.20.10", Compiler:"gc", Platform:"linux/arm64"}
```

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

N/A

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
CSI
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueå†…å®¹ï¼Œç”¨æˆ·åœ¨ä½¿ç”¨AWS EFS CSIé©±åŠ¨æ—¶ï¼Œé‡åˆ°äº†PVCï¼ˆPersistentVolumeClaimï¼‰æŒ‚è½½å¤±è´¥çš„é—®é¢˜ã€‚å…·ä½“è¡¨ç°ä¸ºKubeletåœ¨æ£€æŸ¥`STAGE_UNSTAGE_VOLUME`èƒ½åŠ›æ—¶å¤±è´¥ï¼Œæ— æ³•æ­£å¸¸æŒ‚è½½PVCï¼Œå‡ºç°äº†å¤šæ¬¡é‡è¯•ä»æ— æ³•æŒ‚è½½çš„æƒ…å†µã€‚

ä»å®‰å…¨é£é™©çš„è§’åº¦åˆ†æï¼Œè¿™ä¸ªé—®é¢˜ä¸»è¦æ˜¯ç”±äºKubeletä¸CSIé©±åŠ¨ä¹‹é—´çš„é€šä¿¡é”™è¯¯æˆ–CSIæ’ä»¶æœªæ­£å¸¸è¿è¡Œå¯¼è‡´çš„èµ„æºä¸å¯ç”¨é—®é¢˜ï¼Œå±äºç³»ç»Ÿç¨³å®šæ€§å’Œå¯ç”¨æ€§æ–¹é¢çš„é—®é¢˜ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. æ”»å‡»è€…æ— æ³•åˆ©ç”¨è¯¥é—®é¢˜è·å–æ›´é«˜æƒé™æˆ–æ‰§è¡Œæœªæˆæƒçš„æ“ä½œã€‚
2. è¯¥é—®é¢˜ä¸ä¼šå¯¼è‡´ç³»ç»Ÿå­˜åœ¨å¯è¢«åˆ†é…CVEç¼–å·çš„æ¼æ´ï¼Œä¸”æ ¹æ®CVSS 3.1è¯„åˆ†æ ‡å‡†ï¼Œé£é™©ç­‰çº§ä¸ä¼šè¾¾åˆ°é«˜é£é™©çº§åˆ«ã€‚
6. å¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126564 kubectl logs -f stop after log rotation

- Issue é“¾æ¥ï¼š[#126564](https://github.com/kubernetes/kubernetes/issues/126564)

### Issue å†…å®¹

#### What happened?

I have a problem that kubectl logs -f stop after log file rotation. 
https://github.com/kubernetes/kubernetes/pull/115702
In this link said this problem solved but when I update my kuber version to 1.29.0, I have this problem yet.

#### What did you expect to happen?

`kubectl logs -f` continue showing logs after log rotation.

#### How can we reproduce it (as minimally and precisely as possible)?

I have create this pod:
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: counter
spec:
  containers:
  - args:
    - /bin/sh
    - -c
    - 'i=0; while true; do echo "$i: $(date)"; i=$((i+1)); sleep 0.1; done'
    image: busybox:1.28
    imagePullPolicy: IfNotPresent
    name: count
```

Then I use this command to check logs:
```
k logs -f counter
```

After rotation `k logs -f counter` stop showing logs. When I check time of last log find this is as same as time of rotation of log file ( after 10M)

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.29.0

#### Cloud provider

<details>
on-premise
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ä½¿ç”¨`kubectl logs -f`è·Ÿè¸ªPodæ—¥å¿—æ—¶ï¼Œå½“æ—¥å¿—æ–‡ä»¶å‘ç”Ÿè½®è½¬åï¼Œæ—¥å¿—è·Ÿè¸ªä¼šåœæ­¢çš„é—®é¢˜ã€‚è¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ç¼ºé™·ï¼Œå½±å“äº†æ—¥å¿—çš„è¿ç»­æ€§ã€‚ä½†æ˜¯ï¼Œæ­¤é—®é¢˜å¹¶ä¸å­˜åœ¨è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨é£é™©ï¼Œä¸ä¼šå¯¼è‡´ç³»ç»Ÿè¢«æ”»å‡»æˆ–äº§ç”Ÿé«˜å±æ¼æ´ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬6æ¡ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œå› æ­¤é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

---

## Issue #126552 Slow FSGroup recursive permission changes cause customer confusion

- Issue é“¾æ¥ï¼š[#126552](https://github.com/kubernetes/kubernetes/issues/126552)

### Issue å†…å®¹

#### What happened?

[FSGroup volume permission setting](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#configure-volume-permission-and-ownership-change-policy-for-pods) is very useful for making NFS volumes just work in spite of the impedance between containers and traditional user id based linux file permissions. 

However, in practice with large volumes, fsgroup settings can cause workloads to effectively get stuck, as it can take hours or days to recursively change permissions on multi-terabyte servers. The fsGroupChangePolicy has helped that (onRootMismatch is a win), but this has still been a perennial source of customer issues.

The [kubelet does log when a long fsgroup change is in progress](https://github.com/kubernetes/kubernetes/blob/master/pkg/volume/volume_linux.go#L49), but this does not provide enough actionable information.


#### What did you expect to happen?


The kubelet logging could be extended to provide updates of long-running fsgroup recursion, and maybe even some kind of progress (eg, some estimates of how much of the file tree has been walked).

#### How can we reproduce it (as minimally and precisely as possible)?

Reproduction requires a large volume with many files, but it can be done with any volume type, eg RWO.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
All versions
</details>


#### Cloud provider

<details>
We've seen customer complaints on GKE, but I imagine it happens on all providers.
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueè®¨è®ºäº†åœ¨Kubernetesä¸­ä½¿ç”¨FSGroupè¿›è¡Œå·çš„æƒé™æ›´æ”¹æ—¶ï¼Œå½“å·åŒ…å«å¤§é‡æ–‡ä»¶æ—¶ï¼Œé€’å½’æ›´æ”¹æƒé™å¯èƒ½éœ€è¦è€—è´¹æ•°å°æ—¶ç”šè‡³æ•°å¤©ï¼Œå¯¼è‡´å·¥ä½œè´Ÿè½½è¢«å¡ä½ã€‚Issueå»ºè®®æ”¹è¿›kubeletçš„æ—¥å¿—è®°å½•ï¼Œä»¥æä¾›é•¿æ—¶é—´è¿è¡Œçš„fsgroupé€’å½’æ“ä½œçš„æ›´æ–°å’Œè¿›åº¦ä¼°è®¡ã€‚è¿™å±äºæ€§èƒ½å’Œç”¨æˆ·ä½“éªŒæ–¹é¢çš„é—®é¢˜ï¼Œä¸æ¶‰åŠä»»ä½•å®‰å…¨é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ã€‚

---

## Issue #126547 duplicate init kube-apiserver gorestful.container.ServeMux

- Issue é“¾æ¥ï¼š[#126547](https://github.com/kubernetes/kubernetes/issues/126547)

### Issue å†…å®¹

#### What happened?

https://github.com/kubernetes/kubernetes/blob/00236ae0d73d2455a2470469ed1005674f8ed61f/staging/src/k8s.io/apiserver/pkg/server/handler.go#L80
https://github.com/emicklei/go-restful/blob/33de94869dbe48c2ad3bba44083546d0672fc359/container.go#L39

#### What did you expect to happen?

whether should init gorestful.container.ServeMux once

#### How can we reproduce it (as minimally and precisely as possible)?

none

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
é€šè¿‡åˆ†æè¯¥Issueå†…å®¹ï¼Œå¯ä»¥çœ‹å‡ºï¼Œæäº¤è€…æŒ‡å‡ºäº†åœ¨kube-apiserverä¸­é‡å¤åˆå§‹åŒ–äº†`gorestful.container.ServeMux`ï¼Œæå‡ºæ˜¯å¦åº”è¯¥åªåˆå§‹åŒ–ä¸€æ¬¡ã€‚

é‡å¤åˆå§‹åŒ–`ServeMux`å¯èƒ½ä¼šå¯¼è‡´ç¨‹åºçš„å¼‚å¸¸è¡Œä¸ºï¼Œä¾‹å¦‚è·¯ç”±å†²çªã€è¦†ç›–ç­‰ã€‚ä½†ä»å½“å‰æä¾›çš„ä¿¡æ¯æ¥çœ‹ï¼Œæ²¡æœ‰å…·ä½“æŒ‡å‡ºè¿™ç§é‡å¤åˆå§‹åŒ–ä¼šå¯¼è‡´ä»»ä½•å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. è¯¥é£é™©æ˜¯å¦èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨ï¼Ÿç›®å‰å¹¶æ²¡æœ‰è¯æ®è¡¨æ˜æ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ­¤é—®é¢˜ã€‚
2. è¯¥é£é™©æ˜¯å¦æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä¸”CVSSè¯„åˆ†åœ¨é«˜é£é™©ä»¥ä¸Šï¼Ÿæ ¹æ®ç°æœ‰ä¿¡æ¯ï¼Œæ— æ³•åˆ¤æ–­æ­¤é—®é¢˜ä¼šå¯¼è‡´é«˜é£é™©æ¼æ´ã€‚

å› æ­¤ï¼ŒåŸºäºç›®å‰çš„ä¿¡æ¯ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126546 Impossible to notice without major delay when a container exits while the POD is in terminating state

- Issue é“¾æ¥ï¼š[#126546](https://github.com/kubernetes/kubernetes/issues/126546)

### Issue å†…å®¹

#### What happened?

When a POD which is part of a service starts graceful termination, and the main container exits the related endpoint in endpointslice does not go to serving=false. Only after readiness probe fails will the endpoint enter serving=false state. Which doesn't make sense in my opinion.

I also tried watching for POD changes and determine that the container exited based on container_statuses list. But there I hit this issue: https://github.com/kubernetes/kubernetes/issues/106896

Then I tried to remove the label which selects the POD as an endpoint by the service, but then I experienced ~5s delay till kubernetes notified that the endpoint has disappeared.

#### What did you expect to happen?

The endpoint should go to serving=false when any container exits in a POD which is under graceful termination.
Also the alternatives I tried should have worked I think.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a POD with 2 containers, 1 which exits immediately after receiving SIGTRERM, the other doesn't so it is killed by SIGKILL after termination grace period. The first container should also have a readiness probe which doesn't fail quick. ex.: period 5s, failureThreshold 3. Make the POD be part of a service. Delete the pod. Check the endpoint in the endpointslice only going to serving=false, after the readiness probe fails. 

#### Anything else we need to know?

_No response_

#### Kubernetes version

kubectl version
Client Version: v1.28.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.1

#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Podç»ˆæ­¢è¿‡ç¨‹ä¸­ï¼Œå½“å…¶ä¸­ä¸€ä¸ªå®¹å™¨é€€å‡ºæ—¶ï¼Œendpointsliceæ²¡æœ‰ç«‹å³æ›´æ–°endpointçš„servingçŠ¶æ€ï¼Œåªæœ‰åœ¨readiness probeå¤±è´¥åï¼Œendpointæ‰è¿›å…¥serving=falseçŠ¶æ€ã€‚è¿™å¯èƒ½å¯¼è‡´åœ¨Podç»ˆæ­¢æœŸé—´ï¼ŒæœåŠ¡ä»ç„¶å°†æµé‡è·¯ç”±åˆ°å·²é€€å‡ºçš„å®¹å™¨ï¼Œå¯èƒ½å¼•èµ·çŸ­æš‚çš„æœåŠ¡ä¸å¯ç”¨ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1. è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨ï¼Ÿæ­¤é—®é¢˜éœ€è¦æ”»å‡»è€…èƒ½å¤Ÿè§¦å‘Podçš„åˆ é™¤æˆ–ç»ˆæ­¢ï¼Œä»¥åŠæ§åˆ¶å®¹å™¨çš„é€€å‡ºè¡Œä¸ºï¼Œè¿™é€šå¸¸éœ€è¦å…·å¤‡ç›¸åº”çš„æƒé™ã€‚
4. å½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨highä»¥ä¸‹ã€‚
6. å¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œæ­¤Issueæ¶‰åŠçš„æ˜¯Kubernetesåœ¨Podç»ˆæ­¢è¿‡ç¨‹ä¸­çš„çŠ¶æ€æ›´æ–°å»¶è¿Ÿé—®é¢˜ï¼Œå±äºåŠŸèƒ½æ€§æˆ–å¯é æ€§é—®é¢˜ï¼Œå¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126531 Inconsistency between the code and the doc on how `matchFields` works

- Issue é“¾æ¥ï¼š[#126531](https://github.com/kubernetes/kubernetes/issues/126531)

### Issue å†…å®¹

#### What happened?

The `pod.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms.matchFields` is introduced in PR [#62202](https://github.com/kubernetes/kubernetes/pull/62002) and used to bind a pod directly to nodes via `metadata.name`. However, we find that there are some hidden constraints on how to use this field that are not documented well.

More concretely, the current API documentation says that
```
> kubectl explain pod.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms.matchFields
    KIND:       Pod
    VERSION:    v1

    FIELD: matchFields <[]NodeSelectorRequirement>


    DESCRIPTION:
        A list of node selector requirements by node's fields.
        A node selector requirement is a selector that contains values, a key, and
        an operator that relates the key and values.
    
    FIELDS:
      key   <string> -required-
        The label key that the selector applies to.
    
      operator      <string> -required-
      enum: DoesNotExist, Exists, Gt, In, ....
        Represents a key's relationship to a set of values. Valid operators are In,
        NotIn, Exists, DoesNotExist. Gt, and Lt.
    
        Possible enum values:
         - `"DoesNotExist"`
         - `"Exists"`
         - `"Gt"`
         - `"In"`
         - `"Lt"`
         - `"NotIn"`
    
      values        <[]string>
        An array of string values. If the operator is In or NotIn, the values array
        must be non-empty. If the operator is Exists or DoesNotExist, the values
        array must be empty. If the operator is Gt or Lt, the values array must have
        a single element, which will be interpreted as an integer. This array is
        replaced during a strategic merge patch.
```
And the doc does not mention that its key must be `metadata.name`, it's operator must be in `In/NotIn`, and the number of its value<s>s</s> must be equal to 1 (thought the term itself can be defined multiple times in `nodeSelectorTerms`).


This issue has also caused confusion in [#115980](https://github.com/kubernetes/kubernetes/issues/115980), [#81725](https://github.com/kubernetes/kubernetes/issues/81725), [#78238](https://github.com/kubernetes/kubernetes/issues/78238) and a reddit [thread](https://stackoverflow.com/questions/67018171/kubernetes-what-are-valid-node-fields)


#### What did you expect to happen?

The hidden constraints should be explicitly documented if they reflect the developers' real intent. For example, we can mention this field in kubernetes.io docs, and clarify in the API doc on the constraints.

Besides improving the doc, we are thinking about how to prevent misconfiguration in this field (e.g., someone specifies things other than node name in `matchFields`) in the first place. One direction is to make the API more restrictive rather than take in any valid strings.


#### How can we reproduce it (as minimally and precisely as possible)?

Read the documentation and the code.

#### Anything else we need to know?

/sig scheduling docs

#### Kubernetes version

<details>

Since 1.11, exists in 1.30

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueå†…å®¹ï¼Œè®¨è®ºçš„æ˜¯`matchFields`å­—æ®µåœ¨æ–‡æ¡£å’Œä»£ç ä¸­çš„ä¸ä¸€è‡´æ€§ï¼Œä»¥åŠè¯¥å­—æ®µä½¿ç”¨æ—¶çš„éšè—çº¦æŸæ²¡æœ‰æ˜ç¡®è®°å½•ã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´ç”¨æˆ·åœ¨é…ç½®Podçš„NodeAffinityæ—¶å‡ºç°å›°æƒ‘æˆ–è¯¯ç”¨ã€‚ç„¶è€Œï¼Œè¿™å¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œä¹Ÿæ²¡æœ‰æåŠä»»ä½•å¯èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨çš„æ¼æ´ã€‚å› æ­¤ï¼ŒæŒ‰ç…§é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬6æ¡ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ã€‚

---

## Issue #126527 [FG:InPlacePodVerticalScaling] Handle pod resize even if the pod has not started yet

- Issue é“¾æ¥ï¼š[#126527](https://github.com/kubernetes/kubernetes/issues/126527)

### Issue å†…å®¹

#### What happened?

If an unacceptable pod resizing that causes `Deferred` or `Infeasible` is requested before the container is started (for example, while an init container is running), the container is started with the unacceptable spec.

```
$ kubectl create -f pod.yaml; sleep 5; kubectl patch pod resize-pod --patch '{"spec": {"containers": [{"name": "resize-container", "resources":{"requests": {"cpu": "100"}, "limits": {"cpu": "100"}}}]}}'
pod/resize-pod created
pod/resize-pod patched
$ kubectl get pod resize-pod -o jsonpath='spec: {.spec.containers[0].resources}{"\nallocatedResources: "}{.status.containerStatuses[0].allocatedResources}{"\nstatus: "}{.status.containerStatuses[0].resources}{"\nresize: "}{.status.resize}{"\n"}'
spec: {"limits":{"cpu":"100","memory":"200Mi"},"requests":{"cpu":"100","memory":"200Mi"}}
allocatedResources: {"cpu":"200m","memory":"200Mi"}
status: {"limits":{"cpu":"100","memory":"200Mi"},"requests":{"cpu":"100","memory":"200Mi"}}
resize: Infeasible
```


The pod is admitted with the initial spec when the pod is created. Then, the resized spec is not verified for admission because the pod is not running yet:
https://github.com/kubernetes/kubernetes/blob/dbc2b0a5c7acc349ea71a14e49913661eaf708d2/pkg/kubelet/kubelet.go#L2811-L2814
As a result, the container is started with the unacceptable spec. Eventually, the pod gets into `Infeasible` resize status after the pod is started because the allocated resources that are not updated differs from the resized pod spec.

It does not seems that this issue affects actual resource consumption similarly to #126033. Because the pod cgroup is not updated in this case, the container resource will keep limited. In addition, since `AllocatedResources` in the container is not updated, this infeasible resizing will not affect the pod resource calculation of the scheduler.

#### What did you expect to happen?

The pod is started with the initial spec and gets into `Infeasible` resize status or the pod fails to start.


#### How can we reproduce it (as minimally and precisely as possible)?

0. Enable `InPlacePodVerticalScaling`.
1. Create a pod with an init container that takes a few seconds to complete:
    <Details>

   ```
   apiVersion: v1
   kind: Pod
   metadata:
     creationTimestamp: null
     labels:
       run: resize-pod
     name: resize-pod
   spec:
     initContainers:
     - image: busybox
       name: init-container
       command:
         - sleep
         - "10"
       resources:
         requests:
           cpu: 100m
           memory: 100Mi
         limits:
           cpu: 100m
           memory: 100Mi
     containers:
     - image: busybox
       name: resize-container
       command:
         - sh
         - -c
         - trap "exit 0" SIGTERM; while true; do sleep 1; done
       resources:
         requests:
           cpu: 200m
           memory: 200Mi
         limits:
           cpu: 200m
           memory: 200Mi
       resizePolicy:
       - resourceName: cpu
         restartPolicy: NotRequired
       - resourceName: memory
         restartPolicy: NotRequired
     restartPolicy: Always
   ```

   </Details>
2. While the init container is running, patch the pod with an infeasible resize request:
   ```
   $ kubectl create -f pod.yaml; sleep 5; kubectl patch pod resize-pod --patch '{"spec": {"containers": [{"name": "resize-container", "resources":{"requests": {"cpu": "100"}, "limits": {"cpu": "100"}}}]}}'
   ```
3. Watch the pod:
   ```
   $ kubectl get pod resize-pod -o jsonpath='spec: {.spec.containers[0].resources}{"\nallocatedResources: "}{.status.containerStatuses[0].allocatedResources}{"\nstatus: "}{.status.containerStatuses[0].resources}{"\nresize: "}{.status.resize}{"\n"}' -w
   ```


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.2
```

</details>


#### Cloud provider

N/A


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨InPlacePodVerticalScalingåŠŸèƒ½æ—¶ï¼Œå¦‚æœåœ¨å®¹å™¨å¯åŠ¨å‰è¯·æ±‚äº†ä¸€ä¸ªä¸å¯æ¥å—çš„Podèµ„æºè°ƒæ•´ï¼ˆå¯¼è‡´`Deferred`æˆ–`Infeasible`çŠ¶æ€ï¼‰ï¼Œå®¹å™¨ä¼šä»¥ä¸å¯æ¥å—çš„è§„æ ¼å¯åŠ¨ã€‚ç„¶è€Œï¼Œæ ¹æ®Issueå†…å®¹ï¼Œè¯¥é—®é¢˜å¹¶ä¸ä¼šå½±å“å®é™…çš„èµ„æºæ¶ˆè€—ï¼Œå› ä¸ºPodçš„cgroupæ²¡æœ‰è¢«æ›´æ–°ï¼Œå®¹å™¨èµ„æºä»ç„¶å—é™ã€‚æ­¤å¤–ï¼Œå®¹å™¨çš„`AllocatedResources`æœªè¢«æ›´æ–°ï¼Œè¿™ç§ä¸å¯è¡Œçš„è°ƒæ•´ä¹Ÿä¸ä¼šå½±å“è°ƒåº¦å™¨å¯¹Podèµ„æºçš„è®¡ç®—ã€‚å› æ­¤ï¼Œè¯¥é—®é¢˜å¹¶ä¸å¯¼è‡´èµ„æºæ³„éœ²ã€æƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œç­‰å®‰å…¨é£é™©ï¼Œåªæ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ç¼ºé™·ï¼Œå±äºæ­£å¸¸çš„BugèŒƒç•´ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†6ï¼Œå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

---

## Issue #126630 Cannot set kubelet config `resolvConf` with drop-in config files

- Issue é“¾æ¥ï¼š[#126630](https://github.com/kubernetes/kubernetes/issues/126630)

### Issue å†…å®¹

#### What happened?

I created a kubernetes cluster with kubeadm and created a drop-in directory for kubelet configuration at `/etc/kubernetes/kubelet.conf.d`.

I created a config file within the directory to change the value of `resolvConf`. After restarting kubelet, the value of `resolvConf` stayed same.

#### What did you expect to happen?

The value of `resolvConf` is updated.

#### How can we reproduce it (as minimally and precisely as possible)?

```sh
# Create custom resolv.conf
echo 'nameserver 8.8.8.8' | sudo tee /etc/resolv.kubelet.conf

# Create drop-in config directory
sudo mkdir /etc/kubernetes/kubelet.conf.d
sudo chmod 700 /etc/kubernetes/kubelet.conf.d

# Create drop-in config file
cat <<EOF | sudo tee /etc/kubernetes/kubelet.conf.d/99-resolv-conf.conf
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
resolvConf: /etc/resolv.kubelet.conf
EOF
sudo chmod 600 /etc/kubernetes/kubelet.conf.d/99-resolv-conf.conf

# Add --config-dir flag
echo 'KUBELET_EXTRA_ARGS=--config-dir=/etc/kubernetes/kubelet.conf.d' | sudo tee /etc/default/kubelet

sudo systemctl daemon-reload
sudo systemctl restart kubelet

# Check if the value of resolvConf is changed.
kubectl get --raw "/api/v1/nodes/<node name>/proxy/configz" | jq
```

#### Anything else we need to know?

I think this issue is particular to the drop-in config and `resolvConf` option because:

- I can configure `resolvConf` by editing `/var/lib/kubelet/config.yaml`
- I can update another option like `streamingConnectionIdleTimeout` with drop-in config.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.3
```

</details>


#### Cloud provider

<details>
No cloud provider
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"
$ uname -a
Linux vm-1aefb131-31 6.1.0-21-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.90-1 (2024-05-03) x86_64 GNU/Linux
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæ¶‰åŠåˆ°åœ¨ä½¿ç”¨kubeadmåˆ›å»ºçš„Kubernetesé›†ç¾¤ä¸­ï¼Œé€šè¿‡åœ¨`/etc/kubernetes/kubelet.conf.d`ç›®å½•ä¸‹åˆ›å»ºdrop-iné…ç½®æ–‡ä»¶æ¥ä¿®æ”¹kubeletçš„`resolvConf`é…ç½®ã€‚ç„¶è€Œï¼Œé‡å¯kubeletåï¼Œ`resolvConf`çš„å€¼å¹¶æœªæ›´æ–°ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1. è¯¥é—®é¢˜å±äºé…ç½®é—®é¢˜ï¼Œæ²¡æœ‰æåŠä»»ä½•å¯èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨çš„æ¼æ´æˆ–å®‰å…¨é£é™©ã€‚
2. æ­¤é—®é¢˜ä¸ä¼šæˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œä¸ä¼šè¢«åˆ†é…CVEç¼–å·ï¼ŒæŒ‰ç…§CVSS 3.1è¯„åˆ†æ ‡å‡†ä¹Ÿä¸ä¼šè¾¾åˆ°Highçº§åˆ«ã€‚
3. Issueæäº¤è€…æœªåœ¨å†…å®¹ä¸­æš´éœ²æ•æ„Ÿä¿¡æ¯æˆ–å­˜åœ¨ä¸å½“æ“ä½œã€‚
4. ä¸æ¶‰åŠæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚
5. æœªæåŠå‡­æ®æ³„éœ²ç­‰é«˜é£é™©æƒ…å†µã€‚
6. å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ã€‚

---

## Issue #126616 [FG:InPlacePodVerticalScaling] Change in version-skewed behavior in v1.31

- Issue é“¾æ¥ï¼š[#126616](https://github.com/kubernetes/kubernetes/issues/126616)

### Issue å†…å®¹

For features that span nodes & control planes, they must support the case where the feature is enabled in the control plane but not on nodes, in order to support version skew after the feature is promoted to a default-on state.

Previously, the node/control-plane skewed behavior of `InPlacePodVerticalScaling` was to sort of support the resize by restarting the resized container. The API is eventually consistent with the running container.

1. Someone / something updates pod resources on a running pod
2. apiserver has the feature gate enabled, so it accepts the request and sets the resize status to pending
3. Kubelet sees a change in the hash and restarts the container, with the new resource amount
4. Kubelet patches the status, but doesn't set the resize status (and doesn't copy it from the old status), thus clearing it

In v1.31 the hashing logic was changed to avoid accidental container restarts: https://github.com/kubernetes/kubernetes/pull/124220. However, this also prevents the Kubelet from seeing the changed resources in step 3, so the container is not restarted with the new resource value. If the container is restarted for any other reason, it will be restarted with the new resource value. There is no way to tell from the API which value the container is using.

The previous behavior is consistent with the skew handling described in the KEP, except this part was never implemented:
> kubelet: When feature-gate is disabled, if kubelet sees a Proposed resize, it rejects the resize as Infeasible.

This is a regression, but shouldn't be release blocking as it is only triggered when the alpha feature is enabled.

/kind bug
/kind regression
/milestone v1.31
/sig node
/priority important-longterm

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ç»è¿‡åˆ†æï¼Œè¯¥Issueæè¿°äº†Kubernetesåœ¨v1.31ç‰ˆæœ¬ä¸­çš„ä¸€ä¸ªå…³äº`InPlacePodVerticalScaling`ç‰¹æ€§çš„è¡Œä¸ºå˜æ›´ã€‚å½“æ§åˆ¶å¹³é¢å¯ç”¨äº†è¯¥ç‰¹æ€§ï¼Œè€ŒèŠ‚ç‚¹ä¸Šçš„kubeletæœªå¯ç”¨è¯¥ç‰¹æ€§ï¼ˆå¯èƒ½ç”±äºç‰ˆæœ¬å·®å¼‚ï¼‰ï¼Œåœ¨v1.31ä¹‹å‰ï¼Œå½“æ›´æ–°è¿è¡Œä¸­Podçš„èµ„æºæ—¶ï¼Œkubeletä¼šæ£€æµ‹åˆ°å“ˆå¸Œå€¼å˜åŒ–ï¼Œå¹¶é‡å¯å®¹å™¨ä»¥åº”ç”¨æ–°çš„èµ„æºé…ç½®ã€‚ç„¶è€Œï¼Œåœ¨v1.31ç‰ˆæœ¬ä¸­ï¼Œç”±äºä¸ºäº†é¿å…æ„å¤–çš„å®¹å™¨é‡å¯ï¼Œæ›´æ”¹äº†å“ˆå¸Œé€»è¾‘ï¼Œå¯¼è‡´kubeletæ— æ³•æ£€æµ‹åˆ°èµ„æºçš„å˜åŒ–ï¼Œå› æ­¤å®¹å™¨ä¸ä¼šç«‹å³ä½¿ç”¨æ–°çš„èµ„æºé…ç½®ã€‚

è¯¥é—®é¢˜å¯¼è‡´äº†APIå’Œå®é™…è¿è¡ŒçŠ¶æ€ä¹‹é—´çš„ä¸ä¸€è‡´ï¼Œå³APIè®¤ä¸ºå®¹å™¨åº”è¯¥ä½¿ç”¨æ–°çš„èµ„æºé…ç½®ï¼Œä½†å®¹å™¨å®é™…ä¸Šä»åœ¨ä½¿ç”¨æ—§çš„èµ„æºé…ç½®ã€‚è™½ç„¶è¿™å¯èƒ½ä¼šé€ æˆä¸€å®šçš„ç®¡ç†æ··ä¹±æˆ–èµ„æºåˆ†é…ä¸Šçš„ä¸ä¸€è‡´ï¼Œä½†è¯¥Issueä¸­å¹¶æœªæåŠä»»ä½•å¯èƒ½å¯¼è‡´å®‰å…¨é£é™©çš„æƒ…å†µã€‚

**æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š**

1. **æ”»å‡»è€…åˆ©ç”¨æ€§ï¼š** è¯¥é—®é¢˜éœ€è¦æ”»å‡»è€…å…·å¤‡æ›´æ–°Podèµ„æºçš„æƒé™ï¼Œå³éœ€è¦å…·å¤‡åˆ›å»ºæˆ–ä¿®æ”¹ç­‰éåªè¯»æƒé™ï¼Œæ™®é€šç”¨æˆ·æ— æ³•åˆ©ç”¨ã€‚

2. **é£é™©ç­‰çº§ï¼š** è¯¥é—®é¢˜ä¸ä¼šå¯¼è‡´å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰é«˜å®‰å…¨é£é™©ï¼Œä¹Ÿä¸ä¼šå¯¼è‡´å‡­æ®æ³„éœ²ï¼Œç¬¦åˆæ ‡å‡†ä¸­å¯¹ä½é£é™©æˆ–ä¸æ¶‰åŠçš„æè¿°ã€‚

3. **å½±å“èŒƒå›´ï¼š** é—®é¢˜ä»…åœ¨å¯ç”¨äº†alphaç‰¹æ€§çš„æƒ…å†µä¸‹è§¦å‘ï¼Œå½±å“èŒƒå›´æœ‰é™ï¼Œå¹¶ä¸”éœ€è¦ç‰¹å®šçš„ç‰ˆæœ¬å’Œé…ç½®ç»„åˆã€‚

å› æ­¤ï¼Œç»¼åˆä»¥ä¸Šå› ç´ ï¼Œåˆ¤æ–­è¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126607 Pod Stuck In Terminating State After Kublet Restart

- Issue é“¾æ¥ï¼š[#126607](https://github.com/kubernetes/kubernetes/issues/126607)

### Issue å†…å®¹

#### What happened?

A node in our GKE cluster was experiencing an extremely heavy load during load testing.

CNI pods (calico) and CSI pods crashed several times as well as the kubelet and caused the following.

```
INFO 2024-07-23T02:33:56.075736Z "Updating ready status of pod to false" pod="performance-citus/mirror-citus-shard0-0"
WARNING 2024-07-23T02:33:56Z Node is not ready
INFO 2024-07-23T02:39:01Z Marking for deletion Pod performance-citus/mirror-citus-shard0-0
DEFAULT 2024-07-23T02:33:56.178205Z audit_log, method: "io.k8s.discovery.v1.endpointslices.update", principal_email: "system:serviceaccount:kube-system:endpointslice-controller"
DEFAULT 2024-07-23T02:33:56.177543Z audit_log, method: "io.k8s.core.v1.endpoints.update", principal_email: "system:serviceaccount:kube-system:endpoint-controller"
INFO 2024-07-23T02:33:56.165537Z "Event occurred" object="performance-citus/mirror-citus-shard0-0" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="NodeNotReady" message="Node is not ready"
```

While the kublet was down, I believe a 3rd party operator(stackgres - which manages mirror-citus-shard0-0) attempted to delete the pod
```
INFO 2024-07-23T02:39:01.404541Z "HTTP" verb="DELETE" URI="/api/v1/namespaces/performance-citus/pods/mirror-citus-shard0-0" latency="23.911565ms" userAgent="kube-controller-manager/v1.29.6 (linux/amd64) kubernetes/7466e0b/system:serviceaccount:kube-system:node-controller" audit-ID="48eb34a8-c904-452c-9d24-e265c5b9b555" srcIP="172.16.0.4:44026" apf_pl="workload-high" apf_fs="kube-system-service-accounts" apf_iseats=1 apf_fseats=2 apf_additionalLatency="5ms" apf_execution_time="21.683835ms" resp=200
```

The kublet started back up
```
DEFAULT 2024-07-23T02:40:46.695093Z I0723 02:40:46.695068 1907 server.go:1256] "Started kubelet"
```

And then got the following events:
```
DEFAULT 2024-07-23T02:41:29.716685Z I0723 02:41:29.716630 1907 util.go:48] "No ready sandbox for pod can be found. Need to start a new one" pod="performance-citus/mirror-citus-shard0-0"
DEFAULT 2024-07-23T02:41:29.827338Z I0723 02:41:29.827304 1907 kubelet.go:2431] "SyncLoop DELETE" source="api" pods=["performance-citus/mirror-citus-shard0-0"]
INFO 2024-07-23T02:41:29.974546Z "Delete event for scheduled pod" pod="performance-citus/mirror-citus-shard0-0"
INFO 2024-07-23T02:41:29.973757Z "Delete event for scheduled pod" pod="performance-citus/mirror-citus-shard0-0"
INFO 2024-07-23T02:41:29.971246Z "Delete event for scheduled pod" pod="performance-citus/mirror-citus-shard0-0"
INFO 2024-07-23T02:41:33.490122527Z [background-id:f866480ddd35e7aa pod:performance-citus/mirror-citus-shard0-0] Pod not found
```

Also at this time, the csi driver (openebs zfs local pv) pod also restarted
```
DEFAULT 2024-07-23T02:40:59.843466Z I0723 02:40:59.843471 1907 kubelet.go:2447] "SyncLoop (PLEG): event for pod" pod="common/mirror-zfs-localpv-node-nlcwh" event={"ID":"565886d5-330a-4315-ab51-e4090e4faa81","Type":"ContainerDied","Data":"963386a20eb25eb4d10c9a35b20569b11c90d9da8902707a1b4dfaf4353964da"}
DEFAULT 2024-07-23T02:41:00.849481Z I0723 02:41:00.849355 1907 kubelet.go:2447] "SyncLoop (PLEG): event for pod" pod="common/mirror-zfs-localpv-node-nlcwh" event={"ID":"565886d5-330a-4315-ab51-e4090e4faa81","Type":"ContainerStarted","Data":"0bd24b519c769d49741aa3aa47946c26b3d75c5b536a9ec39a892aa16b0cba29"}
```

After the driver started, it attempted  to publish the node volume.
```
INFO 2024-07-23T02:41:20.552995067Z GRPC call: /csi.v1.Node/NodePublishVolume requests {"target_path":"/var/lib/kubelet/pods/f3e6dda8-df2a-464a-9b77-f72225bcbd52/volumes/kubernetes.io~csi/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c/mount","volume_capability":{"AccessType":{"Mount":{"fs_type":"zfs"}},"access_mode":{"mode":1}},"volume_context":{"openebs.io/cas-type":"localpv-zfs","openebs.io/poolname":"zfspv-pool","storage.kubernetes.io/csiProvisionerIdentity":"1711133401917-1665-zfs.csi.openebs.io"},"volume_id":"pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c"}
ERROR 2024-07-23T02:41:20.565802524Z GRPC error: rpc error: code = Internal desc = zfs get mountpoint failed, cannot open 'zfspv-pool/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c': dataset does not exist
ERROR 2024-07-23T02:41:20.565768826Z zfs: could not get mountpoint on dataset zfspv-pool/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c cmd [get -pH -o value mountpoint zfspv-pool/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c] error: cannot open 'zfspv-pool/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c': dataset does not exist
```

This publish failed but the kubelet logged a successful mount:
```
DEFAULT 2024-07-23T02:41:20.549581Z I0723 02:41:20.549480 1907 operation_generator.go:664] "MountVolume.MountDevice succeeded for volume \"pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c\" (UniqueName: \"kubernetes.io/csi/zfs.csi.openebs.io^pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c\") pod \"mirror-citus-shard0-0\" (UID: \"f3e6dda8-df2a-464a-9b77-f72225bcbd52\") device mount path \"/var/lib/kubelet/plugins/kubernetes.io/csi/zfs.csi.openebs.io/ba33a2244c475d064d33ba6b1934dd720ee89fd2e931120ee8ee6af1e612d16d/globalmount\"" pod="performance-citus/mirror-citus-shard0-0"
```

A few milliseconds later I see this from the kubet logs
```
DEFAULT 2024-07-23T02:41:20.566467Z E0723 02:41:20.566432 1907 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/zfs.csi.openebs.io^pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c podName: nodeName:}" failed. No retries permitted until 2024-07-23 02:41:52.566409131 +0000 UTC m=+65.995713484 (durationBeforeRetry 32s). Error: MountVolume.SetUp failed for volume "pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c" (UniqueName: "kubernetes.io/csi/zfs.csi.openebs.io^pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c") pod "mirror-citus-shard0-0" (UID: "f3e6dda8-df2a-464a-9b77-f72225bcbd52") : rpc error: code = Internal desc = zfs get mountpoint failed, cannot open 'zfspv-pool/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c': dataset does not exist
```

The zfs csi driver had not yet synced the dataset but did so here
```
INFO 2024-07-23T02:42:22.346842379Z Successfully synced 'common/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c'
```

However, the mount request has already happened and also the pod was attempted to be deleted above and stuck because the volume couldn't unmount. This log has been flooding logs ever since
```
E0723 02:41:30.012618    1907 reconciler_common.go:169] "operationExecutor.UnmountVolume failed (controllerAttachDetachEnabled true) for volume \"pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c\" (UniqueName: \"kubernetes.io/csi/zfs.csi.openebs.io^pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c\") pod \"f3e6dda8-df2a-464a-9b77-f72225bcbd52\" (UID: \"f3e6dda8-df2a-464a-9b77-f72225bcbd52\") : UnmountVolume.NewUnmounter failed for volume \"pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c\" (UniqueName: \"kubernetes.io/csi/zfs.csi.openebs.io^pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c\") pod \"f3e6dda8-df2a-464a-9b77-f72225bcbd52\" (UID: \"f3e6dda8-df2a-464a-9b77-f72225bcbd52\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/f3e6dda8-df2a-464a-9b77-f72225bcbd52/volumes/kubernetes.io~csi/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/f3e6dda8-df2a-464a-9b77-f72225bcbd52/volumes/kubernetes.io~csi/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c/vol_data.json]: open /var/lib/kubelet/pods/f3e6dda8-df2a-464a-9b77-f72225bcbd52/volumes/kubernetes.io~csi/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c/vol_data.json: no such file or directory" err="UnmountVolume.NewUnmounter failed for volume \"pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c\" (UniqueName: \"kubernetes.io/csi/zfs.csi.openebs.io^pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c\") pod \"f3e6dda8-df2a-464a-9b77-f72225bcbd52\" (UID: \"f3e6dda8-df2a-464a-9b77-f72225bcbd52\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/f3e6dda8-df2a-464a-9b77-f72225bcbd52/volumes/kubernetes.io~csi/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/f3e6dda8-df2a-464a-9b77-f72225bcbd52/volumes/kubernetes.io~csi/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c/vol_data.json]: open /var/lib/kubelet/pods/f3e6dda8-df2a-464a-9b77-f72225bcbd52/volumes/kubernetes.io~csi/pvc-09c95a39-cf08-4035-9f2e-2bef87ec738c/vol_data.json: no such file or directory"
```


#### What did you expect to happen?

The volume to be unmounted or if not present/already unmounted, the failed unmount that will never succeed (due to the missing file) should not block the pod from terminating

#### How can we reproduce it (as minimally and precisely as possible)?

Don't really have steps to reproduce

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.29.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.6-gke.1038001
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd github.com/containerd/containerd 1.7.13-0ubuntu0~22.04.1~gke1
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico, openebs zfs local-pv
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ç»è¿‡åˆ†æï¼Œè¯¥Issueæè¿°äº†åœ¨GKEé›†ç¾¤ä¸­ï¼Œä¸€ä¸ªèŠ‚ç‚¹ç”±äºè´Ÿè½½è¿‡é‡å¯¼è‡´kubeletå’ŒCNIã€CSIç­‰ç»„ä»¶å´©æºƒï¼Œé‡å¯åå‡ºç°äº†Podæ— æ³•åˆ é™¤ï¼Œå¡åœ¨TerminatingçŠ¶æ€çš„é—®é¢˜ã€‚æ—¥å¿—æ˜¾ç¤ºå·çš„å¸è½½å¤±è´¥ï¼Œå¯¼è‡´Podæ— æ³•æ­£å¸¸ç»ˆæ­¢ã€‚

è¿™ä¸ªé—®é¢˜æ˜¯ç”±äºç³»ç»Ÿåœ¨é«˜è´Ÿè½½æƒ…å†µä¸‹ï¼Œç»„ä»¶é‡å¯å¯¼è‡´èµ„æºæœªèƒ½æ­£ç¡®æ¸…ç†ï¼Œå¼•èµ·äº†Podæ— æ³•åˆ é™¤çš„ç°è±¡ã€‚æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œæ²¡æœ‰æ¶‰åŠåˆ°ä»»ä½•å®‰å…¨é£é™©ï¼Œæ²¡æœ‰è¯æ®è¡¨æ˜æ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ­¤é—®é¢˜è¿›è¡Œæ”»å‡»ï¼Œä¹Ÿæ²¡æœ‰æ¶‰åŠæƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰é«˜é£é™©å®‰å…¨é—®é¢˜ã€‚

å› æ­¤ï¼Œç»¼åˆåˆ¤æ–­ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126598 Unable to access control plane after Kubernetes worker node reboot

- Issue é“¾æ¥ï¼š[#126598](https://github.com/kubernetes/kubernetes/issues/126598)

### Issue å†…å®¹

#### What happened?

After running the kubectl drain master-1 --ignore-daemonsets command, I rebooted the node. (The node's name is master-1, but it's actually a worker node.) After rebooting, I ran kubectl describe node master-1 and was able to confirm the following.

```
NetworkUnavailable False Thu, 08 Aug 2024 13:03:33 +0900 Thu, 08 Aug 2024 13:03:33 +0900 CalicoIsUp Calico is running on this node
MemoryPressure Unknown Fri, 09 Aug 2024 00:24:51 +0900 Fri, 09 Aug 2024 00:28:34 +0900 NodeStatusUnknown Kubelet stopped posting node status. DiskPressure Unknown Fri, 09 Aug 2024 00:24:51 +0900 Fri, 09 Aug 2024 00:28:34 +0900 NodeStatusUnknown Kubelet stopped posting node status.
 PIDPressure Unknown Fri, 09 Aug 2024 00:24:51 +0900 Fri, 09 Aug 2024 00:28:34 +0900 NodeStatusUnknown Kubelet stopped posting node status.
 Ready Unknown Fri, 09 Aug 2024 00:24:51 +0900 Fri, 09 Aug 2024 00:28:34 +0900 NodeStatusUnknown Kubelet stopped posting node status.

```
After that, I tried sudo service kubelet restart, but the status above did not change, so I ran journalctl -xeu kubelet and was able to see the following log.

`Aug 09 00:53:43 master-1 kubelet[4037]: E0809 00:53:43.977693 4037 event.go:368] "Unable to write event (may retry after sleeping)" err="Post \"https://172.30.1.25:6443/api/v1/namespaces/default/events\": dial tc p 172.30.1.25:6443: connect: connection refused" event="&Event{ObjectMeta:{master-1.17e9cb4b4ae2670b default 0 0001-01-01 00:00:00 +0000 UTC <> Aug 09 00:53:44 master-1 kubelet[4037]: W0809 00:53:44.689016 4037 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "https://172.30.1.25:6443/api/v1/services?limit=500&resourceVersion=0": dial tcp 172.30.1.2 5:6443: connect: connection refused Aug 09 00:53:44 master-1 kubelet[4037]: E0809 00:53:44.689188 4037 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://172.30.1.25:6443/api/v1/services?limit=500&resourceVersion=0": dial tcp 172.30.1.25:6443: connect: connection refused Aug 09 00:53:46 master-1 kubelet[4037]: E0809 00:53:46.719516 4037 eviction_manager.go:282] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"master-1\" not found" Aug 09 00:53:49 master-1 kubelet[4037]: E0809 00:53:49.431097 4037 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://172.30.1.25:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/master-1?timeout=10s\": dial tcp 172.30.1.25:6443: connect: connection refused" interval="7s" Aug 09 00:53:49 master- 1 kubelet[4037]: I0809 00:53:49.796996 4037 kubelet_node_status.go:73] "Attempting to register node" node="master-1" Aug 09 00:53:49 master-1 kubelet[4037]: E0809 00:53:49.800718 4037 kubelet_node_status.go:96] "Unable to register node with API server" err="Post \"https://172.30.1.25:6443/api/v1/nodes\": dial tcp 172.30.1.25:6443: connect: connection refused" node="master-1"
`
The strange thing is that when I ran curl https://172.30.1.25:6443 --insecure on other worker nodes and the master node, I got a 403 error, but when I ran the above command on the node that had been rebooted, I got the following output.

`curl: (7) Failed to connect to 172.30.1.25 port 6443 after 0 ms: Couldn't connect to server`

The actual master node ip is 172.30.1.4, and 172.30.1.25 is an address exposed through HAProxy and Keepalived on a separate load balancer node. Could this be the issue?
Or did I do something wrong during the node reboot process?

#### What did you expect to happen?

When this happened previously, the solution was to reset the entire cluster.

#### How can we reproduce it (as minimally and precisely as possible)?

sudo kubeadm init --pod-network-cidr=192.168.0.0/16 --upload-certs --control-plane-endpoint=172.30.1.25:6443
After running the above command, add the worker node using the kubeadm join command. After installing calico and metallb, the above-described thing happened.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.3
```

</details>


#### Cloud provider

<details>
self hosted ubuntu 24 server
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 24.04 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo
$ uname -a
Linux master-1 6.8.0-39-generic #39-Ubuntu SMP PREEMPT_DYNAMIC Fri Jul  5 21:49:14 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd containerd.io 1.7.19
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico v3.28.1
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ç”¨æˆ·åœ¨é‡å¯Kuberneteså·¥ä½œèŠ‚ç‚¹åï¼Œæ— æ³•è®¿é—®æ§åˆ¶å¹³é¢çš„é—®é¢˜ã€‚ä»æ—¥å¿—ä¿¡æ¯å’Œæè¿°æ¥çœ‹ï¼Œé—®é¢˜çš„åŸå› å¯èƒ½æ˜¯èŠ‚ç‚¹æ— æ³•è¿æ¥åˆ°APIæœåŠ¡å™¨ï¼Œå¯¼è‡´kubeletæ— æ³•æ­£å¸¸å·¥ä½œã€‚è¿™ä¼¼ä¹æ˜¯ç”±äºç½‘ç»œè¿æ¥æˆ–é…ç½®é—®é¢˜å¯¼è‡´çš„ã€‚æ²¡æœ‰ä»»ä½•è¿¹è±¡è¡¨æ˜å­˜åœ¨å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨æ¼æ´ï¼Œä¹Ÿæ²¡æœ‰æ¶‰åŠåˆ°æ•æ„Ÿä¿¡æ¯æ³„éœ²æˆ–é«˜é£é™©çš„å®‰å…¨é—®é¢˜ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126587 CVE PLACEHOLDER ISSUE

- Issue é“¾æ¥ï¼š[#126587](https://github.com/kubernetes/kubernetes/issues/126587)

### Issue å†…å®¹

/triage accepted
/lifecycle frozen
/area security
/kind bug
/committee security-response

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueå†…å®¹ï¼Œæ ‡é¢˜ä¸ºâ€œCVE PLACEHOLDER ISSUEâ€ï¼Œå†…å®¹ä»…åŒ…å«äº†ä¸€äº›æ ‡ç­¾æŒ‡ä»¤ï¼Œå¦‚`/triage accepted`ã€`/lifecycle frozen`ã€`/area security`ç­‰ã€‚è¿™äº›å†…å®¹æ²¡æœ‰æä¾›å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚æˆ–å®‰å…¨æ¼æ´çš„ä¿¡æ¯ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬6æ¡ï¼šâ€œå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠâ€ã€‚å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126584 When setting kubeReserved non-provided values should fallback to config

- Issue é“¾æ¥ï¼š[#126584](https://github.com/kubernetes/kubernetes/issues/126584)

### Issue å†…å®¹

#### What happened?

Kubelet fails to start due to invalid KubeReserved


#### What did you expect to happen?

cpu and ephemeral-storage to be configured for kubeReserved

#### How can we reproduce it (as minimally and precisely as possible)?

Start kubelet with KUBELET_EXTRA_ARGS='--kube-reserved="memory=1355Mi"' and  /etc/kubernetes/kubelet/kubelet-config.json  with kubeReserved set:
```
{
  "kind": "KubeletConfiguration",
  "apiVersion": "kubelet.config.k8s.io/v1beta1",
  "address": "0.0.0.0",
  "authentication": {
    "anonymous": {
      "enabled": false
    },
    "webhook": {
      "cacheTTL": "2m0s",
      "enabled": true
    },
    "x509": {
      "clientCAFile": "/etc/kubernetes/pki/ca.crt"
    }
  },
  "authorization": {
    "mode": "Webhook",
    "webhook": {
      "cacheAuthorizedTTL": "5m0s",
      "cacheUnauthorizedTTL": "30s"
    }
  },
  "clusterDomain": "cluster.local",
  "hairpinMode": "hairpin-veth",
  "cgroupDriver": "cgroupfs",
  "cgroupRoot": "/",
  "featureGates": {
    "RotateKubeletServerCertificate": true
  },
  "serializeImagePulls": false,
  "serverTLSBootstrap": true,
  "tlsCipherSuites": [
    "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256",
    "TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256",
    "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305",
    "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384",
    "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305",
    "TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384",
    "TLS_RSA_WITH_AES_256_GCM_SHA384",
    "TLS_RSA_WITH_AES_128_GCM_SHA256"
  ],
  "evictionHard": {
    "memory.available": "100Mi",
    "nodefs.available": "10%",
    "nodefs.inodesFree": "5%"
  },
  "kubeReserved": {
    "cpu": "90m",
    "ephemeral-storage": "1Gi",
    "memory": "893Mi"
  }
}
```

#### Anything else we need to know?

Via cli flag and KUBELET_EXTRA_ARGS I'm setting KubeReserved memory and would like cpu and ephemeral-storage to be read from config file.

#### Kubernetes version

<details>

```console
$ kubectl version
1.28
```

</details>


#### Cloud provider

AWS EKS


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueå†…å®¹ï¼Œè¯¥é—®é¢˜æ¶‰åŠåˆ°Kubeletåœ¨é…ç½®`kubeReserved`æ—¶ï¼Œç”±äºå‘½ä»¤è¡Œå‚æ•°å’Œé…ç½®æ–‡ä»¶çš„è®¾ç½®å¯¼è‡´æ— æ³•å¯åŠ¨ã€‚å…·ä½“æ¥è¯´ï¼Œç”¨æˆ·é€šè¿‡å‘½ä»¤è¡Œå‚æ•°è®¾ç½®äº†`kubeReserved`çš„å†…å­˜ï¼ˆ`memory=1355Mi`ï¼‰ï¼Œå¹¶å¸Œæœ›CPUå’Œä¸´æ—¶å­˜å‚¨ï¼ˆ`ephemeral-storage`ï¼‰çš„é…ç½®èƒ½å¤Ÿä»é…ç½®æ–‡ä»¶ä¸­è¯»å–ã€‚ä½†è¿™ç§éƒ¨åˆ†é€šè¿‡å‘½ä»¤è¡Œå‚æ•°ï¼Œéƒ¨åˆ†é€šè¿‡é…ç½®æ–‡ä»¶çš„æ··åˆé…ç½®æ–¹å¼å¯¼è‡´äº†Kubeletå¯åŠ¨å¤±è´¥ã€‚

æ­¤é—®é¢˜å±äºé…ç½®ä¸å½“å¼•èµ·çš„åŠŸèƒ½æ€§é”™è¯¯ï¼Œå¹¶æœªæ¶‰åŠä»»ä½•å®‰å…¨æ¼æ´æˆ–å®‰å…¨é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬6æ¡ï¼šâ€œå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠâ€ã€‚å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126582 Using runtime scheme.Convert for Validating Admission Policy version conversion results in error

- Issue é“¾æ¥ï¼š[#126582](https://github.com/kubernetes/kubernetes/issues/126582)

### Issue å†…å®¹

#### What happened?

Converting `v1beta1.ValidatingAdmissionPolicy` to `v1.ValidatingAdmissionPolicy` and `v1beta1.ValidatingAdmissionPolicyBinding` to `v1.ValidatingAdmissionPolicyBinding` using [runtime#Scheme.Convert](https://pkg.go.dev/k8s.io/apimachinery@v0.30.3/pkg/runtime#Scheme.Convert), results below errors - 

- `converting (v1beta1.ValidatingAdmissionPolicy) to (v1.ValidatingAdmissionPolicy): unknown conversion`

- `converting (v1beta1.ValidatingAdmissionPolicyBinding) to (v1.ValidatingAdmissionPolicyBinding): unknown conversion`

#### What did you expect to happen?

`scheme.Convert` should convert the validating admission policy and validating admission policy binding resource to respective `v1` versions.

#### How can we reproduce it (as minimally and precisely as possible)?

- Construct a `v1beta1` versioned ValidatingAdmissionPolicy or ValidatingAdmissionPolicyBinding object through go-client.
- Use `runtime#Scheme.Convert()` to convert the object to `v1` versioned Validating Admission Policy.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Ubuntu"
VERSION="20.04.6 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.6 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal

$ uname -a
# paste output here
Linux <user-name> 5.15.0-1068-azure #77~20.04.1-Ubuntu SMP Fri Jun 21 22:05:38 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ç»è¿‡å¯¹Issueçš„åˆ†æï¼Œè¯¥é—®é¢˜æè¿°äº†åœ¨ä½¿ç”¨`runtime#Scheme.Convert`å°†`v1beta1.ValidatingAdmissionPolicy`è½¬æ¢ä¸º`v1.ValidatingAdmissionPolicy`ä»¥åŠå°†`v1beta1.ValidatingAdmissionPolicyBinding`è½¬æ¢ä¸º`v1.ValidatingAdmissionPolicyBinding`æ—¶ï¼Œå‡ºç°äº†`unknown conversion`çš„é”™è¯¯ã€‚è¿™æ˜¯ä¸€ä¸ªæ¶‰åŠKubernetes APIç‰ˆæœ¬è½¬æ¢çš„é—®é¢˜ï¼Œå±äºåŠŸèƒ½æ€§é”™è¯¯ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šè¯¥é—®é¢˜å¹¶ä¸ä¼šè¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œå› ä¸ºå®ƒä»…æ¶‰åŠåˆ°å¼€å‘è€…åœ¨ä½¿ç”¨APIæ—¶é‡åˆ°çš„ç‰ˆæœ¬è½¬æ¢é”™è¯¯ã€‚

2. **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä½¿ç”¨CVSS 3.1è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœè¦åœ¨highä»¥ä¸Š**ï¼šè¯¥é—®é¢˜ä¸æ¶‰åŠå®‰å…¨æ¼æ´ï¼Œä¸ä¼šè¢«åˆ†é…CVEç¼–å·ï¼Œé£é™©è¯„çº§ä¹Ÿä¸ä¼šè¾¾åˆ°highä»¥ä¸Šã€‚

6. **å¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠ**ï¼šæ­¤Issueå±äºåŠŸèƒ½æ€§é—®é¢˜ï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126578 Still seeing the issue for endpoints staying out of sync

- Issue é“¾æ¥ï¼š[#126578](https://github.com/kubernetes/kubernetes/issues/126578)

### Issue å†…å®¹

#### What happened?

This issue https://github.com/kubernetes/kubernetes/issues/125638 was supposed to have fixed the issue where endpoint stay out of sync 
```
I0807 14:01:51.613700       2 endpoints_controller.go:348] "Error syncing endpoints, retrying" service="test1/test-qa" err="endpoints informer cache is out of date, resource version 10168236546 already processed for endpoints test1/test-qa"
I0807 14:01:51.624576       2 endpoints_controller.go:348] "Error syncing endpoints, retrying" service="test1/test-qa" err="endpoints informer cache is out of date, resource version 10168236546 already processed for endpoints test1/test-qa"
I0807 14:01:51.645704       2 endpoints_controller.go:348] "Error syncing endpoints, retrying" service="test1/test-qa" err="endpoints informer cache is out of date, resource version 10168236546 already processed for endpoints test1/test-qa"
I0807 14:01:51.686942       2 endpoints_controller.go:348] "Error syncing endpoints, retrying" service="test1/test-qa" err="endpoints informer cache is out of date, resource version 10168236546 already processed for endpoints test1/test-qa"
I0807 14:01:51.768648       2 endpoints_controller.go:348] "Error syncing endpoints, retrying" service="test1/test-qa" err="endpoints informer cache is out of date, resource version 10168236546 already processed for endpoints test1/test-qa"
I0807 14:01:51.808043       2 endpoints_controller.go:348] "Error syncing endpoints, retrying" service="test1/test2-qa" err="endpoints informer cache is out of date, resource version 10168250766 already processed for endpoints test1/test2-qa"
I0807 14:01:51.930345       2 endpoints_controller.go:348] "Error syncing endpoints, retrying" service="test1/test-qa" err="endpoints informer cache is out of date, resource version 10168236546 already processed for endpoints test1/test-qa"
```
I also wrote a small script which would get me the out of sync endpoints compared to the endpointslices 
```
from kubernetes.client import CoreV1Api, DiscoveryV1Api
from hubspot_kube_utils.client import build_kube_client
import json
import os
from datetime import datetime

def extract_ips_from_endpoint(endpoint):
    ips = set()
    if endpoint.subsets:
        for subset in endpoint.subsets:
            if subset.addresses:
                ips.update(addr.ip for addr in subset.addresses)
            if subset.not_ready_addresses:
                ips.update(addr.ip for addr in subset.not_ready_addresses)
    return ips

def extract_ips_from_endpoint_slice(slice):
    if not slice.endpoints:
        return set()
    return set(address for endpoint in slice.endpoints
               for address in (endpoint.addresses or []))

def compare_endpoints_and_slices(core_client, discovery_client):
    all_mismatches = []

    try:
        namespaces = core_client.list_namespace()
    except Exception as e:
        print(f"Error listing namespaces: {e}")
        return all_mismatches

    for ns in namespaces.items:
        namespace = ns.metadata.name
        print(f"Processing namespace: {namespace}")

        try:
            endpoints = core_client.list_namespaced_endpoints(namespace)
        except Exception as e:
            print(f"Error listing endpoints in namespace {namespace}: {e}")
            continue

        for endpoint in endpoints.items:
            name = endpoint.metadata.name

            try:
                slices = discovery_client.list_namespaced_endpoint_slice(namespace, label_selector=f"kubernetes.io/service-name={name}")
            except Exception as e:
                print(f"Error listing endpoint slices for service {name} in namespace {namespace}: {e}")
                continue

            endpoint_ips = extract_ips_from_endpoint(endpoint)
            slice_ips = set()

            for slice in slices.items:
                slice_ips.update(extract_ips_from_endpoint_slice(slice))

            if endpoint_ips != slice_ips:
                mismatch = {
                    "namespace": namespace,
                    "service_name": name,
                    "endpoint_ips": list(endpoint_ips),
                    "slice_ips": list(slice_ips),
                    "missing_in_endpoint": list(slice_ips - endpoint_ips),
                    "missing_in_slice": list(endpoint_ips - slice_ips)
                }
                all_mismatches.append(mismatch)

        print(f"Completed processing namespace: {namespace}")
        print("---")

    return all_mismatches

def save_to_json(data, cluster_name):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{cluster_name}_mismatches_{timestamp}.json"

    with open(filename, 'w') as f:
        json.dump(data, f, indent=2)

    print(f"Mismatch data for cluster {cluster_name} saved to {filename}")

def main():
    clusters = ["test"]
    all_cluster_mismatches = {}

    for cluster_name in clusters:
        print(f"Processing cluster: {cluster_name}")

        try:
            kube_client = build_kube_client(host="TEST",
                              token="TOKEN")

            core_client = CoreV1Api(kube_client)
            discovery_client = DiscoveryV1Api(kube_client)

            mismatches = compare_endpoints_and_slices(core_client, discovery_client)

            all_cluster_mismatches[cluster_name] = mismatches

            save_to_json(mismatches, cluster_name)

            print(f"Completed processing cluster: {cluster_name}")
            print(f"Total mismatches found in this cluster: {len(mismatches)}")
        except Exception as e:
            print(f"Error processing cluster {cluster_name}: {e}")


if __name__ == "__main__":
    main()
```

#### What did you expect to happen?

I expect the endpoints to eventually sync and reflect the most upto date information. 

#### How can we reproduce it (as minimally and precisely as possible)?

I have just deployed the newer patch to our cluster and that has resulted in endpoints never ending up being updated if the status goes out of sync. 

#### Anything else we need to know?

_No response_

#### Kubernetes version

Client Version: v1.29.7
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.7


#### Cloud provider

<details>

</details>


#### OS version

almalinux-9

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

cri-o

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueä¸»è¦æè¿°äº†Kubernetesé›†ç¾¤ä¸­endpointsä¸endpointslicesä¸åŒæ­¥çš„é—®é¢˜ï¼Œå¯¼è‡´endpointsæ— æ³•æ­£ç¡®æ›´æ–°ã€‚æä¾›çš„æ—¥å¿—ä¿¡æ¯å’ŒPythonè„šæœ¬ç”¨äºè¯Šæ–­è¿™ä¸ªåŒæ­¥é—®é¢˜ã€‚

æ ¹æ®æä¾›çš„å†…å®¹ï¼Œæ²¡æœ‰æ¶‰åŠä»»ä½•å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨é£é™©ã€‚è„šæœ¬ä¸­ä½¿ç”¨çš„ä¸»æœºåœ°å€å’Œä»¤ç‰Œï¼ˆhost="TEST"ï¼Œtoken="TOKEN"ï¼‰éƒ½æ˜¯å ä½ç¬¦ï¼Œå¹¶æœªæ³„éœ²çœŸå®çš„æ•æ„Ÿä¿¡æ¯ã€‚

æŒ‰ç…§é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ã€‚

---

## Issue #126572 PreStop don't work if hostNetwork set to true

- Issue é“¾æ¥ï¼š[#126572](https://github.com/kubernetes/kubernetes/issues/126572)

### Issue å†…å®¹

#### What happened?

`lifecycle.preStop.httpGet` don't work if `hostNetwork` set to `true`

#### What did you expect to happen?

`lifecycle.preStop.httpGet` worked if `hostNetwork` set to `true`

#### How can we reproduce it (as minimally and precisely as possible)?

I have simple code on Golang for test this issue - https://github.com/zvlb/k8s-prestop-check (simple HTTP server with 1 url `/sleep` - for sleeping 15 second and SIGTERM handler)
And image - `zvlb/k8s-prestop-check:v0.0.1`

#### First example

If deploy this image with simple deployment
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8s-prestop-check
  labels:
    app: k8s-prestop-check
spec:
  replicas: 1
  selector:
    matchLabels:
      app: k8s-prestop-check
  template:
    metadata:
      labels:
        app: k8s-prestop-check
    spec:
      containers:
      - name: k8s-prestop-check
        image: zvlb/k8s-prestop-check:v0.0.1
        lifecycle:
          preStop:
            httpGet:
              path: /sleep
              port: 8080
              scheme: HTTP
```

And delete pod after creating. I see in logs how `lifecycle.preStop` get `/sleep` request and pod deleted after it:

```
â¯ k logs k8s-prestop-check-5fcf5f7cc7-lx7ht -f
1723027959. awaiting signal   
1723029549. Sleep request run                # DELETE POD HERE 
1723029564. Sleep request done

1723029564. terminated
1723029564. exiting
```

All work good

#### Second example

If deploy this image with deployment with hostNetwork
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8s-prestop-check-hostnetwork
  labels:
    app: k8s-prestop-check-hostnetwork
spec:
  replicas: 1
  selector:
    matchLabels:
      app: k8s-prestop-check-hostnetwork
  template:
    metadata:
      labels:
        app: k8s-prestop-check-hostnetwork
    spec:
      containers:
      - name: k8s-prestop-check
        image: zvlb/k8s-prestop-check:v0.0.1
        lifecycle:
          preStop:
            httpGet:
              path: /sleep
              port: 8080
              scheme: HTTP
      hostNetwork: true
```

And delete pod after creating. I see in logs how `lifecycle.preStop` get `/sleep` request and pod deleted after it:

```
â¯ k logs k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2 -f
1723028763. awaiting signal

1723029866. terminated                # DELETE POD HERE 
1723029866. exiting
```

In kubelet logs:
```json
{
    "ts": 1723030170266.806,
    "caller": "kuberuntime/kuberuntime_container.go:645",
    "msg": "Running preStop hook",
    "v": 3,
    "pod": {
        "name": "k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2",
    },
    "podUID": "fb0d78fd-645c-4c88-86d8-a528ce55e4ed",
    "containerName": "k8s-prestop-check",
    "containerID": "containerd://5f1a0ae6fae3957d8ed6d7a2083a00a3819a905e77f49647b7707cbeb1934e80"
}
{
    "ts": 1723030170266.92,
    "caller": "record/event.go:376",
    "msg": "Event occurred",
    "v": 3,
    "object": {
        "name": "k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2",
    },
    "fieldPath": "spec.containers{k8s-prestop-check}",
    "kind": "Pod",
    "apiVersion": "v1",
    "type": "Normal",
    "reason": "Killing",
    "message": "Stopping container k8s-prestop-check"
}
{
    "ts": 1723030170268.5867,
    "caller": "lifecycle/handlers.go:87",
    "msg": "HTTP lifecycle hook for Container in Pod failed",
    "path": "/sleep",
    "containerName": "k8s-prestop-check",
    "pod": {
        "name": "k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2",
    },
    "err": "failed to find networking container: &{fb0d78fd-645c-4c88-86d8-a528ce55e4ed k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2 [] [0xc004b0c200] [&PodSandboxStatus{Id:7299112c0f8e5033f356bf04836221d039b0aab69907c2e3b9bf4fea4caf30ff,Metadata:&PodSandboxMetadata{Name:k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2,Uid:fb0d78fd-645c-4c88-86d8-a528ce55e4ed,Namespace:default,Attempt:0,},State:SANDBOX_READY,CreatedAt:1723029867435367792,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{app: k8s-prestop-check-hostnetwork,io.kubernetes.pod.name: k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2,io.kubernetes.pod.namespace: default,io.kubernetes.pod.uid: fb0d78fd-645c-4c88-86d8-a528ce55e4ed,pod-template-hash: 5b94b797bc,},Annotations:map[string]string{kubernetes.io/config.seen: 2024-08-07T14:24:27.123470171+03:00,kubernetes.io/config.source: api,kubernetes.io/limit-ranger: LimitRanger plugin set: cpu, memory request for container k8s-prestop-check; cpu, memory limit for container k8s-prestop-check,},RuntimeHandler:,}] 0001-01-01 00:00:00 +0000 UTC}"
}
{
    "ts": 1723030170268.621,
    "caller": "kuberuntime/kuberuntime_container.go:653",
    "msg": "PreStop hook failed",
    "pod": {
        "name": "k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2",
    },
    "podUID": "fb0d78fd-645c-4c88-86d8-a528ce55e4ed",
    "containerName": "k8s-prestop-check",
    "containerID": "containerd://5f1a0ae6fae3957d8ed6d7a2083a00a3819a905e77f49647b7707cbeb1934e80",
    "err": "failed to find networking container: &{fb0d78fd-645c-4c88-86d8-a528ce55e4ed k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2 default [] [0xc004b0c200] [&PodSandboxStatus{Id:7299112c0f8e5033f356bf04836221d039b0aab69907c2e3b9bf4fea4caf30ff,Metadata:&PodSandboxMetadata{Name:k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2,Uid:fb0d78fd-645c-4c88-86d8-a528ce55e4ed,Namespace:default,Attempt:0,},State:SANDBOX_READY,CreatedAt:1723029867435367792,Network:&PodSandboxNetworkStatus{Ip:,AdditionalIps:[]*PodIP{},},Linux:&LinuxPodSandboxStatus{Namespaces:&Namespace{Options:&NamespaceOption{Network:NODE,Pid:CONTAINER,Ipc:POD,TargetId:,UsernsOptions:nil,},},},Labels:map[string]string{app: k8s-prestop-check-hostnetwork,io.kubernetes.pod.name: k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2,io.kubernetes.pod.namespace: default,io.kubernetes.pod.uid: fb0d78fd-645c-4c88-86d8-a528ce55e4ed,pod-template-hash: 5b94b797bc,},Annotations:map[string]string{kubernetes.io/config.seen: 2024-08-07T14:24:27.123470171+03:00,kubernetes.io/config.source: api,kubernetes.io/limit-ranger: LimitRanger plugin set: cpu, memory request for container k8s-prestop-check; cpu, memory limit for container k8s-prestop-check,},RuntimeHandler:,}] 0001-01-01 00:00:00 +0000 UTC}"
}
{
    "ts": 1723030170268.6584,
    "caller": "kuberuntime/kuberuntime_container.go:665",
    "msg": "PreStop hook completed",
    "v": 3,
    "pod": {
        "name": "k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2",
    },
    "podUID": "fb0d78fd-645c-4c88-86d8-a528ce55e4ed",
    "containerName": "k8s-prestop-check",
    "containerID": "containerd://5f1a0ae6fae3957d8ed6d7a2083a00a3819a905e77f49647b7707cbeb1934e80"
}
{
    "ts": 1723030170268.67,
    "caller": "record/event.go:376",
    "msg": "Event occurred",
    "v": 3,
    "object": {
        "name": "k8s-prestop-check-hostnetwork-5b94b797bc-rs6z2",
    },
    "fieldPath": "spec.containers{k8s-prestop-check}",
    "kind": "Pod",
    "apiVersion": "v1",
    "type": "Warning",
    "reason": "FailedPreStopHook",
    "message": "PreStopHook failed"
}
```


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.5
```

</details>


#### Cloud provider

<details>
baremetal
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetesä¸­ï¼Œå½“Podè®¾ç½®äº†`hostNetwork: true`æ—¶ï¼Œç”Ÿå‘½å‘¨æœŸé’©å­`preStop`çš„`httpGet`æ–¹å¼æ— æ³•æ­£å¸¸å·¥ä½œï¼Œå¹¶æä¾›äº†è¯¦ç»†çš„å¤ç°æ­¥éª¤å’Œæ—¥å¿—ã€‚è¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ç¼ºé™·æˆ–è€…Bugï¼Œå¯¼è‡´é¢„æœŸçš„`preStop`é’©å­æœªè¢«æ‰§è¡Œã€‚ä½†æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥é—®é¢˜å¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

æ ¹æ®æ ‡å‡†ï¼š

- **ç¬¬6æ¡**ï¼šå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

è¯¥é—®é¢˜å¹¶æœªå¯¼è‡´æƒé™æå‡ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€è¿œç¨‹ä»£ç æ‰§è¡Œç­‰å®‰å…¨é£é™©ã€‚å®ƒä¸ä¼šè¢«æ”»å‡»è€…åˆ©ç”¨æ¥ç ´åç³»ç»Ÿçš„å®‰å…¨æ€§ï¼Œæ•…è¯„ä¼°ä¸ºä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126682 Container network metrics missing with kubelet 1.30 and crio-1.30

- Issue é“¾æ¥ï¼š[#126682](https://github.com/kubernetes/kubernetes/issues/126682)

### Issue å†…å®¹

#### What happened?

Upgraded kubernetes cluster from 1.29 to 1.30, and container network metrics disappeared.

#### What did you expect to happen?

Expected metrics to be available as before.

#### How can we reproduce it (as minimally and precisely as possible)?

Run crio-1.30 with default config, deploy kubernetes 1.30, deploy a pod, and collect cadvisor metrics. `container_network_receive_bytes_total` and other metrics are not reported for pod.

#### Anything else we need to know?

Seems that crio dropping infra containers and a change in cadvisor results in this misbehavior. https://github.com/google/cadvisor/issues/3577

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.2
WARNING: version difference between client (1.28) and server (1.30) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
Bare metal
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"
$ uname -a
Linux k8s-node01 5.15.162 #2 SMP Tue Jul 9 08:02:50 UTC 2024 aarch64 GNU/Linux
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
cri-o 1.30
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
kube-router v2.1.3
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®Issueæè¿°ï¼Œå‡çº§Kubernetesé›†ç¾¤åˆ°1.30ç‰ˆæœ¬åï¼Œå®¹å™¨çš„ç½‘ç»œæŒ‡æ ‡æ¶ˆå¤±äº†ã€‚è¿™æ˜¯ç”±äºcrioå’Œcadvisorçš„å˜åŒ–å¯¼è‡´çš„æŒ‡æ ‡æ”¶é›†é—®é¢˜ã€‚

è¯¥é—®é¢˜å±äºåŠŸèƒ½æ€§ç¼ºé™·ï¼Œå½±å“äº†è¿ç»´äººå‘˜å¯¹å®¹å™¨ç½‘ç»œæ€§èƒ½çš„ç›‘æ§å’Œç»Ÿè®¡ã€‚ä½†å®ƒå¹¶ä¸æ¶‰åŠä»»ä½•å®‰å…¨é£é™©ï¼Œæ”»å‡»è€…æ— æ³•åˆ©ç”¨è¯¥é—®é¢˜è¿›è¡Œæ”»å‡»ï¼Œä¹Ÿä¸ä¼šå¯¼è‡´æƒé™æå‡ã€æ•°æ®æ³„éœ²æˆ–æœåŠ¡æ‹’ç»ç­‰å®‰å…¨é—®é¢˜ã€‚

---

## Issue #126681 [kubelet] Pod is deleted due to Node not ready, deletions is cancelled, kubelet is not aware container is not running

- Issue é“¾æ¥ï¼š[#126681](https://github.com/kubernetes/kubernetes/issues/126681)

### Issue å†…å®¹

#### What happened?

We had following sequence of events:
* node was not ready
* pod was deleted becasue of it
* then deletion of the pod was cancelled

at the end:
* main container was terminated, and kubelet thinks it is running, but it is not. It tries to execute liveness and readiness check, but they are failing 
* pod stuck in terminating phase

#### What did you expect to happen?

* kubelet recognized that container is not running, it proceed with termination of pod

#### How can we reproduce it (as minimally and precisely as possible)?

My idea is to have:
* pod which terminates for a while
* put node in not ready state to trigger taint based eviction
* "fix" the node to cancel deletion

#### Anything else we need to know?

[slack discussion](https://kubernetes.slack.com/archives/C0BP8PW9G/p1722933365605479)

Logs:
```
2024-08-05 10:19:52	Cancelling deletion of Pod xxx
2024-08-05 10:47:03	Node is not ready
2024-08-05 10:47:34	Cancelling deletion of Pod xxx
2024-08-05 11:04:08	Node is not ready
2024-08-05 11:07:19	Cancelling deletion of Pod xxx
2024-08-05 11:08:59	Readiness probe failed: Get "http://10.184.23.109:15021/healthz/ready": dial tcp 10.184.23.109:15021: connect: connection refused
2024-08-05 11:09:54	Readiness probe errored: rpc error: code = Unknown desc = failed to exec in container: failed to start exec "8b251f765bc1008ac8d59927f78e3b962fb1c147135be96c3dc411cc5fdf2758": OCI runtime exec failed: exec failed: cannot exec in a stopped container: unknown
2024-08-05 11:09:54	Liveness probe errored: rpc error: code = Unknown desc = failed to exec in container: failed to start exec "50bd9e7d0c45c55025dd2a722b6f7bbe7bb7cad9dca49c394cc27c7efc990d72": OCI runtime exec failed: exec failed: cannot exec in a stopped container: unknown
2024-08-05 11:09:54	Readiness probe errored: rpc error: code = Unknown desc = failed to exec in container: failed to start exec "33016f203cc77f545ce131fb3be9bbc93e4031e14eb79ba303911f7269abd332": OCI runtime exec failed: exec failed: cannot exec in a stopped container: unknown
2024-08-05 11:09:54	Liveness probe errored: rpc error: code = Unknown desc = failed to exec in container: failed to start exec "73920379d5305b92e563104cab3ad96840f3f33fe80bfad420bc6c261507caa2": OCI runtime exec failed: exec failed: cannot exec in a stopped container: unknown
2024-08-05 11:10:03	Readiness probe errored: rpc error: code = Unknown desc = failed to exec in container: failed to start exec "33cc325ee13a9f3aee27f2d3ef5b0d5ead6989cb1410b65093272e7d616730ad": OCI runtime exec failed: exec failed: cannot exec in a stopped container: unknown
2024-08-05 11:10:13	Readiness probe errored: rpc error: code = Unknown desc = failed to exec in container: failed to start exec "caec0276c24dd30d29a0702f4c1f4ba9fedabc2aac6da7e51c1bd5c316e52711": OCI runtime exec failed: exec failed: cannot exec in a stopped container: unknown
2024-08-05 11:10:13	Liveness probe errored: rpc error: code = Unknown desc = failed to exec in container: failed to start exec "812a6251f5a1c620fea52c7295c73c3c9870a89c12fa2971798f8b6927b92613": OCI runtime exec failed: exec failed: cannot exec in a stopped container: unknown
2024-08-05 11:10:23	Readiness probe errored: rpc error: code = Unknown desc = failed to exec in container: failed to start exec "f548dc750d2782295a1a383ba8341f63897cbc98b068e529de1f950fe740a361": OCI runtime exec failed: exec failed: cannot exec in a stopped container: unknown
2024-08-05 11:10:34	(combined from similar events): Readiness probe errored: rpc error: code = Unknown desc = failed to exec in container: failed to start exec "3bcc62a49570d57b79b26b204cc19b32371439cd55907dcbc1b208d8f5765031": OCI runtime exec failed: exec failed: cannot exec in a stopped container: unknown
2024-08-05 19:32:42	error killing pod: [failed to "KillContainer" for "yyy" with KillContainerError: "rpc error: code = DeadlineExceeded desc = context deadline exceeded", failed to "KillPodSandbox" for "b23ad3e5-1d53-4999-9ca4-af928620bd10" with KillPodSandboxError: "rpc error: code = DeadlineExceeded desc = context deadline exceeded"]
2024-08-05 19:32:43	Stopping container yyy
2024-08-05 20:03:14	Readiness probe errored: rpc error: code = Unknown desc = failed to exec in container: failed to start exec "0f9320bc18d366466eaa289fd9d03bd6dddd124e42c17e4f4fcd889ee234309d": OCI runtime exec failed: exec failed: cannot exec in a stopped container: unknown
```



#### Kubernetes version

<details>

```console
1.27.9
```

</details>


#### Cloud provider

<details>
Azure/AKS
</details>


#### OS version

<details>

```console
Ubuntu 22.04.4 LTS
AKSUbuntu-2204gen2containerd-202407.03.0

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd://1.7.15-1

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ç‰¹å®šæƒ…å†µä¸‹ï¼Œkubeletæœªèƒ½æ­£ç¡®è¯†åˆ«å®¹å™¨å·²åœæ­¢è¿è¡Œï¼Œå¯¼è‡´Podå¡åœ¨terminatingçŠ¶æ€ã€‚å…·ä½“æƒ…å¢ƒæ˜¯èŠ‚ç‚¹å¤„äºNot ReadyçŠ¶æ€ï¼ŒPodè¢«åˆ é™¤ï¼Œä½†åˆ é™¤æ“ä½œåˆè¢«å–æ¶ˆï¼Œç»“æœå¯¼è‡´kubeletè®¤ä¸ºå®¹å™¨ä»åœ¨è¿è¡Œï¼Œè€Œå®é™…ä¸Šå®¹å™¨å·²ç»ˆæ­¢ã€‚

ç»è¿‡åˆ†æï¼Œè¿™å±äºkubeletçš„é€»è¾‘é”™è¯¯æˆ–çŠ¶æ€åŒæ­¥é—®é¢˜ï¼Œå¹¶æœªæ¶‰åŠä»»ä½•å®‰å…¨æ¼æ´ã€‚æ ¹æ®æä¾›çš„ä¿¡æ¯ï¼Œæ²¡æœ‰æ˜¾ç¤ºæ”»å‡»è€…èƒ½å¤Ÿåˆ©ç”¨è¯¥é—®é¢˜æ¥è·å¾—æœªç»æˆæƒçš„è®¿é—®ã€æ‰§è¡Œå‘½ä»¤ã€æå‡æƒé™æˆ–è¿›è¡Œæ‹’ç»æœåŠ¡æ”»å‡»ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ²¡æœ‰è¯æ®è¡¨æ˜æ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ­¤é—®é¢˜ã€‚
2. **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä½¿ç”¨CVSS 3.1è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœè¦åœ¨highä»¥ä¸Š**ï¼šæ­¤é—®é¢˜ä¸ç¬¦åˆã€‚
6. **å¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠ**ã€‚

å› æ­¤ï¼Œç»¼åˆä»¥ä¸Šåˆ†æï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126663 kube-apiserver and other components no longer honor --version build ID overrides

- Issue é“¾æ¥ï¼š[#126663](https://github.com/kubernetes/kubernetes/issues/126663)

### Issue å†…å®¹

#### What happened?

Starting in 1.31, --version no longer works to modify the build ID of the running components.

/kind regression
/priority important-soon


#### What did you expect to happen?

Running kube-apiserver v1.31.0 with `--version=v1.31.0-example.123` ignores the modified build ID and reports `v1.31.0`

In 1.28, 1.29, and 1.30 (since https://github.com/kubernetes/kubernetes/pull/117688), this allowed modifying the build ID of the running server.

/sig api-machinery
/assign @jpbetz @siyuanfoundation

#### How can we reproduce it (as minimally and precisely as possible)?

Run `kube-apiserver` v1.31.0 with `--version=v1.31.0-example.123`

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.31.0

#### Cloud provider

n/a

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueåæ˜ äº†kube-apiserveråœ¨1.31ç‰ˆæœ¬ä¸­ï¼Œ`--version`å‚æ•°ä¸å†èµ·ä½œç”¨ï¼Œæ— æ³•ä¿®æ”¹è¿è¡Œç»„ä»¶çš„æ„å»ºIDï¼Œè¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½ä¸Šçš„å›å½’é—®é¢˜ã€‚è¿™æ„å‘³ç€ç®¡ç†å‘˜æ— æ³•é€šè¿‡`--version`å‚æ•°æ¥è®¾ç½®è‡ªå®šä¹‰çš„ç‰ˆæœ¬å·ã€‚ç„¶è€Œï¼Œè¿™å¹¶ä¸æ¶‰åŠä»»ä½•å®‰å…¨é£é™©ã€‚

é¦–å…ˆï¼Œè¯¥é—®é¢˜å¹¶ä¸æ¶‰åŠä»»ä½•å¯ä»¥è¢«æ”»å‡»è€…åˆ©ç”¨çš„æ¼æ´ã€‚æ”»å‡»è€…æ— æ³•é€šè¿‡è¿™ä¸ªé—®é¢˜æ¥è·å–æœªæˆæƒçš„è®¿é—®ã€æå‡æƒé™ã€è¿œç¨‹ä»£ç æ‰§è¡Œæˆ–å…¶ä»–å®‰å…¨å¨èƒã€‚

å…¶æ¬¡ï¼Œè¯¥é—®é¢˜ä¸ä¼šå¯¼è‡´æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€æƒé™æå‡æˆ–å…¶ä»–å®‰å…¨ç›¸å…³çš„å½±å“ã€‚

å› æ­¤ï¼Œæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œé£é™©è¯„çº§åˆ¤æ–­ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

---

## Issue #126659 ResourceSliceList object has `listMeta` field instead of `metadata` field

- Issue é“¾æ¥ï¼š[#126659](https://github.com/kubernetes/kubernetes/issues/126659)

### Issue å†…å®¹

#### What happened?

Listing `v1alpha3.ResourceSlice`s on v1.31.0 returns an object with a `listMeta` field instead of the standard `metadata` field for (list) objects:

```sh
$ curl -kL --cert client.pem --key client.key.pem 'https://127.0.0.1:39987/apis/resource.k8s.io/v1alpha3/resourceslices'

{
  "kind": "ResourceSliceList",
  "apiVersion": "resource.k8s.io/v1alpha3",
  "listMeta": {
    "resourceVersion": "522"
  },
  "items": []
}
```

Listing `v1alpha2.ResourceSlice`s on v1.30.3 behaves correctly:

```sh
$ curl -kL --cert client.pem --key client.key.pem 'https://127.0.0.1:34519/apis/resource.k8s.io/v1alpha2/resourceslices'

{
  "kind": "ResourceSliceList",
  "apiVersion": "resource.k8s.io/v1alpha2",
  "metadata": {
    "resourceVersion": "422"
  },
  "items": []
}
```

This is also propagated to the Swagger spec (which is how I noticed it, since I maintain the Rust bindings for the client API). v1.31.0:

```json
    "io.k8s.api.resource.v1alpha3.ResourceSliceList": {
        "listMeta": {
          "$ref": "#/definitions/io.k8s.apimachinery.pkg.apis.meta.v1.ListMeta",
          "description": "Standard list metadata"
        }
```

v1.30.3:

```json
    "io.k8s.api.resource.v1alpha2.ResourceSliceList": {
        "metadata": {
          "$ref": "#/definitions/io.k8s.apimachinery.pkg.apis.meta.v1.ListMeta",
          "description": "Standard list metadata"
        }
```

#### What did you expect to happen?

As explained above.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Deploy v1.31.0 with `DynamicResourceAllocation` feature gate enabled and runtime config `resource.k8s.io/v1alpha3=true`. With kind, this is:

    ```yaml
    kind: Cluster
    apiVersion: kind.x-k8s.io/v1alpha4
    featureGates:
      DynamicResourceAllocation: true
    runtimeConfig:
      'resource.k8s.io/v1alpha3': 'true'
    ```

2. Perform the API server request as in the above curl command in any way you like as long as it lets you see the raw JSON response.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version

Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.0
```

</details>


#### Cloud provider

<details>
kind 0.23.0
</details>


#### OS version

<details>

```console
N/A
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ­¤IssueæŠ¥å‘Šäº†åœ¨Kubernetes v1.31.0ä¸­ï¼Œ`ResourceSliceList`å¯¹è±¡çš„APIå“åº”ä¸­ä½¿ç”¨äº†`listMeta`å­—æ®µï¼Œè€Œä¸æ˜¯æ ‡å‡†çš„`metadata`å­—æ®µã€‚è¿™å¯èƒ½å¯¼è‡´åŸºäºAPIçš„å®¢æˆ·ç«¯æˆ–å·¥å…·åœ¨è§£æå“åº”æ—¶å‡ºç°é—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™ä»…ä»…æ˜¯APIå“åº”ä¸­çš„å­—æ®µå‘½åä¸ä¸€è‡´é—®é¢˜ï¼Œå¹¶ä¸ä¼šå¯¼è‡´ä»»ä½•å®‰å…¨æ¼æ´ã€‚æ”»å‡»è€…æ— æ³•åˆ©ç”¨è¯¥é—®é¢˜è¿›è¡Œæ”»å‡»ï¼Œä¹Ÿä¸ä¼šå¯¼è‡´æƒé™æå‡ã€æ•°æ®æ³„éœ²æˆ–æ‰§è¡Œä»»æ„ä»£ç ç­‰å®‰å…¨é£é™©ã€‚å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126650 Pod status phase misses Pending transition after node reboot

- Issue é“¾æ¥ï¼š[#126650](https://github.com/kubernetes/kubernetes/issues/126650)

### Issue å†…å®¹

#### What happened?

When a kubernetes node is rebooted, all the containers running on the node are terminated and then created & started over again. When the pod has a list of init containers runing, and if the init container takes a little bit time (longer than PLEG period), the pod is supposed to have `status.phase: Pending`, but instead, during the whole operation, `status.phase` is always `Running`.

This new behavior (always has `status.phase: Running`) started when "restartable init container" was [introduced][1]. After node reboots, all the regular init containers are started one by one, so `pendingInitialization` is supposed by larger than 0 (unless the init container is completed pretty fast that PLEG doesn't catch any running init container), and should report `Pending` phase. But the behavior has changed to check further whether there are any other regular containers that have started. In the case of node reboot, all the other regular containers are already terminated, and then considered as "yes, they have started", and then no longer reported as `Pending`.

I think this behavior change is unexpected, as that violates the [definition][2] of "Running":

> // PodRunning means the pod has been bound to a node and all of the containers have been started.
	// At least one container is still running or is in the process of being restarted.

None of the containers are started or running, and they are not in the process of being started or restarted since init containers are still running.

[1]: https://github.com/kubernetes/kubernetes/blob/099a88370d017dacf16e67306ebcdec8394fae83/pkg/kubelet/kubelet_pods.go#L1675-L1685
[2]: https://github.com/kubernetes/kubernetes/blob/099a88370d017dacf16e67306ebcdec8394fae83/pkg/apis/core/types.go#L2871


#### What did you expect to happen?

I expect that pods with init containers should transition into "Pending" when kubelet observed that its regular init container is still running and none of its main containers are started.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Launch a cluster with minikube:

```console
$ minikube start --nodes=2 --kubernetes-version=v1.30.3
```

2. Create the following pod:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sleep
spec:
  restartPolicy: Always
  containers:
  - name: sleep
    image: busybox
    command:
    - sleep
    - infinity
  initContainers:
  - name: snap
    image: busybox
    command:
    - sleep
    - "10"
```

3. Log in the node and simulate "reboot"

```console
$ minikube ssh --node <node-which-runs-the-pod-above>
...
# systemctl stop kubelet
# docker stop $(docker ps -aq)
# systemctl start kubelet
```

4. Observe the pod status via:

```console
$ k get pods -w -o jsonpath='{$.status.phase}{"\n"}'
```

On 1.30.3, the output from 4 is `Running` constantly, while in previous kubernetes version (for e,g, v1.23.17), it will show "Pending" while the init container is still Running.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.0", GitCommit:"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d", GitTreeState:"clean", BuildDate:"2022-12-08T19:58:30Z", GoVersion:"go1.19.4", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"30", GitVersion:"v1.30.3", GitCommit:"6fc0a69044f1ac4c13841ec4391224a2df241460", GitTreeState:"clean", BuildDate:"2024-07-16T23:48:12Z", GoVersion:"go1.22.5", Compiler:"gc", Platform:"linux/amd64"}
WARNING: version difference between client (1.26) and server (1.30) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
None (minikube)
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueå†…å®¹ï¼Œè¯¥é—®é¢˜æè¿°äº†åœ¨KubernetesèŠ‚ç‚¹é‡å¯åï¼Œå…·æœ‰åˆå§‹åŒ–å®¹å™¨ï¼ˆinit containersï¼‰çš„Podçš„çŠ¶æ€æœªæ­£ç¡®è¿‡æ¸¡åˆ°`Pending`ï¼Œè€Œæ˜¯æŒç»­æ˜¾ç¤ºä¸º`Running`ã€‚è¿™æ˜¯å› ä¸ºåœ¨èŠ‚ç‚¹é‡å¯åï¼Œæ‰€æœ‰å®¹å™¨éƒ½ä¼šè¢«ç»ˆæ­¢å¹¶é‡æ–°å¯åŠ¨ï¼Œå½“åˆå§‹åŒ–å®¹å™¨è¿è¡Œæ—¶ï¼Œé¢„æœŸPodçš„çŠ¶æ€åº”ä¸º`Pending`ï¼Œä½†å®é™…è¡¨ç°ä¸º`Running`ã€‚

ä»å®‰å…¨é£é™©çš„è§’åº¦æ¥çœ‹ï¼Œè¿™ä¸€é—®é¢˜ä¸»è¦æ¶‰åŠåˆ°PodçŠ¶æ€æ˜¾ç¤ºçš„ä¸ä¸€è‡´æˆ–ä¸å‡†ç¡®ã€‚ä½†æ˜¯ï¼Œè¿™å¹¶ä¸æ¶‰åŠæ”»å‡»è€…å¯ä»¥åˆ©ç”¨çš„æ¼æ´ã€‚æ”»å‡»è€…æ— æ³•é€šè¿‡æ­¤é—®é¢˜è·å¾—æœªæˆæƒçš„è®¿é—®æˆ–æå‡æƒé™ï¼Œä¹Ÿæ— æ³•é€šè¿‡æ­¤é—®é¢˜è¿›è¡Œæ‹’ç»æœåŠ¡æ”»å‡»æˆ–å…¶ä»–å®‰å…¨å¨èƒã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤é—®é¢˜ä¸ç¬¦åˆé«˜é£é™©çš„æ¡ä»¶ï¼Œä¸ä¼šè¢«åˆ†é…CVEç¼–å·ï¼Œä¸”ä¸å­˜åœ¨å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126637 `kubectl wait --for=jsonpath='{.status.readyReplicas}'=1` fails in 1.31.0-rc.1

- Issue é“¾æ¥ï¼š[#126637](https://github.com/kubernetes/kubernetes/issues/126637)

### Issue å†…å®¹

#### What happened?

https://kubernetes.slack.com/archives/C0EG7JC6T/p1723471158762629

In v1.31.0-rc.1 `kubectl wait --for=jsonpath='{.status.readyReplicas}'=1` commands hang, see relevant thread for details.



#### What did you expect to happen?

Second command should work, as it does with kubectl v1.30.3

#### How can we reproduce it (as minimally and precisely as possible)?

Reproducer against a 1.30.3 server:

```
(
	curl https://dl.k8s.io/v1.31.0-rc.1/kubernetes-client-linux-amd64.tar.gz -L -o v1.31.0-rc.1.tar.gz
	tar xvzf v1.31.0-rc.1.tar.gz -C v1.31.0-rc.1
)


./v1.31.0-rc.1/kubernetes/client/bin/kubectl create deploy nginx --image nginx --replicas 1
./v1.31.0-rc.1/kubernetes/client/bin/kubectl wait deploy nginx --for=condition=available                # works
./v1.31.0-rc.1/kubernetes/client/bin/kubectl wait deploy nginx --for=jsonpath='{.status.readyReplicas}'=1    # does not work
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ ./v1.31.0-rc.1/kubernetes/client/bin/kubectl version
Client Version: v1.31.0-rc.1
Kustomize Version: v5.4.2
Server Version: v1.30.3

```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨kubectl v1.31.0-rc.1ç‰ˆæœ¬æ—¶ï¼Œæ‰§è¡Œ`kubectl wait --for=jsonpath='{.status.readyReplicas}'=1`å‘½ä»¤ä¼šæŒ‚èµ·ï¼Œè€Œåœ¨v1.30.3ç‰ˆæœ¬ä¸­å¯ä»¥æ­£å¸¸å·¥ä½œã€‚è¿™å¯èƒ½æ˜¯ç”±äºæ–°ç‰ˆæœ¬ä¸­çš„åŠŸèƒ½æ€§å›å½’æˆ–Bugå¯¼è‡´çš„å‘½ä»¤æ— æ³•æ­£å¸¸æ‰§è¡Œã€‚ä»æä¾›çš„ä¿¡æ¯æ¥çœ‹ï¼Œå¹¶æœªæ¶‰åŠä»»ä½•å®‰å…¨é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤é—®é¢˜ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126741 featuregates_linter fails to update the corresponding files

- Issue é“¾æ¥ï¼š[#126741](https://github.com/kubernetes/kubernetes/issues/126741)

### Issue å†…å®¹

#### Repro

```diff
diff --git a/pkg/features/kube_features.go b/pkg/features/kube_features.go
index 80a25132bca..fb598b95ddd 100644
--- a/pkg/features/kube_features.go
+++ b/pkg/features/kube_features.go
@@ -1125,7 +1125,7 @@ var defaultKubernetesFeatureGates = map[featuregate.Feature]featuregate.FeatureS
 
        MinDomainsInPodTopologySpread: {Default: true, PreRelease: featuregate.GA, LockToDefault: true}, // remove in 1.32
 
-       MultiCIDRServiceAllocator: {Default: false, PreRelease: featuregate.Beta},
+       MultiCIDRServiceAllocator: {Default: true, PreRelease: featuregate.Beta},
 
        NewVolumeManagerReconstruction: {Default: true, PreRelease: featuregate.GA, LockToDefault: true}, // remove in 1.32
```

```sh
hack/update-featuregates.sh 
found 158 features in FeatureSpecMap var defaultKubernetesFeatureGates in file: /usr/local/google/home/aojea/src/kubernetes/pkg/features/kube_features.go
found 2 features in FeatureSpecMap var defaultKubernetesFeatureGates in file: /usr/local/google/home/aojea/src/kubernetes/staging/src/k8s.io/apiextensions-apiserver/pkg/features/kube_features.go
found 37 features in FeatureSpecMap var defaultKubernetesFeatureGates in file: /usr/local/google/home/aojea/src/kubernetes/staging/src/k8s.io/apiserver/pkg/features/kube_features.go
found 3 features in FeatureSpecMap of func featureGates in file: /usr/local/google/home/aojea/src/kubernetes/staging/src/k8s.io/component-base/logs/api/v1/kube_features.go
found 1 features in FeatureSpecMap of func featureGates in file: /usr/local/google/home/aojea/src/kubernetes/staging/src/k8s.io/component-base/metrics/features/kube_features.go
found 3 features in FeatureSpecMap var cloudPublicFeatureGates in file: /usr/local/google/home/aojea/src/kubernetes/staging/src/k8s.io/controller-manager/pkg/features/kube_features.go
panic: feature MultiCIDRServiceAllocator changed with diff:   cmd.featureInfo{
                Name:     "MultiCIDRServiceAllocator",
                FullName: "",
                VersionedSpecs: []cmd.featureSpec{
                        {
        -                       Default:       false,
        +                       Default:       true,
                                LockToDefault: false,
                                PreRelease:    "Beta",
                                Version:       "",
                        },
                },
          }


goroutine 1 [running]:
k8s.io/kubernetes/test/featuregates_linter/cmd.updateFeatureListFunc(0xc0001f4d00?, {0x69953d?, 0x4?, 0x6994ed?})
        /usr/local/google/home/aojea/src/kubernetes/test/featuregates_linter/cmd/feature_gates.go:108 +0x91
github.com/spf13/cobra.(*Command).execute(0xc0001fc608, {0x8c8000, 0x0, 0x0})
        /usr/local/google/home/aojea/src/kubernetes/vendor/github.com/spf13/cobra/command.go:989 +0xa91
github.com/spf13/cobra.(*Command).ExecuteC(0x8a3dc0)
        /usr/local/google/home/aojea/src/kubernetes/vendor/github.com/spf13/cobra/command.go:1117 +0x3ff
github.com/spf13/cobra.(*Command).Execute(...)
        /usr/local/google/home/aojea/src/kubernetes/vendor/github.com/spf13/cobra/command.go:1041
k8s.io/kubernetes/test/featuregates_linter/cmd.Execute()
        /usr/local/google/home/aojea/src/kubernetes/test/featuregates_linter/cmd/root.go:32 +0x1a
main.main()
        /usr/local/google/home/aojea/src/kubernetes/test/featuregates_linter/main.go:22 +0xf
```

`verifyFeatureDeletionOnly` does not consider the command is being executed to update the files and fails, so it never updates the corresponding files


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä¿®æ”¹`MultiCIDRServiceAllocator`ç‰¹æ€§æ—¶ï¼Œ`featuregates_linter`è„šæœ¬æ— æ³•æ­£ç¡®æ›´æ–°ç›¸åº”çš„æ–‡ä»¶ï¼Œå¯¼è‡´è„šæœ¬å‘ç”Ÿpanicã€‚è¿™æ˜¯ç”±äºè„šæœ¬é€»è¾‘ä¸Šçš„é”™è¯¯ï¼Œå¯¼è‡´åœ¨æ›´æ–°ç‰¹æ€§é—¨æ§æ—¶éªŒè¯å¤±è´¥ã€‚è¿™å±äºå¼€å‘å·¥å…·è‡ªèº«çš„é—®é¢˜ï¼Œä¸ä¼šè¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œä¹Ÿä¸ä¼šå¼•å…¥å®‰å…¨æ¼æ´ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ã€‚

---

## Issue #126739 `ValidatingAdmissionPolicyBindingList` is always returned with non-zero length

- Issue é“¾æ¥ï¼š[#126739](https://github.com/kubernetes/kubernetes/issues/126739)

### Issue å†…å®¹

#### What happened?

Performing request on `ValidatingAdmissionPolicyList` or `ValidatingAdmissionPolicyBindingList` returns a non-zero length response, event if there are no `ValidatingAdmissionPolicy` in the cluster.

#### What did you expect to happen?

Return empty list, as other resource types.

#### How can we reproduce it (as minimally and precisely as possible)?

Kubernetes v1.30
```
kubectl get ValidatingAdmissionPolicyBinding -A -o yaml
apiVersion: v1
items:
- apiVersion: admissionregistration.k8s.io/v1
  kind: ValidatingAdmissionPolicyBindingList
  metadata:
    resourceVersion: "29454"
kind: List
metadata:
  resourceVersion: ""
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version

warning: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.0", GitCommit:"1b4df30b3cdfeaba6024e81e559a6cd09a089d65", GitTreeState:"clean", BuildDate:"2023-04-11T17:10:18Z", GoVersion:"go1.20.3", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"30", GitVersion:"v1.30.0", GitCommit:"7c48c2bd72b9bf5c44d21d7338cc7bea77d0ad2a", GitTreeState:"clean", BuildDate:"2024-05-13T22:00:36Z", GoVersion:"go1.22.2", Compiler:"gc", Platform:"linux/amd64"}
WARNING: version difference between client (1.27) and server (1.30) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„ä¿¡æ¯ï¼Œæ‰§è¡Œ `kubectl get ValidatingAdmissionPolicyBinding -A -o yaml` å‘½ä»¤æ—¶ï¼Œå³ä½¿é›†ç¾¤ä¸­æ²¡æœ‰ `ValidatingAdmissionPolicy`ï¼Œä»ç„¶è¿”å›ä¸€ä¸ªéé›¶é•¿åº¦çš„å“åº”ã€‚

ç»è¿‡åˆ†æï¼Œè¿™ç§è¡Œä¸ºå¯èƒ½æ˜¯ç”±äº API è¿”å›çš„åˆ—è¡¨ä¸­åŒ…å«äº†é»˜è®¤çš„å…ƒæ•°æ®ï¼Œå³ä½¿å®é™…çš„ items åˆ—è¡¨ä¸ºç©ºã€‚è¿™æ›´åƒæ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§æˆ–æ˜¾ç¤ºä¸Šçš„é—®é¢˜ï¼Œå¹¶ä¸æ¶‰åŠåˆ°æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1. è¯¥é—®é¢˜å¹¶ä¸èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨æ¥è¿›è¡Œæ”»å‡»ã€‚
2. è¯¥é—®é¢˜ä¸ä¼šå¯¼è‡´æ¼æ´äº§ç”Ÿï¼Œä¸ä¼šè¢«åˆ†é… CVE ç¼–å·ï¼ŒæŒ‰ç…§ CVSS 3.1 æ ‡å‡†ä¹Ÿæ— æ³•è·å¾— high ä»¥ä¸Šçš„è¯„åˆ†ã€‚
3. è¯¥é—®é¢˜ä¸æ¶‰åŠæ•æ„Ÿä¿¡æ¯æ³„éœ²æˆ–æƒé™æå‡ç­‰å®‰å…¨é£é™©ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œæ­¤ Issue ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126732 K8s pod will restart indefinitely

- Issue é“¾æ¥ï¼š[#126732](https://github.com/kubernetes/kubernetes/issues/126732)

### Issue å†…å®¹

#### What happened?

Pod sandbox changed, it will be killed and re-created

#### What did you expect to happen?

How to identify the root cause

#### How can we reproduce it (as minimally and precisely as possible)?

How to identify the root cause

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>
1.28

#### Cloud provider

<details>

</details>
null

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueå†…å®¹ï¼Œç”¨æˆ·æŠ¥å‘ŠKubernetes Podä¼šæ— é™é‡å¯ï¼Œæƒ³è¦æ‰¾å‡ºæ ¹æœ¬åŸå› ã€‚è¿™æ˜¯ä¸€ä¸ªå…³äºKubernetesé›†ç¾¤ä¸­Podç¨³å®šæ€§çš„é—®é¢˜ï¼Œæ¶‰åŠåˆ°åº”ç”¨éƒ¨ç½²å’Œå®¹å™¨ç®¡ç†çš„å¸¸è§é—®é¢˜ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤IssueæœªæåŠä»»ä½•å®‰å…¨é£é™©ã€æ¼æ´åˆ©ç”¨æˆ–æ•æ„Ÿä¿¡æ¯æ³„éœ²ï¼Œä¸æ¶‰åŠæ”»å‡»è€…å¯ä»¥åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚

---

## Issue #126726 client-go 1.31 fake Apply requires `metadata.name` to get set, while live client does not

- Issue é“¾æ¥ï¼š[#126726](https://github.com/kubernetes/kubernetes/issues/126726)

### Issue å†…å®¹

Trying out the (excellent!) new Apply with the fake client, I found a different from the fake vs real client


My patch:

```yaml
{
  "kind" : "Service",
  "apiVersion" : "/v1",
  "status" : {
    "conditions" : [ {
      "type" : "t1",
      "status" : "True",
      "lastTransitionTime" : "2024-08-15T23:24:19.307605301Z",
      "reason" : "some reason"
    } ]
  }
}
```

Here
https://github.com/kubernetes/kubernetes/blob/a221d3a40c04b02134a4d1deb436f0787c1f2028/staging/src/k8s.io/client-go/testing/fixture.go#L217 we lose the name. We extract the namespace from the action, but use `applyConfigurationMeta.GetName()` which is empty. This means we have no name, the `Get` fails, and we attempt to `Create` a nameless object.

This doesn't apply to other patch types, which get the live object and apply the patch to them before this point.

This could be fixed, perhaps, by inserting `patchObj.SetName(action.GetName())` before we call `Apply()`?

cc @jpbetz 

/sig api-machinery


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†åœ¨ä½¿ç”¨client-go 1.31çš„fakeå®¢æˆ·ç«¯æ—¶ï¼Œè°ƒç”¨Applyæ–¹æ³•éœ€è¦è®¾ç½®`metadata.name`ï¼Œè€ŒçœŸå®çš„å®¢æˆ·ç«¯ä¸éœ€è¦ã€‚è¿™å¯¼è‡´äº†fakeå®¢æˆ·ç«¯å’ŒçœŸå®å®¢æˆ·ç«¯ä¹‹é—´çš„è¡Œä¸ºä¸ä¸€è‡´ã€‚

ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¿™ä¸ªé—®é¢˜ä¸»è¦å½±å“çš„æ˜¯æµ‹è¯•ç¯å¢ƒä¸­çš„è¡Œä¸ºä¸€è‡´æ€§ï¼Œå¯èƒ½å¯¼è‡´æµ‹è¯•ç»“æœä¸å®é™…ç”Ÿäº§ç¯å¢ƒä¸åŒã€‚ä½†æ˜¯ï¼Œè¿™å¹¶ä¸æ¶‰åŠæ”»å‡»è€…å¯ä»¥åˆ©ç”¨çš„æ¼æ´æˆ–å®‰å…¨é£é™©ã€‚å®ƒä¸ä¼šå¯¼è‡´å‘½ä»¤æ‰§è¡Œã€æƒé™æå‡ã€æ•°æ®æ³„éœ²ç­‰é«˜å®‰å…¨é£é™©çš„é—®é¢˜ã€‚

å› æ­¤ï¼Œæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126722 Andoka Cloud B 

- Issue é“¾æ¥ï¼š[#126722](https://github.com/kubernetes/kubernetes/issues/126722)

### Issue å†…å®¹

#### What happened?

#### 
``

1. 

- [x]  [](url)

#### What did you expect to happen?

[](url) 

####  
#126395  [ ]  __

#### How can we reproduce it (as minimally and precisely as possible)?

1. [x]   `` 
2. [x] __****

#### Anything else we need to know?

#### [](url) 
__ 

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueå†…å®¹ï¼Œå†…å®¹ä¸»è¦ä¸ºç©ºç™½æˆ–å ä½ç¬¦ï¼Œç¼ºä¹å…·ä½“çš„ä¿¡æ¯å’Œç»†èŠ‚ï¼Œæ— æ³•åˆ†æå‡ºä»»ä½•æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬6æ¡ï¼Œå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

---

## Issue #126704 kube-controller-manager takes 100% cpu on master

- Issue é“¾æ¥ï¼š[#126704](https://github.com/kubernetes/kubernetes/issues/126704)

### Issue å†…å®¹

#### What happened?

When starting K8s on master the kube-controller-manager 100% cpu all the time in an idle system. I have 2 cores on my VM's so kube-controller-manager hogs 1 core.

#### What did you expect to happen?

kube-controller-manager should take ~0% cpu in an idle system

#### How can we reproduce it (as minimally and precisely as possible)?

Start K8s on master and run "top"

#### Anything else we need to know?

I made a bisect, and the problem appear in commit eb16aa1d4a5d36482bd58da6737364ef81759002.

I start kube-controller-manager from init scripts, so no POD.

#### Kubernetes version

master (v1.32.0-alpha.0.22+f6a11da279877b)

#### Cloud provider

none

#### OS version

Linux 6.10.0

#### Install tools

none

#### Container runtime (CRI) and version (if applicable)

crio version 1.29.2

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

Probably all CNI-plugins. Tested:

1. flannel:v0.24.2
2. calico/cni:v3.27.0
3. cilium:v1.15.4
4. antrea-ubuntu:v1.14.1


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†`kube-controller-manager`åœ¨ç©ºé—²ç³»ç»Ÿä¸­å ç”¨100% CPUçš„é—®é¢˜ã€‚è¿™å±äºæ€§èƒ½é—®é¢˜ï¼Œæ²¡æœ‰æåŠä»»ä½•å®‰å…¨ç›¸å…³çš„å†…å®¹ï¼Œä¹Ÿæ²¡æœ‰è¯´æ˜æ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¯¥é—®é¢˜å¯¹ç³»ç»Ÿè¿›è¡Œæ”»å‡»ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬6æ¡ï¼šå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

---

## Issue #126881 Volume expand controller doesn't have a support for the flexvolume plugin

- Issue é“¾æ¥ï¼š[#126881](https://github.com/kubernetes/kubernetes/issues/126881)

### Issue å†…å®¹

#### What happened?

The flexVolume plugin implements the `ExpandableVolumePlugin` interface, but the expander controller does not add support for it.

- https://github.com/kubernetes/kubernetes/blob/remove-unnecessary-permissions/pkg/controller/volume/expand/expand_controller.go#L118
- https://github.com/kubernetes/kubernetes/blob/remove-unnecessary-permissions/pkg/volume/flexvolume/plugin.go#L59

Is it a bug? the flexVolume plugin has been marked as `deprecated` since v1.23, but it still works.

https://github.com/kubernetes/kubernetes/pull/67851 implemented the Flexvolume resize and said that the flex plugins are not installed on the controller. 

@gnufied said that the flex volume plugin IMO does support allowing expansion from control-plane too. It is just that, most deployments typically don't install the flex volume plugin in the control-plane for various reasons, but if they do and plugin implements expandvolume command, then control-plane expansion will work. 

> if they do and plugin implements expandvolume command, then control-plane expansion will work. 

The prequisite is that the expand controller needs to add support for the flexVolume plugin, but it doesn't now.

What did you expect to happen?



#### What did you expect to happen?

One of the following:

- [ ] The expand controller should add support for the flexVolume plugin. (may not be recommended because the flexVolume plugin is deprecated since v1.23. And no users reported this issue in my knowledge)

- [ ] Once the portworx plugin completes csi migration, can we deprecated the expand controller and then remove it in the future. (recommended) but flex plugin also optionally supports attach which can be executed from control-plane. it means that the flex plugin can be installed on the controller.

#### How can we reproduce it (as minimally and precisely as possible)?

NONE

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
All supported versions
```

</details>


#### Cloud provider

<details>
NONE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®Issueå†…å®¹ï¼Œè¯¥é—®é¢˜æ¶‰åŠåˆ°Kubernetesçš„å·æ‰©å±•æ§åˆ¶å™¨ä¸æ”¯æŒflexVolumeæ’ä»¶çš„é—®é¢˜ã€‚è¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ç¼ºé™·æˆ–ç¼ºå¤±ï¼Œå¹¶ä¸æ˜¯ä¸€ä¸ªå®‰å…¨æ¼æ´ã€‚æ²¡æœ‰è¯æ®è¡¨æ˜æ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¯¥é—®é¢˜è¿›è¡Œæ”»å‡»ï¼Œä¹Ÿæ²¡æœ‰è¿¹è±¡è¡¨æ˜è¯¥é—®é¢˜å¯èƒ½å¯¼è‡´å®‰å…¨é£é™©ã€‚å› æ­¤ï¼Œæœ¬Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ã€‚

---

## Issue #126879 when 'hostPort' is unset from pre-existing deployment using hostNetwork, it spawns new replicaset but fails to delete pre-existing replicaset

- Issue é“¾æ¥ï¼š[#126879](https://github.com/kubernetes/kubernetes/issues/126879)

### Issue å†…å®¹

#### What happened?

- i prepared an nginx container to listen on port 9033
- deployed it with a deployment that was configured to use 'hostNetwork: true' and explicitly set 'hostPort: 9033'
- i edited the deployment by simply deleting the 'hostPort' entry
- i then applied the edited deployment manifest
- a second pod was launched, but got stuck in 'Pending' state. The pre-existing pod was not deleted as expected. It kept running
- a second replicaset was launched, but its pod (above) was stuck in pending state. The pre-existing replicaset was not deleted as expected.

```
# diff manifest-original.yaml manifest-tweak.yaml
32d31
<           hostPort: 9033
# KUBECONFIG=kconf kubectl apply -f manifest-original.yaml
deployment.apps/myapp created
$ KUBECONFIG=kconf kubectl get rs -n my-system
NAME                                               DESIRED   CURRENT   READY   AGE
myapp-55758c985                                    1         1         1       8s

$ KUBECONFIG=kconf kubectl get pods -n my-system
NAME                                                     READY   STATUS    RESTARTS   AGE
myapp-55758c985-dft2b                                    1/1     Running   0          20s

$ KUBECONFIG=kconf kubectl apply -f manifest-tweak.yaml 
deployment.apps/myapp configured

$ KUBECONFIG=kconf kubectl get rs -n my-system
NAME                                               DESIRED   CURRENT   READY   AGE
myapp-5564fdd866                                   1         1         0       7s
myapp-55758c985                                    1         1         1       59s

$ KUBECONFIG=kconf kubectl get pods -n my-system
NAME                                                     READY   STATUS    RESTARTS   AGE
myapp-5564fdd866-2wwbg                                   0/1     Pending   0          18s
myapp-55758c985-dft2b                                    1/1     Running   0          70s

$ cat manifest-original.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  namespace: my-system
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  selector:
    matchLabels:
      app: myapp
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
      - name: mgr
        image: docker-local.artifactory.eng.yadayada.com/tilt/nginx:v0 
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 9033
          hostPort: 9033
          name: api
          protocol: TCP
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: false
          runAsNonRoot: false
      dnsPolicy: ClusterFirst
      hostNetwork: true
      nodeSelector:
        node-role.kubernetes.io/control-plane: ""
      priorityClassName: system-cluster-critical
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: my-sa
      serviceAccountName: my-sa
      terminationGracePeriodSeconds: 30
      tolerations:
      - key: CriticalAddonsOnly
        operator: Exists
      - effect: NoSchedule
        key: node-role.kubernetes.io/control-plane
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
      - effect: NoSchedule
        key: node.kubernetes.io/not-ready
      - effect: NoSchedule
        key: node.cloudprovider.kubernetes.io/uninitialized
        value: "true"

$ KUBECONFIG=kconf kubectl version
Client Version: v1.29.7
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.10

```

#### What did you expect to happen?

- i expected the pre-existing replicaset & its corresponding pod to be completely deleted, and another replicaset created which then successfully runs its pod.

- i did NOT expect the pre-existing replicaset & its corresponding pod to persist, and another replicaset launched whose pod gets stuck in 'Pending' state

#### How can we reproduce it (as minimally and precisely as possible)?

- see linux terminal output in  "What happened?" section above.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ KUBECONFIG=kconf kubectl version
Client Version: v1.29.7
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.10
```

</details>


#### Cloud provider

<details>
vsphere
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release 
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

$ uname -a
# paste output here
Linux bug-repro-grpch-jrrd4 5.15.0-117-generic #127-Ubuntu SMP Fri Jul 5 20:13:28 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueå†…å®¹ï¼Œç”¨æˆ·åœ¨Kubernetesä¸­éƒ¨ç½²äº†ä¸€ä¸ªä½¿ç”¨`hostNetwork: true`å’Œæ˜¾å¼è®¾ç½®äº†`hostPort: 9033`çš„nginxå®¹å™¨ã€‚åœ¨åˆ é™¤äº†`hostPort`é…ç½®å¹¶é‡æ–°åº”ç”¨åï¼Œå‘ç°æ—§çš„ReplicaSetå’ŒPodæœªè¢«åˆ é™¤ï¼Œæ–°çš„Podå¤„äº`Pending`çŠ¶æ€ã€‚è¿™ä¸ªé—®é¢˜ä¸Kubernetesåœ¨æ›´æ–°Deploymentæ—¶çš„èµ„æºç®¡ç†å’Œè°ƒåº¦è¡Œä¸ºæœ‰å…³ï¼Œå¯èƒ½æ˜¯Deploymentæ›´æ–°ç­–ç•¥æˆ–èµ„æºè°ƒåº¦çš„é—®é¢˜ã€‚

ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¿™ä¸ªé—®é¢˜å¹¶ä¸æ¶‰åŠæ½œåœ¨çš„å®‰å…¨é£é™©ã€‚æ²¡æœ‰è¿¹è±¡è¡¨æ˜æ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ­¤é—®é¢˜è¿›è¡Œæ”»å‡»ï¼Œå¯¼è‡´æƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰é«˜é£é™©å®‰å…¨é—®é¢˜ã€‚å› æ­¤ï¼Œæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126866 ValidatingAdmissionPolicy's Validate func returns decision with Evaluation="" when Action is ActionDeny

- Issue é“¾æ¥ï¼š[#126866](https://github.com/kubernetes/kubernetes/issues/126866)

### Issue å†…å®¹

#### What happened?

This issue is about the `Validate` function in `staging/src/k8s.io/apiserver/pkg/admission/plugin/policy/validating/validator.go`.

In this function, `ValidatingAdmissionPolicy`'s `spec.validations.expression` is evaluated in this line:

```
evalResults, remainingBudget, err := v.validationFilter.ForInput(ctx, versionedAttr, admissionRequest, optionalVars, ns, runtimeCELCostBudget)
```

and if `evalResult` is not `true`, `PolicyDecision`'s `Action` is set to `ActionDeny`:

```
} else if evalResult.EvalResult != celtypes.True {
	decision.Action = ActionDeny
...
```

But `decision.Evaluation` remains empty string.


#### What did you expect to happen?

`decision.Evaluation` should be set to `EvalDeny`.

#### How can we reproduce it (as minimally and precisely as possible)?


#### Anything else we need to know?

_No response_

#### Kubernetes version

I imported public library

<details>

<summary>go.mod</summary>

```
module k8s-bug-report

go 1.22.5

require (
	k8s.io/api v0.31.0
	k8s.io/apimachinery v0.31.0
	k8s.io/apiserver v0.31.0
)

require (
	github.com/antlr4-go/antlr/v4 v4.13.0 // indirect
	github.com/asaskevich/govalidator v0.0.0-20190424111038-f61b66f89f4a // indirect
	github.com/beorn7/perks v1.0.1 // indirect
	github.com/blang/semver/v4 v4.0.0 // indirect
	github.com/cenkalti/backoff/v4 v4.3.0 // indirect
	github.com/cespare/xxhash/v2 v2.3.0 // indirect
	github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc // indirect
	github.com/emicklei/go-restful/v3 v3.11.0 // indirect
	github.com/felixge/httpsnoop v1.0.4 // indirect
	github.com/fxamacker/cbor/v2 v2.7.0 // indirect
	github.com/go-logr/logr v1.4.2 // indirect
	github.com/go-logr/stdr v1.2.2 // indirect
	github.com/go-openapi/jsonpointer v0.19.6 // indirect
	github.com/go-openapi/jsonreference v0.20.2 // indirect
	github.com/go-openapi/swag v0.22.4 // indirect
	github.com/gogo/protobuf v1.3.2 // indirect
	github.com/golang/protobuf v1.5.4 // indirect
	github.com/google/cel-go v0.20.1 // indirect
	github.com/google/gnostic-models v0.6.8 // indirect
	github.com/google/go-cmp v0.6.0 // indirect
	github.com/google/gofuzz v1.2.0 // indirect
	github.com/google/uuid v1.6.0 // indirect
	github.com/grpc-ecosystem/grpc-gateway/v2 v2.20.0 // indirect
	github.com/imdario/mergo v0.3.6 // indirect
	github.com/inconshreveable/mousetrap v1.1.0 // indirect
	github.com/josharian/intern v1.0.0 // indirect
	github.com/json-iterator/go v1.1.12 // indirect
	github.com/mailru/easyjson v0.7.7 // indirect
	github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd // indirect
	github.com/modern-go/reflect2 v1.0.2 // indirect
	github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 // indirect
	github.com/pkg/errors v0.9.1 // indirect
	github.com/prometheus/client_golang v1.19.1 // indirect
	github.com/prometheus/client_model v0.6.1 // indirect
	github.com/prometheus/common v0.55.0 // indirect
	github.com/prometheus/procfs v0.15.1 // indirect
	github.com/spf13/cobra v1.8.1 // indirect
	github.com/spf13/pflag v1.0.5 // indirect
	github.com/stoewer/go-strcase v1.2.0 // indirect
	github.com/x448/float16 v0.8.4 // indirect
	go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp v0.53.0 // indirect
	go.opentelemetry.io/otel v1.28.0 // indirect
	go.opentelemetry.io/otel/exporters/otlp/otlptrace v1.28.0 // indirect
	go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc v1.27.0 // indirect
	go.opentelemetry.io/otel/metric v1.28.0 // indirect
	go.opentelemetry.io/otel/sdk v1.28.0 // indirect
	go.opentelemetry.io/otel/trace v1.28.0 // indirect
	go.opentelemetry.io/proto/otlp v1.3.1 // indirect
	golang.org/x/exp v0.0.0-20230515195305-f3d0a9c9a5cc // indirect
	golang.org/x/net v0.26.0 // indirect
	golang.org/x/oauth2 v0.21.0 // indirect
	golang.org/x/sync v0.7.0 // indirect
	golang.org/x/sys v0.21.0 // indirect
	golang.org/x/term v0.21.0 // indirect
	golang.org/x/text v0.16.0 // indirect
	golang.org/x/time v0.3.0 // indirect
	google.golang.org/genproto/googleapis/api v0.0.0-20240528184218-531527333157 // indirect
	google.golang.org/genproto/googleapis/rpc v0.0.0-20240701130421-f6361c86f094 // indirect
	google.golang.org/grpc v1.65.0 // indirect
	google.golang.org/protobuf v1.34.2 // indirect
	gopkg.in/evanphx/json-patch.v4 v4.12.0 // indirect
	gopkg.in/inf.v0 v0.9.1 // indirect
	gopkg.in/yaml.v2 v2.4.0 // indirect
	gopkg.in/yaml.v3 v3.0.1 // indirect
	k8s.io/client-go v0.31.0 // indirect
	k8s.io/component-base v0.31.0 // indirect
	k8s.io/klog/v2 v2.130.1 // indirect
	k8s.io/kube-openapi v0.0.0-20240228011516-70dd3763d340 // indirect
	k8s.io/utils v0.0.0-20240711033017-18e509b52bc8 // indirect
	sigs.k8s.io/apiserver-network-proxy/konnectivity-client v0.30.3 // indirect
	sigs.k8s.io/json v0.0.0-20221116044647-bc3834ca7abd // indirect
	sigs.k8s.io/structured-merge-diff/v4 v4.4.1 // indirect
	sigs.k8s.io/yaml v1.4.0 // indirect
)

```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ValidatingAdmissionPolicyçš„Validateå‡½æ•°ä¸­ï¼Œå½“spec.validations.expressionçš„è¯„ä¼°ç»“æœä¸ä¸ºçœŸæ—¶ï¼Œdecision.Actionè¢«è®¾ç½®ä¸ºActionDenyï¼Œä½†decision.Evaluationä»ç„¶æ˜¯ç©ºå­—ç¬¦ä¸²ã€‚æœŸæœ›è¡Œä¸ºæ˜¯å½“Actionä¸ºActionDenyæ—¶ï¼Œdecision.Evaluationåº”è¢«è®¾ç½®ä¸ºEvalDenyã€‚

æ­¤é—®é¢˜å±äºåŠŸèƒ½æ€§é”™è¯¯ï¼Œå¯èƒ½å½±å“ç­–ç•¥è¯„ä¼°ç»“æœçš„å‡†ç¡®æ€§æˆ–å®Œæ•´æ€§ï¼Œä½†ä¸ä¼šå¯¼è‡´æ”»å‡»è€…åˆ©ç”¨å®ƒæ¥è¿›è¡Œæ”»å‡»ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼ŒIssueä¸­æåˆ°çš„é—®é¢˜ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œå› ä¸ºå®ƒä¸æ»¡è¶³èƒ½å¤Ÿè¢«æ”»å‡»è€…åˆ©ç”¨æˆ–å¯¼è‡´é«˜é£é™©æ¼æ´çš„æ¡ä»¶ã€‚

---

## Issue #126850 The Clientset returned from the new NewClientset function does not work for CRDs

- Issue é“¾æ¥ï¼š[#126850](https://github.com/kubernetes/kubernetes/issues/126850)

### Issue å†…å®¹

#### What happened?

Calling the `Create` method fails with, eg

`failed to convert new object (/cluster-egressIP; submariner.io/v1, Kind=ClusterGlobalEgressIP) to smd typed: schema error: no type found matching: com.github.submariner-io.submariner.pkg.apis.submariner.io.v1.ClusterGlobalEgressIP`

Long story short, It ends up trying to validate a resource using the global `schemaYAML` var defined in the generated file _pkg/client/applyconfiguration/internal/internal.go_. However this is not populated with the appropriate types. So `NewClientset` is unusable even though `NewSimpleClientset` is now marked as deprecated.

Note that `NewClientset` works fine with core K8s types because its [schemaYAML](https://github.com/kubernetes/client-go/blob/46965213e4561ad1b9c585d1c3551a0cc8d3fcd6/applyconfigurations/internal/internal.go#L41) is populated appropriately. However [apiextensions](https://github.com/kubernetes/kubernetes/blob/243fdafc6824d6b33faa081efec2ebc4cb164010/staging/src/k8s.io/apiextensions-apiserver/pkg/client/applyconfiguration/internal/internal.go#L41) is not.

#### What did you expect to happen?

I expect the `Clientset` methods to work for CRDs. It seems the code generator needs to be updated.

Also `NewSimpleClientset` should not be deprecated unless/until `NewClientset` works.

#### How can we reproduce it (as minimally and precisely as possible)?

```
import fakeclientset "k8s.io/apiextensions-apiserver/pkg/client/clientset/clientset/fake"
...
client := fakeclientset.NewClientset()
client.ApiextensionsV1().CustomResourceDefinitions().Create(ctx, crd, metav1.CreateOptions{})
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨`NewClientset`å‡½æ•°åˆ›å»ºClientsetæ—¶ï¼Œå¯¹äºCRDï¼ˆè‡ªå®šä¹‰èµ„æºå®šä¹‰ï¼‰çš„æ“ä½œä¼šå¤±è´¥ï¼ŒåŸå› æ˜¯schemaæœªæ­£ç¡®å¡«å……ï¼Œå¯¼è‡´æ— æ³•æ‰¾åˆ°åŒ¹é…çš„ç±»å‹ã€‚è¿™æ˜¯ç”±äºä»£ç ç”Ÿæˆå™¨çš„é—®é¢˜ï¼Œéœ€è¦æ›´æ–°ä»¥æ”¯æŒCRDã€‚ç„¶è€Œï¼Œæ•´ä¸ªé—®é¢˜ä»…æ¶‰åŠåˆ°åŠŸèƒ½æ€§é”™è¯¯ï¼Œå¹¶ä¸å­˜åœ¨å®‰å…¨é£é™©ã€‚

æŒ‰ç…§é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤Issueæœªæ¶‰åŠä»»ä½•å®‰å…¨é£é™©ï¼š
1. æ²¡æœ‰æåŠæ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¯¥é—®é¢˜ã€‚
2. ä¸å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œä¸ä¼šè¢«åˆ†é…CVEç¼–å·ã€‚
3. æœªæš´éœ²æ•æ„Ÿä¿¡æ¯æˆ–é…ç½®é—®é¢˜ã€‚
4. ä¸æ¶‰åŠæ‹’ç»æœåŠ¡æ”»å‡»ã€‚
5. æœªæ¶‰åŠå‡­æ®æ³„éœ²ã€‚
6. æ•…é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

---

## Issue #126795 [Windows] Unable to connect to host process container pod on windows node

- Issue é“¾æ¥ï¼š[#126795](https://github.com/kubernetes/kubernetes/issues/126795)

### Issue å†…å®¹

#### What happened?

I have a windows node with a pod running as a host process container, and therefore with hostNetwork:true. I have the pod exposed as a ClusterIP service. I have a separate pod on the same node (not running as a host process container or with hostNetwork: true) trying to communicate to the ClusterIP, but instead get a connection error.

```
PS C:\> telnet 10.100.24.184 4316
Connecting To 10.100.24.184...Could not open connection to the host, on port 4316: Connect failed
```

#### What did you expect to happen?

I expect a non host process container pod to be able to communicate to a host process container pod via a kubernetes Service without issues on Windows nodes.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a kubernetes cluster with a Windows node. (ex. EKS with Windows Server 2019)
2. Create a pod running with hostNetwork: true. I am using the [amazon cloudwatch observability helm chart](https://github.com/aws-observability/helm-charts/tree/main/charts/amazon-cloudwatch-observability): 
```
helm repo add aws-observability https://aws-observability.github.io/helm-charts
helm repo update aws-observability
helm install --wait --create-namespace --namespace amazon-cloudwatch amazon-cloudwatch aws-observability/amazon-cloudwatch-observability --set clusterName=my-cluster-name --set region=my-cluster-region
```
3. Exec into a separate pod in the cluster and attempt to see if the cluster ip is reachable on the service port:
```
PS C:\> telnet 10.100.146.5 4316
Connecting To 10.100.146.5...Could not open connection to the host, on port 4316: Connect failed
```


#### Anything else we need to know?

Relevant discussion in #sig-windows on slack: https://kubernetes.slack.com/archives/C0SJ4AFB7/p1719608329839819

#### Kubernetes version

<details>

```console
$ kubectl version

Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.2-eks-db838b0
```

</details>


#### Cloud provider

<details>
Amazon EKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture

BuildNumber  Caption                                   OSArchitecture  Version
17763        Microsoft Windows Server 2019 Datacenter  64-bit          10.0.17763

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»Issueå†…å®¹æ¥çœ‹ï¼Œç”¨æˆ·åœ¨WindowsèŠ‚ç‚¹ä¸Šè¿è¡Œäº†ä¸€ä¸ªå¸¦æœ‰hostNetwork: trueçš„Podï¼Œå¹¶å°†å…¶ä½œä¸ºClusterIPæœåŠ¡æš´éœ²ã€‚ä½†æ˜¯ï¼Œå½“å…¶ä»–éhostNetworkçš„Podå°è¯•é€šè¿‡ClusterIPä¸å…¶é€šä¿¡æ—¶ï¼Œè¿æ¥å¤±è´¥ã€‚è¿™ä¼¼ä¹æ˜¯ä¸€ä¸ªå…³äºWindowsèŠ‚ç‚¹ä¸ŠhostNetworkç½‘ç»œè¿æ¥çš„é—®é¢˜ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥é—®é¢˜å¹¶æœªæ¶‰åŠä»»ä½•å®‰å…¨é£é™©ï¼Œä¹ŸæœªæåŠå¯èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨çš„æ¼æ´ï¼Œå› æ­¤é£é™©è¯„çº§åˆ¤æ–­ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

---

## Issue #126790 Inconsistency between the code and the doc when toleration has an empty key

- Issue é“¾æ¥ï¼š[#126790](https://github.com/kubernetes/kubernetes/issues/126790)

### Issue å†…å®¹

#### What happened?

According to the [concept documentation](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#concepts)
> **An empty key** with operator Exists matches all keys, values and effects which means this **will tolerate everything**.

This means when a toleration's `key` is empty, it can match any taints no matter what value the `effect` is.

But this is inconsistent with the [API documentation](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/#concepts):

> **If the key is empty**, operator must be Exists; this combination **means to match all values and all keys.**

This means even if a toleration's `key` is empty, the `effect` should also be the same to match a taint.

And **the code** implementation is following **the API documentation**, thus contradicting **the concept documentation**.


#### What did you expect to happen?

The concept documentation, the API documentation and the code should be consistent.


#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a node with a taint
```bash
kubectl apply -f - <<EOF
apiVersion: v1
kind: Node
metadata:
  name: node-1
spec:
  taints:
    - key: simple-key
      value: simple-value
      effect: NoSchedule
EOF
```

2. Create a pod with a toleration whose key is empty, it should be able to land on `node-1` according to the concept documentation
```bash
kubectl apply -f - <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: pod-1
spec:
  containers:
    - name: container-1
      image: nginx
  tolerations:
    - key: ''
      operator: Exists
      effect: NoExecute
EOF
```

3. Check the pod's scheduling result, it failed to be scheduled to `node-1`
```bash
$ kubectl describe pods pod-1
 Warning  FailedScheduling  0/1 nodes are available: 1 node(s) had untolerated taint {simple-key: simple-value}. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.
```



#### Anything else we need to know?

/sig docs scheduling

#### Kubernetes version

1.30.2


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ç»è¿‡åˆ†æï¼Œè¯¥IssueæŠ¥å‘Šçš„æ˜¯Kubernetesåœ¨tolerationsçš„`key`ä¸ºç©ºæ—¶ï¼Œä»£ç å®ç°å’Œæ¦‚å¿µæ–‡æ¡£ä¹‹é—´å­˜åœ¨ä¸ä¸€è‡´çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œæ¦‚å¿µæ–‡æ¡£æè¿°äº†å½“`key`ä¸ºç©ºä¸”`operator`ä¸º`Exists`æ—¶ï¼Œåº”è¯¥åŒ¹é…æ‰€æœ‰çš„`key`ã€`value`å’Œ`effect`ï¼Œå³å®¹å¿æ‰€æœ‰çš„æ±¡ç‚¹ï¼›è€ŒAPIæ–‡æ¡£å’Œä»£ç å®ç°åˆ™è¦æ±‚`effect`ä¹Ÿéœ€è¦åŒ¹é…æ‰èƒ½å®¹å¿ç›¸åº”çš„æ±¡ç‚¹ã€‚è¿™ä¸ªé—®é¢˜å¯èƒ½å¯¼è‡´ç”¨æˆ·æ ¹æ®æ¦‚å¿µæ–‡æ¡£è¿›è¡Œé…ç½®æ—¶ï¼Œé¢„æœŸçš„å®¹å¿è¡Œä¸ºä¸å®é™…ä¸ç¬¦ï¼Œå¯èƒ½ä¼šå½±å“è°ƒåº¦ã€‚ä½†è¿™å±äºæ–‡æ¡£å’ŒåŠŸèƒ½å®ç°ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œä¸æ¶‰åŠæ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬6æ¡ï¼Œå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

---

## Issue #126785 e2epod.DeletePodWithWait{,ByName} does not handle pods that get restarted

- Issue é“¾æ¥ï¼š[#126785](https://github.com/kubernetes/kubernetes/issues/126785)

### Issue å†…å®¹

#### What happened?

`e2epod.DeletePodWithWaitByName` does (simplified)

```
        err := c.CoreV1().Pods(podNamespace).Delete(ctx, podName, metav1.DeleteOptions{})
        err = WaitForPodNotFoundInNamespace(ctx, c, podName, podNamespace, PodDeleteTimeout)
```

If the pod in question is managed by a controller or operator, then it is possible that it will be restarted in between the Delete and the WaitForPodNotFoundInNamespace, in which case WaitForPodNotFoundInNamespace will wait for the _new_ pod to not be present rather than the old one.

eg, some of the ovn-kubernetes e2e tests use this function to restart ovn-k control-plane pods in various circumstances (the e2e test kills a pod which is managed by a StatefulSet, knowing it will be restarted), and sometimes hits the race condition. eg, [here](https://github.com/ovn-org/ovn-kubernetes/actions/runs/10451772693/job/28939840358?pr=4623):

```
2024-08-19T11:40:56.3501692Z   I0819 11:40:56.349677 79626 delete.go:62] Deleting pod "ovnkube-db-2" in namespace "ovn-kubernetes"
2024-08-19T11:40:56.3606091Z   I0819 11:40:56.360068 79626 delete.go:70] Wait up to 5m0s for pod "ovnkube-db-2" to be fully deleted
...
2024-08-19T11:45:57.4798660Z   [FAILED] failed to delete pod ovnkube-db-2: pod "ovnkube-db-2" was not deleted: expected pod to not be found: Timed out after 300.001s.
2024-08-19T11:45:57.4799987Z   Expected
2024-08-19T11:45:57.4800476Z       <*v1.Pod | 0xc000eba488>: 
2024-08-19T11:45:57.4801044Z           metadata:
2024-08-19T11:45:57.4801863Z             creationTimestamp: "2024-08-19T11:41:00Z"
...
2024-08-19T11:45:57.5299664Z   to be nil
```

note that the `creationTimestamp` of the pod in question is _after_ the deletion.

#### What did you expect to happen?

Either

1. DeletePodWithWait{,ByName} checks the `resourceVersion` of the pod before deleting it, and waits specifically for _that_ version of the pod to not exist any more, or
2. DeletePodWithWait{,ByName} are documented as not handling this case

#### How can we reproduce it (as minimally and precisely as possible)?

Write an e2e test that deletes a pod that will be recreated automatically (by an `apps` API or by an operator).

#### Anything else we need to know?

/sig testing
/area e2e-test-framework

#### Kubernetes version

`v1.30.2`


#### Cloud provider

N/A


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¿™ä¸ªIssueæè¿°äº†åœ¨ä½¿ç”¨`e2epod.DeletePodWithWait{,ByName}`å‡½æ•°åˆ é™¤ä¸€ä¸ªç”±æ§åˆ¶å™¨æˆ–æ“ä½œå™¨ï¼ˆå¦‚StatefulSetï¼‰ç®¡ç†çš„Podæ—¶ï¼Œå¯èƒ½å‡ºç°çš„ç«æ€æ¡ä»¶é—®é¢˜ã€‚å…·ä½“è€Œè¨€ï¼Œå½“åˆ é™¤æ“ä½œæ‰§è¡Œåï¼Œæ§åˆ¶å™¨å¯èƒ½ä¼šç«‹å³é‡å»ºè¯¥Podï¼Œå¯¼è‡´`WaitForPodNotFoundInNamespace`å‡½æ•°ç­‰å¾…çš„æ–°Podè€ŒéåŸå…ˆçš„Podè¢«åˆ é™¤ã€‚è¿™å¯èƒ½å¯¼è‡´e2eæµ‹è¯•ç”¨ä¾‹æ— æ³•æ­£ç¡®åˆ¤æ–­Podçš„åˆ é™¤çŠ¶æ€ï¼Œå‡ºç°è¶…æ—¶æˆ–é”™è¯¯ã€‚

ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¿™ä¸ªé—®é¢˜ä¸»è¦å½±å“çš„æ˜¯æµ‹è¯•æ¡†æ¶çš„ç¨³å®šæ€§å’Œå‡†ç¡®æ€§ï¼Œå¹¶ä¸æ¶‰åŠæ½œåœ¨çš„å®‰å…¨é£é™©ã€‚è¯¥é—®é¢˜ä¸ä¼šè¢«æ”»å‡»è€…åˆ©ç”¨æ¥è¿›è¡Œæ”»å‡»ï¼Œä¹Ÿä¸ä¼šå¯¼è‡´ç³»ç»Ÿçš„å®‰å…¨æ¼æ´ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1. è¯¥é£é™©ä¸èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨ã€‚
2. ä¸ä¼šæˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œä¹Ÿä¸ä¼šè¢«åˆ†é…CVEç¼–å·ï¼Œä½¿ç”¨CVSS 3.1è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœåœ¨highä»¥ä¸‹ã€‚
6. Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œé£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

---

## Issue #126778 kubectl exec to pods causes unexpected exit 0 in concurrence with jobs running

- Issue é“¾æ¥ï¼š[#126778](https://github.com/kubernetes/kubernetes/issues/126778)

### Issue å†…å®¹

#### What happened?

We are having our EKS cluster and Node Groups working fine in our environment however whenever we are logged into a pod of our cluster then we are getting logged out after an hour automatically - this is happening when we are running batch using some scripts :-
Screen Shots -
EKS Version - 1.28
![image](https://github.com/user-attachments/assets/ddace079-ef8e-498a-80ff-954d63bdcf9f)

EKS Cluster
![image](https://github.com/user-attachments/assets/96265b59-9926-4ee2-8c9c-4c3c10921a23)
When we are logged in to a pod
![image](https://github.com/user-attachments/assets/e532bf77-abad-4d8c-b518-b4bac6c6f96b)

Kubectl Version [Client Version: version.Info{Major:"1", Minor:"23+", GitVersion:"v1.23.17-eks-0a21954", GitCommit:"cd5c12c51b0899612375453f7a7c2e7b6563f5e9", GitTreeState:"clean", BuildDate:"2023-04-15T00:35:51Z", GoVersion:"go1.19.6", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"28+", GitVersion:"v1.28.11-eks-db838b0", GitCommit:"5ec1757cec753faac1ea8374cf6bfee9a8b35c2a", GitTreeState:"clean", BuildDate:"2024-06-27T19:09:54Z", GoVersion:"go1.21.11", Compiler:"gc", Platform:"linux/amd64"}]
![image](https://github.com/user-attachments/assets/d549880d-b912-4721-9ae3-864d6cd9d386)

Docker version
[root@ip-10-152-145-146 bin]# docker version
Client: Docker Engine - Community
 Version:           19.03.8
 API version:       1.40
 Go version:        go1.12.17
 Git commit:        afacb8b7f0
 Built:             Wed Mar 11 01:22:56 2020
 OS/Arch:           linux/amd64
 Experimental:      false
![image](https://github.com/user-attachments/assets/df8210aa-8f77-43bc-a3ff-3406b8ca690b)

 


#### What did you expect to happen?

We do not want the automated signing out from EKS pods

#### How can we reproduce it (as minimally and precisely as possible)?

We are having our EKS cluster and Node Groups working fine in our environment however whenever we are logged into a pod of our cluster then we are getting logged out after an hour automatically - this is happening when we are running batch using some scripts

#### Anything else we need to know?

_No response_

#### Kubernetes version

Kubernetes Version is 1.28
![image](https://github.com/user-attachments/assets/63344523-3209-452b-b8a2-8a54456d2527)
Public Cloud Platform is AWS

Kubectl Version [Client Version: version.Info{Major:"1", Minor:"23+", GitVersion:"v1.23.17-eks-0a21954", GitCommit:"cd5c12c51b0899612375453f7a7c2e7b6563f5e9", GitTreeState:"clean", BuildDate:"2023-04-15T00:35:51Z", GoVersion:"go1.19.6", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"28+", GitVersion:"v1.28.11-eks-db838b0", GitCommit:"5ec1757cec753faac1ea8374cf6bfee9a8b35c2a", GitTreeState:"clean", BuildDate:"2024-06-27T19:09:54Z", GoVersion:"go1.21.11", Compiler:"gc", Platform:"linux/amd64"}]
![image](https://github.com/user-attachments/assets/d549880d-b912-4721-9ae3-864d6cd9d386)

Docker version
[root@ip-10-152-145-146 bin]# docker version
Client: Docker Engine - Community
 Version:           19.03.8
 API version:       1.40
 Go version:        go1.12.17
 Git commit:        afacb8b7f0
 Built:             Wed Mar 11 01:22:56 2020
 OS/Arch:           linux/amd64
 Experimental:      false
![image](https://github.com/user-attachments/assets/df8210aa-8f77-43bc-a3ff-3406b8ca690b)


#### Cloud provider

AWS


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Amazon Linux"
VERSION="2"
ID="amzn"
ID_LIKE="centos rhel fedora"
VERSION_ID="2"
PRETTY_NAME="Amazon Linux 2"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2"
HOME_URL="https://amazonlinux.com/"
SUPPORT_END="2025-06-30"
$ uname -a
Linux ip-10-51-155-146.eu-west-1.compute.internal 5.10.214-202.855.amzn2.x86_64 #1 SMP Tue Apr 9 06:57:12 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux


#### Install tools

<details>


```
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨EKSé›†ç¾¤ä¸­ï¼Œä½¿ç”¨kubectl execç™»å½•åˆ°Podåï¼Œè¿è¡Œæ‰¹å¤„ç†è„šæœ¬æ—¶ï¼Œä¼šåœ¨ä¸€å°æ—¶åè‡ªåŠ¨é€€å‡ºã€‚è¿™å¯èƒ½æ˜¯ç”±äºä¼šè¯è¶…æ—¶ã€ç½‘ç»œæ–­å¼€ã€èµ„æºé™åˆ¶æˆ–Podæœ¬èº«çš„ç”Ÿå‘½å‘¨æœŸå¯¼è‡´çš„ã€‚æ ¹æ®æä¾›çš„ä¿¡æ¯ï¼Œæ­¤é—®é¢˜å±äºæ“ä½œä½¿ç”¨æˆ–é…ç½®é—®é¢˜ï¼Œæ²¡æœ‰è¯æ®è¡¨æ˜å­˜åœ¨å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨é£é™©ï¼Œä¹Ÿæ²¡æœ‰æ¶‰åŠé«˜å±æ¼æ´ã€‚æŒ‰ç…§é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126770 client-go ListWatch may only get partial items when user provide a limit option

- Issue é“¾æ¥ï¼š[#126770](https://github.com/kubernetes/kubernetes/issues/126770)

### Issue å†…å®¹

#### What happened?

There are some situations user wants to always list resources from etcd. eg, with no limit option, the server may return full list from apiserver cache, if there are lots of resources, there may be a 60s timeout of apiserver, and user can provide a limit=500 option to force request go through etcd with pagination.

But the following codes have a problem, when an expired error occurs, it will try to do a full list, but user code update the limit options in `TweakListOptionsFunc`, so this will get only 500 items. In another word, currently, client-go do not support user provided limit option.

```
func (p *ListPager) List(ctx context.Context, options metav1.ListOptions) (runtime.Object, bool, error) {
	if options.Limit == 0 {
		options.Limit = p.PageSize
	}
       // ...
	for {
		select {
		case <-ctx.Done():
			return nil, paginatedResult, ctx.Err()
		default:
		}
		obj, err := p.PageFn(ctx, options)
		if err != nil {
			if !errors.IsResourceExpired(err) || !p.FullListIfExpired || options.Continue == "" {
				return nil, paginatedResult, err
			}
			options.Limit = 0
			options.Continue = ""
			options.ResourceVersion = requestedResourceVersion
			options.ResourceVersionMatch = requestedResourceVersionMatch
			result, err := p.PageFn(ctx, options)
			return result, paginatedResult, err
		}
```

#### What did you expect to happen?

Support user supplied limit option, so there a way user can force request go through etcd, to avoid apiserver 60s timeout problem, this is very import for long running controllers, there controllers usually do very small amount list operations. to some extent, this will also help to reduce apiserver memory usage.

#### How can we reproduce it (as minimally and precisely as possible)?

1. create a 50000 pods
2. start a informer with with `TweakListOptionsFunc` and set option.Limit to 500, and set option.ResourceVersion to "" if it is "0" (because apiserver dosn't honor limit when resourceVersion is "0")
3. when the list in operating (we can see lots of pagination request), excute etcd compaction command: `etcdctl compaction $(etcdctl endpoint status --write-out="json"  | jq '.[0].Status.header.revision' | tr -d '\n')`

#### Anything else we need to know?

_No response_

#### Kubernetes version

I think it is for all versions


#### Cloud provider

I think all clouds


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ç»è¿‡åˆ†æï¼Œè¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨client-goçš„ListWatchåŠŸèƒ½æ—¶ï¼Œå½“ç”¨æˆ·æä¾›äº†limité€‰é¡¹ï¼Œåœ¨èµ„æºè¿‡æœŸé”™è¯¯å‘ç”Ÿæ—¶ï¼Œå®¢æˆ·ç«¯ä¼šå°è¯•è¿›è¡Œå…¨é‡åˆ—è¡¨æ“ä½œï¼Œä½†ç”±äºç”¨æˆ·åœ¨`TweakListOptionsFunc`ä¸­æ›´æ–°äº†limité€‰é¡¹ï¼Œå¯¼è‡´æœ€ç»ˆåªèƒ½è·å–éƒ¨åˆ†æ•°æ®ã€‚è¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§çš„é—®é¢˜ï¼Œå¯èƒ½å¯¼è‡´æ•°æ®è·å–ä¸å®Œæ•´ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤é—®é¢˜å±äºåŠŸèƒ½æ€§ç¼ºé™·ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚

1. **æ”»å‡»è€…æ— æ³•åˆ©ç”¨**ï¼šè¯¥é—®é¢˜ä¸ä¼šè¢«æ”»å‡»è€…åˆ©ç”¨æ¥è¿›è¡Œæœªç»æˆæƒçš„æ“ä½œã€‚
2. **ä¸å¯èƒ½æˆä¸ºæ¼æ´**ï¼šæ­¤é—®é¢˜ä¸ä¼šå¯¼è‡´ç³»ç»Ÿçš„æœºå¯†æ€§ã€å®Œæ•´æ€§å’Œå¯ç”¨æ€§å—åˆ°ç ´åï¼Œæ— æ³•åˆ†é…CVEç¼–å·ï¼ŒæŒ‰ç…§CVSS 3.1è¯„åˆ†æ ‡å‡†æ‰“åˆ†ç»“æœä¹Ÿä¸ä¼šåœ¨Highä»¥ä¸Šã€‚
3. **ä¸æ¶‰åŠæ•æ„Ÿä¿¡æ¯æ³„éœ²**ï¼šæ²¡æœ‰æ•æ„Ÿä¿¡æ¯çš„æš´éœ²æˆ–æƒé™æå‡çš„é£é™©ã€‚

å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126747 Bogus prohibition of `uniqueItems` in JSON schema in CRD

- Issue é“¾æ¥ï¼š[#126747](https://github.com/kubernetes/kubernetes/issues/126747)

### Issue å†…å®¹

#### What happened?

https://github.com/kubernetes/apiextensions-apiserver/blob/v0.31.0/pkg/apis/apiextensions/validation/validation.go#L1007-L1009 has been there forever but makes no sense to me. Checking that constraint can be done in O(N) time and O(N) space. And I _can_ impose the functionally same constraint on a slice by setting `+listType=map` and using the corresponding (collection of) `+listMapKey` settings.

#### What did you expect to happen?

I expected that I can use `uniqueItems`.

#### How can we reproduce it (as minimally and precisely as possible)?

_I_ produced it while using kubebuilder. I put the `// +kubebuilder:validation:UniqueItems=true` comment on a field holding a slice (the `Destinations` field in `BindingSpec` in https://github.com/kubestellar/kubestellar/pull/2405).

#### Anything else we need to know?

ğŸ¤· 

#### Kubernetes version

<details>

This has been there _forever_.

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

N/A

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueè®¨è®ºäº†åœ¨Kubernetesçš„CRDï¼ˆè‡ªå®šä¹‰èµ„æºå®šä¹‰ï¼‰ä¸­ï¼ŒJSON Schemaä¸­ç¦æ­¢ä½¿ç”¨`uniqueItems`çš„é—®é¢˜ã€‚æäº¤è€…è®¤ä¸ºè¿™ä¸ªé™åˆ¶ä¸åˆç†ï¼Œå› ä¸º`uniqueItems`å¯ä»¥åœ¨O(N)æ—¶é—´å’ŒO(N)ç©ºé—´å¤æ‚åº¦ä¸‹æ£€æŸ¥æ•°ç»„ä¸­çš„å…ƒç´ å”¯ä¸€æ€§ï¼Œè€Œä¸”å¯ä»¥é€šè¿‡è®¾ç½®`+listType=map`å’Œ`+listMapKey`æ¥å®ç°ç›¸åŒçš„åŠŸèƒ½ã€‚

ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¯¥Issueå¹¶æœªæ¶‰åŠä»»ä½•æ½œåœ¨çš„å®‰å…¨é£é™©ï¼š

1. æ²¡æœ‰æåˆ°ä»»ä½•å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„æ¼æ´ã€‚
2. è¯¥é—®é¢˜ä¸»è¦æ˜¯å…³äºCRDä¸­JSON SchemaéªŒè¯è§„åˆ™çš„è®¾è®¡ï¼Œä¸æ¶‰åŠæƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œç­‰é«˜é£é™©é—®é¢˜ã€‚
3. æ²¡æœ‰æ¶‰åŠæ—¥å¿—æ³„éœ²ã€æ•æ„Ÿä¿¡æ¯æš´éœ²ç­‰å®‰å…¨é—®é¢˜ã€‚
4. æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126934 Kube-proxy conntrack logic does not consider Service traffic topology

- Issue é“¾æ¥ï¼š[#126934](https://github.com/kubernetes/kubernetes/issues/126934)

### Issue å†…å®¹

_Originally posted by @adrianmoisey in https://github.com/kubernetes/kubernetes/issues/126130#issuecomment-2301842312_
            

An UDP Service with internalTrafficPolicy set to Local leaves stale conntrack entries after the endpoint is deleted.

This happens because the Service is deployed as a daemonset with one endpoint per node.

When the endpoint is deleted, the endpoint conntrack entry is deleted but , when the new endpoint is added it should delete the Service ClusterIP entry because the Service effectively went from 0 endpoints in the node (InternalTrafficPolicy Local) to 1 endpoint.

However, the kube-proxy logic to detect stale entries does not have into consideration any of the traffic distribution features for Services, making wrong assumptions because of this and leaving stale entries

https://github.com/kubernetes/kubernetes/blob/619b0059cf8c224461cb411fda090c8ff25d9f71/pkg/proxy/endpointschangetracker.go#L290-L350

Kudos to @adrianmoisey for the thorough investigation and reproducer

/sig network
/kind bug
/assign @aojea

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetesä¸­ï¼Œå½“UDP Serviceçš„internalTrafficPolicyè®¾ç½®ä¸ºLocalæ—¶ï¼Œkube-proxyçš„conntracké€»è¾‘æœªè€ƒè™‘æœåŠ¡æµé‡æ‹“æ‰‘ï¼Œå¯¼è‡´åœ¨åˆ é™¤endpointåé—ç•™äº†è¿‡æœŸçš„conntrackæ¡ç›®ã€‚æ­¤é—®é¢˜å¯èƒ½å¯¼è‡´æœåŠ¡çš„è¿æ¥è·Ÿè¸ªä¿¡æ¯ä¸æ­£ç¡®ï¼Œå½±å“æœåŠ¡çš„å¯ç”¨æ€§ã€‚ä½†è¿™ç§å½±å“ä»…é™äºæœåŠ¡çš„æ­£ç¡®æ€§å’Œå¯ç”¨æ€§ï¼Œæ²¡æœ‰æ¶‰åŠæ½œåœ¨çš„å®‰å…¨é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤é—®é¢˜ä¸ä¼šè¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œä¹Ÿä¸ä¼šå¯¼è‡´é«˜é£é™©çš„æ¼æ´ï¼Œæ•…é£é™©è¯„çº§ä¸ºä¸æ¶‰åŠã€‚

---

## Issue #126929 Encryption at rest KMS plugin receives invalid authority headers from k8s grpc client

- Issue é“¾æ¥ï¼š[#126929](https://github.com/kubernetes/kubernetes/issues/126929)

### Issue å†…å®¹

#### What happened?

KMS Plugins require communication via grpc UDS. Currently, the k8s grpc client sets the authority header to the socket path which is marked as invalid preventing successful socket communication. 

#### What did you expect to happen?

I expect a custom KMS provider plugin to be sent valid authority headers so that successful communication can be established between the k8s and grpc client/server.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a grpc server according to the [k8s documentation on kms v2](https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/#configuring-the-kms-provider-kms-v2) and attempt to establish socket communication.

#### Anything else we need to know?

This issue has been resolved in [this PR](https://github.com/kubernetes/kubernetes/pull/112597) for device plugins and CSI. The same solution will likely fix this issue as well. I will create a PR for this and link it here after.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.4+k3s1
```

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

$ uname -a
Linux computer-XPS-15-9530 6.5.0-44-generic #44~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Jun 18 14:36:16 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨KMSæ’ä»¶ä½¿ç”¨gRPC UDSé€šä¿¡æ—¶ï¼ŒKubernetesçš„gRPCå®¢æˆ·ç«¯å°†authorityå¤´è®¾ç½®ä¸ºsocketè·¯å¾„ï¼Œè¿™æ˜¯æ— æ•ˆçš„ï¼Œå¯¼è‡´æ— æ³•æˆåŠŸå»ºç«‹é€šä¿¡ã€‚è¿™ä¸ªé—®é¢˜å½±å“äº†KMSæ’ä»¶å’ŒgRPCå®¢æˆ·ç«¯/æœåŠ¡å™¨ä¹‹é—´çš„æ­£å¸¸é€šä¿¡ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥é—®é¢˜å±äºåŠŸèƒ½æ€§ç¼ºé™·æˆ–é…ç½®é”™è¯¯ï¼Œå¹¶æœªæåŠä»»ä½•å¯èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨é£é™©ã€‚æ²¡æœ‰è¿¹è±¡è¡¨æ˜æ­¤é—®é¢˜ä¼šå¯¼è‡´å®‰å…¨æ¼æ´ï¼Œä¹Ÿä¸ä¼šè¢«åˆ†é…CVEç¼–å·ã€‚å› æ­¤ï¼Œæ­¤é—®é¢˜ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126919 StatefulSet PersistentVolumeClaimRetentionPolicy not deleting pods when `WhenScaled: Delete` is used

- Issue é“¾æ¥ï¼š[#126919](https://github.com/kubernetes/kubernetes/issues/126919)

### Issue å†…å®¹

#### What happened?

When OwnerReferencesPermissionEnforcement admission plugin is used the PVCs are not deleted and KCM logs show

```
stateful_set.go:438] "Unhandled Error" err="error syncing StatefulSet e2e-statefulset-1256/ss, requeuing: could not update claim datadir-ss-2 for delete policy ownerRefs: persistentvolumeclaims \"datadir-ss-2\" is forbidden: cannot set blockOwnerDeletion if an ownerReference refers to a resource you can't set finalizers on: , <nil>" logger="UnhandledError"
```

#### What did you expect to happen?

PVCs should have ownerReferences set and the PVCs should be garbage collected on scale down

#### How can we reproduce it (as minimally and precisely as possible)?

3 of 6 of the following tests fail when OwnerReferencesPermissionEnforcement admission plugin is turned on
```
./hack/ginkgo-e2e.sh --ginkgo.focus=".*Non-retain StatefulSetPersistentVolumeClaimPolicy should.*" --ginkgo.v
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.8
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.32.0-alpha.0.233+b3c725627b15bb-dirty

```

</details>


#### Cloud provider

<details>
NA
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†å½“å¯ç”¨OwnerReferencesPermissionEnforcementå‡†å…¥æ’ä»¶æ—¶ï¼ŒStatefulSetåœ¨ç¼©å®¹æ—¶æ— æ³•æ­£ç¡®åˆ é™¤å…³è”çš„PersistentVolumeClaimï¼ˆPVCï¼‰ã€‚é”™è¯¯æ—¥å¿—æ˜¾ç¤ºï¼Œç”±äºæƒé™é™åˆ¶ï¼Œæ— æ³•è®¾ç½®PVCçš„ownerReferencesï¼Œå¯¼è‡´PVCæœªè¢«åˆ é™¤ã€‚è¿™æ˜¯ä¸€ä¸ªç”±äºæƒé™æ§åˆ¶å¯¼è‡´çš„åŠŸèƒ½æ€§é—®é¢˜ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1. è¯¥é—®é¢˜æ˜¯ç”±äºæƒé™é™åˆ¶å¯¼è‡´çš„èµ„æºæ¸…ç†å¤±è´¥ï¼Œä¸å­˜åœ¨è¢«æ”»å‡»è€…åˆ©ç”¨çš„é£é™©ã€‚
2. è¯¥é—®é¢˜ä¸ä¼šå¯¼è‡´æ¼æ´äº§ç”Ÿï¼Œä¸ä¼šè¢«åˆ†é…CVEç¼–å·ï¼Œä¸”æŒ‰ç…§CVSS 3.1è¯„åˆ†æ ‡å‡†ï¼Œé£é™©è¯„åˆ†åœ¨é«˜å±ä»¥ä¸‹ã€‚
3. ä¸æ¶‰åŠæ•æ„Ÿä¿¡æ¯æ³„éœ²ã€ä¸å½“æ“ä½œæˆ–é…ç½®ç­‰å®‰å…¨é£é™©ã€‚
4. è¯¥é—®é¢˜ä¸æ¶‰åŠæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ï¼Œä¸”å³ä½¿å­˜åœ¨èµ„æºæ³„éœ²é£é™©ï¼Œä¹Ÿéœ€è¦ç‰¹å®šæƒé™æ“ä½œï¼Œå½±å“æœ‰é™ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126908 kubernetes v1.31.0: TypedNewDelayingQueue should be renamed to NewTypedDelayingQueue

- Issue é“¾æ¥ï¼š[#126908](https://github.com/kubernetes/kubernetes/issues/126908)

### Issue å†…å®¹

#### What happened?

TypedNewDelayingQueue in k8s.io/client-go/util/workqueue does not follow the naming convention in the same package.


#### What did you expect to happen?

The naming pattern observed in k8s.io/client-go/util/workqueue suggests that TypedNewDelayingQueue should be renamed to NewTypedDelayingQueue.
Because we have NewTypedDelayingQueueWithConfig, and other functions are named in such rules, I think this is an obvious conversion tooling error and/or cut and paste error.

#### How can we reproduce it (as minimally and precisely as possible)?

There is no reproduction steps.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

kubernetes v1.31.0

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ­¤IssueæŠ¥å‘Šäº†kubernetesä¸­`k8s.io/client-go/util/workqueue`åŒ…å†…ä¸€ä¸ªå‡½æ•°å‘½åä¸ç¬¦åˆå‘½åçº¦å®šçš„é—®é¢˜ã€‚å…·ä½“è€Œè¨€ï¼Œå‡½æ•°`TypedNewDelayingQueue`åº”è¯¥é‡å‘½åä¸º`NewTypedDelayingQueue`ï¼Œä»¥ç¬¦åˆè¯¥åŒ…å†…å…¶ä»–å‡½æ•°çš„å‘½åè§„åˆ™ã€‚è¿™æ˜¯ä¸€ä¸ªä»£ç é£æ ¼å’Œå‘½åè§„èŒƒçš„é—®é¢˜ï¼Œä¸æ¶‰åŠä»»ä½•å®‰å…¨é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬6æ¡ï¼Œå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚å› æ­¤ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126906 Get call to a non-existent namespace returns not-nil object when using the fake client

- Issue é“¾æ¥ï¼š[#126906](https://github.com/kubernetes/kubernetes/issues/126906)

### Issue å†…å®¹

#### What happened?

When using client-go fake client to Get a resource (in this case, a namespace) that does not exist, the Get call is returning a non nil object along with the not found error.

#### What did you expect to happen?

If the resource does not exist, I would expect the returned object to be nil.

#### How can we reproduce it (as minimally and precisely as possible)?

```	
	client := fake.NewSimpleClientset()
	ns, err := client.CoreV1().Namespaces().Get(context.Background(), "some-namespace", metav1.GetOptions{})
	if err != nil {
		fmt.Println(err)
	}

	if ns != nil {
		fmt.Println("should not have happened")
	}
	fmt.Println(ns)
```

- When run with v0.31.0:
```
namespaces "some-namespace" not found
should not have happened
&Namespace{ObjectMeta:{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[] map[] [] [] []},Spec:NamespaceSpec{Finalizers:[],},Status:NamespaceStatus{Phase:,Conditions:[]NamespaceCondition{},},}
```

- When run with v0.30.3:
```
namespaces "some-namespace" not found
nil
```

#### Anything else we need to know?

This seems to be an issue specific to client-go version v0.31.0.  The exact same snippet pasted above, when run with v0.30.3 behaves as expected.

#### Kubernetes version

<details>
The kubectl version is not really relevant.  The issue is specific to v0.31.0.  The issue is *not* observed in v0.30.3.
</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
$ uname -a
Darwin MW6YR2N7X4 23.6.0 Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:30 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6000 arm64

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueå†…å®¹ï¼Œè¯¥é—®é¢˜æ¶‰åŠåˆ°åœ¨ä½¿ç”¨client-goçš„fake clientæ—¶ï¼Œå½“è·å–ä¸€ä¸ªä¸å­˜åœ¨çš„èµ„æºï¼ˆå¦‚namespaceï¼‰æ—¶ï¼Œè¿”å›äº†ä¸€ä¸ªénilå¯¹è±¡ä»¥åŠä¸€ä¸ª"not found"é”™è¯¯ã€‚åœ¨ä¹‹å‰çš„ç‰ˆæœ¬ä¸­ï¼ˆv0.30.3ï¼‰ï¼Œç›¸åŒçš„æ“ä½œä¼šè¿”å›ä¸€ä¸ªnilå¯¹è±¡ã€‚

è¿™ä¸ªé—®é¢˜ä¸»è¦å½±å“å¼€å‘è€…åœ¨æµ‹è¯•ç¯å¢ƒä¸‹çš„ä»£ç è¡Œä¸ºï¼Œå¯èƒ½å¯¼è‡´åœ¨æµ‹è¯•æ—¶å¯¹è¿”å›å¯¹è±¡çš„æ£€æŸ¥å‡ºç°è¯¯åˆ¤ã€‚ç„¶è€Œï¼Œåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œä½¿ç”¨çš„æ˜¯çœŸå®çš„clientï¼Œè€Œä¸æ˜¯fake clientï¼Œå› æ­¤ç”Ÿäº§ç¯å¢ƒçš„è¡Œä¸ºä¸ä¼šå—åˆ°å½±å“ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é£é™©ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ­¤é—®é¢˜ä»…å½±å“æµ‹è¯•ç¯å¢ƒä¸‹çš„fake clientè¡Œä¸ºï¼Œæ”»å‡»è€…æ— æ³•åˆ©ç”¨ã€‚
2. **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä½¿ç”¨CVSS 3.1è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœè¦åœ¨highä»¥ä¸Š**ï¼šæ­¤é—®é¢˜ä¸ä¼šå¯¼è‡´å®‰å…¨æ¼æ´ï¼Œä¸ä¼šè¢«åˆ†é…CVEç¼–å·ã€‚
6. **å¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠ**ï¼šå› æ­¤ï¼Œæ­¤Issueé£é™©è¯„çº§ä¸ºä¸æ¶‰åŠã€‚

---

## Issue #126885 e2e can't assert inside wait loops

- Issue é“¾æ¥ï¼š[#126885](https://github.com/kubernetes/kubernetes/issues/126885)

### Issue å†…å®¹

https://github.com/kubernetes/kubernetes/blob/1e827f4b2a46981e4f3056b54b43363e787bbaaa/test/e2e/network/kube_proxy.go#L364-L366


seen in this https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/kubernetes-sigs_cloud-provider-kind/120/pull-cloud-provider-kind-conformance-parallel-ga-only/1826605298777853952

```
0822 13:27:52.097309 76548 builder.go:135] rc: 7
I0822 13:27:52.611009 76548 kube_proxy.go:362] Unexpected error: 
    <exec.CodeExitError>: 
    error running /home/prow/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://127.0.0.1:42531 --kubeconfig=/root/.kube/kind-test-config --namespace=kube-system exec grab-kube-proxy-metrics-9275 -- /bin/sh -x -c curl --silent 127.0.0.1:10249/metrics:
    Command stdout:
    
    stderr:
    + curl --silent 127.0.0.1:10249/metrics
    command terminated with exit code 7
    
    error:
    exit status 7
    {
        Err: <*errors.errorString | 0xc000c911e0>{
            s: "error running /home/prow/go/src/k8s.io/kubernetes/_output/bin/kubectl --server=https://127.0.0.1:42531 --kubeconfig=/root/.kube/kind-test-config --namespace=kube-system exec grab-kube-proxy-metrics-9275 -- /bin/sh -x -c curl --silent 127.0.0.1:10249/metrics:\nCommand stdout:\n\nstderr:\n+ curl --silent 127.0.0.1:10249/metrics\ncommand terminated with exit code 7\n\nerror:\nexit status 7",
        },
        Code: 7,
    }
E0822 13:27:52.611557   76548 core_dsl.go:427] "Observed a panic" panic=<
	ï¿½[1mï¿½[38;5;9mYour Test Panickedï¿½[0m
	ï¿½[38;5;243mk8s.io/kubernetes/test/e2e/network/kube_proxy.go:362ï¿½[0m
	  When you, or your assertion library, calls Ginkgo's Fail(),
	  Ginkgo panics to prevent subsequent assertions from running.
	
	  Normally Ginkgo rescues this panic so you shouldn't see it.
	
	  However, if you make an assertion in a goroutine, Ginkgo can't capture the
	  panic.
	  To circumvent this, you should call
	
	  	defer GinkgoRecover()
	
	  at the top of the goroutine that caused this panic.
	
	  Alternatively, you may have made an assertion outside of a Ginkgo
	  leaf node (e.g. in a container node or some out-of-band function) - please
	  move your assertion to
	  an appropriate Ginkgo node (e.g. a BeforeSuite, BeforeEach, It, etc...).
	
	  ï¿½[1mLearn more at:ï¿½[0m

```

inside a `wait.Poll` like loop we execute a function that return a bool and an error.

If we want to fail fast we return the error directly and assert on the output of the wait function.
If we want to retry we log the error and return false nil

/kind flake
/kind bug
/sig testing
/sig network

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueè®¨è®ºçš„æ˜¯åœ¨Kubernetesçš„E2Eæµ‹è¯•ä»£ç ä¸­ï¼Œåœ¨`wait.Poll`å¾ªç¯å†…æ‰§è¡Œæ–­è¨€æ—¶å‡ºç°çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå½“åœ¨`wait.Poll`å¾ªç¯å†…è°ƒç”¨æ–­è¨€ä¼šå¯¼è‡´Ginkgoçš„panicæ— æ³•è¢«æ­£å¸¸æ•è·ã€‚è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ä»£ç ä¸­çš„é—®é¢˜ï¼Œæ¶‰åŠæµ‹è¯•æ¡†æ¶çš„ä½¿ç”¨æ–¹å¼ï¼Œå¹¶ä¸æ¶‰åŠä»»ä½•å®‰å…¨é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥é—®é¢˜ä¸å±äºå®‰å…¨é£é™©ï¼Œä¸ä¼šè¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œä¹Ÿä¸ä¼šå¯¼è‡´ä»»ä½•å®‰å…¨æ¼æ´ã€‚

---

## Issue #126918 APIServerTracing causing huge memory consumption/memory leak

- Issue é“¾æ¥ï¼š[#126918](https://github.com/kubernetes/kubernetes/issues/126918)

### Issue å†…å®¹

#### What happened?

Since updating from Kubernetes 1.26.* to 1.29.* we experiences OOM-Kills on control plane nodes.
After some investigation we found that adding `--feature-gates=APIServerTracing=false` to the `kube-apiserver` fixed the issue.
(Found this via pprof/heap of one apiserver: heap dump could be added if needed)

Before the update to 1.29.* the control planes were running with 4GB of memory with the same worklow in the cluster without issues. After the workarround using the feature gate the memory consumption of the whole control plane is not at ~2GB.

#### What did you expect to happen?

In case there is an issue with other components, memory consumption of the apiserver should not be that excessive.

#### How can we reproduce it (as minimally and precisely as possible)?

We just had the K8s cluster running and saw a permant increase in memory usage of the control plane nodes. And after some time (can not give a specific duration) the OS of the control plane node started to OOM kill processes.

#### Anything else we need to know?

Let us know if you need additional information.


#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.4
```

</details>


#### Cloud provider

on-premise; control-planes are running with 4 CPU cores, 8GB of memory and and NVME discs

#### OS version

<details>

```console
$ cat /etc/os-release 
PRETTY_NAME="Debian GNU/Linux 11 (bullseye)"
NAME="Debian GNU/Linux"
VERSION_ID="11"
VERSION="11 (bullseye)"
VERSION_CODENAME=bullseye
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ uname -a
Linux kube-master01-003 5.10.0-32-cloud-amd64 #1 SMP Debian 5.10.223-1 (2024-08-10) x86_64 GNU/Linux

```

</details>


#### Install tools

<details>

```console
$ kubeadm version -o yaml
clientVersion:
  buildDate: "2024-04-16T15:05:51Z"
  compiler: gc
  gitCommit: 55019c83b0fd51ef4ced8c29eec2c4847f896e74
  gitTreeState: clean
  gitVersion: v1.29.4
  goVersion: go1.21.9
  major: "1"
  minor: "29"
  platform: linux/amd64
```

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

```console
$ crictl version
Version:  0.1.0
RuntimeName:  containerd
RuntimeVersion:  1.6.31
RuntimeApiVersion:  v1
```

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†åœ¨å‡çº§Kubernetesç‰ˆæœ¬è‡³1.29.*åï¼Œå¯ç”¨`APIServerTracing`åŠŸèƒ½å¯¼è‡´`kube-apiserver`å†…å­˜æ¶ˆè€—å·¨å¤§ï¼Œæœ€ç»ˆå¼•å‘æ§åˆ¶å¹³é¢èŠ‚ç‚¹çš„OOM-Killé—®é¢˜ã€‚é€šè¿‡ç¦ç”¨`APIServerTracing`åŠŸèƒ½ï¼Œå†…å­˜æ¶ˆè€—æ¢å¤æ­£å¸¸ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šIssueä¸­æ²¡æœ‰æåŠæ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¯¥é—®é¢˜ã€‚
2. **é£é™©è¯„çº§**ï¼šæ²¡æœ‰è¯æ®è¡¨æ˜è¯¥é—®é¢˜ä¼šå¯¼è‡´é«˜å±æ¼æ´ï¼Œä¹Ÿæ²¡æœ‰è¢«åˆ†é…CVEç¼–å·çš„å¯èƒ½æ€§ã€‚
4. **æ‹’ç»æœåŠ¡æ”»å‡»**ï¼šè™½ç„¶å­˜åœ¨å†…å­˜æ¶ˆè€—è¿‡é«˜çš„é—®é¢˜ï¼Œä½†Issueä¸­æ²¡æœ‰è¯´æ˜æ”»å‡»è€…å¯ä»¥é€šè¿‡ç‰¹å®šæ“ä½œè§¦å‘å†…å­˜æ³„æ¼å¯¼è‡´DoSæ”»å‡»ã€‚
6. **ä¸æ¶‰åŠå®‰å…¨é—®é¢˜**ï¼šè¯¥Issueæ˜¯å…³äºç³»ç»Ÿæ€§èƒ½å’Œèµ„æºæ¶ˆè€—çš„Bugï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥é—®é¢˜å±äºæ€§èƒ½é—®é¢˜ï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #127004 Cannot mount the same PVC multiple times on one Pod

- Issue é“¾æ¥ï¼š[#127004](https://github.com/kubernetes/kubernetes/issues/127004)

### Issue å†…å®¹

#### What happened?

When I mount the same PVC multiple times on one Pod, the Pod is stuck in `containerCreating` state.

Same issue described here: https://stackoverflow.com/questions/65931457/why-cant-i-mount-the-same-pvc-twice-with-different-subpaths-to-single-pod

#### What did you expect to happen?

The mount of volumes should succeed, and the Pod should be created successfully.

Or, if Kubernetes treats this as a mis-configuration, a clear error message should be returned.

#### How can we reproduce it (as minimally and precisely as possible)?

I am testing on a GKE cluster. The same PVC is used by one Pod twice.

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-claim
spec:
  accessModes:
    - ReadWriteOnce  
  resources:
    requests:
      storage: 1Gi  
  storageClassName: standard  
---
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: my-container
    image: nginx  
    volumeMounts:
    - name: my-pvc
      mountPath: /data/path1
    - name: my-pvc-2
      mountPath: /data/path2
  volumes:
  - name: my-pvc
    persistentVolumeClaim:
      claimName: my-claim  
  - name: my-pvc-2
    persistentVolumeClaim:
      claimName: my-claim  
```

#### Anything else we need to know?

We have another similar issue documented: https://github.com/GoogleCloudPlatform/gcs-fuse-csi-driver/issues/48

According to the kubelet code: https://github.com/kubernetes/kubernetes/blob/8f15859afc9cfaeb05d4915ffa204d84da512094/pkg/kubelet/volumemanager/cache/desired_state_of_world.go#L296-L298

> For non-attachable and non-device-mountable volumes, generate a unique name based on the pod namespace and name and the name of the volume within the pod.

Since the user has two same PVCs specified on the Pod spec, the tow PVCs must bound to the same PV, meaning the `volumeHandle` is the same for the two volumes. Therefore, different volumes will be treated as the same volume. After kubelet mounts one of the volumes, the other volume will be treated as already mounted. As a result, the Pod will be stuck in volume mount stage.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.7
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.6-gke.1326000
```

Note that this issue is not limited to any specific k8s verisons.

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ç»è¿‡åˆ†æï¼Œè¯¥Issueæè¿°äº†åœ¨åŒä¸€ä¸ªPodä¸­å¤šæ¬¡æŒ‚è½½åŒä¸€ä¸ªPVCä¼šå¯¼è‡´Podå¡åœ¨`containerCreating`çŠ¶æ€çš„é—®é¢˜ã€‚è¿™æ˜¯ç”±äºKuberneteså¯¹åŒä¸€PVCåœ¨åŒä¸€Podä¸­çš„å¤šæ¬¡æŒ‚è½½å­˜åœ¨é™åˆ¶ï¼Œå¯¼è‡´Podæ— æ³•æ­£å¸¸åˆ›å»ºã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

- **ç¬¬3æ¡**ï¼šIssueæäº¤è€…åœ¨æäº¤å†…å®¹ä¸­æš´éœ²çš„é…ç½®é—®é¢˜ï¼Œä¸å±äºå®‰å…¨é£é™©ï¼Œè¿™æ˜¯é…ç½®ä½¿ç”¨ä¸å½“å¯¼è‡´çš„é—®é¢˜ã€‚
- **ç¬¬6æ¡**ï¼šå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126972 Watches are not drained during graceful termination when feature gate APIServingWithRoutine is on

- Issue é“¾æ¥ï¼š[#126972](https://github.com/kubernetes/kubernetes/issues/126972)

### Issue å†…å®¹

#### What happened?

I tried to observe the graceful termination of kube-apiserver, in particular, how watches are drained.
1) I created a cluster in 1.30 with kube-apiserver flags
```
--shutdown-delay-duration=10s --shutdown-send-retry-after=true --shutdown-watch-termination-grace-period=60s 
```
and with feature gate `APIServingWithRoutine` on.
Also, I added to kube-apiserver's manifest the following line
```
"terminationGracePeriodSeconds"=60,
```
2) I killed the kube-apiserver, and found the following log during graceful termination:
```
"[graceful-termination] active watch request(s) have drained" duration="1m0s" activeWatchesBefore=0 activeWatchesAfter=0 error=null
```
`activeWatchesBefore=0` is not the expected behavior.
Also, I do not observe any logs of watches being closed.

#### What did you expect to happen?

I expected a non-zero number of watches to be drained during graceful termination.
When I do the same procedure as above but for cluster 1.29.6 (or a cluster with feature gate `APIServingWithRoutine` off) I see a log similar to this one
```
"[graceful-termination] active watch request(s) have drained" duration="1m0s" activeWatchesBefore=623 activeWatchesAfter=0 error=null"
```
and, preceding it, there are logs of watches being closed (with latency in ~minutes)

#### How can we reproduce it (as minimally and precisely as possible)?

Create clusters in 1.30 with provided kube-apiserver flags and with feature gate `APIServingWithRoutine` on and off, respectively.

#### Anything else we need to know?

This issue seems to be related to:
- https://github.com/kubernetes/kubernetes/issues/125614

In particular, disabling the feature gate `APIServingWithRoutine` on the kube-apiserver leads to the correct behavior (watches are being drained as in 1.29 version)

#### Kubernetes version

Observed in 1.30.2+

#### Cloud provider

N/A


#### OS version

N/A

#### Install tools

N/A

#### Container runtime (CRI) and version (if applicable)

N/A


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

N/A

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨å¯ç”¨ç‰¹æ€§é—¨`APIServingWithRoutine`çš„æƒ…å†µä¸‹ï¼Œkube-apiserveråœ¨ä¼˜é›…å…³é—­æœŸé—´æ²¡æœ‰æ­£ç¡®åœ°æ¸…ç†watchè¿æ¥ï¼Œ`activeWatchesBefore=0`ï¼Œè¿™ä¸é¢„æœŸä¸ç¬¦ã€‚ç„¶è€Œï¼Œè¿™å±äºåŠŸèƒ½æ€§ç¼ºé™·æˆ–Bugï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚æ²¡æœ‰è¯æ®è¡¨æ˜æ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ­¤é—®é¢˜å®æ–½æ”»å‡»ï¼Œä¹Ÿä¸ä¼šå¯¼è‡´æƒé™æå‡ã€æ‹’ç»æœåŠ¡ç­‰å®‰å…¨é—®é¢˜ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬6æ¡ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ã€‚

---

## Issue #126965 kubelet fail to start on Windows since v1.31.0

- Issue é“¾æ¥ï¼š[#126965](https://github.com/kubernetes/kubernetes/issues/126965)

### Issue å†…å®¹

#### What happened?

Since I upgraded my kubernetes cluster from `v1.30.4` to `v1.31.0`, kubelet fails to restart on Windows.

The error messages in the logs are:
```
E0828 03:15:28.934935    5404 server.go:102] "Failed to listen to socket while starting device plugin registry" err="listen unix C:\\var\\lib\\kubelet\\device-plugins\\kubelet.sock: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted."
E0828 03:15:28.934935    5404 kubelet.go:1566] "Failed to start ContainerManager" err="listen unix C:\\var\\lib\\kubelet\\device-plugins\\kubelet.sock: bind: Only one usage of each socket address (protocol/network address/port) is normally permitted."
```

#### What did you expect to happen?

I expected kubelet to start even if the `kubelet.sock` file exists, this was the behavior in the previous versions.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Setup a Windows Kubernetes Node
2. Start kubelet.exe
3. Stop kubelet.exe
4. Start kubelet.exe again and watch it fail due to the kubelet.sock file already existing.

#### Anything else we need to know?

I think this Issue has to do with the commit 4060ee6 where socket files are not removed anymore causing the start to fail because the file already exists.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.31.0
```

</details>

#### Cloud provider

On Premise / No Cloud Provider

#### OS version

<details>

```console
C:\> wmic os get Caption,Version,BuildNumber,OSArchitecture
BuildNumber  Caption                                              OSArchitecture  Version
20348        Microsoft Windows Server 2022 Datacenter Evaluation  64-bit          10.0.20348
```

</details>

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

<details>

Containerd Version: 1.7.20

</details>

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Windowsç³»ç»Ÿä¸Šè¿è¡Œkubeletæ—¶ï¼Œå¦‚æœ`kubelet.sock`æ–‡ä»¶å·²å­˜åœ¨ï¼Œå¯¼è‡´kubeletæ— æ³•å¯åŠ¨çš„é—®é¢˜ã€‚è¿™æ˜¯ç”±äºkubeletåœ¨å¯åŠ¨æ—¶å°è¯•ç»‘å®šåˆ°å·²å­˜åœ¨çš„socketæ–‡ä»¶ï¼Œå› è€Œå‘ç”Ÿäº†é”™è¯¯ã€‚è¿™å±äºè½¯ä»¶çš„åŠŸèƒ½æ€§ç¼ºé™·ï¼Œå¯¼è‡´äº†æœåŠ¡ä¸å¯ç”¨ã€‚ä½†è¦åˆ©ç”¨è¯¥ç¼ºé™·ï¼Œæ”»å‡»è€…éœ€è¦å…·å¤‡åœ¨ç³»ç»Ÿä¸­åˆ›å»ºæˆ–åˆ é™¤`C:\var\lib\kubelet\device-plugins\kubelet.sock`æ–‡ä»¶çš„æƒé™ï¼Œè¿™é€šå¸¸éœ€è¦ç®¡ç†å‘˜æˆ–é«˜æƒé™ç”¨æˆ·æ‰èƒ½åšåˆ°ã€‚æ™®é€šéç‰¹æƒç”¨æˆ·æ— æ³•åœ¨è¯¥ç›®å½•ä¸‹åˆ›å»ºæ–‡ä»¶ã€‚å› æ­¤ï¼Œæ”»å‡»è€…æ— æ³•åˆ©ç”¨è¯¥é—®é¢˜è¿›è¡Œæ”»å‡»ï¼Œä¸”è¯¥é—®é¢˜ä¸æ¶‰åŠæ•æ„Ÿä¿¡æ¯æ³„éœ²ã€æƒé™æå‡ã€è¿œç¨‹ä»£ç æ‰§è¡Œç­‰å®‰å…¨é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œç¬¬4æ¡æŒ‡å‡ºï¼Œå½“æ”»å‡»è€…éœ€è¦åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126960 kube-proxy hope listen all zero addresses in dual-stack env without health check error

- Issue é“¾æ¥ï¼š[#126960](https://github.com/kubernetes/kubernetes/issues/126960)

### Issue å†…å®¹

#### What happened?

I have deployed a latest 1.31 Kubernetes dual-stack cluster, and cube-proxy is running in IPVS mode
Also created a dual-stack service with `externalTrafficPolicy: Local`:
```
  ...
  externalTrafficPolicy: Local
  healthCheckNodePort: 30256
  internalTrafficPolicy: Cluster
  ipFamilies:
    - IPv4
    - IPv6
  ipFamilyPolicy: PreferDualStack
  ...
```
In kube-proxyâ€™s configuration, â€”nodeport-addresses is unset. (we know this strongly is recommended in 1.31, but we donâ€™t expect to just listen on certain specific ip, just want to listen on all interface)

And the error in kube-proxy's log:
```
 "Opening healthcheck" service="default/service-1" port=30256
 "Opening healthcheck" service="default/service-1" port=30256
 "Failed to start healthcheck" err="listen tcp 0.0.0.0:30256: bind: address already in use" node="xxxxxxx" service="default/service-1" port=30256
```

And the cause of this problem is the way how kube-proxy handles health check nodePortAddresses in dual-stack.

There are some previous discussions about this issue:
https://github.com/kubernetes/kubernetes/issues/114702
https://github.com/kubernetes/kubernetes/issues/122899
But still can't solve our problem or meet our needs.

#### What did you expect to happen?

In [#123105](https://github.com/kubernetes/kubernetes/pull/123105), setting `â€”nodeport-addresses` to the value `"primary"`, which means only listening for NodePort connections on the node's primary IPv4 and/or IPv6 address, may solve the address problem. 

But in some scenarios (some users do have this kind of needs), we still hope to listen on all interfaces, and this is the current default behavior in IPVS mode, and when you create a dual-service, there is always the problem mentioned above.

Itâ€™s a quite long-standing question for kube-proxy, but still unsolved, since ipv6 "::" also includes ipv4 on Linux https://github.com/kubernetes/kubernetes/issues/122899, and then the â€œaddress already bindâ€ error seems to be inevitable, and in this situation, `â€”nodeport-addresses` is still helpless.

One possible solution I could figure out is:
When a dual-satck service change comes, if nodePortAddress have all zero addresses in IPv6 ip family, only open the healthcheck in IPv6's proxier.

But this may require a large change to the code: the IPv4 proxier also need to watch the change in IPv6, and pass relative parameters to its healthcheck server.

#### How can we reproduce it (as minimally and precisely as possible)?

In a dual-stack cluster environment, `--nodeport-addresses` is not configured.
Additionally, a dual-stack LoadBalancer Service with healthCheckNodePort is created.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# 1.31.0
```

</details>


#### Cloud provider

<details>
nil
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
é€šè¿‡å¯¹Issueå†…å®¹çš„åˆ†æï¼Œè¿™æ˜¯ä¸€ä¸ªå…³äºkube-proxyåœ¨åŒæ ˆï¼ˆIPv4å’ŒIPv6ï¼‰ç¯å¢ƒä¸‹ç›‘å¬æ‰€æœ‰æ¥å£æ—¶å‡ºç°å¥åº·æ£€æŸ¥é”™è¯¯çš„é—®é¢˜ã€‚å…·ä½“è€Œè¨€ï¼Œå½“æœªè®¾ç½®`--nodeport-addresses`å‚æ•°ä¸”åˆ›å»ºäº†å…·æœ‰`externalTrafficPolicy: Local`çš„åŒæ ˆæœåŠ¡æ—¶ï¼Œkube-proxyä¼šç”±äºåœ°å€å†²çªè€Œæ— æ³•å¯åŠ¨å¥åº·æ£€æŸ¥æœåŠ¡ï¼ˆå‡ºç°"address already in use"é”™è¯¯ï¼‰ã€‚è¿™ä¸€é—®é¢˜å¯¼è‡´å¥åº·æ£€æŸ¥æ— æ³•æ­£å¸¸å·¥ä½œï¼Œå¯èƒ½å½±å“æœåŠ¡çš„å¯ç”¨æ€§ã€‚

æ ¹æ®æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šè¯¥é—®é¢˜ä¸»è¦æ˜¯é…ç½®å’Œå®ç°ä¸Šçš„é—®é¢˜ï¼Œæ²¡æœ‰æåŠæ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¯¥é—®é¢˜è¿›è¡Œæ”»å‡»çš„é€”å¾„ã€‚

2. **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä½¿ç”¨CVSS 3.1è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœè¦åœ¨highä»¥ä¸Š**ï¼šæ­¤é—®é¢˜æœªæ¶‰åŠåˆ°æƒé™æå‡ã€æœªæˆæƒè®¿é—®ã€å‘½ä»¤æ‰§è¡Œç­‰é«˜é£é™©å®‰å…¨é—®é¢˜ï¼Œæ›´å¤šæ˜¯åŠŸèƒ½æ€§é”™è¯¯ã€‚

4. **åœ¨é£é™©ä¸ºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»æ—¶ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å¤Ÿå®æ–½è¯¥æ”»å‡»ï¼Œåˆ™è§†æƒ…å†µéœ€è¦é™çº§å¤„ç†**ï¼šè¯¥é—®é¢˜å¹¶éç”±æ”»å‡»è€…å¼•å‘çš„æ‹’ç»æœåŠ¡ï¼Œè€Œæ˜¯é…ç½®å¯¼è‡´çš„æœåŠ¡ä¸å¯ç”¨ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueä¸­æ²¡æœ‰æ¶‰åŠåˆ°å®‰å…¨é£é™©ã€‚

---

## Issue #126958 failed to sync secret/configmap cache: timed out waiting for the condition when WatchList feature is enable

- Issue é“¾æ¥ï¼š[#126958](https://github.com/kubernetes/kubernetes/issues/126958)

### Issue å†…å®¹

#### What happened?

In k8s 1.29 cluster, kubelet will report warning event about `failed to sync secret/configmap cache: timed out waiting for the condition` sometimes when create a pod. It was caused by following code:
https://github.com/kubernetes/kubernetes/blob/f1a922c8e6f951381450ee3c2922ca018f14a82e/pkg/kubelet/util/manager/watch_based_manager.go#L322

As the above code shows, secret/configmap cache should be synced during 1 second, but it may can't. When WatchList feature is enable, reflector need to receive a special bookmark to know whether the cache is synced. But in kube-apiserver:
https://github.com/kubernetes/kubernetes/blob/f1a922c8e6f951381450ee3c2922ca018f14a82e/staging/src/k8s.io/apiserver/pkg/storage/cacher/cacher.go#L1024

bookmark tick period is `1s to 1.25s`, so cache in reflect may can not be synced during `1s`.

Solutions to fix this issue:
1.   Shorten the bookmark tick time or Extended cache synced timeout
2.  Modify how bookmarks are sent in the WatchList feature, like https://github.com/Chaunceyctx/kubernetes/commit/3525f29bd383de59df69dff018641b18f382cdfd

After sending all items in watchCache store, we are able to send bookmark to client right now because store in watchCache is fresh enough which is guaranteed by:
https://github.com/kubernetes/kubernetes/blob/8486ed06200f019c65777e6b028f7a17299a8f85/staging/src/k8s.io/apiserver/pkg/storage/cacher/cacher.go#L1417

fix before:
```
I0828 10:50:16.052616 3101121 reflector.go:790] exiting secret3782178982722928679 Watch because received the bookmark that marks the end of initial events st ream, total 1 items received in 741.998362ms
I0828 10:50:34.191047 3101121 reflector.go:790] exiting secret5351765028251036481 Watch because received the bookmark that marks the end of initial events st ream, total 1 items received in 866.105471ms
I0828 10:50:45.713883 3101121 reflector.go:790] exiting secret8541420941097821224 Watch because received the bookmark that marks the end of initial events stream, total 1 items received in 380.334209ms
```

fix after:
```
I0828 10:55:27.530189 3107457 reflector.go:790] exiting secret778191698868956976 Watch because received the bookmark that marks the end of initial events str eam, total 1 items received in 13.256389ms
I0828 10:55:33.530398 3107457 reflector.go:790] exiting secret8547880469315277671 Watch because received the bookmark that marks the end of initial events st ream, total 1 items received in 8.192703ms
I0828 10:55:39.534974 3107457 reflector.go:790] exiting secret2690312991024336831 Watch because received the bookmark that marks the end of initial events st ream, total 1 items received in 8.180822ms
```
I prefer the latter. What's about experts' opinions? @wojtek-t @p0lyn0mial @liggitt 

#### What did you expect to happen?

secret/configmap cache becomes synced successfully duration 1s

#### How can we reproduce it (as minimally and precisely as possible)?

1. enable watchlist feature in client
2. change klog level to 4
3. create a reflector to watchlist some resources like pods/secrets
4. you will get a log message ` "exiting %v Watch because received the bookmark that marks the end of initial events stream, total %v items received in %v" ` see time cost

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†åœ¨ Kubernetes 1.29 é›†ç¾¤ä¸­ï¼Œå½“å¯ç”¨ WatchList åŠŸèƒ½å¹¶åˆ›å»º Pod æ—¶ï¼Œkubelet æœ‰æ—¶ä¼šæŠ¥å‘Šè­¦å‘Šäº‹ä»¶ï¼š`failed to sync secret/configmap cache: timed out waiting for the condition`ã€‚è¿™æ˜¯ç”±äºåœ¨å¯ç”¨ WatchList åŠŸèƒ½åï¼Œreflector éœ€è¦æ¥æ”¶åˆ°ä¸€ä¸ªç‰¹æ®Šçš„ bookmark æ‰èƒ½ç¡®è®¤ç¼“å­˜å·²åŒæ­¥ã€‚ä½† kube-apiserver å‘é€ bookmark çš„é—´éš”æ—¶é—´åœ¨ 1 ç§’åˆ° 1.25 ç§’ä¹‹é—´ï¼Œå¯¼è‡´ reflector å¯èƒ½æ— æ³•åœ¨é¢„æœŸçš„ 1 ç§’å†…å®ŒæˆåŒæ­¥ã€‚

è¿™ä¸ªé—®é¢˜çš„å½±å“æ˜¯å¯èƒ½å¯¼è‡´åŒæ­¥å»¶è¿Ÿå’Œè­¦å‘Šæ—¥å¿—äº§ç”Ÿï¼Œä½†å¹¶æœªå¯¹ç³»ç»Ÿçš„å®‰å…¨æ€§é€ æˆå½±å“ã€‚æ²¡æœ‰è¯æ®è¡¨æ˜æ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ­¤é—®é¢˜è¿›è¡Œæ”»å‡»ï¼Œä¹Ÿä¸å­˜åœ¨ä¿¡æ¯æ³„éœ²ã€æƒé™æå‡æˆ–æ‹’ç»æœåŠ¡ç­‰å®‰å…¨é£é™©ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1. è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨ï¼šä¸å­˜åœ¨æ”»å‡»è€…å¯åˆ©ç”¨çš„é£é™©ã€‚
2. è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é… CVE ç¼–å·ï¼Œä½¿ç”¨ CVSS 3.1 è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœè¦åœ¨ high ä»¥ä¸Šï¼šä¸ç¬¦åˆã€‚
6. å¦‚æœ Issue ä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

å› æ­¤ï¼Œè¯¥ Issue ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126951 `"failed to initialize nfacct client" err="nfacct sub-system not available"` logged when `k8s.io/kubernetes/pkg/proxy/metrics` module is initialized as non-root user

- Issue é“¾æ¥ï¼š[#126951](https://github.com/kubernetes/kubernetes/issues/126951)

### Issue å†…å®¹

#### What happened?

The `k8s.io/kubernetes/pkg/proxy/metrics` module now logs two errors during module initialization. Simply loading the module will cause two errors to be printed to stderr if the code is not run as root, or is run on a node without nfacct support. 

This is due to `newNFAcctMetricCollector` being used when creating Prometheus metrics during module variable initialization, which calls `klog.Errors()` if the nfacct client cannot be initialized.

This regression was introduced by:
* https://github.com/kubernetes/kubernetes/pull/125866

K3s ships all the various Kubernetes components bundled in a single multicall binary, which means that our `kubectl` now logs these errors when it is run as a non-root user.

#### What did you expect to happen?

No error logs on module init

#### How can we reproduce it (as minimally and precisely as possible)?

https://github.com/brandond/kube-proxy-repro

```golang
import (
	"k8s.io/kubernetes/cmd/kube-proxy/app"
)

func main() {
	_ = app.NewProxyCommand()
}
```

#### Anything else we need to know?

cc @aroradaman @danwinship

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0+k3s1
Kustomize Version: v5.4.2
Server Version: v1.31.0+k3s1
```

</details>


#### Cloud provider

n/a

#### OS version

n/a


#### Install tools

n/a

#### Container runtime (CRI) and version (if applicable)

n/a


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

n/a

area/proxy
sig/network

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®Issueçš„æè¿°ï¼Œ`kube-proxy`çš„metricsæ¨¡å—åœ¨érootç”¨æˆ·è¿è¡Œæ—¶ä¼šè®°å½•ä¸¤ä¸ªé”™è¯¯æ—¥å¿—ã€‚è¿™æ˜¯ç”±äºåœ¨æ¨¡å—å˜é‡åˆå§‹åŒ–æ—¶ï¼Œå¦‚æœnfacctå®¢æˆ·ç«¯æ— æ³•åˆå§‹åŒ–ï¼Œè°ƒç”¨äº†`klog.Errors()`å¯¼è‡´çš„ã€‚è¿™ç§è¡Œä¸ºå±äºæ—¥å¿—è®°å½•çš„é—®é¢˜ï¼Œå¯èƒ½ä¼šå¼•èµ·ä¸€äº›å›°æ‰°æˆ–ä¸å¿…è¦çš„æ—¥å¿—è¾“å‡ºï¼Œä½†å¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

æŒ‰ç…§é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1. è¯¥é£é™©æ— æ³•è¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œä¸ä¼šå¯¼è‡´æ”»å‡»è€…è·å–æ•æ„Ÿä¿¡æ¯æˆ–æå‡æƒé™ã€‚
2. è¯¥é—®é¢˜ä¸å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œä¹Ÿä¸ä¼šè¢«åˆ†é…CVEç¼–å·ï¼ŒCVSSè¯„åˆ†ä¹Ÿä¸ä¼šè¾¾åˆ°highä»¥ä¸Šã€‚
6. å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œé£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

---

## Issue #126947 Change kube-controller-manager flags documentation related to --service-account-private-key-file, remove outdated warnings during initialization & update documentation

- Issue é“¾æ¥ï¼š[#126947](https://github.com/kubernetes/kubernetes/issues/126947)

### Issue å†…å®¹

#### What happened?

The kube-controller-manager issues a warning during startup when `--use-service-account-credentials` is specified without providing a `--service-account-private-key-file`. This warning is misleading because the legacy service account token controller, which relies on the `--service-account-private-key-file`, is no longer required for most modern Kubernetes clusters unless explicitly needed for generating long-lived tokens. Additionally, the documentation does not clearly state the purpose of this flag or its impact on legacy token mechanisms.

#### What did you expect to happen?

I expected that the warning related to the `--service-account-private-key-file` would either not appear or be updated to reflect its relevance only to the legacy token mechanism. Furthermore, I expected the documentation to clearly explain that this flag is only necessary for enabling the legacy service account token controller and to provide updated guidance on generating long-lived tokens.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Start kube-controller-manager with the --use-service-account-credentials flag but without the --service-account-private-key-file flag.
2. Observe the warning message that is issued during initialization.
3. Review the current documentation for kube-controller-manager flags and the process for generating long-lived service account tokens.

#### Anything else we need to know?

[Slack Thread](https://kubernetes.slack.com/archives/C0EN96KUY/p1724435410277869) for context

#### Kubernetes version

<details>

```console
Client Version: v1.32.0-alpha.0.170+cb7b4ea648a97b-dirty
Kustomize Version: v5.4.2
Server Version: v1.32.0-alpha.0.22+09f025973a0c61

```

</details>


#### Cloud provider

<details>
Bare metal
</details>


#### OS version

<details>

```console
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ uname -a
Linux bastion 6.1.0-23-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.99-1 (2024-07-15) x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæå‡ºäº†`kube-controller-manager`åœ¨ä½¿ç”¨`--use-service-account-credentials`å‚æ•°è€Œæœªæä¾›`--service-account-private-key-file`æ—¶ä¼šäº§ç”Ÿè¯¯å¯¼æ€§çš„è­¦å‘Šä¿¡æ¯ã€‚è¿™ä¸ªè­¦å‘Šä¸é—ç•™çš„æœåŠ¡è´¦æˆ·ä»¤ç‰Œæ§åˆ¶å™¨æœ‰å…³ï¼Œä½†åœ¨ç°ä»£çš„Kubernetesé›†ç¾¤ä¸­ï¼Œè¯¥æ§åˆ¶å™¨é€šå¸¸ä¸å†éœ€è¦ï¼Œé™¤ééœ€è¦ç”Ÿæˆé•¿æœŸæœ‰æ•ˆçš„ä»¤ç‰Œã€‚

æ­¤é—®é¢˜ä¸»è¦æ¶‰åŠæ—¥å¿—è­¦å‘Šä¿¡æ¯å’Œæ–‡æ¡£çš„æ›´æ–°ï¼Œä»¥æ¶ˆé™¤ä¸å¿…è¦çš„è­¦å‘Šå¹¶æ¾„æ¸…å‚æ•°çš„ç”¨é€”ã€‚æ ¹æ®æä¾›çš„å†…å®¹ï¼Œæ²¡æœ‰æ¶‰åŠä»»ä½•å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨æ¼æ´æˆ–é…ç½®é”™è¯¯ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šä¸å­˜åœ¨å¯è¢«åˆ©ç”¨çš„é£é™©ã€‚
2. **é£é™©è¯„çº§åœ¨highä»¥ä¸Š**ï¼šä¸é€‚ç”¨ã€‚
6. **å¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠ**ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #126943 Extended resources could not be zeroed while node has been recreated

- Issue é“¾æ¥ï¼š[#126943](https://github.com/kubernetes/kubernetes/issues/126943)

### Issue å†…å®¹

#### What happened?

https://github.com/kubernetes/kubernetes/blob/7436ca32bc766ff202109a7541d2e7bb41ee7d13/pkg/kubelet/kubelet_node_status.go#L181

the `reconcileExtendedResource` check with devicemanager to see if there is no checkpoints to  decide whether to zero out ER, but while kubelet start, the checkpoint dir could not be empty as the kubelet.sock will be created first

https://github.com/kubernetes/kubernetes/blob/7436ca32bc766ff202109a7541d2e7bb41ee7d13/pkg/kubelet/checkpointmanager/checkpoint_manager.go#L105

the `ListCheckpoints ` list all file except ones which has prefix ".",  but not filter the sock files, so `kl.containerManager.ShouldResetExtendedResourceCapacity`  will always return false


#### What did you expect to happen?

the `ListCheckpoints ` should only list the real checkpoint file,  the kubelet_internal_checkpoint

#### How can we reproduce it (as minimally and precisely as possible)?

1. register a node with GPU 
2. recreate the node with a new vm
3. see the node status GPU resource,  not zerod out

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.28.x
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨èŠ‚ç‚¹é‡å»ºåï¼Œæ‹“å±•èµ„æºï¼ˆå¦‚GPUï¼‰æœªè¢«å½’é›¶çš„é—®é¢˜ã€‚åŸå› æ˜¯åœ¨åˆ¤æ–­æ˜¯å¦éœ€è¦å½’é›¶æ‹“å±•èµ„æºæ—¶ï¼Œ`ListCheckpoints`å‡½æ•°ä¼šåˆ—å‡ºé™¤äº†ä»¥`.`å¼€å¤´çš„æ–‡ä»¶ä¹‹å¤–çš„æ‰€æœ‰æ–‡ä»¶ï¼Œä½†æœªè¿‡æ»¤æ‰`sock`æ–‡ä»¶ï¼Œå¯¼è‡´`kl.containerManager.ShouldResetExtendedResourceCapacity`å§‹ç»ˆè¿”å›`false`ï¼Œä»è€Œæœªèƒ½æ­£ç¡®å½’é›¶èµ„æºã€‚è¿™ä¼šå¯¼è‡´è°ƒåº¦å™¨å°†éœ€è¦GPUèµ„æºçš„Podè°ƒåº¦åˆ°å®é™…ä¸Šä¸å…·å¤‡GPUçš„èŠ‚ç‚¹ä¸Šï¼Œé€ æˆèµ„æºè°ƒåº¦é”™è¯¯ã€‚ç„¶è€Œï¼Œè¿™å±äºåŠŸèƒ½æ€§ç¼ºé™·ï¼Œå½±å“ç³»ç»Ÿçš„å¯ç”¨æ€§ï¼Œä½†å¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œç¬¬6æ¡ï¼Œå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

---

