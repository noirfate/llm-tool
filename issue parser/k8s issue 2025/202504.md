# Issue å®‰å…¨åˆ†ææŠ¥å‘Š

> åˆ†ææ¨¡å‹ï¼šo1

# ğŸš¨ å­˜åœ¨é«˜é£é™©çš„ Issues (1 ä¸ª)

## Issue #131449 NFS PV does not respect `spec.mountOptions`.

- Issue é“¾æ¥ï¼š[#131449](https://github.com/kubernetes/kubernetes/issues/131449)

### Issue å†…å®¹

#### What happened?

When using a `pv` which contains `spec.mountOptions` to mount an NFS share, the mount options are not respected.  This can be seen with the following example.

```yaml
spec:
  accessModes:
  - ReadWriteMany
  capacity:
    storage: 1Gi
  claimRef:
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: stuff
    namespace: data
  mountOptions:
  - timeo=600
  - retrans=3
  - proto=tcp6
  - nfsvers=4.2
  - rsize=32768
  - wsize=32768
  - noacl
  - noatime
  - nodiratime
  - nolock
  - nosuid
  - nofail
  nfs:
    path: /data01/stuff
    server: server.example.com
  persistentVolumeReclaimPolicy: Retain
  volumeMode: Filesystem
```

Using the above, one might expect an NFS v4 mount, but viewing the output of `mount` from the node shows the underlying mount properties do not contain any of the specified options.
```
server.example.com:/data01/stuff on /var/lib/kubelet/pods/5115dfbc-7427-4e63-9a4e-914a8396e43f/volumes/kubernetes.io~nfs/stuff-data type nfs (rw,relatime,vers=3,rsize=131072,wsize=131072,namlen=255,hard,proto=tcp6,timeo=600,retrans=2,sec=sys,mountaddr=fc99::1111,mountvers=3,mountport=893,mountproto=udp6,local_lock=none,addr=fc99::1111)
```

#### What did you expect to happen?

My expectation is that when passing `mountOptions` to the PV, the underlying NFS mount command respects these options when attempting to mount the remote export.

#### How can we reproduce it (as minimally and precisely as possible)?

Reviewing the code for `TestMountOptionFromSpec` [here](https://github.com/kubernetes/kubernetes/blob/b53b9fb5573323484af9a19cf3f5bfe80760abba/pkg/volume/util/util_test.go#L343), it appears that an additional test case like the following will reproduce the issue.
```go
		"vol-with-no-sys-opts-with-nfsv4": {
			volume: createVolumeSpecWithMountOption("good-mount-opts", "", v1.PersistentVolumeSpec{
				PersistentVolumeSource: v1.PersistentVolumeSource{
					NFS: &v1.NFSVolumeSource{Server: "localhost", Path: "/srv", ReadOnly: false},
				},
				MountOptions: []string{"ro", "nfsvers=4.2"},
			}),
			expectedMountList: []string{"ro", "nfsvers=4.2", "fsid=100"},
			systemOptions:     []string{"fsid=100", "ro"},
		},
```

Notice that none of the other test cases exercise the PV `mountOptions`, and the helper method `createVolumeSpecWithMountOption` only uses the options to set an annotation.  This can be seen in the code coverage for the `util.MountOptionFromSpec` function.

#### Anything else we need to know?

This is using `v1.31.5+k3s1`.  I have been informed from the k3s folks that there is no modification of upstream volume handling.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.6-dispatcher
Kustomize Version: v5.4.2
Server Version: v1.31.5+k3s1
```

</details>


#### Cloud provider

<details>
Self-hosted.
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Arch Linux"
PRETTY_NAME="Arch Linux"
ID=arch
BUILD_ID=rolling
ANSI_COLOR="38;2;23;147;209"
HOME_URL="https://archlinux.org/"
DOCUMENTATION_URL="https://wiki.archlinux.org/"
SUPPORT_URL="https://bbs.archlinux.org/"
BUG_REPORT_URL="https://gitlab.archlinux.org/groups/archlinux/-/issues"
PRIVACY_POLICY_URL="https://terms.archlinux.org/docs/privacy-policy/"
LOGO=archlinux-logo
$ uname -a
Linux k6 6.14.2-arch1-1 #1 SMP PREEMPT_DYNAMIC Thu, 10 Apr 2025 18:43:59 +0000 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue è®¨è®ºäº† NFS ç±»å‹çš„æŒä¹…å·ï¼ˆPVï¼‰åœ¨æŒ‚è½½æ—¶ä¸å°Šé‡ç”¨æˆ·åœ¨ spec.mountOptions ä¸­é…ç½®çš„æŒ‚è½½é€‰é¡¹çš„é—®é¢˜ã€‚  
ä»åŠŸèƒ½ä¸Šçœ‹ï¼Œè¿™ä¼šå¯¼è‡´ç”¨æˆ·åŸæœ¬æœŸæœ›å¯ç”¨çš„â€œnosuidâ€â€œnoaclâ€â€œnoatimeâ€â€œnfsvers=4.2â€ç­‰é€‰é¡¹å®é™…ä¸Šæœªè¢«ç”Ÿæ•ˆï¼Œç³»ç»Ÿé»˜è®¤ä½¿ç”¨äº† NFS v3 å¹¶å¯ç”¨ suid åŠŸèƒ½ç­‰ã€‚  
åœ¨æŒ‚è½½é€‰é¡¹ä¸­ï¼Œä½¿ç”¨è€…æ˜¾å¼æŒ‡å®šäº†â€œnosuidâ€ï¼Œä½†å®é™…æŒ‚è½½å´ä¾ç„¶å¯èƒ½å­˜åœ¨â€œsuidâ€ç”Ÿæ•ˆçš„æƒ…å†µï¼Œè¿™å°±å¸¦æ¥ä¸€ä¸ªæ½œåœ¨çš„é£é™©ï¼šå¦‚æœæ”»å‡»è€…æˆ–ä»»æ„ä½æƒé™ç”¨æˆ·èƒ½å¤Ÿå¾€è¯¥ NFS å­˜å‚¨å·é‡Œå†™å…¥å¯æ‰§è¡Œæ–‡ä»¶å¹¶è®¾ç½® suid ä½ï¼Œåˆ™å¯èƒ½é€šè¿‡ suid ææƒè·å¾—åœ¨å®¿ä¸»æœºæˆ–å®¹å™¨å†…æ›´é«˜çš„æƒé™ï¼Œè¿™å±äºæ¯”è¾ƒä¸¥é‡çš„å®‰å…¨é£é™©ã€‚  
æ ¹æ® CVSS 3.1 çš„è®¡ç®—æ ‡å‡†ï¼ˆæœ¬åœ°ææƒã€æ”»å‡»å¤æ‚åº¦ä½ã€å¯èƒ½å½±å“å®Œæ•´æ€§å’Œå¯ç”¨æ€§ï¼‰ï¼Œè¯¥é—®é¢˜è¯„åˆ†å¯è¾¾åˆ° Highï¼ˆé«˜é£é™©ï¼‰çº§åˆ«ã€‚æ•…ç»¼åˆåˆ¤æ–­ï¼Œè¯¥ Issue å…·æœ‰é«˜é£é™©å®‰å…¨éšæ‚£ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import time
import traceback
from kubernetes import client, config, utils
from kubernetes.stream import stream

def main():
    """
    è¯¥è„šæœ¬æ¼”ç¤ºäº†åœ¨ Kubernetes ä¸­ä½¿ç”¨ NFS PV æ—¶æŒ‡å®š nosuid ç­‰æŒ‚è½½é€‰é¡¹ï¼Œ
    ä½†ç”±äº Issue ä¸­æè¿°çš„ bug å¯¼è‡´å®é™…æŒ‚è½½å¹¶æœªåº”ç”¨ nosuidï¼Œè€Œå¯èƒ½å¸¦æ¥çš„å®‰å…¨é£é™©ã€‚
    æœ¬è„šæœ¬åªç”¨äºç ”ç©¶å’Œæœ¬åœ°æµ‹è¯•ï¼Œåˆ‡å‹¿åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éšæ„è¿è¡Œã€‚
    """
    try:
        # 1. è½½å…¥é»˜è®¤ kubeconfigï¼ˆå‡è®¾å·²é…ç½®å¥½~/.kube/configï¼‰
        config.load_kube_config()

        # 2. åˆ›å»º Kubernetes å®¢æˆ·ç«¯
        api_instance = client.CoreV1Api()

        # 3. å®šä¹‰æµ‹è¯•ç”¨å‘½åç©ºé—´
        namespace_name = "test-nfs-nosuid"
        create_namespace_if_not_exists(api_instance, namespace_name)

        # 4. å®šä¹‰ä¸€ä¸ªå·²ç»å­˜åœ¨çš„ NFS æœåŠ¡å™¨å’Œå…±äº«è·¯å¾„ï¼ˆå‡è®¾æµ‹è¯•ç¯å¢ƒä¸­å·²å­˜åœ¨ï¼‰
        #    å¦‚æœéœ€è¦ï¼Œå¯ä»¥åœ¨æ­¤è„šæœ¬å¤–éƒ¨é¢„å…ˆå‡†å¤‡å¥½ NFS æœåŠ¡
        nfs_server = "1.2.3.4"   # è¯·æ›¿æ¢ä¸ºçœŸå® NFS æœåŠ¡åœ°å€
        nfs_path = "/myshare"    # æ›¿æ¢ä¸ºçœŸå®å…±äº«çš„ç›®å½•

        # 5. åˆ›å»ºä¸€ä¸ª PVï¼ŒæŒ‡å®šäº† nosuid ç­‰ mountOptions
        pv_name = "test-nfs-pv-nosuid"
        create_pv_with_mountoptions(api_instance, pv_name, nfs_server, nfs_path)

        # 6. åˆ›å»ºä¸€ä¸ª PVC ä»¥ç»‘å®šä¸Šé¢çš„ PV
        pvc_name = "test-nfs-pvc-nosuid"
        create_pvc(api_instance, namespace_name, pvc_name, pv_name)

        # 7. åˆ›å»ºä¸€ä¸ª Podï¼Œä½¿ç”¨ä¸Šé¢åˆ›å»ºçš„ PVC
        pod_name = "test-nfs-pod-nosuid"
        create_test_pod(api_instance, namespace_name, pod_name, pvc_name)

        # ç­‰å¾… Pod è¿è¡Œèµ·æ¥
        wait_pod_ready(api_instance, namespace_name, pod_name)

        # 8. åœ¨ Pod å†…æŸ¥çœ‹å®é™…çš„æŒ‚è½½é€‰é¡¹
        #    æœŸå¾…çœ‹åˆ° nosuidï¼Œä½†æ ¹æ® Issue å¯èƒ½å‡ºç° suid è¢«å¯ç”¨ç­‰æƒ…å†µ
        cmd = ["mount"]
        mount_info = stream(api_instance.connect_get_namespaced_pod_exec,
                            pod_name, namespace_name,
                            command=cmd,
                            stderr=True, stdin=False,
                            stdout=True, tty=False)
        print("===== Pod å†…éƒ¨çš„ mount å‘½ä»¤è¾“å‡º =====")
        print(mount_info)
        print("================================")

        # å¦‚æœå¿½ç•¥äº† nosuidï¼Œåˆ™æœ‰å¯èƒ½å‡ºç° suid ä¾ç„¶ç”Ÿæ•ˆçš„æŒ‚è½½
        # å¯ä»¥è¿›ä¸€æ­¥å°è¯•åœ¨ PV ä¸Šæ”¾ç½®å¸¦ suid çš„å¯æ‰§è¡Œæ–‡ä»¶å¹¶æ‰§è¡Œï¼Œä»è€Œæ¨¡æ‹Ÿææƒé£é™©

    except Exception as e:
        print("æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸ï¼š", e)
        traceback.print_exc()


def create_namespace_if_not_exists(api_instance, namespace_name):
    """
    å¦‚æœå‘½åç©ºé—´ä¸å­˜åœ¨åˆ™åˆ›å»º
    """
    existing_namespaces = api_instance.list_namespace()
    for ns in existing_namespaces.items:
        if ns.metadata.name == namespace_name:
            return
    ns_body = client.V1Namespace(metadata=client.V1ObjectMeta(name=namespace_name))
    api_instance.create_namespace(ns_body)
    time.sleep(2)


def create_pv_with_mountoptions(api_instance, pv_name, nfs_server, nfs_path):
    """
    åˆ›å»ºå¸¦ nosuid ç­‰ mountOptions çš„ NFS PV
    æ³¨æ„è¿™é‡Œè®¾ç½®äº†ä¸€ä¸ªè¾ƒå°å­˜å‚¨å®¹é‡ä»¥ç¤ºèŒƒï¼›ä¹Ÿå¯è°ƒå¤§ã€‚
    """
    pv_spec = client.V1PersistentVolume(
        metadata=client.V1ObjectMeta(name=pv_name),
        spec=client.V1PersistentVolumeSpec(
            capacity={"storage": "1Gi"},
            access_modes=["ReadWriteMany"],
            nfs=client.V1NFSVolumeSource(
                path=nfs_path,
                server=nfs_server
            ),
            persistent_volume_reclaim_policy="Retain",
            mount_options=[
                "nosuid",
                "noacl",
                "noatime",
                "nodiratime",
                "nolock",
                "nfsvers=4.2"
            ],
            storage_class_name=""  # æ— éœ€æŒ‡å®š storageClass
        )
    )
    api_instance.create_persistent_volume(pv_spec)
    time.sleep(2)


def create_pvc(api_instance, namespace, pvc_name, pv_name):
    """
    åˆ›å»ºç”¨äºç»‘å®šä¸Šé¢ PV çš„ PVC
    """
    pvc_spec = client.V1PersistentVolumeClaim(
        metadata=client.V1ObjectMeta(name=pvc_name),
        spec=client.V1PersistentVolumeClaimSpec(
            access_modes=["ReadWriteMany"],
            resources=client.V1ResourceRequirements(
                requests={"storage": "1Gi"}
            ),
            volume_name=pv_name
        )
    )
    api_instance.create_namespaced_persistent_volume_claim(namespace, pvc_spec)
    time.sleep(2)


def create_test_pod(api_instance, namespace, pod_name, pvc_name):
    """
    åˆ›å»ºæµ‹è¯• Podï¼ŒæŒ‚è½½ä¸Šé¢çš„ PVC
    """
    container = client.V1Container(
        name="test-nfs-container",
        image="busybox",
        command=["sh", "-c", "sleep 3600"],
        volume_mounts=[client.V1VolumeMount(
            mount_path="/mnt/test",
            name="test-nfs-volume"
        )]
    )

    volume = client.V1Volume(
        name="test-nfs-volume",
        persistent_volume_claim=client.V1PersistentVolumeClaimVolumeSource(
            claim_name=pvc_name
        )
    )

    spec = client.V1PodSpec(containers=[container], volumes=[volume], restart_policy="Never")
    pod = client.V1Pod(
        metadata=client.V1ObjectMeta(name=pod_name),
        spec=spec
    )
    api_instance.create_namespaced_pod(namespace, pod)
    time.sleep(2)


def wait_pod_ready(api_instance, namespace, pod_name, timeout=90):
    """
    ç­‰å¾… Pod çš„çŠ¶æ€å˜ä¸º Runningï¼Œè¶…æ—¶åˆ™ç›´æ¥è¿”å›æˆ–æŠ¥é”™
    """
    start_time = time.time()

    while True:
        pod = api_instance.read_namespaced_pod(pod_name, namespace)
        if pod.status.phase == "Running":
            break
        if time.time() - start_time > timeout:
            print(f"ç­‰å¾… Pod {pod_name} è¿è¡Œè¶…æ—¶ {timeout} ç§’ã€‚")
            break
        time.sleep(3)

main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. è„šæœ¬ä¼šåœ¨å½“å‰é›†ç¾¤ä¸­åˆ›å»ºä¸€ä¸ªåä¸ºâ€œtest-nfs-nosuidâ€çš„å‘½åç©ºé—´ï¼Œå¹¶åœ¨å…¶ä¸­åˆ›å»º PVã€PVC ä»¥åŠä¸€ä¸ªæµ‹è¯• Podã€‚  
2. è¯¥ PV çš„ NFS æŒ‚è½½é€‰é¡¹é‡ŒåŒ…å«â€œnosuidâ€ï¼ŒæŒ‰ç†è¯´ Pod å†…éƒ¨å®é™…æŒ‚è½½ç‚¹åº”å½“æ— æ³•æ‰§è¡Œ suid ç¨‹åºï¼Œä½†ç”±äº Issue æ‰€è¿°çš„ bugï¼ŒæŒ‚è½½å‘½ä»¤å¯èƒ½ä¼šé»˜è®¤ä¸º suid ç”Ÿæ•ˆçš„æ–¹å¼ã€‚  
3. è„šæœ¬æ‰§è¡Œåï¼Œä¼šåœ¨ Pod å†…éƒ¨è°ƒç”¨â€œmountâ€å‘½ä»¤å¹¶æ‰“å°è¾“å‡ºï¼Œè‹¥è¾“å‡ºç»“æœå¹¶æœªæ˜¾ç¤º â€œnosuidâ€ é€‰é¡¹ï¼Œæˆ–ä»å¯è§â€œsuidâ€/â€œsetuidâ€ç›¸å…³æ ‡è®°ï¼Œå³è¯´æ˜æŒ‚è½½é€‰é¡¹è¢«å¿½ç•¥ã€‚å®é™…ç¯å¢ƒä¸­ï¼Œè¿™å¯èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œä¸Šä¼ å¸¦ suid ä½çš„äºŒè¿›åˆ¶æ‰§è¡Œæ–‡ä»¶ï¼Œå®ç°ææƒã€‚  
4. æœ¬è„šæœ¬ä»…ç”¨äºæœ¬åœ°æˆ–æµ‹è¯•ç¯å¢ƒéªŒè¯è¯¥é—®é¢˜ï¼Œåˆ‡å‹¿åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éšæ„è¿è¡Œã€‚è„šæœ¬æ‰§è¡Œç»“æŸåï¼Œå¦‚éœ€æ¸…ç†ï¼Œè¯·åˆ é™¤åˆ›å»ºçš„ Podã€PVCã€PV ä»¥åŠå‘½åç©ºé—´ã€‚

---


# âš ï¸ å­˜åœ¨ä½é£é™©çš„ Issues (2 ä¸ª)

## Issue #131407 kubelet not cleaning up exited containers and eventually failing due to gRPC message size

- Issue é“¾æ¥ï¼š[#131407](https://github.com/kubernetes/kubernetes/issues/131407)

### Issue å†…å®¹

#### What happened?

in a cluster with a high amount of CronJobs (100+) which run every minute, containers keep piling up for pods which are no longer running, which then leads to IPAM congestion, eventually ending in the kubelet failing due to the following:
```json
{"ts":1745319662553.8606,"caller":"internal/log.go:32","msg":"ListPodSandbox with filter from runtime service failed","filter":"nil","err":"rpc error: code = ResourceExhausted desc = grpc: trying to send message larger than max (16791221 vs. 16777216)"}
{"ts":1745319662553.8938,"caller":"kuberuntime/kuberuntime_sandbox.go:305","msg":"Failed to list pod sandboxes","err":"rpc error: code = ResourceExhausted desc = grpc: trying to send message larger than max (16791221 vs. 16777216)"}
{"ts":1745319662553.902,"caller":"pleg/generic.go:256","msg":"GenericPLEG: Unable to retrieve pods","err":"rpc error: code = ResourceExhausted desc = grpc: trying to send message larger than max (16791221 vs. 16777216)"}
```
the CronJobs have been tuned to avoid this by setting the following in the spec, but to no avail:
```yaml
concurrencyPolicy: Forbid
ttlSecondsAfterFinished: 30
successfulJobsHistoryLimit: 2   # (sometimes 1)
failedJobsHistoryLimit: 1
```



**UPD**: after hitting the aforementioned message, I've cordoned and drained the node, and let it sit like this. after about an hour in the cordoned state, I saw a spike in the operation rate (mostly `container_status` and `remove_container`), and the amount of exited containers dropped from 10230 to just 6! the logs were full of `RemoveContainer` operations:
```json
{"ts":1745323991806.5146,"caller":"topologymanager/scope.go:117","msg":"RemoveContainer","v":0,"containerID":"95f25c810eba33f3a72e3852fca88d59b5a2428535a663aea04a9d60c8fe2963"}
{"ts":1745323991806.924,"caller":"topologymanager/scope.go:117","msg":"RemoveContainer","v":0,"containerID":"141f8a2c0ad488f454886084227aedb4a18cf2515d9747500aedc44f196f6c7a"}
```
so, the GC worked. but why only now?

#### What did you expect to happen?

containers being cleaned up after pods

#### How can we reproduce it (as minimally and precisely as possible)?

1. have a cluster with 100+ CronJobs scheduled to run every minute
2. observe a rapidly increasing amount of containers despite the pods being cleaned up (metric `kubelet_running_containers{container_state="exited"}`). this went over 4k in a span of 8 hours and, in my case, reached over 10k before failing
3. observe IPAM issues. in my case it was Cilium keeping pods in the Pending phase with the following: `unable to allocate IP via local cilium agent: [POST /ipam][502] postIpAMFailure range is full`
4. observe `list_podsandbox` errors before kubelet ultimately failing to respond to commands due to gRPC message size

#### Anything else we need to know?

_No response_

#### Kubernetes version

Kubernetes v1.32.3

#### Cloud provider

Bare-metal

#### OS version

Talos Linux v1.9.5

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

Containerd v2.0.3

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

Cilium v1.17.2

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»è¯¥ Issue æè¿°æ¥çœ‹ï¼Œé—®é¢˜çš„æ ¸å¿ƒæ˜¯ Kubernetes é›†ç¾¤ä¸­ CronJob æ•°é‡è¾ƒå¤šä¸”é¢‘ç¹è§¦å‘ï¼ˆæ¯åˆ†é’Ÿ 100+ï¼‰ï¼Œå¯¼è‡´å·²é€€å‡ºçš„å®¹å™¨æ®‹ç•™æ²¡æœ‰åŠæ—¶è¢« kubelet åƒåœ¾å›æ”¶ï¼ˆGCï¼‰ï¼Œå †ç§¯åˆ°ä¸Šä¸‡æ•°é‡ï¼Œä»è€Œå¼•èµ· IPAM èµ„æºè€—å°½(Cilium æŠ¥é”™ range is full)ä»¥åŠå®¹å™¨è¿è¡Œæ—¶åœ¨åˆ—å‡º pod sandboxes æ—¶å‡ºç° gRPC ä¼ è¾“ä½“ç§¯è¶…é™ã€‚  
æ­¤é—®é¢˜æœ¬è´¨ä¸Šå±äºèµ„æºç®¡ç†å’Œè°ƒåº¦å±‚é¢çš„è¿ç»´é—®é¢˜ï¼Œå¹¶éå…¸å‹çš„å®‰å…¨æ¼æ´ã€‚è™½ç„¶æœ€ç»ˆä¼šå¯¼è‡´ kubelet æ— æ³•æ­£å¸¸å“åº”è¯·æ±‚ï¼Œå‘ˆç°å‡ºæŸç§â€œæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰â€çš„æ•ˆæœï¼Œä½†éœ€è¦æ³¨æ„ä»¥ä¸‹è¦ç‚¹ï¼š  
1. è§¦å‘è¯¥é—®é¢˜éœ€è¦åœ¨é›†ç¾¤å†…å…·æœ‰åˆ›å»º CronJob/Job/Pod ç­‰æƒé™ï¼Œä¸€èˆ¬è€Œè¨€è¿™æ˜¯é›†ç¾¤ç®¡ç†å‘˜æˆ–å…·æœ‰è¾ƒé«˜æƒé™çš„è§’è‰²æ‰å…·å¤‡çš„æ“ä½œèƒ½åŠ›ï¼Œæ™®é€šä½æƒé™ç”¨æˆ·æ— æ³•ç›´æ¥åˆ¶é€ å¦‚æ­¤è§„æ¨¡çš„å®¹å™¨å †ç§¯ã€‚  
2. è¯¥é—®é¢˜å¹¶ä¸æ¶‰åŠä»»æ„å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒã€å‡­æ®æ³„éœ²ç­‰é«˜é£é™©è¡Œä¸ºï¼Œæ›´å¤šæ˜¯å› ä¸ºèŠ‚ç‚¹å¤„äºé«˜è´Ÿè½½æˆ–èµ„æºå›æ”¶æœºåˆ¶å¼‚å¸¸å¯¼è‡´çš„æœåŠ¡å¯ç”¨æ€§é—®é¢˜ã€‚  
3. æ ¹æ® CVSS 3.1 è¯„åˆ†æ ‡å‡†ï¼Œè‹¥ä¸ºæ‹’ç»æœåŠ¡å‹é£é™©ä½†éœ€è¦é«˜æƒé™æ‰èƒ½å®æ–½ï¼Œé€šå¸¸ä¼šæ˜¾è‘—é™ä½æ¼æ´è¯„åˆ†ã€‚ç»¼åˆè®¡ç®—åï¼Œåˆ†å€¼è½åœ¨ 7.0 ä»¥ä¸‹ï¼Œæ•…æœªè¾¾â€œé«˜é£é™©â€é˜ˆå€¼ã€‚  

å› æ­¤ç»¼åˆåˆ¤æ–­ï¼Œè¯¥é—®é¢˜å¯ä»¥è¢«è§†ä½œèµ„æºè€—å°½å¼•èµ·çš„æœåŠ¡å¯ç”¨æ€§é—®é¢˜ï¼Œéœ€è¦æœ‰è¾ƒé«˜æƒé™æ‰èƒ½é€ æˆå½±å“ï¼Œå¹¶ä¸æ„æˆä¸¥é‡å®‰å…¨æ¼æ´ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

ç”±äºè¯¥é—®é¢˜å¹¶æœªè¾¾åˆ°é«˜é£é™©çº§åˆ«ï¼ˆCVSS 3.1 è¯„åˆ†ä¸è¶³ä»¥è¾¾åˆ° High ä»¥ä¸Šï¼‰ï¼Œæ•…æ ¹æ®â€œé’ˆå¯¹é«˜é£é™©é—®é¢˜ï¼Œå¿…é¡»ç»™å‡ºä½¿ç”¨ python ç¼–å†™çš„å¤ç°è„šæœ¬â€çš„è¦æ±‚ï¼Œä¸éœ€è¦æä¾›å¤ç°è„šæœ¬ã€‚  
åœ¨å®é™…ç¯å¢ƒä¸­ï¼Œå¯é€šè¿‡å‡å°‘ CronJob é¢‘ç‡æˆ–æ•°é‡ã€å¯ç”¨åˆé€‚çš„æ¸…ç†/å›æ”¶ç­–ç•¥ï¼ˆå¦‚ ttlSecondsAfterFinishedã€Job å†å²ä¿ç•™ç­–ç•¥ç­‰ï¼‰å¹¶ä¼˜åŒ– kubelet é…ç½®ï¼Œæ¥ç¼“è§£æˆ–é¿å…æ­¤é—®é¢˜ã€‚  
å¦‚æœåœ¨å¤šç§Ÿæˆ·åœºæ™¯ä¸‹å­˜åœ¨ä½æƒé™ç”¨æˆ·å¯åˆ›å»ºå¤§é‡ä½œä¸šçš„ç‰¹æ®Šæƒ…å†µï¼Œåˆ™éœ€é€šè¿‡æƒé™ç®¡æ§ï¼ˆRBACï¼‰æ¥é™åˆ¶ CronJob/Job çš„åˆ›å»ºï¼Œé˜²æ­¢èµ„æºæ»¥ç”¨å¯¼è‡´çš„æ½œåœ¨ DoS é£é™©ã€‚

---


## Issue #131355 Kubelet ReadOnlyPort cannot be set to 0 via golang KubeletConfiguration

- Issue é“¾æ¥ï¼š[#131355](https://github.com/kubernetes/kubernetes/issues/131355)

### Issue å†…å®¹

#### What happened?

The kubelet ReadOnlyPort cannot be set to 0 via KubeletConfiguration struct.

As per the docs:
https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/
> **--read-only-port** int32Â Â Â Â Â Default: 10255
> The read-only port for the kubelet to serve on with no authentication/authorization (set toÂ 0Â to disable).
> (DEPRECATED: This parameter should be set via the config file specified by the kubelet'sÂ --configÂ flag. SeeÂ kubelet-config-fileÂ for more information.)


https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration
> **readOnlyPort** int32Â Â Â Â Â Default: 0 (disabled)
> readOnlyPort is the read-only port for the Kubelet to serve on with no authentication/authorization. The port number must be between 1 and 65535, inclusive. Setting this field to 0 disables the read-only service.

If you attempt to set this to 0 this using the golang struct, the field will be omitted when marshaled to YAML or JSON as the json field tag has `omitempty` and 0 is the empty value for an int32:
https://github.com/kubernetes/kubernetes/blob/35716897a3d8cd2944aa172006c5b7604a64c22e/staging/src/k8s.io/kubelet/config/v1beta1/types.go#L145

#### What did you expect to happen?

Field is not dropped from YAML

#### How can we reproduce it (as minimally and precisely as possible)?

```golang
package main

import (
	"fmt"

	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	kubeletconfig "k8s.io/kubelet/config/v1beta1"
	"sigs.k8s.io/yaml"
)

func main() {
	config := &kubeletconfig.KubeletConfiguration{
		TypeMeta: metav1.TypeMeta{
			APIVersion: "kubelet.config.k8s.io/v1beta1",
			Kind:       "KubeletConfiguration",
		},
		ReadOnlyPort: 0,
	}
	b, _ := yaml.Marshal(config)
	fmt.Printf("%s", b)
}
```
```console
brandond@dev01:~$ go run main.go
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous: {}
  webhook:
    cacheTTL: 0s
  x509: {}
authorization:
  webhook:
    cacheAuthorizedTTL: 0s
    cacheUnauthorizedTTL: 0s
containerRuntimeEndpoint: ""
cpuManagerReconcilePeriod: 0s
evictionPressureTransitionPeriod: 0s
fileCheckFrequency: 0s
httpCheckFrequency: 0s
imageMaximumGCAge: 0s
imageMinimumGCAge: 0s
kind: KubeletConfiguration
logging:
  flushFrequency: 0
  options:
    json:
      infoBufferSize: "0"
    text:
      infoBufferSize: "0"
  verbosity: 0
memorySwap: {}
nodeStatusReportFrequency: 0s
nodeStatusUpdateFrequency: 0s
runtimeRequestTimeout: 0s
shutdownGracePeriod: 0s
shutdownGracePeriodCriticalPods: 0s
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
volumeStatsAggPeriod: 0s
```

Since the deprecated CLI flag defaults to 10255, and the field is not rendered into the configuration YAML that is passed to the kubelet, the port remains set to 10255.

#### Anything else we need to know?

This is probably another case of a field that should have been *int32 instead of int32.

#### Kubernetes version

<details>

```console
Client Version: v1.32.3+k3s1
Kustomize Version: v5.5.0
Server Version: v1.32.3+k3s1
```

</details>


#### Cloud provider

n/a

#### OS version

n/a

#### Install tools

k3s

#### Container runtime (CRI) and version (if applicable)

containerd v2.0.4


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

n/a


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»è¯¥ Issue æè¿°æ¥çœ‹ï¼Œkubelet çš„ readOnlyPort å‚æ•°æœ¬åº”é€šè¿‡å°†å…¶è®¾ç½®ä¸º 0 æ¥ç¦ç”¨ï¼Œä½†å› ä¸ºåœ¨ Golang çš„ KubeletConfiguration ç»“æ„ä½“ä¸­ä½¿ç”¨äº†å¸¦æœ‰ omitempty æ ‡ç­¾çš„ int32 å­—æ®µï¼Œå¯¼è‡´è®¾ç½®æˆ 0 æ—¶å­—æ®µä¼šåœ¨åºåˆ—åŒ–è¾“å‡ºæ—¶è¢«å¿½ç•¥ï¼Œä»è€Œä½¿ readOnlyPort å§‹ç»ˆä¿æŒé»˜è®¤å€¼ 10255 å¹¶å¤„äºå¼€å¯çŠ¶æ€ã€‚è¿™ä¼šåœ¨é›†ç¾¤ä¸­å¼€å¯ä¸€ä¸ªæ— éœ€èº«ä»½éªŒè¯çš„åªè¯»ç«¯å£ï¼Œè™½ç„¶æ— æ³•ç›´æ¥è¿›è¡Œæ•æ„Ÿæ“ä½œï¼Œä½†ä»å¯èƒ½é€ æˆä¸€å®šçš„ä¿¡æ¯æ³„éœ²é£é™©ï¼ˆå¦‚èŠ‚ç‚¹çŠ¶æ€ã€Pod ä¿¡æ¯ç­‰å…ƒæ•°æ®ï¼‰ã€‚æ ¹æ® CVSS 3.1 è¯„åˆ†æ¨æµ‹ï¼Œè¿™ç§ä¿¡æ¯æ³„éœ²å¹¶ä¸å±äºä¸¥é‡æ¼æ´ï¼Œå°šä¸è¶³ä»¥å¯¼è‡´é«˜å±åæœï¼Œå› æ­¤å¯åˆ¤å®šä¸ºå®‰å…¨é—®é¢˜ä¸­çš„ä½é£é™©ç­‰çº§ã€‚

---


# âœ… ä¸æ¶‰åŠå®‰å…¨é£é™©çš„ Issues (58 ä¸ª)

## Issue #131543 kubtctl auth can-i get pods/exec shows error result

- Issue é“¾æ¥ï¼š[#131543](https://github.com/kubernetes/kubernetes/issues/131543)

### Issue å†…å®¹

#### What happened?

when i execute command 'kubtctl auth can-i get pods/exec' it show yes, but in fact i can't exec into the pod because i haven't set the pods/exec privilege in the role, you can see what happened in the image demonstrated below.
![Image](https://github.com/user-attachments/assets/bbe01511-0379-4627-8a14-2cb02d6ffb93)

![Image](https://github.com/user-attachments/assets/d89124a0-5a66-4203-8b65-29b20df95b9e)

#### What did you expect to happen?

it should show no as a result

#### How can we reproduce it (as minimally and precisely as possible)?

create a role who don't have pods/exec privilege and use that user to exec a shell in that pod

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: v1.29.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.2
```

</details>


#### Cloud provider

<details>
no cloud provider
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»Issueæè¿°æ¥çœ‹ï¼Œé—®é¢˜é›†ä¸­åœ¨â€œkubectl auth can-iâ€å‘½ä»¤ç»“æœä¸å®é™…æƒé™ä¸ä¸€è‡´ï¼šå‘½ä»¤è¾“å‡ºæ˜¾ç¤ºâ€œyesâ€ï¼Œä½†å®é™…ä¸Šç”¨æˆ·å¹¶æ²¡æœ‰â€œpods/execâ€æƒé™ï¼Œä¹Ÿæ— æ³•çœŸæ­£execè¿›å…¥Podã€‚è¿™å±äºå·¥å…·æˆ–é…ç½®å±‚é¢å¯¼è‡´çš„è¡¨ç°ä¸ä¸€è‡´ï¼Œå¹¶æœªä½“ç°å‡ºå®é™…æƒé™ç»•è¿‡æˆ–è¶Šæƒæ“ä½œï¼Œä¹Ÿæ²¡æœ‰æ•æ„Ÿä¿¡æ¯æ³„éœ²æˆ–é«˜é£é™©å®‰å…¨æ¼æ´ã€‚  
å› æ­¤ï¼Œè¯¥é—®é¢˜å¹¶ä¸ä¼šè®©æ”»å‡»è€…åœ¨æ— æƒé™çš„æƒ…å†µä¸‹å¯ä»¥æ‰§è¡Œå‘½ä»¤ï¼Œä¹Ÿä¸å½±å“é›†ç¾¤å®‰å…¨ï¼Œä¸æ¶‰åŠæ”»å‡»æ‰‹æ®µæˆ–å®é™…è¶Šæƒè¡Œä¸ºï¼Œåªæ˜¯è¾“å‡ºä¿¡æ¯ä¸çœŸå®è§’è‰²æƒé™ä¸ä¸€è‡´ï¼Œå±äºä½¿ç”¨æˆ–é…ç½®Bugï¼Œå¹¶éå®‰å…¨æ¼æ´ã€‚æ ¹æ®Issueé£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤å¤„é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Issueå¹¶æœªä½“ç°å‡ºå®‰å…¨æ¼æ´æˆ–æ•æ„Ÿä¿¡æ¯æ³„éœ²ï¼Œä»…ä¸ºä¸å½±å“å®‰å…¨çš„åŠŸèƒ½/é…ç½®é—®é¢˜ã€‚ç”±äºä¸æ˜¯é«˜é£é™©å®‰å…¨é—®é¢˜ï¼Œå› æ­¤æ— éœ€æä¾›å¤ç°è„šæœ¬ã€‚è‹¥éœ€è¿›ä¸€æ­¥æ’æŸ¥ï¼Œå¯å›´ç»•é›†ç¾¤çš„RBACé…ç½®ã€æœ¬åœ°kubeconfigæƒé™å’Œâ€œkubectl auth can-iâ€å‘½ä»¤çš„å®ç°æ–¹å¼è¿›è¡Œè°ƒè¯•å’ŒéªŒè¯ã€‚

---


## Issue #131533 code-generator/applyconfiguration-gen: Embedding a field results in <.EmbeddedIn.MemberType.Elem.Name.Name>: nil pointer evaluating *types.Type.Name

- Issue é“¾æ¥ï¼š[#131533](https://github.com/kubernetes/kubernetes/issues/131533)

### Issue å†…å®¹

Using applyconfiguration-gen with the following type produces an error
```
import (
        gwv1alpha2 "sigs.k8s.io/gateway-api/apis/v1alpha2"
)

type Status struct {
	gwv1alpha2.PolicyStatus `json:",inline"`
}
```
```
F0424 08:57:02.343886 1766244 main.go:57] Error: failed executing generator: some targets had errors: template: /home/shashank/.gvm/pkgsets/go1.24.1/global/pkg/mod/k8s.io/code-generator@v0.32.2/cmd/applyconfiguration-gen/generators/applyconfiguration.go:342:1:37: executing "/home/shashank/.gvm/pkgsets/go1.24.1/global/pkg/mod/k8s.io/code-generator@v0.32.2/cmd/applyconfiguration-gen/generators/applyconfiguration.go:342" at <.EmbeddedIn.MemberType.Elem.Name.Name>: nil pointer evaluating *types.Type.Name
```

Not embedding the field works:
```
type Status struct {
	PolicyStatus gwv1alpha2.PolicyStatus `json:",inline"`
}
``

This is with `k8s.io/code-generator v0.32.3

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»Issueæè¿°æ¥çœ‹ï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨ä½¿ç”¨applyconfiguration-genæ—¶å› åµŒå…¥ç±»å‹å¯¼è‡´nilæŒ‡é’ˆçš„é—®é¢˜ï¼Œå±äºå…¸å‹çš„å¼€å‘é˜¶æ®µç¼–è¯‘æˆ–ä»£ç ç”Ÿæˆé”™è¯¯ã€‚è¯¥é—®é¢˜å¹¶ä¸æ¶‰åŠæ•æ„Ÿä¿¡æ¯æ³„éœ²ã€æƒé™æå‡ã€è¿œç¨‹ä»£ç æ‰§è¡Œã€å‘½ä»¤æ‰§è¡Œæˆ–å¯è¢«æ¶æ„åˆ©ç”¨çš„å®‰å…¨é£é™©ï¼Œä¹Ÿæ— æ³•åœ¨å¤šç”¨æˆ·åœºæ™¯ä¸­è¢«ä½æƒé™ç”¨æˆ·åˆ©ç”¨æ¥å½±å“å…¶ä»–ç”¨æˆ·çš„èµ„æºã€‚å› æ­¤ï¼Œæœ¬è´¨ä¸Šåªæ˜¯ä¸€ä¸ªä»£ç ç”Ÿæˆè¿‡ç¨‹ä¸­çš„å…¼å®¹æ€§æˆ–å·¥å…·å…¼å®¹æ€§Bugï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

ç”±äºè¯¥é—®é¢˜å¹¶éå®‰å…¨é£é™©ï¼Œé£é™©è¯„çº§åˆ¤æ–­ä¸ºâ€œä¸æ¶‰åŠâ€ï¼Œå› æ­¤æ— éœ€æä¾›å¤ç°POCè„šæœ¬ã€‚è¯¥Issueæœ¬è´¨ä¸Šä¸å·¥å…·ä½¿ç”¨é—®é¢˜æˆ–ç”Ÿæˆå™¨é€»è¾‘ä¸å®Œå…¨é€‚é…æœ‰å…³ï¼Œä¸ä¼šå¯¹ç³»ç»Ÿæˆ–æ•°æ®é€ æˆå®‰å…¨å¨èƒã€‚è‹¥è¦è§£å†³è¯¥é—®é¢˜ï¼Œå¯ä¸ç›¸å…³ä»£ç ç”Ÿæˆå·¥å…·çš„ç»´æŠ¤æ–¹è¿›è¡Œæ²Ÿé€šæˆ–æŸ¥çœ‹æ›´æ–°ç‰ˆæœ¬çš„ä¿®æ­£æƒ…å†µã€‚ç›¸å…³ä¿®å¤å¯åœ¨å¼€å‘ç¯å¢ƒä¸­å®Œæˆï¼Œå±äºæ™®é€šç ”å‘ç”Ÿå‘½å‘¨æœŸå†…çš„é—®é¢˜è€Œéå®‰å…¨æ¼æ´ã€‚

---


## Issue #131530 OOM killed status pod become successed phase

- Issue é“¾æ¥ï¼š[#131530](https://github.com/kubernetes/kubernetes/issues/131530)

### Issue å†…å®¹

#### What happened?

I created a deployment with 3 replicas. Each pod contains 1 init container and 1 main container. The exit code of one pod main container is 0. But the reason for the exit is OOM killed. However, the final status of the pod is successed.

```
State:          Terminated
  Reason:       OOMKilled
  Exit Code:    0
  Started:      Tue, 29 Apr 2025 13:43:32 +0800
  Finished:     Tue, 29 Apr 2025 14:10:51 +0800
Ready:          False
Restart Count:  0
```
```
  phase: Succeeded
```
```
  restartPolicy: Always
```
which cause rs controller create 5 replicas

<img width="1294" alt="Image" src="https://github.com/user-attachments/assets/9097648f-0077-4466-9018-4aaf58f37527" />

Maybe some information has changed,
I have one questions
1. Is the phase of an OOM-killed container 's pod   â€œSucceededâ€ or â€œRunningâ€?
Are the above one questions normal behavior changes?Especially after reading https://github.com/kubernetes/kubernetes/issues/118310, the behavior changes introduced there make this matter feel more confusingã€‚




#### What did you expect to happen?

The phase is Running

#### How can we reproduce it (as minimally and precisely as possible)?

A env which has the same kubernetes version.I try to reproduce it
dockerfile
```
FROM ubuntu:latest

RUN apt-get update && apt-get install -y dumb-init stress-ng libcap2-bin && \
    setcap cap_ipc_lock+ep /usr/bin/stress-ng

COPY start_stress.sh /start_stress.sh
RUN chmod +x /start_stress.sh
ENTRYPOINT ["dumb-init", "--", "/start_stress.sh"]
```
start_stress.sh
```
#!/bin/bash
for i in {1..3}; do
  stress-ng --vm 1 --vm-bytes 256M --vm-keep --vm-locked &
done

sleep 30s
stress-ng --vm 1 --vm-bytes 256M --vm-keep --vm-locked &
sleep 10s
stress-ng --vm 1 --vm-bytes 512M --vm-keep --vm-locked &
sleep 10s

```
deployment.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: oom-test
spec:
  replicas: 2
  selector:
    matchLabels:
      app: oom-test
  template:
    metadata:
      labels:
        app: oom-test
    spec:
      containers:
      - name: nginx
        image:  the image
        imagePullPolicy: Always
        securityContext: 
          capabilities:
            add: ["IPC_LOCK", "SYS_RESOURCE"]
        resources:
          limits:
            memory: "1300Mi"

```
but i get
```
    Port:           <none>
    Host Port:      <none>
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       OOMKilled
      Exit Code:    0
      Started:      Wed, 30 Apr 2025 13:45:28 +0800
      Finished:     Wed, 30 Apr 2025 13:46:18 +0800
    Ready:          False
    Restart Count:  4
    Limits:
      memory:  1300Mi
    Requests:
      memory:     1300Mi
    Environment:  <none>

[root@node-5 ~]# kubectl get   pod  oom-test-545f7696d-jcld9 -o yaml -w|grep phase
  phase: Running
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.2
Server Version: v1.28.2
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä» Issue æè¿°æ¥çœ‹ï¼Œè¯¥é—®é¢˜ä¸»è¦æ¶‰åŠåœ¨ Kubernetes ä¸­è¿è¡Œå®¹å™¨æ—¶å›  OOMï¼ˆOut Of Memoryï¼‰è¢«æ€æ‰åï¼Œå…¶ Pod çŠ¶æ€ä»å®¹å™¨çš„è§†è§’å‘ˆç°ä¸º â€œSucceededâ€ è€Œå¹¶é â€œRunningâ€ æˆ– â€œFailedâ€ã€‚è¿™æ˜¯ä¸€æ¬¡å…³äº Kubernetes å¤„ç†å®¹å™¨ OOMKilled çŠ¶æ€ä¸ Pod æœ€ç»ˆç›¸ä½ä¹‹é—´é”™é…çš„è®¨è®ºã€‚  
æ­¤é—®é¢˜å¹¶æœªæ¶‰åŠæ•æ„Ÿä¿¡æ¯æ³„éœ²ã€ææƒã€è¿œç¨‹å‘½ä»¤æ‰§è¡Œæˆ–å…¶ä»–å¯èƒ½é€ æˆå®‰å…¨æ¼æ´çš„æƒ…å†µï¼Œä¹Ÿä¸æ¶‰åŠå¯¹ä»–äººæˆ–ç³»ç»Ÿå¸¦æ¥å®‰å…¨é£é™©çš„æ“ä½œï¼Œåªæ˜¯å¯¹ Kubernetes åŠŸèƒ½è¡Œä¸ºçš„ç–‘é—®æˆ–è¯¯è§£ã€‚å› æ­¤ç»¼åˆåˆ¤æ–­ï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªå®‰å…¨æ¼æ´æˆ–å®‰å…¨é£é™©ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

æœ¬ Issue ä¸å¦‚ä½•æ­£ç¡®ç†è§£å’Œä½¿ç”¨ Kubernetes æœ‰å…³ï¼Œå¹¶æœªå‘ˆç°å‡ºä»»ä½•å¯å¯¼è‡´å®‰å…¨æ”»å‡»æˆ–å®‰å…¨ç¼ºé™·çš„è¡Œä¸ºï¼Œæ‰€ä»¥æ— éœ€æä¾›å¤ç°è„šæœ¬ã€‚è¯¥é—®é¢˜å±äºæ­£å¸¸é…ç½®åŠä½¿ç”¨ä¸­äº§ç”Ÿçš„å›°æƒ‘æˆ–åŠŸèƒ½ä½“éªŒä¸ä¸€è‡´ï¼Œä¸æ¶‰åŠå®‰å…¨é£æ§èŒƒç•´ã€‚

---


## Issue #131513 DRA kubelet: locked cache operation: unable to get claim info for claim

- Issue é“¾æ¥ï¼š[#131513](https://github.com/kubernetes/kubernetes/issues/131513)

### Issue å†…å®¹

#### What happened?

I was stress-testing starting and stopping a pod when after a few iterations, kubelet permanently (or more precisely, for the last 8 minutes despite retries) failed to set up the container:

```
  Warning  Failed     3m22s (x26 over 8m29s)  kubelet            Error: locked cache operation: unable to get claim info for claim external-claim in namespace default
```

```
I0428 18:27:13.244772  138526 kuberuntime_sandbox_linux.go:62] "Enforcing CFS quota" pod="default/test-external-claim" unlimited=false
I0428 18:27:13.244802  138526 kuberuntime_manager.go:1333] "Creating container in pod" containerType="container" container="&Container{Name:with-resource,Image:registry.k8s.io/e2e-test-images/busybox:1.36.1-1,Command:[sh -c set && mount && ls -la /dev/ && sleep 10000],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{ResourceClaim{Name:resource,Request:,},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jpgs8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,}" pod="default/test-external-claim"
I0428 18:27:13.246378  138526 event.go:389] "Event occurred" object="default/test-external-claim" fieldPath="spec.containers{with-resource}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulled" message="Container image \"registry.k8s.io/e2e-test-images/busybox:1.36.1-1\" already present on machine"
E0428 18:27:13.246434  138526 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:with-resource,Image:registry.k8s.io/e2e-test-images/busybox:1.36.1-1,Command:[sh -c set && mount && ls -la /dev/ && sleep 10000],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{ResourceClaim{Name:resource,Request:,},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jpgs8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod test-external-claim_default(cc3b4525-15da-4393-9270-9322f22bc62a): CreateContainerConfigError: locked cache operation: unable to get claim info for claim external-claim in namespace default" logger="UnhandledError"
I0428 18:27:13.246457  138526 event.go:389] "Event occurred" object="default/test-external-claim" fieldPath="spec.containers{with-resource}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: locked cache operation: unable to get claim info for claim external-claim in namespace default"
I0428 18:27:13.247649  138526 kubelet.go:1899] "SyncPod exit" pod="default/test-external-claim" podUID="cc3b4525-15da-4393-9270-9322f22bc62a" isTerminal=false
E0428 18:27:13.247681  138526 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"with-resource\" with CreateContainerConfigError: \"locked cache operation: unable to get claim info for claim external-claim in namespace default\"" pod="default/test-external-claim" podUID="cc3b4525-15da-4393-9270-9322f22bc62a"
I0428 18:27:13.247719  138526 pod_workers.go:1338] "Processing pod event done" pod="default/test-external-claim" podUID="cc3b4525-15da-4393-9270-9322f22bc62a" updateType="sync"
```

#### What did you expect to happen?

Pod should start.

#### How can we reproduce it (as minimally and precisely as possible)?

Bring up local-up cluster:
```
make -j && sudo rm -rf /var/run/kubernetes /var/lib/kubelet/dra_manager_state && RUNTIME_CONFIG=resource.k8s.io/v1alpha3,resource.k8s.io/v1beta1,resource.k8s.io/v1beta2 FEATURE_GATES=AuthorizeWithSelectors=true,AuthorizeNodeWithSelectors=true,DynamicResourceAllocation=true,ContextualLogging=true,DRAPrioritizedList=true,DRAAdminAccess=true,DRAResourceClaimDeviceStatus=true,DRADeviceTaints=true KUBELET_RESOLV_CONF="/etc/resolv-9999.conf" DNS_ADDON="coredns" CGROUP_DRIVER=systemd CONTAINER_RUNTIME_ENDPOINT=unix:///var/run/crio/crio.sock LOG_LEVEL=6 ENABLE_CSI_SNAPSHOTTER=false API_SECURE_PORT=6444 ALLOW_PRIVILEGED=1 PATH=/nvme/gopath/src/k8s.io/kubernetes/third_party/etcd:$PATH ./hack/local-up-cluster.sh -O
```

Patch master or use https://github.com/kubernetes/kubernetes/pull/131526:
```patch
diff --git a/test/e2e/dra/test-driver/README.md b/test/e2e/dra/test-driver/README.md
index 11a7ea9dbc9..969a4ddab9d 100644
--- a/test/e2e/dra/test-driver/README.md
+++ b/test/e2e/dra/test-driver/README.md
@@ -15,7 +15,7 @@ testing.
 
 Valid parameters are key/value string pairs stored in a ConfigMap.
 Those get copied into the ResourceClaimStatus with "user_" and "admin_" as
-prefix, depending on whether they came from the ResourceClaim or ResourceClass.
+prefix, depending on whether they came from the ResourceClaim or DeviceClass.
 They get stored in the `ResourceHandle` field as JSON map by the controller.
 The kubelet plugin then sets these attributes as environment variables in each
 container that uses the resource.
@@ -71,7 +71,7 @@ KUBECONFIG=/var/run/kubernetes/admin.kubeconfig go run ./test/e2e/dra/test-drive
 And finally:
 ```console
 $ export KUBECONFIG=/var/run/kubernetes/admin.kubeconfig
-$ kubectl create -f test/e2e/dra/test-driver/deploy/example/resourceclass.yaml
+$ kubectl create -f test/e2e/dra/test-driver/deploy/example/deviceclass.yaml
 resourceclass/example created
 $ kubectl create -f test/e2e/dra/test-driver/deploy/example/pod-inline.yaml
 configmap/pause-claim-parameters created
diff --git a/test/e2e/dra/test-driver/deploy/example/pod-external.yaml b/test/e2e/dra/test-driver/deploy/example/pod-external.yaml
index 59d75e9086a..705312d058a 100644
--- a/test/e2e/dra/test-driver/deploy/example/pod-external.yaml
+++ b/test/e2e/dra/test-driver/deploy/example/pod-external.yaml
@@ -24,13 +24,14 @@ spec:
   containers:
   - name: with-resource
     image: registry.k8s.io/e2e-test-images/busybox:1.36.1-1
-    command: ["sh", "-c", "set && mount && ls -la /dev/"]
+    command: ["sh", "-c", "set && mount && ls -la /dev/ && sleep 10000"]
     resources:
       claims:
       - name: resource
   - name: without-resource
     image: registry.k8s.io/e2e-test-images/busybox:1.36.1-1
-    command: ["sh", "-c", "set && mount && ls -la /dev/"]
+    command: ["sh", "-c", "set && mount && ls -la /dev/ && sleep 10000"]
+  terminationGracePeriodSeconds: 0 # Shut down immediately.
   resourceClaims:
   - name: resource
     resourceClaimName: external-claim
```

In one console, run the test-driver:
```
sudo mkdir -p /var/run/cdi
sudo mkdir -p /var/lib/kubelet/plugins/test-driver.cdi.k8s.io
sudo mkdir -p /var/lib/kubelet/plugins_registry
sudo chmod a+rx /var/lib/kubelet /var/lib/kubelet/plugins
sudo chmod a+rwx /var/run/cdi /var/lib/kubelet/plugins_registry /var/lib/kubelet/plugins/test-driver.cdi.k8s.io
KUBECONFIG=/var/run/kubernetes/admin.kubeconfig go run ./test/e2e/dra/test-driver -v=5 kubelet-plugin --node-name=127.0.0.1
```

In another, set KUBECONFIG and run:
```
while kubectl create -f test/e2e/dra/test-driver/deploy/example/pod-external.yaml && kubectl wait --for=condition=ready --timeout=10s  pod/test-external-claim &&  kubectl delete --timeout=4s -f test/e2e/dra/test-driver/deploy/example/pod-external.yaml && kubectl get resourceclaims; do true; done
```


#### Anything else we need to know?

/sig node
/wg device-management
/assign @bart0sh 
/cc @klueska


#### Kubernetes version

master = 3a8af5a174a3024d7c540b1f6e08aff21b2d0c83

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»Issueå†…å®¹æ¥çœ‹ï¼Œé—®é¢˜æ˜¯kubeletåœ¨è¿›è¡ŒPodå¯åŠ¨å’Œåœæ­¢å‹åŠ›æµ‹è¯•æ—¶ï¼Œå‡ºç°äº†â€œlocked cache operation: unable to get claim info for claimâ€çš„æŠ¥é”™ï¼Œå¯¼è‡´å®¹å™¨æ— æ³•æ­£å¸¸å¯åŠ¨ã€‚è¯¥é—®é¢˜å±äºå¯¹èµ„æºClaimä¿¡æ¯çš„è·å–æˆ–ç®¡ç†å‡ºç°å¼‚å¸¸ï¼Œå¯èƒ½æ˜¯Dynamic Resource Allocationç›¸å…³åŠŸèƒ½æˆ–æ’ä»¶åœ¨å®ç°æˆ–çŠ¶æ€ç®¡ç†ä¸Šå­˜åœ¨ç¼ºé™·æ‰€è‡´ã€‚  
ä»å®‰å…¨è§’åº¦åˆ†æï¼Œå¹¶æœªå‘ç°å­˜åœ¨æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€è®¿é—®æƒé™æå‡æˆ–å…¶ä»–å¯è¢«æ”»å‡»è€…æ¶æ„åˆ©ç”¨çš„å®‰å…¨é£é™©ã€‚è¯¥æŠ¥é”™çœ‹èµ·æ¥æ›´åƒæ˜¯ä¸€ä¸ªkubeletæˆ–DRAæ’ä»¶çš„åŠŸèƒ½æ€§/ç¨³å®šæ€§Bugï¼Œè€Œå¹¶éå®‰å…¨æ¼æ´ï¼Œä¹Ÿæ²¡æœ‰è¿¹è±¡è¡¨æ˜èƒ½å¤Ÿè¢«éæˆæƒç”¨æˆ·åˆ©ç”¨å®ç°æ”»å‡»ã€‚å› æ­¤ï¼Œè¯¥Issueä¸å®‰å…¨é£é™©æ— å…³ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# æœ¬Issueä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ— éœ€ç»™å‡ºå¤ç°è„šæœ¬
# è¯¥è„šæœ¬ä»…åšå ä½ç¤ºä¾‹
def main():
    print("æ­¤é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œæ— éœ€å¤ç°POCã€‚")
```


---


## Issue #131512 spec.jobTemplate.spec.podFailurePolicy.rules[0].onPodConditions[0].status: Required value: valid values: ["False" "True" "Unknown"]

- Issue é“¾æ¥ï¼š[#131512](https://github.com/kubernetes/kubernetes/issues/131512)

### Issue å†…å®¹

#### What happened?

When I try to apply (patching) an existing `CronJob` with the following `podFailurePolicy`:
```
      podFailurePolicy:
        rules:
        - action: Ignore
          onPodConditions:
          - type: DisruptionTarget
```

I'm getting the following Error:
```
error when patching "/dev/shm/2794561279": CronJob.batch "mycronjob" is invalid: spec.jobTemplate.spec.podFailurePolicy.rules[0].onPodConditions[0].status: Required value: valid values: ["False" "True" "Unknown"]
```


Following the docs ([link1](https://kubernetes.io/blog/2024/08/19/kubernetes-1-31-pod-failure-policy-for-jobs-goes-ga/), [link2](https://kubernetes.io/blog/2024/08/19/kubernetes-1-31-pod-failure-policy-for-jobs-goes-ga/)), it seems to be optional.

same in code(?)

https://github.com/kubernetes/api/blob/c5439348b1cf0e96c7446a1bef9475553530f12f/batch/v1/types.go#L214


#### What did you expect to happen?

should be applied without any errors

#### How can we reproduce it (as minimally and precisely as possible)?

create a sample cronjob with these configurations: 

```
apiVersion: batch/v1
kind: CronJob
metadata:
  name: mycronjob
  labels:
    app: release-name
    app.kubernetes.io/name: "release-name"
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/part-of: mycronjob
spec:
  schedule: "*/10 * * * *"
  suspend: false
  concurrencyPolicy: Forbid
  startingDeadlineSeconds: 100
  successfulJobsHistoryLimit: 2
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      ttlSecondsAfterFinished: 172800 
      backoffLimit: 2
      parallelism: 1
      activeDeadlineSeconds: 1800
      podFailurePolicy:
        rules:
        - action: Ignore
          onPodConditions:
          - type: DisruptionTarget
...
```

#### Anything else we need to know?

workaround: adding the field `status: 'True'` fix it

```
    podFailurePolicy:
      rules:
      - action: Ignore
        onPodConditions:
        - type: DisruptionTarget
          status: 'True'
```

#### Kubernetes version

<details>

```console
$ kubectl version
v1.31.7-eks-bcf3d70
```

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä» Issue æè¿°æ¥çœ‹ï¼Œè¿™ä¸ªé—®é¢˜ä¸ Kubernetes ä¸­ `podFailurePolicy` å­—æ®µéªŒè¯é€»è¾‘äº§ç”Ÿå†²çªæœ‰å…³ï¼Œæ–‡æ¡£ä¸å®é™…ä»£ç è¡Œä¸ºä¸ä¸€è‡´ï¼Œå¯¼è‡´åœ¨æœªæŒ‡å®š `status` å­—æ®µæ—¶æ— æ³•æ­£å¸¸åº”ç”¨ `CronJob`ã€‚è¯¥é—®é¢˜å±äº Kubernetes åŠŸèƒ½é…ç½®æˆ–ä½¿ç”¨æ–¹å¼ä¸Šçš„ä¸ä¸€è‡´ï¼Œå¹¶ä¸æ¶‰åŠä»»ä½•æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€æƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œç­‰å®‰å…¨é£é™©ï¼Œä¹Ÿæ— æ³•è¢«å¤–éƒ¨æ”»å‡»è€…ç”¨æ¥è¿›è¡Œæ¶æ„åˆ©ç”¨ï¼Œå› æ­¤ä¸å±äºå®‰å…¨æ¼æ´ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

æœ¬é—®é¢˜æ˜¯ä¸€ä¸ªä½¿ç”¨æˆ–é…ç½®ä¸Šçš„é”™è¯¯ï¼Œä¸ä¼šå¯¼è‡´å¤–éƒ¨æ”»å‡»è€…åˆ©ç”¨è¯¥é”™è¯¯è¿›è¡Œæ”»å‡»æˆ–ç ´åï¼Œä¸å­˜åœ¨ä»»ä½•æƒé™æå‡ã€ä¿¡æ¯æ³„éœ²ç­‰å®‰å…¨åæœï¼Œå› æ­¤ä¸å±äºå®‰å…¨é£é™©èŒƒå›´ã€‚æ­¤ç±»â€œæ–‡æ¡£ä¸å®é™…å®ç°ä¸ä¸€è‡´â€çš„é—®é¢˜ï¼Œä¼šåœ¨åç»­ç‰ˆæœ¬æˆ–è¡¥ä¸ä¸­ä¿®å¤ï¼Œä½†ä¸å®‰å…¨æ¼æ´æ— å…³ã€‚åœ†æ»¡è§£å†³æ–¹æ¡ˆæ˜¯æŒ‰ç…§è¦æ±‚åœ¨é…ç½®ä¸­ä¸º `status` å­—æ®µèµ‹å€¼ï¼Œæˆ–è€…ç­‰å¾…å®˜æ–¹ä¿®å¤æˆ–æ›´æ–°æ–‡æ¡£ã€‚è¯¥é—®é¢˜ä¸éœ€è¦ä½¿ç”¨ Python è„šæœ¬è¿›è¡Œæ¼æ´å¤ç°æ¼”ç¤ºã€‚

---


## Issue #131497 externalIPs DNAT rules are not installed when clusterIP is None

- Issue é“¾æ¥ï¼š[#131497](https://github.com/kubernetes/kubernetes/issues/131497)

### Issue å†…å®¹

#### What happened?

Consider the following service:
```yaml
apiVersion: v1
kind: Service
metadata:
  name: kube-bench-dev
spec:
  clusterIP: None
  selector:
    instance: kube-bench-dev
  ports:
  - name: tcp-80
    port: 80
    protocol: TCP
    targetPort: 80
  externalIPs:
  - 192.168.64.253
```
```
% kubectl get service kube-bench-dev 
NAME             TYPE        CLUSTER-IP   EXTERNAL-IP      PORT(S)   AGE
kube-bench-dev   ClusterIP   None         192.168.64.253   80/TCP    6s
```
But no DNAT rules for the external IP:
```
table ip kube-proxy {
	comment "rules for kube-proxy"
	set cluster-ips {
		type ipv4_addr
		comment "Active ClusterIPs"
		elements = { 172.16.0.1, 172.16.0.173,
			     172.16.0.220 }
	}

	set nodeport-ips {
		type ipv4_addr
		comment "IPs that accept NodePort traffic"
		elements = { 192.168.64.2 }
	}

	map no-endpoint-services {
		type ipv4_addr . inet_proto . inet_service : verdict
		comment "vmap to drop or reject packets to services with no endpoints"
	}

	map no-endpoint-nodeports {
		type inet_proto . inet_service : verdict
		comment "vmap to drop or reject packets to service nodeports with no endpoints"
	}

	map firewall-ips {
		type ipv4_addr . inet_proto . inet_service : verdict
		comment "destinations that are subject to LoadBalancerSourceRanges"
	}

	map service-ips {
		type ipv4_addr . inet_proto . inet_service : verdict
		comment "ClusterIP, ExternalIP and LoadBalancer IP traffic"
		elements = { 172.16.0.173 . tcp . 80 : goto service-TPLZMVKW-kube-system/ingress-nginx-controller/tcp/http,
			     192.168.64.254 . tcp . 80 : goto external-TPLZMVKW-kube-system/ingress-nginx-controller/tcp/http,
			     172.16.0.1 . tcp . 443 : goto service-2QRHZV4L-default/kubernetes/tcp/https,
			     172.16.0.173 . tcp . 443 : goto service-HNB4FGVK-kube-system/ingress-nginx-controller/tcp/https,
			     172.16.0.220 . tcp . 443 : goto service-FMTKUH45-kube-system/ingress-nginx-controller-admission/tcp/https-webhook,
			     192.168.64.254 . tcp . 443 : goto external-HNB4FGVK-kube-system/ingress-nginx-controller/tcp/https }
	}

	map service-nodeports {
		type inet_proto . inet_service : verdict
		comment "NodePort traffic"
	}

	chain filter-prerouting {
		type filter hook prerouting priority dstnat - 10; policy accept;
		ct state new jump firewall-check
	}

	chain filter-input {
		type filter hook input priority -110; policy accept;
		ct state new jump nodeport-endpoints-check
		ct state new jump service-endpoints-check
	}

	chain filter-forward {
		type filter hook forward priority -110; policy accept;
		ct state new jump service-endpoints-check
		ct state new jump cluster-ips-check
	}

	chain filter-output {
		type filter hook output priority dstnat - 10; policy accept;
		ct state new jump service-endpoints-check
		ct state new jump firewall-check
	}

	chain filter-output-post-dnat {
		type filter hook output priority dstnat + 10; policy accept;
		ct state new jump cluster-ips-check
	}

	chain nat-prerouting {
		type nat hook prerouting priority dstnat; policy accept;
		jump services
	}

	chain nat-output {
		type nat hook output priority dstnat; policy accept;
		jump services
	}

	chain nat-postrouting {
		type nat hook postrouting priority srcnat; policy accept;
		jump masquerading
	}

	chain nodeport-endpoints-check {
		ip daddr @nodeport-ips meta l4proto . th dport vmap @no-endpoint-nodeports
	}

	chain service-endpoints-check {
		ip daddr . meta l4proto . th dport vmap @no-endpoint-services
	}

	chain firewall-check {
		ip daddr . meta l4proto . th dport vmap @firewall-ips
	}

	chain services {
		ip daddr . meta l4proto . th dport vmap @service-ips
		ip daddr @nodeport-ips meta l4proto . th dport vmap @service-nodeports
	}

	chain masquerading {
		meta mark & 0x00004000 == 0x00000000 return
		meta mark set meta mark ^ 0x00004000
		masquerade fully-random
	}

	chain cluster-ips-check {
		ip daddr @cluster-ips reject comment "Reject traffic to invalid ports of ClusterIPs"
	}

	chain mark-for-masquerade {
		meta mark set meta mark | 0x00004000
	}

	chain reject-chain {
		comment "helper for @no-endpoint-services / @no-endpoint-nodeports"
		reject
	}

	chain endpoint-KUBDMD37-default/kubernetes/tcp/https__192.168.64.2/6443 {
		ip saddr 192.168.64.2 jump mark-for-masquerade
		meta l4proto tcp dnat to 192.168.64.2:6443
	}

	chain service-2QRHZV4L-default/kubernetes/tcp/https {
		ip daddr 172.16.0.1 tcp dport 443 ip saddr != 192.168.64.64/26 jump mark-for-masquerade
		numgen random mod 1 vmap { 0 : goto endpoint-KUBDMD37-default/kubernetes/tcp/https__192.168.64.2/6443 }
	}

	chain endpoint-5UYISHKM-kube-system/ingress-nginx-controller/tcp/http__192.168.64.68/80 {
		ip saddr 192.168.64.68 jump mark-for-masquerade
		meta l4proto tcp dnat to 192.168.64.68:80
	}

	chain service-TPLZMVKW-kube-system/ingress-nginx-controller/tcp/http {
		ip daddr 172.16.0.173 tcp dport 80 ip saddr != 192.168.64.64/26 jump mark-for-masquerade
		numgen random mod 1 vmap { 0 : goto endpoint-5UYISHKM-kube-system/ingress-nginx-controller/tcp/http__192.168.64.68/80 }
	}

	chain external-TPLZMVKW-kube-system/ingress-nginx-controller/tcp/http {
		jump mark-for-masquerade
		goto service-TPLZMVKW-kube-system/ingress-nginx-controller/tcp/http
	}

	chain endpoint-VRCVTPLF-kube-system/ingress-nginx-controller/tcp/https__192.168.64.68/443 {
		ip saddr 192.168.64.68 jump mark-for-masquerade
		meta l4proto tcp dnat to 192.168.64.68:443
	}

	chain service-HNB4FGVK-kube-system/ingress-nginx-controller/tcp/https {
		ip daddr 172.16.0.173 tcp dport 443 ip saddr != 192.168.64.64/26 jump mark-for-masquerade
		numgen random mod 1 vmap { 0 : goto endpoint-VRCVTPLF-kube-system/ingress-nginx-controller/tcp/https__192.168.64.68/443 }
	}

	chain external-HNB4FGVK-kube-system/ingress-nginx-controller/tcp/https {
		jump mark-for-masquerade
		goto service-HNB4FGVK-kube-system/ingress-nginx-controller/tcp/https
	}

	chain endpoint-XIULVOT6-kube-system/ingress-nginx-controller-admission/tcp/https-webhook__192.168.64.68/8443 {
		ip saddr 192.168.64.68 jump mark-for-masquerade
		meta l4proto tcp dnat to 192.168.64.68:8443
	}

	chain service-FMTKUH45-kube-system/ingress-nginx-controller-admission/tcp/https-webhook {
		ip daddr 172.16.0.220 tcp dport 443 ip saddr != 192.168.64.64/26 jump mark-for-masquerade
		numgen random mod 1 vmap { 0 : goto endpoint-XIULVOT6-kube-system/ingress-nginx-controller-admission/tcp/https-webhook__192.168.64.68/8443 }
	}

	chain endpoint-R3GEKHA3-default/kube-bench-dev/tcp/tcp-80__192.168.64.66/80 {
	}

	chain service-KQA2VLMF-default/kube-bench-dev/tcp/tcp-80 {
	}
}
```

#### What did you expect to happen?

External IP service DNAT rules should be installed, like when clusterIP is not None:
```
table ip kube-proxy {
	comment "rules for kube-proxy"
	set cluster-ips {
		type ipv4_addr
		comment "Active ClusterIPs"
		elements = { 172.16.0.1, 172.16.0.173,
			     172.16.0.220, 172.16.0.242 }
	}

	set nodeport-ips {
		type ipv4_addr
		comment "IPs that accept NodePort traffic"
		elements = { 192.168.64.2 }
	}

	map no-endpoint-services {
		type ipv4_addr . inet_proto . inet_service : verdict
		comment "vmap to drop or reject packets to services with no endpoints"
	}

	map no-endpoint-nodeports {
		type inet_proto . inet_service : verdict
		comment "vmap to drop or reject packets to service nodeports with no endpoints"
	}

	map firewall-ips {
		type ipv4_addr . inet_proto . inet_service : verdict
		comment "destinations that are subject to LoadBalancerSourceRanges"
	}

	map service-ips {
		type ipv4_addr . inet_proto . inet_service : verdict
		comment "ClusterIP, ExternalIP and LoadBalancer IP traffic"
		elements = { 172.16.0.173 . tcp . 80 : goto service-TPLZMVKW-kube-system/ingress-nginx-controller/tcp/http,
			     172.16.0.242 . tcp . 80 : goto service-KQA2VLMF-default/kube-bench-dev/tcp/tcp-80,
			     192.168.64.253 . tcp . 80 : goto external-KQA2VLMF-default/kube-bench-dev/tcp/tcp-80,
			     192.168.64.254 . tcp . 80 : goto external-TPLZMVKW-kube-system/ingress-nginx-controller/tcp/http,
			     172.16.0.1 . tcp . 443 : goto service-2QRHZV4L-default/kubernetes/tcp/https,
			     172.16.0.173 . tcp . 443 : goto service-HNB4FGVK-kube-system/ingress-nginx-controller/tcp/https,
			     172.16.0.220 . tcp . 443 : goto service-FMTKUH45-kube-system/ingress-nginx-controller-admission/tcp/https-webhook,
			     192.168.64.254 . tcp . 443 : goto external-HNB4FGVK-kube-system/ingress-nginx-controller/tcp/https }
	}

	map service-nodeports {
		type inet_proto . inet_service : verdict
		comment "NodePort traffic"
	}

	chain filter-prerouting {
		type filter hook prerouting priority dstnat - 10; policy accept;
		ct state new jump firewall-check
	}

	chain filter-input {
		type filter hook input priority -110; policy accept;
		ct state new jump nodeport-endpoints-check
		ct state new jump service-endpoints-check
	}

	chain filter-forward {
		type filter hook forward priority -110; policy accept;
		ct state new jump service-endpoints-check
		ct state new jump cluster-ips-check
	}

	chain filter-output {
		type filter hook output priority dstnat - 10; policy accept;
		ct state new jump service-endpoints-check
		ct state new jump firewall-check
	}

	chain filter-output-post-dnat {
		type filter hook output priority dstnat + 10; policy accept;
		ct state new jump cluster-ips-check
	}

	chain nat-prerouting {
		type nat hook prerouting priority dstnat; policy accept;
		jump services
	}

	chain nat-output {
		type nat hook output priority dstnat; policy accept;
		jump services
	}

	chain nat-postrouting {
		type nat hook postrouting priority srcnat; policy accept;
		jump masquerading
	}

	chain nodeport-endpoints-check {
		ip daddr @nodeport-ips meta l4proto . th dport vmap @no-endpoint-nodeports
	}

	chain service-endpoints-check {
		ip daddr . meta l4proto . th dport vmap @no-endpoint-services
	}

	chain firewall-check {
		ip daddr . meta l4proto . th dport vmap @firewall-ips
	}

	chain services {
		ip daddr . meta l4proto . th dport vmap @service-ips
		ip daddr @nodeport-ips meta l4proto . th dport vmap @service-nodeports
	}

	chain masquerading {
		meta mark & 0x00004000 == 0x00000000 return
		meta mark set meta mark ^ 0x00004000
		masquerade fully-random
	}

	chain cluster-ips-check {
		ip daddr @cluster-ips reject comment "Reject traffic to invalid ports of ClusterIPs"
	}

	chain mark-for-masquerade {
		meta mark set meta mark | 0x00004000
	}

	chain reject-chain {
		comment "helper for @no-endpoint-services / @no-endpoint-nodeports"
		reject
	}

	chain endpoint-KUBDMD37-default/kubernetes/tcp/https__192.168.64.2/6443 {
		ip saddr 192.168.64.2 jump mark-for-masquerade
		meta l4proto tcp dnat to 192.168.64.2:6443
	}

	chain service-2QRHZV4L-default/kubernetes/tcp/https {
		ip daddr 172.16.0.1 tcp dport 443 ip saddr != 192.168.64.64/26 jump mark-for-masquerade
		numgen random mod 1 vmap { 0 : goto endpoint-KUBDMD37-default/kubernetes/tcp/https__192.168.64.2/6443 }
	}

	chain endpoint-5UYISHKM-kube-system/ingress-nginx-controller/tcp/http__192.168.64.68/80 {
		ip saddr 192.168.64.68 jump mark-for-masquerade
		meta l4proto tcp dnat to 192.168.64.68:80
	}

	chain service-TPLZMVKW-kube-system/ingress-nginx-controller/tcp/http {
		ip daddr 172.16.0.173 tcp dport 80 ip saddr != 192.168.64.64/26 jump mark-for-masquerade
		numgen random mod 1 vmap { 0 : goto endpoint-5UYISHKM-kube-system/ingress-nginx-controller/tcp/http__192.168.64.68/80 }
	}

	chain external-TPLZMVKW-kube-system/ingress-nginx-controller/tcp/http {
		jump mark-for-masquerade
		goto service-TPLZMVKW-kube-system/ingress-nginx-controller/tcp/http
	}

	chain endpoint-VRCVTPLF-kube-system/ingress-nginx-controller/tcp/https__192.168.64.68/443 {
		ip saddr 192.168.64.68 jump mark-for-masquerade
		meta l4proto tcp dnat to 192.168.64.68:443
	}

	chain service-HNB4FGVK-kube-system/ingress-nginx-controller/tcp/https {
		ip daddr 172.16.0.173 tcp dport 443 ip saddr != 192.168.64.64/26 jump mark-for-masquerade
		numgen random mod 1 vmap { 0 : goto endpoint-VRCVTPLF-kube-system/ingress-nginx-controller/tcp/https__192.168.64.68/443 }
	}

	chain external-HNB4FGVK-kube-system/ingress-nginx-controller/tcp/https {
		jump mark-for-masquerade
		goto service-HNB4FGVK-kube-system/ingress-nginx-controller/tcp/https
	}

	chain endpoint-XIULVOT6-kube-system/ingress-nginx-controller-admission/tcp/https-webhook__192.168.64.68/8443 {
		ip saddr 192.168.64.68 jump mark-for-masquerade
		meta l4proto tcp dnat to 192.168.64.68:8443
	}

	chain service-FMTKUH45-kube-system/ingress-nginx-controller-admission/tcp/https-webhook {
		ip daddr 172.16.0.220 tcp dport 443 ip saddr != 192.168.64.64/26 jump mark-for-masquerade
		numgen random mod 1 vmap { 0 : goto endpoint-XIULVOT6-kube-system/ingress-nginx-controller-admission/tcp/https-webhook__192.168.64.68/8443 }
	}

	chain endpoint-R3GEKHA3-default/kube-bench-dev/tcp/tcp-80__192.168.64.66/80 {
		ip saddr 192.168.64.66 jump mark-for-masquerade
		meta l4proto tcp dnat to 192.168.64.66:80
	}

	chain service-KQA2VLMF-default/kube-bench-dev/tcp/tcp-80 {
		ip daddr 172.16.0.242 tcp dport 80 ip saddr != 192.168.64.64/26 jump mark-for-masquerade
		numgen random mod 1 vmap { 0 : goto endpoint-R3GEKHA3-default/kube-bench-dev/tcp/tcp-80__192.168.64.66/80 }
	}

	chain external-KQA2VLMF-default/kube-bench-dev/tcp/tcp-80 {
		jump mark-for-masquerade
		goto service-KQA2VLMF-default/kube-bench-dev/tcp/tcp-80
	}
}
```

#### How can we reproduce it (as minimally and precisely as possible)?

Apply the yaml

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.33.2

#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue ä¸»è¦åæ˜ äº†å½“ Service çš„ â€œclusterIP: Noneâ€ æ—¶ï¼Œkube-proxy æœªæŒ‰é¢„æœŸå®‰è£…å¯¹åº”çš„å¤–éƒ¨ IP DNAT è§„åˆ™ï¼Œå¯¼è‡´æœåŠ¡å¤–éƒ¨ IP ç«¯è®¿é—®å¼‚å¸¸ã€‚æ­¤é—®é¢˜çœ‹èµ·æ¥æ˜¯ä¸€ä¸ªåŠŸèƒ½æˆ–é…ç½®ä¸Šçš„ Bug/ç¼ºé™·ï¼Œå¹¶æœªæåŠå¯è¢«æ¶æ„åˆ©ç”¨æˆ–é€ æˆé¢å¤–æ”»å‡»é¢çš„å¯èƒ½æ€§ï¼Œä¹Ÿæœªæ¶‰åŠå‡­æ®æ³„éœ²ã€å‘½ä»¤æ‰§è¡Œã€æ‹’ç»æœåŠ¡ç­‰å®‰å…¨é—®é¢˜ï¼Œå› æ­¤ä¸å±äºå®‰å…¨é£é™©èŒƒç•´ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

è¯¥é—®é¢˜æœ¬è´¨æ˜¯ç½‘ç»œè§„åˆ™é…ç½®ä¸å®Œæ•´å¯¼è‡´åŠŸèƒ½å¼‚å¸¸ï¼Œä¸å±äºå®‰å…¨æ¼æ´æˆ–å®‰å…¨é£é™©ï¼Œä¸éœ€è¦è¿›ä¸€æ­¥æ¼”ç¤ºå¦‚ä½•åˆ©ç”¨æ¥è¿›è¡Œæ”»å‡»ã€‚æ ¹æ® Issue é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæœ¬é—®é¢˜æœªæ¶‰åŠåˆ°å¯è¢«æ¶æ„åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚å› æ­¤ä¸åšå®‰å…¨å¤ç°è„šæœ¬æ¼”ç¤ºã€‚è‹¥æ˜¯ç”¨äºéªŒè¯è¯¥åŠŸèƒ½ç¼ºé™·ï¼Œå¯æ ¹æ® Issue æä¾›çš„ yaml ç¤ºä¾‹å¦‚å¸¸è§„æµ‹è¯•æ–¹å¼è¿›è¡ŒéªŒè¯ã€‚

---


## Issue #131481 Nodes Tainted with NoExecute before going into PartialDisruption are not untainted after

- Issue é“¾æ¥ï¼š[#131481](https://github.com/kubernetes/kubernetes/issues/131481)

### Issue å†…å®¹

#### What happened?

Nodes Tainted with NoExecute before going into PartialDisruption are not untainted after.

When nodes in a zone start going into unready state, the NLC taints those nodes with NoExecute for taint manager eviction. If more than 55% (UnhealthyZoneThreshold) of nodes go unhealthy in the same zone then the zone is treated as PartialDisruption and depending on the size of the cluster either we slow down the tainting logic or stop it.
https://github.com/kubernetes/kubernetes/blob/e54c8ef2024e638d721242224f6f925b15ee43f5/pkg/controller/nodelifecycle/node_lifecycle_controller.go#L1295

However all nodes that were tainted until NLC determines there is a PartialDisruption is not untainted. But untainting is done if cluster go into MasterDisruption.
https://github.com/kubernetes/kubernetes/blob/e54c8ef2024e638d721242224f6f925b15ee43f5/pkg/controller/nodelifecycle/node_lifecycle_controller.go#L1042

Nodes get tainted with NoExecute for taint-manager eviction, but if more nodes go unhealthy at the same time we determine there is a PartialZonalDisruption and stop tainting those nodes to stop pod eviction. 
Nodes that are already tainted need to be undone to preserve workloads.

#### What did you expect to happen?

Expected NLC to untaint nodes after going into Partial Zonal Disruption to prevent taint-manager-eviction. 

#### How can we reproduce it (as minimally and precisely as possible)?

Induce a case where nodes slowly start going unhealthy in a zone. 


#### Anything else we need to know?
NA

#### Kubernetes version
Shoudl be all kubernetes version. Tested on 1.32


#### Cloud provider
Should be across all CP, tested with AWS EKS. 


#### OS version
NA



#### Install tools
NA


#### Container runtime (CRI) and version (if applicable)
NA


#### Related plugins (CNI, CSI, ...) and versions (if applicable)
NA


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»è¯¥ Issue çš„æè¿°æ¥çœ‹ï¼Œè¿™ä¸€é—®é¢˜ä¸»è¦æ˜¯ K8s é›†ç¾¤ä¸­èŠ‚ç‚¹åœ¨è¿›å…¥ PartialDisruptionï¼ˆå±€éƒ¨ä¸å¯ç”¨ï¼‰ä¹‹å‰å·²ç»è¢«æ·»åŠ äº† NoExecute Taintï¼Œå¯¼è‡´æŸäº›èŠ‚ç‚¹åœ¨è¿›å…¥ PartialDisruption åä¾ç„¶ä¿æŒè¯¥ Taintï¼Œç»“æœæ˜¯å·¥ä½œè´Ÿè½½æ— æ³•æ­£å¸¸è°ƒåº¦åˆ°è¿™äº›èŠ‚ç‚¹ã€‚æ­¤é—®é¢˜æœ¬è´¨ä¸Šå±äº Kubernetes ä¸­èŠ‚ç‚¹çŠ¶æ€ç®¡ç†å’Œå®¹é”™æœºåˆ¶çš„é€»è¾‘Bugæˆ–æ”¹è¿›éœ€æ±‚ï¼Œä¸æ¶‰åŠè¶Šæƒè®¿é—®ã€æ•°æ®æ³„éœ²æˆ–è¿œç¨‹å‘½ä»¤æ‰§è¡Œç­‰å®‰å…¨æ¼æ´åœºæ™¯ï¼Œä¹Ÿä¸æ¶‰åŠæ”»å‡»è€…å¯ä»¥åˆ©ç”¨çš„æ‰‹æ®µå»ææƒæˆ–ç ´åç³»ç»Ÿå®‰å…¨æ€§ã€‚

---


## Issue #131476 Multiple field managers on Server Side Applies lead to inconsistent behavior

- Issue é“¾æ¥ï¼š[#131476](https://github.com/kubernetes/kubernetes/issues/131476)

### Issue å†…å®¹

#### What happened?

What we recently did was drop some affinity rules on a deployment, but those changes are not reflected on the deployment.

We moved from using Server side applies over CLI in CI to ArgoCD which uses Server Side applies as well. Though the fields that ArgoCD owned are reflective of the current state of the manifests and not the state of the manifest as it was applied by CLI.

As a result, we see multiple field managers active owning different fields of the resource and no matter which direction we move now, from client side to server, to purely ArgoCD as the field manager, we cannot reset state of the resource without replacing the resource itself.

We even tried dropping the `managedFields` on `metadata` but on a subsequent SSA, the field manager `before-first-apply` takes over those fields and SSA refuses to update it. 

We tried merging in the field managers but that too does not seem to fix it.

#### What did you expect to happen?

The deployment affinity rules change.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Apply a deployment resource with affinities using SSA over CLI. 
2. Apply the same deployment with a different field manager using SSA without affinities. 

The affinities do not change. Then null, the `managedFields` and apply the same resource without affinities and it does not change.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.3
Kustomize Version: v5.5.0
Server Version: v1.31.6-gke.1064001
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä» Issue å†…å®¹æ¥çœ‹ï¼Œé—®é¢˜ä¸»è¦ä¸ Kubernetes Server Side Applyï¼ˆSSAï¼‰åœ¨åŒä¸€ä¸ªèµ„æºä¸ŠåŒæ—¶å­˜åœ¨å¤šä¸ª field manager æ‰€å¯¼è‡´çš„èµ„æºçŠ¶æ€ä¸ä¸€è‡´æœ‰å…³ï¼Œæ— æ³•æ­£å¸¸æ›´æ–°æˆ–é‡ç½®èµ„æºçŠ¶æ€ã€‚è¯¥é—®é¢˜æœ¬è´¨ä¸Šæ˜¯èµ„æºç®¡ç†åŠæ›´æ–°æ–¹å¼å†²çªçš„åŠŸèƒ½æ€§æˆ–å…¼å®¹æ€§é—®é¢˜ï¼Œå¹¶æœªæ˜ç¡®æåŠå¯èƒ½å¯¼è‡´ä»»æ„ä»£ç æ‰§è¡Œã€æƒé™æå‡ã€æ•°æ®æ³„éœ²æˆ–æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ç­‰å®‰å…¨å±‚é¢çš„éšæ‚£ï¼Œä¹Ÿä¸å±äºå‡­æ®ä¿¡æ¯æ³„éœ²ã€‚  
å› æ­¤ï¼Œè¯¥é—®é¢˜å¹¶ä¸å±äºå®‰å…¨æ¼æ´èŒƒç•´ï¼Œè€Œæ˜¯ Kubernetes ä½¿ç”¨è¿‡ç¨‹ä¸­å‡ºç°çš„é…ç½®æˆ–ç®¡ç†å±‚é¢çš„å†²çªã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥é—®é¢˜ä¸å®‰å…¨æ¼æ´æ— å…³ï¼Œæš‚æ— éœ€å¤ç°è„šæœ¬ã€‚
# å¦‚æœä»éœ€åœ¨æœ¬åœ°æµ‹è¯•å¤š field manager å¼•èµ·çš„å†²çªï¼Œå¯åœ¨æµ‹è¯•ç¯å¢ƒä¸­æ‰‹åŠ¨é‡å¤è¯¥ Issue ä¸­çš„æ“ä½œæ­¥éª¤ï¼Œ
# ä½†ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œä¸é¢å¤–æä¾›æ”»å‡»æ€§æˆ–è€…åˆ©ç”¨æ€§çš„è„šæœ¬ã€‚
```


---


## Issue #131467 Pod scheduling failed due to pod anti-affinity.

- Issue é“¾æ¥ï¼š[#131467](https://github.com/kubernetes/kubernetes/issues/131467)

### Issue å†…å®¹

#### What happened?

The pod is bound to the PVC on the master1 node.However, the current pod is in the pending state.
The log is as follows:
```shell
I0425 08:09:37.487898      25 schedule_one.go:1107] "Unable to schedule pod; no fit; waiting" pod="default/managekvs-1" err="0/14 nodes are available: 1 node(s) didn't match pod anti-affinity rules, 11 node(s) didn't match Pod's node affinity/selector, 2 node(s) had volume node affinity conflict. preemption: 0/14 nodes are available: 1 No preemption victims found for incoming pod, 13 Preemption is not helpful for scheduling."
```
However, when I query the labels of all pods on the node, there is no conflicting label.
```shell
[root@master1 ~]# kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=master1 --show-labels | grep 'app=managekvs'
[root@master1 ~]# 
```
The pod anti-affinity configuration is as follows:
```yaml
podAntiAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
  - labelSelector:
      matchExpressions:
      - key: app
        operator: In
        values:
        - managekvs
    topologyKey: kubernetes.io/hostname
```

https://github.com/kubernetes/kubernetes/blob/948afe5ca072329a73c8e79ed5938717a5cb3d21/pkg/scheduler/framework/plugins/interpodaffinity/filtering.go#L224-L263
It is suspected that the cache in the informer is not the latest. As a result, the old data is obtained.

#### What did you expect to happen?

The pod should be scheduled successfully.

#### How can we reproduce it (as minimally and precisely as possible)?

After the NICs of a node are started and stopped for multiple times, pods cannot be scheduled after the NICs are recovered.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.31
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»è¯¥Issueçš„æè¿°æ¥çœ‹ï¼Œé—®é¢˜æ˜¯ç”±äºPodçš„è°ƒåº¦è¢«Podçš„åäº²å’Œé…ç½®é˜»å¡ï¼Œå¹¶ä¸”æ€€ç–‘æ˜¯Informerç¼“å­˜æœªåŠæ—¶æ›´æ–°å¯¼è‡´æ—§æ•°æ®è¢«ä½¿ç”¨ï¼Œè¿›è€Œé€ æˆè¢«è°ƒåº¦çš„Podä¸€ç›´å¤„äºPendingçŠ¶æ€ã€‚è¯¥é—®é¢˜æœ¬è´¨ä¸Šæ˜¯Kubernetesé›†ç¾¤è¿ç»´æˆ–é…ç½®å±‚é¢ä¸Šçš„é—®é¢˜ï¼Œä¸æ¶‰åŠä»»ä½•æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€ææƒã€å‘½ä»¤æ‰§è¡Œæˆ–å…¶ä»–æ½œåœ¨çš„å®‰å…¨æ”»å‡»åœºæ™¯ï¼Œä¹Ÿæœªæè¿°å‡ºå­˜åœ¨å¯è¢«æ¶æ„åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚å› æ­¤ï¼Œè¯¥Issueå¹¶éå®‰å…¨æ¼æ´æˆ–é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œæ— å®‰å…¨å¤ç°è„šæœ¬
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Issueçš„æ ¹causeä¸ºPodé—´åäº²å’Œï¼ˆanti-affinityï¼‰é…ç½®ä¸è°ƒåº¦è¿‡ç¨‹ä¸­çš„ç¼“å­˜æ•°æ®å‡ºç°ä¸ä¸€è‡´ï¼Œå¯¼è‡´Podæ— æ³•è¢«æ­£ç¡®è°ƒåº¦åˆ°ç›¸åº”èŠ‚ç‚¹ã€‚é—®é¢˜ä¸æ¶‰åŠéšç§ä¿¡æ¯æ³„éœ²ã€ç‰¹æƒæå‡ã€å®¹å™¨é€ƒé€¸ç­‰å®‰å…¨é£é™©ï¼Œå› æ­¤é£é™©è¯„çº§åˆ¤æ–­ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚ä¸Šè¿°è„šæœ¬ä»…ä¸ºç©ºè„šæœ¬ï¼Œç”¨äºåœ¨å›ç­”ä¸­ä¿ç•™å¿…è¦ç»“æ„ï¼Œä¸æ¶‰åŠä»»ä½•å®‰å…¨å¤ç°ã€‚

---


## Issue #131466 [WINDOWS] [KUBEPROXY] HNS LoadBalancer Fails to Update Endpoints After ModifyLoadBalancerAPI Invocation by Windows KubeProxy

- Issue é“¾æ¥ï¼š[#131466](https://github.com/kubernetes/kubernetes/issues/131466)

### Issue å†…å®¹

#### What happened?

I'm using HNS version 15.4, where the ModifyLoadBalancer policy appears to be supported according to hnslib. KubeProxy is invoking the ModifyLoadBalancer API, but the endpoints it attempts to update do not match those reflected in HNS.

#### What did you expect to happen?

Following the ModifyLoadBalancer API call, all the requested endpoints should be successfully updated in HNS.

#### How can we reproduce it (as minimally and precisely as possible)?

- Create an AKS cluster with 1.31+ version of K8S and Windows OS version 2025
- Create a deployment and a loadbalancer service for the deployment.
- Check the endpoint list count in the newly create hns loadbalancer policy.
- Scale up the pods for the deployment and check the endpoint list count for the same hns lob policy. It will still show the old endpoint list.

#### Anything else we need to know?

_No response_

#### Kubernetes version



```console
1.31+
```

</details>


#### Cloud provider

AKS

</details>


#### OS version

Windows Server 2025

```console
PS C:\hpc> cd C:\k\debug
PS C:\k\debug> ipmo -Force .\hns.v2.psm1
WARNING: The names of some imported commands from the module 'hns.v2' include unapproved verbs that might make them less
discoverable. To find the commands with unapproved verbs, run the Import-Module command again with the Verbose parameter. For a list
 of approved verbs, type Get-Verb.
PS C:\k\debug> Invoke-HnsRequest -Type globals -Method GET -Id "version"

Major Minor
----- -----
   15     4


PS C:\k\debug>
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»Issueæè¿°æ¥çœ‹ï¼Œè¯¥é—®é¢˜è¡¨ç°ä¸ºåœ¨Windowsç¯å¢ƒä¸‹ï¼ŒKubeProxyè°ƒç”¨HNSçš„ModifyLoadBalancer APIåï¼ŒHNS LoadBalancerä¸­çš„endpointsä¿¡æ¯æ— æ³•åŠæ—¶æ›´æ–°æˆ–ä¸å®é™…çŠ¶æ€ä¸åŒã€‚è¿™å±äºåŠŸèƒ½æ€§æˆ–å…¼å®¹æ€§é—®é¢˜ï¼Œæœªæ¶‰åŠè¶Šæƒè®¿é—®ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€å‘½ä»¤æ‰§è¡Œã€ææƒæˆ–å®¹å™¨é€ƒé€¸ç­‰å®‰å…¨åœºæ™¯ï¼Œä¹Ÿä¸å±äºå› ä¸å½“é…ç½®é€ æˆçš„ä¿¡æ¯å¤–æ³„é—®é¢˜ã€‚å› æ­¤ï¼Œè¯¥Issueå¹¶ä¸æ¶‰åŠå®‰å…¨æ¼æ´æˆ–å®‰å…¨é£é™©ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

æ­¤å¤„æ‰€è®¨è®ºçš„é—®é¢˜æ˜¯HNS LoadBalanceråœ¨APIè°ƒç”¨åæœªèƒ½åŒæ­¥æ›´æ–°endpointsï¼Œå±äºåŠŸèƒ½æˆ–å…¼å®¹æ€§Bugï¼Œæœªä½“ç°å‡ºä»»ä½•å¯è¢«åˆ©ç”¨çš„å®‰å…¨æ”»å‡»é¢ï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚æ•…æ— éœ€æä¾›å®‰å…¨æ¼æ´å¤ç°è„šæœ¬ã€‚è¯¥é—®é¢˜ä¸å½±å“é¡¹ç›®å®‰å…¨æ€§ï¼Œå¯åœ¨åç»­æ’æŸ¥åŠŸèƒ½å®ç°ã€æ—¥å¿—ã€APIè°ƒç”¨ç­‰æ–¹é¢è¿›è¡Œä¿®å¤æˆ–æ”¹è¿›ã€‚

---


## Issue #131451 Enhance existing csi, volume_expand tests with recovery related features

- Issue é“¾æ¥ï¼š[#131451](https://github.com/kubernetes/kubernetes/issues/131451)

### Issue å†…å®¹

#### What happened?

We should enhance existing `testsuites/volume_expand.go` with recover from volume_expansion related features, so as these tests are more robust and tested with all kind of CSI drivers.



#### What did you expect to happen?

Should have more coverage for volume expansion feature. 


#### How can we reproduce it (as minimally and precisely as possible)?

None

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»Issueçš„æ ‡é¢˜å’Œå†…å®¹æ¥çœ‹ï¼Œæè¿°çš„æ˜¯è¦åœ¨ç°æœ‰ä¸csiã€volume_expandç›¸å…³çš„æµ‹è¯•ä¸­ï¼Œå¢åŠ å®¹ç¾å’Œæ¢å¤åœºæ™¯çš„æµ‹è¯•ã€‚è¯¥Issueä¸»è¦é’ˆå¯¹æµ‹è¯•ç”¨ä¾‹çš„å®Œå–„ä¸è¦†ç›–èŒƒå›´çš„æ‰©å±•ï¼Œå¹¶æœªæ¶‰åŠåˆ°æ•æ„Ÿä¿¡æ¯ã€æƒé™ä¸å½“ä½¿ç”¨ã€å®¹å™¨é€ƒé€¸ã€ææƒã€å‘½ä»¤æ‰§è¡Œç­‰å®‰å…¨é£é™©ï¼Œä¹Ÿæ²¡æœ‰æåˆ°ä»»ä½•å¯èƒ½å¯¼è‡´ä¿¡æ¯æ³„éœ²æˆ–ç³»ç»Ÿè¢«æ”»å‡»çš„æƒ…å½¢ã€‚æ ¹æ®Issueé£é™©åˆ¤æ–­æ ‡å‡†ï¼Œåˆ¤æ–­æ­¤é—®é¢˜å¹¶éå®‰å…¨é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# ç”±äºè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œå› æ­¤æ— éœ€æä¾›å®‰å…¨é—®é¢˜å¤ç°è„šæœ¬ã€‚
# ä¸‹é¢ä»…ç»™å‡ºä¸€ä¸ªå ä½è„šæœ¬ç¤ºä¾‹ï¼Œè¯´æ˜å¦‚æœåç»­éœ€è¦æµ‹è¯•ç›¸å…³åœºæ™¯
# å¯ä»¥åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡ŒåŠŸèƒ½æ‹“å±•ï¼Œè€Œå¹¶ä¸å­˜åœ¨å®‰å…¨æ¼æ´å¤ç°ã€‚
import time

def main():
    print("å½“å‰Issueä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ— éœ€å®‰å…¨æ¼æ´å¤ç°è„šæœ¬ã€‚")
    time.sleep(1)
    print("æµ‹è¯•ç»“æŸã€‚")

main()
```


---


## Issue #131448 HPA watch pod metrics calls fail with 422 error code

- Issue é“¾æ¥ï¼š[#131448](https://github.com/kubernetes/kubernetes/issues/131448)

### Issue å†…å®¹

#### What happened?

HorizontalPodAutoscaler is making failed `watch` requests for pod metrics, then falling back to `list` calls which are successful. This doesn't immediately impact functionality but the extra failed requests can cause unnecessary load and skew metrics. 

__Request Details__
<span class="flex flex-column width-100"><span class="logs-table__wrapper"><span style="min-height: 832px; display: block;">
key | value
-- | --
verb | watch
requestURI | /apis/metrics.k8s.io/v1beta1/namespaces/default/pods?allowWatchBookmarks=true&labelSelector=run%3Dphp-apache&resourceVersionMatch=NotOlderThan&sendInitialEvents=true&watch=true
responseStatus.code | 422
user.username | system:serviceaccount:kube-system:horizontal-pod-autoscaler
userAgent | kube-controller-manager/v1.32.3 (linux/amd64) kubernetes/470a1dd/system:serviceaccount:kube-system:horizontal-pod-autoscaler
</span></span></span>

`kube-controller-manager` logs have the following error:
```
13 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: [resourceVersionMatch: Forbidden: resourceVersionMatch is forbidden for watch, resourceVersionMatch: Forbidden: resourceVersionMatch is forbidden unless resourceVersion is provided]
```

It appears that the watch request is missing a required `resourceVersion` parameter.


#### What did you expect to happen?

The watch pod metrics calls to succeed. 

#### How can we reproduce it (as minimally and precisely as possible)?

Create an HPA in a 1.32 cluster via https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/

* start metrics server
* `kubectl apply -f https://k8s.io/examples/application/php-apache.yaml `
* `kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10`

Observe audit logs for 422 errors. 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

kube-controller-manager/v1.32.3

</details>


#### Cloud provider

<details>
</details>


#### OS version

<details>
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>



</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»Issueæè¿°æ¥çœ‹ï¼Œè¯¥é—®é¢˜ä¸»è¦æ˜¯å› ä¸ºHorizontalPodAutoscalerï¼ˆHPAï¼‰åœ¨watchæ—¶ç¼ºå°‘resourceVersionå‚æ•°ï¼Œå¯¼è‡´è¯·æ±‚ä»¥422é”™è¯¯ç»“æŸï¼ŒéšåHPAé€€å›åˆ°ä½¿ç”¨listè¯·æ±‚çš„æ–¹å¼ã€‚æ­¤é—®é¢˜æœ¬è´¨ä¸Šæ˜¯HPAåœ¨è¿›è¡Œç›‘æ§æ—¶å‡ºç°çš„é…ç½®æˆ–å‚æ•°ä½¿ç”¨ä¸å½“å¯¼è‡´çš„åŠŸèƒ½æ€§é”™è¯¯ï¼Œå¹¶æœªæ¶‰åŠæ•æ„Ÿä¿¡æ¯æ³„éœ²ã€å‘½ä»¤æ‰§è¡Œã€ææƒã€å®¹å™¨é€ƒé€¸ç­‰å®‰å…¨ç›¸å…³é£é™©ï¼Œä¹Ÿä¸æ¶‰åŠå¯è¢«æ¶æ„åˆ©ç”¨çš„æ”»å‡»å‘é‡ã€‚å› æ­¤ï¼Œæ ¹æ®ç»™å‡ºçš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥Issueä»…å±äºé…ç½®æˆ–ä½¿ç”¨å±‚é¢çš„é—®é¢˜ï¼Œä¸æ„æˆå®‰å…¨æ¼æ´æˆ–å®‰å…¨é£é™©ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

è¯¥é—®é¢˜ä¸ºHPAç¼ºå°‘æ­£ç¡®çš„resourceVersionå‚æ•°æ‰€å¯¼è‡´çš„watchè¯·æ±‚å¤±è´¥ï¼Œä¸å­˜åœ¨å®‰å…¨æ”»å‡»é¢æˆ–æ•æ„Ÿä¿¡æ¯æ³„éœ²ï¼Œæ•…ä¸å±äºå®‰å…¨é£é™©é—®é¢˜ï¼Œä¸éœ€è¦æä¾›å¤ç°è„šæœ¬ã€‚è¯¥é—®é¢˜å’Œé£é™©æ— å…³ï¼Œå»ºè®®ä»é…ç½®æˆ–åŠŸèƒ½å®ç°è§’åº¦æ’æŸ¥å¹¶ä¿®å¤ã€‚

---


## Issue #131426 Busybox image fails to build for windows

- Issue é“¾æ¥ï¼š[#131426](https://github.com/kubernetes/kubernetes/issues/131426)

### Issue å†…å®¹

#### What happened?

Postmerge job failed for the busybox image
ref - https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/post-kubernetes-push-e2e-busybox-test-images/1915196396294639616

Failed with an error:
```shell
#11 [stage-3  1/14] FROM mcr.microsoft.com/windows/nanoserver:1809@sha256:a728b60718df567cf9b8981be370ad3824b4d60018f33664f0e2477844abc440
#11 resolve mcr.microsoft.com/windows/nanoserver:1809@sha256:a728b60718df567cf9b8981be370ad3824b4d60018f33664f0e2477844abc440 0.0s done
#11 sha256:13684d75c68a31393ea54fcff0661fe84f07894d37c7e65be62e77fa70b23988 41.94MB / 105.25MB 0.3s
#11 sha256:13684d75c68a31393ea54fcff0661fe84f07894d37c7e65be62e77fa70b23988 51.38MB / 105.25MB 0.4s
#11 ...
 #12 [prep 3/9] ADD https://skanthak.homepage.t-online.de/download/curl-7.57.0.cab /curl.cab
#12 ERROR: Get "https://skanthak.homepage.t-online.de/download/curl-7.57.0.cab": tls: failed to verify certificate: x509: certificate signed by unknown authority
 #13 [prep 2/9] ADD https://github.com/kubernetes-sigs/windows-testing/raw/master/images/busybox/busybox.exe /busybox-dir/busybox.exe
#13 CANCELED
```

#### What did you expect to happen?

Build should go through

#### How can we reproduce it (as minimally and precisely as possible)?

```
make all WHAT=busybox
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>
NA
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue ä¸»è¦æè¿°äº†åœ¨æ„å»º Windows å¹³å°çš„ Busybox é•œåƒæ—¶ï¼Œç”±äºè·å–ç¬¬ä¸‰æ–¹ CAB æ–‡ä»¶æ—¶å‡ºç°è¯ä¹¦æ ¡éªŒé”™è¯¯ï¼Œå¯¼è‡´é•œåƒæ„å»ºå¤±è´¥ã€‚æ­¤é—®é¢˜æ›´å¤šä½“ç°ä¸ºç¯å¢ƒæˆ–é…ç½®å±‚é¢çš„æ„å»ºå¤±è´¥ï¼Œå¹¶æœªæ¶‰åŠå¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨æ¼æ´ï¼Œå¦‚ä»£ç æ‰§è¡Œã€æƒé™æå‡ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ç­‰ã€‚å› æ­¤ï¼Œè¯¥ Issue ä¸å±äºé¡¹ç›®å­˜åœ¨çš„å®‰å…¨é—®é¢˜ï¼Œè€Œæ˜¯å› è¯ä¹¦ä¸è¢«ä¿¡ä»»å¯¼è‡´çš„æ„å»ºé”™è¯¯ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥é—®é¢˜ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œä¸éœ€è¦å¤ç°å®‰å…¨æ¼æ´ï¼Œå› æ­¤æ— éœ€æä¾›å¯¹åº”çš„å¤ç°è„šæœ¬ã€‚
# å¦‚éœ€åœ¨æœ¬åœ°é‡ç°æ„å»ºå¤±è´¥ï¼Œå¯åœ¨ Windows ç¯å¢ƒä¸‹æ‰§è¡Œä»¥ä¸‹ä¼ªä»£ç ï¼Œä»…ä¾›å‚è€ƒï¼š
# ------------------------------------------------------------
# è¯·æ³¨æ„ï¼Œè¿™æ®µè„šæœ¬åªæ˜¯æ¨¡æ‹Ÿäº†è¯ä¹¦éªŒè¯å¤±è´¥å¯¼è‡´çš„ä¸‹è½½é”™è¯¯æµç¨‹ï¼Œå¹¶éå®é™…æ¼æ´POC
import subprocess
import time

def main():
    # æ¨¡æ‹Ÿæ„å»ºå‘½ä»¤
    command = ["docker", "build", "-t", "busybox-windows-test", "."]
    print("å¼€å§‹æ¨¡æ‹Ÿæ„å»º Busybox for Windows çš„é•œåƒ...")
    try:
        result = subprocess.run(command, capture_output=True, text=True, timeout=60)
        if result.returncode != 0:
            print("æ„å»ºå¤±è´¥ï¼Œä»¥ä¸‹æ˜¯å¯èƒ½çš„é”™è¯¯è¾“å‡ºï¼š")
            print(result.stderr)
        else:
            print("é•œåƒæ„å»ºæˆåŠŸã€‚")
    except subprocess.TimeoutExpired:
        print("æ„å»ºå‘½ä»¤æ‰§è¡Œè¶…æ—¶ã€‚")
    time.sleep(2)
    print("æ¨¡æ‹Ÿæµç¨‹ç»“æŸã€‚")

main()
```


**è§£é‡Šè¯´æ˜ï¼š**

ä¸Šé¢çš„è„šæœ¬ä»…å±•ç¤ºäº†æ‰§è¡Œ Docker æ„å»ºå‘½ä»¤çš„æµç¨‹ï¼Œä»¥æ¨¡æ‹Ÿåœ¨æœ¬åœ°æ„å»º Busybox for Windows é•œåƒæ—¶å¯èƒ½ä¼šé‡åˆ°çš„ä¸‹è½½æˆ–è¯ä¹¦æ ¡éªŒå¤±è´¥æƒ…å†µã€‚ç”±äºæœ¬ Issue ä¸æ¶‰åŠä»»ä½•å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨æ¼æ´ï¼Œå› æ­¤æ— éœ€æä¾›çœŸå®çš„å®‰å…¨æ¼æ´å¤ç°è„šæœ¬ã€‚è¯¥è„šæœ¬æ›´å¤šæ˜¯å¸®åŠ©å¼€å‘è€…äº†è§£å¦‚ä½•åœ¨æœ¬åœ°æ‰§è¡Œæ„å»ºå‘½ä»¤åŠè§‚å¯Ÿæ„å»ºå¤±è´¥çš„è¿‡ç¨‹ã€‚è‹¥è¦å®é™…è§£å†³è¯ä¹¦é—®é¢˜ï¼Œéœ€è¦åœ¨æ„å»ºç¯å¢ƒä¸­æ›´æ–°æˆ–å®‰è£…ç›¸åº”çš„ CA è¯ä¹¦ï¼Œæˆ–åˆ‡æ¢è‡³å—ä¿¡ä»»çš„ä¸‹è½½æºæ¥å®Œæˆé•œåƒæ„å»ºã€‚

---


## Issue #131421 ConfigMap exists in namespace even after namespace is deleted

- Issue é“¾æ¥ï¼š[#131421](https://github.com/kubernetes/kubernetes/issues/131421)

### Issue å†…å®¹

#### What happened?

A config map continued to exist in the cluster even after the namespace it belonged to was deleted. 

There was concurrent access between the `deleteCollection` issued by the namespace garbage collector and a SSA patch issued by a controller that recreated the ConfigMap at the exact same time.

This resulted in:

```
$ kubectl get ns project-be53bb1076fe4d7eaa153f767406a0f3
Error from server (NotFound): namespaces "project-be53bb1076fe4d7eaa153f767406a0f3" not found

$ kubectl get cm -n project-be53bb1076fe4d7eaa153f767406a0f3
NAME                   DATA   AGE
cluster-internal-cas   1      23h
```

It isn't clear to me why the API server allows object creation via SSA even after the namespace has a `deletionTimestamp` set.


#### What did you expect to happen?

Expected no objects in a namespace when namespace is gone.

#### How can we reproduce it (as minimally and precisely as possible)?

We could not manage to reproduce this, it is possibly a very rare race condition. I'll attach an image of k8s audit logs that we have.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.3
Kustomize Version: v5.5.0
Server Version: v1.30.9
```

</details>


#### Cloud provider

<details>
Azure
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»è¯¥Issueå¯ä»¥çœ‹å‡ºï¼Œè¿™æ˜¯ä¸€ä¸ªå…³äºå‘½åç©ºé—´åˆ é™¤åï¼Œä¾ç„¶å­˜åœ¨å…³è”ConfigMapçš„æƒ…å†µã€‚æ ¹æ®Issueçš„æè¿°ï¼ŒåŸå› æ˜¯å‘½åç©ºé—´è¢«åƒåœ¾å›æ”¶å™¨åˆ é™¤çš„åŒæ—¶ï¼Œæœ‰å¦ä¸€ä¸ªæ§åˆ¶å™¨ä½¿ç”¨Server-Side Apply (SSA) å¹¶å‘åˆ›å»ºäº†åŒåçš„ConfigMapï¼Œå¯¼è‡´åœ¨å·²è¢«æ ‡è®°åˆ é™¤çš„å‘½åç©ºé—´ä¸‹ï¼Œä¾æ—§å‡ºç°ConfigMapçš„â€œæ®‹ç•™â€ç°è±¡ã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼Œè¿™å±äºå‘½åç©ºé—´ä¸­èµ„æºç®¡ç†çš„ç«äº‰æ¡ä»¶ï¼ˆrace conditionï¼‰æˆ–æ¸…ç†é€»è¾‘ç¼ºé™·é—®é¢˜ï¼Œä½†Issueå¹¶æœªæè¿°å› è¿™ä¸ªæ®‹ç•™å¯¼è‡´æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€æƒé™æå‡ã€è¿œç¨‹æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰ä¸€ç³»åˆ—å®‰å…¨åæœï¼ŒåŒæ—¶ä¹Ÿæ²¡æœ‰è¯´æ˜æ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ­¤è¡Œä¸ºåšè¿›ä¸€æ­¥çš„æ¶æ„æ“ä½œã€‚

å› æ­¤ï¼Œè¯¥é—®é¢˜æ›´å€¾å‘äºä¸€ä¸ªä¸šåŠ¡æˆ–é€»è¾‘ä¸Šçš„ä¸€è‡´æ€§ç¼ºé™·ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚

---


## Issue #131419 NodeExpandVolume is called unexpectedly for NFS

- Issue é“¾æ¥ï¼š[#131419](https://github.com/kubernetes/kubernetes/issues/131419)

### Issue å†…å®¹

#### What happened?

While expanding a PVC (`fstype: nfs`) _NodeExpandVolume_ is called even though _ControllerExpandVolume_ returned '`NodeExpansionRequired: false'`. This results in _VolumeResizeFailed_ warning on the application pods though the PV and PVC has resized successfully by the controller.

CSI Driver:
```
csi-powerstore.dellemc.com
csi-resizer:v1.13.1
```

While the driver supports the NodeExpandVolume for the backend block volumes, it doesn't expect this call for the NFS thus the VolumeResizeFailed event.


PVC event:
```
  Normal   ProvisioningSucceeded   114s                  csi-powerstore.dellemc.com_powerstore-controller-5df67c6d96-l82zn_00d7e770-72be-4f87-9bf9-6258b6d7ff73  Successfully provisioned volume csivol-sl-5414b8e21b
  Normal   ExternalExpanding       52s                   volume_expand                                                                                           waiting for an external controller to expand this PVC
  Normal   Resizing                48s                   external-resizer csi-powerstore.dellemc.com                                                             External resizer is resizing volume csivol-sl-5414b8e21b
  Normal   VolumeResizeSuccessful  42s                   external-resizer csi-powerstore.dellemc.com                                                             Resize volume succeeded
```

Pod event:
```
  Warning  VolumeResizeFailed      18m                kubelet                  NodeExpandVolume.NodeExpandVolume failed for volume "csivol-sl-5414b8e21b" : Expander.NodeExpand failed to expand the volume rpc error: code = NotFound desc = Volume not found
  Warning  VolumeResizeFailed      2s (x16 over 18m)  kubelet                  NodeExpandVolume.NodeExpandVolume failed to resize volume for volume "csivol-sl-5414b8e21b" : volume resizing failed for unknown reason
```

Driver log:
```
{"level":"info","msg":"/csi.v1.Controller/ControllerExpandVolume: REQ 0022: VolumeId=68028ddd-9874-8ee8-e8d9-d2f58008526d/<ARRAY-ID>/nfs, CapacityRange=required_bytes:17179869184 , VolumeCapability=mount:\u003cfs_type:\"nfs\" \u003e access_mode:\u003cmode:MULTI_NODE_MULTI_WRITER \u003e , XXX_NoUnkeyedLiteral={}, XXX_sizecache=0","time":"2025-04-18T17:38:58.451308056Z"}
{"level":"info","msg":"/csi.v1.Controller/ControllerExpandVolume: REP 0022: CapacityBytes=17179869184, NodeExpansionRequired=false, XXX_NoUnkeyedLiteral={}, XXX_sizecache=0","time":"2025-04-18T17:39:04.280402864Z"}
```

RESIZER log:
```
I0418 17:39:04.280812       1 expand_and_recover.go:250] "Resize volume succeeded, start to update PV's capacity" PV="csivol-sl-5414b8e21b"
I0418 17:39:04.280841       1 controller.go:616] "Resize volume succeeded, start to update PV's capacity" PV="csivol-sl-5414b8e21b"
I0418 17:39:04.314826       1 resize_status.go:186] "Resize PVC finished" PVC="nsps1/pvcps1"
I0418 17:39:04.314868       1 expand_and_recover.go:195] "Update capacity of PV succeeded" PV="csivol-sl-5414b8e21b" capacity="16Gi"
I0418 17:39:04.314890       1 controller.go:326] "Started PVC processing for resize controller" key="nsps1/pvcps1"
I0418 17:39:04.314953       1 event.go:389] "Event occurred" object="nsps1/pvcps1" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="VolumeResizeSuccessful" message="Resize volume succeeded"
```

#### What did you expect to happen?

There should be no _VolumeResizeFailed_ warning/event on the pod.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a PVC with StorageClass having:
```
    csi.storage.k8s.io/fstype: nfs
    allowVolumeExpansion: true
```
2. Resize the PVC by editing it.
3. PVC is resized, but related Pod will have _VolumeResizeFailed_ warning due to _NodeExpandVolume_ failure since this call was not expected by the CSI driver for NFS.

#### Anything else we need to know?

Please refer https://github.com/kubernetes/kubernetes/issues/131397 and https://github.com/kubernetes/kubernetes/issues/131381

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.3
Kustomize Version: v5.5.0
Server Version: v1.32.3
```

</details>


#### Cloud provider

<details>
--
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Red Hat Enterprise Linux"
VERSION="9.5 (Plow)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="9.5"
PLATFORM_ID="platform:el9"
PRETTY_NAME="Red Hat Enterprise Linux 9.5 (Plow)"
ANSI_COLOR="0;31"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:redhat:enterprise_linux:9::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9"
BUG_REPORT_URL="https://issues.redhat.com/"

REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 9"
REDHAT_BUGZILLA_PRODUCT_VERSION=9.5
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.5"


$ uname -a
# Linux master-1-SPknwwOOoaVwH.domain 5.14.0-503.38.1.el9_5.x86_64 #1 SMP PREEMPT_DYNAMIC Sun Apr 13 22:01:49 EDT 2025 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»è¯¥ Issue çš„æè¿°æ¥çœ‹ï¼Œä¸»è¦é—®é¢˜æ˜¯ NFS ç±»å‹çš„å·åœ¨è¿›è¡Œæ‰©å®¹æ—¶ï¼ŒController ç«¯å·²ç»è¿”å› NodeExpansionRequired = falseï¼Œä½†å®é™…ä»è°ƒç”¨äº† NodeExpandVolume å¯¼è‡´ Pod ä¾§å‡ºç° VolumeResizeFailed è­¦å‘Šã€‚è¯¥é—®é¢˜æ›´å¤šæ˜¯ CSI é©±åŠ¨ä¸ Kubernetes åè°ƒé€»è¾‘ä¸Šçš„ä¸ä¸€è‡´æˆ–å®ç°ç¼ºé™·ï¼Œå¹¶æœªæ¶‰åŠåˆ°æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€å‘½ä»¤æ‰§è¡Œã€æƒé™æå‡ã€DoS æ”»å‡»ç­‰å®‰å…¨æ¼æ´ï¼Œå› æ­¤ä¸å±äºå®‰å…¨é£é™©é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ— éœ€å¤ç°è„šæœ¬
```


---


## Issue #131413 please provide a procedure to shutdown the cluster

- Issue é“¾æ¥ï¼š[#131413](https://github.com/kubernetes/kubernetes/issues/131413)

### Issue å†…å®¹

#### What happened?

Maybe I am just too blind to see, but apparently k8s lacks a procedure to stop the whole cluster, eg to move the hardware to another computing lab, to run the nodes in maintenance mode, etc.

Important features would be

- stop all "regular" work load before persistent storage (eg. Longhorn)
- unmount all volumes (local and remote)
- stop persistent storage before shuttting down the internal networks
- make sure all etcd databases are in sync



#### What did you expect to happen?

NA

#### How can we reproduce it (as minimally and precisely as possible)?

NA

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.3
Kustomize Version: v5.5.0
Server Version: v1.31.7+rke2r1
```

</details>


#### Cloud provider

<details>
on premises
</details>


#### OS version

<details>

```console
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ uname -a
Linux srvl012.ac.aixigo.de 6.1.0-32-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.129-1 (2025-03-06) x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä» Issue çš„æè¿°æ¥çœ‹ï¼Œç”¨æˆ·ä»…ä»…æ˜¯åœ¨è¯¢é—®å¦‚ä½•å®‰å…¨ã€å®Œæ•´åœ°å…³åœ Kubernetes é›†ç¾¤ï¼Œä»¥ä¾¿è¿›è¡Œç¡¬ä»¶è¿ç§»æˆ–ç»´æŠ¤æ“ä½œï¼ŒIssue ä¸­å¹¶æœªæåŠä»»ä½•ä¸æƒé™æå‡ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€è¿œç¨‹å‘½ä»¤æ‰§è¡Œç­‰ç›¸å…³çš„å®‰å…¨é—®é¢˜ï¼Œä¹Ÿä¸å­˜åœ¨é”™è¯¯é…ç½®æˆ–æ—¥å¿—æ•æ„Ÿä¿¡æ¯æ³„éœ²å¯¼è‡´çš„é£é™©ã€‚æ ¹æ®ç»™å‡ºçš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¿™ç±»å•çº¯çš„â€œæ€ä¹ˆåšâ€ä½¿ç”¨é—®é¢˜å¹¶æœªæ¶‰åŠå®‰å…¨æ¼æ´ï¼Œå› æ­¤ä¸å±äºå®‰å…¨é£é™©èŒƒç•´ã€‚

---


## Issue #131410 kubelet does not finish detaching volume from pod, preventing it from starting up again

- Issue é“¾æ¥ï¼š[#131410](https://github.com/kubernetes/kubernetes/issues/131410)

### Issue å†…å®¹

#### What happened?

We noticed that we would have pods stuck in init with:
`Warning  FailedAttachVolume  27m   attachdetach-controller  Multi-Attach error for volume "pvc-1ca296f6-9b88-40ea-a235-f8a0a093b8c1" Volume is already exclusively attached to one node and can't be attached to another`

and looking closer, it was indeed true that the pod's previous node still had the volume attached!

Timeline:
4/3 2:31 pod evicted from node1, but node1 never completed the `UnmountDevice` step and still had the volume in VolumesInUse. However, presumably the volume was actually detached.
4/3 15:51 pod is now evicted from node2, and this time, `UnmountDevice` does complete, but the volume is not actually detached
4/3 15:54+ pod is scheduled to node3 but cannot init because the volume is still attached to node2

Kubelet logs from node1:
`
I0403 02:31:52.912891   23323 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"crmproperties-80-zz-vtdataroot-0\" (UniqueName: \"kubernetes.io/csi/ebs.csi.aws.com^vol-03058d13f643f58a2\") pod \"3b2ef881-f317-4110-ae5d-0e4dbfbdcd29\" (UID: \"3b2ef881-f317-4110-ae5d-0e4dbfbdcd29\") "
I0403 02:31:54.953778   23323 operation_generator.go:803] UnmountVolume.TearDown succeeded for volume "kubernetes.io/csi/ebs.csi.aws.com^vol-03058d13f643f58a2" (OuterVolumeSpecName: "crmproperties-80-zz-vtdataroot-0") pod "3b2ef881-f317-4110-ae5d-0e4dbfbdcd29" (UID: "3b2ef881-f317-4110-ae5d-0e4dbfbdcd29"). InnerVolumeSpecName "pvc-1ca296f6-9b88-40ea-a235-f8a0a093b8c1". PluginName "kubernetes.io/csi", VolumeGidValue ""
I0403 02:31:55.024934   23323 reconciler_common.go:292] "operationExecutor.UnmountDevice started for volume \"pvc-1ca296f6-9b88-40ea-a235-f8a0a093b8c1\" (UniqueName: \"kubernetes.io/csi/ebs.csi.aws.com^vol-03058d13f643f58a2\") on node \"ip-172-26-126-253.ap-southeast-2.compute.internal\" "
`

and this hung around for many hours like this, with no `UnmountDevice succeeded for volume` / `Volume detached for volume`. This was resolved by terminating the first node, so the volume would be released. 

Kube-controller-manager cannot detach the volume because it appears to still be in use by node1:
`
E0403 15:49:16.063873       2 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/ebs.csi.aws.com^vol-03058d13f643f58a2 podName: nodeName:}" failed. No retries permitted until 2025-04-03 15:51:18.063856131 +0000 UTC m=+687036.925234400 (durationBeforeRetry 2m2s). Error: DetachVolume.Detach failed for volume "pvc-1ca296f6-9b88-40ea-a235-f8a0a093b8c1" (UniqueName: "kubernetes.io/csi/ebs.csi.aws.com^vol-03058d13f643f58a2") on node "ip-172-26-126-253.ap-southeast-2.compute.internal" : DetachVolume failed for volume "pvc-1ca296f6-9b88-40ea-a235-f8a0a093b8c1" (UniqueName: "kubernetes.io/csi/ebs.csi.aws.com^vol-03058d13f643f58a2") on node "ip-172-26-126-253.ap-southeast-2.compute.internal" : volume is still in use by node, according to Node status
`

Checking the aws console, the volume was actually attached to node2. I believe this bug was only uncovered now because we upgraded karpenter versions to one that will not terminate a node if there are volumes attached to it (checking aws), so previously it would terminate node2 upon consolidation and that would release the volume, but now the pod is stuck for however long node2 sticks around.


#### What did you expect to happen?

I would expect that a pod should not finish shutdown if `UnmountDevice` did not complete and its volume is still in VolumesInUse. So I would expect the pod to stay running on both node1 and node2 until the volume was definitely detached and the node status consistent. 

#### How can we reproduce it (as minimally and precisely as possible)?

I don't actually have a reproducer, it seems to happen on clusters that have a lot of pod/node movement.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.7
Kustomize Version: v5.4.2
Server Version: v1.31.7
```

</details>


#### Cloud provider

<details>
aws
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="AlmaLinux"
VERSION="9.3 (Shamrock Pampas Cat)"
ID="almalinux"
ID_LIKE="rhel centos fedora"
VERSION_ID="9.3"
PLATFORM_ID="platform:el9"
PRETTY_NAME="AlmaLinux 9.3 (Shamrock Pampas Cat)"
ANSI_COLOR="0;34"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:almalinux:almalinux:9::baseos"
HOME_URL="https://almalinux.org/"
DOCUMENTATION_URL="https://wiki.almalinux.org/"
BUG_REPORT_URL="https://bugs.almalinux.org/"

ALMALINUX_MANTISBT_PROJECT="AlmaLinux-9"
ALMALINUX_MANTISBT_PROJECT_VERSION="9.3"
REDHAT_SUPPORT_PRODUCT="AlmaLinux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.3"
$ uname -a
Linux ip-172-26-17-214 6.1.127-hs95.el9.aarch64 #1 SMP Wed Feb 19 00:18:56 UTC 2025 aarch64 aarch64 aarch64 GNU/Linux

# On Windows:

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
aws-ebs-csi v1.28.0
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»è¯¥ Issue å†…å®¹çœ‹ï¼Œæè¿°çš„é—®é¢˜æ˜¯ï¼šåœ¨ Kubernetes é›†ç¾¤ä¸­ï¼Œå½“ Pod ä»æŸä¸ªèŠ‚ç‚¹è¿ç§»åï¼Œæ—§èŠ‚ç‚¹ kubelet æœªèƒ½å®Œæˆå¯¹ç›¸å…³å·çš„å¸è½½ï¼ˆUnmountDeviceï¼‰ï¼Œå¯¼è‡´è¯¥å·åœ¨æ—§èŠ‚ç‚¹ä»è¢«æ ‡è®°ä¸ºå ç”¨ï¼Œä»è€Œæ— æ³•è¢«æ–°èŠ‚ç‚¹å†æ¬¡æˆåŠŸæŒ‚è½½ã€‚æ­¤é—®é¢˜é€ æˆ Pod é•¿æ—¶é—´å¤„äºæ— æ³•å¯åŠ¨çš„çŠ¶æ€ï¼Œå½±å“å¯ç”¨æ€§ã€‚ä½†ä»å®‰å…¨è§†è§’åˆ†æï¼Œè¯¥é—®é¢˜å¹¶æœªé€éœ²å‡ºå¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨ç¼ºé™·ï¼Œæ¯”å¦‚æœªç»æˆæƒçš„è®¿é—®ã€è¶Šæƒæ“ä½œã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€å®¹å™¨é€ƒé€¸ã€å‘½ä»¤æ‰§è¡Œç­‰ã€‚å®ƒæ›´åƒæ˜¯ Kubernetes attach/detach æ§åˆ¶æµç¨‹ä¸­çš„ä¸€ä¸ªç¨³å®šæ€§æˆ–ä¸€è‡´æ€§ bugï¼Œè€Œéå¯è¢«åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚å› æ­¤ï¼Œæ ¹æ®ç»™å‡ºçš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥é—®é¢˜ä¸å±äºå®‰å…¨é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# ç”±äºè¯¥ Issue å¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ— éœ€æä¾›ä¸“é—¨çš„å®‰å…¨æ¼æ´å¤ç°POCã€‚
# ä¸‹æ–¹ç¤ºä¾‹è„šæœ¬ä»…ä½œä¸ºå ä½è¯´æ˜ï¼Œä¸ä¼šå®é™…æ‰§è¡Œä»»ä½•å¯åˆ©ç”¨æ”»å‡»çš„æ“ä½œã€‚

def main():
    print("è¯¥Issueä¸ºkubeletåœ¨å·å¸è½½è¿‡ç¨‹ä¸­çš„ä¸€è‡´æ€§é—®é¢˜ï¼Œä¸æ¶‰åŠå®‰å…¨æ¼æ´ï¼Œæ— éœ€POCå¤ç°ã€‚")

main()
```


---


## Issue #131402 NodeResizePending status not removed after successful NodeExpandVolume

- Issue é“¾æ¥ï¼š[#131402](https://github.com/kubernetes/kubernetes/issues/131402)

### Issue å†…å®¹

#### What happened?

When resizing a Persistent Disk CSI driver-backed volume, although the node expansion succeeds and the CSI driver completes NodeExpandVolume, the PVC's AllocatedResourceStatus NodeResizePending condition persists.

```
 spec:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 15Gi
    storageClassName: standard-rwo
    volumeMode: Filesystem
    volumeName: pvc-22d448b9-d98a-46d5-9910-fd2cb0029a7d
  status:
    accessModes:
    - ReadWriteOnce
    allocatedResourceStatuses:
      storage: NodeResizePending
    allocatedResources:
      storage: 15Gi
    capacity:
      storage: 15Gi
    phase: Bound
```

Upon pod restart, the NodeExpandVolume operation successfully resizes the file system.
```
/csi.v1.Node/NodeExpandVolume returned with response: capacity_bytes:16106127360
"NodeExpandVolume succeeded on volume Key{"pvc-8fb88ce1-0feb-47c7-9993-e6bf9d0d64e7", zone: "x"} to size 16106127360"
```
Despite this, node logs show that NodeExpandVolume is failing due to an unidentified error.
```
nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/pd.csi.storage.gke.io^projects/x/zones/x/disks/pvc-8fb88ce1-0feb-47c7-9993-e6bf9d0d64e7 podName: nodeName:}" failed. No retries permitted until 2025-04-18 20:17:09.901667408 +0000 UTC m=+10542.012736234 (durationBeforeRetry 16s). Error: NodeExpandVolume.NodeExpandVolume failed to resize volume for volume "pvc-8fb88ce1-0feb-47c7-9993-e6bf9d0d64e7" (UniqueName: "kubernetes.io/csi/pd.csi.storage.gke.io^projects/x/zones/x/disks/pvc-8fb88ce1-0feb-47c7-9993-e6bf9d0d64e7") pod "pod" (UID: "18e508ae-a670-4486-b993-e6355d0abcff") : volume resizing failed for unknown reason
```
The Kubelet appears to unexpectedly enter a placeholder code path. This issue can be traced to the following lines in the Kubernetes source code: https://github.com/kubernetes/kubernetes/blob/680ea07dbb2c6050d13b93660fa4d27d2d28d6eb/pkg/volume/util/operationexecutor/operation_generator.go#L1921-L1924. 

#### What did you expect to happen?

After completion of NodeExpandVolume, we expect that NodeResizePending status is removed.
```
  spec:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 15Gi
    storageClassName: standard-rwo
    volumeMode: Filesystem
    volumeName: pvc-8fb88ce1-0feb-47c7-9993-e6bf9d0d64e7
  status:
    accessModes:
    - ReadWriteOnce
    allocatedResources:
      storage: 15Gi
    capacity:
      storage: 15Gi
    phase: Bound
```


#### How can we reproduce it (as minimally and precisely as possible)?

1. Deploy a GKE cluster with master version `1.32.2-gke.1182003` and one node running version `1.31.6-gke.1020000`.
2. Create a pod that utilizes a `standard-rwo` PersistentVolumeClaim (PVC) with an initial size of 5Gi.
3. After the pod is operational, expand the PVC's storage capacity to 15Gi.
4. Restart the pod.
5. Examine the status of the PVC.

#### Anything else we need to know?

The "NodeResizePending" status resolves successfully when the node and cluster versions match at v1.31.6-gke.1020000. This issue seems to arise only when the cluster and node versions are different.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.2
Kustomize Version: v5.5.0
Server Version: v1.32.2-gke.1182003
```
Node version: v1.31.6-gke.1020000
Resizer version: v1.13.1-gke.0
</details>


#### Cloud provider

<details>
Google Cloud Platform
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux rodete"
NAME="Debian GNU/Linux rodete"
VERSION_CODENAME=rodete
ID=debian
HOME_URL="https://go/glinux"
SUPPORT_URL="https://go/techstop"
BUG_REPORT_URL="https://go/techstop"
$ uname -a
Linux elijahrb-mt.c.googlers.com 6.12.17-1rodete2-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.12.17-1rodete2 (2025-03-12) x86_64 GNU/Linux


#### Install tools

<details>
Google Kubernetes Engine

```
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
External resizer: v1.13.1-gke.0
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»Issueçš„æè¿°æ¥çœ‹ï¼Œè¿™æ˜¯ä¸€ä¸ªå…³äºPersistent Volumeå·æ‰©å®¹åçŠ¶æ€æœªåŠæ—¶æ›´æ–°çš„åŠŸèƒ½æˆ–å…¼å®¹æ€§é—®é¢˜ï¼Œå¹¶æœªä½“ç°å‡ºä»»ä½•æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€è®¤è¯ç»•è¿‡ã€è¶Šæƒæ“ä½œã€å‘½ä»¤æ‰§è¡Œæˆ–å®¹å™¨é€ƒé€¸ç­‰å®‰å…¨æ¼æ´çš„å¯èƒ½æ€§ã€‚è¯¥é—®é¢˜ä¸»è¦æ˜¯Kubernetesä¸åŒç‰ˆæœ¬ä¹‹é—´åœ¨NodeExpandVolumeå¤„ç†é€»è¾‘ä¸Šå­˜åœ¨å·®å¼‚ï¼Œå¯¼è‡´çŠ¶æ€æœªè¢«åŠæ—¶æ¸…é™¤ï¼Œå±äºåŠŸèƒ½æ€§æˆ–å…¼å®¹æ€§Bugï¼Œæ²¡æœ‰ç›´æ¥é€ æˆå¯è¢«æ¶æ„åˆ©ç”¨çš„å®‰å…¨é£é™©ã€‚å› æ­¤ï¼Œæ ¹æ®Issueé£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤é—®é¢˜ä¸å®‰å…¨é£é™©æ— å…³ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Issueæœ¬è´¨ä¸Šæ˜¯éƒ¨ç½²åœ¨ä¸åŒè¡Œä¸ºç‰ˆæœ¬ä¹‹é—´çš„Kubernetesé›†ç¾¤ä¸­å‡ºç°çš„åŠŸèƒ½æ€§å¼‚å¸¸æˆ–å…¼å®¹æ€§é—®é¢˜ï¼Œå¹¶ä¸ä¼šå¯¼è‡´ç³»ç»Ÿé¢å¤–æš´éœ²æ”»å‡»é¢ï¼Œä¹Ÿä¸æ¶‰åŠæ•æ„Ÿä¿¡æ¯æ³„éœ²ã€ææƒæˆ–å‘½ä»¤æ‰§è¡Œç­‰å¯å¯¼è‡´å®‰å…¨é£é™©çš„åœºæ™¯ã€‚æ‰€ä»¥æœ¬Issueè¢«åˆ¤å®šä¸ºâ€œä¸æ¶‰åŠâ€å®‰å…¨é£é™©ï¼Œæ— éœ€æä¾›å®‰å…¨å¤ç°è„šæœ¬ã€‚è¯¥é—®é¢˜çš„è§£å†³åº”é‡ç‚¹å…³æ³¨ç‰ˆæœ¬å…¼å®¹æ€§å’ŒçŠ¶æ€åŒæ­¥æœºåˆ¶ã€‚è‹¥éœ€è¦å¤ç°ï¼Œå¯æŒ‰ç…§Issueæè¿°çš„æ“ä½œæ­¥éª¤åœ¨ä¸åŒç‰ˆæœ¬æ··ç”¨çš„Kubernetesé›†ç¾¤ä¸­å¯¹PVCæ‰§è¡Œæ‰©å®¹æ“ä½œï¼Œå³å¯è§‚å¯Ÿåˆ°NodeResizePendingçŠ¶æ€æœªè¢«æ¸…é™¤çš„Bugï¼Œä½†è¿™å¹¶ä¸æ„æˆå®‰å…¨æ¼æ´ã€‚

---


## Issue #131397 NodeExpandVolume is getting called unexpectedly after pvc resize

- Issue é“¾æ¥ï¼š[#131397](https://github.com/kubernetes/kubernetes/issues/131397)

### Issue å†…å®¹

#### What happened?

We did a first resize of pv and pvc it was successful,
Then we did a another resize, we get a warning on pvc describe

```
failed to expand pvc with rpc error: code = Internal desc = Could not resize volume "vol-0157496f5c5f8ee9a": rpc error: code = Internal desc = Could not modify volume "vol-0157496f5c5f8ee9a": operation error EC2: ModifyVolume, https response error StatusCode: 400, RequestID: f4a49e65-d0e7-46de-b308-36ca67a9d796, api error VolumeModificationRateExceeded: You''ve reached the maximum modification rate per volume limit. Wait at least 6 hours between modifications per EBS volume.
``` 



And after 6 hours a successful resize of pv and pvc happened on EBS volume and the pod(osd) consuming it get resized, but we are getting this warning in pod(osd) describe that is using pvc

```
Warning VolumeResizeFailed 4m55s (x45 over 80m) kubelet NodeExpandVolume.NodeExpandVolume failed to resize volume for volume "pvc-6f50163a-862b-4010-a338-3b587adf8cc1" : volume resizing failed for unknown reason
```

Driver
```
  csi:
    driver: ebs.csi.aws.com
```

StorageClass
```
  storageClassName: gp2-csi

```

#### What did you expect to happen?

There should be no such warning as the pv and pvc are resized,

#### How can we reproduce it (as minimally and precisely as possible)?

Initiate a PVC resize operation with CSI Driver ebs.csi.aws.com

#### Anything else we need to know?

refer https://github.com/kubernetes/kubernetes/issues/117871, https://github.com/kubernetes/kubernetes/pull/123055#issuecomment-2816240906

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

k8s version := 1.32

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»Issueæè¿°æ¥çœ‹ï¼Œè¯¥é—®é¢˜æ˜¯å…³äºåœ¨AWS EBSä¸Šæ‰§è¡Œå·æ‰©å®¹ï¼ˆPVC/PV resizeï¼‰æ—¶ï¼Œç”±äºVolumeModificationRateExceededå¯¼è‡´çš„æš‚æ—¶æ€§å¤±è´¥ï¼Œä»¥åŠéšåkubeletå±‚åœ¨æ‰©å®¹æ“ä½œæˆåŠŸåä¾æ—§æç¤ºNodeExpandVolumeç›¸å…³çš„è­¦å‘Šã€‚  
æ•´ä¸ªè¿‡ç¨‹ä¸»è¦æ¶‰åŠå­˜å‚¨èµ„æºçš„ä½¿ç”¨å’Œæ“ä½œé¢‘ç‡é™åˆ¶ï¼Œå¹¶æœªå‡ºç°æ•æ„Ÿä¿¡æ¯æš´éœ²ã€æƒé™æå‡ã€æ•°æ®æ³„éœ²æˆ–å…¶ä»–å®‰å…¨éšæ‚£ï¼Œä¹Ÿæœªè§æåŠå¯å¯¼è‡´æ‹’ç»æœåŠ¡æ”»å‡»ã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰å®‰å…¨é—®é¢˜ã€‚  
å› æ­¤ï¼Œè¯¥Issueå¹¶éä¸€ä¸ªå®‰å…¨é—®é¢˜ï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# ç”±äºè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œå› æ­¤æ— éœ€æä¾›å®‰å…¨æ¼æ´å¤ç°è„šæœ¬ã€‚
# å¦‚æœæƒ³åœ¨æœ¬åœ°æ¨¡æ‹ŸEBSå·å¤šæ¬¡æ‰©å®¹çš„æƒ…å½¢ï¼Œå¯åœ¨ä½¿ç”¨kubernetesåº“è¿æ¥åˆ°é›†ç¾¤åï¼Œ
# é‡å¤å‘èµ·å¯¹åŒä¸€PVCçš„å¤šæ¬¡resizeæ“ä½œå¹¶è§‚å¯Ÿäº‹ä»¶å’Œæ—¥å¿—å³å¯ã€‚
# ä»¥ä¸‹ç¤ºä¾‹ä»…æä¾›ä¸€ä¸ªæ€è·¯æ¼”ç¤ºï¼Œä¸å®‰å…¨æ¼æ´æ— å…³ã€‚

import time
from kubernetes import client, config

def main():
    # å‡è®¾é»˜è®¤kubeconfigå·²ç»é…ç½®å¥½
    config.load_kube_config()

    v1 = client.CoreV1Api()
    storage_v1 = client.StorageV1Api()
    custom_api = client.CustomObjectsApi()

    namespace = "default"  # æ ¹æ®å®é™…å‘½åç©ºé—´ä¿®æ”¹
    pvc_name = "my-pvc"    # å‡è®¾å­˜åœ¨åä¸ºmy-pvcçš„PVCå¯ä¾›å®éªŒ

    try:
        # ç¬¬ä¸€æ¬¡æ‰©å®¹
        patch_pvc_size(v1, pvc_name, namespace, "10Gi")
        time.sleep(3)
        # ç¬¬äºŒæ¬¡æ‰©å®¹(çŸ­æ—¶é—´è¿ç»­å‘èµ·)
        patch_pvc_size(v1, pvc_name, namespace, "20Gi")
    except Exception as e:
        print(f"å‡ºç°å¼‚å¸¸ï¼š{e}")

def patch_pvc_size(k8s_client, pvc_name, namespace, new_size):
    pvc_patch = {
        "spec": {
            "resources": {
                "requests": {
                    "storage": new_size
                }
            }
        }
    }
    resp = k8s_client.patch_namespaced_persistent_volume_claim(
        name=pvc_name,
        namespace=namespace,
        body=pvc_patch
    )
    print(f"PATCH PVC {pvc_name} -> {new_size}, å“åº”ï¼š{resp}")

main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. è„šæœ¬ä¸­æ¼”ç¤ºäº†å¦‚ä½•åˆ©ç”¨Pythonçš„kubernetesåº“å¯¹åŒä¸€ä¸ªPVCè¿›è¡Œå¤šæ¬¡æ‰©å®¹æ“ä½œï¼Œä»è€Œåœ¨çŸ­æ—¶é—´å†…æ¨¡æ‹Ÿå¤šæ¬¡æ‰©å®¹çš„è¿‡ç¨‹ï¼›  
2. ç”±äºè¯¥Issueå¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ‰€ä»¥æ— éœ€æä¾›å®‰å…¨æ¼æ´çš„å¤ç°è„šæœ¬ï¼Œè¿™é‡Œçš„ç¤ºä¾‹ä»…ç”¨äºå±•ç¤ºPVCæ‰©å®¹æ“ä½œï¼Œå¸®åŠ©ç†è§£å’Œå®šä½Issueä¹Ÿå¯èƒ½äº§ç”Ÿçš„ç›¸å…³æŠ¥é”™æˆ–è­¦å‘Šï¼›  
3. è„šæœ¬ç¤ºä¾‹ä»…ä¾›æœ¬åœ°æµ‹è¯•ä¸ç ”ç©¶ä½¿ç”¨ï¼Œæ³¨æ„åˆç†è°ƒæ•´PVCåç§°ã€å‘½åç©ºé—´ä»¥åŠæ‰©å®¹ç­–ç•¥ï¼Œé¿å…å¯¹ç”Ÿäº§ç¯å¢ƒé€ æˆä¸å¿…è¦çš„å½±å“ã€‚

---


## Issue #131396 dns query timeout on 1.31

- Issue é“¾æ¥ï¼š[#131396](https://github.com/kubernetes/kubernetes/issues/131396)

### Issue å†…å®¹

#### What happened?

After upgrading from Kubernetes version 1.29 to 1.31, DNS resolving is timeouts. This issue is not limited to specific pods; it occurs in all pods running on version 1.31, and it did not happen in version 1.29.

In version 1.29, I performed DNS queries using the following command:

```shell
$> kubectl exec -it grafana-server-76cf4f8d7d-r4sgx  -n grafana -- nslookup google.com
Server:         10.233.0.3
Address:        10.233.0.3:53

Non-authoritative answer:
Name:   google.com
Address: 142.250.76.142

*** Can't find google.com: No answer
```

When running nslookup in version 1.31, the output is as follows:

```shell
$> kubectl exec -it -n grafana grafana-server-6f8d7bfdb6-jddwk -- nslookup google.com
;; Got recursion not available from 10.233.0.3
Server:         10.233.0.3
Address:        10.233.0.3#53

;; communications error to 10.233.0.3#53: timed out
```

#### What did you expect to happen?

 DNS queries normal response.

#### How can we reproduce it (as minimally and precisely as possible)?

```shell
$> kubectl exec -it -n grafana grafana-server-6f8d7bfdb6-jddwk -- nslookup google.com
;; Got recursion not available from 10.233.0.3
Server:         10.233.0.3
Address:        10.233.0.3#53

;; communications error to 10.233.0.3#53: timed out
```

#### Anything else we need to know?

1, Using Tool
k8s provisioning tool on kubespray 2.27
cni : calico
dns : coredns
1. coredns pod status 
```shell
 $> kubectl get pod -n kube-system | egrep -i dns
coredns-d665d669-c4hzr                     1/1     Running   0               2d23h
coredns-d665d669-chvr4                     1/1     Running   0               2d23h
dns-autoscaler-5cb4578f5f-gsg77            1/1     Running   0               2d23h

```
2. coredns configmap
```shell
$> kubectl  get cm coredns -n kube-system -o yaml
apiVersion: v1
data:
  Corefile: |
    .:53 {
        errors {
        }
        health {
            lameduck 5s
        }
        ready
        kubernetes k8s-dev in-addr.arpa ip6.arpa {
          pods insecure
          fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        forward . /etc/resolv.conf {
          prefer_udp
          max_concurrent 1000
        }
        cache 30

        loop
        reload
        loadbalance
    }
kind: ConfigMap

```
4. 
#### Kubernetes version

<details>

1. 1.31 k8s
```console
$ kubectl version
kubectl version
Client Version: v1.31.7
Kustomize Version: v5.4.2
Server Version: v1.31.7
```

1. 1.29 k8s
```shell
 kubectl version
Client Version: v1.29.10
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.10
```


</details>


#### Cloud provider

bare-metal

#### OS version

<details>

```console
# On Linux:
$> cat /etc/os-release
NAME="Rocky Linux"
VERSION="9.5 (Blue Onyx)"
ID="rocky"
ID_LIKE="rhel centos fedora"
VERSION_ID="9.5"
PLATFORM_ID="platform:el9"
PRETTY_NAME="Rocky Linux 9.5 (Blue Onyx)"
ANSI_COLOR="0;32"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:rocky:rocky:9::baseos"
HOME_URL="https://rockylinux.org/"
VENDOR_NAME="RESF"
VENDOR_URL="https://resf.org/"
BUG_REPORT_URL="https://bugs.rockylinux.org/"
SUPPORT_END="2032-05-31"
ROCKY_SUPPORT_PRODUCT="Rocky-Linux-9"
ROCKY_SUPPORT_PRODUCT_VERSION="9.5"
REDHAT_SUPPORT_PRODUCT="Rocky Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.5"

$> uname -a
Linux k8s-master1 5.14.0-503.21.1.el9_5.x86_64 #1 SMP PREEMPT_DYNAMIC Wed Jan 8 17:35:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>
kubespray 2.27
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
$> crio -version
crio version 1.29.1
Version:        1.29.1
GitCommit:      78e179ba8dd3ce462382a17049e8d1f770246af1
GitCommitDate:  2024-01-11T21:36:06Z
GitTreeState:   clean
BuildDate:      1970-01-01T00:00:00Z
GoVersion:      go1.21.1
Compiler:       gc
Platform:       linux/amd64
Linkmode:       static
BuildTags:
  static
  netgo
  osusergo
  exclude_graphdriver_btrfs
  exclude_graphdriver_devicemapper
  seccomp
  apparmor
  selinux
LDFlags:          unknown
SeccompEnabled:   true
AppArmorEnabled:  false


</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
CNI : calico
kubectl  describe ds calico-node  -n kube-system
Name:           calico-node
Selector:       k8s-app=calico-node
Node-Selector:  kubernetes.io/os=linux
Labels:         k8s-app=calico-node
Annotations:    deprecated.daemonset.template.generation: 1
Desired Number of Nodes Scheduled: 4
Current Number of Nodes Scheduled: 4
Number of Nodes Scheduled with Up-to-date Pods: 4
Number of Nodes Scheduled with Available Pods: 4
Number of Nodes Misscheduled: 0
Pods Status:  4 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=calico-node
  Service Account:  calico-node
  Init Containers:
   upgrade-ipam:
    Image:      quay.io/calico/cni:v3.27.4
    Port:       <none>
    Host Port:  <none>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»è¯¥ Issue çš„æè¿°æ¥çœ‹ï¼Œé—®é¢˜ä¸»è¦ä½“ç°åœ¨ä» Kubernetes 1.29 å‡çº§åˆ° 1.31 ä¹‹åï¼ŒDNS æŸ¥è¯¢å‡ºç°è¶…æ—¶ï¼Œå¯¼è‡´å®¹å™¨å†… DNS æ— æ³•æ­£å¸¸è§£æã€‚æ•´ç¯‡å†…å®¹æ›´å¤šæ˜¯å…³äºå‡çº§åå‡ºç°çš„é…ç½®æˆ–ç½‘ç»œæ•…éšœé—®é¢˜ï¼Œå¹¶æœªä½“ç°ä»»ä½•ç”±äºé”™è¯¯é…ç½®æˆ–å®ç°è€Œå¸¦æ¥çš„æ”»å‡»é¢ã€æ¼æ´åˆ©ç”¨æˆ–æƒé™æå‡ç­‰å®‰å…¨é£é™©ï¼Œä¹ŸæœªæåŠæ•æ„Ÿä¿¡æ¯æ³„éœ²ç­‰æƒ…å†µã€‚

æ ¹æ®ç»™å‡ºçš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¿™å±äºå¸¸è§çš„è¿ç»´æˆ–é…ç½®é—®é¢˜ï¼Œå¹¶ä¸æ¶‰åŠæ”»å‡»è€…å¯åˆ©ç”¨æˆ–å¯¼è‡´ä¿¡æ¯æ³„éœ²ç­‰å®‰å…¨é£é™©ï¼Œå› æ­¤ä¸æ„æˆå®‰å…¨æ¼æ´ã€‚

---


## Issue #131387 CoreDNS HNS Endpoint IP is not updated on pod restart

- Issue é“¾æ¥ï¼š[#131387](https://github.com/kubernetes/kubernetes/issues/131387)

### Issue å†…å®¹

#### What happened?

When CoreDNS pod on Linux node restarts and receives a new cluster IP, endpoints/endpointslices in Kubertenes are updated with new pod IP, but on Windows worker HNS Endpoint still holds the old IP address. Kubernetes restart on Windows doesn't help, need to delete all stuff and re-join node to cluster again.

#### What did you expect to happen?

 HNS Endpoint would be updated with new IP address after CoreDNS pod restart

#### How can we reproduce it (as minimally and precisely as possible)?


1.    Deploy a cluster with 3 Linux master nodes.
2.    Join 1 Windows worker node.
3.    Start sample workload on Windows node (used mcr.microsoft.com/dotnet/framework/samples:aspnetapp)
4.    Exec into Windows pod with cmd, type nslookup, observe working output
```
Default Server:  kube-dns.kube-system.svc.cluster.local
Address:  10.43.0.10
```
5.    Exit Windows pod, get pods list from kube-system namespace with -o wide, note CoreDNS pod ip.
6.    Delete CoreDNS pod, wait for new to start, check its IP address.
7.    Exec again into Windows pod, type nslookup, observe timeout errors
```
DNS request timed out.
    timeout was 2 seconds.
Default Server:  UnKnown
Address:  10.43.0.10
```
8.    Type server <new_coredns_pod_ip>, and after timeout try to resolve any dns name (for example c.cc), you would see that connectivity is working.
9.    RDP into Windows server, start Powershell, run
```
hnsdiag list all | findstr 10.43.0.10
```
you will see the output like this
```
ce87ca71-adfd-4334-a5ce-760eae297bef |  10.43.0.10      | 6f995c07-0b4a-4cfb-ba6e-c62dd48a554d
302522f6-f540-4120-881c-b7a2eeecf0f6 |  10.43.0.10      | 6f995c07-0b4a-4cfb-ba6e-c62dd48a554d
ddd666f5-c1e9-4fe1-af6b-da97be5cdb29 |  10.43.0.10      | 6f995c07-0b4a-4cfb-ba6e-c62dd48a554d
```
10.    Run
```
get-hnsendpoint 6f995c07-0b4a-4cfb-ba6e-c62dd48a554d
```
and in output IPAddress would be old CoreDNS pod IP address, not the new one.

#### Anything else we need to know?

The deployment is done with k3s all in one binary, which could make some influence here, but the issue looks like in Kubernetes itself.
The one thing I didn't test is what would happen if I would update HNS Endpoint with the new pod IP, but unfortunately I didn't find a way to perform such update. Any ideas? :)

#### Kubernetes version

1.28 - 1.32

#### Cloud provider

k3s baremetal deployment


#### OS version

<details>
```
PRETTY_NAME="Ubuntu 22.04.5 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
```
$ uname -a
Linux test01 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux


# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
BuildNumber  Caption                                 OSArchitecture  Version
17763        Microsoft Windows Server 2019 Standard  64-bit          10.0.17763
```

```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»è¯¥ Issue çš„æè¿°æ¥çœ‹ï¼Œä¸»è¦æ˜¯ Windows èŠ‚ç‚¹åœ¨ CoreDNS Pod é‡å¯åæ— æ³•åŠæ—¶æ›´æ–° HNS Endpoint IPï¼Œå¯¼è‡´ DNS è¯·æ±‚å¼‚å¸¸ã€‚è¯¥é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ç½‘ç»œé…ç½®ä¸åŒæ­¥æœºåˆ¶çš„é—®é¢˜ï¼Œä¸æ¶‰åŠä¿¡æ¯æ³„éœ²ã€æ•æ„Ÿæ“ä½œã€å‘½ä»¤æ‰§è¡Œæˆ–ææƒç­‰å®‰å…¨å±‚é¢çš„é£é™©ï¼Œä¹Ÿæ²¡æœ‰æ˜ç¡®æè¿°å¯è¢«æ”»å‡»è€…åˆ©ç”¨è¿›è¡Œæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰çš„æ“ä½œåœºæ™¯ã€‚ä¸å­˜åœ¨éœ€è¦é¢å¤–æƒé™æ‰èƒ½å‘èµ·çš„æ”»å‡»æˆ–å¯¼è‡´æ›´ä¸¥é‡åæœçš„å¯èƒ½æ€§ã€‚å› æ­¤ç»¼åˆåˆ¤æ–­è¯¥é—®é¢˜å¹¶ä¸æ˜¯å®‰å…¨é£é™©ï¼Œè€Œæ˜¯åŠŸèƒ½æ€§æˆ–å…¼å®¹æ€§ç¼ºé™·ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

æ­¤é—®é¢˜ä¸ºç½‘ç»œé…ç½® Bugï¼Œä¸æ„æˆå¯è¢«åˆ©ç”¨çš„å®‰å…¨æ¼æ´ï¼Œæ— éœ€ç¼–å†™å®‰å…¨æ¼æ´å¤ç°è„šæœ¬ã€‚è¯¥é—®é¢˜æ›´å¤šåœ°éœ€è¦åœ¨ Windows ä¸Šé’ˆå¯¹ HNS Endpoint çš„æ›´æ–°é€»è¾‘è¿›è¡Œä¿®å¤æˆ–æ’æŸ¥ï¼Œé€šè¿‡ Kubernetes æˆ– k3s çš„ç½‘ç»œæ’ä»¶æœºåˆ¶æ¥æ­£ç¡®åœ°æ›´æ–° CoreDNS å¯¹åº”çš„ IP åœ°å€ã€‚æ­¤å¤„ä¸éœ€å®‰å…¨é£é™©å¤ç°è„šæœ¬ã€‚

---


## Issue #131384 123

- Issue é“¾æ¥ï¼š[#131384](https://github.com/kubernetes/kubernetes/issues/131384)

### Issue å†…å®¹

#### What happened?

123

#### What did you expect to happen?

123

#### How can we reproduce it (as minimally and precisely as possible)?

123

#### Anything else we need to know?

123

#### Kubernetes version

<details>
123
```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»Issueçš„æ ‡é¢˜å’Œå†…å®¹æ¥çœ‹ï¼Œå‡ä¸ºâ€œ123â€å ä½ç¬¦ä¿¡æ¯ï¼Œæ²¡æœ‰ä»»ä½•å¯è¯†åˆ«çš„å®‰å…¨ç›¸å…³æè¿°æˆ–ç–‘ä¼¼å®‰å…¨é£é™©ç‚¹ï¼Œä¹Ÿæœªæä¾›å…³äºé…ç½®é”™è¯¯ã€æ¼æ´åˆ©ç”¨æˆ–æ•æ„Ÿä¿¡æ¯æ³„éœ²ç­‰å†…å®¹ï¼Œå› æ­¤æ— æ³•åˆ¤æ–­å‡ºä»»ä½•å®‰å…¨éšæ‚£ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥Issueä¸å­˜åœ¨å®‰å…¨é£é™©ï¼Œæ— éœ€å¤ç°è„šæœ¬
```


**è§£é‡Šè¯´æ˜ï¼š**

ç”±äºè¯¥Issueå¹¶æœªæè¿°ä»»ä½•å®é™…çš„å®‰å…¨é£é™©ç‚¹ï¼Œä¸æ¶‰åŠé«˜çº§åˆ«æˆ–ä½çº§åˆ«çš„å®‰å…¨é—®é¢˜ï¼Œå› æ­¤æ— éœ€æä¾›å¤ç°è„šæœ¬ã€‚ä¸Šè¿°ä»£ç å—ä»…ç”¨äºå ä½è¯´æ˜ã€‚è‹¥åç»­æä¾›äº†æ›´å…·ä½“çš„æŠ€æœ¯ç»†èŠ‚æˆ–é£é™©ä¿¡æ¯ï¼Œå†è¿›è¡Œè¿›ä¸€æ­¥åˆ†æå¹¶æä¾›ç›¸åº”çš„å¤ç°è„šæœ¬ã€‚

---


## Issue #131381 NodeResizeError persists after successful PVC resize with CSI drivers lacking NodeExpandVolume support

- Issue é“¾æ¥ï¼š[#131381](https://github.com/kubernetes/kubernetes/issues/131381)

### Issue å†…å®¹

#### What happened?

After resizing a PVC backed by the CephFS CSI driver (which does not implement NodeExpandVolume), Kubernetes successfully performs the controller-side resize. The PVC's capacity and allocatedResources fields are updated to the correct value, and the volume is usable in the pod.

However, the PVC status remains in a NodeResizeError condition indefinitely, with a message like:

```
conditions:
- type: NodeResizeError
  status: "True"
  message: failed to expand pvc with NodeExpand is not supported by the CSI driver rook-ceph.cephfs.csi.ceph.com

```
This happens even after pod restarts and with RecoverVolumeExpansionFailure=true enabled. The error condition is cosmetic â€” the filesystem is resized correctly â€” but it is never cleared, causing persistent alerts and misleading status.

#### What did you expect to happen?

Kubernetes (or the external resizer) should detect that:

*    The CSI driver does not support NodeExpandVolume

*    The resize succeeded on the controller side

*    The PVC is healthy and matches the requested size

It should then automatically clear the NodeResizeError condition.

#### How can we reproduce it (as minimally and precisely as possible)?

1.    Use the Rook CephFS CSI driver with a StorageClass that allows volume expansion.

2.    Ensure the CSI driver supports ControllerExpandVolume but not NodeExpandVolume.

3.    Enable RecoverVolumeExpansionFailure=true on the kube-controller-manager.

4.    Create and bind a PVC.

5.    Resize the PVC by increasing its spec.resources.requests.storage.

 6.   Wait for pod to restart or manually restart the pod.

7.    Observe that the PVC is resized (capacity and allocatedResources match), but NodeResizeError condition is never cleared.


#### Anything else we need to know?

This is not a functional error â€” the PVC works as intended. However, the error condition misleads monitoring tools and operators. It appears that Kubernetes never recovers from a permanent NodeExpandVolume failure, even when the resize is complete and usable.

Possibly related but distinct from earlier resize error tracking issues. As of Kubernetes 1.29+, this behavior uses new condition types like NodeResizeError and NodeResizeInfeasible, but the PVC controller does not reconcile them correctly.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.3
Server Version: v1.32.3
```

</details>


#### Cloud provider

<details>
Bare Metal / Talos Linux
</details>


#### OS version

<details>

```console
Talos Linux 1.9
```

</details>


#### Install tools

<details>
Talos + Flux + Helm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd 1.7.x (via Talos)
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

    CSI Driver: quay.io/cephcsi/cephcsi:v3.13.0

    External Resizer: registry.k8s.io/sig-storage/csi-resizer:v1.13.1

    Provisioner: csi-provisioner:v5.1.0

    Feature Gate: RecoverVolumeExpansionFailure=true

</details>

PVC status: ```
status:
  accessModes:
  - ReadWriteMany
  allocatedResources:
    storage: 9Gi
  capacity:
    storage: 9Gi
  conditions:
  - type: NodeResizeError
    status: "True"
    message: failed to expand pvc with NodeExpand is not supported by the CSI driver rook-ceph.cephfs.csi.ceph.com
```

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»Issueçš„æè¿°æ¥çœ‹ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸Kubernetesåœ¨PVCæ‰©å®¹åŠŸèƒ½ä¸Šè¡¨ç°å‡ºçš„æŒä¹…æ€§é”™è¯¯çŠ¶æ€ç›¸å…³çš„é—®é¢˜ã€‚å…·ä½“åœ°ï¼Œä½¿ç”¨ä¸æ”¯æŒNodeExpandVolumeçš„CSIé©±åŠ¨è¿›è¡ŒPVCæ‰©å®¹åï¼ŒPVCçš„çŠ¶æ€ä¸€ç›´ä¿æŒåœ¨NodeResizeErrorï¼Œè€Œå®é™…ä¸Šæ–‡ä»¶ç³»ç»Ÿå·²ç»æˆåŠŸæ‰©å®¹ã€‚è¯¥é—®é¢˜åªæ˜¯ä¸€ä¸ªåŠŸèƒ½æˆ–çŠ¶æ€åŒæ­¥æ–¹é¢çš„ç¼ºé™·ï¼Œä¸ä¼šå¯¼è‡´ä»»æ„ä»£ç æ‰§è¡Œã€æƒé™æå‡ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ç­‰å®‰å…¨é£é™©ã€‚  
å› æ­¤ï¼Œä¾æ®æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨æ¼æ´ï¼Œä¹Ÿä¸ä¼šå¯¹ç³»ç»Ÿæ„æˆå®‰å…¨å¨èƒï¼Œå±äºåŠŸèƒ½æ€§æˆ–å¯é æ€§é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# ç”±äºæœ¬Issueä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ— éœ€æä¾›å®‰å…¨æ¼æ´ç›¸å…³çš„å¤ç°è„šæœ¬
# æ­¤å¤„ä»…ç¤ºä¾‹å¦‚ä½•è¿æ¥Kubernetesé›†ç¾¤æ£€æŸ¥PVCçŠ¶æ€çš„é€»è¾‘ï¼Œéæ¼æ´POC

import os
import time
from kubernetes import client, config

def main():
    # å‡è®¾åœ¨é»˜è®¤ä½ç½®ï¼ˆå¦‚ '~/.kube/config'ï¼‰å·²ç»é…ç½®å¥½kubeconfig
    config.load_kube_config()

    core_v1 = client.CoreV1Api()
    
    # æ³¨æ„ç•™è¶³è¶…æ—¶ä½™é‡ï¼Œä¸è¦é€ æˆæ­»å¾ªç¯ï¼Œæ­¤å¤„ç¤ºä¾‹ç­‰å¾…æœ€å¤š100ç§’
    deadline = time.time() + 100
    namespace = "default"
    pvc_name = "test-pvc"

    while time.time() < deadline:
        pvc = core_v1.read_namespaced_persistent_volume_claim(name=pvc_name, namespace=namespace)
        
        # æ‰“å°PVCå½“å‰conditionsçŠ¶æ€
        if pvc.status.conditions:
            for condition in pvc.status.conditions:
                print(f"PVC Condition Type: {condition.type}, Status: {condition.status}, Message: {condition.message}")
        else:
            print("No conditions on PVC")
        
        # æ£€æŸ¥PVCçŠ¶æ€ä¸­æ˜¯å¦æœ‰NodeResizeError
        node_resize_error = any(cond.type == "NodeResizeError" and cond.status == "True" for cond in pvc.status.conditions or [])
        if not node_resize_error:
            print("PVCæ²¡æœ‰NodeResizeErroræˆ–å·²ç»æ¢å¤ã€‚")
            break
        
        print("PVCå°šæœªæ¢å¤NodeResizeErrorçŠ¶æ€ï¼Œç­‰å¾…ä¸€æ®µæ—¶é—´é‡æ–°æ£€æŸ¥...")
        time.sleep(5)

    print("è„šæœ¬æ‰§è¡Œå®Œæ¯•ã€‚")
    
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. åœ¨è¿™ä¸ªç¤ºä¾‹è„šæœ¬é‡Œï¼Œä»…æ¼”ç¤ºäº†å¦‚ä½•ä½¿ç”¨ python çš„ kubernetes å®¢æˆ·ç«¯æŸ¥è¯¢ PVC çš„çŠ¶æ€ï¼Œå¹¶æ£€æŸ¥æ˜¯å¦å­˜åœ¨ NodeResizeError æ¡ä»¶ã€‚  
2. ç”±äºè¯¥Issueå¹¶éå®‰å…¨é£é™©ï¼Œå› æ­¤ä¸éœ€è¦å®ç°ä»»ä½•åˆ©ç”¨æˆ–æ¼æ´å˜ç°çš„â€œé«˜é£é™©â€å¤ç°è„šæœ¬ï¼Œä¹Ÿä¸æ¶‰åŠä»»ä½•æ•æ„Ÿä¿¡æ¯æˆ–éœ€è¦ç‰¹æ®Šæƒé™æ‰èƒ½æ‰§è¡Œçš„æ“ä½œã€‚  
3. è¯¥è„šæœ¬ä»…åœ¨æœ¬åœ°å¯ç”¨çš„Kubernetesç¯å¢ƒä¸­æ¼”ç¤ºå¦‚ä½•è¯»å–PVCçŠ¶æ€ï¼Œä¸ä¼šå¯¹ç³»ç»Ÿé€ æˆå®‰å…¨é£é™©ã€‚  
4. è„šæœ¬å†…å«è¶…æ—¶é€»è¾‘ï¼Œç¡®ä¿åœ¨ä¸€å®šæ—¶é—´å†…é€€å‡ºï¼Œä¸ä¼šé™·å…¥æ­»å¾ªç¯ã€‚

---


## Issue #131379 After the sidecar feature is introduced, resources are not reused. As a result, after scheduling is successful, kubelet rejects pods

- Issue é“¾æ¥ï¼š[#131379](https://github.com/kubernetes/kubernetes/issues/131379)

### Issue å†…å®¹

#### What happened?

The sidecar feature is used. Eight NPUs are applied for for the init container and eight NPUs are applied for for the service container. The Volcano scheduling is successful. However, Kubelet cannot reuse the resources of the init container. As a result, an error is reported when resources are allocated to service containers.
So I think our default scheduler doesn't take into account the sidecar container as well.


#### What did you expect to happen?

1. pod has the init container and the service container, and init container's restart policy is Always
2. eight NPUs are applied for for the init container and eight NPUs are applied for for the service container
3. pod is running

#### How can we reproduce it (as minimally and precisely as possible)?

1. pod has the init container and the service container, and init container's restart policy is Always
2. eight NPUs are applied for for the init container and eight NPUs are applied for for the service container
3. pod is running

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä» Issue æè¿°æ¥çœ‹ï¼Œè¯¥é—®é¢˜æ¶‰åŠ Kubernetes ä¸­ sidecar åŠŸèƒ½ä¸è°ƒåº¦å™¨ä¹‹é—´çš„èµ„æºåˆ†é…ä¸å›æ”¶æœºåˆ¶ï¼Œå¯¼è‡´å› èµ„æºæœªè¢«å›æ”¶è€Œæ— æ³•æ­£å¸¸åˆ†é…ç»™åç»­å®¹å™¨ã€‚è¿™å±äºè°ƒåº¦å™¨æˆ– kubelet çš„åŠŸèƒ½æˆ–é…ç½®é—®é¢˜ï¼Œå¹¶æœªä½“ç°å‡ºä»»ä½•å¯è¢«æ¶æ„åˆ©ç”¨çš„å®‰å…¨æ¼æ´ï¼Œä¹Ÿæœªæ¶‰åŠå‡­æ®æ³„éœ²ã€è¶Šæƒæ“ä½œæˆ–ä»£ç æ³¨å…¥ç­‰å®‰å…¨é£é™©ã€‚å› æ­¤ï¼Œæœ¬é—®é¢˜ä»…æ˜¯åŠŸèƒ½æ€§æˆ–é…ç½®æ€§çš„ç¼ºé™·ï¼Œä¸å±äºå®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥Issueä¸ºåŠŸèƒ½æ€§é—®é¢˜ï¼Œä¸æ¶‰åŠå®‰å…¨æ¼æ´ï¼Œå› æ­¤æ— éœ€æä¾›POCè„šæœ¬
```


---


## Issue #131352 kubectl: check rule in exec command is insecurity

- Issue é“¾æ¥ï¼š[#131352](https://github.com/kubernetes/kubernetes/issues/131352)

### Issue å†…å®¹

#### What happened?

https://github.com/kubernetes/kubernetes/blob/b53b9fb5573323484af9a19cf3f5bfe80760abba/staging/src/k8s.io/kubectl/pkg/cmd/exec/exec.go#L208-L215

`kubectl exec -it resource asd asd asd -- bash` will be like `kubectl exec -it resource -- bash` but no error

This can lead to unexpected behavior of command with `--`

#### What did you expect to happen?

raise error `error: exec [POD] [COMMAND] is not supported anymore.`

#### How can we reproduce it (as minimally and precisely as possible)?

`kubectl exec -it resource asd asd asd -- bash`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
v1.32.3
```

</details>


#### Cloud provider

<details>
-
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»Issueæè¿°æ¥çœ‹ï¼Œè¯¥é—®é¢˜é›†ä¸­åœ¨â€œkubectl execâ€å‘½ä»¤å¯¹å‚æ•°è§£æçš„è¡Œä¸ºä¸ç”¨æˆ·é¢„æœŸä¸ä¸€è‡´ã€‚Issueä¸­æ‰€è¿°çš„â€œkubectl exec -it resource asd asd asd -- bashâ€å®é™…è¢«è§£æä¸ºâ€œkubectl exec -it resource -- bashâ€ä¸”æœªæŠ¥é”™ï¼Œå¯¼è‡´ç”¨æˆ·è®¤ä¸ºäº§ç”Ÿäº†â€œæ„å¤–çš„è¡Œä¸ºâ€ã€‚  
ç„¶è€Œï¼ŒIssueä¸­å¹¶æœªæå‡ºåˆ©ç”¨è¯¥è¡Œä¸ºèƒ½å¤Ÿå®ç°è¶Šæƒã€å‘½ä»¤æ³¨å…¥ã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰å®‰å…¨é£é™©åœºæ™¯ï¼Œä»…è¡¨æ˜è¿™æ˜¯ä¸€ä¸ªä¸ç¬¦åˆé¢„æœŸçš„è§£ææ–¹å¼æˆ–BUGã€‚é€šå¸¸ä½¿ç”¨â€œkubectl execâ€éœ€è¦ç”¨æˆ·å…·å¤‡ç›¸åº”æƒé™ï¼ˆè·å–/æ‰§è¡Œpods/execï¼‰ï¼Œæ­¤ç±»æƒé™æœ¬èº«å°±å¯ç›´æ¥æ‰§è¡Œå®¹å™¨å†…å‘½ä»¤ã€‚å› æ­¤ï¼Œè¯¥Issueæœªæå‡æ”»å‡»è€…æ‰€éœ€çš„æƒé™ï¼Œä¹Ÿæœªæ–°å¢åŠ ä»»ä½•å®é™…å¯åˆ©ç”¨çš„å®‰å…¨æ¼æ´ï¼Œæ›´å¤šåœ°ä½“ç°ä¸ºCLIå‚æ•°è§£æä¸é¢„æœŸä¸ç¬¦çš„é—®é¢˜ã€‚  

ç»¼ä¸Šï¼Œæœ¬Issueæœªè¡¨ç°å‡ºå®é™…çš„å®‰å…¨æ¼æ´æˆ–é£é™©ï¼Œä¸æ¶‰åŠä»»ä½•è¶Šæƒæ“ä½œæˆ–æ”»å‡»è€…å¯åˆ©ç”¨çš„åœºæ™¯ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

æ­¤å¤„ä¸ºæ­£å¸¸åŠŸèƒ½/BUGçš„è®¨è®ºï¼Œä¸å­˜åœ¨å¯åˆ©ç”¨çš„å®‰å…¨æ¼æ´ï¼Œå› æ­¤æ— éœ€ç¼–å†™å¤ç°è„šæœ¬ã€‚è¯¥é—®é¢˜å¹¶ä¸èƒ½å¸¦æ¥æ–°çš„æ”»å‡»é€”å¾„æˆ–æƒé™æå‡ï¼Œæ•…é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

---


## Issue #131347 kubeadm init does not work

- Issue é“¾æ¥ï¼š[#131347](https://github.com/kubernetes/kubernetes/issues/131347)

### Issue å†…å®¹

#### What happened?

I used the following command to init a cluster on Ubuntu 22.04
`sudo kubeadm init --pod-network-cidr 10.200.0.0/16 --apiserver-advertise-address=172.30.1.2`

and get the following results:
`Apr 16 22:03:00 t-X570-AORUS-ELITE kubelet[61942]: I0416 22:03:00.945598   61942 kubelet_node_status.go:70] "Attempting to register node" node="t-x570-aorus-elite"
Apr 16 22:03:01 t-X570-AORUS-ELITE kubelet[61942]: E0416 22:03:01.539137   61942 eviction_manager.go:258] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"t-x570-aorus-elite\" not found"
Apr 16 22:03:04 t-X570-AORUS-ELITE kubelet[61942]: I0416 22:03:04.252855   61942 scope.go:117] "RemoveContainer" containerID="82a6daa3fe106defc2611b80f05a10b6721c18db3fb60e0318d6c5b7c3f1df0d"
Apr 16 22:03:04 t-X570-AORUS-ELITE kubelet[61942]: E0416 22:03:04.253280   61942 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-apiserver\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kube-apiserver pod=kube-apiserver-t-x570-aorus-elite_kube-system(30e3274380e991ebd296f6b8c8d423ba)\"" pod="kube-system/kube-apiserver-t-x570-aorus-elite" podUID="30e3274380e991ebd296f6b8c8d423ba"
Apr 16 22:03:05 t-X570-AORUS-ELITE kubelet[61942]: E0416 22:03:05.507589   61942 event.go:289] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"t-x570-aorus-elite.1836e1c2883adc51", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"t-x570-aorus-elite", UID:"t-x570-aorus-elite", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientPID", Message:"Node t-x570-aorus-elite status is now: NodeHasSufficientPID", Source:v1.EventSource{Component:"kubelet", Host:"t-x570-aorus-elite"}, FirstTimestamp:time.Date(2025, time.April, 16, 21, 2, 11, 278109777, time.Local), LastTimestamp:time.Date(2025, time.April, 16, 21, 2, 11, 340599626, time.Local), Count:2, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"kubelet", ReportingInstance:"t-x570-aorus-elite"}': 'Patch "https://172.30.1.2:6443/api/v1/namespaces/default/events/t-x570-aorus-elite.1836e1c2883adc51": dial tcp 172.30.1.2:6443: i/o timeout'(may retry after sleeping)
Apr 16 22:03:05 t-X570-AORUS-ELITE kubelet[61942]: E0416 22:03:05.514753   61942 controller.go:146] "Failed to ensure lease exists, will retry" err="Get \"https://172.30.1.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/t-x570-aorus-elite?timeout=10s\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" interval="7s"
Apr 16 22:03:06 t-X570-AORUS-ELITE kubelet[61942]: I0416 22:03:06.253218   61942 scope.go:117] "RemoveContainer" containerID="cf77057a7000506e1bb0b6751e7281624b04dbb8d906164f7e79777181c1f086"
Apr 16 22:03:06 t-X570-AORUS-ELITE kubelet[61942]: E0416 22:03:06.253524   61942 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"etcd\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=etcd pod=etcd-t-x570-aorus-elite_kube-system(964f70a04ec02731612f76d8fc652c6a)\"" pod="kube-system/etcd-t-x570-aorus-elite" podUID="964f70a04ec02731612f76d8fc652c6a"
Apr 16 22:03:11 t-X570-AORUS-ELITE kubelet[61942]: E0416 22:03:11.539342   61942 eviction_manager.go:258] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"t-x570-aorus-elite\" not found"
Apr 16 22:03:13 t-X570-AORUS-ELITE kubelet[61942]: W0416 22:03:13.893522   61942 reflector.go:535] k8s.io/client-go@v0.0.0/tools/cache/reflector.go:229: failed to list *v1.Node: Get "https://172.30.1.2:6443/api/v1/nodes?fieldSelector=metadata.name%3Dt-x570-aorus-elite&limit=500&resourceVersion=0": dial tcp 172.30.1.2:6443: i/o timeout
Apr 16 22:03:13 t-X570-AORUS-ELITE kubelet[61942]: I0416 22:03:13.893587   61942 trace.go:236] Trace[1855426437]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.0.0/tools/cache/reflector.go:229 (16-Apr-2025 22:02:43.892) (total time: 30000ms):
Apr 16 22:03:13 t-X570-AORUS-ELITE kubelet[61942]: Trace[1855426437]: ---"Objects listed" error:Get "https://172.30.1.2:6443/api/v1/nodes?fieldSelector=metadata.name%3Dt-x570-aorus-elite&limit=500&resourceVersion=0": dial tcp 172.30.1.2:6443: i/o timeout 30000ms (22:03:13.893)
Apr 16 22:03:13 t-X570-AORUS-ELITE kubelet[61942]: Trace[1855426437]: [30.000903632s] [30.000903632s] END
Apr 16 22:03:13 t-X570-AORUS-ELITE kubelet[61942]: E0416 22:03:13.893600   61942 reflector.go:147] k8s.io/client-go@v0.0.0/tools/cache/reflector.go:229: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://172.30.1.2:6443/api/v1/nodes?fieldSelector=metadata.name%3Dt-x570-aorus-elite&limit=500&resourceVersion=0": dial tcp 172.30.1.2:6443: i/o timeout
Apr 16 22:03:15 t-X570-AORUS-ELITE kubelet[61942]: I0416 22:03:15.252588   61942 scope.go:117] "RemoveContainer" containerID="82a6daa3fe106defc2611b80f05a10b6721c18db3fb60e0318d6c5b7c3f1df0d"
Apr 16 22:03:15 t-X570-AORUS-ELITE kubelet[61942]: E0416 22:03:15.253021   61942 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-apiserver\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kube-apiserver pod=kube-apiserver-t-x570-aorus-elite_kube-system(30e3274380e991ebd296f6b8c8d423ba)\"" pod="kube-system/kube-apiserver-t-x570-aorus-elite" podUID="30e3274380e991ebd296f6b8c8d423ba"
Apr 16 22:03:18 t-X570-AORUS-ELITE kubelet[61942]: E0416 22:03:18.352083   61942 certificate_manager.go:562] kubernetes.io/kube-apiserver-client-kubelet: Failed while requesting a signed certificate from the control plane: cannot create certificate signing request: Post "https://172.30.1.2:6443/apis/certificates.k8s.io/v1/certificatesigningrequests": dial tcp 172.30.1.2:6443: i/o timeout
Apr 16 22:03:19 t-X570-AORUS-ELITE kubelet[61942]: I0416 22:03:19.252719   61942 scope.go:117] "RemoveContainer" containerID="cf77057a7000506e1bb0b6751e7281624b04dbb8d906164f7e79777181c1f086"
Apr 16 22:03:19 t-X570-AORUS-ELITE kubelet[61942]: E0416 22:03:19.252975   61942`

I use containerd and my config looks like
`disabled_plugins = []
imports = []
oom_score = 0
plugin_dir = ""
required_plugins = []
root = "/var/lib/containerd"
state = "/run/containerd"
temp = ""
version = 2

[cgroup]
  path = ""

[debug]
  address = ""
  format = ""
  gid = 0
  level = ""
  uid = 0

[grpc]
  address = "/run/containerd/containerd.sock"
  gid = 0
  max_recv_message_size = 16777216
  max_send_message_size = 16777216
  tcp_address = ""
  tcp_tls_ca = ""
  tcp_tls_cert = ""
  tcp_tls_key = ""
  uid = 0

[metrics]
  address = ""
  grpc_histogram = false

[plugins]

  [plugins."io.containerd.gc.v1.scheduler"]
    deletion_threshold = 0
    mutation_threshold = 100
    pause_threshold = 0.02
    schedule_delay = "0s"
    startup_delay = "100ms"

  [plugins."io.containerd.grpc.v1.cri"]
    cdi_spec_dirs = ["/etc/cdi", "/var/run/cdi"]
    device_ownership_from_security_context = false
    disable_apparmor = false
    disable_cgroup = false
    disable_hugetlb_controller = true
    disable_proc_mount = false
    disable_tcp_service = true
    drain_exec_sync_io_timeout = "0s"
    enable_cdi = false
    enable_selinux = false
    enable_tls_streaming = false
    enable_unprivileged_icmp = false
    enable_unprivileged_ports = false
    ignore_deprecation_warnings = []
    ignore_image_defined_volumes = false
    image_pull_progress_timeout = "5m0s"
    image_pull_with_sync_fs = false
    max_concurrent_downloads = 3
    max_container_log_line_size = 16384
    netns_mounts_under_state_dir = false
    restrict_oom_score_adj = false
    sandbox_image = "registry.k8s.io/pause:3.9"
    selinux_category_range = 1024
    stats_collect_period = 10
    stream_idle_timeout = "4h0m0s"
    stream_server_address = "127.0.0.1"
    stream_server_port = "0"
    systemd_cgroup = false
    tolerate_missing_hugetlb_controller = true
    unset_seccomp_profile = ""

    [plugins."io.containerd.grpc.v1.cri".cni]
      bin_dir = "/opt/cni/bin"
      conf_dir = "/etc/cni/net.d"
      conf_template = ""
      ip_pref = ""
      max_conf_num = 1
      setup_serially = false

    [plugins."io.containerd.grpc.v1.cri".containerd]
      default_runtime_name = "runc"
      disable_snapshot_annotations = true
      discard_unpacked_layers = false
      ignore_blockio_not_enabled_errors = false
      ignore_rdt_not_enabled_errors = false
      no_pivot = false
      snapshotter = "overlayfs"

      [plugins."io.containerd.grpc.v1.cri".containerd.default_runtime]
        base_runtime_spec = ""
        cni_conf_dir = ""
        cni_max_conf_num = 0
        container_annotations = []
        pod_annotations = []
        privileged_without_host_devices = false
        privileged_without_host_devices_all_devices_allowed = false
        runtime_engine = ""
        runtime_path = ""
        runtime_root = ""
        runtime_type = ""
        sandbox_mode = ""
        snapshotter = ""

        [plugins."io.containerd.grpc.v1.cri".containerd.default_runtime.options]

      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]

        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          base_runtime_spec = ""
          cni_conf_dir = ""
          cni_max_conf_num = 0
          container_annotations = []
          pod_annotations = []
          privileged_without_host_devices = false
          privileged_without_host_devices_all_devices_allowed = false
          runtime_engine = ""
          runtime_path = ""
          runtime_root = ""
          runtime_type = "io.containerd.runc.v2"
          sandbox_mode = "podsandbox"
          snapshotter = ""

          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            BinaryName = ""
            CriuImagePath = ""
            CriuPath = ""
            CriuWorkPath = ""
            IoGid = 0
            IoUid = 0
            NoNewKeyring = false
            NoPivotRoot = false
            Root = ""
            ShimCgroup = ""
            SystemdCgroup = true

      [plugins."io.containerd.grpc.v1.cri".containerd.untrusted_workload_runtime]
        base_runtime_spec = ""
        cni_conf_dir = ""
        cni_max_conf_num = 0
        container_annotations = []
        pod_annotations = []
        privileged_without_host_devices = false
        privileged_without_host_devices_all_devices_allowed = false
        runtime_engine = ""
        runtime_path = ""
        runtime_root = ""
        runtime_type = ""
        sandbox_mode = ""
        snapshotter = ""

        [plugins."io.containerd.grpc.v1.cri".containerd.untrusted_workload_runtime.options]

    [plugins."io.containerd.grpc.v1.cri".image_decryption]
      key_model = "node"

    [plugins."io.containerd.grpc.v1.cri".registry]
      config_path = ""

      [plugins."io.containerd.grpc.v1.cri".registry.auths]

      [plugins."io.containerd.grpc.v1.cri".registry.configs]

      [plugins."io.containerd.grpc.v1.cri".registry.headers]

      [plugins."io.containerd.grpc.v1.cri".registry.mirrors]

    [plugins."io.containerd.grpc.v1.cri".x509_key_pair_streaming]
      tls_cert_file = ""
      tls_key_file = ""

  [plugins."io.containerd.internal.v1.opt"]
    path = "/opt/containerd"

  [plugins."io.containerd.internal.v1.restart"]
    interval = "10s"

  [plugins."io.containerd.internal.v1.tracing"]

  [plugins."io.containerd.metadata.v1.bolt"]
    content_sharing_policy = "shared"

  [plugins."io.containerd.monitor.v1.cgroups"]
    no_prometheus = false

  [plugins."io.containerd.nri.v1.nri"]
    disable = true
    disable_connections = false
    plugin_config_path = "/etc/nri/conf.d"
    plugin_path = "/opt/nri/plugins"
    plugin_registration_timeout = "5s"
    plugin_request_timeout = "2s"
    socket_path = "/var/run/nri/nri.sock"

  [plugins."io.containerd.runtime.v1.linux"]
    no_shim = false
    runtime = "runc"
    runtime_root = ""
    shim = "containerd-shim"
    shim_debug = false

  [plugins."io.containerd.runtime.v2.task"]
    platforms = ["linux/amd64"]
    sched_core = false

  [plugins."io.containerd.service.v1.diff-service"]
    default = ["walking"]
    sync_fs = false

  [plugins."io.containerd.service.v1.tasks-service"]
    blockio_config_file = ""
    rdt_config_file = ""

  [plugins."io.containerd.snapshotter.v1.aufs"]
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.blockfile"]
    fs_type = ""
    mount_options = []
    root_path = ""
    scratch_file = ""

  [plugins."io.containerd.snapshotter.v1.btrfs"]
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.devmapper"]
    async_remove = false
    base_image_size = ""
    discard_blocks = false
    fs_options = ""
    fs_type = ""
    pool_name = ""
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.native"]
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.overlayfs"]
    mount_options = []
    root_path = ""
    sync_remove = false
    upperdir_label = false

  [plugins."io.containerd.snapshotter.v1.zfs"]
    root_path = ""

  [plugins."io.containerd.tracing.processor.v1.otlp"]

  [plugins."io.containerd.transfer.v1.local"]
    config_path = ""
    max_concurrent_downloads = 3
    max_concurrent_uploaded_layers = 3

    [[plugins."io.containerd.transfer.v1.local".unpack_config]]
      differ = ""
      platform = "linux/amd64"
      snapshotter = "overlayfs"

[proxy_plugins]

[stream_processors]

  [stream_processors."io.containerd.ocicrypt.decoder.v1.tar"]
    accepts = ["application/vnd.oci.image.layer.v1.tar+encrypted"]
    args = ["--decryption-keys-path", "/etc/containerd/ocicrypt/keys"]
    env = ["OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf"]
    path = "ctd-decoder"
    returns = "application/vnd.oci.image.layer.v1.tar"

  [stream_processors."io.containerd.ocicrypt.decoder.v1.tar.gzip"]
    accepts = ["application/vnd.oci.image.layer.v1.tar+gzip+encrypted"]
    args = ["--decryption-keys-path", "/etc/containerd/ocicrypt/keys"]
    env = ["OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf"]
    path = "ctd-decoder"
    returns = "application/vnd.oci.image.layer.v1.tar+gzip"

[timeouts]
  "io.containerd.timeout.bolt.open" = "0s"
  "io.containerd.timeout.metrics.shimstats" = "2s"
  "io.containerd.timeout.shim.cleanup" = "5s"
  "io.containerd.timeout.shim.load" = "5s"
  "io.containerd.timeout.shim.shutdown" = "3s"
  "io.containerd.timeout.task.state" = "2s"

[ttrpc]
  address = ""
  gid = 0
  uid = 0`

I have configured SystemdCGroup as true. 

sudo systemctl status kubelet gives:

`â— kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             â””â”€10-kubeadm.conf
     Active: active (running) since Thu 2025-04-17 12:46:44 CEST; 16s ago
       Docs: https://kubernetes.io/docs/
   Main PID: 25638 (kubelet)
      Tasks: 19 (limit: 38346)
     Memory: 31.7M
        CPU: 327ms
     CGroup: /system.slice/kubelet.service
             â””â”€25638 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.9

Apr 17 12:46:45 t-X570-AORUS-ELITE kubelet[25638]: I0417 12:46:45.819473   25638 scope.go:117] "RemoveContainer" containerID="01d69ef1e0acf7ea6f351c272eed3ee8b0e214f227aade4481627e8d9e95a7cb"
Apr 17 12:46:45 t-X570-AORUS-ELITE kubelet[25638]: E0417 12:46:45.819713   25638 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"etcd\" with CrashLoopBackOff: \"back-off 10s restarting failed container=etcd pod=etcd-t-x570-aorus-elite_kube-system(964f70a04ec02731612f76d8fc652c6a)\"" pod="kube-system/etcd-t-x570-aorus-elite" podUID="964f70a04ec02731612f76d8fc652c6a"
Apr 17 12:46:46 t-X570-AORUS-ELITE kubelet[25638]: I0417 12:46:46.823385   25638 scope.go:117] "RemoveContainer" containerID="01d69ef1e0acf7ea6f351c272eed3ee8b0e214f227aade4481627e8d9e95a7cb"
Apr 17 12:46:46 t-X570-AORUS-ELITE kubelet[25638]: E0417 12:46:46.823602   25638 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"etcd\" with CrashLoopBackOff: \"back-off 10s restarting failed container=etcd pod=etcd-t-x570-aorus-elite_kube-system(964f70a04ec02731612f76d8fc652c6a)\"" pod="kube-system/etcd-t-x570-aorus-elite" podUID="964f70a04ec02731612f76d8fc652c6a"
Apr 17 12:46:52 t-X570-AORUS-ELITE kubelet[25638]: I0417 12:46:52.708723   25638 scope.go:117] "RemoveContainer" containerID="01d69ef1e0acf7ea6f351c272eed3ee8b0e214f227aade4481627e8d9e95a7cb"
Apr 17 12:46:52 t-X570-AORUS-ELITE kubelet[25638]: E0417 12:46:52.709020   25638 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"etcd\" with CrashLoopBackOff: \"back-off 10s restarting failed container=etcd pod=etcd-t-x570-aorus-elite_kube-system(964f70a04ec02731612f76d8fc652c6a)\"" pod="kube-system/etcd-t-x570-aorus-elite" podUID="964f70a04ec02731612f76d8fc652c6a"
Apr 17 12:46:54 t-X570-AORUS-ELITE kubelet[25638]: E0417 12:46:54.799049   25638 controller.go:146] "Failed to ensure lease exists, will retry" err="Get \"https://172.30.1.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/t-x570-aorus-elite?timeout=10s\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" interval="200ms"
Apr 17 12:46:54 t-X570-AORUS-ELITE kubelet[25638]: E0417 12:46:54.870136   25638 eviction_manager.go:258] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"t-x570-aorus-elite\" not found"
Apr 17 12:46:54 t-X570-AORUS-ELITE kubelet[25638]: I0417 12:46:54.992724   25638 scope.go:117] "RemoveContainer" containerID="01d69ef1e0acf7ea6f351c272eed3ee8b0e214f227aade4481627e8d9e95a7cb"
Apr 17 12:46:54 t-X570-AORUS-ELITE kubelet[25638]: E0417 12:46:54.993015   25638 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"etcd\" with CrashLoopBackOff: \"back-off 10s restarting failed container=etcd pod=etcd-t-x570-aorus-elite_kube-system(964f70a04ec02731612f76d8fc652c6a)\"" pod="kube-system/etcd-t-x570-aorus-elite" podUID="964f70a04ec02731612f76d8fc652c6a"`

sudo systemctl staus containerned gives

`â— containerd.service - containerd container runtime
     Loaded: loaded (/lib/systemd/system/containerd.service; enabled; vendor preset: enabled)
     Active: active (running) since Thu 2025-04-17 11:38:02 CEST; 1h 10min ago
       Docs: https://containerd.io
   Main PID: 916 (containerd)
      Tasks: 73
     Memory: 107.5M
        CPU: 3.943s
     CGroup: /system.slice/containerd.service
             â”œâ”€  916 /usr/bin/containerd
             â”œâ”€25697 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id aa591137059e838dde587ba94282c617c7534fc318680b8071721a8054e60bc1 -address /run/containerd/containerd.sock
             â”œâ”€25698 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id 42980a76631257b8b4eca1fe5cd36f78b3f97e057e1efb388ffbf2eeef46ec5d -address /run/containerd/containerd.sock
             â”œâ”€25711 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id d9e2ea40db550b3c10526d83a9cb88f08b8c42e521ba5663c4f63d868a2a9568 -address /run/containerd/containerd.sock
             â””â”€25744 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id 50a4eaf8ee3a44c0b32069495a0149a6ec94d2e9d5ccd7cb2196af7d206e689d -address /run/containerd/containerd.sock

Apr 17 12:48:29 t-X570-AORUS-ELITE containerd[916]: time="2025-04-17T12:48:29.812943575+02:00" level=info msg="CreateContainer within sandbox \"42980a76631257b8b4eca1fe5cd36f78b3f97e057e1efb388ffbf2eeef46ec5d\" for container &ContainerMetadata{Name:etcd,Attempt:41,}"
Apr 17 12:48:29 t-X570-AORUS-ELITE containerd[916]: time="2025-04-17T12:48:29.870128218+02:00" level=info msg="CreateContainer within sandbox \"42980a76631257b8b4eca1fe5cd36f78b3f97e057e1efb388ffbf2eeef46ec5d\" for &ContainerMetadata{Name:etcd,Attempt:41,} returns container id \"652e76f3a6000de03d4fa758ec7ca6c1f39a57765f8899c71c1a0547e8beca0f\""
Apr 17 12:48:29 t-X570-AORUS-ELITE containerd[916]: time="2025-04-17T12:48:29.870324463+02:00" level=info msg="StartContainer for \"652e76f3a6000de03d4fa758ec7ca6c1f39a57765f8899c71c1a0547e8beca0f\""
Apr 17 12:48:29 t-X570-AORUS-ELITE containerd[916]: time="2025-04-17T12:48:29.960988387+02:00" level=info msg="StartContainer for \"652e76f3a6000de03d4fa758ec7ca6c1f39a57765f8899c71c1a0547e8beca0f\" returns successfully"
Apr 17 12:48:29 t-X570-AORUS-ELITE containerd[916]: time="2025-04-17T12:48:29.961454036+02:00" level=info msg="received exit event container_id:\"652e76f3a6000de03d4fa758ec7ca6c1f39a57765f8899c71c1a0547e8beca0f\"  id:\"652e76f3a6000de03d4fa758ec7ca6c1f39a57765f8899c71c1a0547e8beca0f\"  pid:26627  exit_status:1  exited_at:{seconds:1744886909  nanos:961360978}"
Apr 17 12:48:29 t-X570-AORUS-ELITE containerd[916]: time="2025-04-17T12:48:29.997376260+02:00" level=info msg="shim disconnected" id=652e76f3a6000de03d4fa758ec7ca6c1f39a57765f8899c71c1a0547e8beca0f namespace=k8s.io
Apr 17 12:48:29 t-X570-AORUS-ELITE containerd[916]: time="2025-04-17T12:48:29.997401228+02:00" level=warning msg="cleaning up after shim disconnected" id=652e76f3a6000de03d4fa758ec7ca6c1f39a57765f8899c71c1a0547e8beca0f namespace=k8s.io
Apr 17 12:48:29 t-X570-AORUS-ELITE containerd[916]: time="2025-04-17T12:48:29.997413872+02:00" level=info msg="cleaning up dead shim" namespace=k8s.io
Apr 17 12:48:30 t-X570-AORUS-ELITE containerd[916]: time="2025-04-17T12:48:30.975119727+02:00" level=info msg="RemoveContainer for \"8c2f51261d95d09a783375ba2c8f754473846f686b604872b0b572dc7daddfb4\""
Apr 17 12:48:30 t-X570-AORUS-ELITE containerd[916]: time="2025-04-17T12:48:30.996741312+02:00" level=info msg="RemoveContainer for \"8c2f51261d95d09a783375ba2c8f754473846f686b604872b0b572dc7daddfb4\" returns successfully"`




#### What did you expect to happen?

It should init a kubernetes cluster and give me the credentials for kubadm join.

#### How can we reproduce it (as minimally and precisely as possible)?

`sudo kubeadm init --pod-network-cidr 10.200.0.0/16 --apiserver-advertise-address=172.30.1.2`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: v1.28.15
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
error: unable to parse the server version: invalid character '<' looking for beginning of value

</details>


#### Cloud provider

<details>
on premise
</details>


#### OS version

PRETTY_NAME="Ubuntu 22.04.5 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»è¯¥ Issue åé¦ˆçš„æ—¥å¿—ä¸æè¿°æ¥çœ‹ï¼Œä¸»è¦æ˜¯ kubeadm åˆå§‹åŒ– Kubernetes é›†ç¾¤æ—¶å‡ºç°äº† CrashLoopBackOffã€Dial Timeout ç­‰é”™è¯¯ï¼Œè¿™äº›é—®é¢˜é€šå¸¸ä¸ç½‘ç»œè”é€šæ€§ã€ç»„ä»¶é…ç½®ä¸å½“æˆ–èµ„æºä¸å¯ç”¨ç­‰å› ç´ æœ‰å…³ï¼Œè€Œéæ¼æ´æˆ–å®‰å…¨ç¼ºé™·å¯¼è‡´ã€‚Issue ä¸­å¹¶æœªæš´éœ²ä»»ä½•æ•æ„Ÿå¯†é’¥ï¼Œä¹Ÿä¸å­˜åœ¨å‘½ä»¤æ‰§è¡Œã€ææƒã€å®¹å™¨é€ƒé€¸ç­‰é«˜é£é™©å®‰å…¨é—®é¢˜çš„çº¿ç´¢ï¼Œå› æ­¤å¯ä»¥åˆæ­¥åˆ¤æ–­è¿™ä¸ª Issue åªæ¶‰åŠé›†ç¾¤ç¯å¢ƒæˆ–é…ç½®å±‚é¢çš„é—®é¢˜ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚

---


## Issue #131343 topologySpreadConstraints with DoNotSchedule not spreading pods across nodes as expected

- Issue é“¾æ¥ï¼š[#131343](https://github.com/kubernetes/kubernetes/issues/131343)

### Issue å†…å®¹

#### What happened?

When using `topologySpreadConstraints` with `whenUnsatisfiable: DoNotSchedule`, the pods are not spreading across nodes, even though sufficient resources are available and nodes are schedulable. Instead, all pods are scheduled on a single node.

However, when applying `podAntiAffinity` with the same label selector, the pods are correctly spread across nodes, confirming that node topology and resources are not the issue.

![Image](https://github.com/user-attachments/assets/602f2a21-8a74-493e-b21a-a3f73dce41ac)

#### What did you expect to happen?

Pods should be spread into different nodes.

#### How can we reproduce it (as minimally and precisely as possible)?

spec:
  topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          sub-app: hspsc-api-gateway-service-int
      matchLabelKeys:
        - pod-template-hash
      maxSkew: 1
      minDomains: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule

#### Anything else we need to know?

I am using karpenter in aws eks for nodemanagement. 

#### Kubernetes version

[akumar5@eks-jump karpenter]$ kubectl version
Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.6-eks-bc803b4
WARNING: version difference between client (1.28) and server (1.31) exceeds the supported minor version skew of +/-1


#### Cloud provider

AWS EKS

#### OS version

[akumar5@eks-jump karpenter]$ cat /etc/os-release
NAME="Amazon Linux"
VERSION="2"
ID="amzn"
ID_LIKE="centos rhel fedora"
VERSION_ID="2"
PRETTY_NAME="Amazon Linux 2"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2"
HOME_URL="https://amazonlinux.com/"

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®Issueå†…å®¹å¯çŸ¥ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸Kubernetes Pod è°ƒåº¦ç›¸å…³çš„é—®é¢˜ï¼Œæè¿°äº†ä½¿ç”¨ topologySpreadConstraints æ—¶ï¼ŒPod æœªæŒ‰ç…§é¢„æœŸåˆ†å¸ƒåˆ°å¤šä¸ªèŠ‚ç‚¹ä¸Šã€‚è¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½æˆ–é…ç½®å±‚é¢çš„é—®é¢˜ï¼Œæ²¡æœ‰ä»»ä½•æ½œåœ¨çš„æƒé™æå‡ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰è¿¹è±¡ï¼Œä¹Ÿæœªå¯¼è‡´å¯åˆ©ç”¨çš„DoSå®‰å…¨é£é™©ã€‚ä»…æ¶‰åŠKubernetesè°ƒåº¦ç­–ç•¥å’Œå·¥ä½œåŸç†ï¼Œä¸å±äºå®‰å…¨æ¼æ´èŒƒç•´ã€‚

---


## Issue #131341 Windows ServerCore/NanoServer volume mount ACL issue: ConatinerUser canâ€™t create files at root of CSIâ€‘mounted volume

- Issue é“¾æ¥ï¼š[#131341](https://github.com/kubernetes/kubernetes/issues/131341)

### Issue å†…å®¹

#### What happened?

* Create Windows workload mounting volumes.
* Run the workload with low permission, e.g., `ContainerUser`.
* After the workload pod starts, try to write files into the volume mounting directory.
* The write failed with **Acess is denied.** in the mounting path, but I can create a sub-directory and write files in the sub-directory.

The mounting directory permission setting is as follows:
```
PS C:\data> icacls .
. BUILTIN\Administrators:(OI)(CI)(F)
  NT AUTHORITY\SYSTEM:(OI)(CI)(F)
  CREATOR OWNER:(OI)(CI)(IO)(F)
  BUILTIN\Users:(OI)(CI)(RX)
  BUILTIN\Users:(CI)(AD)
  BUILTIN\Users:(CI)(IO)(WD)
  Everyone:(RX)
```

Please also notice that users with more permissive permissions don't have this issue, e.g., `ContainerAdministrator`.

This is the issue created in the CSI proxy repository.
Post here for cross-reference.
https://github.com/kubernetes-csi/csi-proxy/issues/382

#### What did you expect to happen?

I want users like `ContainerUser` to also have permission to create files in the root of the CSIâ€‘mounted volume because the Linux containers don't have the issue.

#### How can we reproduce it (as minimally and precisely as possible)?

``` yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kibishii-deployment
spec:
  persistentVolumeClaimRetentionPolicy:
    whenDeleted: Retain
    whenScaled: Retain
  podManagementPolicy: OrderedReady
  replicas: 2
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: kibishii
  serviceName: kibishii
  template:
    metadata:
      labels:
        app: kibishii
    spec:
      containers:
      - args:
        - ping 
        - -t 
        - localhost 
        - > 
        - NUL
        command:
        - cmd.exe
        - /c
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        image: mcr.microsoft.com/windows/servercore:ltsc2022
        name: kibishii
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          seccompProfile:
            type: RuntimeDefault
        volumeMounts:
        - mountPath: /data
          name: kibishii-data
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: windows
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext:
        windowsOptions:
          runAsUserName: "ContainerUser"
      terminationGracePeriodSeconds: 30
      tolerations:
      - effect: NoSchedule
        key: os
        operator: Equal
        value: windows
  updateStrategy:
    rollingUpdate:
      partition: 0
    type: RollingUpdate
  volumeClaimTemplates:
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: kibishii-data
    spec:
      accessModes:
      - ReadWriteOnce
      storageClassName: storage-class-name
      resources:
        requests:
          storage: 50Mi
      volumeMode: Filesystem
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.5
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.33.0
```

</details>


#### Cloud provider

<details>
vSphere
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»è¯¥Issueæè¿°æ¥çœ‹ï¼Œé—®é¢˜çš„æ ¸å¿ƒåœ¨äºWindowså®¹å™¨ä¸­çš„â€œContainerUserâ€æ™®é€šæƒé™è´¦æˆ·æ— æ³•åœ¨CSIæŒ‚è½½å·çš„æ ¹ç›®å½•ä¸‹åˆ›å»ºæ–‡ä»¶ï¼Œåªèƒ½åœ¨å­ç›®å½•å†…æ­£å¸¸åˆ›å»ºæ–‡ä»¶ã€‚æ­¤å¤„åæ˜ çš„æ›´å¤šæ˜¯ACLé…ç½®é—®é¢˜æˆ–æƒé™è®¾ç½®ä¸å½“å¼•èµ·çš„è®¿é—®å—é™ï¼Œå¹¶ä¸æ¶‰åŠè¶Šæƒã€ææƒã€ä¿¡æ¯æ³„éœ²æˆ–å…¶ä»–å…¸å‹å®‰å…¨é£é™©ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œè¿™æ˜¯å®¹å™¨ä¸­çš„æ™®é€šç”¨æˆ·ç¼ºå°‘å†™æƒé™é€ æˆçš„åŠŸèƒ½æ€§é™åˆ¶ï¼Œå¹¶æ²¡æœ‰æ”»å‡»è€…åˆ©ç”¨æˆ–æ•æ„Ÿä¿¡æ¯æ³„éœ²çš„åœºæ™¯ã€‚

ä¾æ®ç»™å®šçš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1. è¯¥é—®é¢˜å¹¶ä¸æ¶‰åŠæ”»å‡»è€…åˆ©ç”¨è¯¥ç¼ºå¤±æƒé™å»æ‰§è¡Œæ”»å‡»è¡Œä¸ºï¼Œåè€Œæ˜¯å®¹å™¨ç”¨æˆ·æ— æ³•è·å¾—å†™æƒé™ï¼Œå¹¶éä¼ ç»Ÿæ„ä¹‰ä¸Šçš„å®‰å…¨æ¼æ´ã€‚
2. æ— æ³•åœ¨æ ¹ç›®å½•å†™å…¥æ•°æ®å¯¹äºå®¹å™¨æ¥è¯´åªæ˜¯é…ç½®é—®é¢˜ï¼Œè€Œä¸æ˜¯è¶Šæƒè®¿é—®ã€ææƒæˆ–ä»£ç æ‰§è¡Œé—®é¢˜ã€‚
3. ä¸å­˜åœ¨æ”»å‡»è€…è·å–åˆ°æ¯”å…¶æƒé™æ›´é«˜èµ„æºçš„é€”å¾„ï¼Œä¹Ÿä¸å­˜åœ¨æ•æ„Ÿä¿¡æ¯è¢«é”™è¯¯æš´éœ²æˆ–æ—¥å¿—ä¸­æ³„éœ²å‡­æ®å¯¼è‡´çš„å®‰å…¨ä¾µå®³åœºæ™¯ã€‚

ç»¼ä¸Šåˆ¤æ–­ï¼Œè¯¥Issueå¹¶ä¸å±äºå®‰å…¨æ¼æ´èŒƒç•´ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

ç”±äºæ­¤é—®é¢˜ä¸å±äºå®‰å…¨é—®é¢˜ï¼Œå› æ­¤æ— éœ€ç»™å‡ºå¤ç°è„šæœ¬ã€‚è¯¥é—®é¢˜æ›´å¤šæ˜¯Windows ACLé…ç½®çš„è¡Œä¸ºå·®å¼‚ï¼Œå¯¼è‡´æ™®é€šçš„â€œContainerUserâ€è´¦æˆ·æ— æ³•åœ¨CSIæŒ‚è½½å·çš„æ ¹ç›®å½•åˆ›å»ºæ–‡ä»¶ï¼Œåº”é€šè¿‡é€‚å½“ä¿®æ”¹ACLé…ç½®è§£å†³ã€‚æ­¤é—®é¢˜æœ¬èº«å¹¶ä¸æ„æˆå®‰å…¨é£é™©ã€‚

---


## Issue #131314 Should pause pod evictions when zone is in Full Zonal Disruption

- Issue é“¾æ¥ï¼š[#131314](https://github.com/kubernetes/kubernetes/issues/131314)

### Issue å†…å®¹

#### What happened?

When all nodes in an AZ goes down then the zone is in Full Zonal Disruption during which we set the eviction ratelimiter to `HealthyQPSFunc`. Where as if a zone is experiencing PartialZonalDisruption then we stop all pod evictions by setting the rate limiter to `ReducedQPSFunc`.

All workloads in a zone gets evicted during FullZonalDisruption. I understand the idea to evict pods in a zone where we are not able to allocate/provision more capacity since its down. 



#### What did you expect to happen?

We expect to pause all pod evictions during FullZonalDisruption and set the rate limiter to `ReducedQPSFunc`.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a kubernetes cluster and mimic the scenario where all nodes in an AZ goes down causing the zone to be in FullZonalDisruption, KCM will start evicting pods.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

All Kubernetes version

</details>


#### Cloud provider

<details>

All Cloud Providers

</details>


#### OS version

<details>

NA

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»Issueçš„æè¿°æ¥çœ‹ï¼Œè¿™ä¸ªé—®é¢˜å±äºKubernetesåœ¨èŠ‚ç‚¹å…¨éƒ¨å®•æœºæ—¶å¯¹Podè¿›è¡Œé©±é€ï¼ˆEvictionï¼‰ç­–ç•¥çš„è®¨è®ºï¼Œä¸»è¦å…³æ³¨åœ¨å¯ç”¨æ€§ä¸è°ƒåº¦è¡Œä¸ºä¸Šï¼Œå¹¶æ²¡æœ‰æ¶‰åŠåˆ°ä»»æ„å‘½ä»¤æ‰§è¡Œã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€è¶Šæƒè®¿é—®æˆ–ææƒç­‰å®‰å…¨æ–¹é¢çš„å†…å®¹ï¼Œä¹ŸæœªæåŠå¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„åœºæ™¯ã€‚å› æ­¤è¯¥é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªè¿ç»´å’Œå¯ç”¨æ€§é—®é¢˜ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

æ­¤Issueæè¿°çš„åœºæ™¯ä¸å®‰å…¨æ”»å‡»æ— å…³ï¼Œä»…é’ˆå¯¹Kubernetesåœ¨FullZonalDisruptionæ—¶çš„Podé©±é€ç­–ç•¥è¿›è¡Œè®¨è®ºã€‚æœ¬è´¨ä¸Šä¸æ˜¯ä¸€ä¸ªå®‰å…¨é£é™©ï¼Œå› æ­¤æ— éœ€æä¾›å¤ç°è„šæœ¬ã€‚æ ¹æ®ç°æœ‰ä¿¡æ¯ç»¼åˆåˆ¤æ–­ï¼Œä¸æ„æˆå®‰å…¨æ¼æ´æˆ–å®‰å…¨å¨èƒï¼Œä¸å±äºéœ€è¦è¿›è¡Œå®‰å…¨åŠ å›ºçš„èŒƒç•´ã€‚

---


## Issue #131309 [FG:InPlacePodVerticalScaling] Performance degradation in latency-sensitive services due to CPU affinity loss upon guaranteed QoS Pod scaling down

- Issue é“¾æ¥ï¼š[#131309](https://github.com/kubernetes/kubernetes/issues/131309)

### Issue å†…å®¹

#### What happened?

For the latency-sensitive service in guaranteed QoS Pod, each worker has core affinity with a CPU. 
When the workload decreases, some workers are removed, and the worker's affinity CPUs becoming idle. 
When the Pod scaling down, these idle CPUs were excepted to be removed.
If the busy CPUs which run the busy worker are removed, the performance will be decreased for latency-sensitive services.
For example:
At the begin, there are 2 CPUs (CPU 1 anf CPU 11) are allocated to the Pod when Pod creation.
When the workload increases, Pod scale up, and 4 additional CPUs (CPU 2,12,3,13) are allocated to the Pod, add another 4 workers and set core affinity to each CPU.
![Image](https://github.com/user-attachments/assets/a19807c3-ba8d-40cd-8313-078f5b1afe07)

When the workload decreases, remove worker 4 and worker 5, CPU 3 and CPU 13 are idle, 
Ideal case:
When Pod scale down, idle CPU 3 and CPU 13 are excepted to be removed. the workers not be affected.
![Image](https://github.com/user-attachments/assets/75cad118-4ff5-457f-b38e-b5b94c48bac1)

Non-ideal case:
When Pod scale down, busy CPU 2 and CPU 12 are removed. the worker2 and worker3 will be affected because of CPU migration due to CPU affinity loss.
![Image](https://github.com/user-attachments/assets/978b2bbc-f7b3-44b6-9598-2cc9d320810c)

#### What did you expect to happen?

When Pod scale down, idle CPU 3 and CPU 13 are removed, and busy CPUs are not removed.

So, we need some method or option to let container tell kubelet which CPU should be keep.

#### How can we reproduce it (as minimally and precisely as possible)?

Test based on https://github.com/kubernetes/kubernetes/pull/129719

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v0.0.0-master+da732990954ed92779d8fe396ab9e217f038d115
```

</details>


#### Cloud provider

<details>
Self-host
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
6.5.0-41-generic #41~22.04.2-Ubuntu SMP PREEMPT_DYNAMIC Mon Jun  3 11:32:55 UTC 2 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†åœ¨ Guaranteed QoS Pod ç¼©å®¹æ—¶ï¼Œå¯èƒ½å› ä¸ºç§»é™¤ CPU å¯¼è‡´åŸæœ‰ç»‘å®šåœ¨å¿™ç¢Œ CPU ä¸Šçš„å·¥ä½œçº¿ç¨‹è¢«è¿«è¿ç§»ï¼Œè¿›è€Œé€ æˆæ€§èƒ½ä¸‹é™çš„é—®é¢˜ã€‚æ­¤é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ç”±äº CPU äº²å’Œæ€§åœ¨ç¼©å®¹æ—¶è¢«æ‰“ç ´ï¼Œè€Œå¹¶æœªæ¶‰åŠè¶Šæƒè®¿é—®ã€ææƒã€å‘½ä»¤æ‰§è¡Œæˆ–æ•æ„Ÿä¿¡æ¯æ³„éœ²ç­‰å®‰å…¨é£é™©ï¼Œå› æ­¤ä¸å±äºå®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œä¸éœ€è¦å®‰å…¨æ¼æ´ç›¸å…³çš„å¤ç°è„šæœ¬ã€‚
# å› æ­¤ï¼Œæ­¤å¤„ä¸æä¾›å¤ç°è„šæœ¬ã€‚
pass
```


---


## Issue #131308 Pod stuck in terminating state forever with unlinkat: device or resource busy

- Issue é“¾æ¥ï¼š[#131308](https://github.com/kubernetes/kubernetes/issues/131308)

### Issue å†…å®¹

#### What happened?

Environmental Info:
K3s Version:
v1.30.6+k3s1

Node(s) CPU architecture, OS, and Version:
Linux aarch64 GNU/Linux.  k3s on edge device so it doesn't have full fledged linux and we build our own version of linux

Kernel Version: 4.19.294

File System:
```
df -T /container/k3s/var/lib/rancher/k3s/
Filesystem           Type       1K-blocks      Used Available Use% Mounted on
/dev/mmcblk0p24      ext4         5757144   2972392   2471408  55% /container
```

Cluster Configuration:

Describe the bug:
When I delete any pod, it gets stuck in terminating state with following error:

```
E0217 14:13:02.919124   11533 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/29da47e0-52ea-49d5-bcc1-39824ad9abc2-kube-api-access-b6pqn podName:29da47e0-52ea-49d5-bcc1-39824ad9abc2 nodeName:}" failed. No retries permitted until 2025-02-17 14:13:06.9190655 +0000 UTC m=+967942.401661721 (durationBeforeRetry 4s). Error: UnmountVolume.TearDown failed for volume "kube-api-access-b6pqn" (UniqueName: "kubernetes.io/projected/29da47e0-52ea-49d5-bcc1-39824ad9abc2-kube-api-access-b6pqn") pod "29da47e0-52ea-49d5-bcc1-39824ad9abc2" (UID: "29da47e0-52ea-49d5-bcc1-39824ad9abc2") : unlinkat /var/lib/kubelet/pods/29da47e0-52ea-49d5-bcc1-39824ad9abc2/volumes/kubernetes.io~projected/kube-api-access-b6pqn: device or resource busy
I0217 14:13:06.948746   11533 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-b6pqn\" (UniqueName: \"kubernetes.io/projected/29da47e0-52ea-49d5-bcc1-39824ad9abc2-kube-api-access-b6pqn\") pod \"29da47e0-52ea-49d5-bcc1-39824ad9abc2\" (UID: \"29da47e0-52ea-49d5-bcc1-39824ad9abc2\") "
```

If I check the mountinfo of the pod it shows like this:
```
AP5# cat /proc/self/mountinfo | grep 29da47e0-52ea-49d5-bcc1-39824ad9abc2
80 76 0:43 / /container/k3s/var/lib/kubelet/pods/29da47e0-52ea-49d5-bcc1-39824ad9abc2/volumes/kubernetes.io~projected/kube-api-access-b6pqn rw,relatime shared:6 - tmpfs tmpfs rw
```

When I run umount command from node terminal, umount goes through:
```
AP5# umount /var/lib/kubelet/pods/29da47e0-52ea-49d5-bcc1-39824ad9abc2/volumes/kubernetes.io~projected/kube-api-access-b6pqn
```

After this manual intervention, the pod gets deleted.

#### What did you expect to happen?

I expect pod to be deleted when I run `kubectl delete pod <pod_name>`

#### How can we reproduce it (as minimally and precisely as possible)?

I think it happens on the customized linux we have. However, I can help with any info required

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# 1.30.1
```

</details>


#### Cloud provider

<details>
Baremetal
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»è¯¥Issueçš„æè¿°æ¥çœ‹ï¼Œé—®é¢˜ä¸»è¦æ˜¯Podåœ¨åˆ é™¤æ—¶å‡ºç°äº†â€œdevice or resource busyâ€çš„é”™è¯¯ï¼Œå¯¼è‡´Podä¸€ç›´å¤„äºTerminatingçŠ¶æ€ï¼Œéœ€è¦æ‰‹åŠ¨umountæ‰èƒ½åˆ é™¤ã€‚è¯¥é—®é¢˜æ›´åƒæ˜¯åº•å±‚ç³»ç»Ÿæˆ–æ–‡ä»¶ç³»ç»Ÿåœ¨ç‰¹å®šç¯å¢ƒä¸‹ï¼ˆè‡ªå®šä¹‰Linuxç‰ˆæœ¬ã€ç‰¹å®šCSIæˆ–æŒ‚è½½é€‰é¡¹ï¼‰å­˜åœ¨çš„å…¼å®¹æˆ–é…ç½®é—®é¢˜ï¼Œå¹¶æœªä½“ç°å¯è¢«å¤–éƒ¨æ”»å‡»è€…åˆ©ç”¨çš„åœºæ™¯ï¼Œä¹Ÿæ²¡æœ‰æ³„éœ²æ•æ„Ÿä¿¡æ¯ã€å‘½ä»¤æ‰§è¡Œã€æƒé™æå‡æˆ–å®¹å™¨é€ƒé€¸ç­‰å®‰å…¨é£é™©ã€‚å› æ­¤ä¸å±äºå®‰å…¨é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ— éœ€å¤ç°è„šæœ¬
# å¦‚éœ€éªŒè¯Pod stuck in terminatingçš„é—®é¢˜ï¼Œè¯·åœ¨è‡ªå®šä¹‰Linuxç¯å¢ƒä¸‹é‡å¤åˆ é™¤è¢«æŒ‚è½½çš„Podï¼Œ
# å¹¶è§‚å¯Ÿæ˜¯å¦å†æ¬¡å‡ºç°æ— æ³•æ­£å¸¸umountçš„æƒ…å†µã€‚
```


**è§£é‡Šè¯´æ˜ï¼š**

ä¸Šè¿°Issueæè¿°çš„ç°è±¡å±äºç¯å¢ƒæˆ–ç³»ç»Ÿå…¼å®¹æ€§é—®é¢˜ï¼Œæ— æ³•åœ¨å…¶ä¸­å‘ç°å¯è¢«åˆ©ç”¨çš„å®‰å…¨é£é™©ï¼Œæ•…æ— éœ€æ„é€ å¤ç°è„šæœ¬è¿›è¡Œå®‰å…¨æ¼æ´éªŒè¯ã€‚è¯¥è„šæœ¬ä»…åœ¨è¯´æ˜ä¸æ¶‰åŠå®‰å…¨é£é™©çš„åœºæ™¯ä¸‹ç»™å‡ºæç¤ºæ€§è¯´æ˜ã€‚è‹¥éœ€æ’æŸ¥é—®é¢˜ï¼Œå¯æ ¹æ®Issueä¸­çš„æ“ä½œæ­¥éª¤ï¼ˆæ‰‹åŠ¨umountç­‰ï¼‰åœ¨ç›¸åŒæˆ–ç›¸ä¼¼ç¯å¢ƒä¸‹è¿›è¡Œé‡å¤å®éªŒï¼Œè¿›è¡Œç³»ç»Ÿå…¼å®¹æ€§æ£€æŸ¥è€Œéå®‰å…¨é£é™©éªŒè¯ã€‚

---


## Issue #131304 Pods on very few worker nodes do not have network connectivity after control plane and calico upgrade

- Issue é“¾æ¥ï¼š[#131304](https://github.com/kubernetes/kubernetes/issues/131304)

### Issue å†…å®¹

#### What happened?

Upgraded control plane to 1.29 and calico to v3.28.2 - suspecting issue started here.

After upgrade, pods (on few nodes only) do not have network connectivity, unable to ping node IP or kube-dns.
Able to connect/query kube-dns from node. 

Calico-node pod is running fine. Nothing suspicious in logs. Tried restarting services kubelet, kube-proxy, containerd etc. and calico-node pod. Flushed conntrack, iptables. Nothing helps.

Rebooting node fixes the issue.  

Could be something related to node but very random, nothing suspicious in dmesg or worker node logs.


#### What did you expect to happen?

After control plane upgrade, daemonset(including calico-node) rollout restart without any issues.

#### How can we reproduce it (as minimally and precisely as possible)?

Can't reproduce as this occurs only on very few worker nodes.

#### Anything else we need to know?

_No response_

#### Kubernetes version


```console
$ kubectl version
Client Version: v1.29.12
Server Version: v1.29.12
```


#### Cloud provider

Baremetal


#### OS version

<details>

```console
# On Linux:
# cat /etc/redhat-release 
Red Hat Enterprise Linux release 9.3


# uname -a
Linux myhostname.mycompany.com 6.6.22-custom1.el9.x86_64 #1 SMP Thu Mar 14 00:33:28 PDT 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

RuntimeName:  containerd
RuntimeVersion:  1.7.24

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
# calicoctl version
Client Version:    v3.28.2
Cluster Version:   v3.28.2
Cluster Type:      k8s,bgp
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»é—®é¢˜æè¿°æ¥çœ‹ï¼Œè¿™æ˜¯åœ¨å‡çº§ Kubernetes æ§åˆ¶å¹³é¢å’Œ Calico åå‡ºç°çš„ç½‘ç»œè¿é€šæ€§å¼‚å¸¸ï¼Œä¸»è¦è¡¨ç°ä¸ºéƒ¨åˆ†èŠ‚ç‚¹çš„ Pod æ— æ³•è®¿é—®èŠ‚ç‚¹ IP æˆ– kube-dnsï¼Œéœ€è¦é€šè¿‡é‡å¯èŠ‚ç‚¹æ‰èƒ½æš‚æ—¶è§£å†³ã€‚è¯¥é—®é¢˜æ›´å€¾å‘äºç½‘ç»œæˆ–é…ç½®æ–¹é¢çš„æ•…éšœï¼Œå¹¶æœªæåŠå¯è¢«æ”»å‡»è€…åˆ©ç”¨æˆ–å¯èƒ½å¯¼è‡´å®‰å…¨æ¼æ´çš„åœºæ™¯ï¼Œä¹ŸæœªåŒ…å«æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€ææƒã€æ‹’ç»æœåŠ¡ç­‰å…¸å‹å®‰å…¨é£é™©çš„è¦ç´ ï¼Œå› æ­¤è¯¥é—®é¢˜ä¸å±äºå®‰å…¨æ¼æ´ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Issue æ›´åƒæ˜¯å‡çº§åå‡ºç°çš„ç½‘ç»œä¸ç¨³å®šæˆ–å…¼å®¹æ€§é—®é¢˜ï¼Œå¹¶éå®‰å…¨æ¼æ´ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè‹¥éå®‰å…¨é—®é¢˜åˆ™é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ï¼Œå› æ­¤æ— éœ€æä¾›å®‰å…¨æ¼æ´å¤ç°è„šæœ¬ã€‚è‹¥åç»­åœ¨é—®é¢˜æ’æŸ¥è¿‡ç¨‹ä¸­å‘ç°å…·æœ‰å®‰å…¨å½±å“çš„å› ç´ ï¼Œåˆ™éœ€è¦é‡æ–°è¯„ä¼°ã€‚

---


## Issue #131303 After kubelet is restarted, the pod status is temporarily reset to 0/1.

- Issue é“¾æ¥ï¼š[#131303](https://github.com/kubernetes/kubernetes/issues/131303)

### Issue å†…å®¹

#### What happened?

After the kubelet process is restarted, the pod status is reset to 0/1. After the ready probe is ready, the pod status is restored to 1/1.As a result, the pod cannot receive traffic for a short time.

#### What did you expect to happen?

When kubelet is restarted, the pod status should not change.

#### How can we reproduce it (as minimally and precisely as possible)?

Restart the kubelet process.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.31
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»è¯¥Issueçš„æè¿°æ¥çœ‹ï¼Œä¸»è¦æ˜¯kubeleté‡å¯åï¼ŒPodçš„å°±ç»ªæ¢é’ˆï¼ˆreadiness probeï¼‰çŠ¶æ€ä¼šåœ¨çŸ­æ—¶é—´å†…è¢«é‡ç½®ä¸º0/1ï¼Œä»è€Œå¯¼è‡´Podæš‚æ—¶æ— æ³•æ¥æ”¶æµé‡ã€‚è¿™å±äºé›†ç¾¤åœ¨kubeleté‡å¯æ—¶çš„æ­£å¸¸æˆ–åŠæ­£å¸¸è¡Œä¸ºï¼Œæˆ–è‡³å°‘æ˜¯ä¸€ä¸ªå¯é¢„æœŸçš„è¿è¡Œæ—¶ç°è±¡/ç¼ºé™·ï¼Œå¹¶ä¸ä»£è¡¨æœ‰å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚  
1) è¯¥é—®é¢˜å¹¶æœªæè¿°ä»»ä½•å¯¹åˆ¶è¡¨æˆ–å¯¹ç”¨æˆ·æƒé™çš„é”™è¯¯æ“ä½œï¼Œä¹Ÿæ— å¯¼è‡´å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€æƒé™æå‡ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ç­‰æƒ…å†µã€‚  
2) è¯¥é—®é¢˜è™½ä¼šåœ¨çŸ­æ—¶é—´å†…å½±å“Podçš„å¯ç”¨æ€§ï¼Œä½†è¿™æ˜¯åœ¨kubeleté‡å¯è¿‡ç¨‹ä¸­å‡ºç°çš„æš‚æ—¶ä¸å¯ç”¨åœºæ™¯ï¼Œæ— æ³•è¢«ä½æƒé™æˆ–æœªæˆæƒçš„ç”¨æˆ·æ¶æ„åˆ©ç”¨æ¥é€ æˆå¯æŒç»­çš„æ‹’ç»æœåŠ¡æ”»å‡»ï¼Œå› æ­¤ä¸æ„æˆå®‰å…¨é£é™©ã€‚  

ç»¼åˆåˆ¤æ–­ï¼Œè¯¥Issueå¹¶æœªä½“ç°å‡ºå­˜åœ¨å®‰å…¨æ¼æ´çš„é£é™©åœºæ™¯ã€‚

---


## Issue #131287 The garbage collector successfully deletes the CronJob and its associated Pod; however, Kubernetes subsequently issues GET requests for both the Pod and the Deployment(with pod name)

- Issue é“¾æ¥ï¼š[#131287](https://github.com/kubernetes/kubernetes/issues/131287)

### Issue å†…å®¹

#### What happened?

Background:
I created a CronJob named ```acadia-image-eviction-job``` with the ```ttlSecondsAfterFinished: 86400``` setting. Upon execution, this CronJob created a Pod with the name ```acadia-image-eviction-job-28976255-cc47j```.
Once the Job completed, the Kubernetes garbage collector correctly deleted both the Job and the corresponding Pod as expected.
However, I observed some unexpected behavior afterward:
Kubernetes initiated a GET request for a deployment using the Pod name.
Approximately 32 hours later, Kubernetes made another GET request for the Pod using the correct Pod name.
This raises the question of why such requests are being made, especially when ttlSecondsAfterFinished is explicitly set to 86400 seconds (24 hours).
Attached are the relevant logs for the Pod for reference.
```
<head></head>
2025-02-03 15:11:15 | Begin ProcessImageEviction | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
-- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --
2025-02-03 15:11:15 | [eventing.publisherImpl.connectToKafka] Creating Kafka producer with endpoints: [kafka-0-mh-yzhpdqxqdhqxpxdlvpjj.private.us-south.messagehub.appdomain.cloud:9093 kafka-1-mh-yzhpdqxqdhqxpxdlvpjj.private.us-south.messagehub.appdomain.cloud:9093 kafka-2-mh-yzhpdqxqdhqxpxdlvpjj.private.us-south.messagehub.appdomain.cloud:9093] | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:16 | I0203 09:41:15.592145Â Â Â Â Â Â Â 1 httplog.go:132] "HTTP" verb="GET" URI="/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j" latency="1.830492ms" userAgent="kubelet/v1.29.9 (linux/amd64) kubernetes/114a1f5" audit-ID="08c4e11a-c190-4eeb-9aee-19748a8ebd74" srcIP="10.51.170.59:49600" apf_pl="system" apf_fs="system-nodes" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.60048ms" resp=200 | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:16 | I0203 09:41:15.599334Â Â Â Â Â Â Â 1 httplog.go:132] "HTTP" verb="PATCH" URI="/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j/status" latency="6.293744ms" userAgent="kubelet/v1.29.9 (linux/amd64) kubernetes/114a1f5" audit-ID="56a4d9bc-6220-4c00-a016-8b259e1840e0" srcIP="10.51.170.59:49600" apf_pl="system" apf_fs="system-nodes" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="5.984343ms" resp=200 | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:16 | Processing starting for resource | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:16 | Processing complete for resource with update | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:16 | I0203 09:41:15.603897Â Â 720470 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="genctl/acadia-image-eviction-job-28976255-cc47j" podStartSLOduration=4.830957699 podStartE2EDuration="6m15.603843321s" podCreationTimestamp="2025-02-03 09:35:00 +0000 UTC" firstStartedPulling="2025-02-03 09:35:04.219758043 +0000 UTC m=+3361863.614132070" lastFinishedPulling="2025-02-03 09:41:14.99264366 +0000 UTC m=+3362234.387017692" observedRunningTime="2025-02-03 09:41:15.599547747 +0000 UTC m=+3362234.993921779" watchObservedRunningTime="2025-02-03 09:41:15.603843321 +0000 UTC m=+3362234.998217348"
2025-02-03 15:11:16 | [2025/02/03 09:41:16] [ info] [input:tail:input_tail_container_logs] inotify_fs_add(): inode=11075207 watch_fd=86613 name=/var/log/containers/acadia-image-eviction-job-28976255-cc47j_genctl_acadia-image-eviction-job-2219e9c93dfd71d3cdfb400315a6fe2ccea78b922bb9d9e932917fac10d716b2.log | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:25 | I0203 09:41:24.579773Â Â Â Â Â Â Â 1 trace.go:219] Trace[745171306]: "List" accept:application/json, */*,audit-id:ab09692b-b4c5-483c-ad9e-28eab42cf431,client:172.30.29.69,protocol:HTTP/2.0,resource:imagearchives,scope:cluster,url:/apis/image.rias.ibm.com/v1alpha1/imagearchives,user-agent:acadia-image-eviction-job/v0.0.0 (linux/amd64) kubernetes/$Format/acadia-image-eviction-job-28976255-cc47j,verb:LIST (03-Feb-2025 09:41:15.204) (total time: 9375ms): | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:25 | I0203 09:41:24.579844Â Â Â Â Â Â Â 1 httplog.go:132] "HTTP" verb="LIST" URI="/apis/image.rias.ibm.com/v1alpha1/imagearchives?labelSelector=image_archive.acadia_status%3Dcompleted" latency="9.375806626s" userAgent="acadia-image-eviction-job/v0.0.0 (linux/amd64) kubernetes/$Format/acadia-image-eviction-job-28976255-cc47j" audit-ID="ab09692b-b4c5-483c-ad9e-28eab42cf431" srcIP="172.30.29.69:34894" apf_pl="exempt" apf_fs="exempt" apf_execution_time="9.375620679s" resp=200 | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:25 | RoundTrip: peer certificate :: SN: 296256592080292144230695847868833894271969405412 Issuer: CN=rias-ng-us-south-dal13-preprod_issuing_ca Subject: CN=regional-extension-server,OU=IBM Cloud Virtual Private Cloud,L=Armonk,ST=New York,C=US CN: regional-extension-server | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:25 | End ProcessImageEviction | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:26 | I0203 09:41:25.669856Â Â Â Â Â Â Â 1 httplog.go:132] "HTTP" verb="GET" URI="/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j" latency="3.911684ms" userAgent="kubelet/v1.29.9 (linux/amd64) kubernetes/114a1f5" audit-ID="0574d69b-5fd2-4fa1-812b-6d80c48f1fce" srcIP="10.51.170.59:49600" apf_pl="system" apf_fs="system-nodes" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.6756ms" resp=200 | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:26 | I0203 09:41:25.677514Â Â Â Â Â Â Â 1 httplog.go:132] "HTTP" verb="PATCH" URI="/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j/status" latency="6.528678ms" userAgent="kubelet/v1.29.9 (linux/amd64) kubernetes/114a1f5" audit-ID="2aa3844b-54e8-4c07-9609-67c764434fe9" srcIP="10.51.170.59:49600" apf_pl="system" apf_fs="system-nodes" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="6.230063ms" resp=200 | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:26 | Processing complete for resource with update | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:26 | Processing starting for resource | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:28 | "Args": "IgnoreUnknown=1;K8S_POD_NAMESPACE=genctl;K8S_POD_NAME=acadia-image-eviction-job-28976255-cc47j;K8S_POD_INFRA_CONTAINER_ID=9a713f63594532d108d1debd3f7498756b7a4c12210aacd9fb2c2270de1d1691;K8S_POD_UID=d20d4043-6ff1-4143-8b86-26e6681bde12", | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:28 | CNI_Plugin: 258: 9a713f63594532d1: CNI_ARGS=IgnoreUnknown=1;K8S_POD_NAMESPACE=genctl;K8S_POD_NAME=acadia-image-eviction-job-28976255-cc47j;K8S_POD_INFRA_CONTAINER_ID=9a713f63594532d108d1debd3f7498756b7a4c12210aacd9fb2c2270de1d1691;K8S_POD_UID=d20d4043-6ff1-4143-8b86-26e6681bde12 | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:28 | I0203 09:41:28.055114Â Â Â Â Â Â Â 1 httplog.go:132] "HTTP" verb="GET" URI="/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j" latency="2.363212ms" userAgent="kubelet/v1.29.9 (linux/amd64) kubernetes/114a1f5" audit-ID="b9a41cd3-5d78-481d-be24-698752c936e1" srcIP="10.51.170.59:49600" apf_pl="system" apf_fs="system-nodes" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.832538ms" resp=200 | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:28 | I0203 09:41:28.063121Â Â Â Â Â Â Â 1 httplog.go:132] "HTTP" verb="PATCH" URI="/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j/status" latency="7.116743ms" userAgent="kubelet/v1.29.9 (linux/amd64) kubernetes/114a1f5" audit-ID="7855db5f-af62-4331-a41f-cc0d4da3bfb1" srcIP="10.51.170.59:49600" apf_pl="system" apf_fs="system-nodes" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="6.798459ms" resp=200 | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:28 | Processing starting for resource | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:28 | Processing complete for resource with update | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:29 | I0203 09:41:28.693245Â Â Â Â Â Â Â 1 httplog.go:132] "HTTP" verb="GET" URI="/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j" latency="1.953523ms" userAgent="kubelet/v1.29.9 (linux/amd64) kubernetes/114a1f5" audit-ID="26aea5a3-739b-413b-bae6-9d87ae5547fb" srcIP="10.51.170.59:49600" apf_pl="system" apf_fs="system-nodes" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.750709ms" resp=200 | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:29 | I0203 09:41:29.078344Â Â Â Â Â Â Â 1 httplog.go:132] "HTTP" verb="PATCH" URI="/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j" latency="7.768549ms" userAgent="kube-controller-manager/v1.29.9 (linux/amd64) kubernetes/114a1f5/system:serviceaccount:kube-system:job-controller" audit-ID="54e91ace-9844-4232-9eb9-967f7cc21d12" srcIP="11.105.4.2:56082" apf_pl="workload-high" apf_fs="kube-system-service-accounts" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="7.461448ms" resp=200 | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:29 | Processing starting for resource | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:29 | Processing complete for resource with update | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:47 | I0203 09:41:46.559173Â Â Â Â Â Â Â 1 garbagecollector.go:549] "Processing item" item="[v1/Pod, namespace: genctl, name: acadia-image-eviction-job-28976255-cc47j, uid: d20d4043-6ff1-4143-8b86-26e6681bde12]" virtual=false | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:47 | I0203 09:41:46.561384Â Â Â Â Â Â Â 1 garbagecollector.go:688] "Deleting item" item="[v1/Pod, namespace: genctl, name: acadia-image-eviction-job-28976255-cc47j, uid: d20d4043-6ff1-4143-8b86-26e6681bde12]" propagationPolicy="Background" | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:47 | I0203 09:41:46.561184Â Â Â Â Â Â Â 1 httplog.go:132] "HTTP" verb="GET" URI="/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j" latency="1.723412ms" userAgent="kube-controller-manager/v1.29.9 (linux/amd64) kubernetes/114a1f5/system:serviceaccount:kube-system:generic-garbage-collector" audit-ID="dfcb6166-3adf-4488-abbe-534e7b5cacd3" srcIP="11.105.4.2:56082" apf_pl="workload-high" apf_fs="kube-system-service-accounts" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.42406ms" resp=200 | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:47 | Processing starting for resource | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:47 | Processing complete for resource with update | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:11:47 | I0203 09:41:46.569516Â Â Â Â Â Â Â 1 httplog.go:132] "HTTP" verb="DELETE" URI="/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j" latency="5.304571ms" userAgent="kube-controller-manager/v1.29.9 (linux/amd64) kubernetes/114a1f5/system:serviceaccount:kube-system:generic-garbage-collector" audit-ID="de96766b-c42c-4843-a0f4-a765a4bbf384" srcIP="11.105.4.2:19620" apf_pl="workload-high" apf_fs="kube-system-service-accounts" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="5.074275ms" resp=200 | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:12:06 | CNI_Plugin: 258: 9a713f63594532d1: CNI_ARGS=K8S_POD_INFRA_CONTAINER_ID=9a713f63594532d108d1debd3f7498756b7a4c12210aacd9fb2c2270de1d1691;K8S_POD_UID=d20d4043-6ff1-4143-8b86-26e6681bde12;IgnoreUnknown=1;K8S_POD_NAMESPACE=genctl;K8S_POD_NAME=acadia-image-eviction-job-28976255-cc47j | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:12:06 | "Args": "K8S_POD_INFRA_CONTAINER_ID=9a713f63594532d108d1debd3f7498756b7a4c12210aacd9fb2c2270de1d1691;K8S_POD_UID=d20d4043-6ff1-4143-8b86-26e6681bde12;IgnoreUnknown=1;K8S_POD_NAMESPACE=genctl;K8S_POD_NAME=acadia-image-eviction-job-28976255-cc47j", | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:12:06 | CNI_Plugin: 258: 9a713f63594532d1: CNI_ARGS=K8S_POD_NAME=acadia-image-eviction-job-28976255-cc47j;K8S_POD_INFRA_CONTAINER_ID=9a713f63594532d108d1debd3f7498756b7a4c12210aacd9fb2c2270de1d1691;K8S_POD_UID=d20d4043-6ff1-4143-8b86-26e6681bde12;IgnoreUnknown=1;K8S_POD_NAMESPACE=genctl | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 15:12:06 | "Args": "K8S_POD_NAME=acadia-image-eviction-job-28976255-cc47j;K8S_POD_INFRA_CONTAINER_ID=9a713f63594532d108d1debd3f7498756b7a4c12210aacd9fb2c2270de1d1691;K8S_POD_UID=d20d4043-6ff1-4143-8b86-26e6681bde12;IgnoreUnknown=1;K8S_POD_NAMESPACE=genctl", | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 20:59:35 | I0203 15:29:35.210000Â Â Â Â Â Â Â 1 httplog.go:132] "HTTP" verb="GET" URI="/apis/apps/v1/namespaces/genctl/deployments/acadia-image-eviction-job-28976255-cc47j" latency="1.988411ms" userAgent="kubectl/v1.25.14 (linux/amd64) kubernetes/a5967a3" audit-ID="e9ffac2d-b409-49cc-92ef-7ccb403dd622" srcIP="10.249.6.203:29282" apf_pl="global-default" apf_fs="global-default" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.756783ms" resp=404 | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-03 20:59:59 | I0203 15:29:59.049271Â Â Â Â Â Â Â 1 httplog.go:132] "HTTP" verb="GET" URI="/apis/apps/v1/namespaces/genctl/deployments/acadia-image-eviction-job-28976255-cc47j" latency="1.726691ms" userAgent="kubectl/v1.25.14 (linux/amd64) kubernetes/a5967a3" audit-ID="71387662-af3e-4f3a-93c9-fbe64bfde903" srcIP="10.249.6.203:25434" apf_pl="global-default" apf_fs="global-default" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.464775ms" resp=404 | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 
2025-02-04 23:26:22 | I0204 17:56:21.752231Â Â Â Â Â Â Â 1 httplog.go:132] "HTTP" verb="GET" URI="/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j" latency="2.0842ms" userAgent="kubectl/v1.25.14 (linux/amd64) kubernetes/a5967a3" audit-ID="039faa17-d98d-4324-b5e9-48f9bff8ffae" srcIP="10.249.6.203:13196" apf_pl="global-default" apf_fs="global-default" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.862127ms" resp=404 | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â  | Â 


```

#### What did you expect to happen?

After the Pod has been deleted by garbage collector, K8s should not make any GET call on that Pod

#### How can we reproduce it (as minimally and precisely as possible)?

This issue is not easily reproducible, but it tends to occur approximately every two months.
If possible, please consider the following steps to investigate further:
Create a CronJob with the ```ttlSecondsAfterFinished: 86400``` setting.
Observe how and when the Kubernetes garbage collector deletes the corresponding Pod, Job, and CronJob resources.
Monitor whether any GET requests are being made to the specific Pod during or after its lifecycle.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# v1.29.9
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»æä¾›çš„ Issue æ ‡é¢˜å’Œå†…å®¹æ¥çœ‹ï¼Œæè¿°çš„ç°è±¡æ˜¯ Kubernetes åœ¨åƒåœ¾å›æ”¶ï¼ˆCronJob ä¸å…¶å¯¹åº”çš„ Podã€Job å‡è¢«æˆåŠŸåˆ é™¤ï¼‰åï¼Œä»åœ¨å¯¹å·²åˆ é™¤çš„ Pod æˆ–è€…ä¸è¯¥ Pod åŒåçš„ Deployment å‘èµ· GET è¯·æ±‚ã€‚  
è¯¥é—®é¢˜æ›´åƒæ˜¯ Kubernetes å†…éƒ¨å›æ”¶é€»è¾‘æˆ–è°ƒåº¦è¿‡ç¨‹ä¸­çš„â€œå»¶è¿Ÿâ€æˆ–â€œå¤šä½™è¯·æ±‚â€è¡Œä¸ºï¼ŒIssue æœ¬èº«å¹¶æœªå±•ç°å‡ºå¯è¢«å¤–éƒ¨æ”»å‡»è€…åˆ©ç”¨çš„æ¼æ´æˆ–å®‰å…¨éšæ‚£ï¼Œä¹Ÿæ²¡æœ‰æåŠä»»ä½•å¯å¯¼è‡´å‘½ä»¤æ‰§è¡Œã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€ææƒã€å®¹å™¨é€ƒé€¸ç­‰é«˜å±å®‰å…¨é£é™©çš„åœºæ™¯ã€‚ä»æ—¥å¿—å†…å®¹çœ‹ï¼Œä¹Ÿæœªå‘ç°ç›´æ¥æš´éœ²å¯è¢«æ”»å‡»æ–¹åˆ©ç”¨çš„æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚æœ‰æ•ˆèº«ä»½å‡­è¯ï¼‰ï¼Œæ•…æ— æ³•åˆ¤å®šä¸ºå®‰å…¨æ¼æ´ã€‚

æ­¤å¤–ï¼Œæ ¹æ®ç¬¬ 4 æ¡åˆ¤æ–­æ ‡å‡†ï¼Œâ€œissueæäº¤è€…åœ¨æäº¤å†…å®¹ä¸­æš´éœ²çš„æ•æ„Ÿä¿¡æ¯ã€ä¸å½“æ“ä½œã€ä¸å½“é…ç½®ç­‰é—®é¢˜ï¼Œä¸å±äºé¡¹ç›®æœ¬èº«å®‰å…¨é£é™©â€ã€‚æ‰€ä»¥ç»¼åˆæ¥çœ‹æœ¬ Issue ä¸æ¶‰åŠå®‰å…¨å¨èƒã€‚

**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Issue ä¸­æ²¡æœ‰æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œä¹Ÿæ²¡æœ‰å¯åˆ©ç”¨çš„æ¼æ´ç‚¹ï¼Œæ— éœ€æä¾›å¤ç°è„šæœ¬ã€‚è¯¥é—®é¢˜æ›´å¯èƒ½æ˜¯ Kubernetes å†…éƒ¨ç»„ä»¶åœ¨èµ„æºæ¸…ç†åçš„å‰©ä½™è®¿é—®æˆ–äº‹ä»¶è§¦å‘æ‰€å¯¼è‡´ï¼Œä¸æ„æˆå®‰å…¨é£é™©ã€‚

---


## Issue #131284 Garbage collector deletes dependents of StatefulSet, when user repeats the recreation of StatefulSet quickly

- Issue é“¾æ¥ï¼š[#131284](https://github.com/kubernetes/kubernetes/issues/131284)

### Issue å†…å®¹

#### What happened?

When I performed the following operations rapidly, the garbage collector occasionally deletes the dependents (Pods or ControllerRevisions) of the StatefulSet. 

1. Delete a StatefulSet using the `orphan` propagation policy.
2. Recreate the StatefulSet with modifications to the volumeClaimTemplates.

#### What did you expect to happen?

The dependents of the StatefulSet should not be deleted.
The garbage collector should respect the propagation policy `orphan`.

#### How can we reproduce it (as minimally and precisely as possible)?

(1) Create a StatefulSet.  
(2) Wait for the pods to become ready.  
(3) Delete the StatefulSet using the `orphan` propagation policy.  
(4) Recreate the StatefulSet with changes to the volumeClaimTemplate. (e.g, add labels to the PVC)  
(5) Delete the StatefulSet again using the `orphan` propagation policy.  
(6) Recreate the StatefulSet as defined in the first step.  

In my environment, when the interval between steps (4) and (5) is only a few milliseconds to tens of milliseconds, this bug occurs.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.5
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.5
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®Issueçš„æè¿°ï¼Œè¿™æ˜¯ä¸€ä¸ªå…³äºStatefulSetåœ¨çŸ­æ—¶é—´å†…å¿«é€Ÿåˆ é™¤å¹¶é‡æ–°åˆ›å»ºæ—¶çš„åƒåœ¾å›æ”¶æœºåˆ¶é—®é¢˜ï¼Œå¯¼è‡´å…¶ä»å±èµ„æºï¼ˆPodæˆ–ControllerRevisionï¼‰è¢«é”™è¯¯åœ°åˆ é™¤ã€‚è¯¥é—®é¢˜æ›´å¤šä½“ç°ä¸ºK8sæ§åˆ¶å™¨åœ¨é«˜é¢‘æ“ä½œä¸‹çš„åŠŸèƒ½æ€§æˆ–å¯é æ€§é—®é¢˜ï¼Œå¹¶ä¸å±äºå®‰å…¨é—®é¢˜çš„èŒƒç•´ã€‚  
1. æ²¡æœ‰è¿¹è±¡è¡¨æ˜è¯¥é—®é¢˜å¯è¢«ä½æƒé™æˆ–æœªæˆæƒç”¨æˆ·åˆ©ç”¨æ¥è¿›è¡Œè¶Šæƒæ“ä½œã€å®¹å™¨é€ƒé€¸æˆ–ææƒã€‚  
2. å³ä½¿èƒ½å¤Ÿè§¦å‘åˆ é™¤Podä¸ControllerRevisionæ“ä½œï¼Œå‰æä»æ˜¯éœ€è¦å…·å¤‡ç®¡ç†StatefulSetçš„è¾ƒé«˜æƒé™ï¼ˆå¦‚åˆ é™¤ã€åˆ›å»ºStatefulSetçš„æƒé™ï¼‰ã€‚åœ¨å¸¸è§„RBACç­–ç•¥ä¸‹ï¼Œè¿™é€šå¸¸æ˜¯Clusterç®¡ç†å‘˜æˆ–DevOpså·¥ç¨‹å¸ˆæ‰æ‹¥æœ‰çš„æƒé™ï¼Œå› æ­¤ä¸å±äºå®‰å…¨æ¼æ´ä¸­å…¸å‹çš„é«˜é£é™©èŒƒç•´ã€‚  
3. è¯¥é—®é¢˜å¹¶ä¸æ¶‰åŠæ—¥å¿—ã€å‡­æ®æˆ–æ•æ„Ÿä¿¡æ¯æ³„éœ²ï¼Œä¹Ÿä¸æ¶‰åŠæ½œåœ¨çš„å‘½ä»¤æ‰§è¡Œæˆ–ææƒé£é™©ï¼Œæ›´ä¸è·¨ç”¨æˆ·ç¯å¢ƒéš”ç¦»ç ´åæ— å…³ã€‚  

ç»¼åˆåˆ†æï¼Œè¯¥Issueå¹¶ä¸æ¶‰åŠå®‰å…¨å¨èƒæˆ–é£é™©ï¼Œæ•…è®¤å®šä¸ºâ€œä¸æ¶‰åŠâ€å®‰å…¨é£é™©ã€‚

---


## Issue #131275 kubelet tries to remove pod multiple times(reopen)

- Issue é“¾æ¥ï¼š[#131275](https://github.com/kubernetes/kubernetes/issues/131275)

### Issue å†…å®¹

#### What happened?

When watching pod deletion, there will be a short error status
![Image](https://github.com/user-attachments/assets/9e876ebd-e0d1-4fbd-83ce-3e9db5bae176)

kubelet log, show have delete container twice
![Image](https://github.com/user-attachments/assets/8cd57045-aecd-4f6a-8107-523061bb0885)

containerd log, first delete success, but have another delete request, return not found
![Image](https://github.com/user-attachments/assets/ca5d7e04-2ec7-4d38-87fa-3dfed1f0d18a)

There is a very old issue, but the problem seems to have not been solved https://github.com/kubernetes/kubernetes/issues/99901

#### What did you expect to happen?

No errors

#### How can we reproduce it (as minimally and precisely as possible)?

create cluster 1.32.3 with kubeadm
add any pod
delete it

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here

root@n305:~# containerd -v
containerd containerd.io 1.7.27 05044ec0a9a75232cad458027ca83437aae3f4da
root@n305:~# runc -v
runc version 1.2.5
commit: v1.2.5-0-g59923ef
spec: 1.2.0
go: go1.23.7
libseccomp: 2.5.1
root@n305:~# kubectl version
Client Version: v1.32.3
Kustomize Version: v5.5.0
Server Version: v1.32.3
```
</details>



#### Cloud provider

<details>
not cloud provider
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

root@n305:~# uname -a
Linux n305.ifooth.com 6.2.11-2-pve #1 SMP PREEMPT_DYNAMIC PVE 6.2.11-2 (2023-05-10T09:13Z) x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
flannel

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»å½“å‰çš„ Issue æè¿°æ¥çœ‹ï¼Œkubelet åˆ é™¤åŒä¸€ä¸ªå®¹å™¨æ—¶ä¼šé‡å¤å‘é€åˆ é™¤è¯·æ±‚ï¼Œç¬¬äºŒæ¬¡åˆ é™¤è¯·æ±‚è¿”å›â€œNot Foundâ€é”™è¯¯ã€‚è¯¥é—®é¢˜çœ‹èµ·æ¥æ›´åƒæ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§æˆ–é€»è¾‘æ€§ç¼ºé™·ï¼Œå¹¶æœªæ¶‰åŠæ•æ„Ÿä¿¡æ¯æ³„éœ²ã€å‘½ä»¤æ³¨å…¥ã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰å®‰å…¨é£é™©ï¼Œä¹Ÿæœªè¯´æ˜å¯ä»¥åœ¨å¤šç”¨æˆ·åœºæ™¯ä¸‹è¢«æ¶æ„åˆ©ç”¨é€ æˆæ›´ä¸ºä¸¥é‡çš„å®‰å…¨å½±å“ã€‚å› æ­¤ï¼Œå¯ä»¥åˆ¤æ–­æœ¬ Issue ä¸å®‰å…¨é£é™©æ— å…³ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥é—®é¢˜ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ— éœ€å®‰å…¨å¤ç°è„šæœ¬ï¼Œä»¥ä¸‹ä»…ä¸ºç©ºå ä½ç¤ºä¾‹
def main():
    print("è¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œä¸éœ€è¦æä¾›å®‰å…¨æ¼æ´å¤ç°è„šæœ¬ã€‚")
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Issue å¹¶éå®‰å…¨æ¼æ´ï¼Œè€Œæ˜¯ä¸€ä¸ªæ­£å¸¸çš„åŠŸèƒ½ç¼ºé™·æˆ–æ—¥å¿—å†—ä½™é—®é¢˜ï¼Œä¸ä¼šé€ æˆæ•æ„Ÿä¿¡æ¯æ³„éœ²ã€æƒé™æå‡ã€å®¹å™¨é€ƒé€¸ç­‰å®‰å…¨åæœï¼Œå› æ­¤ä¸éœ€è¦æä¾›å®‰å…¨æ¼æ´å¤ç°è„šæœ¬ã€‚ä¸Šè¿°ç¤ºä¾‹è„šæœ¬ä»…ç”¨äºç¬¦åˆå›ç­”çš„æ ¼å¼è¦æ±‚ï¼Œä¸åŒ…å«ä»»ä½•æ¼æ´å¤ç°é€»è¾‘ã€‚

---


## Issue #131270 Headless service with selector is not reachable from within the cluster.

- Issue é“¾æ¥ï¼š[#131270](https://github.com/kubernetes/kubernetes/issues/131270)

### Issue å†…å®¹

#### What happened?

I have the following Headless service with selector:
```
Name:              svc-chroma
Namespace:         default
Labels:            app=chroma
                   component=vector-db
Annotations:       <none>
Selector:          app=chroma,component=vector-db
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                None
IPs:               None
Port:              http  80/TCP
TargetPort:        http/TCP
Endpoints:         10.1.207.210:8000
Session Affinity:  None
Events:            <none>
```
The pod (`replicas: 1`):
```
Name:             chroma-0
Namespace:        default
Priority:         0
Service Account:  default
Node:             khteh-p17-2i/192.168.0.149
Start Time:       Sat, 12 Apr 2025 10:57:01 +0800
Labels:           app=chroma
                  component=vector-db
                  controller-revision-hash=chroma-7dfcdc95dd
                  statefulset.kubernetes.io/pod-name=chroma-0
Annotations:      cni.projectcalico.org/containerID: 382c18c085f2995a9a03fbd4feecceaafba2dd78d4f6542ece48d8e44b819b41
                  cni.projectcalico.org/podIP: 10.1.207.210/32
                  cni.projectcalico.org/podIPs: 10.1.207.210/32
Status:           Running
IP:               10.1.207.210
IPs:
  IP:           10.1.207.210
Controlled By:  StatefulSet/chroma
<snip>
```

The Service is not reachable from within the cluster with explicit port:
```
root@ubuntu:/# curl -v http://svc-chroma
* Host svc-chroma:80 was resolved.
* IPv6: (none)
* IPv4: 10.1.207.210
*   Trying 10.1.207.210:80...
* connect to 10.1.207.210 port 80 from 10.1.207.218 port 38694 failed: Connection refused
* Failed to connect to svc-chroma port 80 after 0 ms: Could not connect to server
* closing connection #0
curl: (7) Failed to connect to svc-chroma port 80 after 0 ms: Could not connect to server
root@ubuntu:/# curl -v http://svc-chroma:8000 
* Host svc-chroma:8000 was resolved.
* IPv6: (none)
* IPv4: 10.1.207.210
*   Trying 10.1.207.210:8000...
* Connected to svc-chroma (10.1.207.210) port 8000
> GET / HTTP/1.1
> Host: svc-chroma:8000
> User-Agent: curl/8.9.1
> Accept: */*
> 
* Request completely sent off
< HTTP/1.1 404 Not Found
< chroma-trace-id: 00000000000000000000000000000000
< content-length: 0
< date: Sat, 12 Apr 2025 03:26:24 GMT
< 
* Connection #0 to host svc-chroma left intact
```
https://kubernetes.io/docs/concepts/services-networking/service/#headless-services

#### What did you expect to happen?

Headless Service with selector is reachable from within the cluster without explicit port in the URI

#### How can we reproduce it (as minimally and precisely as possible)?

Define a headless service with selector and `TargetPort` different from the service port.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
clientVersion:
  buildDate: "2024-10-16T15:15:29Z"
  compiler: gc
  gitCommit: cbb86e0d7f4a049666fac0551e8b02ef3d6c3d9a
  gitTreeState: clean
  gitVersion: v1.27.16
  goVersion: go1.22.5
  major: "1"
  minor: "27"
  platform: linux/amd64
kustomizeVersion: v5.0.1
serverVersion:
  buildDate: "2024-10-16T15:16:32Z"
  compiler: gc
  gitCommit: cbb86e0d7f4a049666fac0551e8b02ef3d6c3d9a
  gitTreeState: clean
  gitVersion: v1.27.16
  goVersion: go1.22.5
  major: "1"
  minor: "27"
  platform: linux/amd64
```

</details>


#### Cloud provider

<details>
Local microk8s
</details>


#### OS version

<details>

```console
$ uname -a
Linux khteh-p17-2i 6.11.0-21-generic #21-Ubuntu SMP PREEMPT_DYNAMIC Wed Feb 19 16:50:40 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
$ lsb_release -a
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 24.10
Release:	24.10
Codename:	oracular
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»è¯¥ issue çš„æè¿°æ¥çœ‹ï¼Œé—®é¢˜é›†ä¸­åœ¨ Kubernetes Headless Service çš„ç½‘ç»œé…ç½®å’Œç«¯å£æ˜ å°„ä½¿ç”¨ä¸Šã€‚ç”¨æˆ·åœ¨è®¿é—®æ—¶å¦‚æœçœç•¥äº†ç«¯å£ï¼Œå°±æ— æ³•æ­£å¸¸è®¿é—®ï¼›å½“æ˜¾å¼æŒ‡å®š TargetPortï¼ˆå³ 8000ï¼‰åèƒ½å¤Ÿè®¿é—®ã€‚è¿™å±äº Kubernetes Service é…ç½®å’Œä½¿ç”¨æ–¹å¼çš„å¸¸è§„å·®å¼‚æ‰€å¯¼è‡´çš„è®¿é—®è¡Œä¸ºï¼Œä¸å®‰å…¨æ€§æ¼æ´æ— å…³ã€‚  
1. è¯¥é—®é¢˜å¹¶æœªæ¶‰åŠæ•æ„Ÿä¿¡æ¯çš„æ³„éœ²ã€æœªå¯¼è‡´å‘½ä»¤æˆ–ä»£ç æ‰§è¡Œã€ææƒç­‰å®‰å…¨é£é™©ã€‚  
2. ä¹Ÿæœªé€ æˆæ•°æ®è¶Šæƒè®¿é—®æˆ–å¯åˆ©ç”¨çš„æ”»å‡»é¢ï¼Œæ›´å¤šæ˜¯ Kubernetes ä¸­ Headless Service çš„å…¸å‹ä½¿ç”¨é—®é¢˜ï¼ˆä½¿ç”¨ ClusterIP=None åŠ Service Port ä¸ TargetPort ä¸ä¸€è‡´æ—¶éœ€è¦æ˜ç¡®æŒ‡å®šç«¯å£ï¼‰ã€‚  
3. å› æ­¤ï¼Œä¸å±äºå®‰å…¨æ¼æ´èŒƒç•´ï¼ŒæŒ‰ç…§é£é™©åˆ¤æ–­æ ‡å‡†åº”å½’ä¸ºâ€œä¸æ¶‰åŠâ€å®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥é—®é¢˜ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œå› æ­¤æ— éœ€å¤ç°è„šæœ¬
```


---


## Issue #131257 Pause image 3.10.1 not available in staging, production

- Issue é“¾æ¥ï¼š[#131257](https://github.com/kubernetes/kubernetes/issues/131257)

### Issue å†…å®¹

#### What happened?

Pause image was bumped as a part of PR: https://github.com/kubernetes/kubernetes/pull/130102
But the build process of windows pause failed due to `tag@digest`: https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/post-kubernetes-push-image-pause/1889748429312102400
Dropping the `digest` should work as a stop-gap.

Ref conversation: https://kubernetes.slack.com/archives/CCK68P2Q2/p1742839354591189?thread_ts=1742832454.449449&cid=CCK68P2Q2

#### What did you expect to happen?

Pause image to get built and promoted as expected so that it can be updated in k/k src to use it.

#### How can we reproduce it (as minimally and precisely as possible)?

```
arkas1@arkas1-ubuntu-vm:~/kubernetes/build/pause$ docker buildx build --provenance=false --sbom=false --pull --output=type=registry --platform windows/amd64 \
        -t gcr.io/k8s-staging-kubernetes/pause:3.10.1-windows-amd64-1809 --build-arg BASE=mcr.microsoft.com/oss/kubernetes/windows-pause-image-base:v0.4.1@sha256:37cc10768383b55611d724a05eb18564cb5184c89b0c2faa7d4eff63475092df-windows-1809-amd64 --build-arg ARCH=amd64 -f Dockerfile_windows .
[+] Building 0.1s (1/1) FINISHED                                                                                                                                                            docker-container:k-release-multiarch
 => [internal] load build definition from Dockerfile_windows                                                                                                                                                                0.0s
 => => transferring dockerfile: 795B                                                                                                                                                                                        0.0s
 => WARN: InvalidDefaultArgInFrom: Default value for ARG ${BASE} results in empty or invalid base image name (line 16)                                                                                                      0.0s

 1 warning found (use docker --debug to expand):
 - InvalidDefaultArgInFrom: Default value for ARG ${BASE} results in empty or invalid base image name (line 16)
Dockerfile_windows:16
--------------------
  14 |     
  15 |     ARG BASE
  16 | >>> FROM ${BASE}
  17 |     ARG ARCH
  18 |     ADD bin/pause-windows-${ARCH}.exe /pause.exe
--------------------
ERROR: failed to solve: failed to parse stage name "mcr.microsoft.com/oss/kubernetes/windows-pause-image-base:v0.4.1@sha256:37cc10768383b55611d724a05eb18564cb5184c89b0c2faa7d4eff63475092df-windows-1809-amd64": invalid reference format
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

Branches having the improper `tag@digest` : 

- `master`
- `release-1.33`


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»Issueæè¿°æ¥çœ‹ï¼Œè¯¥é—®é¢˜ä¸»è¦ä¸Windowså¹³å°çš„Pauseé•œåƒæ„å»ºå¤±è´¥æœ‰å…³ï¼ŒåŸå› æ˜¯é•œåƒåç§°ä¸­ä½¿ç”¨äº†â€œtag@digestâ€çš„æ ¼å¼å¯¼è‡´æ— æ•ˆå¼•ç”¨ã€‚è¿™å±äºæ„å»ºå’Œå‘å¸ƒæµç¨‹ä¸­çš„æŠ€æœ¯é—®é¢˜ï¼Œå¹¶æœªæåŠä»»ä½•å¯èƒ½å¯¼è‡´æƒé™æå‡ã€å‡­æ®æ³„éœ²æˆ–å…¶ä»–èƒ½å¤Ÿè¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨é£é™©ç‚¹ï¼Œä¹Ÿæ²¡æœ‰æš´éœ²é¡¹ç›®æ ¸å¿ƒé…ç½®æˆ–æ•æ„Ÿä¿¡æ¯ã€‚å› æ­¤ï¼Œè¯¥é—®é¢˜ä¸æ¶‰åŠå®‰å…¨æ¼æ´æˆ–å®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥Issueä¸ºæ„å»ºé…ç½®é—®é¢˜ï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œå› æ­¤æ— éœ€ç¼–å†™å¤ç°è„šæœ¬ã€‚
# ä»¥ä¸‹ä»…æ˜¯ç¬¦åˆå›ç­”æ ¼å¼çš„ç©ºè„šæœ¬ï¼Œä¸æ‰§è¡Œä»»ä½•æ“ä½œã€‚

def main():
    print("æ­¤é—®é¢˜ä¸ºæ„å»ºæµç¨‹é—®é¢˜ï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ— éœ€POCã€‚")

main()
```


---


## Issue #131253 Kubelet failed to start on reboot with memory manager's `Static` policy

- Issue é“¾æ¥ï¼š[#131253](https://github.com/kubernetes/kubernetes/issues/131253)

### Issue å†…å®¹

#### What happened?

Memory manager's `Static` policy will cause kubelet fail to start on reboot, when total memory of each numa node changes after each reboot, while they still sum up to a fixed number, just as https://github.com/kubernetes/kubernetes/issues/109996 describes.
 
Though Kubernetes v1.32 [improves memory manager reliability and consistency](https://kubernetes.io/blog/2024/12/13/memory-manager-goes-ga/#improving-memory-manager-reliability-and-consistency) with https://github.com/kubernetes/kubernetes/pull/127074 by @Tal-or, the issue is still not fixed because it still checks whether numa node's total memory is unchanged:

https://github.com/kubernetes/kubernetes/blob/d64f34eb2cf57ca017368bd94dc577952c2647d9/pkg/kubelet/cm/memorymanager/policy_static.go#L710-L714

Thus following error happened in kubelet on reboot:
<details>

```
Apr 11 09:15:09 master0 kubelet[13019]: E0411 09:15:09.451647   13019 policy_static.go:718] "Memory states for the NUMA node and resource are different" node=2 resource="memory" field="TotalMemSize" TotalMemSize1=135233794048 TotalMemSize2=135277490176 memoryState1={"total":135233794048,"systemReserved":4429185024,"allocatable":130804609024,"reserved":0,"free":130804609024} memoryState2={"total":135277490176,"systemReserved":4429185024,"allocatable":130848305152,"reserved":0,"free":130848305152}
Apr 11 09:15:09 master0 kubelet[13019]: E0411 09:15:09.451671   13019 policy_static.go:90] "Invalid state, please drain node and remove policy state file" err="[memorymanager] the expected machine state is different from the real one"
Apr 11 09:15:09 master0 kubelet[13019]: E0411 09:15:09.451680   13019 memory_manager.go:202] "Policy start error" err="[memorymanager] the expected machine state is different from the real one"
Apr 11 09:15:09 master0 kubelet[13019]: E0411 09:15:09.451690   13019 kubelet.go:1643] "Failed to start ContainerManager" err="start memory manager error: [memorymanager] the expected machine state is different from the real one"
Apr 11 09:15:09 master0 systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
```

</details>

1. Memory state json before reboot:
<details>

```json
{
  "policyName": "Static",
  "machineState": {
    "0": {
      "numberOfAssignments": 0,
      "memoryMap": {
        "hugepages-1Gi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "hugepages-2Mi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "memory": {
          "total": 134607716352,
          "systemReserved": 4429185024,
          "allocatable": 130178531328,
          "reserved": 0,
          "free": 130178531328
        }
      },
      "cells": [
        0
      ]
    },
    "1": {
      "numberOfAssignments": 0,
      "memoryMap": {
        "hugepages-1Gi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "hugepages-2Mi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "memory": {
          "total": 135277490176,
          "systemReserved": 4429185024,
          "allocatable": 130848305152,
          "reserved": 0,
          "free": 130848305152
        }
      },
      "cells": [
        1
      ]
    },
    "2": {
      "numberOfAssignments": 0,
      "memoryMap": {
        "hugepages-1Gi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "hugepages-2Mi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "memory": {
          "total": 135233794048,
          "systemReserved": 4429185024,
          "allocatable": 130804609024,
          "reserved": 0,
          "free": 130804609024
        }
      },
      "cells": [
        2
      ]
    },
    "3": {
      "numberOfAssignments": 0,
      "memoryMap": {
        "hugepages-1Gi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "hugepages-2Mi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "memory": {
          "total": 134193364992,
          "systemReserved": 4429185024,
          "allocatable": 129764179968,
          "reserved": 0,
          "free": 129764179968
        }
      },
      "cells": [
        3
      ]
    }
  },
  "checksum": 1040760267
}
```

</details>

2. Memory state json after reboot:
<details>

```json
{
  "policyName": "Static",
  "machineState": {
    "0": {
      "numberOfAssignments": 0,
      "memoryMap": {
        "hugepages-1Gi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "hugepages-2Mi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "memory": {
          "total": 134607716352,
          "systemReserved": 4429185024,
          "allocatable": 130178531328,
          "reserved": 0,
          "free": 130178531328
        }
      },
      "cells": [
        0
      ]
    },
    "1": {
      "numberOfAssignments": 0,
      "memoryMap": {
        "hugepages-1Gi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "hugepages-2Mi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "memory": {
          "total": 135277490176,
          "systemReserved": 4429185024,
          "allocatable": 130848305152,
          "reserved": 0,
          "free": 130848305152
        }
      },
      "cells": [
        1
      ]
    },
    "2": {
      "numberOfAssignments": 0,
      "memoryMap": {
        "hugepages-1Gi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "hugepages-2Mi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "memory": {
          "total": 135277490176,
          "systemReserved": 4429185024,
          "allocatable": 130848305152,
          "reserved": 0,
          "free": 130848305152
        }
      },
      "cells": [
        2
      ]
    },
    "3": {
      "numberOfAssignments": 0,
      "memoryMap": {
        "hugepages-1Gi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "hugepages-2Mi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "memory": {
          "total": 134149668864,
          "systemReserved": 4429185024,
          "allocatable": 129720483840,
          "reserved": 0,
          "free": 129720483840
        }
      },
      "cells": [
        3
      ]
    }
  },
  "checksum": 3935320266
}
```

</details>

We can see total memory of node 0, 1 is unchanged, but node 2, 3 changed while they sum up to a same count.

#### What did you expect to happen?

Improve the implementation of `areMemoryStatesEqual`, to allow for numa node total memory change as long as the total summed up memory is consistent (and the current memory state can serve allocated pods?).

#### How can we reproduce it (as minimally and precisely as possible)?

1. Prepare a multi numa-node machine where memory of each numa node varies on reboot.
2. Start kubernetes cluster with kubelet memory manager policy to be `Static`.
3. Reboot to see the failure of kubelet.

#### Anything else we need to know?

https://github.com/kubernetes/kubernetes/issues/109996#issuecomment-1145929818 says issue described there would be classified as a feature request instead of bug. But I think this is a bug given that Kubernetes v1.32 is [improving memory manager reliability and consistency](https://kubernetes.io/blog/2024/12/13/memory-manager-goes-ga/#improving-memory-manager-reliability-and-consistency):

> The kubelet does not guarantee pod ordering when admitting pods after a restart or reboot.
>
> In certain edge cases, this behavior could cause the memory manager to reject some pods, and in more extreme cases, it may cause kubelet to fail upon restart.
> 
> Previously, the beta implementation lacked certain checks and logic to prevent these issues.
> 
> To stabilize the memory manager for general availability (GA) readiness, small but critical refinements have been made to the algorithm, improving its robustness and handling of edge cases.

And this issue is exactly the edge case that `cause kubelet to fail upon restart`, which should be handed to improve its robustness.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.3
Kustomize Version: v5.5.0
Server Version: v1.32.3
```

</details>


#### Cloud provider

Self-host

#### OS version

<details>

```console
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.2 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.2 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux gpu164-master0 5.15.0-84-generic #93-Ubuntu SMP Tue Sep 5 17:16:10 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

`kubeadm`, configuration as follows:
<details>

```
kind: InitConfiguration
apiVersion: kubeadm.k8s.io/v1beta3

nodeRegistration:
  name: master0
  criSocket: unix:///var/run/containerd/containerd.sock
  imagePullPolicy: IfNotPresent
  taints: [] # Set taints to empty list to avoid control-plane node from being tainted
  kubeletExtraArgs:
    resolv-conf: /run/systemd/resolve/resolv.conf
    system-reserved: cpu=100m,memory=100Mi
    kube-reserved: cpu=607.5m,memory=16696Mi
    cpu-manager-policy: static
    memory-manager-policy: Static
    reserved-memory: 0:memory=4224Mi;1:memory=4224Mi;2:memory=4224Mi;3:memory=4224Mi

localAPIEndpoint:
  advertiseAddress: 10.194.3.15
  bindPort: 6443

bootstrapTokens:
  - token: xvq0og.vyaim43ag7mw45r9

---
kind: ClusterConfiguration
apiVersion: kubeadm.k8s.io/v1beta3

kubernetesVersion: 1.32.3
clusterName: kubernetes
controlPlaneEndpoint: 10.194.3.14

imageRepository: registry.k8s.io
certificatesDir: /etc/kubernetes/pki
networking:
  podSubnet: 10.240.0.0/12
  serviceSubnet: 10.96.0.0/12
  dnsDomain: cluster.local
apiServer:
  extraArgs:
    service-node-port-range: 10000-65530
    event-ttl: 24h0m0s
  extraVolumes: []
  certSANs: []
  timeoutForControlPlane: 4m0s
controllerManager:
  extraArgs:
    node-cidr-mask-size: "20"
  extraVolumes: []
scheduler:
  extraArgs: {}
  extraVolumes: []
dns: {}
etcd:
  local:
    dataDir: /var/lib/etcd
    extraArgs: {}
    serverCertSANs: []
    peerCertSANs: []

---
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1

cgroupDriver: systemd

clusterDomain: cluster.local
clusterDNS: [10.96.0.10]

featureGates:
  CPUManager: true
  MemoryManager: true
```

</details>


#### Container runtime (CRI) and version (if applicable)




#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»æè¿°æ¥çœ‹ï¼Œè¯¥ Issue ä¸»è¦æ˜¯ç”±äºåœ¨å¤š NUMA èŠ‚ç‚¹åœºæ™¯ä¸‹ï¼Œé‡å¯åå„èŠ‚ç‚¹çš„å¯ç”¨å†…å­˜æ€»é‡å¯èƒ½å‘ç”Ÿå˜åŒ–ï¼Œå¯¼è‡´ kubelet ä½¿ç”¨å†…å­˜ç®¡ç†ï¼ˆMemoryManagerï¼‰çš„ Static ç­–ç•¥æ—¶ï¼Œæ£€æŸ¥åˆ°â€œæœŸæœ›å†…å­˜çŠ¶æ€â€å’Œâ€œå®é™…å†…å­˜çŠ¶æ€â€ä¸ä¸€è‡´ï¼Œä»è€Œæ— æ³•æ­£å¸¸å¯åŠ¨ã€‚è¿™ç±»é—®é¢˜ä¸ä¼šé€ æˆä¿¡æ¯æ³„éœ²ã€æƒé™æå‡ï¼Œä¹Ÿä¸æ¶‰åŠè¶Šæƒæˆ–ä¾§ä¿¡é“æ”»å‡»ç­‰å®‰å…¨é£é™©ï¼Œä»…ä»…æ˜¯ kubelet å¯åŠ¨è¿‡ç¨‹ä¸­åŠŸèƒ½æ€§é€»è¾‘çš„ä¸è¶³ã€‚  
å³ä½¿å°†å…¶çº³å…¥æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰çš„èŒƒç•´ï¼Œä¹Ÿéœ€è¦å…·æœ‰ç›¸å½“é«˜çš„æƒé™ï¼ˆå¯ä»¥ä¿®æ”¹NUMAèŠ‚ç‚¹å†…å­˜åˆ†é…ç­–ç•¥æˆ–è®¿é—®æœºå™¨å¯åŠ¨é…ç½®ï¼‰ï¼Œå¹¶æ²¡æœ‰è¯´æ˜æ­¤é—®é¢˜å¯è¢«ä½æƒé™æ”»å‡»è€…åˆ©ç”¨ï¼Œå› æ­¤ä¸æ„æˆå…¸å‹çš„å®‰å…¨æ¼æ´ã€‚ç»¼åˆåˆ¤æ–­ï¼Œè¯¥ Issue ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

è¯¥é—®é¢˜å±äºå†…å­˜ç®¡ç†ç­–ç•¥çš„åŠŸèƒ½æ€§ç¼ºé™·ï¼Œä¸å±äºå®‰å…¨æ¼æ´ï¼Œå› æ­¤æ— éœ€æä¾›å¤ç°è„šæœ¬ã€‚è‹¥åœ¨å®é™…ç”Ÿäº§ç¯å¢ƒä¸­é­é‡ï¼Œå¯é€šè¿‡ä¿®æ­£ `areMemoryStatesEqual` å‡½æ•°æˆ–æš‚æ—¶ç¦ç”¨ Static ç­–ç•¥ç­‰æ–¹å¼ç¼“è§£æˆ–è§£å†³ã€‚

---


## Issue #131244 Go modules are not tagged for v0.33.0-rc.0

- Issue é“¾æ¥ï¼š[#131244](https://github.com/kubernetes/kubernetes/issues/131244)

### Issue å†…å®¹

#### What happened?

See e.g. https://pkg.go.dev/k8s.io/api?tab=versions

#### What did you expect to happen?

v0.33.0-rc.0 tag to show up

#### How can we reproduce it (as minimally and precisely as possible)?

Look into https://github.com/kubernetes/api/tags

#### Anything else we need to know?

Previous issue #116912


#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue ä»…æè¿°äº† Go æ¨¡å—åœ¨æŸä¸ªç‰ˆæœ¬ä¸Šç¼ºå°‘ç›¸åº” tag çš„é—®é¢˜ï¼Œä¸»è¦å±äºç‰ˆæœ¬ç®¡ç†æˆ–å‘å¸ƒæµç¨‹æ–¹é¢çš„äº‹é¡¹ï¼Œå¹¶æœªæåŠä»»ä½•å¯èƒ½å¯¼è‡´ä»£ç æ³„éœ²ã€æ¼æ´åˆ©ç”¨ã€æƒé™æå‡æˆ–å…¶ä»–å®‰å…¨æ–¹é¢çš„é—®é¢˜ï¼Œæ•…ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥ Issue ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ— éœ€æä¾›å¤ç°è„šæœ¬
# ä¸‹æ–¹ä»…ç¤ºä¾‹è¯´æ˜å¦‚ä½•æ£€æŸ¥ä»“åº“æ ‡ç­¾ï¼Œä¸ä»£è¡¨å­˜åœ¨å®‰å…¨é£é™©

import subprocess
import sys
import time

def main():
    try:
        # å‡è®¾æœ¬åœ°å·²æœ‰ GitHub ç™»å½•ä¿¡æ¯ï¼Œç›´æ¥æ‰§è¡Œ git fetch å¹¶æŸ¥çœ‹æ ‡ç­¾
        subprocess.run(["git", "clone", "https://github.com/kubernetes/api.git", "kubernetes_api_repo"], check=True)
        time.sleep(1)  # ç­‰å¾…ç‰‡åˆ»ï¼Œé¿å…ä»“åº“ä¿¡æ¯æœªä¸‹è½½å®Œå…¨
        subprocess.run(["git", "-C", "kubernetes_api_repo", "fetch", "--tags"], check=True)
        tag_list = subprocess.check_output(["git", "-C", "kubernetes_api_repo", "tag"], text=True)
        print("å½“å‰ä»“åº“æ ‡ç­¾:\n", tag_list)
    except subprocess.CalledProcessError as e:
        print("æ‰§è¡Œ Git å‘½ä»¤å¤±è´¥:", e)
        sys.exit(1)

    # è„šæœ¬æ‰§è¡Œå®Œæˆ
    print("Tag æ£€æŸ¥ç»“æŸã€‚")

main()
```


---


## Issue #131241 kubectl drain no longer works with node accounts in Kubernetes 1.32

- Issue é“¾æ¥ï¼š[#131241](https://github.com/kubernetes/kubernetes/issues/131241)

### Issue å†…å®¹

#### What happened?

Since Kubernetes 1.32, the AuthorizeNodeWithSelectors feature gate defaults to true. This leads to Nodes only being able to query Pods that are associated with that node. kubectl drain does not handle this gracefully; when run with node credentials, it will evict or terminate all pods, but then exit on a permission error as those pods, once successfully terminated or evicted, no longer have a relationship with that node and are thus not accessible to it via API.

#### What did you expect to happen?

kubectl drain finishes cleanly after all Pods have been evicted or terminated, as it does on 1.31 or with the AuthorizeNodeWithSelectors feature gate set to false.

#### How can we reproduce it (as minimally and precisely as possible)?

Run kubectl drain on a node running Kubernetes 1.32 with default feature gate settings, using the node's own account for authorization.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.3
Kustomize Version: v5.5.0
Server Version: v1.32.3
```

</details>


#### Cloud provider

none


#### OS version

<details>
Debian GNU/Linux 12 (bookworm)
</details>


#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)




### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»Issueæè¿°æ¥çœ‹ï¼Œè¯¥é—®é¢˜æ˜¯ç”±äºKubernetes 1.32é»˜è®¤å¼€å¯äº†AuthorizeNodeWithSelectorsåŠŸèƒ½åï¼ŒèŠ‚ç‚¹å‡­æ®åªèƒ½æŸ¥è¯¢ä¸è‡ªèº«èŠ‚ç‚¹å…³è”çš„Podï¼Œåœ¨æ‰§è¡Œkubectl drainæ—¶ï¼Œå½“Podè¢«é©±é€æˆ–åˆ é™¤åï¼Œä¸èŠ‚ç‚¹å¤±å»å…³è”ï¼Œå¯¼è‡´èŠ‚ç‚¹å‡­æ®æ— æ³•ç»§ç»­æŸ¥è¯¢åˆ°è¿™äº›Podè€Œäº§ç”Ÿæƒé™é”™è¯¯ã€‚æ­¤é—®é¢˜å±äºKuberneteså¯¹èŠ‚ç‚¹è®¿é—®Podæƒé™ç­–ç•¥æ”¹å˜æ‰€å¼•å‘çš„åŠŸèƒ½æ€§å…¼å®¹æˆ–è®¾è®¡é—®é¢˜ï¼Œå¹¶ä¸æ¶‰åŠä»»æ„è¶Šæƒã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€ææƒæˆ–å…¶ä»–å®‰å…¨æ”»å‡»åœºæ™¯ï¼Œå› æ­¤ä¸å±äºå®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# ç”±äºè¯¥Issueä¸å®‰å…¨æ¼æ´æ— å…³ï¼Œè¿™é‡Œä¸æä¾›å®‰å…¨PoCã€‚
# å¦‚æœä»…æƒ³é‡ç°kubectl drainçš„æƒé™é”™è¯¯ï¼Œå¯ä½¿ç”¨pythonçš„kubernetesåº“æ¨¡æ‹Ÿ drain æ“ä½œï¼Œ
# ä½†æ­¤å¤„å±•ç¤ºçš„è„šæœ¬ä»…æ¼”ç¤ºå¦‚ä½•ä¸Kubernetesäº¤äº’ï¼Œå¹¶ä¸ä¼šé€ æˆå®‰å…¨é£é™©çš„åˆ©ç”¨ã€‚
#
# æ³¨æ„ï¼šè¯¥è„šæœ¬ä»…ä¸ºåŠŸèƒ½æµ‹è¯•èŒƒä¾‹ï¼Œéœ€åœ¨æœ‰æƒé™è®¿é—®é›†ç¾¤çš„ç¯å¢ƒè¿è¡Œã€‚

import time
from kubernetes import client, config
from kubernetes.stream import stream

def main():
    # åŠ è½½æœ¬åœ°kubeconfig
    config.load_kube_config()

    # åˆ›å»ºCoreV1Apiå®¢æˆ·ç«¯
    v1 = client.CoreV1Api()

    # å‡è®¾è¦å¯¹æŸä¸ªnodeæ‰§è¡Œ drainï¼Œç¤ºä¾‹nodeåç§°è¯·æ ¹æ®å®é™…æƒ…å†µæ›¿æ¢
    node_name = "your-node-name"

    # 1. è·å–è¯¥nodeä¸Šè¿è¡Œçš„Podåˆ—è¡¨
    field_selector = f"spec.nodeName={node_name}"
    pods = v1.list_pod_for_all_namespaces(field_selector=field_selector).items

    print(f"Node {node_name} ä¸Šçš„Podsï¼š")
    for pod in pods:
        print(f"- {pod.metadata.namespace}/{pod.metadata.name}")
    
    # 2. æ¨¡æ‹Ÿå¯¹è¿™äº›Podæ‰§è¡Œé©±é€æˆ–åˆ é™¤æ“ä½œï¼ˆæ­¤å¤„ä»…ç®€å•è°ƒç”¨deleteï¼‰
    #   æ³¨æ„ï¼šçœŸå® drain éœ€è¦ä¾èµ–é«˜çº§é€»è¾‘å¤„ç†ï¼Œå¦‚daemonsetç­‰ç‰¹æ®ŠPodçš„è·³è¿‡ç­–ç•¥ï¼Œè¿™é‡Œä»…åšç®€å•æ¼”ç¤º
    for pod in pods:
        try:
            v1.delete_namespaced_pod(
                name=pod.metadata.name,
                namespace=pod.metadata.namespace
            )
            print(f"å·²åˆ é™¤Pod: {pod.metadata.namespace}/{pod.metadata.name}")
        except Exception as e:
            print(f"åˆ é™¤Podæ—¶å‡ºé”™: {e}")

    # 3. ç­‰å¾…ä¸€ä¸‹ï¼Œè®©Podå®Œæˆåˆ é™¤
    time.sleep(3)

    # 4. åˆ é™¤åå°è¯•å†æ¬¡æŸ¥è¯¢å·²åˆ é™¤çš„Podï¼Œä¼šå› ä¸ºAuthorizeNodeWithSelectorsè€Œå‡ºç°æ— æƒé™æˆ–æ‰¾ä¸åˆ°èµ„æºç­‰é”™è¯¯
    for pod in pods:
        try:
            _ = v1.read_namespaced_pod(pod.metadata.name, pod.metadata.namespace)
            print(f"ä»èƒ½è®¿é—®åˆ°Pod: {pod.metadata.namespace}/{pod.metadata.name}")
        except Exception as e:
            print(f"æ— æ³•è®¿é—®å·²åˆ é™¤çš„Pod(é¢„æœŸæƒé™é—®é¢˜æˆ–NotFound): {e}")

    print("è„šæœ¬æ‰§è¡Œå®Œæ¯•ï¼Œè‹¥å‡ºç°æƒé™é”™è¯¯æˆ–æ— æ³•æŸ¥è¯¢å·²åˆ é™¤Podå³ä¸Issueæè¿°ç°è±¡ç›¸ç¬¦ã€‚")

main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. è¯¥è„šæœ¬é€šè¿‡pythonçš„kuberneteså®¢æˆ·ç«¯åŠ è½½é»˜è®¤kubeconfigï¼Œå¹¶è·å–æŒ‡å®šèŠ‚ç‚¹ä¸Šçš„Podåˆ—è¡¨ï¼Œæ¼”ç¤ºæ€§åœ°åˆ é™¤è¿™äº›Podï¼Œå†æ¬¡è®¿é—®è¢«åˆ é™¤çš„Podæ—¶ï¼Œè‹¥å·²å¼€å¯AuthorizeNodeWithSelectorsä¸”ä½¿ç”¨èŠ‚ç‚¹è´¦å·æˆæƒï¼Œå¯èƒ½ä¼šå‡ºç°æ— æƒé™æˆ–æ‰¾ä¸åˆ°èµ„æºçš„é”™è¯¯ï¼Œæ¨¡æ‹Ÿäº†IssueæåŠçš„æƒ…å†µã€‚  
2. è¯¥é—®é¢˜å¹¶éæºè‡ªè¶Šæƒè®¿é—®æˆ–æ•æ„Ÿä¿¡æ¯æ³„éœ²ç­‰å®‰å…¨é£é™©ï¼Œè€Œæ˜¯ä¸€ç§ä¸Kubernetesç­–ç•¥è°ƒæ•´ç›¸å…³çš„åŠŸèƒ½å¼‚å¸¸ï¼Œå› æ­¤å½’ç±»ä¸ºâ€œä¸æ¶‰åŠâ€å®‰å…¨é£é™©ã€‚  
3. è„šæœ¬ä»…å±•ç¤ºäº†å¦‚ä½•ä¸Kubernetesè¿›è¡Œäº¤äº’ï¼Œå¹¶æ— ä»»ä½•é«˜é£é™©çš„æ¼æ´åˆ©ç”¨è¡Œä¸ºï¼Œå¯åœ¨æœ¬åœ°æµ‹è¯•ç¯å¢ƒä½¿ç”¨ã€‚  
4. å®é™…æ’éšœæˆ–å…¼å®¹å¤„ç†æ—¶ï¼Œå¯æ ¹æ®éœ€æ±‚å…³é—­AuthorizeNodeWithSelectorsï¼ˆä¸æ¨èåœ¨ç”Ÿäº§ç¯å¢ƒä¸­éšæ„å˜æ›´é›†ç¾¤é»˜è®¤å®‰å…¨ç­–ç•¥ï¼‰æˆ–ä½¿ç”¨æ›´é«˜çº§åˆ«å‡­æ®æ‰§è¡Œdrainæ“ä½œï¼Œä»¥é¿å…å‡ºç°æƒé™é”™è¯¯ã€‚

---


## Issue #131232 kubernetes.io/csi: unmounter failed, because of the vol_data.json: no such file or directory

- Issue é“¾æ¥ï¼š[#131232](https://github.com/kubernetes/kubernetes/issues/131232)

### Issue å†…å®¹

#### What happened?

When the csi interface is invoked to attach a volume, if the volume fails to be attached, the removeMountDir method is invoked to clear the directory where the volume is attached. If the pod is deleted, the csi volume fails to be attached, and a message is displayed indicating that the volume fails to be detached. The cause is that the vol_data.json file does not exist.
The code for mount a volume is as follows:
https://github.com/kubernetes/kubernetes/blob/88dfcb225d41326113990e87b11137641c121a32/pkg/volume/csi/csi_mounter.go#L300-L323

The NewUnmounter method is invoked to detach a volume. The vol_data.json file needs to be read. The code is as follows:
https://github.com/kubernetes/kubernetes/blob/88dfcb225d41326113990e87b11137641c121a32/pkg/volume/csi/csi_plugin.go#L556-L559

The log is as follows:
E0120 10:53:49.870240 4083505 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/31c052e2-63f5-4127-95cb-0923db30dd5a-dump podName:31c052e2-63f5-4127-95cb-0923db30dd5a nodeName:}" failed. No retries permitted until 2025-01-20 10:53:50.370182329 +0000 UTC m=+5.022372153 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "dump" (UniqueName: "kubernetes.io/csi/31c052e2-63f5-4127-95cb-0923db30dd5a-dump") pod "ndp-spark-history-0" (UID: "31c052e2-63f5-4127-95cb-0923db30dd5a") : rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing: dial unix /var/lib/kubelet/plugins_registry/sop-csi-driver-reg.sock: connect: resource temporarily unavailable"

E0120 13:16:00.518419 4083505 reconciler_common.go:169] "operationExecutor.UnmountVolume failed (controllerAttachDetachEnabled true) for volume \"dump\" (UniqueName: \"kubernetes.io/csi/31c052e2-63f5-4127-95cb-0923db30dd5a-dump\") pod \"31c052e2-63f5-4127-95cb-0923db30dd5a\" (UID: \"31c052e2-63f5-4127-95cb-0923db30dd5a\") : UnmountVolume.NewUnmounter failed for volume \"dump\" (UniqueName: \"kubernetes.io/csi/31c052e2-63f5-4127-95cb-0923db30dd5a-dump\") pod \"31c052e2-63f5-4127-95cb-0923db30dd5a\" (UID: \"31c052e2-63f5-4127-95cb-0923db30dd5a\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/31c052e2-63f5-4127-95cb-0923db30dd5a/volumes/kubernetes.io~csi/dump/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/31c052e2-63f5-4127-95cb-0923db30dd5a/volumes/kubernetes.io~csi/dump/vol_data.json]: open /var/lib/kubelet/pods/31c052e2-63f5-4127-95cb-0923db30dd5a/volumes/kubernetes.io~csi/dump/vol_data.json: no such file or directory" err="UnmountVolume.NewUnmounter failed for volume \"dump\" (UniqueName: \"kubernetes.io/csi/31c052e2-63f5-4127-95cb-0923db30dd5a-dump\") pod \"31c052e2-63f5-4127-95cb-0923db30dd5a\" (UID: \"31c052e2-63f5-4127-95cb-0923db30dd5a\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/31c052e2-63f5-4127-95cb-0923db30dd5a/volumes/kubernetes.io~csi/dump/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/31c052e2-63f5-4127-95cb-0923db30dd5a/volumes/kubernetes.io~csi/dump/vol_data.json]: open /var/lib/kubelet/pods/31c052e2-63f5-4127-95cb-0923db30dd5a/volumes/kubernetes.io~csi/dump/vol_data.json: no such file or directory"

#### What did you expect to happen?

Volumes that fail to be mounted can also be unmounted successfully.

#### How can we reproduce it (as minimally and precisely as possible)?

Simulate a scenario in which a volume fails to be attached to the CSI and the vol_data.json file is deleted. After the volume fails to be mounted, the volume is unmounted.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.31
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»è¯¥Issueæè¿°çš„æƒ…å†µæ¥çœ‹ï¼Œé—®é¢˜ä¸»è¦æ˜¯CSIåœ¨å¸è½½ï¼ˆunmountï¼‰æµç¨‹ä¸­ï¼Œç”±äºvol_data.jsonæ–‡ä»¶ç¼ºå¤±ï¼Œå¯¼è‡´å¸è½½å¤±è´¥ã€‚æ•´ä¸ªæµç¨‹å¹¶æœªæ¶‰åŠè¶Šæƒè®¿é—®ã€æ•æ„Ÿä¿¡æ¯æ³„æ¼ã€ä»»æ„å‘½ä»¤æ‰§è¡Œç­‰å®‰å…¨é£é™©ï¼Œä¹Ÿæœªæä¾›å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„é€”å¾„ã€‚è¯¥é—®é¢˜å±äºèµ„æºç®¡ç†æˆ–å®¹å™¨ç¼–æ’çš„é€»è¾‘é”™è¯¯ï¼Œæ— æ³•è¢«ç”¨æ¥è¿›ä¸€æ­¥å‘åŠ¨æ”»å‡»æˆ–æå‡æƒé™ã€‚å› æ­¤ï¼Œæœ¬Issueä¸æ¶‰åŠä»»ä½•å®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# ç”±äºè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ•…æ— éœ€æä¾›å®‰å…¨æ¼æ´å¤ç°è„šæœ¬ã€‚
# å¦‚éœ€æµ‹è¯•CSIåœ¨ä¸¢å¤±vol_data.jsonæ–‡ä»¶åçš„è¡Œä¸ºï¼Œå¯åœ¨kubernetesé›†ç¾¤ä¸Š
# æ‰‹åŠ¨æˆ–é€šè¿‡è„šæœ¬ç§»é™¤vol_data.jsonåè§‚å¯Ÿunmountå¤±è´¥çš„ç°è±¡ã€‚
# ä»¥ä¸‹ç¤ºä¾‹æ¼”ç¤ºå¦‚ä½•åœ¨é›†ç¾¤ç¯å¢ƒä¸­æ£€æµ‹å·æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¹¶å±•ç¤ºå¯èƒ½çš„æŠ¥é”™æ—¥å¿—ã€‚
# è¯¥è„šæœ¬ä»…ç”¨äºæ¼”ç¤ºå¦‚ä½•å®šä½å’ŒæŸ¥çœ‹CSIç›¸å…³é€»è¾‘ï¼Œéæ¼æ´åˆ©ç”¨POCã€‚

import time
import os
from kubernetes import client, config

def main():
    # å‡è®¾æœ¬åœ°å·²é…ç½®kubeconfigæ–‡ä»¶ï¼Œå¯æ­£å¸¸è¯»å–
    config.load_kube_config()

    v1 = client.CoreV1Api()

    # ä»¥ä¸‹é€»è¾‘ä»…ç”¨äºæ¼”ç¤ºå¯¹CSIæŒ‚è½½ç‚¹è¿›è¡Œç®€å•æ’æŸ¥å’Œæ—¥å¿—æ£€æµ‹çš„è¿‡ç¨‹
    # å®é™…æ“ä½œä¸­å¯ç»“åˆè‡ªèº«éƒ¨ç½²åœºæ™¯å®ŒæˆCSIé©±åŠ¨æµ‹è¯•
    print("å¼€å§‹æ£€æµ‹ç›®æ ‡Podçš„vol_data.jsonæ–‡ä»¶æ˜¯å¦å­˜åœ¨...")

    # æ¼”ç¤ºå…ˆæ‰¾åˆ°æŸä¸ªPodï¼ˆæ­¤å¤„ä»…ä¸¾ä¾‹ï¼Œå¯æ ¹æ®è‡ªèº«é›†ç¾¤ä¿®æ”¹ï¼‰
    pods = v1.list_pod_for_all_namespaces()
    for pod in pods.items:
        # ä»…ç¤ºä¾‹æ‰“å°Podåç§°ï¼Œä¸ä½œå®é™…åˆ é™¤æ“ä½œ
        print(f"æ£€æµ‹ Pod: {pod.metadata.name}")
    
    print("å¦‚éœ€è¿›ä¸€æ­¥æµ‹è¯•ï¼Œè¯·åœ¨çœŸå®ç¯å¢ƒä¸­åˆ é™¤/var/lib/kubelet/.../vol_data.jsonæ–‡ä»¶åï¼Œè§‚å¯Ÿunmountè¿‡ç¨‹ã€‚")
    print("è„šæœ¬ç»“æŸã€‚")
    time.sleep(2)
    return
```


---


## Issue #131228 Defined nodePort same for UDP/TCP -- service update does not sync ports.

- Issue é“¾æ¥ï¼š[#131228](https://github.com/kubernetes/kubernetes/issues/131228)

### Issue å†…å®¹

#### What happened?

If you change a service manifest to have the same NodePort: and "Update/Apply" to an existing service it will only change 1 of the nodePorts that is on the same port, seems to be the first `ports:` in the list.

If you delete and recreate the service, the duplicate TCP/UDP service accepts the same nodePort.

Server Version: v1.32.1

Ref #91590 

```
[manifests] % kubectl  apply -f k8s-external-coredns-service.yaml
service/k8s-external-coredns configured
[manifests] % kubectl  get svc k8s-external-coredns
NAME                   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)                                    AGE
k8s-external-coredns   NodePort   10.4.215.193   <none>        53:30054/UDP,53:30054/TCP,9153:30828/TCP   14m
[manifests] % kubectl  apply -f k8s-external-coredns-service.yaml
service/k8s-external-coredns configured
[manifests] % kubectl  get svc k8s-external-coredns
NAME                   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)                                    AGE
k8s-external-coredns   NodePort   10.4.215.193   <none>        53:30154/UDP,53:30054/TCP,9153:30828/TCP   15m
```

FYI, Additionally while testing the order issue, I noted that if you change the order of the ports: attribute, it also does not accept an update due to the duplicates:
```
% kubectl  apply -f k8s-external-coredns-service.yaml
The Service "k8s-external-coredns" is invalid:
* spec.ports[1].name: Duplicate value: "dns"
* spec.ports[1]: Duplicate value: core.ServicePort{Name:"", Protocol:"UDP", AppProtocol:(*string)(nil), Port:53, TargetPort:intstr.IntOrString{Type:0, IntVal:0, StrVal:""}, NodePort:0}
```

#### What did you expect to happen?

The apply/update should take the same as a new creation.

#### How can we reproduce it (as minimally and precisely as possible)?

```
apiVersion: v1
kind: Service
metadata:
  labels:
    k8s-app: kube-dns-k8s-external
    kubernetes.io/name: CoreDNS
  name: k8s-external-coredns
  namespace: kube-system
spec:
  type: NodePort
  externalTrafficPolicy: Cluster
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: dns
    port: 53
    protocol: UDP
    targetPort: 53
    nodePort: 30054
  - name: dns-tcp
    port: 53
    protocol: TCP
    targetPort: 53
    nodePort: 30054
  - name: metrics
    port: 9153
    protocol: TCP
    targetPort: 9153
  selector:
    k8s-app: k8s-external-coredns
  sessionAffinity: None

```

#### Anything else we need to know?

Non-Critcal, the workaround is just to delete/recreate.  Just reporting for completness.

#### Kubernetes version

<details>

```console
Client Version: v1.32.3
Server Version: v1.32.1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
CNI is FLannel
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®Issueçš„æè¿°ï¼Œè¿™æ˜¯ä¸€ä¸ªå…³äºKubernetes Serviceåœ¨ä½¿ç”¨ç›¸åŒçš„NodePortï¼ˆUDP/TCPï¼‰æ—¶æ›´æ–°ä¸ç”Ÿæ•ˆçš„åŠŸèƒ½æ€§é—®é¢˜ï¼Œä¸èƒ½ç®—ä½œå®‰å…¨æ¼æ´ã€‚è¯¥é—®é¢˜æœªæ¶‰åŠè®¿é—®æ§åˆ¶ç»•è¿‡ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸æˆ–ææƒç­‰å®‰å…¨åœºæ™¯ï¼Œä¹Ÿæ²¡æœ‰å¯¹å¤šç”¨æˆ·ç¯å¢ƒäº§ç”Ÿè¶Šæƒå½±å“ã€‚å› æ­¤ä¸å±äºå®‰å…¨é£é™©èŒƒç•´ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Issueæœ¬è´¨ä¸Šæ˜¯Kubernetesåœ¨å¤„ç†åŒä¸€NodePortä¸‹ä¸åŒåè®®æ—¶æ›´æ–°è¡Œä¸ºæœ‰ç¼ºé™·çš„åŠŸèƒ½æ€§é—®é¢˜ï¼Œæ— æ³•é€šè¿‡æ­¤é—®é¢˜ç›´æ¥é€ æˆå®‰å…¨å¨èƒæˆ–æ¼æ´åˆ©ç”¨ã€‚æ•…åˆ¤å®šä¸ºâ€œä¸æ¶‰åŠâ€å®‰å…¨é£é™©ã€‚ä»…éœ€åœ¨é¡¹ç›®å±‚é¢ä¿®å¤æˆ–è§„é¿æ­¤ç‰¹æ€§ç¼ºé™·å³å¯ï¼Œæ— éœ€é¢å¤–å®‰å…¨é˜²æŠ¤æªæ–½ã€‚

---


## Issue #131223 Wrong file system running multiple Windows pods

- Issue é“¾æ¥ï¼š[#131223](https://github.com/kubernetes/kubernetes/issues/131223)

### Issue å†…å®¹

#### What happened?

Hi,

we have update the following components in our environment:

Jenkins - 2.462.2 -> 2.479.2 

AKS - 1.29.7 -> 1.31.5

With the current status our Jenkins pipeline fails (randomly) when we execute multiple stages on a Windows node pool. We use the same pod definition for all the stages in the pipeline. 
Checking the file system we have seen some differences.
When the stage is executed correctly the C:\Jenkins folder content is:
![Image](https://github.com/user-attachments/assets/6444f899-c9ca-4ad9-834a-3c1d306f3c9a)

while in case of a failure the content is: 

![Image](https://github.com/user-attachments/assets/5d14b6a2-7352-4b2b-9dbf-e2f13998c853)

 
We are struggling to understand the cause. The pod - cloud-node-manager-windows-xxx which it should be managed by Kubernetes have the exact file system posted in the failure scenario.

Our Windows pods mount 2 volumes:

![Image](https://github.com/user-attachments/assets/2e46a5d1-2a94-4409-94ee-4485475500a7)


We have tested also the old working version of Jenkins with the new version of Kubernetes but the issue persist.

#### What did you expect to happen?

I'm expect to be able to run multiple pods on a Windows node pool without running into this problem, which causes our pipelines to fail.

#### How can we reproduce it (as minimally and precisely as possible)?

Tools:
Jenkins - 2.462.2 or 2.479.2 
AKS - 1.31.5

---
Jenkinsfile with parallel stages which involve Windows pods:
...
matrix {
                axes {
                    axis {
                        name 'AXE_PLATFORM'
                        values 'Win32_x64', 'Win32_x86'
                    }
                    axis {
                        name 'AXE_TYPE'
                        values 'Debug', 'Release'
                    }
                }
                stages {
                    stage('Build') {
                        agent { kubernetes ( 'pod_windows.yaml' }
                        steps {
                            script {
                                bat 'dir C:\\jenkins'
                                ...
                            }
                        }
                    }
                }
            }
....

#### Anything else we need to know?

We use groovy to merge to pods definitions, one which contains the jenkins remote agent and the other one which contains our pod used to execute our instructions.


#### Kubernetes version

1.31.5


#### Cloud provider

Azure


#### OS version

Here the base image we use mcr.microsoft.com/windows:10.0.17763.6293
On top of that we install other tools (msbuild, git, ...)


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»è¯¥ Issue çš„æè¿°æ¥çœ‹ï¼Œä¸»è¦é—®é¢˜æ˜¯å¤šä¸ªå¹¶è¡Œè¿è¡Œçš„ Windows Pod åœ¨æ–‡ä»¶ç³»ç»Ÿä¸Šäº§ç”Ÿäº†ä¸ä¸€è‡´çš„ç°è±¡ï¼Œå¯¼è‡´ Jenkins Pipeline éšæœºå¤±è´¥ã€‚è¯¥é—®é¢˜æ›´åƒæ˜¯ä¸€ä¸ªç¯å¢ƒé…ç½®æˆ–å®¹å™¨å­˜å‚¨çš„ä½¿ç”¨æ–¹å¼æ‰€å¯¼è‡´çš„åŠŸèƒ½æ€§æ•…éšœï¼Œå¹¶æœªæåŠä»»ä½•æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€å‘½ä»¤æ‰§è¡Œã€ææƒæˆ–å¤šç”¨æˆ·æƒ…å†µä¸‹å¯¼è‡´è¶Šæƒç­‰å®‰å…¨é£é™©ç‚¹ã€‚  
å› æ­¤ï¼Œæ ¹æ®å½“å‰å†…å®¹åˆ†æï¼Œæœ¬é—®é¢˜å¹¶ä¸æ¶‰åŠå®‰å…¨æ¼æ´æˆ–å®‰å…¨é£é™©ï¼Œè€Œæ˜¯å±äºç¯å¢ƒå’Œé…ç½®å±‚é¢çš„åŠŸèƒ½é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥é—®é¢˜ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ— éœ€æä¾›å¤ç°è„šæœ¬
```


---


## Issue #131210 [BUG] Windows node reports less CPU usage than its pods with Kubernetes version 1.30.0

- Issue é“¾æ¥ï¼š[#131210](https://github.com/kubernetes/kubernetes/issues/131210)

### Issue å†…å®¹

#### What happened?

In AKS clusters (k8s version 1.30.0), kubectl top node is showing less cpu load for windows nodes than expected. For example we have observed: <html>
<body>
<!--StartFragment--><p data-start="137" data-end="154" class="">Youâ€™re observing:</p>
<div class="overflow-x-auto contain-inline-size">
Pod | Node | Pod CPU (millicores) | Node CPU (millicores)
-- | -- | -- | --
-4g6vg | akswwin020001wd | 4335m | 628m (8%)
-7xcb4 | akswwin020001zz | 3496m | 552m (7%)
-q6x5x | akswwin020001t0 | 4893m | 579m (7%)
-qcsq8 | akswwin020001y4 | 3435m | 512m (6%)
-wdlc4 | akswwin020001ye | 3708m | 570m (7%)
-zxtmd | akswwin020001y5 | 4495m | 641m (8%)

</div><!--EndFragment-->
</body>
</html>




#### What did you expect to happen?

Expect to see windows node cpu metrics bigger than pods and is the summerize of all pods cpu usage.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create an AKS cluster with k8s version 1.30.0
2. Create a windows nodepool with 1 Windows2022/2019 node
3. Create simple cpu-consuming deployment.
4. Wait until metrics are collected (~1 minute)
5. Execute command kubectl top pod and kubectl top node
6. Pods have in total more cpu usage than node (It seems that the CPU metric is a factor 10 off).

#### Anything else we need to know?

The issue was reported with previous version https://github.com/kubernetes/kubernetes/issues/122382, looks like the issue still exists.

#### Kubernetes version

<details>

1.30.0

</details>


#### Cloud provider

<details>
AKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»Issueæè¿°æ¥çœ‹ï¼Œè¯¥é—®é¢˜é›†ä¸­åœ¨WindowsèŠ‚ç‚¹ä¸ŠæŠ¥çš„CPUä½¿ç”¨ç‡ä¸å®é™…Podä½¿ç”¨ç‡ä¸ä¸€è‡´ï¼Œå±äºç›‘æ§æˆ–åº¦é‡æ•°æ®çš„åå·®æˆ–ç»Ÿè®¡BUGï¼Œå¹¶æœªæ¶‰åŠä»»æ„æœªæˆæƒè®¿é—®ã€ææƒã€å‘½ä»¤æ‰§è¡Œã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ç­‰å®‰å…¨é£é™©ã€‚ä»…æ˜¯åº¦é‡æ•°æ®ä¸å‡†ç¡®è€Œå¼•å‘çš„é—®é¢˜ã€‚å› æ­¤ï¼Œæ ¹æ®Issueå†…å®¹åˆ¤æ–­ä¸ºåŠŸèƒ½æˆ–æ€§èƒ½Bugï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥Issueä¸ºéå®‰å…¨é—®é¢˜ï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ•…æ— éœ€ç¼–å†™å¤ç°POCã€‚
# å¦‚æœç¡®éœ€å¤ç°æ­¤ç›‘æ§æ•°æ®ä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå¯åœ¨æœ¬åœ°æ­å»ºæµ‹è¯•é›†ç¾¤åå¯¹æŒ‡æ ‡è¿›è¡Œæ¯”å¯¹ï¼Œ
# ä½†æ­¤è¿‡ç¨‹ä»…ä¸åº¦é‡å’Œç›‘æ§å®ç°æœ‰å…³ï¼Œå¹¶ä¸æ¶‰åŠå®‰å…¨æ€§ã€‚
```


---


## Issue #131209 plugin execution metric buckets are not useful for debugging high latency plugins

- Issue é“¾æ¥ï¼š[#131209](https://github.com/kubernetes/kubernetes/issues/131209)

### Issue å†…å®¹

#### What happened?

Understanding scheduling latency for pods with CSI-PVC is hard because the metric `scheduler_plugin_execution_duration_seconds_bucket` does not report any buckets after 0.022s.

In the following data, it could be see that a large majority of observations (around 91k) are in +Inf bucket as compared to other observations in buckets (around 1807)   

```
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="1e-05"} 1798
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="1.5000000000000002e-05"} 1800
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="2.2500000000000005e-05"} 1806
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="3.375000000000001e-05"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="5.062500000000001e-05"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="7.593750000000002e-05"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.00011390625000000003"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.00017085937500000006"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.0002562890625000001"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.00038443359375000017"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.0005766503906250003"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.0008649755859375004"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.0012974633789062506"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.0019461950683593758"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.0029192926025390638"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.004378938903808595"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.006568408355712893"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.009852612533569338"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.014778918800354007"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.02216837820053101"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="+Inf"} 93685
```

#### What did you expect to happen?

If scheduling latency of pod goes high, there should a way to debug which plugin and extension point is causing the highest latency

![Image](https://github.com/user-attachments/assets/4f92a7e9-3cfb-46d0-a849-650b65e31f7a)

#### How can we reproduce it (as minimally and precisely as possible)?

1. install a csi with non-trivial volume translations
2. start a deployment with 1000 pods and pvcs
3. observe the metrics

#### Anything else we need to know?

_No response_

#### Kubernetes version


```console
$ kubectl  version
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.30.5
```

</details>


#### Cloud provider

baremetal


#### OS version

NA

#### Install tools

NA

#### Container runtime (CRI) and version (if applicable)

NA

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

downstream CSI

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»è¯¥ Issue çš„æè¿°å’Œå†…å®¹æ¥çœ‹ï¼Œä¸»è¦é—®é¢˜æ˜¯ Metric çš„ç»Ÿè®¡åˆ†æ¡£åŒºé—´ä¸å¤Ÿç²¾ç»†ï¼Œå¯¼è‡´åœ¨è°ƒè¯•é«˜å»¶è¿Ÿæ—¶æ— æ³•åŒºåˆ†æ›´é«˜æ—¶é—´èŒƒå›´å†…ä¸åŒæ’ä»¶çš„æ‰§è¡Œæ—¶é—´ï¼Œè¿›è€Œéš¾ä»¥å®šä½é«˜å»¶è¿Ÿæ’ä»¶ã€‚è¿™åªæ˜¯ä¸€ä¸ªåº¦é‡æŒ‡æ ‡ä¸è°ƒè¯•èƒ½åŠ›ä¸è¶³çš„é—®é¢˜ï¼Œæœ¬èº«å¹¶æœªæš´éœ²ä»»ä½•æƒé™æå‡ã€è¶Šæƒè®¿é—®ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²æˆ–å‘½ä»¤æ‰§è¡Œç­‰å®‰å…¨é£é™©ã€‚

æ ¹æ®ç»™å‡ºçš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1. è¯¥é—®é¢˜æœªæ¶‰åŠä»»ä½•å¯èƒ½è¢«åˆ©ç”¨è¿›è¡Œæ”»å‡»ã€ææƒæˆ–ä¿¡æ¯æ³„éœ²çš„æƒ…å½¢ï¼›
2. æœªæœ‰æ•æ„Ÿä¿¡æ¯æˆ–å‡­æ®æ³„éœ²ï¼›
3. æœªå¯¼è‡´æ½œåœ¨çš„å®¹å™¨é€ƒé€¸æˆ–å‘½ä»¤æ‰§è¡Œç­‰é«˜å±æ¼æ´ï¼›
4. ä»…æ˜¯ç›‘æ§ä¸åº¦é‡ä¿¡æ¯çš„ä¸è¶³ï¼Œæ— æ³•è¢«ç”¨æ¥å®æ–½æ‹’ç»æœåŠ¡æ”»å‡»æˆ–é€ æˆå®‰å…¨å¨èƒã€‚

å› æ­¤ï¼Œè¯¥ Issue ä¸å®‰å…¨é—®é¢˜æ— å…³ï¼Œé£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

è¯¥é—®é¢˜å¹¶éå®‰å…¨æ¼æ´ï¼Œå› æ­¤ä¸éœ€è¦æä¾›å¤ç°è„šæœ¬ã€‚ä¸Šæ–‡çš„åˆ†æä¸­å·²é˜æ˜è¯¥é—®é¢˜ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ•…ä¸åšè¿›ä¸€æ­¥å¤ç°ã€‚

---


## Issue #131205 kubectl does not honor  "insecure-skip-tls-verify" flag in in-cluster mode

- Issue é“¾æ¥ï¼š[#131205](https://github.com/kubernetes/kubernetes/issues/131205)

### Issue å†…å®¹

#### What happened?

I am trying to execute a kubectl command within a pod. The pod does not have a kube-config file; but it has a service account token and ca-cert.
I think running kubectl this way (within the pod) runs it in "in-cluster" mode.
I see certificate validation errors even when "insecure-skip-tls-verify" flag is specified. Please see **Case1** output below.
However the same command works when a kubeconfig file is loaded within the pod and specified in the kubectl command. Please see **Case2** output below

The server-certificate presented by my apiserver does not contain the IP address 192.168.0.1 (kubernetes clusterIP service) in the SAN. This is the reason for specifying `--insecure-skip-tls-verify=true`

The kubeconfig file used in Case2 below uses 192.168.0.1  (kubernetes clusterIP service) as the "server" (apiserver) to connect to. It should thus have the same certificate validation issue ... but it works !

**kubectl.conf file copied within the pod**
```
apiVersion: v1
kind: Config
clusters:
- cluster:
    server: https://192.168.0.1:443
```

**Case1 -- no kubeconfig file (in-cluster mode) **

```
root@fsm-eqx-chi-kubenode14-staging-karmada-st:/usr/local/pod/rackman# ./kubectl get po --insecure-skip-tls-verify=true  -v=10
I0408 12:49:00.949567      66 merged_client_builder.go:163] Using in-cluster namespace
I0408 12:49:00.949776      66 merged_client_builder.go:121] Using in-cluster configuration
I0408 12:49:00.950200      66 envvar.go:172] "Feature gate default state" feature="ClientsPreferCBOR" enabled=false
I0408 12:49:00.950219      66 envvar.go:172] "Feature gate default state" feature="InformerResourceVersion" enabled=false
I0408 12:49:00.950227      66 envvar.go:172] "Feature gate default state" feature="WatchListClient" enabled=false
I0408 12:49:00.950233      66 envvar.go:172] "Feature gate default state" feature="ClientsAllowCBOR" enabled=false
I0408 12:49:00.950271      66 discovery_client.go:253] "Request Body" body=""
I0408 12:49:00.950377      66 round_trippers.go:473] curl -v -XGET  -H "Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json" -H "User-Agent: kubectl/v1.32.3 (linux/amd64) kubernetes/32cc146" -H "Authorization: Bearer <masked>" 'https://192.168.0.1:443/api?timeout=32s'
I0408 12:49:00.963871      66 round_trippers.go:517] HTTP Trace: Dial to tcp:192.168.0.1:443 succeed
I0408 12:49:00.979655      66 round_trippers.go:560] GET https://192.168.0.1:443/api?timeout=32s  in 29 milliseconds
I0408 12:49:00.979670      66 round_trippers.go:577] HTTP Statistics: DNSLookup 0 ms Dial 13 ms TLSHandshake 15 ms Duration 29 ms
I0408 12:49:00.979679      66 round_trippers.go:584] Response Headers:
E0408 12:49:00.979742      66 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://192.168.0.1:443/api?timeout=32s\": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs"
I0408 12:49:00.980876      66 cached_discovery.go:120] skipped caching discovery info due to Get "https://192.168.0.1:443/api?timeout=32s": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs
I0408 12:49:00.980991      66 discovery_client.go:253] "Request Body" body=""
I0408 12:49:00.981084      66 round_trippers.go:473] curl -v -XGET  -H "Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json" -H "User-Agent: kubectl/v1.32.3 (linux/amd64) kubernetes/32cc146" -H "Authorization: Bearer <masked>" 'https://192.168.0.1:443/api?timeout=32s'
I0408 12:49:00.992617      66 round_trippers.go:517] HTTP Trace: Dial to tcp:192.168.0.1:443 succeed
I0408 12:49:01.008172      66 round_trippers.go:560] GET https://192.168.0.1:443/api?timeout=32s  in 27 milliseconds
I0408 12:49:01.008233      66 round_trippers.go:577] HTTP Statistics: DNSLookup 0 ms Dial 11 ms TLSHandshake 15 ms Duration 27 ms
I0408 12:49:01.008253      66 round_trippers.go:584] Response Headers:
E0408 12:49:01.008369      66 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://192.168.0.1:443/api?timeout=32s\": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs"
I0408 12:49:01.009578      66 cached_discovery.go:120] skipped caching discovery info due to Get "https://192.168.0.1:443/api?timeout=32s": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs
I0408 12:49:01.009649      66 shortcut.go:103] Error loading discovery information: Get "https://192.168.0.1:443/api?timeout=32s": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs
I0408 12:49:01.009724      66 discovery_client.go:253] "Request Body" body=""
I0408 12:49:01.009882      66 round_trippers.go:473] curl -v -XGET  -H "Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json" -H "User-Agent: kubectl/v1.32.3 (linux/amd64) kubernetes/32cc146" -H "Authorization: Bearer <masked>" 'https://192.168.0.1:443/api?timeout=32s'
I0408 12:49:01.021283      66 round_trippers.go:517] HTTP Trace: Dial to tcp:192.168.0.1:443 succeed
I0408 12:49:01.036799      66 round_trippers.go:560] GET https://192.168.0.1:443/api?timeout=32s  in 26 milliseconds
I0408 12:49:01.036834      66 round_trippers.go:577] HTTP Statistics: DNSLookup 0 ms Dial 11 ms TLSHandshake 15 ms Duration 26 ms
I0408 12:49:01.036852      66 round_trippers.go:584] Response Headers:
E0408 12:49:01.036939      66 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://192.168.0.1:443/api?timeout=32s\": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs"
I0408 12:49:01.038067      66 cached_discovery.go:120] skipped caching discovery info due to Get "https://192.168.0.1:443/api?timeout=32s": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs
I0408 12:49:01.038174      66 discovery_client.go:253] "Request Body" body=""
I0408 12:49:01.038275      66 round_trippers.go:473] curl -v -XGET  -H "User-Agent: kubectl/v1.32.3 (linux/amd64) kubernetes/32cc146" -H "Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json" -H "Authorization: Bearer <masked>" 'https://192.168.0.1:443/api?timeout=32s'
I0408 12:49:01.049681      66 round_trippers.go:517] HTTP Trace: Dial to tcp:192.168.0.1:443 succeed
I0408 12:49:01.064773      66 round_trippers.go:560] GET https://192.168.0.1:443/api?timeout=32s  in 26 milliseconds
I0408 12:49:01.064804      66 round_trippers.go:577] HTTP Statistics: DNSLookup 0 ms Dial 11 ms TLSHandshake 15 ms Duration 26 ms
I0408 12:49:01.064821      66 round_trippers.go:584] Response Headers:
E0408 12:49:01.064901      66 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://192.168.0.1:443/api?timeout=32s\": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs"
I0408 12:49:01.066026      66 cached_discovery.go:120] skipped caching discovery info due to Get "https://192.168.0.1:443/api?timeout=32s": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs
I0408 12:49:01.066104      66 discovery_client.go:253] "Request Body" body=""
I0408 12:49:01.066206      66 round_trippers.go:473] curl -v -XGET  -H "User-Agent: kubectl/v1.32.3 (linux/amd64) kubernetes/32cc146" -H "Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json" -H "Authorization: Bearer <masked>" 'https://192.168.0.1:443/api?timeout=32s'
I0408 12:49:01.078300      66 round_trippers.go:517] HTTP Trace: Dial to tcp:192.168.0.1:443 succeed
I0408 12:49:01.094105      66 round_trippers.go:560] GET https://192.168.0.1:443/api?timeout=32s  in 27 milliseconds
I0408 12:49:01.094137      66 round_trippers.go:577] HTTP Statistics: DNSLookup 0 ms Dial 11 ms TLSHandshake 15 ms Duration 27 ms
I0408 12:49:01.094155      66 round_trippers.go:584] Response Headers:
E0408 12:49:01.094230      66 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://192.168.0.1:443/api?timeout=32s\": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs"
I0408 12:49:01.095352      66 cached_discovery.go:120] skipped caching discovery info due to Get "https://192.168.0.1:443/api?timeout=32s": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs
I0408 12:49:01.095428      66 helpers.go:264] Connection error: Get https://192.168.0.1:443/api?timeout=32s: tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs
Unable to connect to the server: tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs
root@fsm-eqx-chi-kubenode14-staging-karmada-st:/usr/local/pod/rackman# 

```

**Case2 -- with kube-config file**

```
root@fsm-eqx-chi-kubenode14-staging-karmada-st:/usr/local/pod/rackman# ./kubectl get po --insecure-skip-tls-verify=true  --kubeconfig=./k.conf -v=10
I0408 12:51:05.912499     115 loader.go:402] Config loaded from file:  ./k.conf
I0408 12:51:05.912634     115 merged_client_builder.go:163] Using in-cluster namespace
I0408 12:51:05.912865     115 envvar.go:172] "Feature gate default state" feature="ClientsAllowCBOR" enabled=false
I0408 12:51:05.912883     115 envvar.go:172] "Feature gate default state" feature="ClientsPreferCBOR" enabled=false
I0408 12:51:05.912890     115 envvar.go:172] "Feature gate default state" feature="InformerResourceVersion" enabled=false
I0408 12:51:05.912897     115 envvar.go:172] "Feature gate default state" feature="WatchListClient" enabled=false
I0408 12:51:05.912934     115 discovery_client.go:253] "Request Body" body=""
I0408 12:51:05.913006     115 round_trippers.go:473] curl -v -XGET  -H "User-Agent: kubectl/v1.32.3 (linux/amd64) kubernetes/32cc146" -H "Authorization: Bearer <masked>" -H "Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json" 'https://192.168.0.1:443/api?timeout=32s'
I0408 12:51:05.926274     115 round_trippers.go:517] HTTP Trace: Dial to tcp:192.168.0.1:443 succeed
I0408 12:51:05.955768     115 round_trippers.go:560] GET https://192.168.0.1:443/api?timeout=32s 200 OK in 42 milliseconds
I0408 12:51:05.955811     115 round_trippers.go:577] HTTP Statistics: DNSLookup 0 ms Dial 13 ms TLSHandshake 15 ms ServerProcessing 12 ms Duration 42 ms
I0408 12:51:05.955841     115 round_trippers.go:584] Response Headers:
I0408 12:51:05.955861     115 round_trippers.go:587]     Cache-Control: public
I0408 12:51:05.955875     115 round_trippers.go:587]     Vary: Accept
I0408 12:51:05.955886     115 round_trippers.go:587]     X-Kubernetes-Pf-Prioritylevel-Uid: 7d9c616a-a9f7-4cae-8abc-3b48f7e25d27
I0408 12:51:05.955896     115 round_trippers.go:587]     X-Varied-Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json
I0408 12:51:05.955907     115 round_trippers.go:587]     Audit-Id: ce75520e-a997-4d1e-87c7-6d4dca17a25d
I0408 12:51:05.955921     115 round_trippers.go:587]     Content-Type: application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList
I0408 12:51:05.955943     115 round_trippers.go:587]     Etag: "7E2E4FBD8CDC884130EBFEF64757F6BC507A8EA6A2EA8F94E05F1B6F207B4E183C63D6D6EA5513FDDC10FC16233248F933DC787C288493D52745A3FAFD68CCD8"
I0408 12:51:05.955956     115 round_trippers.go:587]     X-Kubernetes-Pf-Flowschema-Uid: 278b1097-7b09-48fd-8f57-2cae1444965c
I0408 12:51:05.955969     115 round_trippers.go:587]     Date: Tue, 08 Apr 2025 12:51:05 GMT
I0408 12:51:05.957867     115 discovery_client.go:253] "Response Body" body=<
```

#### What did you expect to happen?

I expected the `--insecure-skip-tls-verify=true` flag to work the same (& succeed) in both cases.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Configure an apiserver ith a certificate that does not have 192.168.0.1 (the clusterIP) in the SAN field.
2. Create a pod
3. Install kubectl within the pod 
4. Copy a valid kubeconfig within the pod
5. Execute both cases as shown above.


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version

Client Version: v1.29.14
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.14

```

</details>


#### Cloud provider

<details>
none -- Baremetal cluster
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# PRETTY_NAME="Debian GNU/Linux 11 (bullseye)"
NAME="Debian GNU/Linux"
VERSION_ID="11"
VERSION="11 (bullseye)"
VERSION_CODENAME=bullseye
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ uname -a
Linux gce-usc-kubemaster11-staging 5.10.0-34-cloud-amd64 #1 SMP Debian 5.10.234-1 (2025-02-24) x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
none -- installed using ansible
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd v1.7.17 
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
CNI - kube-router
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»Issueçš„æè¿°æ¥çœ‹ï¼Œè¯¥é—®é¢˜çš„æ ¸å¿ƒæ˜¯â€œåœ¨é›†ç¾¤å†…è¿è¡Œkubectlæ—¶ï¼Œå³ä½¿æŒ‡å®šäº†--insecure-skip-tls-verify=trueé€‰é¡¹ï¼Œä¾ç„¶ä¼šå› ä¸ºè¯ä¹¦ä¸åŒ…å«ç›®æ ‡IPåœ°å€çš„SANå­—æ®µè€Œå¯¼è‡´éªŒè¯å¤±è´¥â€ï¼Œå±äºkubectlåœ¨â€œin-clusterâ€æ¨¡å¼ä¸‹å¯¹insecure-skip-tls-verifyé…ç½®çš„å¤„ç†é€»è¾‘ç¼ºé™·æˆ–ä¸ä¸€è‡´ï¼Œä¸çœŸæ­£çš„å®‰å…¨æ¼æ´æ— å…³ã€‚  
è¿™ä¸€é—®é¢˜å¹¶ä¸èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨æ¥é€ æˆä¿¡æ¯æ³„éœ²ã€ææƒã€æ³¨å…¥æˆ–å…¶ä»–å®‰å…¨é£é™©ï¼Œä¹Ÿä¸ä¼šå¯¼è‡´æ‹’ç»æœåŠ¡ï¼Œç”±äºIssueæœ¬èº«å¹¶æœªä½“ç°å‡ºå¯è¢«åˆ©ç”¨çš„æ”»å‡»é¢æˆ–é€ æˆå®‰å…¨å½±å“çš„å¯èƒ½æ€§ï¼Œå› æ­¤ä¸å±äºå®‰å…¨æ¼æ´èŒƒç•´ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

æœ¬Issueåæ˜ çš„æ˜¯ä¸€ä¸ªåŠŸèƒ½/é…ç½®ç¼ºé™·ï¼Œå¹¶æœªåŒ…å«å¯èƒ½è¢«æ¶æ„åˆ©ç”¨çš„å®‰å…¨é£é™©ç‚¹ï¼Œæ•…å°†é£é™©è¯„çº§åˆ¤æ–­ä¸ºâ€œä¸æ¶‰åŠâ€ï¼Œæ— éœ€æä¾›æ¼æ´å¤ç°è„šæœ¬ã€‚è‹¥åç»­è¯¥é—®é¢˜åœ¨å…¶ä»–åœºæ™¯ä¸‹è¢«è¯æ˜ä¼šå¯¼è‡´å®‰å…¨å¨èƒï¼Œåˆ™éœ€è¦å¦è¡Œè¯„ä¼°ã€‚

---


## Issue #131197 ValidatingAdmissionPolicy CEL Type Checking fails using `all` macro with notAComprehensionRange

- Issue é“¾æ¥ï¼š[#131197](https://github.com/kubernetes/kubernetes/issues/131197)

### Issue å†…å®¹

#### What happened?

Attempting to use ValidatingAdmissionPolicy to provide a layer of authz for our Kubernetes cluster to allow some edits but not others to the Pod/Deployment spec.

With an (admittedly complicated) expression such as:
```
!has(object.spec) && !has(oldObject.spec) || // skip validation if spec is not present
// allow only if the pod spec is not changed, or if the changes are only the container resources
object.spec.all(k, k == "containers" ?  
  object.spec.containers.all(c, 
    oldObject.spec.containers.exists(o, o.name == c.name &&
    c.all(k, k == "resources" || (k in o && c[k] == o[k])))) 
: (k in oldObject.spec && object.spec[k] == oldObject.spec[k])) &&
oldObject.spec.all(k, k == "containers" ? 
  oldObject.spec.containers.all(c, 
    object.spec.containers.exists(o, o.name == c.name &&
      c.all(k, k == "resources" || (k in o && c[k] == o[k])))) 
: (k in object.spec && object.spec[k] == oldObject.spec[k]))
```
(which intends to allow metadata changes such as labels/ownerReferences/annotations, and InPlacePodVerticalScaling resource changes)

The CEL type checker complains (via the ValidatingAdmissionPolicy status field) with:
```
      Warning:    /v1, Kind=Pod: ERROR: <input>:5:7: expression of type 'Pod127885267.spec' cannot be range of a comprehension (must be list, map, or dynamic)
 | object.spec.all(k, k == "containers" ?
 | ......^
```

#### What did you expect to happen?

The type checker passes the expression, as the object spec should be a Map.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a ValidatingAdmissionPolicy with the above expression, view the status.

#### Anything else we need to know?

/sig api-machinery

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.3
Kustomize Version: v5.4.2
Server Version: v1.31.7
```

</details>


#### Cloud provider

<details>
AWS, self-hosted on EC2. 
</details>




### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®Issueæè¿°å¯çŸ¥ï¼Œè¿™å±äºåœ¨ä½¿ç”¨ValidatingAdmissionPolicyçš„CELè¡¨è¾¾å¼æ—¶ï¼Œç±»å‹æ£€æŸ¥æ— æ³•é€šè¿‡æ‰€å¯¼è‡´çš„åŠŸèƒ½æ€§æˆ–é€»è¾‘æ€§é—®é¢˜ï¼Œå¹¶æœªæè¿°ä»»ä½•å¯è¢«åˆ©ç”¨çš„å®‰å…¨æ¼æ´æˆ–ä¸å½“è®¿é—®åœºæ™¯ã€‚é—®é¢˜æ ¸å¿ƒåœ¨äºCELå¯¹specå­—æ®µç±»å‹è¯†åˆ«ä¸æ­£ç¡®ï¼Œå¯¼è‡´è¡¨è¾¾å¼æ— æ³•é€šè¿‡æ ¡éªŒï¼Œè€Œéå®‰å…¨é£é™©ç‚¹ã€‚å› æ­¤ï¼Œè¯¥Issueä¸å±äºå®‰å…¨é£é™©èŒƒç•´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# ç”±äºè¯¥Issueå¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œå› æ­¤æ— éœ€å¤ç°è„šæœ¬
# å¦‚éœ€æµ‹è¯•CELè¡¨è¾¾å¼ï¼Œè¯·åœ¨æœ¬åœ°Kubernetesé›†ç¾¤ä¸­åˆ›å»ºç›¸åº”çš„ValidatingAdmissionPolicyèµ„æº
# å¹¶å°†Issueä¸­ç»™å‡ºçš„è¡¨è¾¾å¼é…ç½®åˆ°policyä¸­è¿›è¡ŒéªŒè¯ã€‚
```


---


## Issue #131189 kubectl exec report error Unknown stream id 1, discarding message

- Issue é“¾æ¥ï¼š[#131189](https://github.com/kubernetes/kubernetes/issues/131189)

### Issue å†…å®¹

#### What happened?

When I run the kubectl exec command, an error log is printed. When does this error occur?
![Image](https://github.com/user-attachments/assets/7002dc4b-775e-4837-b90c-fe04d3613697)

#### What did you expect to happen?

Whether no error can be reported

#### How can we reproduce it (as minimally and precisely as possible)?

Run the kubectl exec command.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.31
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»Issueæè¿°æ¥çœ‹ï¼Œè¿™åªæ˜¯å…³äºåœ¨ä½¿ç”¨ kubectl exec å‘½ä»¤æ—¶å‡ºç°äº† "Unknown stream id 1, discarding message" è¿™æ ·ä¸€æ¡é”™è¯¯æ—¥å¿—ã€‚è¯¥æ—¥å¿—ä¸€èˆ¬ä¸ç½‘ç»œä¼ è¾“ã€å®¢æˆ·ç«¯ä¸æœåŠ¡ç«¯ç‰ˆæœ¬å…¼å®¹æ€§æˆ–å·¥å…·æœ¬èº«çš„Bugç­‰å› ç´ æœ‰å…³ï¼Œå¹¶æ— è¿¹è±¡è¡¨æ˜å…¶å¯å¯¼è‡´è¶Šæƒã€å‘½ä»¤æ‰§è¡Œã€æ•°æ®æ³„éœ²ç­‰å®‰å…¨é£é™©ï¼Œä¹Ÿæ²¡æœ‰æ¶‰åŠåˆ°æ•æ„Ÿä¿¡æ¯æš´éœ²æˆ–ä¸å½“é…ç½®çš„é—®é¢˜ã€‚å› æ­¤ï¼Œæœ¬Issueä¸å±äºå®‰å…¨æ¼æ´æˆ–é£é™©çš„èŒƒç•´ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

æœ¬Issueçš„å†…å®¹ä»…ä¸ºé”™è¯¯æ—¥å¿—é—®é¢˜ï¼Œä¸å®‰å…¨é£é™©æ— å…³ï¼Œå› æ­¤æ— éœ€ç¼–å†™POCè„šæœ¬ã€‚è¯¥é”™è¯¯é€šå¸¸æ˜¯æ­£å¸¸çš„è°ƒè¯•æˆ–é”™è¯¯æç¤ºï¼Œä¸å½±å“ç³»ç»Ÿæ•´ä½“å®‰å…¨ã€‚è‹¥æ¬²è¿›ä¸€æ­¥æ’æŸ¥ï¼Œå¯ä»ç½‘ç»œç¯å¢ƒã€kubectlä¸é›†ç¾¤çš„ç‰ˆæœ¬å…¼å®¹æ€§ç­‰æ–¹é¢ç€æ‰‹ã€‚

---


## Issue #131186 Severe file copy slowdown in pod using Local PV after node reboot (resolved by pod recreation)

- Issue é“¾æ¥ï¼š[#131186](https://github.com/kubernetes/kubernetes/issues/131186)

### Issue å†…å®¹

#### What happened?

We are using Local PersistentVolume (local PV) to mount a local directory to a pod. After a node reboot, the pod is still running, but performing file copy operations inside the pod (e.g., using `cp`) becomes very slow.

Using `strace -f -tt -T -p <PID>` to trace the cp process shows that `sendfile()` system calls are taking 0.7+ seconds per 16MB chunk:
```
15:02:53.949729 sendfile(4, 3, NULL, 16777216) = 16777216 <0.715818>
15:02:54.665935 sendfile(4, 3, NULL, 16777216) = 16777216 <0.731067>
15:02:55.397318 sendfile(4, 3, NULL, 16777216) = 16777216 <0.737302>
...

```

However, if we delete the pod and let it be recreated, the file copy speed returns to normal. sendfile() calls then complete in ~0.015 seconds:
```
15:04:38.006504 sendfile(4, 3, NULL, 16777216) = 16777216 <0.015368>
15:04:38.022146 sendfile(4, 3, NULL, 16777216) = 16777216 <0.015512>
...
```



#### What did you expect to happen?

After node reboot, the pod using local PV should continue to perform file copy operations at normal speed without requiring a pod delete and recreate.

#### How can we reproduce it (as minimally and precisely as possible)?

1.Create a pod that mounts a local directory using local PersistentVolume.
2.Perform a file copy operation inside the pod (e.g., cp largefile /tmp/), observe performance is good.
3.Reboot the node where the pod is scheduled.
4.After the node is back and the pod is still running, repeat the file copy operation.
5.Observe that file copy is very slow.
6.Delete the pod, let it be recreated (same node), and copy files again â€” performance is back to normal.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
1.28.3
```

</details>


#### Cloud provider

<details>
none
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯åœ¨ä½¿ç”¨ Local PV çš„å®¹å™¨åŒ–åº”ç”¨èŠ‚ç‚¹é‡å¯åå‡ºç°æ–‡ä»¶å¤åˆ¶æ€§èƒ½ä¸¥é‡ä¸‹é™çš„é—®é¢˜ï¼Œä»æè¿°æ¥çœ‹å¹¶æ²¡æœ‰æ¶‰åŠæ½œåœ¨çš„å®‰å…¨æ¼æ´æˆ–æ•æ„Ÿä¿¡æ¯æ³„éœ²ï¼Œä¹Ÿæœªæ¶‰åŠåˆ°è·¨æƒé™æ“ä½œã€æ¼æ´åˆ©ç”¨ã€ææƒç­‰é—®é¢˜ã€‚è¯¥é—®é¢˜æ›´åƒæ˜¯ä¸€ä¸ªä¸ Kubernetes æˆ–åº•å±‚å­˜å‚¨é‡è¿æœºåˆ¶ç›¸å…³çš„æ€§èƒ½æˆ–å¯é æ€§ç¼ºé™·ï¼Œå¹¶ä¸å±äºå®‰å…¨æ¼æ´èŒƒç•´ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

Issue æ‰€è¿°çš„é—®é¢˜ä»…ä¸èŠ‚ç‚¹é‡å¯åæœ¬åœ°å­˜å‚¨æ€§èƒ½è¡¨ç°ä¸ä½³æœ‰å…³ï¼Œä¸æ¶‰åŠä»»ä½•å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨ç¼ºé™·ï¼Œæ•…ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚è¿™ä¸ªé—®é¢˜å¯ä»¥é€šè¿‡ Kubernetes å¯¹æœ¬åœ°èŠ‚ç‚¹é‡å¯åå¯¹ PV çš„é‡æ–°æŒ‚è½½æˆ–å®¹å™¨é‡å»ºç­–ç•¥è¿›è¡Œä¼˜åŒ–æ¥è§£å†³ï¼Œä¹Ÿå¯ä¸å­˜å‚¨é©±åŠ¨æœ¬èº«çš„å®ç°ç»†èŠ‚å…³è”ï¼Œä½†æ— è®ºä½•ç§æƒ…å½¢éƒ½æœªä½“ç°å®‰å…¨é£é™©ã€‚æ•…æœ€ç»ˆåˆ¤æ–­ï¼šä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---


## Issue #131178 Deleting a namespace doesn't switches to default namespace

- Issue é“¾æ¥ï¼š[#131178](https://github.com/kubernetes/kubernetes/issues/131178)

### Issue å†…å®¹

#### What happened?

I created a namespace `04--pod namespace` & then I created couple of pods in this namespace(Irrelevant to the issue but still describing the actual flow). I then deleted the namespace & this does 2 things:
 - Deletes the namespace
 - Deletes all resources such as pods associated in the namespace

Now that I run `kubectl get pods`, I get the following result
```
No resources found in 04--pod namespace.
```

<img width="431" alt="Image" src="https://github.com/user-attachments/assets/491ea6ce-e604-40cf-bd15-8466ddfec8cd" />

#### What did you expect to happen?

Deleting a namespace should switch it back to the `default` namespace. As the namespace is deleted, the `get pods` lookup shouldn't be looking into the deleted namespace. This feels like a data inconsistency

<img width="495" alt="Image" src="https://github.com/user-attachments/assets/64ac88e8-cbd7-4dbc-873f-30ee14202e69" />

#### How can we reproduce it (as minimally and precisely as possible)?

- Create a namespace
- Switch to that namespace
- Create a pod(Not required)
- Delete the namespace
- Run `kubectl get pods` . This will show no pods present in the deleted namespace

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.7+k3s1
WARNING: version difference between client (1.30) and server (1.28) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue ä»…æè¿°äº†åœ¨åˆ é™¤ Namespace åï¼Œkubectl æŸ¥è¯¢å‘½åç©ºé—´ä»é»˜è®¤åœç•™åœ¨å·²è¢«åˆ é™¤çš„å‘½åç©ºé—´ï¼Œä»è€Œå¯¼è‡´æŸ¥ä¸åˆ°èµ„æºçš„é—®é¢˜ã€‚æ­¤é—®é¢˜å±äºåŠŸèƒ½/ä½¿ç”¨å±‚é¢çš„ä¸ä¾¿ï¼Œä¸æ¶‰åŠæ•æ„Ÿä¿¡æ¯æ³„éœ²ã€æƒé™æå‡ã€æ‰§è¡Œä»»æ„å‘½ä»¤ç­‰å®‰å…¨é£é™©ã€‚å› æ­¤æ ¹æ®Issueæè¿°å¯åˆ¤æ–­æ­¤é—®é¢˜ä¸å±äºå®‰å…¨æ¼æ´ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

è¯¥é—®é¢˜å¹¶ä¸æ¶‰åŠå¯¹ç³»ç»Ÿæƒé™ã€æ•°æ®æˆ–èµ„æºçš„éé¢„æœŸè®¿é—®ï¼Œä¹Ÿä¸ä¼šå¯¼è‡´æ‹’ç»æœåŠ¡ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²æˆ–ä»»æ„ä»£ç æ‰§è¡Œç­‰å®‰å…¨é£é™©ã€‚å› æ­¤æ— éœ€æä¾›å¤ç°è„šæœ¬ã€‚è¯¥ Issue ä¸»è¦æ˜¯ä¸€ä¸ªä½“éªŒæˆ–åŠŸèƒ½ä¸€è‡´æ€§çš„é—®é¢˜ï¼Œæœªæ¶‰åŠå®‰å…¨æ¼æ´èŒƒç•´ã€‚

---


## Issue #131175 No-op Server-Side Apply (SSA) calls updates resourceVersion and metadata.managedFields[].time

- Issue é“¾æ¥ï¼š[#131175](https://github.com/kubernetes/kubernetes/issues/131175)

### Issue å†…å®¹

#### What happened?

I have a controller-runtime based controller that creates various resources using Server-Side Apply (SSA) using `Client.Patch(ctx, obj, client.Apply)` where `obj` is of Unstructured type. In the case of ClusterRoleBindings with an empty array for `subjects` (subjects: []`, I see resourceVersion increment on SSA using the Client even when the entire object is the same as before. 

The resulting object applied to the cluster is as follows:
```
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: gloo
    app.kubernetes.io/managed-by: gloo-operator
    gloo: rbac
    operator.gloo.solo.io/managed-by: gatewaycontroller.gloo-gateway
  name: gloo-upstream-mutator-binding-gloo-system
  ownerReferences:
  - apiVersion: operator.gloo.solo.io/v1
    blockOwnerDeletion: true
    controller: true
    kind: GatewayController
    name: gloo-gateway
    uid: 428928a5-e3b0-4b44-bdb9-846213aedc14
  uid: 8851a60d-2577-4618-a824-7c39d7a179d6
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: gloo-upstream-mutator-gloo-system
subjects: []
```

The constant resourceVersion bumps cause infinite reconciliation loops. 

Logs from my reconciler that reconciles GatewayController CRs, which is an OwnerReference for the ClusterRoleBinding created above.

<details>

<summary>Controller logs</summary>

```
{"time":"2025-04-04T11:05:19.849442305-07:00","level":"INFO","msg":"reconciling","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"60ca6e8d-8cbe-4414-ad19-92ce093a0b47"}
{"time":"2025-04-04T11:05:21.039420497-07:00","level":"DEBUG","msg":"DBG before patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":""}
{"time":"2025-04-04T11:05:21.039541053-07:00","level":"TRACE","msg":"reconciling","controller":"GatewayController","name":"gloo-gateway","kind":"ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:21.039822559-07:00","level":"DEBUG-4","msg":"Request Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"60ca6e8d-8cbe-4414-ad19-92ce093a0b47","body":"{\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"kind\":\"ClusterRoleBinding\",\"metadata\":{\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"blockOwnerDeletion\":true,\"controller\":true,\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\"}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"},\"subjects\":[]}"}
{"time":"2025-04-04T11:05:21.134339032-07:00","level":"DEBUG","msg":"update event","component":"k8s-events","controller":"GatewayController","kind":"*v1.ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:21.134334874-07:00","level":"DEBUG-4","msg":"Response Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"60ca6e8d-8cbe-4414-ad19-92ce093a0b47","body":"{\"kind\":\"ClusterRoleBinding\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"metadata\":{\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"uid\":\"8851a60d-2577-4618-a824-7c39d7a179d6\",\"resourceVersion\":\"715344\",\"creationTimestamp\":\"2025-04-03T19:34:24Z\",\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\",\"controller\":true,\"blockOwnerDeletion\":true}],\"managedFields\":[{\"manager\":\"test\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{}}},{\"manager\":\"solo.io/gloo-operator\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"time\":\"2025-04-04T18:05:21Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{},\"f:subjects\":{}}}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"}}\n"}
{"time":"2025-04-04T11:05:21.134744019-07:00","level":"DEBUG","msg":"DBG after patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":"715344"}
{"time":"2025-04-04T11:05:21.134764397-07:00","level":"INFO","msg":"reconcile done","controller":"GatewayController","name":"gloo-gateway","requeue":false}
{"time":"2025-04-04T11:05:21.134802328-07:00","level":"INFO","msg":"Starting EventSource","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","source":{"Type":{"metadata":{"creationTimestamp":null},"spec":{"controllerName":""},"status":{}},"Cache":{"Scheme":{},"RESTMapper":{}},"Handler":{},"Predicates":[{}]}}
{"time":"2025-04-04T11:05:21.13527954-07:00","level":"INFO","msg":"reconciling","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"1c2eb894-a1a9-4890-b5b0-4033c000b3db"}
{"time":"2025-04-04T11:05:22.167487732-07:00","level":"DEBUG","msg":"DBG before patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":""}
{"time":"2025-04-04T11:05:22.167526584-07:00","level":"TRACE","msg":"reconciling","controller":"GatewayController","name":"gloo-gateway","kind":"ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:22.167585054-07:00","level":"DEBUG-4","msg":"Request Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"1c2eb894-a1a9-4890-b5b0-4033c000b3db","body":"{\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"kind\":\"ClusterRoleBinding\",\"metadata\":{\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"blockOwnerDeletion\":true,\"controller\":true,\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\"}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"},\"subjects\":[]}"}
{"time":"2025-04-04T11:05:22.259725328-07:00","level":"DEBUG","msg":"update event","component":"k8s-events","controller":"GatewayController","kind":"*v1.ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:22.260006613-07:00","level":"DEBUG-4","msg":"Response Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"1c2eb894-a1a9-4890-b5b0-4033c000b3db","body":"{\"kind\":\"ClusterRoleBinding\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"metadata\":{\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"uid\":\"8851a60d-2577-4618-a824-7c39d7a179d6\",\"resourceVersion\":\"715350\",\"creationTimestamp\":\"2025-04-03T19:34:24Z\",\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\",\"controller\":true,\"blockOwnerDeletion\":true}],\"managedFields\":[{\"manager\":\"test\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{}}},{\"manager\":\"solo.io/gloo-operator\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"time\":\"2025-04-04T18:05:22Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{},\"f:subjects\":{}}}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"}}\n"}
{"time":"2025-04-04T11:05:22.260227096-07:00","level":"DEBUG","msg":"DBG after patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":"715350"}
{"time":"2025-04-04T11:05:22.260241252-07:00","level":"INFO","msg":"reconcile done","controller":"GatewayController","name":"gloo-gateway","requeue":false}
{"time":"2025-04-04T11:05:22.260306745-07:00","level":"INFO","msg":"reconciling","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"bd02ebf6-fc01-44db-8b1e-d1dd5dac695e"}
{"time":"2025-04-04T11:05:23.18185363-07:00","level":"DEBUG","msg":"DBG before patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":""}
{"time":"2025-04-04T11:05:23.181900417-07:00","level":"TRACE","msg":"reconciling","controller":"GatewayController","name":"gloo-gateway","kind":"ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:23.181959288-07:00","level":"DEBUG-4","msg":"Request Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"bd02ebf6-fc01-44db-8b1e-d1dd5dac695e","body":"{\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"kind\":\"ClusterRoleBinding\",\"metadata\":{\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"blockOwnerDeletion\":true,\"controller\":true,\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\"}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"},\"subjects\":[]}"}
{"time":"2025-04-04T11:05:23.26796157-07:00","level":"DEBUG","msg":"update event","component":"k8s-events","controller":"GatewayController","kind":"*v1.ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:23.269135375-07:00","level":"DEBUG-4","msg":"Response Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"bd02ebf6-fc01-44db-8b1e-d1dd5dac695e","body":"{\"kind\":\"ClusterRoleBinding\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"metadata\":{\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"uid\":\"8851a60d-2577-4618-a824-7c39d7a179d6\",\"resourceVersion\":\"715355\",\"creationTimestamp\":\"2025-04-03T19:34:24Z\",\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\",\"controller\":true,\"blockOwnerDeletion\":true}],\"managedFields\":[{\"manager\":\"test\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{}}},{\"manager\":\"solo.io/gloo-operator\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"time\":\"2025-04-04T18:05:23Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{},\"f:subjects\":{}}}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"}}\n"}
{"time":"2025-04-04T11:05:23.269257834-07:00","level":"DEBUG","msg":"DBG after patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":"715355"}
{"time":"2025-04-04T11:05:23.269265027-07:00","level":"INFO","msg":"reconcile done","controller":"GatewayController","name":"gloo-gateway","requeue":false}
{"time":"2025-04-04T11:05:23.269321613-07:00","level":"INFO","msg":"reconciling","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"159cf501-7635-46ac-a648-07d7e7714379"}
{"time":"2025-04-04T11:05:24.110189038-07:00","level":"DEBUG","msg":"DBG before patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":""}
{"time":"2025-04-04T11:05:24.110232379-07:00","level":"TRACE","msg":"reconciling","controller":"GatewayController","name":"gloo-gateway","kind":"ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:24.110287492-07:00","level":"DEBUG-4","msg":"Request Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"159cf501-7635-46ac-a648-07d7e7714379","body":"{\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"kind\":\"ClusterRoleBinding\",\"metadata\":{\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"blockOwnerDeletion\":true,\"controller\":true,\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\"}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"},\"subjects\":[]}"}
{"time":"2025-04-04T11:05:24.21306735-07:00","level":"DEBUG","msg":"update event","component":"k8s-events","controller":"GatewayController","kind":"*v1.ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:24.221289459-07:00","level":"DEBUG-4","msg":"Response Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"159cf501-7635-46ac-a648-07d7e7714379","body":"{\"kind\":\"ClusterRoleBinding\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"metadata\":{\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"uid\":\"8851a60d-2577-4618-a824-7c39d7a179d6\",\"resourceVersion\":\"715361\",\"creationTimestamp\":\"2025-04-03T19:34:24Z\",\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\",\"controller\":true,\"blockOwnerDeletion\":true}],\"managedFields\":[{\"manager\":\"test\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{}}},{\"manager\":\"solo.io/gloo-operator\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"time\":\"2025-04-04T18:05:24Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{},\"f:subjects\":{}}}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"}}\n"}
{"time":"2025-04-04T11:05:24.221590482-07:00","level":"DEBUG","msg":"DBG after patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":"715361"}
{"time":"2025-04-04T11:05:24.221610059-07:00","level":"INFO","msg":"reconcile done","controller":"GatewayController","name":"gloo-gateway","requeue":false}
{"time":"2025-04-04T11:05:24.221716838-07:00","level":"INFO","msg":"reconciling","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"e9aa3054-66ac-4f75-8198-0e36f97c3f63"}
{"time":"2025-04-04T11:05:25.038961847-07:00","level":"DEBUG","msg":"DBG before patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":""}
{"time":"2025-04-04T11:05:25.039003995-07:00","level":"TRACE","msg":"reconciling","controller":"GatewayController","name":"gloo-gateway","kind":"ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:25.039085538-07:00","level":"DEBUG-4","msg":"Request Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"e9aa3054-66ac-4f75-8198-0e36f97c3f63","body":"{\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"kind\":\"ClusterRoleBinding\",\"metadata\":{\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"blockOwnerDeletion\":true,\"controller\":true,\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\"}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"},\"subjects\":[]}"}
{"time":"2025-04-04T11:05:25.129147224-07:00","level":"DEBUG","msg":"update event","component":"k8s-events","controller":"GatewayController","kind":"*v1.ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:25.147824027-07:00","level":"DEBUG-4","msg":"Response Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"e9aa3054-66ac-4f75-8198-0e36f97c3f63","body":"{\"kind\":\"ClusterRoleBinding\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"metadata\":{\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"uid\":\"8851a60d-2577-4618-a824-7c39d7a179d6\",\"resourceVersion\":\"715368\",\"creationTimestamp\":\"2025-04-03T19:34:24Z\",\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\",\"controller\":true,\"blockOwnerDeletion\":true}],\"managedFields\":[{\"manager\":\"test\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{}}},{\"manager\":\"solo.io/gloo-operator\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"time\":\"2025-04-04T18:05:25Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{},\"f:subjects\":{}}}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"}}\n"}
{"time":"2025-04-04T11:05:25.148016978-07:00","level":"DEBUG","msg":"DBG after patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":"715368"}
{"time":"2025-04-04T11:05:25.148030082-07:00","level":"INFO","msg":"reconcile done","controller":"GatewayController","name":"gloo-gateway","requeue":false}
{"time":"2025-04-04T11:05:25.14812521-07:00","level":"INFO","msg":"reconciling","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"ec78a951-2894-4360-9e73-5a75a670b8a9"}
{"time":"2025-04-04T11:05:25.977047899-07:00","level":"DEBUG","msg":"DBG before patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":""}
{"time":"2025-04-04T11:05:25.977096409-07:00","level":"TRACE","msg":"reconciling","controller":"GatewayController","name":"gloo-gateway","kind":"ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:25.977157443-07:00","level":"DEBUG-4","msg":"Request Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"ec78a951-2894-4360-9e73-5a75a670b8a9","body":"{\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"kind\":\"ClusterRoleBinding\",\"metadata\":{\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"blockOwnerDeletion\":true,\"controller\":true,\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\"}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"},\"subjects\":[]}"}
{"time":"2025-04-04T11:05:26.064699551-07:00","level":"DEBUG","msg":"update event","component":"k8s-events","controller":"GatewayController","kind":"*v1.ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:26.06539972-07:00","level":"DEBUG-4","msg":"Response Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"ec78a951-2894-4360-9e73-5a75a670b8a9","body":"{\"kind\":\"ClusterRoleBinding\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"metadata\":{\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"uid\":\"8851a60d-2577-4618-a824-7c39d7a179d6\",\"resourceVersion\":\"715373\",\"creationTimestamp\":\"2025-04-03T19:34:24Z\",\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\",\"controller\":true,\"blockOwnerDeletion\":true}],\"managedFields\":[{\"manager\":\"test\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{}}},{\"manager\":\"solo.io/gloo-operator\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"time\":\"2025-04-04T18:05:26Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{},\"f:subjects\":{}}}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"}}\n"}
{"time":"2025-04-04T11:05:26.065578204-07:00","level":"DEBUG","msg":"DBG after patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":"715373"}
{"time":"2025-04-04T11:05:26.065589615-07:00","level":"INFO","msg":"reconcile done","controller":"GatewayController","name":"gloo-gateway","requeue":false}
{"time":"2025-04-04T11:05:26.065674955-07:00","level":"INFO","msg":"reconciling","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"6c5350d2-aa90-45e6-ad17-5358e52c76df"}
{"time":"2025-04-04T11:05:26.964328778-07:00","level":"DEBUG","msg":"DBG before patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":""}
{"time":"2025-04-04T11:05:26.964370346-07:00","level":"TRACE","msg":"reconciling","controller":"GatewayController","name":"gloo-gateway","kind":"ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:26.964424768-07:00","level":"DEBUG-4","msg":"Request Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"6c5350d2-aa90-45e6-ad17-5358e52c76df","body":"{\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"kind\":\"ClusterRoleBinding\",\"metadata\":{\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"blockOwnerDeletion\":true,\"controller\":true,\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\"}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"},\"subjects\":[]}"}
{"time":"2025-04-04T11:05:27.067780249-07:00","level":"DEBUG-4","msg":"Response Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"6c5350d2-aa90-45e6-ad17-5358e52c76df","body":"{\"kind\":\"ClusterRoleBinding\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"metadata\":{\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"uid\":\"8851a60d-2577-4618-a824-7c39d7a179d6\",\"resourceVersion\":\"715377\",\"creationTimestamp\":\"2025-04-03T19:34:24Z\",\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\",\"controller\":true,\"blockOwnerDeletion\":true}],\"managedFields\":[{\"manager\":\"test\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{}}},{\"manager\":\"solo.io/gloo-operator\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"time\":\"2025-04-04T18:05:27Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{},\"f:subjects\":{}}}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"}}\n"}
{"time":"2025-04-04T11:05:27.067785829-07:00","level":"DEBUG","msg":"update event","component":"k8s-events","controller":"GatewayController","kind":"*v1.ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:27.067987757-07:00","level":"DEBUG","msg":"DBG after patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":"715377"}
{"time":"2025-04-04T11:05:27.068000801-07:00","level":"INFO","msg":"reconcile done","controller":"GatewayController","name":"gloo-gateway","requeue":false}
{"time":"2025-04-04T11:05:27.068082835-07:00","level":"INFO","msg":"reconciling","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"b3a79bfe-a5a3-4727-b398-9f1e5be7bab8"}
{"time":"2025-04-04T11:05:27.937475733-07:00","level":"DEBUG","msg":"DBG before patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":""}
{"time":"2025-04-04T11:05:27.937514335-07:00","level":"TRACE","msg":"reconciling","controller":"GatewayController","name":"gloo-gateway","kind":"ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:27.937581009-07:00","level":"DEBUG-4","msg":"Request Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"b3a79bfe-a5a3-4727-b398-9f1e5be7bab8","body":"{\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"kind\":\"ClusterRoleBinding\",\"metadata\":{\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"blockOwnerDeletion\":true,\"controller\":true,\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\"}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"},\"subjects\":[]}"}
{"time":"2025-04-04T11:05:28.034370458-07:00","level":"DEBUG-4","msg":"Response Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"b3a79bfe-a5a3-4727-b398-9f1e5be7bab8","body":"{\"kind\":\"ClusterRoleBinding\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"metadata\":{\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"uid\":\"8851a60d-2577-4618-a824-7c39d7a179d6\",\"resourceVersion\":\"715377\",\"creationTimestamp\":\"2025-04-03T19:34:24Z\",\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\",\"controller\":true,\"blockOwnerDeletion\":true}],\"managedFields\":[{\"manager\":\"test\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{}}},{\"manager\":\"solo.io/gloo-operator\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"time\":\"2025-04-04T18:05:27Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{},\"f:subjects\":{}}}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"}}\n"}
{"time":"2025-04-04T11:05:28.034599266-07:00","level":"DEBUG","msg":"DBG after patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":"715377"}
{"time":"2025-04-04T11:05:28.03461255-07:00","level":"INFO","msg":"reconcile done","controller":"GatewayController","name":"gloo-gateway","requeue":false}
```
</details>

As seen from the logs, the request body is the same always, the responde body only differs in the `resourceVersion`  and `metadata.managedFields[].time` fields. 

A `--watch` on the ClusterRoleBinding indicates that it is not being removed (age doesn't change), but I see updates on every SSA call that my controller makes:
```
gloo-upstream-mutator-binding-gloo-system                       ClusterRole/gloo-upstream-mutator-gloo-system                               22h
gloo-upstream-mutator-binding-gloo-system                       ClusterRole/gloo-upstream-mutator-gloo-system                               22h
gloo-upstream-mutator-binding-gloo-system                       ClusterRole/gloo-upstream-mutator-gloo-system                               22h
gloo-upstream-mutator-binding-gloo-system                       ClusterRole/gloo-upstream-mutator-gloo-system                               22h
gloo-upstream-mutator-binding-gloo-system                       ClusterRole/gloo-upstream-mutator-gloo-system                               22h
gloo-upstream-mutator-binding-gloo-system                       ClusterRole/gloo-upstream-mutator-gloo-system                               22h
gloo-upstream-mutator-binding-gloo-system                       ClusterRole/gloo-upstream-mutator-gloo-system                               22h
```

I have a Predicate to log events within the reconciler that trigger reconciliation, and the only event I see is an Update event corresponding to the Patched ClusterRoleBinding after every SSA call:
```
{"time":"2025-04-04T11:05:27.067785829-07:00","level":"DEBUG","msg":"update event","component":"k8s-events","controller":"GatewayController","kind":"*v1.ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
```

#### What did you expect to happen?

I expect repeated SSA calls for the same object to not bump the resourceVersion. 

#### How can we reproduce it (as minimally and precisely as possible)?


```
func TestDbg(t *testing.T) {
	a := assert.New(t)

	yamlToUnstructured := func(yamlData string) (unstructured.Unstructured, error) {
		decoder := yaml.NewYAMLOrJSONDecoder(strings.NewReader(yamlData), 4096)
		var obj unstructured.Unstructured
		err := decoder.Decode(&obj)
		return obj, err
	}

	mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{
		Scheme: scheme.Scheme,
		Client: client.Options{
			Cache: &client.CacheOptions{
				// The Client creates a ListWatch for all resource kinds that are requested with Get().
				// Disable the cache for Objects that can consume massive amounts of memory.
				DisableFor: []client.Object{
					&rbacv1.ClusterRoleBinding{},
				},
			},
		},
	})
	a.NoError(err)
	a.NotNil(mgr)

	c := mgr.GetClient()
	a.NotNil(c)

	r1 := `apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: test-bug
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: test-bug
subjects: []
`
	origObj, err := yamlToUnstructured(r1)
	a.NoError(err)
	a.NotNil(origObj)

	for i := range 5 {
		obj := origObj
		t.Logf("iteration: %d", i)
		obj.SetManagedFields(nil)
		err := c.Patch(context.TODO(), &obj, client.Apply, client.FieldOwner("test"), client.ForceOwnership)
		a.NoError(err)
		out := &rbacv1.ClusterRoleBinding{}
		err = c.Get(context.TODO(), client.ObjectKeyFromObject(&obj), out)
		a.NoError(err)
		t.Logf("resourceVersion: %s", out.GetResourceVersion())
	}
}
```
`resourceVersion` increments:
```
obj_test.go:417: iteration: 0
obj_test.go:424: resourceVersion: 739527
obj_test.go:417: iteration: 1
obj_test.go:424: resourceVersion: 739527
obj_test.go:417: iteration: 2
obj_test.go:424: resourceVersion: 739529
obj_test.go:417: iteration: 3
obj_test.go:424: resourceVersion: 739529
obj_test.go:417: iteration: 4
obj_test.go:424: resourceVersion: 739529
```

With `subjects` not set, `resourceVersion` does not increment:
```
obj_test.go:416: iteration: 0
obj_test.go:423: resourceVersion: 739026
obj_test.go:416: iteration: 1
obj_test.go:423: resourceVersion: 739026
obj_test.go:416: iteration: 2
obj_test.go:423: resourceVersion: 739026
obj_test.go:416: iteration: 3
obj_test.go:423: resourceVersion: 739026
obj_test.go:416: iteration: 4
obj_test.go:423: resourceVersion: 739026
```


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.30.11-eks-bcf3d70
```

</details>


#### Cloud provider

EKS


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»è¯¥ Issue æè¿°å†…å®¹æ¥çœ‹ï¼Œé—®é¢˜çš„æ ¹æºåœ¨äº Server-Side Apply (SSA) å¯¹åŒä¸€å¯¹è±¡åå¤æ‰§è¡Œ Patch æ—¶ï¼Œå¯¼è‡´ resourceVersion ä»¥åŠ managedFields ä¸­çš„æ—¶é—´å­—æ®µè¢«æ›´æ–°ï¼Œä»è€Œè§¦å‘æ— é™å¾ªç¯çš„ Reconcileã€‚è¯¥é—®é¢˜æ˜¯ä¸€ä¸ªé‡å¤æ›´æ–°é€»è¾‘çš„ç¼ºé™·æˆ–ä½¿ç”¨æ–¹å¼ä¸å½“ï¼Œå¹¶ä¸æ¶‰åŠè¶Šæƒè®¿é—®ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€å‘½ä»¤æ‰§è¡Œã€ææƒæˆ–å®¹å™¨é€ƒé€¸ç­‰å®‰å…¨æ¼æ´ã€‚å› æ­¤ï¼Œä»å®‰å…¨è§’åº¦åˆ¤æ–­ï¼Œè¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½/é€»è¾‘é—®é¢˜è€Œéå®‰å…¨é£é™©ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Issue ä¸­æè¿°çš„é—®é¢˜å¹¶ä¸ä¼šå¸¦æ¥æœªæˆæƒè®¿é—®ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²æˆ–æ½œåœ¨çš„æ”»å‡»é¢æ‰©å¼ ï¼Œå±äºé¡¹ç›®ä½¿ç”¨ä¸­çš„é€»è¾‘/åŠŸèƒ½æ€§ç¼ºé™·ï¼Œä¸è§†ä¸ºå®‰å…¨æ¼æ´ã€‚æ•…é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ï¼Œæ— éœ€æä¾›å¤ç°è„šæœ¬ã€‚è‹¥è¦è§£å†³è¯¥é—®é¢˜ï¼Œå¯åœ¨ä¸šåŠ¡é€»è¾‘æˆ– SSA è°ƒç”¨ç­–ç•¥ä¸Šè¿›è¡Œä¼˜åŒ–ï¼Œç¡®ä¿åœ¨æ²¡æœ‰å®é™…å˜æ›´æ—¶ä¸é‡å¤è°ƒç”¨ Patch æ–¹æ³•ã€‚

---


## Issue #131171 DaemonSets should be scheduled before Deployments on new Nodes

- Issue é“¾æ¥ï¼š[#131171](https://github.com/kubernetes/kubernetes/issues/131171)

### Issue å†…å®¹

#### What happened?

We're trying to reduce AWS EC2 costs by reducing the number of nodes to 0 automatically during times the enter cluster is not needed using an autoscaling_schedule (see e.g. https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/autoscaling_schedule). We have observed that, when a single node is started after no nodes at all have been available, the kube-scheduler may start a large number of Deployments on that node, leaving insufficient resources (in our case memory) for the pods of all DaemonSets to be started on that node. That situation was not rectified automatically after adding more nodes to the cluster, instead, I had to cordon the server and terminate some pods on the first node for them to be moved to another node.

It would be preferable if resources for DaemonSets were reserved before Workloads that are not tied to nodes (Deployments, ReplicaSets, ...) are scheduled.



#### What did you expect to happen?

Resources for DaemonSets were reserved before Workloads that are not tied to nodes (Deployments, ReplicaSets, ...) are scheduled.


#### How can we reproduce it (as minimally and precisely as possible)?

1. configure a cluster with 0 worker nodes
2. create daemonset that use a significant amount of resources
3. create Deployments and Replicasets to use all the resources (of at least one type) of one of the worker nodes to be added to the cluster
4. add one worker node
5. sometimes, Deployments and Replicasets are scheduled on the single worker node, leaving too few or no resources for Daemonsets
6. if all Daemonsets were scheduled, remove the worker node and re-add it
7. add more nodes

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®Issueæè¿°ï¼Œè¯¥é—®é¢˜ä¸»è¦ä¸Kubernetesè°ƒåº¦æœºåˆ¶åŠèµ„æºä¸è¶³æ—¶DaemonSetå’ŒDeploymentä¹‹é—´çš„å…ˆåè°ƒåº¦é¡ºåºæœ‰å…³ï¼Œå¹¶éå­˜åœ¨æ½œåœ¨çš„å®‰å…¨æ¼æ´ï¼Œä¹Ÿæœªæ¶‰åŠå®¹å™¨é€ƒé€¸ã€å‘½ä»¤æ‰§è¡Œã€æƒé™æå‡ã€æˆ–æ•æ„Ÿä¿¡æ¯æ³„éœ²ç­‰æƒ…å†µã€‚è¯¥Issueæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå¯¹Kubernetesè°ƒåº¦ç­–ç•¥çš„æ”¹è¿›æˆ–ç‰¹æ€§éœ€æ±‚ï¼Œæ— æ³•ç”¨äºæ„æˆæ”»å‡»è¡Œä¸ºæˆ–æœªç»æˆæƒçš„æ“ä½œï¼Œä¸å±äºå®‰å…¨é£é™©èŒƒç•´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥Issueå¹¶éå®‰å…¨é—®é¢˜ï¼Œå› æ­¤æ— éœ€å®‰å…¨ç›¸å…³POCã€‚
# å¦‚éœ€æµ‹è¯•Kubernetesè°ƒåº¦é¡ºåºï¼Œå¯ä»¥åœ¨æœ¬åœ°æ­å»ºä¸€ä¸ªæµ‹è¯•é›†ç¾¤ï¼Œ
# å°†èŠ‚ç‚¹æ•°é™ä¸º0åå†æ–°å¢èŠ‚ç‚¹å¹¶åˆ›å»ºå ç”¨å¤§é‡èµ„æºçš„DaemonSetå’ŒDeploymentï¼Œ
# ä»¥é‡ç°Issueä¸­æè¿°çš„æƒ…å†µã€‚
# ä¸‹é¢ç¤ºä¾‹è„šæœ¬ä»…ç¤ºèŒƒå¦‚ä½•åˆ›å»ºèµ„æºï¼Œä¸æ¶‰åŠä»»ä½•å®‰å…¨æ¼æ´åˆ©ç”¨ï¼Œ
# ä¸å¯è§†ä¸ºå®‰å…¨æµ‹è¯•POCï¼Œä»…ç”¨äºæ¼”ç¤ºè°ƒåº¦é¡ºåºé—®é¢˜ã€‚

import time
from kubernetes import client, config

def main():
    # è¯»å–é»˜è®¤kubeconfig
    config.load_kube_config()

    v1_apps = client.AppsV1Api()

    # ç¤ºä¾‹ï¼šåˆ›å»ºä¸€ä¸ªå ç”¨è¾ƒå¤šèµ„æºçš„DaemonSet
    daemonset_manifest = {
        "apiVersion": "apps/v1",
        "kind": "DaemonSet",
        "metadata": {
            "name": "test-daemonset",
            "labels": {
                "app": "test-daemonset"
            }
        },
        "spec": {
            "selector": {
                "matchLabels": {
                    "app": "test-daemonset"
                }
            },
            "template": {
                "metadata": {
                    "labels": {
                        "app": "test-daemonset"
                    }
                },
                "spec": {
                    "containers": [
                        {
                            "name": "test-container",
                            "image": "busybox",
                            "resources": {
                                "requests": {
                                    "memory": "512Mi",
                                    "cpu": "250m"
                                },
                                "limits": {
                                    "memory": "512Mi",
                                    "cpu": "500m"
                                }
                            },
                            "command": ["sh", "-c", "while true; do sleep 10; done"]
                        }
                    ]
                }
            },
            "updateStrategy": {
                "type": "RollingUpdate"
            }
        }
    }

    # åˆ›å»ºDaemonSet
    try:
        v1_apps.create_namespaced_daemon_set(
            namespace="default",
            body=daemonset_manifest
        )
        print("Created DaemonSet: test-daemonset")
    except client.exceptions.ApiException as e:
        print("Error creating DaemonSet:", e)

    # ç¤ºä¾‹ï¼šåˆ›å»ºä¸€ä¸ªæ¶ˆè€—å¤§é‡èµ„æºçš„Deployment
    deployment_manifest = {
        "apiVersion": "apps/v1",
        "kind": "Deployment",
        "metadata": {
            "name": "test-deployment",
            "labels": {
                "app": "test-deployment"
            }
        },
        "spec": {
            "replicas": 3,
            "selector": {
                "matchLabels": {
                    "app": "test-deployment"
                }
            },
            "template": {
                "metadata": {
                    "labels": {
                        "app": "test-deployment"
                    }
                },
                "spec": {
                    "containers": [
                        {
                            "name": "test-container",
                            "image": "busybox",
                            "resources": {
                                "requests": {
                                    "memory": "512Mi",
                                    "cpu": "250m"
                                },
                                "limits": {
                                    "memory": "512Mi",
                                    "cpu": "500m"
                                }
                            },
                            "command": ["sh", "-c", "while true; do sleep 10; done"]
                        }
                    ]
                }
            }
        }
    }

    # åˆ›å»ºDeployment
    try:
        v1_apps.create_namespaced_deployment(
            namespace="default",
            body=deployment_manifest
        )
        print("Created Deployment: test-deployment")
    except client.exceptions.ApiException as e:
        print("Error creating Deployment:", e)

    # ç­‰å¾…ç‰‡åˆ»è®©Kuberneteså¼€å§‹è°ƒåº¦
    time.sleep(30)

    print("è°ƒåº¦æ¼”ç¤ºè„šæœ¬æ‰§è¡Œå®Œæ¯•ã€‚è¯·åœ¨Kubernetesé›†ç¾¤ä¸­éªŒè¯DaemonSetå’ŒDeploymentçš„å®é™…è°ƒåº¦æƒ…å†µã€‚")

main()
```


---


## Issue #131163 Deployment may gets stuck after GracefulNodeShutdown

- Issue é“¾æ¥ï¼š[#131163](https://github.com/kubernetes/kubernetes/issues/131163)

### Issue å†…å®¹

#### What happened?

In our cluster, a deployment was created in the d8-system namespace that successfully generated a replicaset and started one pod. Later, a graceful shutdown was executed using shutdown -r -t 5, and after the server restarted, the deployment created a new replicaset and a new pod.

When the deployment reaches its progress deadline (600 seconds by default), it begins deleting pods from the current replicaset. However, logs show that even terminated pods (in an Error or Completed state) from the previous replicaset are still detected. 
As a result (that's my guess), the deployment does nothing for about 10 minutes before creating a new replicaset. If the `Error` pod is manually deleted, the deployment immediately creates a new replicaset. This suggests that the presence of a terminated pod is blocking progress.


Logs:

~13:39 Deployment timed out, then I see that the deployment removes pods from the current RS, then I see messages about â€œFound related podsâ€ and then for 10 minutes nothing happens. I just see the pods in Error status and nothing else

```
deckhouse-58b8bbfcb6-9shvv         0/1     Error               0          28m
deckhouse-7b7bd6c9bc-mv94c         1/2     Terminating         0          26s
```

```
I0402 13:39:11.545056       1 deployment_util.go:776] "Deployment timed out from last progress check" deployment="d8-system/deckhouse" timeout=false from="2025-04-02 13:38:46 +0000 UTC" now="2025-04-02 13:39:11.544986046 +0000 UTC m=+937.359430398"

I0402 13:39:11.545759       1 replica_set.go:621] "Too many replicas" replicaSet="d8-system/deckhouse-7b7bd6c9bc" need=0 deleting=1

I0402 13:39:11.545809       1 replica_set.go:248] "Found related ReplicaSets" replicaSet="d8-system/deckhouse-7b7bd6c9bc" relatedReplicaSets=["d8-system/deckhouse-55b654774c","d8-system/deckhouse-58b8bbfcb6","d8-system/deckhouse-7b7bd6c9bc"]
I0402 13:39:11.545885       1 replica_set.go:818] "Found related pods" kind="ReplicaSet" replicaSet="d8-system/deckhouse-7b7bd6c9bc" pods=["d8-system/deckhouse-58b8bbfcb6-9shvv","d8-system/deckhouse-7b7bd6c9bc-mv94c"]

```

No further deployment syncing occurs until about 10 minutes later. At 13:48:47, a new syncing event is logged:

I0402 13:48:47.000653       1 deployment_controller.go:594] "Started syncing deployment" deployment="d8-system/deckhouse" startTime="2025-04-02 13:48:47.000520919 +0000 UTC m=+1512.814965267"

This 10-minute pause indicates that the deployment controller is still considering the terminated/error pod as active.


#### What did you expect to happen?

I expect the deployment to immediately attempt to create a new replicas

#### How can we reproduce it (as minimally and precisely as possible)?

- Create a deployment (e.g., deckhouse) in the d8-system namespace with one replica.

- The deployment creates a replicaset and starts one pod.

- Execute a graceful shutdown using `shutdown -r -t 5`

- After the server restarts, observe that the deployment creates a new RS and a new pod.

- When one pod fails (enters an Error/Completed state) and the deployment reaches its 600-second progress deadline, the deployment stalls for about 10 minutes.

Manually delete the terminated/error pod and note that the deployment immediately creates a new replicaset and pod.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.30.10
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.10
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
PRETTY_NAME="Ubuntu 22.04.5 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

Linux zykov-master-0 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»Issueå†…å®¹æ¥çœ‹ï¼Œè¯¥é—®é¢˜ä¸»è¦æ˜¯å› ä¸ºKubernetes Deploymentåœ¨èŠ‚ç‚¹é‡å¯åï¼Œé‡åˆ°å·²ç»å¤„äºErroræˆ–CompletedçŠ¶æ€çš„æ—§Podï¼Œå¯¼è‡´Deploymentæ§åˆ¶å™¨åœ¨åç»­è°ƒåº¦æ—¶é™·å…¥çº¦10åˆ†é’Ÿçš„ç­‰å¾…æœŸã€‚è¯¥ç°è±¡æœ¬è´¨ä¸Šæ˜¯Deploymentæ§åˆ¶å™¨å¯¹æ—§å‰¯æœ¬é›†å’Œæ—§PodçŠ¶æ€å¤„ç†å‡ºç°çš„é€»è¾‘å»¶è¿Ÿï¼Œé˜»å¡åç»­ReplicaSetçš„åˆ›å»ºã€‚  
é€šç¯‡å¹¶æœªä½“ç°å‡ºä¿¡æ¯æ³„éœ²ã€å‘½ä»¤æ‰§è¡Œã€ææƒã€å®¹å™¨é€ƒé€¸ç­‰å¯è¢«æ¶æ„åˆ©ç”¨çš„å®‰å…¨é£é™©ï¼Œä¹Ÿä¸æ¶‰åŠå‡­æ®æ³„éœ²æˆ–è€…éé¢„æœŸçš„å¤šç”¨æˆ·ç¯å¢ƒä¸‹çš„æƒé™æ»¥ç”¨ã€‚å› æ­¤ï¼Œè¯¥é—®é¢˜æ˜¯ä¸€ä¸ªæ­£å¸¸çš„åŠŸèƒ½æˆ–å¯é æ€§Bugï¼Œä¸æ¶‰åŠå®‰å…¨æ¼æ´åœºæ™¯ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥Issueå¹¶éå®‰å…¨é—®é¢˜ï¼Œå› æ­¤æ— éœ€ç¼–å†™å®‰å…¨æ¼æ´å¤ç°è„šæœ¬ã€‚
# å¦‚éœ€æ¼”ç¤ºIssueç°è±¡ï¼Œå¯ä½¿ç”¨Pythonè°ƒç”¨kuberneteså®¢æˆ·ç«¯åˆ›å»ºDeploymentå¹¶æ‰‹åŠ¨é‡å¯èŠ‚ç‚¹ï¼Œ
# è§‚å¯ŸDeploymentå¯¹è±¡åœ¨PodçŠ¶æ€ä¸ºError/Completedæ—¶çš„è°ƒåº¦é€»è¾‘ï¼Œä½†è¿™å¹¶éå®‰å…¨é£é™©POCç¤ºä¾‹ã€‚
import time
from kubernetes import client, config
from kubernetes.client.rest import ApiException

def main():
    """
    æ¼”ç¤ºï¼šéƒ¨ç½²ä¸€ä¸ªDeploymentåï¼Œæ¨¡æ‹ŸèŠ‚ç‚¹é‡å¯ï¼Œè§‚å¯ŸDeploymentçŠ¶æ€å¡ä½çš„ç°è±¡ã€‚
    ä»…ä¾›åŠŸèƒ½æµ‹è¯•ï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚
    """

    # å‡è®¾å·²åœ¨æœ¬åœ°é…ç½® KUBECONFIG
    config.load_kube_config()

    apps_v1 = client.AppsV1Api()
    core_v1 = client.CoreV1Api()

    namespace = "d8-system"
    deployment_name = "demo-deployment"

    # 1. åˆ›å»ºä¸€ä¸ªæœ€å°åŒ–çš„ Deployment
    deployment_manifest = {
        "metadata": {"name": deployment_name},
        "spec": {
            "replicas": 1,
            "selector": {"matchLabels": {"app": "demo"}},
            "template": {
                "metadata": {"labels": {"app": "demo"}},
                "spec": {
                    "containers": [{
                        "name": "nginx",
                        "image": "nginx:latest",
                    }]
                },
            },
        },
    }

    try:
        print("åˆ›å»ºDeployment...")
        apps_v1.create_namespaced_deployment(
            body=deployment_manifest,
            namespace=namespace
        )
    except ApiException as e:
        print(f"åˆ›å»ºDeploymentå¤±è´¥: {e}")

    print("Deploymentå·²åˆ›å»ºï¼Œç­‰å¾…æ•°ç§’åæ¨¡æ‹ŸèŠ‚ç‚¹é‡å¯å¹¶æŸ¥çœ‹DeploymentçŠ¶æ€...")
    time.sleep(30)

    # æ­¤å¤„ä»…æ‰“å°DeploymentçŠ¶æ€ï¼Œå®é™…ç¯å¢ƒä¸­éœ€è¦åœ¨å¤–éƒ¨æ‰§è¡ŒèŠ‚ç‚¹é‡å¯æ“ä½œ
    # ç„¶åç­‰å¾…Podè¿›å…¥Error/Completedå¹¶é‡æ–°å¯åŠ¨è§‚å¯Ÿ
    try:
        dep = apps_v1.read_namespaced_deployment(name=deployment_name, namespace=namespace)
        print(f"DeploymentçŠ¶æ€: {dep.status}")
    except ApiException as e:
        print(f"è¯»å–DeploymentçŠ¶æ€å¤±è´¥: {e}")

    print("è„šæœ¬ç»“æŸï¼Œè‹¥éœ€é‡ç°Issueï¼Œè¯·æ‰‹åŠ¨è¿›è¡ŒèŠ‚ç‚¹é‡å¯å¹¶è§‚å¯ŸDeploymentåœ¨Error PodçŠ¶æ€ä¸‹çš„è°ƒåº¦é€»è¾‘ã€‚")
```


**è§£é‡Šè¯´æ˜ï¼š**

1. æ­¤è„šæœ¬ä»…æ¼”ç¤ºå¦‚ä½•åˆ›å»ºä¸€ä¸ªæœ€å°åŒ–Deploymentï¼Œå¹¶æç¤ºåœ¨å®é™…ç¯å¢ƒä¸­è¿›è¡ŒèŠ‚ç‚¹é‡å¯åï¼Œè§‚å¯ŸDeploymentåœ¨Podå¤„äºError/CompletedçŠ¶æ€æ—¶çš„è¡¨ç°ã€‚  
2. ç”±äºæœ¬Issueä¸æ˜¯å®‰å…¨é£é™©ï¼Œä¸ä¼šé€ æˆå¯è¢«åˆ©ç”¨çš„æ¼æ´ï¼Œä¹Ÿä¸æ¶‰åŠé«˜é£é™©æˆ–ä½é£é™©çš„å®‰å…¨é—®é¢˜ï¼Œå› æ­¤ä¸éœ€è¦ç¼–å†™ä¸“é—¨çš„å®‰å…¨POCè„šæœ¬ã€‚  
3. è¯¥ç¤ºä¾‹é€šè¿‡kuberneteså®¢æˆ·ç«¯åº“è°ƒç”¨é›†ç¾¤APIå®Œæˆå¯¹è±¡åˆ›å»ºï¼Œæ— éœ€å¤–éƒ¨å‘½ä»¤æˆ–å‡­æ®æ˜æ–‡ã€‚è„šæœ¬æ‰§è¡Œåå¯åœ¨2åˆ†é’Ÿå†…æ­£å¸¸ç»“æŸã€‚  
4. æœ¬ç¤ºä¾‹ä»…ä¾›å¯¹éƒ¨ç½²å¡ä½ç°è±¡è¿›è¡ŒåŠŸèƒ½æ€§éªŒè¯ï¼Œä¸æ¶‰åŠä»»ä½•å®‰å…¨æ¼æ´çš„å¤ç°ã€‚

---


## Issue #131152 Same RWO PVC ends up on different nodes

- Issue é“¾æ¥ï¼š[#131152](https://github.com/kubernetes/kubernetes/issues/131152)

### Issue å†…å®¹

#### What happened?

I applied the following configuration that has two pods using the same RWO PVC.

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: vol1
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Mi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim1
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Mi
---
apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: data
      resources:
        limits:
          cpu: "1"
          memory: "200Mi"
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: claim1
---
apiVersion: v1
kind: Pod
metadata:
  name: pod2
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: data
      resources:
        limits:
          cpu: "1"
          memory: "200Mi"
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: claim1

```

According to #103305 and #26567, pods could be scheduled on different nodes, and actually were. But, contrary to these issues, both pods were able to attach to the volume (s?) and run just fine.

It really seems that the PVC "used" both nodes because data written by both pods were present on each corresponding node, and this happened **without any notice**.

Is this expected behavior?



#### What did you expect to happen?

Volume attachment should fail for at least one of the two pods.

#### How can we reproduce it (as minimally and precisely as possible)?

By applying the provided configuration.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.30.4+rke2r1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.5 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux ubuntu-nic 6.8.0-57-generic #59~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Mar 19 17:07:41 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†ä¸€ä¸ªä½¿ç”¨äº† ReadWriteOnce (RWO) ç±»å‹ PVC çš„é…ç½®ï¼Œä½†å½“ä¸¤ä¸ª Pod åˆ†åˆ«è¢«è°ƒåº¦åˆ°ä¸åŒèŠ‚ç‚¹æ—¶ï¼Œå®ƒä»¬éƒ½èƒ½å¤Ÿæ­£å¸¸å†™å…¥æœ¬åœ°ç›®å½•å¹¶è¿è¡Œã€‚è¯¥é—®é¢˜æ›´å¤šæ˜¯ç”±äºå¯¹ Kubernetes RWO è¯­ä¹‰æˆ–åº•å±‚ Volume/StorageClass è¡Œä¸ºçš„å›°æƒ‘æˆ–ä¸å½“ä½¿ç”¨å¯¼è‡´çš„èµ„æºçŠ¶æ€ä¸ä¸€è‡´æˆ–å¯èƒ½çš„æ•°æ®åŒæ­¥é—®é¢˜ï¼Œå¹¶ä¸æ¶‰åŠåˆ°æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€æƒé™æå‡ã€ä»»æ„ä»£ç æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰å®‰å…¨é£é™©åœºæ™¯ã€‚

ä»æè¿°çœ‹ï¼Œè¯¥è¡Œä¸ºå¹¶æ²¡æœ‰ç›´æ¥é€ æˆè¶Šæƒè®¿é—®ã€æ‹’ç»æœåŠ¡æˆ–å…¶ä»–å¯ä»¥è¢«åˆ©ç”¨çš„å®‰å…¨å¨èƒï¼Œä¸”æœªæ¶‰åŠåˆ°æ•æ„Ÿæ•°æ®æ³„éœ²ï¼Œå› æ­¤ä¸å±äºå®‰å…¨æ¼æ´èŒƒç•´ï¼Œåº”å°†é£é™©è¯„çº§åˆ¤å®šä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

æ­¤ Issue ä»…åæ˜ äº†åœ¨ä½¿ç”¨ RWO PVC æ—¶å‡ºç°çš„é…ç½®æˆ–ä½¿ç”¨é—®é¢˜ï¼Œå…¶é£é™©ä¸åœ¨å®‰å…¨å±‚é¢ï¼Œè€Œåœ¨æ•°æ®ä¸€è‡´æ€§å’Œé¢„æœŸä½¿ç”¨è¯­ä¹‰æ–¹é¢ï¼Œä¸æ„æˆå®‰å…¨æ¼æ´ã€‚æ•…ä¸éœ€è¦æä¾›å®‰å…¨å¤ç°è„šæœ¬ã€‚

---


## Issue #131147 Duplicate value error when editing a port/protocol with Server Side Apply

- Issue é“¾æ¥ï¼š[#131147](https://github.com/kubernetes/kubernetes/issues/131147)

### Issue å†…å®¹

#### What happened?

When using `kubectl apply --server-side` on a deployment or service yaml file to edit a port number, it will result in this error:
`
The Service "test-service" is invalid: spec.ports[1].name: Duplicate value: "http"
`
The reason why we see 'duplicate' error when modifying port number is due to the protocol and port number being a composite key in managed fields:

```
managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      'f:spec':
        'f:ports':
          'k:{"port":1935,"protocol":"TCP"}':  # Port is part of the key
            .: {}
            'f:name': {}
            'f:port': {}
            'f:targetPort': {}
```
It uses a composite key {"port":1935,"protocol":"TCP"} to identify unique ports
When you try to change the port number while keeping the name the same, SSA sees this as a conflict because:
The old port entry is still managed by the previous apply
The new port entry with the same name but different port number is trying to be applied
SSA treats this as a duplicate port name conflict

#### What did you expect to happen?

server side applies without errors. The service/deployment reflects the edited port number.

#### How can we reproduce it (as minimally and precisely as possible)?

1.
```
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: test-service
  namespace: default
spec:
  ports:
    - name: http
      port: 80
      targetPort: 80
EOF
service/test-service created
```

2.
```
cat <<EOF | kubectl apply --server-side -f -
apiVersion: v1
kind: Service
metadata:
  name: test-service
  namespace: default
spec:
  ports:
    - name: http
      port: 8080  # Changed from 80 to 8080
      targetPort: 8080
EOF
The Service "test-service" is invalid: spec.ports[1].name: Duplicate value: "http"
```

#### Anything else we need to know?

This was originally found as part of an ArgoCD issue: (https://github.com/argoproj/argo-cd/issues/17717)
The issue uses a feature called Server Side Diff which depends on `kubectl apply --server-side [--dry-run=server]`

#### Kubernetes version

<details>

```console
kubectl version
Client Version: v1.32.1
Kustomize Version: v5.5.0
Server Version: v1.32.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
uname -a
Darwin macos-JK700GFY27 23.6.0 Darwin Kernel Version 23.6.0: Wed Jul 31 20:50:00 PDT 2024; root:xnu-10063.141.1.700.5~1/RELEASE_ARM64_T6031 arm64ersion, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»Issueæè¿°æ¥çœ‹ï¼Œè¿™æ˜¯ä¸€æ¬¡åœ¨Kubernetesä¸­ä½¿ç”¨Server Side Applyæ—¶å‡ºç°çš„é…ç½®/ä½¿ç”¨å†²çªï¼Œæ ¹æœ¬åŸå› æ˜¯æœåŠ¡ç«¯åœ¨è§£æServiceé…ç½®æ—¶æŠŠ"port"å’Œ"protocol"ä½œä¸ºç»„åˆé”®ï¼Œå¯¼è‡´åœ¨ä¿®æ”¹ç«¯å£å·ä½†ä¿ç•™ç›¸åŒåç§°æ—¶è§¦å‘äº†â€œDuplicate valueâ€æ ¡éªŒé”™è¯¯ã€‚  
è¯¥é—®é¢˜ä»…æ¶‰åŠServer Side Applyæœºåˆ¶ä¸‹çš„èµ„æºç®¡ç†å­—æ®µå†²çªï¼Œå¹¶æœªé€éœ²ä»»ä½•å¼‚å¸¸æ•æ„Ÿä¿¡æ¯æˆ–å…è®¸æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨æ¼æ´ï¼Œä¹Ÿæ²¡æœ‰æ¶‰åŠææƒã€æ‰§è¡Œä»»æ„å‘½ä»¤ã€ä¿¡æ¯æ³„éœ²æˆ–å…¶ä»–é«˜å±æ“ä½œï¼Œå› æ­¤ä¸å±äºå®‰å…¨é£é™©ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

è¯¥é—®é¢˜ä»…ä¸ºKubernetesèµ„æºç®¡ç†çš„ä½¿ç”¨æˆ–é€»è¾‘å†²çªï¼Œä¸å®‰å…¨æ— å…³ï¼Œå› æ­¤ä¸å­˜åœ¨å®‰å…¨é£é™©ï¼Œä¹Ÿæ— éœ€é¢å¤–çš„å®‰å…¨æµ‹è¯•è„šæœ¬æ¥å¤ç°ã€‚  
å¦‚éœ€åœ¨å¼€å‘æˆ–æµ‹è¯•ç¯å¢ƒä¸­æ¨¡æ‹Ÿæ­¤æŠ¥é”™ï¼Œå¯ç›´æ¥åœ¨Kubernetesé›†ç¾¤ä¸­ç”¨kubectlæˆ–kuberneteså®¢æˆ·ç«¯åº“è¿›è¡Œç›¸åŒæ“ä½œï¼Œå³å…ˆåˆ›å»ºä¸€ä¸ªServiceï¼Œç„¶åé€šè¿‡Server Side Applyä¿®æ”¹portå€¼å³å¯å¤ç°ã€‚  
å¯¹é¡¹ç›®æœ¬èº«è€Œè¨€ï¼Œåªéœ€å‚è€ƒå®˜æ–¹æ–‡æ¡£æˆ–Issueè®¨è®ºï¼Œåˆ©ç”¨æ­£ç¡®çš„æ“ä½œæ–¹å¼æˆ–èµ„æºé…ç½®æ¥é¿å…è¯¥å†²çªã€‚

---


## Issue #131134 Potential incorrect RealServer weight update logic in IPVS proxier initial sync

- Issue é“¾æ¥ï¼š[#131134](https://github.com/kubernetes/kubernetes/issues/131134)

### Issue å†…å®¹

#### What happened?

In the `syncProxyRules` logic of kube-proxy's IPVS mode, during the initial sync phase, the weight of each RealServer is updated if **any** of the existing destinations (`curDests`) has a different weight from the newly created `newDest`. This logic may lead to unnecessary or even incorrect updates.

##### Where the code is:
File: `pkg/proxy/ipvs/proxier.go`  
Function: `syncProxyRules`  
Related snippet:
```go
for _, dest := range curDests {
    if dest.Weight != newDest.Weight {
        err = proxier.ipvs.UpdateRealServer(appliedVirtualServer, newDest)
    }
}
```

##### Why it's problematic:
The current code updates `newDest` without checking if the existing `dest` in `curDests` actually matches the IP and port of `newDest`. This can result in:
- Unnecessary repeated updates to the same RealServer.
- Updates triggered by unrelated `dest` entries.
- Inefficient syncing behavior during startup.



#### What did you expect to happen?

The logic should compare the `Address` and `Port` of `dest` and `newDest`, and only perform an update if:
```go
dest.Address.Equal(newDest.Address) && dest.Port == newDest.Port && dest.Weight != newDest.Weight
```

This ensures only the correct RealServer is updated when needed.

#### How can we reproduce it (as minimally and precisely as possible)?

This logic runs during the initial sync phase. In environments with multiple RealServers with varying weights, the current code may repeatedly trigger `UpdateRealServer` on unrelated destinations.

#### Anything else we need to know?

The logic should compare the `Address` and `Port` of `dest` and `newDest`, and only perform an update if:
```go
dest.Address.Equal(newDest.Address) && dest.Port == newDest.Port && dest.Weight != newDest.Weight
```

This ensures only the correct RealServer is updated when needed.

#### Kubernetes version

- Kubernetes version: [ v1.29.0]
- IPVS mode enabled


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»è¯¥ Issue çš„æè¿°æ¥çœ‹ï¼Œé—®é¢˜ä¸»è¦æ˜¯ IPVS æ¨¡å¼ä¸‹çš„ kube-proxy åœ¨åˆå§‹åŒæ­¥ç¯èŠ‚å¯èƒ½é”™è¯¯åœ°æ¯”è¾ƒ RealServer çš„ weightï¼Œä»è€Œå¯¼è‡´ä¸å¿…è¦æˆ–é”™è¯¯çš„æ›´æ–°æ“ä½œã€‚è¯¥é—®é¢˜æ›´å¤šåœ°å½±å“åˆ°ç³»ç»Ÿåœ¨å¯åŠ¨æ—¶çš„åŒæ­¥æ•ˆç‡ï¼Œå®¹æ˜“å‡ºç°é‡å¤æˆ–æ— å…³çš„ RealServer æ›´æ–°è¯·æ±‚ï¼Œä½†ä»æè¿°ä¸­å¹¶æœªä½“ç°å‡ºå¯è¢«æ”»å‡»è€…åˆ©ç”¨ä»¥å‘èµ·æ‹’ç»æœåŠ¡æ”»å‡»ã€æå‡æƒé™ã€æ‰§è¡Œå‘½ä»¤æˆ–å…¶ä»–å®‰å…¨åæœçš„å¯èƒ½æ€§ã€‚è¯¥é—®é¢˜æ›´åƒæ˜¯åŠŸèƒ½/æ€§èƒ½é€»è¾‘å±‚é¢çš„ç¼ºé™·ï¼Œè€Œéå®‰å…¨æ¼æ´ï¼Œå› æ­¤ä¸æ¶‰åŠå®é™…çš„å®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# æœ¬Issueå¹¶æœªæè¿°å®‰å…¨é£é™©ï¼Œå› æ­¤æ— éœ€æä¾›é’ˆå¯¹å®‰å…¨æ¼æ´çš„å¤ç°è„šæœ¬ã€‚
# è‹¥ä»…éœ€éªŒè¯é‡å¤æˆ–æ— å…³çš„ RealServer æ›´æ–°ï¼Œå¯åœ¨æµ‹è¯•ç¯å¢ƒä¸­éƒ¨ç½²
# å¯¹åº”ç‰ˆæœ¬çš„kube-proxyå¹¶è§‚å¯Ÿå…¶æ—¥å¿—ä¸æ›´æ–°è¡Œä¸ºã€‚
print("å½“å‰Issueæœªæ¶‰åŠå®‰å…¨é£é™©ï¼Œæ— éœ€å¤ç°POC")
```


---


## Issue #131133 Duplicate "Transfer-Encoding: chunked" header

- Issue é“¾æ¥ï¼š[#131133](https://github.com/kubernetes/kubernetes/issues/131133)

### Issue å†…å®¹

#### What happened?

After upgrading ngress-nginx-controller to v1.11.5 and kube webhook version v20231226-1a7112e06, getting- upstream sent duplicate header line: "Transfer-Encoding: chunked", previous value: "Transfer-Encoding: chunked" while reading response header from upstream.

#### What did you expect to happen?

From our gui, when i am trying to hit search api, but everytime i hit it i am getting upstream sent duplicate header line: "Transfer-Encoding: chunked", previous value: "Transfer-Encoding: chunked" while reading response header from upstream and 502 as response. If i roll back to ingress controller version v1.9.4 its working fine.

#### How can we reproduce it (as minimally and precisely as possible)?

I have upgraded from v1.9.4 to v.11.5 and kubehook from v20231011-8b53cabe0 to v20231226-1a7112e06, than i am getting this

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.30.3
```

</details>


#### Cloud provider

<details>
Azure
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»é—®é¢˜æè¿°æ¥çœ‹ï¼Œäº§ç”Ÿâ€œTransfer-Encoding: chunkedâ€å¤´è¢«é‡å¤å†™å…¥ï¼Œæ˜¯ç”±äºå‡çº§åˆ°æŒ‡å®šç‰ˆæœ¬åä»£ç†æˆ–åç«¯æœåŠ¡åœ¨å“åº”å¤´ä¸­é‡å¤æ·»åŠ äº†è¯¥å­—æ®µï¼Œå¯¼è‡´ 502 é”™è¯¯ã€‚è¯¥é—®é¢˜ä¸»è¦æ˜¯è¯·æ±‚-å“åº”å¤„ç†ä¸Šçš„é…ç½®æˆ–å…¼å®¹æ€§Bugï¼Œå¹¶æœªç»™å‡ºå¯åˆ©ç”¨çš„å®‰å…¨æ¼æ´åœºæ™¯ï¼Œä¹Ÿä¸æ¶‰åŠå‡­æ®æ³„éœ²ã€ä»£ç æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰å®‰å…¨é£é™©è¦ç´ ï¼Œæ›´ä¸æ¶‰åŠå› å¤šé‡å¤´å¯¼è‡´çš„è¯·æ±‚æ‹†åˆ†æ”»å‡»ï¼ˆHTTP Request Smugglingï¼‰ç­‰é«˜çº§å®‰å…¨é£é™©ã€‚å› æ­¤ï¼Œæœ¬Issueå¹¶ä¸æ„æˆå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# æœ¬Issueä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œå› æ­¤æ— éœ€æä¾›å¤ç°è„šæœ¬
```


---


