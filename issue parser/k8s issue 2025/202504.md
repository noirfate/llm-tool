# Issue 安全分析报告

> 分析模型：o1

# 🚨 存在高风险的 Issues (1 个)

## Issue #131449 NFS PV does not respect `spec.mountOptions`.

- Issue 链接：[#131449](https://github.com/kubernetes/kubernetes/issues/131449)

### Issue 内容

#### What happened?

When using a `pv` which contains `spec.mountOptions` to mount an NFS share, the mount options are not respected.  This can be seen with the following example.

```yaml
spec:
  accessModes:
  - ReadWriteMany
  capacity:
    storage: 1Gi
  claimRef:
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: stuff
    namespace: data
  mountOptions:
  - timeo=600
  - retrans=3
  - proto=tcp6
  - nfsvers=4.2
  - rsize=32768
  - wsize=32768
  - noacl
  - noatime
  - nodiratime
  - nolock
  - nosuid
  - nofail
  nfs:
    path: /data01/stuff
    server: server.example.com
  persistentVolumeReclaimPolicy: Retain
  volumeMode: Filesystem
```

Using the above, one might expect an NFS v4 mount, but viewing the output of `mount` from the node shows the underlying mount properties do not contain any of the specified options.
```
server.example.com:/data01/stuff on /var/lib/kubelet/pods/5115dfbc-7427-4e63-9a4e-914a8396e43f/volumes/kubernetes.io~nfs/stuff-data type nfs (rw,relatime,vers=3,rsize=131072,wsize=131072,namlen=255,hard,proto=tcp6,timeo=600,retrans=2,sec=sys,mountaddr=fc99::1111,mountvers=3,mountport=893,mountproto=udp6,local_lock=none,addr=fc99::1111)
```

#### What did you expect to happen?

My expectation is that when passing `mountOptions` to the PV, the underlying NFS mount command respects these options when attempting to mount the remote export.

#### How can we reproduce it (as minimally and precisely as possible)?

Reviewing the code for `TestMountOptionFromSpec` [here](https://github.com/kubernetes/kubernetes/blob/b53b9fb5573323484af9a19cf3f5bfe80760abba/pkg/volume/util/util_test.go#L343), it appears that an additional test case like the following will reproduce the issue.
```go
		"vol-with-no-sys-opts-with-nfsv4": {
			volume: createVolumeSpecWithMountOption("good-mount-opts", "", v1.PersistentVolumeSpec{
				PersistentVolumeSource: v1.PersistentVolumeSource{
					NFS: &v1.NFSVolumeSource{Server: "localhost", Path: "/srv", ReadOnly: false},
				},
				MountOptions: []string{"ro", "nfsvers=4.2"},
			}),
			expectedMountList: []string{"ro", "nfsvers=4.2", "fsid=100"},
			systemOptions:     []string{"fsid=100", "ro"},
		},
```

Notice that none of the other test cases exercise the PV `mountOptions`, and the helper method `createVolumeSpecWithMountOption` only uses the options to set an annotation.  This can be seen in the code coverage for the `util.MountOptionFromSpec` function.

#### Anything else we need to know?

This is using `v1.31.5+k3s1`.  I have been informed from the k3s folks that there is no modification of upstream volume handling.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.6-dispatcher
Kustomize Version: v5.4.2
Server Version: v1.31.5+k3s1
```

</details>


#### Cloud provider

<details>
Self-hosted.
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Arch Linux"
PRETTY_NAME="Arch Linux"
ID=arch
BUILD_ID=rolling
ANSI_COLOR="38;2;23;147;209"
HOME_URL="https://archlinux.org/"
DOCUMENTATION_URL="https://wiki.archlinux.org/"
SUPPORT_URL="https://bbs.archlinux.org/"
BUG_REPORT_URL="https://gitlab.archlinux.org/groups/archlinux/-/issues"
PRIVACY_POLICY_URL="https://terms.archlinux.org/docs/privacy-policy/"
LOGO=archlinux-logo
$ uname -a
Linux k6 6.14.2-arch1-1 #1 SMP PREEMPT_DYNAMIC Thu, 10 Apr 2025 18:43:59 +0000 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
高风险

**判断依据：**  
该 Issue 讨论了 NFS 类型的持久卷（PV）在挂载时不尊重用户在 spec.mountOptions 中配置的挂载选项的问题。  
从功能上看，这会导致用户原本期望启用的“nosuid”“noacl”“noatime”“nfsvers=4.2”等选项实际上未被生效，系统默认使用了 NFS v3 并启用 suid 功能等。  
在挂载选项中，使用者显式指定了“nosuid”，但实际挂载却依然可能存在“suid”生效的情况，这就带来一个潜在的风险：如果攻击者或任意低权限用户能够往该 NFS 存储卷里写入可执行文件并设置 suid 位，则可能通过 suid 提权获得在宿主机或容器内更高的权限，这属于比较严重的安全风险。  
根据 CVSS 3.1 的计算标准（本地提权、攻击复杂度低、可能影响完整性和可用性），该问题评分可达到 High（高风险）级别。故综合判断，该 Issue 具有高风险安全隐患。

**复现过程：**

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import time
import traceback
from kubernetes import client, config, utils
from kubernetes.stream import stream

def main():
    """
    该脚本演示了在 Kubernetes 中使用 NFS PV 时指定 nosuid 等挂载选项，
    但由于 Issue 中描述的 bug 导致实际挂载并未应用 nosuid，而可能带来的安全风险。
    本脚本只用于研究和本地测试，切勿在生产环境中随意运行。
    """
    try:
        # 1. 载入默认 kubeconfig（假设已配置好~/.kube/config）
        config.load_kube_config()

        # 2. 创建 Kubernetes 客户端
        api_instance = client.CoreV1Api()

        # 3. 定义测试用命名空间
        namespace_name = "test-nfs-nosuid"
        create_namespace_if_not_exists(api_instance, namespace_name)

        # 4. 定义一个已经存在的 NFS 服务器和共享路径（假设测试环境中已存在）
        #    如果需要，可以在此脚本外部预先准备好 NFS 服务
        nfs_server = "1.2.3.4"   # 请替换为真实 NFS 服务地址
        nfs_path = "/myshare"    # 替换为真实共享的目录

        # 5. 创建一个 PV，指定了 nosuid 等 mountOptions
        pv_name = "test-nfs-pv-nosuid"
        create_pv_with_mountoptions(api_instance, pv_name, nfs_server, nfs_path)

        # 6. 创建一个 PVC 以绑定上面的 PV
        pvc_name = "test-nfs-pvc-nosuid"
        create_pvc(api_instance, namespace_name, pvc_name, pv_name)

        # 7. 创建一个 Pod，使用上面创建的 PVC
        pod_name = "test-nfs-pod-nosuid"
        create_test_pod(api_instance, namespace_name, pod_name, pvc_name)

        # 等待 Pod 运行起来
        wait_pod_ready(api_instance, namespace_name, pod_name)

        # 8. 在 Pod 内查看实际的挂载选项
        #    期待看到 nosuid，但根据 Issue 可能出现 suid 被启用等情况
        cmd = ["mount"]
        mount_info = stream(api_instance.connect_get_namespaced_pod_exec,
                            pod_name, namespace_name,
                            command=cmd,
                            stderr=True, stdin=False,
                            stdout=True, tty=False)
        print("===== Pod 内部的 mount 命令输出 =====")
        print(mount_info)
        print("================================")

        # 如果忽略了 nosuid，则有可能出现 suid 依然生效的挂载
        # 可以进一步尝试在 PV 上放置带 suid 的可执行文件并执行，从而模拟提权风险

    except Exception as e:
        print("执行过程中出现异常：", e)
        traceback.print_exc()


def create_namespace_if_not_exists(api_instance, namespace_name):
    """
    如果命名空间不存在则创建
    """
    existing_namespaces = api_instance.list_namespace()
    for ns in existing_namespaces.items:
        if ns.metadata.name == namespace_name:
            return
    ns_body = client.V1Namespace(metadata=client.V1ObjectMeta(name=namespace_name))
    api_instance.create_namespace(ns_body)
    time.sleep(2)


def create_pv_with_mountoptions(api_instance, pv_name, nfs_server, nfs_path):
    """
    创建带 nosuid 等 mountOptions 的 NFS PV
    注意这里设置了一个较小存储容量以示范；也可调大。
    """
    pv_spec = client.V1PersistentVolume(
        metadata=client.V1ObjectMeta(name=pv_name),
        spec=client.V1PersistentVolumeSpec(
            capacity={"storage": "1Gi"},
            access_modes=["ReadWriteMany"],
            nfs=client.V1NFSVolumeSource(
                path=nfs_path,
                server=nfs_server
            ),
            persistent_volume_reclaim_policy="Retain",
            mount_options=[
                "nosuid",
                "noacl",
                "noatime",
                "nodiratime",
                "nolock",
                "nfsvers=4.2"
            ],
            storage_class_name=""  # 无需指定 storageClass
        )
    )
    api_instance.create_persistent_volume(pv_spec)
    time.sleep(2)


def create_pvc(api_instance, namespace, pvc_name, pv_name):
    """
    创建用于绑定上面 PV 的 PVC
    """
    pvc_spec = client.V1PersistentVolumeClaim(
        metadata=client.V1ObjectMeta(name=pvc_name),
        spec=client.V1PersistentVolumeClaimSpec(
            access_modes=["ReadWriteMany"],
            resources=client.V1ResourceRequirements(
                requests={"storage": "1Gi"}
            ),
            volume_name=pv_name
        )
    )
    api_instance.create_namespaced_persistent_volume_claim(namespace, pvc_spec)
    time.sleep(2)


def create_test_pod(api_instance, namespace, pod_name, pvc_name):
    """
    创建测试 Pod，挂载上面的 PVC
    """
    container = client.V1Container(
        name="test-nfs-container",
        image="busybox",
        command=["sh", "-c", "sleep 3600"],
        volume_mounts=[client.V1VolumeMount(
            mount_path="/mnt/test",
            name="test-nfs-volume"
        )]
    )

    volume = client.V1Volume(
        name="test-nfs-volume",
        persistent_volume_claim=client.V1PersistentVolumeClaimVolumeSource(
            claim_name=pvc_name
        )
    )

    spec = client.V1PodSpec(containers=[container], volumes=[volume], restart_policy="Never")
    pod = client.V1Pod(
        metadata=client.V1ObjectMeta(name=pod_name),
        spec=spec
    )
    api_instance.create_namespaced_pod(namespace, pod)
    time.sleep(2)


def wait_pod_ready(api_instance, namespace, pod_name, timeout=90):
    """
    等待 Pod 的状态变为 Running，超时则直接返回或报错
    """
    start_time = time.time()

    while True:
        pod = api_instance.read_namespaced_pod(pod_name, namespace)
        if pod.status.phase == "Running":
            break
        if time.time() - start_time > timeout:
            print(f"等待 Pod {pod_name} 运行超时 {timeout} 秒。")
            break
        time.sleep(3)

main()
```


**解释说明：**

1. 脚本会在当前集群中创建一个名为“test-nfs-nosuid”的命名空间，并在其中创建 PV、PVC 以及一个测试 Pod。  
2. 该 PV 的 NFS 挂载选项里包含“nosuid”，按理说 Pod 内部实际挂载点应当无法执行 suid 程序，但由于 Issue 所述的 bug，挂载命令可能会默认为 suid 生效的方式。  
3. 脚本执行后，会在 Pod 内部调用“mount”命令并打印输出，若输出结果并未显示 “nosuid” 选项，或仍可见“suid”/“setuid”相关标记，即说明挂载选项被忽略。实际环境中，这可能被攻击者利用，上传带 suid 位的二进制执行文件，实现提权。  
4. 本脚本仅用于本地或测试环境验证该问题，切勿在生产环境中随意运行。脚本执行结束后，如需清理，请删除创建的 Pod、PVC、PV 以及命名空间。

---


# ⚠️ 存在低风险的 Issues (2 个)

## Issue #131407 kubelet not cleaning up exited containers and eventually failing due to gRPC message size

- Issue 链接：[#131407](https://github.com/kubernetes/kubernetes/issues/131407)

### Issue 内容

#### What happened?

in a cluster with a high amount of CronJobs (100+) which run every minute, containers keep piling up for pods which are no longer running, which then leads to IPAM congestion, eventually ending in the kubelet failing due to the following:
```json
{"ts":1745319662553.8606,"caller":"internal/log.go:32","msg":"ListPodSandbox with filter from runtime service failed","filter":"nil","err":"rpc error: code = ResourceExhausted desc = grpc: trying to send message larger than max (16791221 vs. 16777216)"}
{"ts":1745319662553.8938,"caller":"kuberuntime/kuberuntime_sandbox.go:305","msg":"Failed to list pod sandboxes","err":"rpc error: code = ResourceExhausted desc = grpc: trying to send message larger than max (16791221 vs. 16777216)"}
{"ts":1745319662553.902,"caller":"pleg/generic.go:256","msg":"GenericPLEG: Unable to retrieve pods","err":"rpc error: code = ResourceExhausted desc = grpc: trying to send message larger than max (16791221 vs. 16777216)"}
```
the CronJobs have been tuned to avoid this by setting the following in the spec, but to no avail:
```yaml
concurrencyPolicy: Forbid
ttlSecondsAfterFinished: 30
successfulJobsHistoryLimit: 2   # (sometimes 1)
failedJobsHistoryLimit: 1
```



**UPD**: after hitting the aforementioned message, I've cordoned and drained the node, and let it sit like this. after about an hour in the cordoned state, I saw a spike in the operation rate (mostly `container_status` and `remove_container`), and the amount of exited containers dropped from 10230 to just 6! the logs were full of `RemoveContainer` operations:
```json
{"ts":1745323991806.5146,"caller":"topologymanager/scope.go:117","msg":"RemoveContainer","v":0,"containerID":"95f25c810eba33f3a72e3852fca88d59b5a2428535a663aea04a9d60c8fe2963"}
{"ts":1745323991806.924,"caller":"topologymanager/scope.go:117","msg":"RemoveContainer","v":0,"containerID":"141f8a2c0ad488f454886084227aedb4a18cf2515d9747500aedc44f196f6c7a"}
```
so, the GC worked. but why only now?

#### What did you expect to happen?

containers being cleaned up after pods

#### How can we reproduce it (as minimally and precisely as possible)?

1. have a cluster with 100+ CronJobs scheduled to run every minute
2. observe a rapidly increasing amount of containers despite the pods being cleaned up (metric `kubelet_running_containers{container_state="exited"}`). this went over 4k in a span of 8 hours and, in my case, reached over 10k before failing
3. observe IPAM issues. in my case it was Cilium keeping pods in the Pending phase with the following: `unable to allocate IP via local cilium agent: [POST /ipam][502] postIpAMFailure range is full`
4. observe `list_podsandbox` errors before kubelet ultimately failing to respond to commands due to gRPC message size

#### Anything else we need to know?

_No response_

#### Kubernetes version

Kubernetes v1.32.3

#### Cloud provider

Bare-metal

#### OS version

Talos Linux v1.9.5

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

Containerd v2.0.3

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

Cilium v1.17.2

### 分析结果

**风险定级：**  
低风险

**判断依据：**  
从该 Issue 描述来看，问题的核心是 Kubernetes 集群中 CronJob 数量较多且频繁触发（每分钟 100+），导致已退出的容器残留没有及时被 kubelet 垃圾回收（GC），堆积到上万数量，从而引起 IPAM 资源耗尽(Cilium 报错 range is full)以及容器运行时在列出 pod sandboxes 时出现 gRPC 传输体积超限。  
此问题本质上属于资源管理和调度层面的运维问题，并非典型的安全漏洞。虽然最终会导致 kubelet 无法正常响应请求，呈现出某种“拒绝服务（DoS）”的效果，但需要注意以下要点：  
1. 触发该问题需要在集群内具有创建 CronJob/Job/Pod 等权限，一般而言这是集群管理员或具有较高权限的角色才具备的操作能力，普通低权限用户无法直接制造如此规模的容器堆积。  
2. 该问题并不涉及任意命令执行、容器逃逸、提权、凭据泄露等高风险行为，更多是因为节点处于高负载或资源回收机制异常导致的服务可用性问题。  
3. 根据 CVSS 3.1 评分标准，若为拒绝服务型风险但需要高权限才能实施，通常会显著降低漏洞评分。综合计算后，分值落在 7.0 以下，故未达“高风险”阈值。  

因此综合判断，该问题可以被视作资源耗尽引起的服务可用性问题，需要有较高权限才能造成影响，并不构成严重安全漏洞。

**解释说明：**

由于该问题并未达到高风险级别（CVSS 3.1 评分不足以达到 High 以上），故根据“针对高风险问题，必须给出使用 python 编写的复现脚本”的要求，不需要提供复现脚本。  
在实际环境中，可通过减少 CronJob 频率或数量、启用合适的清理/回收策略（如 ttlSecondsAfterFinished、Job 历史保留策略等）并优化 kubelet 配置，来缓解或避免此问题。  
如果在多租户场景下存在低权限用户可创建大量作业的特殊情况，则需通过权限管控（RBAC）来限制 CronJob/Job 的创建，防止资源滥用导致的潜在 DoS 风险。

---


## Issue #131355 Kubelet ReadOnlyPort cannot be set to 0 via golang KubeletConfiguration

- Issue 链接：[#131355](https://github.com/kubernetes/kubernetes/issues/131355)

### Issue 内容

#### What happened?

The kubelet ReadOnlyPort cannot be set to 0 via KubeletConfiguration struct.

As per the docs:
https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/
> **--read-only-port** int32     Default: 10255
> The read-only port for the kubelet to serve on with no authentication/authorization (set to 0 to disable).
> (DEPRECATED: This parameter should be set via the config file specified by the kubelet's --config flag. See kubelet-config-file for more information.)


https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/#kubelet-config-k8s-io-v1beta1-KubeletConfiguration
> **readOnlyPort** int32     Default: 0 (disabled)
> readOnlyPort is the read-only port for the Kubelet to serve on with no authentication/authorization. The port number must be between 1 and 65535, inclusive. Setting this field to 0 disables the read-only service.

If you attempt to set this to 0 this using the golang struct, the field will be omitted when marshaled to YAML or JSON as the json field tag has `omitempty` and 0 is the empty value for an int32:
https://github.com/kubernetes/kubernetes/blob/35716897a3d8cd2944aa172006c5b7604a64c22e/staging/src/k8s.io/kubelet/config/v1beta1/types.go#L145

#### What did you expect to happen?

Field is not dropped from YAML

#### How can we reproduce it (as minimally and precisely as possible)?

```golang
package main

import (
	"fmt"

	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	kubeletconfig "k8s.io/kubelet/config/v1beta1"
	"sigs.k8s.io/yaml"
)

func main() {
	config := &kubeletconfig.KubeletConfiguration{
		TypeMeta: metav1.TypeMeta{
			APIVersion: "kubelet.config.k8s.io/v1beta1",
			Kind:       "KubeletConfiguration",
		},
		ReadOnlyPort: 0,
	}
	b, _ := yaml.Marshal(config)
	fmt.Printf("%s", b)
}
```
```console
brandond@dev01:~$ go run main.go
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous: {}
  webhook:
    cacheTTL: 0s
  x509: {}
authorization:
  webhook:
    cacheAuthorizedTTL: 0s
    cacheUnauthorizedTTL: 0s
containerRuntimeEndpoint: ""
cpuManagerReconcilePeriod: 0s
evictionPressureTransitionPeriod: 0s
fileCheckFrequency: 0s
httpCheckFrequency: 0s
imageMaximumGCAge: 0s
imageMinimumGCAge: 0s
kind: KubeletConfiguration
logging:
  flushFrequency: 0
  options:
    json:
      infoBufferSize: "0"
    text:
      infoBufferSize: "0"
  verbosity: 0
memorySwap: {}
nodeStatusReportFrequency: 0s
nodeStatusUpdateFrequency: 0s
runtimeRequestTimeout: 0s
shutdownGracePeriod: 0s
shutdownGracePeriodCriticalPods: 0s
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
volumeStatsAggPeriod: 0s
```

Since the deprecated CLI flag defaults to 10255, and the field is not rendered into the configuration YAML that is passed to the kubelet, the port remains set to 10255.

#### Anything else we need to know?

This is probably another case of a field that should have been *int32 instead of int32.

#### Kubernetes version

<details>

```console
Client Version: v1.32.3+k3s1
Kustomize Version: v5.5.0
Server Version: v1.32.3+k3s1
```

</details>


#### Cloud provider

n/a

#### OS version

n/a

#### Install tools

k3s

#### Container runtime (CRI) and version (if applicable)

containerd v2.0.4


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

n/a


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
从该 Issue 描述来看，kubelet 的 readOnlyPort 参数本应通过将其设置为 0 来禁用，但因为在 Golang 的 KubeletConfiguration 结构体中使用了带有 omitempty 标签的 int32 字段，导致设置成 0 时字段会在序列化输出时被忽略，从而使 readOnlyPort 始终保持默认值 10255 并处于开启状态。这会在集群中开启一个无需身份验证的只读端口，虽然无法直接进行敏感操作，但仍可能造成一定的信息泄露风险（如节点状态、Pod 信息等元数据）。根据 CVSS 3.1 评分推测，这种信息泄露并不属于严重漏洞，尚不足以导致高危后果，因此可判定为安全问题中的低风险等级。

---


# ✅ 不涉及安全风险的 Issues (58 个)

## Issue #131543 kubtctl auth can-i get pods/exec shows error result

- Issue 链接：[#131543](https://github.com/kubernetes/kubernetes/issues/131543)

### Issue 内容

#### What happened?

when i execute command 'kubtctl auth can-i get pods/exec' it show yes, but in fact i can't exec into the pod because i haven't set the pods/exec privilege in the role, you can see what happened in the image demonstrated below.
![Image](https://github.com/user-attachments/assets/bbe01511-0379-4627-8a14-2cb02d6ffb93)

![Image](https://github.com/user-attachments/assets/d89124a0-5a66-4203-8b65-29b20df95b9e)

#### What did you expect to happen?

it should show no as a result

#### How can we reproduce it (as minimally and precisely as possible)?

create a role who don't have pods/exec privilege and use that user to exec a shell in that pod

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: v1.29.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.2
```

</details>


#### Cloud provider

<details>
no cloud provider
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue描述来看，问题集中在“kubectl auth can-i”命令结果与实际权限不一致：命令输出显示“yes”，但实际上用户并没有“pods/exec”权限，也无法真正exec进入Pod。这属于工具或配置层面导致的表现不一致，并未体现出实际权限绕过或越权操作，也没有敏感信息泄露或高风险安全漏洞。  
因此，该问题并不会让攻击者在无权限的情况下可以执行命令，也不影响集群安全，不涉及攻击手段或实际越权行为，只是输出信息与真实角色权限不一致，属于使用或配置Bug，并非安全漏洞。根据Issue风险判断标准，此处风险评级为“不涉及”。

**解释说明：**

该Issue并未体现出安全漏洞或敏感信息泄露，仅为不影响安全的功能/配置问题。由于不是高风险安全问题，因此无需提供复现脚本。若需进一步排查，可围绕集群的RBAC配置、本地kubeconfig权限和“kubectl auth can-i”命令的实现方式进行调试和验证。

---


## Issue #131533 code-generator/applyconfiguration-gen: Embedding a field results in <.EmbeddedIn.MemberType.Elem.Name.Name>: nil pointer evaluating *types.Type.Name

- Issue 链接：[#131533](https://github.com/kubernetes/kubernetes/issues/131533)

### Issue 内容

Using applyconfiguration-gen with the following type produces an error
```
import (
        gwv1alpha2 "sigs.k8s.io/gateway-api/apis/v1alpha2"
)

type Status struct {
	gwv1alpha2.PolicyStatus `json:",inline"`
}
```
```
F0424 08:57:02.343886 1766244 main.go:57] Error: failed executing generator: some targets had errors: template: /home/shashank/.gvm/pkgsets/go1.24.1/global/pkg/mod/k8s.io/code-generator@v0.32.2/cmd/applyconfiguration-gen/generators/applyconfiguration.go:342:1:37: executing "/home/shashank/.gvm/pkgsets/go1.24.1/global/pkg/mod/k8s.io/code-generator@v0.32.2/cmd/applyconfiguration-gen/generators/applyconfiguration.go:342" at <.EmbeddedIn.MemberType.Elem.Name.Name>: nil pointer evaluating *types.Type.Name
```

Not embedding the field works:
```
type Status struct {
	PolicyStatus gwv1alpha2.PolicyStatus `json:",inline"`
}
``

This is with `k8s.io/code-generator v0.32.3

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue描述来看，这是一个在使用applyconfiguration-gen时因嵌入类型导致nil指针的问题，属于典型的开发阶段编译或代码生成错误。该问题并不涉及敏感信息泄露、权限提升、远程代码执行、命令执行或可被恶意利用的安全风险，也无法在多用户场景中被低权限用户利用来影响其他用户的资源。因此，本质上只是一个代码生成过程中的兼容性或工具兼容性Bug，而非安全漏洞。

**解释说明：**

由于该问题并非安全风险，风险评级判断为“不涉及”，因此无需提供复现POC脚本。该Issue本质上与工具使用问题或生成器逻辑不完全适配有关，不会对系统或数据造成安全威胁。若要解决该问题，可与相关代码生成工具的维护方进行沟通或查看更新版本的修正情况。相关修复可在开发环境中完成，属于普通研发生命周期内的问题而非安全漏洞。

---


## Issue #131530 OOM killed status pod become successed phase

- Issue 链接：[#131530](https://github.com/kubernetes/kubernetes/issues/131530)

### Issue 内容

#### What happened?

I created a deployment with 3 replicas. Each pod contains 1 init container and 1 main container. The exit code of one pod main container is 0. But the reason for the exit is OOM killed. However, the final status of the pod is successed.

```
State:          Terminated
  Reason:       OOMKilled
  Exit Code:    0
  Started:      Tue, 29 Apr 2025 13:43:32 +0800
  Finished:     Tue, 29 Apr 2025 14:10:51 +0800
Ready:          False
Restart Count:  0
```
```
  phase: Succeeded
```
```
  restartPolicy: Always
```
which cause rs controller create 5 replicas

<img width="1294" alt="Image" src="https://github.com/user-attachments/assets/9097648f-0077-4466-9018-4aaf58f37527" />

Maybe some information has changed,
I have one questions
1. Is the phase of an OOM-killed container 's pod   “Succeeded” or “Running”?
Are the above one questions normal behavior changes?Especially after reading https://github.com/kubernetes/kubernetes/issues/118310, the behavior changes introduced there make this matter feel more confusing。




#### What did you expect to happen?

The phase is Running

#### How can we reproduce it (as minimally and precisely as possible)?

A env which has the same kubernetes version.I try to reproduce it
dockerfile
```
FROM ubuntu:latest

RUN apt-get update && apt-get install -y dumb-init stress-ng libcap2-bin && \
    setcap cap_ipc_lock+ep /usr/bin/stress-ng

COPY start_stress.sh /start_stress.sh
RUN chmod +x /start_stress.sh
ENTRYPOINT ["dumb-init", "--", "/start_stress.sh"]
```
start_stress.sh
```
#!/bin/bash
for i in {1..3}; do
  stress-ng --vm 1 --vm-bytes 256M --vm-keep --vm-locked &
done

sleep 30s
stress-ng --vm 1 --vm-bytes 256M --vm-keep --vm-locked &
sleep 10s
stress-ng --vm 1 --vm-bytes 512M --vm-keep --vm-locked &
sleep 10s

```
deployment.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: oom-test
spec:
  replicas: 2
  selector:
    matchLabels:
      app: oom-test
  template:
    metadata:
      labels:
        app: oom-test
    spec:
      containers:
      - name: nginx
        image:  the image
        imagePullPolicy: Always
        securityContext: 
          capabilities:
            add: ["IPC_LOCK", "SYS_RESOURCE"]
        resources:
          limits:
            memory: "1300Mi"

```
but i get
```
    Port:           <none>
    Host Port:      <none>
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       OOMKilled
      Exit Code:    0
      Started:      Wed, 30 Apr 2025 13:45:28 +0800
      Finished:     Wed, 30 Apr 2025 13:46:18 +0800
    Ready:          False
    Restart Count:  4
    Limits:
      memory:  1300Mi
    Requests:
      memory:     1300Mi
    Environment:  <none>

[root@node-5 ~]# kubectl get   pod  oom-test-545f7696d-jcld9 -o yaml -w|grep phase
  phase: Running
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.2
Server Version: v1.28.2
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从 Issue 描述来看，该问题主要涉及在 Kubernetes 中运行容器时因 OOM（Out Of Memory）被杀掉后，其 Pod 状态从容器的视角呈现为 “Succeeded” 而并非 “Running” 或 “Failed”。这是一次关于 Kubernetes 处理容器 OOMKilled 状态与 Pod 最终相位之间错配的讨论。  
此问题并未涉及敏感信息泄露、提权、远程命令执行或其他可能造成安全漏洞的情况，也不涉及对他人或系统带来安全风险的操作，只是对 Kubernetes 功能行为的疑问或误解。因此综合判断，这不是一个安全漏洞或安全风险。

**解释说明：**

本 Issue 与如何正确理解和使用 Kubernetes 有关，并未呈现出任何可导致安全攻击或安全缺陷的行为，所以无需提供复现脚本。该问题属于正常配置及使用中产生的困惑或功能体验不一致，不涉及安全风控范畴。

---


## Issue #131513 DRA kubelet: locked cache operation: unable to get claim info for claim

- Issue 链接：[#131513](https://github.com/kubernetes/kubernetes/issues/131513)

### Issue 内容

#### What happened?

I was stress-testing starting and stopping a pod when after a few iterations, kubelet permanently (or more precisely, for the last 8 minutes despite retries) failed to set up the container:

```
  Warning  Failed     3m22s (x26 over 8m29s)  kubelet            Error: locked cache operation: unable to get claim info for claim external-claim in namespace default
```

```
I0428 18:27:13.244772  138526 kuberuntime_sandbox_linux.go:62] "Enforcing CFS quota" pod="default/test-external-claim" unlimited=false
I0428 18:27:13.244802  138526 kuberuntime_manager.go:1333] "Creating container in pod" containerType="container" container="&Container{Name:with-resource,Image:registry.k8s.io/e2e-test-images/busybox:1.36.1-1,Command:[sh -c set && mount && ls -la /dev/ && sleep 10000],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{ResourceClaim{Name:resource,Request:,},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jpgs8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,}" pod="default/test-external-claim"
I0428 18:27:13.246378  138526 event.go:389] "Event occurred" object="default/test-external-claim" fieldPath="spec.containers{with-resource}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulled" message="Container image \"registry.k8s.io/e2e-test-images/busybox:1.36.1-1\" already present on machine"
E0428 18:27:13.246434  138526 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:with-resource,Image:registry.k8s.io/e2e-test-images/busybox:1.36.1-1,Command:[sh -c set && mount && ls -la /dev/ && sleep 10000],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{ResourceClaim{Name:resource,Request:,},},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jpgs8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod test-external-claim_default(cc3b4525-15da-4393-9270-9322f22bc62a): CreateContainerConfigError: locked cache operation: unable to get claim info for claim external-claim in namespace default" logger="UnhandledError"
I0428 18:27:13.246457  138526 event.go:389] "Event occurred" object="default/test-external-claim" fieldPath="spec.containers{with-resource}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: locked cache operation: unable to get claim info for claim external-claim in namespace default"
I0428 18:27:13.247649  138526 kubelet.go:1899] "SyncPod exit" pod="default/test-external-claim" podUID="cc3b4525-15da-4393-9270-9322f22bc62a" isTerminal=false
E0428 18:27:13.247681  138526 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"with-resource\" with CreateContainerConfigError: \"locked cache operation: unable to get claim info for claim external-claim in namespace default\"" pod="default/test-external-claim" podUID="cc3b4525-15da-4393-9270-9322f22bc62a"
I0428 18:27:13.247719  138526 pod_workers.go:1338] "Processing pod event done" pod="default/test-external-claim" podUID="cc3b4525-15da-4393-9270-9322f22bc62a" updateType="sync"
```

#### What did you expect to happen?

Pod should start.

#### How can we reproduce it (as minimally and precisely as possible)?

Bring up local-up cluster:
```
make -j && sudo rm -rf /var/run/kubernetes /var/lib/kubelet/dra_manager_state && RUNTIME_CONFIG=resource.k8s.io/v1alpha3,resource.k8s.io/v1beta1,resource.k8s.io/v1beta2 FEATURE_GATES=AuthorizeWithSelectors=true,AuthorizeNodeWithSelectors=true,DynamicResourceAllocation=true,ContextualLogging=true,DRAPrioritizedList=true,DRAAdminAccess=true,DRAResourceClaimDeviceStatus=true,DRADeviceTaints=true KUBELET_RESOLV_CONF="/etc/resolv-9999.conf" DNS_ADDON="coredns" CGROUP_DRIVER=systemd CONTAINER_RUNTIME_ENDPOINT=unix:///var/run/crio/crio.sock LOG_LEVEL=6 ENABLE_CSI_SNAPSHOTTER=false API_SECURE_PORT=6444 ALLOW_PRIVILEGED=1 PATH=/nvme/gopath/src/k8s.io/kubernetes/third_party/etcd:$PATH ./hack/local-up-cluster.sh -O
```

Patch master or use https://github.com/kubernetes/kubernetes/pull/131526:
```patch
diff --git a/test/e2e/dra/test-driver/README.md b/test/e2e/dra/test-driver/README.md
index 11a7ea9dbc9..969a4ddab9d 100644
--- a/test/e2e/dra/test-driver/README.md
+++ b/test/e2e/dra/test-driver/README.md
@@ -15,7 +15,7 @@ testing.
 
 Valid parameters are key/value string pairs stored in a ConfigMap.
 Those get copied into the ResourceClaimStatus with "user_" and "admin_" as
-prefix, depending on whether they came from the ResourceClaim or ResourceClass.
+prefix, depending on whether they came from the ResourceClaim or DeviceClass.
 They get stored in the `ResourceHandle` field as JSON map by the controller.
 The kubelet plugin then sets these attributes as environment variables in each
 container that uses the resource.
@@ -71,7 +71,7 @@ KUBECONFIG=/var/run/kubernetes/admin.kubeconfig go run ./test/e2e/dra/test-drive
 And finally:
 ```console
 $ export KUBECONFIG=/var/run/kubernetes/admin.kubeconfig
-$ kubectl create -f test/e2e/dra/test-driver/deploy/example/resourceclass.yaml
+$ kubectl create -f test/e2e/dra/test-driver/deploy/example/deviceclass.yaml
 resourceclass/example created
 $ kubectl create -f test/e2e/dra/test-driver/deploy/example/pod-inline.yaml
 configmap/pause-claim-parameters created
diff --git a/test/e2e/dra/test-driver/deploy/example/pod-external.yaml b/test/e2e/dra/test-driver/deploy/example/pod-external.yaml
index 59d75e9086a..705312d058a 100644
--- a/test/e2e/dra/test-driver/deploy/example/pod-external.yaml
+++ b/test/e2e/dra/test-driver/deploy/example/pod-external.yaml
@@ -24,13 +24,14 @@ spec:
   containers:
   - name: with-resource
     image: registry.k8s.io/e2e-test-images/busybox:1.36.1-1
-    command: ["sh", "-c", "set && mount && ls -la /dev/"]
+    command: ["sh", "-c", "set && mount && ls -la /dev/ && sleep 10000"]
     resources:
       claims:
       - name: resource
   - name: without-resource
     image: registry.k8s.io/e2e-test-images/busybox:1.36.1-1
-    command: ["sh", "-c", "set && mount && ls -la /dev/"]
+    command: ["sh", "-c", "set && mount && ls -la /dev/ && sleep 10000"]
+  terminationGracePeriodSeconds: 0 # Shut down immediately.
   resourceClaims:
   - name: resource
     resourceClaimName: external-claim
```

In one console, run the test-driver:
```
sudo mkdir -p /var/run/cdi
sudo mkdir -p /var/lib/kubelet/plugins/test-driver.cdi.k8s.io
sudo mkdir -p /var/lib/kubelet/plugins_registry
sudo chmod a+rx /var/lib/kubelet /var/lib/kubelet/plugins
sudo chmod a+rwx /var/run/cdi /var/lib/kubelet/plugins_registry /var/lib/kubelet/plugins/test-driver.cdi.k8s.io
KUBECONFIG=/var/run/kubernetes/admin.kubeconfig go run ./test/e2e/dra/test-driver -v=5 kubelet-plugin --node-name=127.0.0.1
```

In another, set KUBECONFIG and run:
```
while kubectl create -f test/e2e/dra/test-driver/deploy/example/pod-external.yaml && kubectl wait --for=condition=ready --timeout=10s  pod/test-external-claim &&  kubectl delete --timeout=4s -f test/e2e/dra/test-driver/deploy/example/pod-external.yaml && kubectl get resourceclaims; do true; done
```


#### Anything else we need to know?

/sig node
/wg device-management
/assign @bart0sh 
/cc @klueska


#### Kubernetes version

master = 3a8af5a174a3024d7c540b1f6e08aff21b2d0c83

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue内容来看，问题是kubelet在进行Pod启动和停止压力测试时，出现了“locked cache operation: unable to get claim info for claim”的报错，导致容器无法正常启动。该问题属于对资源Claim信息的获取或管理出现异常，可能是Dynamic Resource Allocation相关功能或插件在实现或状态管理上存在缺陷所致。  
从安全角度分析，并未发现存在敏感信息泄露、命令执行、容器逃逸、访问权限提升或其他可被攻击者恶意利用的安全风险。该报错看起来更像是一个kubelet或DRA插件的功能性/稳定性Bug，而并非安全漏洞，也没有迹象表明能够被非授权用户利用实现攻击。因此，该Issue与安全风险无关。

**复现过程：**

```python
# 本Issue不涉及安全风险，无需给出复现脚本
# 该脚本仅做占位示例
def main():
    print("此问题非安全问题，无需复现POC。")
```


---


## Issue #131512 spec.jobTemplate.spec.podFailurePolicy.rules[0].onPodConditions[0].status: Required value: valid values: ["False" "True" "Unknown"]

- Issue 链接：[#131512](https://github.com/kubernetes/kubernetes/issues/131512)

### Issue 内容

#### What happened?

When I try to apply (patching) an existing `CronJob` with the following `podFailurePolicy`:
```
      podFailurePolicy:
        rules:
        - action: Ignore
          onPodConditions:
          - type: DisruptionTarget
```

I'm getting the following Error:
```
error when patching "/dev/shm/2794561279": CronJob.batch "mycronjob" is invalid: spec.jobTemplate.spec.podFailurePolicy.rules[0].onPodConditions[0].status: Required value: valid values: ["False" "True" "Unknown"]
```


Following the docs ([link1](https://kubernetes.io/blog/2024/08/19/kubernetes-1-31-pod-failure-policy-for-jobs-goes-ga/), [link2](https://kubernetes.io/blog/2024/08/19/kubernetes-1-31-pod-failure-policy-for-jobs-goes-ga/)), it seems to be optional.

same in code(?)

https://github.com/kubernetes/api/blob/c5439348b1cf0e96c7446a1bef9475553530f12f/batch/v1/types.go#L214


#### What did you expect to happen?

should be applied without any errors

#### How can we reproduce it (as minimally and precisely as possible)?

create a sample cronjob with these configurations: 

```
apiVersion: batch/v1
kind: CronJob
metadata:
  name: mycronjob
  labels:
    app: release-name
    app.kubernetes.io/name: "release-name"
    app.kubernetes.io/instance: "release-name"
    app.kubernetes.io/part-of: mycronjob
spec:
  schedule: "*/10 * * * *"
  suspend: false
  concurrencyPolicy: Forbid
  startingDeadlineSeconds: 100
  successfulJobsHistoryLimit: 2
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      ttlSecondsAfterFinished: 172800 
      backoffLimit: 2
      parallelism: 1
      activeDeadlineSeconds: 1800
      podFailurePolicy:
        rules:
        - action: Ignore
          onPodConditions:
          - type: DisruptionTarget
...
```

#### Anything else we need to know?

workaround: adding the field `status: 'True'` fix it

```
    podFailurePolicy:
      rules:
      - action: Ignore
        onPodConditions:
        - type: DisruptionTarget
          status: 'True'
```

#### Kubernetes version

<details>

```console
$ kubectl version
v1.31.7-eks-bcf3d70
```

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从 Issue 描述来看，这个问题与 Kubernetes 中 `podFailurePolicy` 字段验证逻辑产生冲突有关，文档与实际代码行为不一致，导致在未指定 `status` 字段时无法正常应用 `CronJob`。该问题属于 Kubernetes 功能配置或使用方式上的不一致，并不涉及任何敏感信息泄露、权限提升、命令执行等安全风险，也无法被外部攻击者用来进行恶意利用，因此不属于安全漏洞。

**解释说明：**

本问题是一个使用或配置上的错误，不会导致外部攻击者利用该错误进行攻击或破坏，不存在任何权限提升、信息泄露等安全后果，因此不属于安全风险范围。此类“文档与实际实现不一致”的问题，会在后续版本或补丁中修复，但与安全漏洞无关。圆满解决方案是按照要求在配置中为 `status` 字段赋值，或者等待官方修复或更新文档。该问题不需要使用 Python 脚本进行漏洞复现演示。

---


## Issue #131497 externalIPs DNAT rules are not installed when clusterIP is None

- Issue 链接：[#131497](https://github.com/kubernetes/kubernetes/issues/131497)

### Issue 内容

#### What happened?

Consider the following service:
```yaml
apiVersion: v1
kind: Service
metadata:
  name: kube-bench-dev
spec:
  clusterIP: None
  selector:
    instance: kube-bench-dev
  ports:
  - name: tcp-80
    port: 80
    protocol: TCP
    targetPort: 80
  externalIPs:
  - 192.168.64.253
```
```
% kubectl get service kube-bench-dev 
NAME             TYPE        CLUSTER-IP   EXTERNAL-IP      PORT(S)   AGE
kube-bench-dev   ClusterIP   None         192.168.64.253   80/TCP    6s
```
But no DNAT rules for the external IP:
```
table ip kube-proxy {
	comment "rules for kube-proxy"
	set cluster-ips {
		type ipv4_addr
		comment "Active ClusterIPs"
		elements = { 172.16.0.1, 172.16.0.173,
			     172.16.0.220 }
	}

	set nodeport-ips {
		type ipv4_addr
		comment "IPs that accept NodePort traffic"
		elements = { 192.168.64.2 }
	}

	map no-endpoint-services {
		type ipv4_addr . inet_proto . inet_service : verdict
		comment "vmap to drop or reject packets to services with no endpoints"
	}

	map no-endpoint-nodeports {
		type inet_proto . inet_service : verdict
		comment "vmap to drop or reject packets to service nodeports with no endpoints"
	}

	map firewall-ips {
		type ipv4_addr . inet_proto . inet_service : verdict
		comment "destinations that are subject to LoadBalancerSourceRanges"
	}

	map service-ips {
		type ipv4_addr . inet_proto . inet_service : verdict
		comment "ClusterIP, ExternalIP and LoadBalancer IP traffic"
		elements = { 172.16.0.173 . tcp . 80 : goto service-TPLZMVKW-kube-system/ingress-nginx-controller/tcp/http,
			     192.168.64.254 . tcp . 80 : goto external-TPLZMVKW-kube-system/ingress-nginx-controller/tcp/http,
			     172.16.0.1 . tcp . 443 : goto service-2QRHZV4L-default/kubernetes/tcp/https,
			     172.16.0.173 . tcp . 443 : goto service-HNB4FGVK-kube-system/ingress-nginx-controller/tcp/https,
			     172.16.0.220 . tcp . 443 : goto service-FMTKUH45-kube-system/ingress-nginx-controller-admission/tcp/https-webhook,
			     192.168.64.254 . tcp . 443 : goto external-HNB4FGVK-kube-system/ingress-nginx-controller/tcp/https }
	}

	map service-nodeports {
		type inet_proto . inet_service : verdict
		comment "NodePort traffic"
	}

	chain filter-prerouting {
		type filter hook prerouting priority dstnat - 10; policy accept;
		ct state new jump firewall-check
	}

	chain filter-input {
		type filter hook input priority -110; policy accept;
		ct state new jump nodeport-endpoints-check
		ct state new jump service-endpoints-check
	}

	chain filter-forward {
		type filter hook forward priority -110; policy accept;
		ct state new jump service-endpoints-check
		ct state new jump cluster-ips-check
	}

	chain filter-output {
		type filter hook output priority dstnat - 10; policy accept;
		ct state new jump service-endpoints-check
		ct state new jump firewall-check
	}

	chain filter-output-post-dnat {
		type filter hook output priority dstnat + 10; policy accept;
		ct state new jump cluster-ips-check
	}

	chain nat-prerouting {
		type nat hook prerouting priority dstnat; policy accept;
		jump services
	}

	chain nat-output {
		type nat hook output priority dstnat; policy accept;
		jump services
	}

	chain nat-postrouting {
		type nat hook postrouting priority srcnat; policy accept;
		jump masquerading
	}

	chain nodeport-endpoints-check {
		ip daddr @nodeport-ips meta l4proto . th dport vmap @no-endpoint-nodeports
	}

	chain service-endpoints-check {
		ip daddr . meta l4proto . th dport vmap @no-endpoint-services
	}

	chain firewall-check {
		ip daddr . meta l4proto . th dport vmap @firewall-ips
	}

	chain services {
		ip daddr . meta l4proto . th dport vmap @service-ips
		ip daddr @nodeport-ips meta l4proto . th dport vmap @service-nodeports
	}

	chain masquerading {
		meta mark & 0x00004000 == 0x00000000 return
		meta mark set meta mark ^ 0x00004000
		masquerade fully-random
	}

	chain cluster-ips-check {
		ip daddr @cluster-ips reject comment "Reject traffic to invalid ports of ClusterIPs"
	}

	chain mark-for-masquerade {
		meta mark set meta mark | 0x00004000
	}

	chain reject-chain {
		comment "helper for @no-endpoint-services / @no-endpoint-nodeports"
		reject
	}

	chain endpoint-KUBDMD37-default/kubernetes/tcp/https__192.168.64.2/6443 {
		ip saddr 192.168.64.2 jump mark-for-masquerade
		meta l4proto tcp dnat to 192.168.64.2:6443
	}

	chain service-2QRHZV4L-default/kubernetes/tcp/https {
		ip daddr 172.16.0.1 tcp dport 443 ip saddr != 192.168.64.64/26 jump mark-for-masquerade
		numgen random mod 1 vmap { 0 : goto endpoint-KUBDMD37-default/kubernetes/tcp/https__192.168.64.2/6443 }
	}

	chain endpoint-5UYISHKM-kube-system/ingress-nginx-controller/tcp/http__192.168.64.68/80 {
		ip saddr 192.168.64.68 jump mark-for-masquerade
		meta l4proto tcp dnat to 192.168.64.68:80
	}

	chain service-TPLZMVKW-kube-system/ingress-nginx-controller/tcp/http {
		ip daddr 172.16.0.173 tcp dport 80 ip saddr != 192.168.64.64/26 jump mark-for-masquerade
		numgen random mod 1 vmap { 0 : goto endpoint-5UYISHKM-kube-system/ingress-nginx-controller/tcp/http__192.168.64.68/80 }
	}

	chain external-TPLZMVKW-kube-system/ingress-nginx-controller/tcp/http {
		jump mark-for-masquerade
		goto service-TPLZMVKW-kube-system/ingress-nginx-controller/tcp/http
	}

	chain endpoint-VRCVTPLF-kube-system/ingress-nginx-controller/tcp/https__192.168.64.68/443 {
		ip saddr 192.168.64.68 jump mark-for-masquerade
		meta l4proto tcp dnat to 192.168.64.68:443
	}

	chain service-HNB4FGVK-kube-system/ingress-nginx-controller/tcp/https {
		ip daddr 172.16.0.173 tcp dport 443 ip saddr != 192.168.64.64/26 jump mark-for-masquerade
		numgen random mod 1 vmap { 0 : goto endpoint-VRCVTPLF-kube-system/ingress-nginx-controller/tcp/https__192.168.64.68/443 }
	}

	chain external-HNB4FGVK-kube-system/ingress-nginx-controller/tcp/https {
		jump mark-for-masquerade
		goto service-HNB4FGVK-kube-system/ingress-nginx-controller/tcp/https
	}

	chain endpoint-XIULVOT6-kube-system/ingress-nginx-controller-admission/tcp/https-webhook__192.168.64.68/8443 {
		ip saddr 192.168.64.68 jump mark-for-masquerade
		meta l4proto tcp dnat to 192.168.64.68:8443
	}

	chain service-FMTKUH45-kube-system/ingress-nginx-controller-admission/tcp/https-webhook {
		ip daddr 172.16.0.220 tcp dport 443 ip saddr != 192.168.64.64/26 jump mark-for-masquerade
		numgen random mod 1 vmap { 0 : goto endpoint-XIULVOT6-kube-system/ingress-nginx-controller-admission/tcp/https-webhook__192.168.64.68/8443 }
	}

	chain endpoint-R3GEKHA3-default/kube-bench-dev/tcp/tcp-80__192.168.64.66/80 {
	}

	chain service-KQA2VLMF-default/kube-bench-dev/tcp/tcp-80 {
	}
}
```

#### What did you expect to happen?

External IP service DNAT rules should be installed, like when clusterIP is not None:
```
table ip kube-proxy {
	comment "rules for kube-proxy"
	set cluster-ips {
		type ipv4_addr
		comment "Active ClusterIPs"
		elements = { 172.16.0.1, 172.16.0.173,
			     172.16.0.220, 172.16.0.242 }
	}

	set nodeport-ips {
		type ipv4_addr
		comment "IPs that accept NodePort traffic"
		elements = { 192.168.64.2 }
	}

	map no-endpoint-services {
		type ipv4_addr . inet_proto . inet_service : verdict
		comment "vmap to drop or reject packets to services with no endpoints"
	}

	map no-endpoint-nodeports {
		type inet_proto . inet_service : verdict
		comment "vmap to drop or reject packets to service nodeports with no endpoints"
	}

	map firewall-ips {
		type ipv4_addr . inet_proto . inet_service : verdict
		comment "destinations that are subject to LoadBalancerSourceRanges"
	}

	map service-ips {
		type ipv4_addr . inet_proto . inet_service : verdict
		comment "ClusterIP, ExternalIP and LoadBalancer IP traffic"
		elements = { 172.16.0.173 . tcp . 80 : goto service-TPLZMVKW-kube-system/ingress-nginx-controller/tcp/http,
			     172.16.0.242 . tcp . 80 : goto service-KQA2VLMF-default/kube-bench-dev/tcp/tcp-80,
			     192.168.64.253 . tcp . 80 : goto external-KQA2VLMF-default/kube-bench-dev/tcp/tcp-80,
			     192.168.64.254 . tcp . 80 : goto external-TPLZMVKW-kube-system/ingress-nginx-controller/tcp/http,
			     172.16.0.1 . tcp . 443 : goto service-2QRHZV4L-default/kubernetes/tcp/https,
			     172.16.0.173 . tcp . 443 : goto service-HNB4FGVK-kube-system/ingress-nginx-controller/tcp/https,
			     172.16.0.220 . tcp . 443 : goto service-FMTKUH45-kube-system/ingress-nginx-controller-admission/tcp/https-webhook,
			     192.168.64.254 . tcp . 443 : goto external-HNB4FGVK-kube-system/ingress-nginx-controller/tcp/https }
	}

	map service-nodeports {
		type inet_proto . inet_service : verdict
		comment "NodePort traffic"
	}

	chain filter-prerouting {
		type filter hook prerouting priority dstnat - 10; policy accept;
		ct state new jump firewall-check
	}

	chain filter-input {
		type filter hook input priority -110; policy accept;
		ct state new jump nodeport-endpoints-check
		ct state new jump service-endpoints-check
	}

	chain filter-forward {
		type filter hook forward priority -110; policy accept;
		ct state new jump service-endpoints-check
		ct state new jump cluster-ips-check
	}

	chain filter-output {
		type filter hook output priority dstnat - 10; policy accept;
		ct state new jump service-endpoints-check
		ct state new jump firewall-check
	}

	chain filter-output-post-dnat {
		type filter hook output priority dstnat + 10; policy accept;
		ct state new jump cluster-ips-check
	}

	chain nat-prerouting {
		type nat hook prerouting priority dstnat; policy accept;
		jump services
	}

	chain nat-output {
		type nat hook output priority dstnat; policy accept;
		jump services
	}

	chain nat-postrouting {
		type nat hook postrouting priority srcnat; policy accept;
		jump masquerading
	}

	chain nodeport-endpoints-check {
		ip daddr @nodeport-ips meta l4proto . th dport vmap @no-endpoint-nodeports
	}

	chain service-endpoints-check {
		ip daddr . meta l4proto . th dport vmap @no-endpoint-services
	}

	chain firewall-check {
		ip daddr . meta l4proto . th dport vmap @firewall-ips
	}

	chain services {
		ip daddr . meta l4proto . th dport vmap @service-ips
		ip daddr @nodeport-ips meta l4proto . th dport vmap @service-nodeports
	}

	chain masquerading {
		meta mark & 0x00004000 == 0x00000000 return
		meta mark set meta mark ^ 0x00004000
		masquerade fully-random
	}

	chain cluster-ips-check {
		ip daddr @cluster-ips reject comment "Reject traffic to invalid ports of ClusterIPs"
	}

	chain mark-for-masquerade {
		meta mark set meta mark | 0x00004000
	}

	chain reject-chain {
		comment "helper for @no-endpoint-services / @no-endpoint-nodeports"
		reject
	}

	chain endpoint-KUBDMD37-default/kubernetes/tcp/https__192.168.64.2/6443 {
		ip saddr 192.168.64.2 jump mark-for-masquerade
		meta l4proto tcp dnat to 192.168.64.2:6443
	}

	chain service-2QRHZV4L-default/kubernetes/tcp/https {
		ip daddr 172.16.0.1 tcp dport 443 ip saddr != 192.168.64.64/26 jump mark-for-masquerade
		numgen random mod 1 vmap { 0 : goto endpoint-KUBDMD37-default/kubernetes/tcp/https__192.168.64.2/6443 }
	}

	chain endpoint-5UYISHKM-kube-system/ingress-nginx-controller/tcp/http__192.168.64.68/80 {
		ip saddr 192.168.64.68 jump mark-for-masquerade
		meta l4proto tcp dnat to 192.168.64.68:80
	}

	chain service-TPLZMVKW-kube-system/ingress-nginx-controller/tcp/http {
		ip daddr 172.16.0.173 tcp dport 80 ip saddr != 192.168.64.64/26 jump mark-for-masquerade
		numgen random mod 1 vmap { 0 : goto endpoint-5UYISHKM-kube-system/ingress-nginx-controller/tcp/http__192.168.64.68/80 }
	}

	chain external-TPLZMVKW-kube-system/ingress-nginx-controller/tcp/http {
		jump mark-for-masquerade
		goto service-TPLZMVKW-kube-system/ingress-nginx-controller/tcp/http
	}

	chain endpoint-VRCVTPLF-kube-system/ingress-nginx-controller/tcp/https__192.168.64.68/443 {
		ip saddr 192.168.64.68 jump mark-for-masquerade
		meta l4proto tcp dnat to 192.168.64.68:443
	}

	chain service-HNB4FGVK-kube-system/ingress-nginx-controller/tcp/https {
		ip daddr 172.16.0.173 tcp dport 443 ip saddr != 192.168.64.64/26 jump mark-for-masquerade
		numgen random mod 1 vmap { 0 : goto endpoint-VRCVTPLF-kube-system/ingress-nginx-controller/tcp/https__192.168.64.68/443 }
	}

	chain external-HNB4FGVK-kube-system/ingress-nginx-controller/tcp/https {
		jump mark-for-masquerade
		goto service-HNB4FGVK-kube-system/ingress-nginx-controller/tcp/https
	}

	chain endpoint-XIULVOT6-kube-system/ingress-nginx-controller-admission/tcp/https-webhook__192.168.64.68/8443 {
		ip saddr 192.168.64.68 jump mark-for-masquerade
		meta l4proto tcp dnat to 192.168.64.68:8443
	}

	chain service-FMTKUH45-kube-system/ingress-nginx-controller-admission/tcp/https-webhook {
		ip daddr 172.16.0.220 tcp dport 443 ip saddr != 192.168.64.64/26 jump mark-for-masquerade
		numgen random mod 1 vmap { 0 : goto endpoint-XIULVOT6-kube-system/ingress-nginx-controller-admission/tcp/https-webhook__192.168.64.68/8443 }
	}

	chain endpoint-R3GEKHA3-default/kube-bench-dev/tcp/tcp-80__192.168.64.66/80 {
		ip saddr 192.168.64.66 jump mark-for-masquerade
		meta l4proto tcp dnat to 192.168.64.66:80
	}

	chain service-KQA2VLMF-default/kube-bench-dev/tcp/tcp-80 {
		ip daddr 172.16.0.242 tcp dport 80 ip saddr != 192.168.64.64/26 jump mark-for-masquerade
		numgen random mod 1 vmap { 0 : goto endpoint-R3GEKHA3-default/kube-bench-dev/tcp/tcp-80__192.168.64.66/80 }
	}

	chain external-KQA2VLMF-default/kube-bench-dev/tcp/tcp-80 {
		jump mark-for-masquerade
		goto service-KQA2VLMF-default/kube-bench-dev/tcp/tcp-80
	}
}
```

#### How can we reproduce it (as minimally and precisely as possible)?

Apply the yaml

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.33.2

#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 主要反映了当 Service 的 “clusterIP: None” 时，kube-proxy 未按预期安装对应的外部 IP DNAT 规则，导致服务外部 IP 端访问异常。此问题看起来是一个功能或配置上的 Bug/缺陷，并未提及可被恶意利用或造成额外攻击面的可能性，也未涉及凭据泄露、命令执行、拒绝服务等安全问题，因此不属于安全风险范畴。

**解释说明：**

该问题本质是网络规则配置不完整导致功能异常，不属于安全漏洞或安全风险，不需要进一步演示如何利用来进行攻击。根据 Issue 风险判断标准，本问题未涉及到可被恶意利用的安全漏洞。因此不做安全复现脚本演示。若是用于验证该功能缺陷，可根据 Issue 提供的 yaml 示例如常规测试方式进行验证。

---


## Issue #131481 Nodes Tainted with NoExecute before going into PartialDisruption are not untainted after

- Issue 链接：[#131481](https://github.com/kubernetes/kubernetes/issues/131481)

### Issue 内容

#### What happened?

Nodes Tainted with NoExecute before going into PartialDisruption are not untainted after.

When nodes in a zone start going into unready state, the NLC taints those nodes with NoExecute for taint manager eviction. If more than 55% (UnhealthyZoneThreshold) of nodes go unhealthy in the same zone then the zone is treated as PartialDisruption and depending on the size of the cluster either we slow down the tainting logic or stop it.
https://github.com/kubernetes/kubernetes/blob/e54c8ef2024e638d721242224f6f925b15ee43f5/pkg/controller/nodelifecycle/node_lifecycle_controller.go#L1295

However all nodes that were tainted until NLC determines there is a PartialDisruption is not untainted. But untainting is done if cluster go into MasterDisruption.
https://github.com/kubernetes/kubernetes/blob/e54c8ef2024e638d721242224f6f925b15ee43f5/pkg/controller/nodelifecycle/node_lifecycle_controller.go#L1042

Nodes get tainted with NoExecute for taint-manager eviction, but if more nodes go unhealthy at the same time we determine there is a PartialZonalDisruption and stop tainting those nodes to stop pod eviction. 
Nodes that are already tainted need to be undone to preserve workloads.

#### What did you expect to happen?

Expected NLC to untaint nodes after going into Partial Zonal Disruption to prevent taint-manager-eviction. 

#### How can we reproduce it (as minimally and precisely as possible)?

Induce a case where nodes slowly start going unhealthy in a zone. 


#### Anything else we need to know?
NA

#### Kubernetes version
Shoudl be all kubernetes version. Tested on 1.32


#### Cloud provider
Should be across all CP, tested with AWS EKS. 


#### OS version
NA



#### Install tools
NA


#### Container runtime (CRI) and version (if applicable)
NA


#### Related plugins (CNI, CSI, ...) and versions (if applicable)
NA


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该 Issue 的描述来看，这一问题主要是 K8s 集群中节点在进入 PartialDisruption（局部不可用）之前已经被添加了 NoExecute Taint，导致某些节点在进入 PartialDisruption 后依然保持该 Taint，结果是工作负载无法正常调度到这些节点。此问题本质上属于 Kubernetes 中节点状态管理和容错机制的逻辑Bug或改进需求，不涉及越权访问、数据泄露或远程命令执行等安全漏洞场景，也不涉及攻击者可以利用的手段去提权或破坏系统安全性。

---


## Issue #131476 Multiple field managers on Server Side Applies lead to inconsistent behavior

- Issue 链接：[#131476](https://github.com/kubernetes/kubernetes/issues/131476)

### Issue 内容

#### What happened?

What we recently did was drop some affinity rules on a deployment, but those changes are not reflected on the deployment.

We moved from using Server side applies over CLI in CI to ArgoCD which uses Server Side applies as well. Though the fields that ArgoCD owned are reflective of the current state of the manifests and not the state of the manifest as it was applied by CLI.

As a result, we see multiple field managers active owning different fields of the resource and no matter which direction we move now, from client side to server, to purely ArgoCD as the field manager, we cannot reset state of the resource without replacing the resource itself.

We even tried dropping the `managedFields` on `metadata` but on a subsequent SSA, the field manager `before-first-apply` takes over those fields and SSA refuses to update it. 

We tried merging in the field managers but that too does not seem to fix it.

#### What did you expect to happen?

The deployment affinity rules change.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Apply a deployment resource with affinities using SSA over CLI. 
2. Apply the same deployment with a different field manager using SSA without affinities. 

The affinities do not change. Then null, the `managedFields` and apply the same resource without affinities and it does not change.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.3
Kustomize Version: v5.5.0
Server Version: v1.31.6-gke.1064001
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从 Issue 内容来看，问题主要与 Kubernetes Server Side Apply（SSA）在同一个资源上同时存在多个 field manager 所导致的资源状态不一致有关，无法正常更新或重置资源状态。该问题本质上是资源管理及更新方式冲突的功能性或兼容性问题，并未明确提及可能导致任意代码执行、权限提升、数据泄露或拒绝服务（DoS）等安全层面的隐患，也不属于凭据信息泄露。  
因此，该问题并不属于安全漏洞范畴，而是 Kubernetes 使用过程中出现的配置或管理层面的冲突。

**复现过程：**

```python
# 该问题与安全漏洞无关，暂无需复现脚本。
# 如果仍需在本地测试多 field manager 引起的冲突，可在测试环境中手动重复该 Issue 中的操作步骤，
# 但不涉及安全风险，不额外提供攻击性或者利用性的脚本。
```


---


## Issue #131467 Pod scheduling failed due to pod anti-affinity.

- Issue 链接：[#131467](https://github.com/kubernetes/kubernetes/issues/131467)

### Issue 内容

#### What happened?

The pod is bound to the PVC on the master1 node.However, the current pod is in the pending state.
The log is as follows:
```shell
I0425 08:09:37.487898      25 schedule_one.go:1107] "Unable to schedule pod; no fit; waiting" pod="default/managekvs-1" err="0/14 nodes are available: 1 node(s) didn't match pod anti-affinity rules, 11 node(s) didn't match Pod's node affinity/selector, 2 node(s) had volume node affinity conflict. preemption: 0/14 nodes are available: 1 No preemption victims found for incoming pod, 13 Preemption is not helpful for scheduling."
```
However, when I query the labels of all pods on the node, there is no conflicting label.
```shell
[root@master1 ~]# kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=master1 --show-labels | grep 'app=managekvs'
[root@master1 ~]# 
```
The pod anti-affinity configuration is as follows:
```yaml
podAntiAffinity:
  requiredDuringSchedulingIgnoredDuringExecution:
  - labelSelector:
      matchExpressions:
      - key: app
        operator: In
        values:
        - managekvs
    topologyKey: kubernetes.io/hostname
```

https://github.com/kubernetes/kubernetes/blob/948afe5ca072329a73c8e79ed5938717a5cb3d21/pkg/scheduler/framework/plugins/interpodaffinity/filtering.go#L224-L263
It is suspected that the cache in the informer is not the latest. As a result, the old data is obtained.

#### What did you expect to happen?

The pod should be scheduled successfully.

#### How can we reproduce it (as minimally and precisely as possible)?

After the NICs of a node are started and stopped for multiple times, pods cannot be scheduled after the NICs are recovered.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.31
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该Issue的描述来看，问题是由于Pod的调度被Pod的反亲和配置阻塞，并且怀疑是Informer缓存未及时更新导致旧数据被使用，进而造成被调度的Pod一直处于Pending状态。该问题本质上是Kubernetes集群运维或配置层面上的问题，不涉及任何敏感信息泄露、提权、命令执行或其他潜在的安全攻击场景，也未描述出存在可被恶意利用的安全漏洞。因此，该Issue并非安全漏洞或风险。

**复现过程：**

```python
# 该Issue不涉及安全问题，无安全复现脚本
```


**解释说明：**

该Issue的根cause为Pod间反亲和（anti-affinity）配置与调度过程中的缓存数据出现不一致，导致Pod无法被正确调度到相应节点。问题不涉及隐私信息泄露、特权提升、容器逃逸等安全风险，因此风险评级判断为“不涉及”。上述脚本仅为空脚本，用于在回答中保留必要结构，不涉及任何安全复现。

---


## Issue #131466 [WINDOWS] [KUBEPROXY] HNS LoadBalancer Fails to Update Endpoints After ModifyLoadBalancerAPI Invocation by Windows KubeProxy

- Issue 链接：[#131466](https://github.com/kubernetes/kubernetes/issues/131466)

### Issue 内容

#### What happened?

I'm using HNS version 15.4, where the ModifyLoadBalancer policy appears to be supported according to hnslib. KubeProxy is invoking the ModifyLoadBalancer API, but the endpoints it attempts to update do not match those reflected in HNS.

#### What did you expect to happen?

Following the ModifyLoadBalancer API call, all the requested endpoints should be successfully updated in HNS.

#### How can we reproduce it (as minimally and precisely as possible)?

- Create an AKS cluster with 1.31+ version of K8S and Windows OS version 2025
- Create a deployment and a loadbalancer service for the deployment.
- Check the endpoint list count in the newly create hns loadbalancer policy.
- Scale up the pods for the deployment and check the endpoint list count for the same hns lob policy. It will still show the old endpoint list.

#### Anything else we need to know?

_No response_

#### Kubernetes version



```console
1.31+
```

</details>


#### Cloud provider

AKS

</details>


#### OS version

Windows Server 2025

```console
PS C:\hpc> cd C:\k\debug
PS C:\k\debug> ipmo -Force .\hns.v2.psm1
WARNING: The names of some imported commands from the module 'hns.v2' include unapproved verbs that might make them less
discoverable. To find the commands with unapproved verbs, run the Import-Module command again with the Verbose parameter. For a list
 of approved verbs, type Get-Verb.
PS C:\k\debug> Invoke-HnsRequest -Type globals -Method GET -Id "version"

Major Minor
----- -----
   15     4


PS C:\k\debug>
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue描述来看，该问题表现为在Windows环境下，KubeProxy调用HNS的ModifyLoadBalancer API后，HNS LoadBalancer中的endpoints信息无法及时更新或与实际状态不同。这属于功能性或兼容性问题，未涉及越权访问、敏感信息泄露、命令执行、提权或容器逃逸等安全场景，也不属于因不当配置造成的信息外泄问题。因此，该Issue并不涉及安全漏洞或安全风险。

**解释说明：**

此处所讨论的问题是HNS LoadBalancer在API调用后未能同步更新endpoints，属于功能或兼容性Bug，未体现出任何可被利用的安全攻击面，不涉及安全风险。故无需提供安全漏洞复现脚本。该问题不影响项目安全性，可在后续排查功能实现、日志、API调用等方面进行修复或改进。

---


## Issue #131451 Enhance existing csi, volume_expand tests with recovery related features

- Issue 链接：[#131451](https://github.com/kubernetes/kubernetes/issues/131451)

### Issue 内容

#### What happened?

We should enhance existing `testsuites/volume_expand.go` with recover from volume_expansion related features, so as these tests are more robust and tested with all kind of CSI drivers.



#### What did you expect to happen?

Should have more coverage for volume expansion feature. 


#### How can we reproduce it (as minimally and precisely as possible)?

None

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue的标题和内容来看，描述的是要在现有与csi、volume_expand相关的测试中，增加容灾和恢复场景的测试。该Issue主要针对测试用例的完善与覆盖范围的扩展，并未涉及到敏感信息、权限不当使用、容器逃逸、提权、命令执行等安全风险，也没有提到任何可能导致信息泄露或系统被攻击的情形。根据Issue风险判断标准，判断此问题并非安全问题。

**复现过程：**

```python
# 由于该Issue不涉及安全风险，因此无需提供安全问题复现脚本。
# 下面仅给出一个占位脚本示例，说明如果后续需要测试相关场景
# 可以在此基础上进行功能拓展，而并不存在安全漏洞复现。
import time

def main():
    print("当前Issue不涉及安全风险，无需安全漏洞复现脚本。")
    time.sleep(1)
    print("测试结束。")

main()
```


---


## Issue #131448 HPA watch pod metrics calls fail with 422 error code

- Issue 链接：[#131448](https://github.com/kubernetes/kubernetes/issues/131448)

### Issue 内容

#### What happened?

HorizontalPodAutoscaler is making failed `watch` requests for pod metrics, then falling back to `list` calls which are successful. This doesn't immediately impact functionality but the extra failed requests can cause unnecessary load and skew metrics. 

__Request Details__
<span class="flex flex-column width-100"><span class="logs-table__wrapper"><span style="min-height: 832px; display: block;">
key | value
-- | --
verb | watch
requestURI | /apis/metrics.k8s.io/v1beta1/namespaces/default/pods?allowWatchBookmarks=true&labelSelector=run%3Dphp-apache&resourceVersionMatch=NotOlderThan&sendInitialEvents=true&watch=true
responseStatus.code | 422
user.username | system:serviceaccount:kube-system:horizontal-pod-autoscaler
userAgent | kube-controller-manager/v1.32.3 (linux/amd64) kubernetes/470a1dd/system:serviceaccount:kube-system:horizontal-pod-autoscaler
</span></span></span>

`kube-controller-manager` logs have the following error:
```
13 type.go:183] The watchlist request for pods ended with an error, falling back to the standard LIST semantics, err = ListOptions.meta.k8s.io "" is invalid: [resourceVersionMatch: Forbidden: resourceVersionMatch is forbidden for watch, resourceVersionMatch: Forbidden: resourceVersionMatch is forbidden unless resourceVersion is provided]
```

It appears that the watch request is missing a required `resourceVersion` parameter.


#### What did you expect to happen?

The watch pod metrics calls to succeed. 

#### How can we reproduce it (as minimally and precisely as possible)?

Create an HPA in a 1.32 cluster via https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/

* start metrics server
* `kubectl apply -f https://k8s.io/examples/application/php-apache.yaml `
* `kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10`

Observe audit logs for 422 errors. 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

kube-controller-manager/v1.32.3

</details>


#### Cloud provider

<details>
</details>


#### OS version

<details>
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>



</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue描述来看，该问题主要是因为HorizontalPodAutoscaler（HPA）在watch时缺少resourceVersion参数，导致请求以422错误结束，随后HPA退回到使用list请求的方式。此问题本质上是HPA在进行监控时出现的配置或参数使用不当导致的功能性错误，并未涉及敏感信息泄露、命令执行、提权、容器逃逸等安全相关风险，也不涉及可被恶意利用的攻击向量。因此，根据给出的风险判断标准，该Issue仅属于配置或使用层面的问题，不构成安全漏洞或安全风险。

**解释说明：**

该问题为HPA缺少正确的resourceVersion参数所导致的watch请求失败，不存在安全攻击面或敏感信息泄露，故不属于安全风险问题，不需要提供复现脚本。该问题和风险无关，建议从配置或功能实现角度排查并修复。

---


## Issue #131426 Busybox image fails to build for windows

- Issue 链接：[#131426](https://github.com/kubernetes/kubernetes/issues/131426)

### Issue 内容

#### What happened?

Postmerge job failed for the busybox image
ref - https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/post-kubernetes-push-e2e-busybox-test-images/1915196396294639616

Failed with an error:
```shell
#11 [stage-3  1/14] FROM mcr.microsoft.com/windows/nanoserver:1809@sha256:a728b60718df567cf9b8981be370ad3824b4d60018f33664f0e2477844abc440
#11 resolve mcr.microsoft.com/windows/nanoserver:1809@sha256:a728b60718df567cf9b8981be370ad3824b4d60018f33664f0e2477844abc440 0.0s done
#11 sha256:13684d75c68a31393ea54fcff0661fe84f07894d37c7e65be62e77fa70b23988 41.94MB / 105.25MB 0.3s
#11 sha256:13684d75c68a31393ea54fcff0661fe84f07894d37c7e65be62e77fa70b23988 51.38MB / 105.25MB 0.4s
#11 ...
 #12 [prep 3/9] ADD https://skanthak.homepage.t-online.de/download/curl-7.57.0.cab /curl.cab
#12 ERROR: Get "https://skanthak.homepage.t-online.de/download/curl-7.57.0.cab": tls: failed to verify certificate: x509: certificate signed by unknown authority
 #13 [prep 2/9] ADD https://github.com/kubernetes-sigs/windows-testing/raw/master/images/busybox/busybox.exe /busybox-dir/busybox.exe
#13 CANCELED
```

#### What did you expect to happen?

Build should go through

#### How can we reproduce it (as minimally and precisely as possible)?

```
make all WHAT=busybox
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>
NA
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 主要描述了在构建 Windows 平台的 Busybox 镜像时，由于获取第三方 CAB 文件时出现证书校验错误，导致镜像构建失败。此问题更多体现为环境或配置层面的构建失败，并未涉及可被攻击者利用的安全漏洞，如代码执行、权限提升、敏感信息泄露等。因此，该 Issue 不属于项目存在的安全问题，而是因证书不被信任导致的构建错误。

**复现过程：**

```python
# 该问题不涉及安全风险，不需要复现安全漏洞，因此无需提供对应的复现脚本。
# 如需在本地重现构建失败，可在 Windows 环境下执行以下伪代码，仅供参考：
# ------------------------------------------------------------
# 请注意，这段脚本只是模拟了证书验证失败导致的下载错误流程，并非实际漏洞POC
import subprocess
import time

def main():
    # 模拟构建命令
    command = ["docker", "build", "-t", "busybox-windows-test", "."]
    print("开始模拟构建 Busybox for Windows 的镜像...")
    try:
        result = subprocess.run(command, capture_output=True, text=True, timeout=60)
        if result.returncode != 0:
            print("构建失败，以下是可能的错误输出：")
            print(result.stderr)
        else:
            print("镜像构建成功。")
    except subprocess.TimeoutExpired:
        print("构建命令执行超时。")
    time.sleep(2)
    print("模拟流程结束。")

main()
```


**解释说明：**

上面的脚本仅展示了执行 Docker 构建命令的流程，以模拟在本地构建 Busybox for Windows 镜像时可能会遇到的下载或证书校验失败情况。由于本 Issue 不涉及任何可被攻击者利用的安全漏洞，因此无需提供真实的安全漏洞复现脚本。该脚本更多是帮助开发者了解如何在本地执行构建命令及观察构建失败的过程。若要实际解决证书问题，需要在构建环境中更新或安装相应的 CA 证书，或切换至受信任的下载源来完成镜像构建。

---


## Issue #131421 ConfigMap exists in namespace even after namespace is deleted

- Issue 链接：[#131421](https://github.com/kubernetes/kubernetes/issues/131421)

### Issue 内容

#### What happened?

A config map continued to exist in the cluster even after the namespace it belonged to was deleted. 

There was concurrent access between the `deleteCollection` issued by the namespace garbage collector and a SSA patch issued by a controller that recreated the ConfigMap at the exact same time.

This resulted in:

```
$ kubectl get ns project-be53bb1076fe4d7eaa153f767406a0f3
Error from server (NotFound): namespaces "project-be53bb1076fe4d7eaa153f767406a0f3" not found

$ kubectl get cm -n project-be53bb1076fe4d7eaa153f767406a0f3
NAME                   DATA   AGE
cluster-internal-cas   1      23h
```

It isn't clear to me why the API server allows object creation via SSA even after the namespace has a `deletionTimestamp` set.


#### What did you expect to happen?

Expected no objects in a namespace when namespace is gone.

#### How can we reproduce it (as minimally and precisely as possible)?

We could not manage to reproduce this, it is possibly a very rare race condition. I'll attach an image of k8s audit logs that we have.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.3
Kustomize Version: v5.5.0
Server Version: v1.30.9
```

</details>


#### Cloud provider

<details>
Azure
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该Issue可以看出，这是一个关于命名空间删除后，依然存在关联ConfigMap的情况。根据Issue的描述，原因是命名空间被垃圾回收器删除的同时，有另一个控制器使用Server-Side Apply (SSA) 并发创建了同名的ConfigMap，导致在已被标记删除的命名空间下，依旧出现ConfigMap的“残留”现象。

从安全角度分析，这属于命名空间中资源管理的竞争条件（race condition）或清理逻辑缺陷问题，但Issue并未描述因这个残留导致敏感信息泄露、权限提升、远程执行、容器逃逸等一系列安全后果，同时也没有说明攻击者可以利用此行为做进一步的恶意操作。

因此，该问题更倾向于一个业务或逻辑上的一致性缺陷，而非安全漏洞。

---


## Issue #131419 NodeExpandVolume is called unexpectedly for NFS

- Issue 链接：[#131419](https://github.com/kubernetes/kubernetes/issues/131419)

### Issue 内容

#### What happened?

While expanding a PVC (`fstype: nfs`) _NodeExpandVolume_ is called even though _ControllerExpandVolume_ returned '`NodeExpansionRequired: false'`. This results in _VolumeResizeFailed_ warning on the application pods though the PV and PVC has resized successfully by the controller.

CSI Driver:
```
csi-powerstore.dellemc.com
csi-resizer:v1.13.1
```

While the driver supports the NodeExpandVolume for the backend block volumes, it doesn't expect this call for the NFS thus the VolumeResizeFailed event.


PVC event:
```
  Normal   ProvisioningSucceeded   114s                  csi-powerstore.dellemc.com_powerstore-controller-5df67c6d96-l82zn_00d7e770-72be-4f87-9bf9-6258b6d7ff73  Successfully provisioned volume csivol-sl-5414b8e21b
  Normal   ExternalExpanding       52s                   volume_expand                                                                                           waiting for an external controller to expand this PVC
  Normal   Resizing                48s                   external-resizer csi-powerstore.dellemc.com                                                             External resizer is resizing volume csivol-sl-5414b8e21b
  Normal   VolumeResizeSuccessful  42s                   external-resizer csi-powerstore.dellemc.com                                                             Resize volume succeeded
```

Pod event:
```
  Warning  VolumeResizeFailed      18m                kubelet                  NodeExpandVolume.NodeExpandVolume failed for volume "csivol-sl-5414b8e21b" : Expander.NodeExpand failed to expand the volume rpc error: code = NotFound desc = Volume not found
  Warning  VolumeResizeFailed      2s (x16 over 18m)  kubelet                  NodeExpandVolume.NodeExpandVolume failed to resize volume for volume "csivol-sl-5414b8e21b" : volume resizing failed for unknown reason
```

Driver log:
```
{"level":"info","msg":"/csi.v1.Controller/ControllerExpandVolume: REQ 0022: VolumeId=68028ddd-9874-8ee8-e8d9-d2f58008526d/<ARRAY-ID>/nfs, CapacityRange=required_bytes:17179869184 , VolumeCapability=mount:\u003cfs_type:\"nfs\" \u003e access_mode:\u003cmode:MULTI_NODE_MULTI_WRITER \u003e , XXX_NoUnkeyedLiteral={}, XXX_sizecache=0","time":"2025-04-18T17:38:58.451308056Z"}
{"level":"info","msg":"/csi.v1.Controller/ControllerExpandVolume: REP 0022: CapacityBytes=17179869184, NodeExpansionRequired=false, XXX_NoUnkeyedLiteral={}, XXX_sizecache=0","time":"2025-04-18T17:39:04.280402864Z"}
```

RESIZER log:
```
I0418 17:39:04.280812       1 expand_and_recover.go:250] "Resize volume succeeded, start to update PV's capacity" PV="csivol-sl-5414b8e21b"
I0418 17:39:04.280841       1 controller.go:616] "Resize volume succeeded, start to update PV's capacity" PV="csivol-sl-5414b8e21b"
I0418 17:39:04.314826       1 resize_status.go:186] "Resize PVC finished" PVC="nsps1/pvcps1"
I0418 17:39:04.314868       1 expand_and_recover.go:195] "Update capacity of PV succeeded" PV="csivol-sl-5414b8e21b" capacity="16Gi"
I0418 17:39:04.314890       1 controller.go:326] "Started PVC processing for resize controller" key="nsps1/pvcps1"
I0418 17:39:04.314953       1 event.go:389] "Event occurred" object="nsps1/pvcps1" fieldPath="" kind="PersistentVolumeClaim" apiVersion="v1" type="Normal" reason="VolumeResizeSuccessful" message="Resize volume succeeded"
```

#### What did you expect to happen?

There should be no _VolumeResizeFailed_ warning/event on the pod.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a PVC with StorageClass having:
```
    csi.storage.k8s.io/fstype: nfs
    allowVolumeExpansion: true
```
2. Resize the PVC by editing it.
3. PVC is resized, but related Pod will have _VolumeResizeFailed_ warning due to _NodeExpandVolume_ failure since this call was not expected by the CSI driver for NFS.

#### Anything else we need to know?

Please refer https://github.com/kubernetes/kubernetes/issues/131397 and https://github.com/kubernetes/kubernetes/issues/131381

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.3
Kustomize Version: v5.5.0
Server Version: v1.32.3
```

</details>


#### Cloud provider

<details>
--
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Red Hat Enterprise Linux"
VERSION="9.5 (Plow)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="9.5"
PLATFORM_ID="platform:el9"
PRETTY_NAME="Red Hat Enterprise Linux 9.5 (Plow)"
ANSI_COLOR="0;31"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:redhat:enterprise_linux:9::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9"
BUG_REPORT_URL="https://issues.redhat.com/"

REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 9"
REDHAT_BUGZILLA_PRODUCT_VERSION=9.5
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.5"


$ uname -a
# Linux master-1-SPknwwOOoaVwH.domain 5.14.0-503.38.1.el9_5.x86_64 #1 SMP PREEMPT_DYNAMIC Sun Apr 13 22:01:49 EDT 2025 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该 Issue 的描述来看，主要问题是 NFS 类型的卷在进行扩容时，Controller 端已经返回 NodeExpansionRequired = false，但实际仍调用了 NodeExpandVolume 导致 Pod 侧出现 VolumeResizeFailed 警告。该问题更多是 CSI 驱动与 Kubernetes 协调逻辑上的不一致或实现缺陷，并未涉及到敏感信息泄露、命令执行、权限提升、DoS 攻击等安全漏洞，因此不属于安全风险问题。

**复现过程：**

```python
# 该Issue不涉及安全风险，无需复现脚本
```


---


## Issue #131413 please provide a procedure to shutdown the cluster

- Issue 链接：[#131413](https://github.com/kubernetes/kubernetes/issues/131413)

### Issue 内容

#### What happened?

Maybe I am just too blind to see, but apparently k8s lacks a procedure to stop the whole cluster, eg to move the hardware to another computing lab, to run the nodes in maintenance mode, etc.

Important features would be

- stop all "regular" work load before persistent storage (eg. Longhorn)
- unmount all volumes (local and remote)
- stop persistent storage before shuttting down the internal networks
- make sure all etcd databases are in sync



#### What did you expect to happen?

NA

#### How can we reproduce it (as minimally and precisely as possible)?

NA

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.3
Kustomize Version: v5.5.0
Server Version: v1.31.7+rke2r1
```

</details>


#### Cloud provider

<details>
on premises
</details>


#### OS version

<details>

```console
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ uname -a
Linux srvl012.ac.aixigo.de 6.1.0-32-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.129-1 (2025-03-06) x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从 Issue 的描述来看，用户仅仅是在询问如何安全、完整地关停 Kubernetes 集群，以便进行硬件迁移或维护操作，Issue 中并未提及任何与权限提升、敏感信息泄露、远程命令执行等相关的安全问题，也不存在错误配置或日志敏感信息泄露导致的风险。根据给出的风险判断标准，这类单纯的“怎么做”使用问题并未涉及安全漏洞，因此不属于安全风险范畴。

---


## Issue #131410 kubelet does not finish detaching volume from pod, preventing it from starting up again

- Issue 链接：[#131410](https://github.com/kubernetes/kubernetes/issues/131410)

### Issue 内容

#### What happened?

We noticed that we would have pods stuck in init with:
`Warning  FailedAttachVolume  27m   attachdetach-controller  Multi-Attach error for volume "pvc-1ca296f6-9b88-40ea-a235-f8a0a093b8c1" Volume is already exclusively attached to one node and can't be attached to another`

and looking closer, it was indeed true that the pod's previous node still had the volume attached!

Timeline:
4/3 2:31 pod evicted from node1, but node1 never completed the `UnmountDevice` step and still had the volume in VolumesInUse. However, presumably the volume was actually detached.
4/3 15:51 pod is now evicted from node2, and this time, `UnmountDevice` does complete, but the volume is not actually detached
4/3 15:54+ pod is scheduled to node3 but cannot init because the volume is still attached to node2

Kubelet logs from node1:
`
I0403 02:31:52.912891   23323 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"crmproperties-80-zz-vtdataroot-0\" (UniqueName: \"kubernetes.io/csi/ebs.csi.aws.com^vol-03058d13f643f58a2\") pod \"3b2ef881-f317-4110-ae5d-0e4dbfbdcd29\" (UID: \"3b2ef881-f317-4110-ae5d-0e4dbfbdcd29\") "
I0403 02:31:54.953778   23323 operation_generator.go:803] UnmountVolume.TearDown succeeded for volume "kubernetes.io/csi/ebs.csi.aws.com^vol-03058d13f643f58a2" (OuterVolumeSpecName: "crmproperties-80-zz-vtdataroot-0") pod "3b2ef881-f317-4110-ae5d-0e4dbfbdcd29" (UID: "3b2ef881-f317-4110-ae5d-0e4dbfbdcd29"). InnerVolumeSpecName "pvc-1ca296f6-9b88-40ea-a235-f8a0a093b8c1". PluginName "kubernetes.io/csi", VolumeGidValue ""
I0403 02:31:55.024934   23323 reconciler_common.go:292] "operationExecutor.UnmountDevice started for volume \"pvc-1ca296f6-9b88-40ea-a235-f8a0a093b8c1\" (UniqueName: \"kubernetes.io/csi/ebs.csi.aws.com^vol-03058d13f643f58a2\") on node \"ip-172-26-126-253.ap-southeast-2.compute.internal\" "
`

and this hung around for many hours like this, with no `UnmountDevice succeeded for volume` / `Volume detached for volume`. This was resolved by terminating the first node, so the volume would be released. 

Kube-controller-manager cannot detach the volume because it appears to still be in use by node1:
`
E0403 15:49:16.063873       2 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/ebs.csi.aws.com^vol-03058d13f643f58a2 podName: nodeName:}" failed. No retries permitted until 2025-04-03 15:51:18.063856131 +0000 UTC m=+687036.925234400 (durationBeforeRetry 2m2s). Error: DetachVolume.Detach failed for volume "pvc-1ca296f6-9b88-40ea-a235-f8a0a093b8c1" (UniqueName: "kubernetes.io/csi/ebs.csi.aws.com^vol-03058d13f643f58a2") on node "ip-172-26-126-253.ap-southeast-2.compute.internal" : DetachVolume failed for volume "pvc-1ca296f6-9b88-40ea-a235-f8a0a093b8c1" (UniqueName: "kubernetes.io/csi/ebs.csi.aws.com^vol-03058d13f643f58a2") on node "ip-172-26-126-253.ap-southeast-2.compute.internal" : volume is still in use by node, according to Node status
`

Checking the aws console, the volume was actually attached to node2. I believe this bug was only uncovered now because we upgraded karpenter versions to one that will not terminate a node if there are volumes attached to it (checking aws), so previously it would terminate node2 upon consolidation and that would release the volume, but now the pod is stuck for however long node2 sticks around.


#### What did you expect to happen?

I would expect that a pod should not finish shutdown if `UnmountDevice` did not complete and its volume is still in VolumesInUse. So I would expect the pod to stay running on both node1 and node2 until the volume was definitely detached and the node status consistent. 

#### How can we reproduce it (as minimally and precisely as possible)?

I don't actually have a reproducer, it seems to happen on clusters that have a lot of pod/node movement.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.7
Kustomize Version: v5.4.2
Server Version: v1.31.7
```

</details>


#### Cloud provider

<details>
aws
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="AlmaLinux"
VERSION="9.3 (Shamrock Pampas Cat)"
ID="almalinux"
ID_LIKE="rhel centos fedora"
VERSION_ID="9.3"
PLATFORM_ID="platform:el9"
PRETTY_NAME="AlmaLinux 9.3 (Shamrock Pampas Cat)"
ANSI_COLOR="0;34"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:almalinux:almalinux:9::baseos"
HOME_URL="https://almalinux.org/"
DOCUMENTATION_URL="https://wiki.almalinux.org/"
BUG_REPORT_URL="https://bugs.almalinux.org/"

ALMALINUX_MANTISBT_PROJECT="AlmaLinux-9"
ALMALINUX_MANTISBT_PROJECT_VERSION="9.3"
REDHAT_SUPPORT_PRODUCT="AlmaLinux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.3"
$ uname -a
Linux ip-172-26-17-214 6.1.127-hs95.el9.aarch64 #1 SMP Wed Feb 19 00:18:56 UTC 2025 aarch64 aarch64 aarch64 GNU/Linux

# On Windows:

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
aws-ebs-csi v1.28.0
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该 Issue 内容看，描述的问题是：在 Kubernetes 集群中，当 Pod 从某个节点迁移后，旧节点 kubelet 未能完成对相关卷的卸载（UnmountDevice），导致该卷在旧节点仍被标记为占用，从而无法被新节点再次成功挂载。此问题造成 Pod 长时间处于无法启动的状态，影响可用性。但从安全视角分析，该问题并未透露出可被攻击者利用的安全缺陷，比如未经授权的访问、越权操作、敏感信息泄露、容器逃逸、命令执行等。它更像是 Kubernetes attach/detach 控制流程中的一个稳定性或一致性 bug，而非可被利用的安全漏洞。因此，根据给出的风险判断标准，该问题不属于安全问题。

**复现过程：**

```python
# 由于该 Issue 并不涉及安全风险，无需提供专门的安全漏洞复现POC。
# 下方示例脚本仅作为占位说明，不会实际执行任何可利用攻击的操作。

def main():
    print("该Issue为kubelet在卷卸载过程中的一致性问题，不涉及安全漏洞，无需POC复现。")

main()
```


---


## Issue #131402 NodeResizePending status not removed after successful NodeExpandVolume

- Issue 链接：[#131402](https://github.com/kubernetes/kubernetes/issues/131402)

### Issue 内容

#### What happened?

When resizing a Persistent Disk CSI driver-backed volume, although the node expansion succeeds and the CSI driver completes NodeExpandVolume, the PVC's AllocatedResourceStatus NodeResizePending condition persists.

```
 spec:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 15Gi
    storageClassName: standard-rwo
    volumeMode: Filesystem
    volumeName: pvc-22d448b9-d98a-46d5-9910-fd2cb0029a7d
  status:
    accessModes:
    - ReadWriteOnce
    allocatedResourceStatuses:
      storage: NodeResizePending
    allocatedResources:
      storage: 15Gi
    capacity:
      storage: 15Gi
    phase: Bound
```

Upon pod restart, the NodeExpandVolume operation successfully resizes the file system.
```
/csi.v1.Node/NodeExpandVolume returned with response: capacity_bytes:16106127360
"NodeExpandVolume succeeded on volume Key{"pvc-8fb88ce1-0feb-47c7-9993-e6bf9d0d64e7", zone: "x"} to size 16106127360"
```
Despite this, node logs show that NodeExpandVolume is failing due to an unidentified error.
```
nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/pd.csi.storage.gke.io^projects/x/zones/x/disks/pvc-8fb88ce1-0feb-47c7-9993-e6bf9d0d64e7 podName: nodeName:}" failed. No retries permitted until 2025-04-18 20:17:09.901667408 +0000 UTC m=+10542.012736234 (durationBeforeRetry 16s). Error: NodeExpandVolume.NodeExpandVolume failed to resize volume for volume "pvc-8fb88ce1-0feb-47c7-9993-e6bf9d0d64e7" (UniqueName: "kubernetes.io/csi/pd.csi.storage.gke.io^projects/x/zones/x/disks/pvc-8fb88ce1-0feb-47c7-9993-e6bf9d0d64e7") pod "pod" (UID: "18e508ae-a670-4486-b993-e6355d0abcff") : volume resizing failed for unknown reason
```
The Kubelet appears to unexpectedly enter a placeholder code path. This issue can be traced to the following lines in the Kubernetes source code: https://github.com/kubernetes/kubernetes/blob/680ea07dbb2c6050d13b93660fa4d27d2d28d6eb/pkg/volume/util/operationexecutor/operation_generator.go#L1921-L1924. 

#### What did you expect to happen?

After completion of NodeExpandVolume, we expect that NodeResizePending status is removed.
```
  spec:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 15Gi
    storageClassName: standard-rwo
    volumeMode: Filesystem
    volumeName: pvc-8fb88ce1-0feb-47c7-9993-e6bf9d0d64e7
  status:
    accessModes:
    - ReadWriteOnce
    allocatedResources:
      storage: 15Gi
    capacity:
      storage: 15Gi
    phase: Bound
```


#### How can we reproduce it (as minimally and precisely as possible)?

1. Deploy a GKE cluster with master version `1.32.2-gke.1182003` and one node running version `1.31.6-gke.1020000`.
2. Create a pod that utilizes a `standard-rwo` PersistentVolumeClaim (PVC) with an initial size of 5Gi.
3. After the pod is operational, expand the PVC's storage capacity to 15Gi.
4. Restart the pod.
5. Examine the status of the PVC.

#### Anything else we need to know?

The "NodeResizePending" status resolves successfully when the node and cluster versions match at v1.31.6-gke.1020000. This issue seems to arise only when the cluster and node versions are different.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.2
Kustomize Version: v5.5.0
Server Version: v1.32.2-gke.1182003
```
Node version: v1.31.6-gke.1020000
Resizer version: v1.13.1-gke.0
</details>


#### Cloud provider

<details>
Google Cloud Platform
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux rodete"
NAME="Debian GNU/Linux rodete"
VERSION_CODENAME=rodete
ID=debian
HOME_URL="https://go/glinux"
SUPPORT_URL="https://go/techstop"
BUG_REPORT_URL="https://go/techstop"
$ uname -a
Linux elijahrb-mt.c.googlers.com 6.12.17-1rodete2-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.12.17-1rodete2 (2025-03-12) x86_64 GNU/Linux


#### Install tools

<details>
Google Kubernetes Engine

```
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
External resizer: v1.13.1-gke.0
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue的描述来看，这是一个关于Persistent Volume卷扩容后状态未及时更新的功能或兼容性问题，并未体现出任何敏感信息泄露、认证绕过、越权操作、命令执行或容器逃逸等安全漏洞的可能性。该问题主要是Kubernetes不同版本之间在NodeExpandVolume处理逻辑上存在差异，导致状态未被及时清除，属于功能性或兼容性Bug，没有直接造成可被恶意利用的安全风险。因此，根据Issue风险判断标准，此问题与安全风险无关。

**解释说明：**

该Issue本质上是部署在不同行为版本之间的Kubernetes集群中出现的功能性异常或兼容性问题，并不会导致系统额外暴露攻击面，也不涉及敏感信息泄露、提权或命令执行等可导致安全风险的场景。所以本Issue被判定为“不涉及”安全风险，无需提供安全复现脚本。该问题的解决应重点关注版本兼容性和状态同步机制。若需要复现，可按照Issue描述的操作步骤在不同版本混用的Kubernetes集群中对PVC执行扩容操作，即可观察到NodeResizePending状态未被清除的Bug，但这并不构成安全漏洞。

---


## Issue #131397 NodeExpandVolume is getting called unexpectedly after pvc resize

- Issue 链接：[#131397](https://github.com/kubernetes/kubernetes/issues/131397)

### Issue 内容

#### What happened?

We did a first resize of pv and pvc it was successful,
Then we did a another resize, we get a warning on pvc describe

```
failed to expand pvc with rpc error: code = Internal desc = Could not resize volume "vol-0157496f5c5f8ee9a": rpc error: code = Internal desc = Could not modify volume "vol-0157496f5c5f8ee9a": operation error EC2: ModifyVolume, https response error StatusCode: 400, RequestID: f4a49e65-d0e7-46de-b308-36ca67a9d796, api error VolumeModificationRateExceeded: You''ve reached the maximum modification rate per volume limit. Wait at least 6 hours between modifications per EBS volume.
``` 



And after 6 hours a successful resize of pv and pvc happened on EBS volume and the pod(osd) consuming it get resized, but we are getting this warning in pod(osd) describe that is using pvc

```
Warning VolumeResizeFailed 4m55s (x45 over 80m) kubelet NodeExpandVolume.NodeExpandVolume failed to resize volume for volume "pvc-6f50163a-862b-4010-a338-3b587adf8cc1" : volume resizing failed for unknown reason
```

Driver
```
  csi:
    driver: ebs.csi.aws.com
```

StorageClass
```
  storageClassName: gp2-csi

```

#### What did you expect to happen?

There should be no such warning as the pv and pvc are resized,

#### How can we reproduce it (as minimally and precisely as possible)?

Initiate a PVC resize operation with CSI Driver ebs.csi.aws.com

#### Anything else we need to know?

refer https://github.com/kubernetes/kubernetes/issues/117871, https://github.com/kubernetes/kubernetes/pull/123055#issuecomment-2816240906

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

k8s version := 1.32

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue描述来看，该问题是关于在AWS EBS上执行卷扩容（PVC/PV resize）时，由于VolumeModificationRateExceeded导致的暂时性失败，以及随后kubelet层在扩容操作成功后依旧提示NodeExpandVolume相关的警告。  
整个过程主要涉及存储资源的使用和操作频率限制，并未出现敏感信息暴露、权限提升、数据泄露或其他安全隐患，也未见提及可导致拒绝服务攻击、命令执行、容器逃逸等安全问题。  
因此，该Issue并非一个安全问题，不涉及安全风险。

**复现过程：**

```python
# 由于该Issue不涉及安全风险，因此无需提供安全漏洞复现脚本。
# 如果想在本地模拟EBS卷多次扩容的情形，可在使用kubernetes库连接到集群后，
# 重复发起对同一PVC的多次resize操作并观察事件和日志即可。
# 以下示例仅提供一个思路演示，与安全漏洞无关。

import time
from kubernetes import client, config

def main():
    # 假设默认kubeconfig已经配置好
    config.load_kube_config()

    v1 = client.CoreV1Api()
    storage_v1 = client.StorageV1Api()
    custom_api = client.CustomObjectsApi()

    namespace = "default"  # 根据实际命名空间修改
    pvc_name = "my-pvc"    # 假设存在名为my-pvc的PVC可供实验

    try:
        # 第一次扩容
        patch_pvc_size(v1, pvc_name, namespace, "10Gi")
        time.sleep(3)
        # 第二次扩容(短时间连续发起)
        patch_pvc_size(v1, pvc_name, namespace, "20Gi")
    except Exception as e:
        print(f"出现异常：{e}")

def patch_pvc_size(k8s_client, pvc_name, namespace, new_size):
    pvc_patch = {
        "spec": {
            "resources": {
                "requests": {
                    "storage": new_size
                }
            }
        }
    }
    resp = k8s_client.patch_namespaced_persistent_volume_claim(
        name=pvc_name,
        namespace=namespace,
        body=pvc_patch
    )
    print(f"PATCH PVC {pvc_name} -> {new_size}, 响应：{resp}")

main()
```


**解释说明：**

1. 脚本中演示了如何利用Python的kubernetes库对同一个PVC进行多次扩容操作，从而在短时间内模拟多次扩容的过程；  
2. 由于该Issue并不涉及安全风险，所以无需提供安全漏洞的复现脚本，这里的示例仅用于展示PVC扩容操作，帮助理解和定位Issue也可能产生的相关报错或警告；  
3. 脚本示例仅供本地测试与研究使用，注意合理调整PVC名称、命名空间以及扩容策略，避免对生产环境造成不必要的影响。

---


## Issue #131396 dns query timeout on 1.31

- Issue 链接：[#131396](https://github.com/kubernetes/kubernetes/issues/131396)

### Issue 内容

#### What happened?

After upgrading from Kubernetes version 1.29 to 1.31, DNS resolving is timeouts. This issue is not limited to specific pods; it occurs in all pods running on version 1.31, and it did not happen in version 1.29.

In version 1.29, I performed DNS queries using the following command:

```shell
$> kubectl exec -it grafana-server-76cf4f8d7d-r4sgx  -n grafana -- nslookup google.com
Server:         10.233.0.3
Address:        10.233.0.3:53

Non-authoritative answer:
Name:   google.com
Address: 142.250.76.142

*** Can't find google.com: No answer
```

When running nslookup in version 1.31, the output is as follows:

```shell
$> kubectl exec -it -n grafana grafana-server-6f8d7bfdb6-jddwk -- nslookup google.com
;; Got recursion not available from 10.233.0.3
Server:         10.233.0.3
Address:        10.233.0.3#53

;; communications error to 10.233.0.3#53: timed out
```

#### What did you expect to happen?

 DNS queries normal response.

#### How can we reproduce it (as minimally and precisely as possible)?

```shell
$> kubectl exec -it -n grafana grafana-server-6f8d7bfdb6-jddwk -- nslookup google.com
;; Got recursion not available from 10.233.0.3
Server:         10.233.0.3
Address:        10.233.0.3#53

;; communications error to 10.233.0.3#53: timed out
```

#### Anything else we need to know?

1, Using Tool
k8s provisioning tool on kubespray 2.27
cni : calico
dns : coredns
1. coredns pod status 
```shell
 $> kubectl get pod -n kube-system | egrep -i dns
coredns-d665d669-c4hzr                     1/1     Running   0               2d23h
coredns-d665d669-chvr4                     1/1     Running   0               2d23h
dns-autoscaler-5cb4578f5f-gsg77            1/1     Running   0               2d23h

```
2. coredns configmap
```shell
$> kubectl  get cm coredns -n kube-system -o yaml
apiVersion: v1
data:
  Corefile: |
    .:53 {
        errors {
        }
        health {
            lameduck 5s
        }
        ready
        kubernetes k8s-dev in-addr.arpa ip6.arpa {
          pods insecure
          fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        forward . /etc/resolv.conf {
          prefer_udp
          max_concurrent 1000
        }
        cache 30

        loop
        reload
        loadbalance
    }
kind: ConfigMap

```
4. 
#### Kubernetes version

<details>

1. 1.31 k8s
```console
$ kubectl version
kubectl version
Client Version: v1.31.7
Kustomize Version: v5.4.2
Server Version: v1.31.7
```

1. 1.29 k8s
```shell
 kubectl version
Client Version: v1.29.10
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.10
```


</details>


#### Cloud provider

bare-metal

#### OS version

<details>

```console
# On Linux:
$> cat /etc/os-release
NAME="Rocky Linux"
VERSION="9.5 (Blue Onyx)"
ID="rocky"
ID_LIKE="rhel centos fedora"
VERSION_ID="9.5"
PLATFORM_ID="platform:el9"
PRETTY_NAME="Rocky Linux 9.5 (Blue Onyx)"
ANSI_COLOR="0;32"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:rocky:rocky:9::baseos"
HOME_URL="https://rockylinux.org/"
VENDOR_NAME="RESF"
VENDOR_URL="https://resf.org/"
BUG_REPORT_URL="https://bugs.rockylinux.org/"
SUPPORT_END="2032-05-31"
ROCKY_SUPPORT_PRODUCT="Rocky-Linux-9"
ROCKY_SUPPORT_PRODUCT_VERSION="9.5"
REDHAT_SUPPORT_PRODUCT="Rocky Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.5"

$> uname -a
Linux k8s-master1 5.14.0-503.21.1.el9_5.x86_64 #1 SMP PREEMPT_DYNAMIC Wed Jan 8 17:35:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>
kubespray 2.27
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
$> crio -version
crio version 1.29.1
Version:        1.29.1
GitCommit:      78e179ba8dd3ce462382a17049e8d1f770246af1
GitCommitDate:  2024-01-11T21:36:06Z
GitTreeState:   clean
BuildDate:      1970-01-01T00:00:00Z
GoVersion:      go1.21.1
Compiler:       gc
Platform:       linux/amd64
Linkmode:       static
BuildTags:
  static
  netgo
  osusergo
  exclude_graphdriver_btrfs
  exclude_graphdriver_devicemapper
  seccomp
  apparmor
  selinux
LDFlags:          unknown
SeccompEnabled:   true
AppArmorEnabled:  false


</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
CNI : calico
kubectl  describe ds calico-node  -n kube-system
Name:           calico-node
Selector:       k8s-app=calico-node
Node-Selector:  kubernetes.io/os=linux
Labels:         k8s-app=calico-node
Annotations:    deprecated.daemonset.template.generation: 1
Desired Number of Nodes Scheduled: 4
Current Number of Nodes Scheduled: 4
Number of Nodes Scheduled with Up-to-date Pods: 4
Number of Nodes Scheduled with Available Pods: 4
Number of Nodes Misscheduled: 0
Pods Status:  4 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:           k8s-app=calico-node
  Service Account:  calico-node
  Init Containers:
   upgrade-ipam:
    Image:      quay.io/calico/cni:v3.27.4
    Port:       <none>
    Host Port:  <none>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该 Issue 的描述来看，问题主要体现在从 Kubernetes 1.29 升级到 1.31 之后，DNS 查询出现超时，导致容器内 DNS 无法正常解析。整篇内容更多是关于升级后出现的配置或网络故障问题，并未体现任何由于错误配置或实现而带来的攻击面、漏洞利用或权限提升等安全风险，也未提及敏感信息泄露等情况。

根据给出的风险判断标准，这属于常见的运维或配置问题，并不涉及攻击者可利用或导致信息泄露等安全风险，因此不构成安全漏洞。

---


## Issue #131387 CoreDNS HNS Endpoint IP is not updated on pod restart

- Issue 链接：[#131387](https://github.com/kubernetes/kubernetes/issues/131387)

### Issue 内容

#### What happened?

When CoreDNS pod on Linux node restarts and receives a new cluster IP, endpoints/endpointslices in Kubertenes are updated with new pod IP, but on Windows worker HNS Endpoint still holds the old IP address. Kubernetes restart on Windows doesn't help, need to delete all stuff and re-join node to cluster again.

#### What did you expect to happen?

 HNS Endpoint would be updated with new IP address after CoreDNS pod restart

#### How can we reproduce it (as minimally and precisely as possible)?


1.    Deploy a cluster with 3 Linux master nodes.
2.    Join 1 Windows worker node.
3.    Start sample workload on Windows node (used mcr.microsoft.com/dotnet/framework/samples:aspnetapp)
4.    Exec into Windows pod with cmd, type nslookup, observe working output
```
Default Server:  kube-dns.kube-system.svc.cluster.local
Address:  10.43.0.10
```
5.    Exit Windows pod, get pods list from kube-system namespace with -o wide, note CoreDNS pod ip.
6.    Delete CoreDNS pod, wait for new to start, check its IP address.
7.    Exec again into Windows pod, type nslookup, observe timeout errors
```
DNS request timed out.
    timeout was 2 seconds.
Default Server:  UnKnown
Address:  10.43.0.10
```
8.    Type server <new_coredns_pod_ip>, and after timeout try to resolve any dns name (for example c.cc), you would see that connectivity is working.
9.    RDP into Windows server, start Powershell, run
```
hnsdiag list all | findstr 10.43.0.10
```
you will see the output like this
```
ce87ca71-adfd-4334-a5ce-760eae297bef |  10.43.0.10      | 6f995c07-0b4a-4cfb-ba6e-c62dd48a554d
302522f6-f540-4120-881c-b7a2eeecf0f6 |  10.43.0.10      | 6f995c07-0b4a-4cfb-ba6e-c62dd48a554d
ddd666f5-c1e9-4fe1-af6b-da97be5cdb29 |  10.43.0.10      | 6f995c07-0b4a-4cfb-ba6e-c62dd48a554d
```
10.    Run
```
get-hnsendpoint 6f995c07-0b4a-4cfb-ba6e-c62dd48a554d
```
and in output IPAddress would be old CoreDNS pod IP address, not the new one.

#### Anything else we need to know?

The deployment is done with k3s all in one binary, which could make some influence here, but the issue looks like in Kubernetes itself.
The one thing I didn't test is what would happen if I would update HNS Endpoint with the new pod IP, but unfortunately I didn't find a way to perform such update. Any ideas? :)

#### Kubernetes version

1.28 - 1.32

#### Cloud provider

k3s baremetal deployment


#### OS version

<details>
```
PRETTY_NAME="Ubuntu 22.04.5 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
```
$ uname -a
Linux test01 5.15.0-136-generic #147-Ubuntu SMP Sat Mar 15 15:53:30 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux


# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
BuildNumber  Caption                                 OSArchitecture  Version
17763        Microsoft Windows Server 2019 Standard  64-bit          10.0.17763
```

```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该 Issue 的描述来看，主要是 Windows 节点在 CoreDNS Pod 重启后无法及时更新 HNS Endpoint IP，导致 DNS 请求异常。该问题本质上是网络配置与同步机制的问题，不涉及信息泄露、敏感操作、命令执行或提权等安全层面的风险，也没有明确描述可被攻击者利用进行拒绝服务（DoS）的操作场景。不存在需要额外权限才能发起的攻击或导致更严重后果的可能性。因此综合判断该问题并不是安全风险，而是功能性或兼容性缺陷。

**解释说明：**

此问题为网络配置 Bug，不构成可被利用的安全漏洞，无需编写安全漏洞复现脚本。该问题更多地需要在 Windows 上针对 HNS Endpoint 的更新逻辑进行修复或排查，通过 Kubernetes 或 k3s 的网络插件机制来正确地更新 CoreDNS 对应的 IP 地址。此处不需安全风险复现脚本。

---


## Issue #131384 123

- Issue 链接：[#131384](https://github.com/kubernetes/kubernetes/issues/131384)

### Issue 内容

#### What happened?

123

#### What did you expect to happen?

123

#### How can we reproduce it (as minimally and precisely as possible)?

123

#### Anything else we need to know?

123

#### Kubernetes version

<details>
123
```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue的标题和内容来看，均为“123”占位符信息，没有任何可识别的安全相关描述或疑似安全风险点，也未提供关于配置错误、漏洞利用或敏感信息泄露等内容，因此无法判断出任何安全隐患。

**复现过程：**

```python
# 该Issue不存在安全风险，无需复现脚本
```


**解释说明：**

由于该Issue并未描述任何实际的安全风险点，不涉及高级别或低级别的安全问题，因此无需提供复现脚本。上述代码块仅用于占位说明。若后续提供了更具体的技术细节或风险信息，再进行进一步分析并提供相应的复现脚本。

---


## Issue #131381 NodeResizeError persists after successful PVC resize with CSI drivers lacking NodeExpandVolume support

- Issue 链接：[#131381](https://github.com/kubernetes/kubernetes/issues/131381)

### Issue 内容

#### What happened?

After resizing a PVC backed by the CephFS CSI driver (which does not implement NodeExpandVolume), Kubernetes successfully performs the controller-side resize. The PVC's capacity and allocatedResources fields are updated to the correct value, and the volume is usable in the pod.

However, the PVC status remains in a NodeResizeError condition indefinitely, with a message like:

```
conditions:
- type: NodeResizeError
  status: "True"
  message: failed to expand pvc with NodeExpand is not supported by the CSI driver rook-ceph.cephfs.csi.ceph.com

```
This happens even after pod restarts and with RecoverVolumeExpansionFailure=true enabled. The error condition is cosmetic — the filesystem is resized correctly — but it is never cleared, causing persistent alerts and misleading status.

#### What did you expect to happen?

Kubernetes (or the external resizer) should detect that:

*    The CSI driver does not support NodeExpandVolume

*    The resize succeeded on the controller side

*    The PVC is healthy and matches the requested size

It should then automatically clear the NodeResizeError condition.

#### How can we reproduce it (as minimally and precisely as possible)?

1.    Use the Rook CephFS CSI driver with a StorageClass that allows volume expansion.

2.    Ensure the CSI driver supports ControllerExpandVolume but not NodeExpandVolume.

3.    Enable RecoverVolumeExpansionFailure=true on the kube-controller-manager.

4.    Create and bind a PVC.

5.    Resize the PVC by increasing its spec.resources.requests.storage.

 6.   Wait for pod to restart or manually restart the pod.

7.    Observe that the PVC is resized (capacity and allocatedResources match), but NodeResizeError condition is never cleared.


#### Anything else we need to know?

This is not a functional error — the PVC works as intended. However, the error condition misleads monitoring tools and operators. It appears that Kubernetes never recovers from a permanent NodeExpandVolume failure, even when the resize is complete and usable.

Possibly related but distinct from earlier resize error tracking issues. As of Kubernetes 1.29+, this behavior uses new condition types like NodeResizeError and NodeResizeInfeasible, but the PVC controller does not reconcile them correctly.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.3
Server Version: v1.32.3
```

</details>


#### Cloud provider

<details>
Bare Metal / Talos Linux
</details>


#### OS version

<details>

```console
Talos Linux 1.9
```

</details>


#### Install tools

<details>
Talos + Flux + Helm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd 1.7.x (via Talos)
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

    CSI Driver: quay.io/cephcsi/cephcsi:v3.13.0

    External Resizer: registry.k8s.io/sig-storage/csi-resizer:v1.13.1

    Provisioner: csi-provisioner:v5.1.0

    Feature Gate: RecoverVolumeExpansionFailure=true

</details>

PVC status: ```
status:
  accessModes:
  - ReadWriteMany
  allocatedResources:
    storage: 9Gi
  capacity:
    storage: 9Gi
  conditions:
  - type: NodeResizeError
    status: "True"
    message: failed to expand pvc with NodeExpand is not supported by the CSI driver rook-ceph.cephfs.csi.ceph.com
```

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue的描述来看，这是一个与Kubernetes在PVC扩容功能上表现出的持久性错误状态相关的问题。具体地，使用不支持NodeExpandVolume的CSI驱动进行PVC扩容后，PVC的状态一直保持在NodeResizeError，而实际上文件系统已经成功扩容。该问题只是一个功能或状态同步方面的缺陷，不会导致任意代码执行、权限提升、敏感信息泄露等安全风险。  
因此，依据提供的风险判断标准，该Issue不涉及安全漏洞，也不会对系统构成安全威胁，属于功能性或可靠性问题。

**复现过程：**

```python
# 由于本Issue不涉及安全风险，无需提供安全漏洞相关的复现脚本
# 此处仅示例如何连接Kubernetes集群检查PVC状态的逻辑，非漏洞POC

import os
import time
from kubernetes import client, config

def main():
    # 假设在默认位置（如 '~/.kube/config'）已经配置好kubeconfig
    config.load_kube_config()

    core_v1 = client.CoreV1Api()
    
    # 注意留足超时余量，不要造成死循环，此处示例等待最多100秒
    deadline = time.time() + 100
    namespace = "default"
    pvc_name = "test-pvc"

    while time.time() < deadline:
        pvc = core_v1.read_namespaced_persistent_volume_claim(name=pvc_name, namespace=namespace)
        
        # 打印PVC当前conditions状态
        if pvc.status.conditions:
            for condition in pvc.status.conditions:
                print(f"PVC Condition Type: {condition.type}, Status: {condition.status}, Message: {condition.message}")
        else:
            print("No conditions on PVC")
        
        # 检查PVC状态中是否有NodeResizeError
        node_resize_error = any(cond.type == "NodeResizeError" and cond.status == "True" for cond in pvc.status.conditions or [])
        if not node_resize_error:
            print("PVC没有NodeResizeError或已经恢复。")
            break
        
        print("PVC尚未恢复NodeResizeError状态，等待一段时间重新检查...")
        time.sleep(5)

    print("脚本执行完毕。")
    
main()
```


**解释说明：**

1. 在这个示例脚本里，仅演示了如何使用 python 的 kubernetes 客户端查询 PVC 的状态，并检查是否存在 NodeResizeError 条件。  
2. 由于该Issue并非安全风险，因此不需要实现任何利用或漏洞变现的“高风险”复现脚本，也不涉及任何敏感信息或需要特殊权限才能执行的操作。  
3. 该脚本仅在本地可用的Kubernetes环境中演示如何读取PVC状态，不会对系统造成安全风险。  
4. 脚本内含超时逻辑，确保在一定时间内退出，不会陷入死循环。

---


## Issue #131379 After the sidecar feature is introduced, resources are not reused. As a result, after scheduling is successful, kubelet rejects pods

- Issue 链接：[#131379](https://github.com/kubernetes/kubernetes/issues/131379)

### Issue 内容

#### What happened?

The sidecar feature is used. Eight NPUs are applied for for the init container and eight NPUs are applied for for the service container. The Volcano scheduling is successful. However, Kubelet cannot reuse the resources of the init container. As a result, an error is reported when resources are allocated to service containers.
So I think our default scheduler doesn't take into account the sidecar container as well.


#### What did you expect to happen?

1. pod has the init container and the service container, and init container's restart policy is Always
2. eight NPUs are applied for for the init container and eight NPUs are applied for for the service container
3. pod is running

#### How can we reproduce it (as minimally and precisely as possible)?

1. pod has the init container and the service container, and init container's restart policy is Always
2. eight NPUs are applied for for the init container and eight NPUs are applied for for the service container
3. pod is running

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从 Issue 描述来看，该问题涉及 Kubernetes 中 sidecar 功能与调度器之间的资源分配与回收机制，导致因资源未被回收而无法正常分配给后续容器。这属于调度器或 kubelet 的功能或配置问题，并未体现出任何可被恶意利用的安全漏洞，也未涉及凭据泄露、越权操作或代码注入等安全风险。因此，本问题仅是功能性或配置性的缺陷，不属于安全漏洞。

**复现过程：**

```python
# 该Issue为功能性问题，不涉及安全漏洞，因此无需提供POC脚本
```


---


## Issue #131352 kubectl: check rule in exec command is insecurity

- Issue 链接：[#131352](https://github.com/kubernetes/kubernetes/issues/131352)

### Issue 内容

#### What happened?

https://github.com/kubernetes/kubernetes/blob/b53b9fb5573323484af9a19cf3f5bfe80760abba/staging/src/k8s.io/kubectl/pkg/cmd/exec/exec.go#L208-L215

`kubectl exec -it resource asd asd asd -- bash` will be like `kubectl exec -it resource -- bash` but no error

This can lead to unexpected behavior of command with `--`

#### What did you expect to happen?

raise error `error: exec [POD] [COMMAND] is not supported anymore.`

#### How can we reproduce it (as minimally and precisely as possible)?

`kubectl exec -it resource asd asd asd -- bash`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
v1.32.3
```

</details>


#### Cloud provider

<details>
-
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue描述来看，该问题集中在“kubectl exec”命令对参数解析的行为与用户预期不一致。Issue中所述的“kubectl exec -it resource asd asd asd -- bash”实际被解析为“kubectl exec -it resource -- bash”且未报错，导致用户认为产生了“意外的行为”。  
然而，Issue中并未提出利用该行为能够实现越权、命令注入、容器逃逸、提权等安全风险场景，仅表明这是一个不符合预期的解析方式或BUG。通常使用“kubectl exec”需要用户具备相应权限（获取/执行pods/exec），此类权限本身就可直接执行容器内命令。因此，该Issue未提升攻击者所需的权限，也未新增加任何实际可利用的安全漏洞，更多地体现为CLI参数解析与预期不符的问题。  

综上，本Issue未表现出实际的安全漏洞或风险，不涉及任何越权操作或攻击者可利用的场景。

**解释说明：**

此处为正常功能/BUG的讨论，不存在可利用的安全漏洞，因此无需编写复现脚本。该问题并不能带来新的攻击途径或权限提升，故风险评级为“不涉及”。

---


## Issue #131347 kubeadm init does not work

- Issue 链接：[#131347](https://github.com/kubernetes/kubernetes/issues/131347)

### Issue 内容

#### What happened?

I used the following command to init a cluster on Ubuntu 22.04
`sudo kubeadm init --pod-network-cidr 10.200.0.0/16 --apiserver-advertise-address=172.30.1.2`

and get the following results:
`Apr 16 22:03:00 t-X570-AORUS-ELITE kubelet[61942]: I0416 22:03:00.945598   61942 kubelet_node_status.go:70] "Attempting to register node" node="t-x570-aorus-elite"
Apr 16 22:03:01 t-X570-AORUS-ELITE kubelet[61942]: E0416 22:03:01.539137   61942 eviction_manager.go:258] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"t-x570-aorus-elite\" not found"
Apr 16 22:03:04 t-X570-AORUS-ELITE kubelet[61942]: I0416 22:03:04.252855   61942 scope.go:117] "RemoveContainer" containerID="82a6daa3fe106defc2611b80f05a10b6721c18db3fb60e0318d6c5b7c3f1df0d"
Apr 16 22:03:04 t-X570-AORUS-ELITE kubelet[61942]: E0416 22:03:04.253280   61942 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-apiserver\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kube-apiserver pod=kube-apiserver-t-x570-aorus-elite_kube-system(30e3274380e991ebd296f6b8c8d423ba)\"" pod="kube-system/kube-apiserver-t-x570-aorus-elite" podUID="30e3274380e991ebd296f6b8c8d423ba"
Apr 16 22:03:05 t-X570-AORUS-ELITE kubelet[61942]: E0416 22:03:05.507589   61942 event.go:289] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"t-x570-aorus-elite.1836e1c2883adc51", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"t-x570-aorus-elite", UID:"t-x570-aorus-elite", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientPID", Message:"Node t-x570-aorus-elite status is now: NodeHasSufficientPID", Source:v1.EventSource{Component:"kubelet", Host:"t-x570-aorus-elite"}, FirstTimestamp:time.Date(2025, time.April, 16, 21, 2, 11, 278109777, time.Local), LastTimestamp:time.Date(2025, time.April, 16, 21, 2, 11, 340599626, time.Local), Count:2, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"kubelet", ReportingInstance:"t-x570-aorus-elite"}': 'Patch "https://172.30.1.2:6443/api/v1/namespaces/default/events/t-x570-aorus-elite.1836e1c2883adc51": dial tcp 172.30.1.2:6443: i/o timeout'(may retry after sleeping)
Apr 16 22:03:05 t-X570-AORUS-ELITE kubelet[61942]: E0416 22:03:05.514753   61942 controller.go:146] "Failed to ensure lease exists, will retry" err="Get \"https://172.30.1.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/t-x570-aorus-elite?timeout=10s\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" interval="7s"
Apr 16 22:03:06 t-X570-AORUS-ELITE kubelet[61942]: I0416 22:03:06.253218   61942 scope.go:117] "RemoveContainer" containerID="cf77057a7000506e1bb0b6751e7281624b04dbb8d906164f7e79777181c1f086"
Apr 16 22:03:06 t-X570-AORUS-ELITE kubelet[61942]: E0416 22:03:06.253524   61942 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"etcd\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=etcd pod=etcd-t-x570-aorus-elite_kube-system(964f70a04ec02731612f76d8fc652c6a)\"" pod="kube-system/etcd-t-x570-aorus-elite" podUID="964f70a04ec02731612f76d8fc652c6a"
Apr 16 22:03:11 t-X570-AORUS-ELITE kubelet[61942]: E0416 22:03:11.539342   61942 eviction_manager.go:258] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"t-x570-aorus-elite\" not found"
Apr 16 22:03:13 t-X570-AORUS-ELITE kubelet[61942]: W0416 22:03:13.893522   61942 reflector.go:535] k8s.io/client-go@v0.0.0/tools/cache/reflector.go:229: failed to list *v1.Node: Get "https://172.30.1.2:6443/api/v1/nodes?fieldSelector=metadata.name%3Dt-x570-aorus-elite&limit=500&resourceVersion=0": dial tcp 172.30.1.2:6443: i/o timeout
Apr 16 22:03:13 t-X570-AORUS-ELITE kubelet[61942]: I0416 22:03:13.893587   61942 trace.go:236] Trace[1855426437]: "Reflector ListAndWatch" name:k8s.io/client-go@v0.0.0/tools/cache/reflector.go:229 (16-Apr-2025 22:02:43.892) (total time: 30000ms):
Apr 16 22:03:13 t-X570-AORUS-ELITE kubelet[61942]: Trace[1855426437]: ---"Objects listed" error:Get "https://172.30.1.2:6443/api/v1/nodes?fieldSelector=metadata.name%3Dt-x570-aorus-elite&limit=500&resourceVersion=0": dial tcp 172.30.1.2:6443: i/o timeout 30000ms (22:03:13.893)
Apr 16 22:03:13 t-X570-AORUS-ELITE kubelet[61942]: Trace[1855426437]: [30.000903632s] [30.000903632s] END
Apr 16 22:03:13 t-X570-AORUS-ELITE kubelet[61942]: E0416 22:03:13.893600   61942 reflector.go:147] k8s.io/client-go@v0.0.0/tools/cache/reflector.go:229: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://172.30.1.2:6443/api/v1/nodes?fieldSelector=metadata.name%3Dt-x570-aorus-elite&limit=500&resourceVersion=0": dial tcp 172.30.1.2:6443: i/o timeout
Apr 16 22:03:15 t-X570-AORUS-ELITE kubelet[61942]: I0416 22:03:15.252588   61942 scope.go:117] "RemoveContainer" containerID="82a6daa3fe106defc2611b80f05a10b6721c18db3fb60e0318d6c5b7c3f1df0d"
Apr 16 22:03:15 t-X570-AORUS-ELITE kubelet[61942]: E0416 22:03:15.253021   61942 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-apiserver\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kube-apiserver pod=kube-apiserver-t-x570-aorus-elite_kube-system(30e3274380e991ebd296f6b8c8d423ba)\"" pod="kube-system/kube-apiserver-t-x570-aorus-elite" podUID="30e3274380e991ebd296f6b8c8d423ba"
Apr 16 22:03:18 t-X570-AORUS-ELITE kubelet[61942]: E0416 22:03:18.352083   61942 certificate_manager.go:562] kubernetes.io/kube-apiserver-client-kubelet: Failed while requesting a signed certificate from the control plane: cannot create certificate signing request: Post "https://172.30.1.2:6443/apis/certificates.k8s.io/v1/certificatesigningrequests": dial tcp 172.30.1.2:6443: i/o timeout
Apr 16 22:03:19 t-X570-AORUS-ELITE kubelet[61942]: I0416 22:03:19.252719   61942 scope.go:117] "RemoveContainer" containerID="cf77057a7000506e1bb0b6751e7281624b04dbb8d906164f7e79777181c1f086"
Apr 16 22:03:19 t-X570-AORUS-ELITE kubelet[61942]: E0416 22:03:19.252975   61942`

I use containerd and my config looks like
`disabled_plugins = []
imports = []
oom_score = 0
plugin_dir = ""
required_plugins = []
root = "/var/lib/containerd"
state = "/run/containerd"
temp = ""
version = 2

[cgroup]
  path = ""

[debug]
  address = ""
  format = ""
  gid = 0
  level = ""
  uid = 0

[grpc]
  address = "/run/containerd/containerd.sock"
  gid = 0
  max_recv_message_size = 16777216
  max_send_message_size = 16777216
  tcp_address = ""
  tcp_tls_ca = ""
  tcp_tls_cert = ""
  tcp_tls_key = ""
  uid = 0

[metrics]
  address = ""
  grpc_histogram = false

[plugins]

  [plugins."io.containerd.gc.v1.scheduler"]
    deletion_threshold = 0
    mutation_threshold = 100
    pause_threshold = 0.02
    schedule_delay = "0s"
    startup_delay = "100ms"

  [plugins."io.containerd.grpc.v1.cri"]
    cdi_spec_dirs = ["/etc/cdi", "/var/run/cdi"]
    device_ownership_from_security_context = false
    disable_apparmor = false
    disable_cgroup = false
    disable_hugetlb_controller = true
    disable_proc_mount = false
    disable_tcp_service = true
    drain_exec_sync_io_timeout = "0s"
    enable_cdi = false
    enable_selinux = false
    enable_tls_streaming = false
    enable_unprivileged_icmp = false
    enable_unprivileged_ports = false
    ignore_deprecation_warnings = []
    ignore_image_defined_volumes = false
    image_pull_progress_timeout = "5m0s"
    image_pull_with_sync_fs = false
    max_concurrent_downloads = 3
    max_container_log_line_size = 16384
    netns_mounts_under_state_dir = false
    restrict_oom_score_adj = false
    sandbox_image = "registry.k8s.io/pause:3.9"
    selinux_category_range = 1024
    stats_collect_period = 10
    stream_idle_timeout = "4h0m0s"
    stream_server_address = "127.0.0.1"
    stream_server_port = "0"
    systemd_cgroup = false
    tolerate_missing_hugetlb_controller = true
    unset_seccomp_profile = ""

    [plugins."io.containerd.grpc.v1.cri".cni]
      bin_dir = "/opt/cni/bin"
      conf_dir = "/etc/cni/net.d"
      conf_template = ""
      ip_pref = ""
      max_conf_num = 1
      setup_serially = false

    [plugins."io.containerd.grpc.v1.cri".containerd]
      default_runtime_name = "runc"
      disable_snapshot_annotations = true
      discard_unpacked_layers = false
      ignore_blockio_not_enabled_errors = false
      ignore_rdt_not_enabled_errors = false
      no_pivot = false
      snapshotter = "overlayfs"

      [plugins."io.containerd.grpc.v1.cri".containerd.default_runtime]
        base_runtime_spec = ""
        cni_conf_dir = ""
        cni_max_conf_num = 0
        container_annotations = []
        pod_annotations = []
        privileged_without_host_devices = false
        privileged_without_host_devices_all_devices_allowed = false
        runtime_engine = ""
        runtime_path = ""
        runtime_root = ""
        runtime_type = ""
        sandbox_mode = ""
        snapshotter = ""

        [plugins."io.containerd.grpc.v1.cri".containerd.default_runtime.options]

      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]

        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          base_runtime_spec = ""
          cni_conf_dir = ""
          cni_max_conf_num = 0
          container_annotations = []
          pod_annotations = []
          privileged_without_host_devices = false
          privileged_without_host_devices_all_devices_allowed = false
          runtime_engine = ""
          runtime_path = ""
          runtime_root = ""
          runtime_type = "io.containerd.runc.v2"
          sandbox_mode = "podsandbox"
          snapshotter = ""

          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            BinaryName = ""
            CriuImagePath = ""
            CriuPath = ""
            CriuWorkPath = ""
            IoGid = 0
            IoUid = 0
            NoNewKeyring = false
            NoPivotRoot = false
            Root = ""
            ShimCgroup = ""
            SystemdCgroup = true

      [plugins."io.containerd.grpc.v1.cri".containerd.untrusted_workload_runtime]
        base_runtime_spec = ""
        cni_conf_dir = ""
        cni_max_conf_num = 0
        container_annotations = []
        pod_annotations = []
        privileged_without_host_devices = false
        privileged_without_host_devices_all_devices_allowed = false
        runtime_engine = ""
        runtime_path = ""
        runtime_root = ""
        runtime_type = ""
        sandbox_mode = ""
        snapshotter = ""

        [plugins."io.containerd.grpc.v1.cri".containerd.untrusted_workload_runtime.options]

    [plugins."io.containerd.grpc.v1.cri".image_decryption]
      key_model = "node"

    [plugins."io.containerd.grpc.v1.cri".registry]
      config_path = ""

      [plugins."io.containerd.grpc.v1.cri".registry.auths]

      [plugins."io.containerd.grpc.v1.cri".registry.configs]

      [plugins."io.containerd.grpc.v1.cri".registry.headers]

      [plugins."io.containerd.grpc.v1.cri".registry.mirrors]

    [plugins."io.containerd.grpc.v1.cri".x509_key_pair_streaming]
      tls_cert_file = ""
      tls_key_file = ""

  [plugins."io.containerd.internal.v1.opt"]
    path = "/opt/containerd"

  [plugins."io.containerd.internal.v1.restart"]
    interval = "10s"

  [plugins."io.containerd.internal.v1.tracing"]

  [plugins."io.containerd.metadata.v1.bolt"]
    content_sharing_policy = "shared"

  [plugins."io.containerd.monitor.v1.cgroups"]
    no_prometheus = false

  [plugins."io.containerd.nri.v1.nri"]
    disable = true
    disable_connections = false
    plugin_config_path = "/etc/nri/conf.d"
    plugin_path = "/opt/nri/plugins"
    plugin_registration_timeout = "5s"
    plugin_request_timeout = "2s"
    socket_path = "/var/run/nri/nri.sock"

  [plugins."io.containerd.runtime.v1.linux"]
    no_shim = false
    runtime = "runc"
    runtime_root = ""
    shim = "containerd-shim"
    shim_debug = false

  [plugins."io.containerd.runtime.v2.task"]
    platforms = ["linux/amd64"]
    sched_core = false

  [plugins."io.containerd.service.v1.diff-service"]
    default = ["walking"]
    sync_fs = false

  [plugins."io.containerd.service.v1.tasks-service"]
    blockio_config_file = ""
    rdt_config_file = ""

  [plugins."io.containerd.snapshotter.v1.aufs"]
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.blockfile"]
    fs_type = ""
    mount_options = []
    root_path = ""
    scratch_file = ""

  [plugins."io.containerd.snapshotter.v1.btrfs"]
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.devmapper"]
    async_remove = false
    base_image_size = ""
    discard_blocks = false
    fs_options = ""
    fs_type = ""
    pool_name = ""
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.native"]
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.overlayfs"]
    mount_options = []
    root_path = ""
    sync_remove = false
    upperdir_label = false

  [plugins."io.containerd.snapshotter.v1.zfs"]
    root_path = ""

  [plugins."io.containerd.tracing.processor.v1.otlp"]

  [plugins."io.containerd.transfer.v1.local"]
    config_path = ""
    max_concurrent_downloads = 3
    max_concurrent_uploaded_layers = 3

    [[plugins."io.containerd.transfer.v1.local".unpack_config]]
      differ = ""
      platform = "linux/amd64"
      snapshotter = "overlayfs"

[proxy_plugins]

[stream_processors]

  [stream_processors."io.containerd.ocicrypt.decoder.v1.tar"]
    accepts = ["application/vnd.oci.image.layer.v1.tar+encrypted"]
    args = ["--decryption-keys-path", "/etc/containerd/ocicrypt/keys"]
    env = ["OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf"]
    path = "ctd-decoder"
    returns = "application/vnd.oci.image.layer.v1.tar"

  [stream_processors."io.containerd.ocicrypt.decoder.v1.tar.gzip"]
    accepts = ["application/vnd.oci.image.layer.v1.tar+gzip+encrypted"]
    args = ["--decryption-keys-path", "/etc/containerd/ocicrypt/keys"]
    env = ["OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf"]
    path = "ctd-decoder"
    returns = "application/vnd.oci.image.layer.v1.tar+gzip"

[timeouts]
  "io.containerd.timeout.bolt.open" = "0s"
  "io.containerd.timeout.metrics.shimstats" = "2s"
  "io.containerd.timeout.shim.cleanup" = "5s"
  "io.containerd.timeout.shim.load" = "5s"
  "io.containerd.timeout.shim.shutdown" = "3s"
  "io.containerd.timeout.task.state" = "2s"

[ttrpc]
  address = ""
  gid = 0
  uid = 0`

I have configured SystemdCGroup as true. 

sudo systemctl status kubelet gives:

`● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: active (running) since Thu 2025-04-17 12:46:44 CEST; 16s ago
       Docs: https://kubernetes.io/docs/
   Main PID: 25638 (kubelet)
      Tasks: 19 (limit: 38346)
     Memory: 31.7M
        CPU: 327ms
     CGroup: /system.slice/kubelet.service
             └─25638 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.9

Apr 17 12:46:45 t-X570-AORUS-ELITE kubelet[25638]: I0417 12:46:45.819473   25638 scope.go:117] "RemoveContainer" containerID="01d69ef1e0acf7ea6f351c272eed3ee8b0e214f227aade4481627e8d9e95a7cb"
Apr 17 12:46:45 t-X570-AORUS-ELITE kubelet[25638]: E0417 12:46:45.819713   25638 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"etcd\" with CrashLoopBackOff: \"back-off 10s restarting failed container=etcd pod=etcd-t-x570-aorus-elite_kube-system(964f70a04ec02731612f76d8fc652c6a)\"" pod="kube-system/etcd-t-x570-aorus-elite" podUID="964f70a04ec02731612f76d8fc652c6a"
Apr 17 12:46:46 t-X570-AORUS-ELITE kubelet[25638]: I0417 12:46:46.823385   25638 scope.go:117] "RemoveContainer" containerID="01d69ef1e0acf7ea6f351c272eed3ee8b0e214f227aade4481627e8d9e95a7cb"
Apr 17 12:46:46 t-X570-AORUS-ELITE kubelet[25638]: E0417 12:46:46.823602   25638 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"etcd\" with CrashLoopBackOff: \"back-off 10s restarting failed container=etcd pod=etcd-t-x570-aorus-elite_kube-system(964f70a04ec02731612f76d8fc652c6a)\"" pod="kube-system/etcd-t-x570-aorus-elite" podUID="964f70a04ec02731612f76d8fc652c6a"
Apr 17 12:46:52 t-X570-AORUS-ELITE kubelet[25638]: I0417 12:46:52.708723   25638 scope.go:117] "RemoveContainer" containerID="01d69ef1e0acf7ea6f351c272eed3ee8b0e214f227aade4481627e8d9e95a7cb"
Apr 17 12:46:52 t-X570-AORUS-ELITE kubelet[25638]: E0417 12:46:52.709020   25638 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"etcd\" with CrashLoopBackOff: \"back-off 10s restarting failed container=etcd pod=etcd-t-x570-aorus-elite_kube-system(964f70a04ec02731612f76d8fc652c6a)\"" pod="kube-system/etcd-t-x570-aorus-elite" podUID="964f70a04ec02731612f76d8fc652c6a"
Apr 17 12:46:54 t-X570-AORUS-ELITE kubelet[25638]: E0417 12:46:54.799049   25638 controller.go:146] "Failed to ensure lease exists, will retry" err="Get \"https://172.30.1.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/t-x570-aorus-elite?timeout=10s\": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)" interval="200ms"
Apr 17 12:46:54 t-X570-AORUS-ELITE kubelet[25638]: E0417 12:46:54.870136   25638 eviction_manager.go:258] "Eviction manager: failed to get summary stats" err="failed to get node info: node \"t-x570-aorus-elite\" not found"
Apr 17 12:46:54 t-X570-AORUS-ELITE kubelet[25638]: I0417 12:46:54.992724   25638 scope.go:117] "RemoveContainer" containerID="01d69ef1e0acf7ea6f351c272eed3ee8b0e214f227aade4481627e8d9e95a7cb"
Apr 17 12:46:54 t-X570-AORUS-ELITE kubelet[25638]: E0417 12:46:54.993015   25638 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"etcd\" with CrashLoopBackOff: \"back-off 10s restarting failed container=etcd pod=etcd-t-x570-aorus-elite_kube-system(964f70a04ec02731612f76d8fc652c6a)\"" pod="kube-system/etcd-t-x570-aorus-elite" podUID="964f70a04ec02731612f76d8fc652c6a"`

sudo systemctl staus containerned gives

`● containerd.service - containerd container runtime
     Loaded: loaded (/lib/systemd/system/containerd.service; enabled; vendor preset: enabled)
     Active: active (running) since Thu 2025-04-17 11:38:02 CEST; 1h 10min ago
       Docs: https://containerd.io
   Main PID: 916 (containerd)
      Tasks: 73
     Memory: 107.5M
        CPU: 3.943s
     CGroup: /system.slice/containerd.service
             ├─  916 /usr/bin/containerd
             ├─25697 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id aa591137059e838dde587ba94282c617c7534fc318680b8071721a8054e60bc1 -address /run/containerd/containerd.sock
             ├─25698 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id 42980a76631257b8b4eca1fe5cd36f78b3f97e057e1efb388ffbf2eeef46ec5d -address /run/containerd/containerd.sock
             ├─25711 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id d9e2ea40db550b3c10526d83a9cb88f08b8c42e521ba5663c4f63d868a2a9568 -address /run/containerd/containerd.sock
             └─25744 /usr/bin/containerd-shim-runc-v2 -namespace k8s.io -id 50a4eaf8ee3a44c0b32069495a0149a6ec94d2e9d5ccd7cb2196af7d206e689d -address /run/containerd/containerd.sock

Apr 17 12:48:29 t-X570-AORUS-ELITE containerd[916]: time="2025-04-17T12:48:29.812943575+02:00" level=info msg="CreateContainer within sandbox \"42980a76631257b8b4eca1fe5cd36f78b3f97e057e1efb388ffbf2eeef46ec5d\" for container &ContainerMetadata{Name:etcd,Attempt:41,}"
Apr 17 12:48:29 t-X570-AORUS-ELITE containerd[916]: time="2025-04-17T12:48:29.870128218+02:00" level=info msg="CreateContainer within sandbox \"42980a76631257b8b4eca1fe5cd36f78b3f97e057e1efb388ffbf2eeef46ec5d\" for &ContainerMetadata{Name:etcd,Attempt:41,} returns container id \"652e76f3a6000de03d4fa758ec7ca6c1f39a57765f8899c71c1a0547e8beca0f\""
Apr 17 12:48:29 t-X570-AORUS-ELITE containerd[916]: time="2025-04-17T12:48:29.870324463+02:00" level=info msg="StartContainer for \"652e76f3a6000de03d4fa758ec7ca6c1f39a57765f8899c71c1a0547e8beca0f\""
Apr 17 12:48:29 t-X570-AORUS-ELITE containerd[916]: time="2025-04-17T12:48:29.960988387+02:00" level=info msg="StartContainer for \"652e76f3a6000de03d4fa758ec7ca6c1f39a57765f8899c71c1a0547e8beca0f\" returns successfully"
Apr 17 12:48:29 t-X570-AORUS-ELITE containerd[916]: time="2025-04-17T12:48:29.961454036+02:00" level=info msg="received exit event container_id:\"652e76f3a6000de03d4fa758ec7ca6c1f39a57765f8899c71c1a0547e8beca0f\"  id:\"652e76f3a6000de03d4fa758ec7ca6c1f39a57765f8899c71c1a0547e8beca0f\"  pid:26627  exit_status:1  exited_at:{seconds:1744886909  nanos:961360978}"
Apr 17 12:48:29 t-X570-AORUS-ELITE containerd[916]: time="2025-04-17T12:48:29.997376260+02:00" level=info msg="shim disconnected" id=652e76f3a6000de03d4fa758ec7ca6c1f39a57765f8899c71c1a0547e8beca0f namespace=k8s.io
Apr 17 12:48:29 t-X570-AORUS-ELITE containerd[916]: time="2025-04-17T12:48:29.997401228+02:00" level=warning msg="cleaning up after shim disconnected" id=652e76f3a6000de03d4fa758ec7ca6c1f39a57765f8899c71c1a0547e8beca0f namespace=k8s.io
Apr 17 12:48:29 t-X570-AORUS-ELITE containerd[916]: time="2025-04-17T12:48:29.997413872+02:00" level=info msg="cleaning up dead shim" namespace=k8s.io
Apr 17 12:48:30 t-X570-AORUS-ELITE containerd[916]: time="2025-04-17T12:48:30.975119727+02:00" level=info msg="RemoveContainer for \"8c2f51261d95d09a783375ba2c8f754473846f686b604872b0b572dc7daddfb4\""
Apr 17 12:48:30 t-X570-AORUS-ELITE containerd[916]: time="2025-04-17T12:48:30.996741312+02:00" level=info msg="RemoveContainer for \"8c2f51261d95d09a783375ba2c8f754473846f686b604872b0b572dc7daddfb4\" returns successfully"`




#### What did you expect to happen?

It should init a kubernetes cluster and give me the credentials for kubadm join.

#### How can we reproduce it (as minimally and precisely as possible)?

`sudo kubeadm init --pod-network-cidr 10.200.0.0/16 --apiserver-advertise-address=172.30.1.2`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: v1.28.15
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
error: unable to parse the server version: invalid character '<' looking for beginning of value

</details>


#### Cloud provider

<details>
on premise
</details>


#### OS version

PRETTY_NAME="Ubuntu 22.04.5 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该 Issue 反馈的日志与描述来看，主要是 kubeadm 初始化 Kubernetes 集群时出现了 CrashLoopBackOff、Dial Timeout 等错误，这些问题通常与网络联通性、组件配置不当或资源不可用等因素有关，而非漏洞或安全缺陷导致。Issue 中并未暴露任何敏感密钥，也不存在命令执行、提权、容器逃逸等高风险安全问题的线索，因此可以初步判断这个 Issue 只涉及集群环境或配置层面的问题，而非安全漏洞。

---


## Issue #131343 topologySpreadConstraints with DoNotSchedule not spreading pods across nodes as expected

- Issue 链接：[#131343](https://github.com/kubernetes/kubernetes/issues/131343)

### Issue 内容

#### What happened?

When using `topologySpreadConstraints` with `whenUnsatisfiable: DoNotSchedule`, the pods are not spreading across nodes, even though sufficient resources are available and nodes are schedulable. Instead, all pods are scheduled on a single node.

However, when applying `podAntiAffinity` with the same label selector, the pods are correctly spread across nodes, confirming that node topology and resources are not the issue.

![Image](https://github.com/user-attachments/assets/602f2a21-8a74-493e-b21a-a3f73dce41ac)

#### What did you expect to happen?

Pods should be spread into different nodes.

#### How can we reproduce it (as minimally and precisely as possible)?

spec:
  topologySpreadConstraints:
    - labelSelector:
        matchLabels:
          sub-app: hspsc-api-gateway-service-int
      matchLabelKeys:
        - pod-template-hash
      maxSkew: 1
      minDomains: 1
      topologyKey: kubernetes.io/hostname
      whenUnsatisfiable: DoNotSchedule

#### Anything else we need to know?

I am using karpenter in aws eks for nodemanagement. 

#### Kubernetes version

[akumar5@eks-jump karpenter]$ kubectl version
Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.6-eks-bc803b4
WARNING: version difference between client (1.28) and server (1.31) exceeds the supported minor version skew of +/-1


#### Cloud provider

AWS EKS

#### OS version

[akumar5@eks-jump karpenter]$ cat /etc/os-release
NAME="Amazon Linux"
VERSION="2"
ID="amzn"
ID_LIKE="centos rhel fedora"
VERSION_ID="2"
PRETTY_NAME="Amazon Linux 2"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2"
HOME_URL="https://amazonlinux.com/"

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据Issue内容可知，这是一个与Kubernetes Pod 调度相关的问题，描述了使用 topologySpreadConstraints 时，Pod 未按照预期分布到多个节点上。这是一个功能或配置层面的问题，没有任何潜在的权限提升、敏感信息泄露、命令执行、容器逃逸等迹象，也未导致可利用的DoS安全风险。仅涉及Kubernetes调度策略和工作原理，不属于安全漏洞范畴。

---


## Issue #131341 Windows ServerCore/NanoServer volume mount ACL issue: ConatinerUser can’t create files at root of CSI‑mounted volume

- Issue 链接：[#131341](https://github.com/kubernetes/kubernetes/issues/131341)

### Issue 内容

#### What happened?

* Create Windows workload mounting volumes.
* Run the workload with low permission, e.g., `ContainerUser`.
* After the workload pod starts, try to write files into the volume mounting directory.
* The write failed with **Acess is denied.** in the mounting path, but I can create a sub-directory and write files in the sub-directory.

The mounting directory permission setting is as follows:
```
PS C:\data> icacls .
. BUILTIN\Administrators:(OI)(CI)(F)
  NT AUTHORITY\SYSTEM:(OI)(CI)(F)
  CREATOR OWNER:(OI)(CI)(IO)(F)
  BUILTIN\Users:(OI)(CI)(RX)
  BUILTIN\Users:(CI)(AD)
  BUILTIN\Users:(CI)(IO)(WD)
  Everyone:(RX)
```

Please also notice that users with more permissive permissions don't have this issue, e.g., `ContainerAdministrator`.

This is the issue created in the CSI proxy repository.
Post here for cross-reference.
https://github.com/kubernetes-csi/csi-proxy/issues/382

#### What did you expect to happen?

I want users like `ContainerUser` to also have permission to create files in the root of the CSI‑mounted volume because the Linux containers don't have the issue.

#### How can we reproduce it (as minimally and precisely as possible)?

``` yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kibishii-deployment
spec:
  persistentVolumeClaimRetentionPolicy:
    whenDeleted: Retain
    whenScaled: Retain
  podManagementPolicy: OrderedReady
  replicas: 2
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: kibishii
  serviceName: kibishii
  template:
    metadata:
      labels:
        app: kibishii
    spec:
      containers:
      - args:
        - ping 
        - -t 
        - localhost 
        - > 
        - NUL
        command:
        - cmd.exe
        - /c
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        image: mcr.microsoft.com/windows/servercore:ltsc2022
        name: kibishii
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          seccompProfile:
            type: RuntimeDefault
        volumeMounts:
        - mountPath: /data
          name: kibishii-data
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: windows
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext:
        windowsOptions:
          runAsUserName: "ContainerUser"
      terminationGracePeriodSeconds: 30
      tolerations:
      - effect: NoSchedule
        key: os
        operator: Equal
        value: windows
  updateStrategy:
    rollingUpdate:
      partition: 0
    type: RollingUpdate
  volumeClaimTemplates:
  - apiVersion: v1
    kind: PersistentVolumeClaim
    metadata:
      name: kibishii-data
    spec:
      accessModes:
      - ReadWriteOnce
      storageClassName: storage-class-name
      resources:
        requests:
          storage: 50Mi
      volumeMode: Filesystem
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.5
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.33.0
```

</details>


#### Cloud provider

<details>
vSphere
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该Issue描述来看，问题的核心在于Windows容器中的“ContainerUser”普通权限账户无法在CSI挂载卷的根目录下创建文件，只能在子目录内正常创建文件。此处反映的更多是ACL配置问题或权限设置不当引起的访问受限，并不涉及越权、提权、信息泄露或其他典型安全风险。也就是说，这是容器中的普通用户缺少写权限造成的功能性限制，并没有攻击者利用或敏感信息泄露的场景。

依据给定的风险判断标准：
1. 该问题并不涉及攻击者利用该缺失权限去执行攻击行为，反而是容器用户无法获得写权限，并非传统意义上的安全漏洞。
2. 无法在根目录写入数据对于容器来说只是配置问题，而不是越权访问、提权或代码执行问题。
3. 不存在攻击者获取到比其权限更高资源的途径，也不存在敏感信息被错误暴露或日志中泄露凭据导致的安全侵害场景。

综上判断，该Issue并不属于安全漏洞范畴。

**解释说明：**

由于此问题不属于安全问题，因此无需给出复现脚本。该问题更多是Windows ACL配置的行为差异，导致普通的“ContainerUser”账户无法在CSI挂载卷的根目录创建文件，应通过适当修改ACL配置解决。此问题本身并不构成安全风险。

---


## Issue #131314 Should pause pod evictions when zone is in Full Zonal Disruption

- Issue 链接：[#131314](https://github.com/kubernetes/kubernetes/issues/131314)

### Issue 内容

#### What happened?

When all nodes in an AZ goes down then the zone is in Full Zonal Disruption during which we set the eviction ratelimiter to `HealthyQPSFunc`. Where as if a zone is experiencing PartialZonalDisruption then we stop all pod evictions by setting the rate limiter to `ReducedQPSFunc`.

All workloads in a zone gets evicted during FullZonalDisruption. I understand the idea to evict pods in a zone where we are not able to allocate/provision more capacity since its down. 



#### What did you expect to happen?

We expect to pause all pod evictions during FullZonalDisruption and set the rate limiter to `ReducedQPSFunc`.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a kubernetes cluster and mimic the scenario where all nodes in an AZ goes down causing the zone to be in FullZonalDisruption, KCM will start evicting pods.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

All Kubernetes version

</details>


#### Cloud provider

<details>

All Cloud Providers

</details>


#### OS version

<details>

NA

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue的描述来看，这个问题属于Kubernetes在节点全部宕机时对Pod进行驱逐（Eviction）策略的讨论，主要关注在可用性与调度行为上，并没有涉及到任意命令执行、敏感信息泄露、越权访问或提权等安全方面的内容，也未提及可被攻击者利用的场景。因此该问题本质上是一个运维和可用性问题，而非安全漏洞。

**解释说明：**

此Issue描述的场景与安全攻击无关，仅针对Kubernetes在FullZonalDisruption时的Pod驱逐策略进行讨论。本质上不是一个安全风险，因此无需提供复现脚本。根据现有信息综合判断，不构成安全漏洞或安全威胁，不属于需要进行安全加固的范畴。

---


## Issue #131309 [FG:InPlacePodVerticalScaling] Performance degradation in latency-sensitive services due to CPU affinity loss upon guaranteed QoS Pod scaling down

- Issue 链接：[#131309](https://github.com/kubernetes/kubernetes/issues/131309)

### Issue 内容

#### What happened?

For the latency-sensitive service in guaranteed QoS Pod, each worker has core affinity with a CPU. 
When the workload decreases, some workers are removed, and the worker's affinity CPUs becoming idle. 
When the Pod scaling down, these idle CPUs were excepted to be removed.
If the busy CPUs which run the busy worker are removed, the performance will be decreased for latency-sensitive services.
For example:
At the begin, there are 2 CPUs (CPU 1 anf CPU 11) are allocated to the Pod when Pod creation.
When the workload increases, Pod scale up, and 4 additional CPUs (CPU 2,12,3,13) are allocated to the Pod, add another 4 workers and set core affinity to each CPU.
![Image](https://github.com/user-attachments/assets/a19807c3-ba8d-40cd-8313-078f5b1afe07)

When the workload decreases, remove worker 4 and worker 5, CPU 3 and CPU 13 are idle, 
Ideal case:
When Pod scale down, idle CPU 3 and CPU 13 are excepted to be removed. the workers not be affected.
![Image](https://github.com/user-attachments/assets/75cad118-4ff5-457f-b38e-b5b94c48bac1)

Non-ideal case:
When Pod scale down, busy CPU 2 and CPU 12 are removed. the worker2 and worker3 will be affected because of CPU migration due to CPU affinity loss.
![Image](https://github.com/user-attachments/assets/978b2bbc-f7b3-44b6-9598-2cc9d320810c)

#### What did you expect to happen?

When Pod scale down, idle CPU 3 and CPU 13 are removed, and busy CPUs are not removed.

So, we need some method or option to let container tell kubelet which CPU should be keep.

#### How can we reproduce it (as minimally and precisely as possible)?

Test based on https://github.com/kubernetes/kubernetes/pull/129719

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v0.0.0-master+da732990954ed92779d8fe396ab9e217f038d115
```

</details>


#### Cloud provider

<details>
Self-host
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
6.5.0-41-generic #41~22.04.2-Ubuntu SMP PREEMPT_DYNAMIC Mon Jun  3 11:32:55 UTC 2 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述了在 Guaranteed QoS Pod 缩容时，可能因为移除 CPU 导致原有绑定在忙碌 CPU 上的工作线程被迫迁移，进而造成性能下降的问题。此问题本质上是由于 CPU 亲和性在缩容时被打破，而并未涉及越权访问、提权、命令执行或敏感信息泄露等安全风险，因此不属于安全漏洞。

**复现过程：**

```python
# 该Issue不涉及安全风险，不需要安全漏洞相关的复现脚本。
# 因此，此处不提供复现脚本。
pass
```


---


## Issue #131308 Pod stuck in terminating state forever with unlinkat: device or resource busy

- Issue 链接：[#131308](https://github.com/kubernetes/kubernetes/issues/131308)

### Issue 内容

#### What happened?

Environmental Info:
K3s Version:
v1.30.6+k3s1

Node(s) CPU architecture, OS, and Version:
Linux aarch64 GNU/Linux.  k3s on edge device so it doesn't have full fledged linux and we build our own version of linux

Kernel Version: 4.19.294

File System:
```
df -T /container/k3s/var/lib/rancher/k3s/
Filesystem           Type       1K-blocks      Used Available Use% Mounted on
/dev/mmcblk0p24      ext4         5757144   2972392   2471408  55% /container
```

Cluster Configuration:

Describe the bug:
When I delete any pod, it gets stuck in terminating state with following error:

```
E0217 14:13:02.919124   11533 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/29da47e0-52ea-49d5-bcc1-39824ad9abc2-kube-api-access-b6pqn podName:29da47e0-52ea-49d5-bcc1-39824ad9abc2 nodeName:}" failed. No retries permitted until 2025-02-17 14:13:06.9190655 +0000 UTC m=+967942.401661721 (durationBeforeRetry 4s). Error: UnmountVolume.TearDown failed for volume "kube-api-access-b6pqn" (UniqueName: "kubernetes.io/projected/29da47e0-52ea-49d5-bcc1-39824ad9abc2-kube-api-access-b6pqn") pod "29da47e0-52ea-49d5-bcc1-39824ad9abc2" (UID: "29da47e0-52ea-49d5-bcc1-39824ad9abc2") : unlinkat /var/lib/kubelet/pods/29da47e0-52ea-49d5-bcc1-39824ad9abc2/volumes/kubernetes.io~projected/kube-api-access-b6pqn: device or resource busy
I0217 14:13:06.948746   11533 reconciler_common.go:161] "operationExecutor.UnmountVolume started for volume \"kube-api-access-b6pqn\" (UniqueName: \"kubernetes.io/projected/29da47e0-52ea-49d5-bcc1-39824ad9abc2-kube-api-access-b6pqn\") pod \"29da47e0-52ea-49d5-bcc1-39824ad9abc2\" (UID: \"29da47e0-52ea-49d5-bcc1-39824ad9abc2\") "
```

If I check the mountinfo of the pod it shows like this:
```
AP5# cat /proc/self/mountinfo | grep 29da47e0-52ea-49d5-bcc1-39824ad9abc2
80 76 0:43 / /container/k3s/var/lib/kubelet/pods/29da47e0-52ea-49d5-bcc1-39824ad9abc2/volumes/kubernetes.io~projected/kube-api-access-b6pqn rw,relatime shared:6 - tmpfs tmpfs rw
```

When I run umount command from node terminal, umount goes through:
```
AP5# umount /var/lib/kubelet/pods/29da47e0-52ea-49d5-bcc1-39824ad9abc2/volumes/kubernetes.io~projected/kube-api-access-b6pqn
```

After this manual intervention, the pod gets deleted.

#### What did you expect to happen?

I expect pod to be deleted when I run `kubectl delete pod <pod_name>`

#### How can we reproduce it (as minimally and precisely as possible)?

I think it happens on the customized linux we have. However, I can help with any info required

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# 1.30.1
```

</details>


#### Cloud provider

<details>
Baremetal
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该Issue的描述来看，问题主要是Pod在删除时出现了“device or resource busy”的错误，导致Pod一直处于Terminating状态，需要手动umount才能删除。该问题更像是底层系统或文件系统在特定环境下（自定义Linux版本、特定CSI或挂载选项）存在的兼容或配置问题，并未体现可被外部攻击者利用的场景，也没有泄露敏感信息、命令执行、权限提升或容器逃逸等安全风险。因此不属于安全问题。

**复现过程：**

```python
# 该Issue不涉及安全风险，无需复现脚本
# 如需验证Pod stuck in terminating的问题，请在自定义Linux环境下重复删除被挂载的Pod，
# 并观察是否再次出现无法正常umount的情况。
```


**解释说明：**

上述Issue描述的现象属于环境或系统兼容性问题，无法在其中发现可被利用的安全风险，故无需构造复现脚本进行安全漏洞验证。该脚本仅在说明不涉及安全风险的场景下给出提示性说明。若需排查问题，可根据Issue中的操作步骤（手动umount等）在相同或相似环境下进行重复实验，进行系统兼容性检查而非安全风险验证。

---


## Issue #131304 Pods on very few worker nodes do not have network connectivity after control plane and calico upgrade

- Issue 链接：[#131304](https://github.com/kubernetes/kubernetes/issues/131304)

### Issue 内容

#### What happened?

Upgraded control plane to 1.29 and calico to v3.28.2 - suspecting issue started here.

After upgrade, pods (on few nodes only) do not have network connectivity, unable to ping node IP or kube-dns.
Able to connect/query kube-dns from node. 

Calico-node pod is running fine. Nothing suspicious in logs. Tried restarting services kubelet, kube-proxy, containerd etc. and calico-node pod. Flushed conntrack, iptables. Nothing helps.

Rebooting node fixes the issue.  

Could be something related to node but very random, nothing suspicious in dmesg or worker node logs.


#### What did you expect to happen?

After control plane upgrade, daemonset(including calico-node) rollout restart without any issues.

#### How can we reproduce it (as minimally and precisely as possible)?

Can't reproduce as this occurs only on very few worker nodes.

#### Anything else we need to know?

_No response_

#### Kubernetes version


```console
$ kubectl version
Client Version: v1.29.12
Server Version: v1.29.12
```


#### Cloud provider

Baremetal


#### OS version

<details>

```console
# On Linux:
# cat /etc/redhat-release 
Red Hat Enterprise Linux release 9.3


# uname -a
Linux myhostname.mycompany.com 6.6.22-custom1.el9.x86_64 #1 SMP Thu Mar 14 00:33:28 PDT 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

RuntimeName:  containerd
RuntimeVersion:  1.7.24

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
# calicoctl version
Client Version:    v3.28.2
Cluster Version:   v3.28.2
Cluster Type:      k8s,bgp
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从问题描述来看，这是在升级 Kubernetes 控制平面和 Calico 后出现的网络连通性异常，主要表现为部分节点的 Pod 无法访问节点 IP 或 kube-dns，需要通过重启节点才能暂时解决。该问题更倾向于网络或配置方面的故障，并未提及可被攻击者利用或可能导致安全漏洞的场景，也未包含敏感信息泄露、提权、拒绝服务等典型安全风险的要素，因此该问题不属于安全漏洞。

**解释说明：**

该 Issue 更像是升级后出现的网络不稳定或兼容性问题，并非安全漏洞。根据风险判断标准，若非安全问题则风险评级为“不涉及”，因此无需提供安全漏洞复现脚本。若后续在问题排查过程中发现具有安全影响的因素，则需要重新评估。

---


## Issue #131303 After kubelet is restarted, the pod status is temporarily reset to 0/1.

- Issue 链接：[#131303](https://github.com/kubernetes/kubernetes/issues/131303)

### Issue 内容

#### What happened?

After the kubelet process is restarted, the pod status is reset to 0/1. After the ready probe is ready, the pod status is restored to 1/1.As a result, the pod cannot receive traffic for a short time.

#### What did you expect to happen?

When kubelet is restarted, the pod status should not change.

#### How can we reproduce it (as minimally and precisely as possible)?

Restart the kubelet process.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.31
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该Issue的描述来看，主要是kubelet重启后，Pod的就绪探针（readiness probe）状态会在短时间内被重置为0/1，从而导致Pod暂时无法接收流量。这属于集群在kubelet重启时的正常或半正常行为，或至少是一个可预期的运行时现象/缺陷，并不代表有可被攻击者利用的安全漏洞。  
1) 该问题并未描述任何对制表或对用户权限的错误操作，也无导致命令执行、容器逃逸、权限提升、敏感信息泄露等情况。  
2) 该问题虽会在短时间内影响Pod的可用性，但这是在kubelet重启过程中出现的暂时不可用场景，无法被低权限或未授权的用户恶意利用来造成可持续的拒绝服务攻击，因此不构成安全风险。  

综合判断，该Issue并未体现出存在安全漏洞的风险场景。

---


## Issue #131287 The garbage collector successfully deletes the CronJob and its associated Pod; however, Kubernetes subsequently issues GET requests for both the Pod and the Deployment(with pod name)

- Issue 链接：[#131287](https://github.com/kubernetes/kubernetes/issues/131287)

### Issue 内容

#### What happened?

Background:
I created a CronJob named ```acadia-image-eviction-job``` with the ```ttlSecondsAfterFinished: 86400``` setting. Upon execution, this CronJob created a Pod with the name ```acadia-image-eviction-job-28976255-cc47j```.
Once the Job completed, the Kubernetes garbage collector correctly deleted both the Job and the corresponding Pod as expected.
However, I observed some unexpected behavior afterward:
Kubernetes initiated a GET request for a deployment using the Pod name.
Approximately 32 hours later, Kubernetes made another GET request for the Pod using the correct Pod name.
This raises the question of why such requests are being made, especially when ttlSecondsAfterFinished is explicitly set to 86400 seconds (24 hours).
Attached are the relevant logs for the Pod for reference.
```
<head></head>
2025-02-03 15:11:15 | Begin ProcessImageEviction |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
-- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | -- | --
2025-02-03 15:11:15 | [eventing.publisherImpl.connectToKafka] Creating Kafka producer with endpoints: [kafka-0-mh-yzhpdqxqdhqxpxdlvpjj.private.us-south.messagehub.appdomain.cloud:9093 kafka-1-mh-yzhpdqxqdhqxpxdlvpjj.private.us-south.messagehub.appdomain.cloud:9093 kafka-2-mh-yzhpdqxqdhqxpxdlvpjj.private.us-south.messagehub.appdomain.cloud:9093] |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:16 | I0203 09:41:15.592145       1 httplog.go:132] "HTTP" verb="GET" URI="/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j" latency="1.830492ms" userAgent="kubelet/v1.29.9 (linux/amd64) kubernetes/114a1f5" audit-ID="08c4e11a-c190-4eeb-9aee-19748a8ebd74" srcIP="10.51.170.59:49600" apf_pl="system" apf_fs="system-nodes" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.60048ms" resp=200 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:16 | I0203 09:41:15.599334       1 httplog.go:132] "HTTP" verb="PATCH" URI="/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j/status" latency="6.293744ms" userAgent="kubelet/v1.29.9 (linux/amd64) kubernetes/114a1f5" audit-ID="56a4d9bc-6220-4c00-a016-8b259e1840e0" srcIP="10.51.170.59:49600" apf_pl="system" apf_fs="system-nodes" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="5.984343ms" resp=200 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:16 | Processing starting for resource |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:16 | Processing complete for resource with update |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:16 | I0203 09:41:15.603897  720470 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="genctl/acadia-image-eviction-job-28976255-cc47j" podStartSLOduration=4.830957699 podStartE2EDuration="6m15.603843321s" podCreationTimestamp="2025-02-03 09:35:00 +0000 UTC" firstStartedPulling="2025-02-03 09:35:04.219758043 +0000 UTC m=+3361863.614132070" lastFinishedPulling="2025-02-03 09:41:14.99264366 +0000 UTC m=+3362234.387017692" observedRunningTime="2025-02-03 09:41:15.599547747 +0000 UTC m=+3362234.993921779" watchObservedRunningTime="2025-02-03 09:41:15.603843321 +0000 UTC m=+3362234.998217348"
2025-02-03 15:11:16 | [2025/02/03 09:41:16] [ info] [input:tail:input_tail_container_logs] inotify_fs_add(): inode=11075207 watch_fd=86613 name=/var/log/containers/acadia-image-eviction-job-28976255-cc47j_genctl_acadia-image-eviction-job-2219e9c93dfd71d3cdfb400315a6fe2ccea78b922bb9d9e932917fac10d716b2.log |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:25 | I0203 09:41:24.579773       1 trace.go:219] Trace[745171306]: "List" accept:application/json, */*,audit-id:ab09692b-b4c5-483c-ad9e-28eab42cf431,client:172.30.29.69,protocol:HTTP/2.0,resource:imagearchives,scope:cluster,url:/apis/image.rias.ibm.com/v1alpha1/imagearchives,user-agent:acadia-image-eviction-job/v0.0.0 (linux/amd64) kubernetes/$Format/acadia-image-eviction-job-28976255-cc47j,verb:LIST (03-Feb-2025 09:41:15.204) (total time: 9375ms): |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:25 | I0203 09:41:24.579844       1 httplog.go:132] "HTTP" verb="LIST" URI="/apis/image.rias.ibm.com/v1alpha1/imagearchives?labelSelector=image_archive.acadia_status%3Dcompleted" latency="9.375806626s" userAgent="acadia-image-eviction-job/v0.0.0 (linux/amd64) kubernetes/$Format/acadia-image-eviction-job-28976255-cc47j" audit-ID="ab09692b-b4c5-483c-ad9e-28eab42cf431" srcIP="172.30.29.69:34894" apf_pl="exempt" apf_fs="exempt" apf_execution_time="9.375620679s" resp=200 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:25 | RoundTrip: peer certificate :: SN: 296256592080292144230695847868833894271969405412 Issuer: CN=rias-ng-us-south-dal13-preprod_issuing_ca Subject: CN=regional-extension-server,OU=IBM Cloud Virtual Private Cloud,L=Armonk,ST=New York,C=US CN: regional-extension-server |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:25 | End ProcessImageEviction |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:26 | I0203 09:41:25.669856       1 httplog.go:132] "HTTP" verb="GET" URI="/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j" latency="3.911684ms" userAgent="kubelet/v1.29.9 (linux/amd64) kubernetes/114a1f5" audit-ID="0574d69b-5fd2-4fa1-812b-6d80c48f1fce" srcIP="10.51.170.59:49600" apf_pl="system" apf_fs="system-nodes" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="3.6756ms" resp=200 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:26 | I0203 09:41:25.677514       1 httplog.go:132] "HTTP" verb="PATCH" URI="/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j/status" latency="6.528678ms" userAgent="kubelet/v1.29.9 (linux/amd64) kubernetes/114a1f5" audit-ID="2aa3844b-54e8-4c07-9609-67c764434fe9" srcIP="10.51.170.59:49600" apf_pl="system" apf_fs="system-nodes" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="6.230063ms" resp=200 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:26 | Processing complete for resource with update |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:26 | Processing starting for resource |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:28 | "Args": "IgnoreUnknown=1;K8S_POD_NAMESPACE=genctl;K8S_POD_NAME=acadia-image-eviction-job-28976255-cc47j;K8S_POD_INFRA_CONTAINER_ID=9a713f63594532d108d1debd3f7498756b7a4c12210aacd9fb2c2270de1d1691;K8S_POD_UID=d20d4043-6ff1-4143-8b86-26e6681bde12", |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:28 | CNI_Plugin: 258: 9a713f63594532d1: CNI_ARGS=IgnoreUnknown=1;K8S_POD_NAMESPACE=genctl;K8S_POD_NAME=acadia-image-eviction-job-28976255-cc47j;K8S_POD_INFRA_CONTAINER_ID=9a713f63594532d108d1debd3f7498756b7a4c12210aacd9fb2c2270de1d1691;K8S_POD_UID=d20d4043-6ff1-4143-8b86-26e6681bde12 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:28 | I0203 09:41:28.055114       1 httplog.go:132] "HTTP" verb="GET" URI="/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j" latency="2.363212ms" userAgent="kubelet/v1.29.9 (linux/amd64) kubernetes/114a1f5" audit-ID="b9a41cd3-5d78-481d-be24-698752c936e1" srcIP="10.51.170.59:49600" apf_pl="system" apf_fs="system-nodes" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.832538ms" resp=200 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:28 | I0203 09:41:28.063121       1 httplog.go:132] "HTTP" verb="PATCH" URI="/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j/status" latency="7.116743ms" userAgent="kubelet/v1.29.9 (linux/amd64) kubernetes/114a1f5" audit-ID="7855db5f-af62-4331-a41f-cc0d4da3bfb1" srcIP="10.51.170.59:49600" apf_pl="system" apf_fs="system-nodes" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="6.798459ms" resp=200 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:28 | Processing starting for resource |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:28 | Processing complete for resource with update |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:29 | I0203 09:41:28.693245       1 httplog.go:132] "HTTP" verb="GET" URI="/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j" latency="1.953523ms" userAgent="kubelet/v1.29.9 (linux/amd64) kubernetes/114a1f5" audit-ID="26aea5a3-739b-413b-bae6-9d87ae5547fb" srcIP="10.51.170.59:49600" apf_pl="system" apf_fs="system-nodes" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.750709ms" resp=200 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:29 | I0203 09:41:29.078344       1 httplog.go:132] "HTTP" verb="PATCH" URI="/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j" latency="7.768549ms" userAgent="kube-controller-manager/v1.29.9 (linux/amd64) kubernetes/114a1f5/system:serviceaccount:kube-system:job-controller" audit-ID="54e91ace-9844-4232-9eb9-967f7cc21d12" srcIP="11.105.4.2:56082" apf_pl="workload-high" apf_fs="kube-system-service-accounts" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="7.461448ms" resp=200 |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:29 | Processing starting for resource |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:29 | Processing complete for resource with update |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:47 | I0203 09:41:46.559173       1 garbagecollector.go:549] "Processing item" item="[v1/Pod, namespace: genctl, name: acadia-image-eviction-job-28976255-cc47j, uid: d20d4043-6ff1-4143-8b86-26e6681bde12]" virtual=false |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:47 | I0203 09:41:46.561384       1 garbagecollector.go:688] "Deleting item" item="[v1/Pod, namespace: genctl, name: acadia-image-eviction-job-28976255-cc47j, uid: d20d4043-6ff1-4143-8b86-26e6681bde12]" propagationPolicy="Background" |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:47 | I0203 09:41:46.561184       1 httplog.go:132] "HTTP" verb="GET" URI="/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j" latency="1.723412ms" userAgent="kube-controller-manager/v1.29.9 (linux/amd64) kubernetes/114a1f5/system:serviceaccount:kube-system:generic-garbage-collector" audit-ID="dfcb6166-3adf-4488-abbe-534e7b5cacd3" srcIP="11.105.4.2:56082" apf_pl="workload-high" apf_fs="kube-system-service-accounts" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.42406ms" resp=200 |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:47 | Processing starting for resource |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:47 | Processing complete for resource with update |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:11:47 | I0203 09:41:46.569516       1 httplog.go:132] "HTTP" verb="DELETE" URI="/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j" latency="5.304571ms" userAgent="kube-controller-manager/v1.29.9 (linux/amd64) kubernetes/114a1f5/system:serviceaccount:kube-system:generic-garbage-collector" audit-ID="de96766b-c42c-4843-a0f4-a765a4bbf384" srcIP="11.105.4.2:19620" apf_pl="workload-high" apf_fs="kube-system-service-accounts" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="5.074275ms" resp=200 |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:12:06 | CNI_Plugin: 258: 9a713f63594532d1: CNI_ARGS=K8S_POD_INFRA_CONTAINER_ID=9a713f63594532d108d1debd3f7498756b7a4c12210aacd9fb2c2270de1d1691;K8S_POD_UID=d20d4043-6ff1-4143-8b86-26e6681bde12;IgnoreUnknown=1;K8S_POD_NAMESPACE=genctl;K8S_POD_NAME=acadia-image-eviction-job-28976255-cc47j |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:12:06 | "Args": "K8S_POD_INFRA_CONTAINER_ID=9a713f63594532d108d1debd3f7498756b7a4c12210aacd9fb2c2270de1d1691;K8S_POD_UID=d20d4043-6ff1-4143-8b86-26e6681bde12;IgnoreUnknown=1;K8S_POD_NAMESPACE=genctl;K8S_POD_NAME=acadia-image-eviction-job-28976255-cc47j", |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:12:06 | CNI_Plugin: 258: 9a713f63594532d1: CNI_ARGS=K8S_POD_NAME=acadia-image-eviction-job-28976255-cc47j;K8S_POD_INFRA_CONTAINER_ID=9a713f63594532d108d1debd3f7498756b7a4c12210aacd9fb2c2270de1d1691;K8S_POD_UID=d20d4043-6ff1-4143-8b86-26e6681bde12;IgnoreUnknown=1;K8S_POD_NAMESPACE=genctl |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 15:12:06 | "Args": "K8S_POD_NAME=acadia-image-eviction-job-28976255-cc47j;K8S_POD_INFRA_CONTAINER_ID=9a713f63594532d108d1debd3f7498756b7a4c12210aacd9fb2c2270de1d1691;K8S_POD_UID=d20d4043-6ff1-4143-8b86-26e6681bde12;IgnoreUnknown=1;K8S_POD_NAMESPACE=genctl", |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 20:59:35 | I0203 15:29:35.210000       1 httplog.go:132] "HTTP" verb="GET" URI="/apis/apps/v1/namespaces/genctl/deployments/acadia-image-eviction-job-28976255-cc47j" latency="1.988411ms" userAgent="kubectl/v1.25.14 (linux/amd64) kubernetes/a5967a3" audit-ID="e9ffac2d-b409-49cc-92ef-7ccb403dd622" srcIP="10.249.6.203:29282" apf_pl="global-default" apf_fs="global-default" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.756783ms" resp=404 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-03 20:59:59 | I0203 15:29:59.049271       1 httplog.go:132] "HTTP" verb="GET" URI="/apis/apps/v1/namespaces/genctl/deployments/acadia-image-eviction-job-28976255-cc47j" latency="1.726691ms" userAgent="kubectl/v1.25.14 (linux/amd64) kubernetes/a5967a3" audit-ID="71387662-af3e-4f3a-93c9-fbe64bfde903" srcIP="10.249.6.203:25434" apf_pl="global-default" apf_fs="global-default" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.464775ms" resp=404 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  
2025-02-04 23:26:22 | I0204 17:56:21.752231       1 httplog.go:132] "HTTP" verb="GET" URI="/api/v1/namespaces/genctl/pods/acadia-image-eviction-job-28976255-cc47j" latency="2.0842ms" userAgent="kubectl/v1.25.14 (linux/amd64) kubernetes/a5967a3" audit-ID="039faa17-d98d-4324-b5e9-48f9bff8ffae" srcIP="10.249.6.203:13196" apf_pl="global-default" apf_fs="global-default" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_execution_time="1.862127ms" resp=404 |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |   |  


```

#### What did you expect to happen?

After the Pod has been deleted by garbage collector, K8s should not make any GET call on that Pod

#### How can we reproduce it (as minimally and precisely as possible)?

This issue is not easily reproducible, but it tends to occur approximately every two months.
If possible, please consider the following steps to investigate further:
Create a CronJob with the ```ttlSecondsAfterFinished: 86400``` setting.
Observe how and when the Kubernetes garbage collector deletes the corresponding Pod, Job, and CronJob resources.
Monitor whether any GET requests are being made to the specific Pod during or after its lifecycle.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# v1.29.9
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从提供的 Issue 标题和内容来看，描述的现象是 Kubernetes 在垃圾回收（CronJob 与其对应的 Pod、Job 均被成功删除）后，仍在对已删除的 Pod 或者与该 Pod 同名的 Deployment 发起 GET 请求。  
该问题更像是 Kubernetes 内部回收逻辑或调度过程中的“延迟”或“多余请求”行为，Issue 本身并未展现出可被外部攻击者利用的漏洞或安全隐患，也没有提及任何可导致命令执行、敏感信息泄露、提权、容器逃逸等高危安全风险的场景。从日志内容看，也未发现直接暴露可被攻击方利用的敏感信息（如有效身份凭证），故无法判定为安全漏洞。

此外，根据第 4 条判断标准，“issue提交者在提交内容中暴露的敏感信息、不当操作、不当配置等问题，不属于项目本身安全风险”。所以综合来看本 Issue 不涉及安全威胁。

**解释说明：**

该 Issue 中没有涉及安全问题，也没有可利用的漏洞点，无需提供复现脚本。该问题更可能是 Kubernetes 内部组件在资源清理后的剩余访问或事件触发所导致，不构成安全风险。

---


## Issue #131284 Garbage collector deletes dependents of StatefulSet, when user repeats the recreation of StatefulSet quickly

- Issue 链接：[#131284](https://github.com/kubernetes/kubernetes/issues/131284)

### Issue 内容

#### What happened?

When I performed the following operations rapidly, the garbage collector occasionally deletes the dependents (Pods or ControllerRevisions) of the StatefulSet. 

1. Delete a StatefulSet using the `orphan` propagation policy.
2. Recreate the StatefulSet with modifications to the volumeClaimTemplates.

#### What did you expect to happen?

The dependents of the StatefulSet should not be deleted.
The garbage collector should respect the propagation policy `orphan`.

#### How can we reproduce it (as minimally and precisely as possible)?

(1) Create a StatefulSet.  
(2) Wait for the pods to become ready.  
(3) Delete the StatefulSet using the `orphan` propagation policy.  
(4) Recreate the StatefulSet with changes to the volumeClaimTemplate. (e.g, add labels to the PVC)  
(5) Delete the StatefulSet again using the `orphan` propagation policy.  
(6) Recreate the StatefulSet as defined in the first step.  

In my environment, when the interval between steps (4) and (5) is only a few milliseconds to tens of milliseconds, this bug occurs.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.5
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.5
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据Issue的描述，这是一个关于StatefulSet在短时间内快速删除并重新创建时的垃圾回收机制问题，导致其从属资源（Pod或ControllerRevision）被错误地删除。该问题更多体现为K8s控制器在高频操作下的功能性或可靠性问题，并不属于安全问题的范畴。  
1. 没有迹象表明该问题可被低权限或未授权用户利用来进行越权操作、容器逃逸或提权。  
2. 即使能够触发删除Pod与ControllerRevision操作，前提仍是需要具备管理StatefulSet的较高权限（如删除、创建StatefulSet的权限）。在常规RBAC策略下，这通常是Cluster管理员或DevOps工程师才拥有的权限，因此不属于安全漏洞中典型的高风险范畴。  
3. 该问题并不涉及日志、凭据或敏感信息泄露，也不涉及潜在的命令执行或提权风险，更与跨用户环境隔离破坏无关。  

综合分析，该Issue并不涉及安全威胁或风险，故认定为“不涉及”安全风险。

---


## Issue #131275 kubelet tries to remove pod multiple times(reopen)

- Issue 链接：[#131275](https://github.com/kubernetes/kubernetes/issues/131275)

### Issue 内容

#### What happened?

When watching pod deletion, there will be a short error status
![Image](https://github.com/user-attachments/assets/9e876ebd-e0d1-4fbd-83ce-3e9db5bae176)

kubelet log, show have delete container twice
![Image](https://github.com/user-attachments/assets/8cd57045-aecd-4f6a-8107-523061bb0885)

containerd log, first delete success, but have another delete request, return not found
![Image](https://github.com/user-attachments/assets/ca5d7e04-2ec7-4d38-87fa-3dfed1f0d18a)

There is a very old issue, but the problem seems to have not been solved https://github.com/kubernetes/kubernetes/issues/99901

#### What did you expect to happen?

No errors

#### How can we reproduce it (as minimally and precisely as possible)?

create cluster 1.32.3 with kubeadm
add any pod
delete it

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here

root@n305:~# containerd -v
containerd containerd.io 1.7.27 05044ec0a9a75232cad458027ca83437aae3f4da
root@n305:~# runc -v
runc version 1.2.5
commit: v1.2.5-0-g59923ef
spec: 1.2.0
go: go1.23.7
libseccomp: 2.5.1
root@n305:~# kubectl version
Client Version: v1.32.3
Kustomize Version: v5.5.0
Server Version: v1.32.3
```
</details>



#### Cloud provider

<details>
not cloud provider
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

root@n305:~# uname -a
Linux n305.ifooth.com 6.2.11-2-pve #1 SMP PREEMPT_DYNAMIC PVE 6.2.11-2 (2023-05-10T09:13Z) x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
flannel

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从当前的 Issue 描述来看，kubelet 删除同一个容器时会重复发送删除请求，第二次删除请求返回“Not Found”错误。该问题看起来更像是一个功能性或逻辑性缺陷，并未涉及敏感信息泄露、命令注入、容器逃逸、提权等安全风险，也未说明可以在多用户场景下被恶意利用造成更为严重的安全影响。因此，可以判断本 Issue 与安全风险无关。

**复现过程：**

```python
# 该问题不涉及安全风险，无需安全复现脚本，以下仅为空占位示例
def main():
    print("该Issue不涉及安全风险，不需要提供安全漏洞复现脚本。")
main()
```


**解释说明：**

该 Issue 并非安全漏洞，而是一个正常的功能缺陷或日志冗余问题，不会造成敏感信息泄露、权限提升、容器逃逸等安全后果，因此不需要提供安全漏洞复现脚本。上述示例脚本仅用于符合回答的格式要求，不包含任何漏洞复现逻辑。

---


## Issue #131270 Headless service with selector is not reachable from within the cluster.

- Issue 链接：[#131270](https://github.com/kubernetes/kubernetes/issues/131270)

### Issue 内容

#### What happened?

I have the following Headless service with selector:
```
Name:              svc-chroma
Namespace:         default
Labels:            app=chroma
                   component=vector-db
Annotations:       <none>
Selector:          app=chroma,component=vector-db
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                None
IPs:               None
Port:              http  80/TCP
TargetPort:        http/TCP
Endpoints:         10.1.207.210:8000
Session Affinity:  None
Events:            <none>
```
The pod (`replicas: 1`):
```
Name:             chroma-0
Namespace:        default
Priority:         0
Service Account:  default
Node:             khteh-p17-2i/192.168.0.149
Start Time:       Sat, 12 Apr 2025 10:57:01 +0800
Labels:           app=chroma
                  component=vector-db
                  controller-revision-hash=chroma-7dfcdc95dd
                  statefulset.kubernetes.io/pod-name=chroma-0
Annotations:      cni.projectcalico.org/containerID: 382c18c085f2995a9a03fbd4feecceaafba2dd78d4f6542ece48d8e44b819b41
                  cni.projectcalico.org/podIP: 10.1.207.210/32
                  cni.projectcalico.org/podIPs: 10.1.207.210/32
Status:           Running
IP:               10.1.207.210
IPs:
  IP:           10.1.207.210
Controlled By:  StatefulSet/chroma
<snip>
```

The Service is not reachable from within the cluster with explicit port:
```
root@ubuntu:/# curl -v http://svc-chroma
* Host svc-chroma:80 was resolved.
* IPv6: (none)
* IPv4: 10.1.207.210
*   Trying 10.1.207.210:80...
* connect to 10.1.207.210 port 80 from 10.1.207.218 port 38694 failed: Connection refused
* Failed to connect to svc-chroma port 80 after 0 ms: Could not connect to server
* closing connection #0
curl: (7) Failed to connect to svc-chroma port 80 after 0 ms: Could not connect to server
root@ubuntu:/# curl -v http://svc-chroma:8000 
* Host svc-chroma:8000 was resolved.
* IPv6: (none)
* IPv4: 10.1.207.210
*   Trying 10.1.207.210:8000...
* Connected to svc-chroma (10.1.207.210) port 8000
> GET / HTTP/1.1
> Host: svc-chroma:8000
> User-Agent: curl/8.9.1
> Accept: */*
> 
* Request completely sent off
< HTTP/1.1 404 Not Found
< chroma-trace-id: 00000000000000000000000000000000
< content-length: 0
< date: Sat, 12 Apr 2025 03:26:24 GMT
< 
* Connection #0 to host svc-chroma left intact
```
https://kubernetes.io/docs/concepts/services-networking/service/#headless-services

#### What did you expect to happen?

Headless Service with selector is reachable from within the cluster without explicit port in the URI

#### How can we reproduce it (as minimally and precisely as possible)?

Define a headless service with selector and `TargetPort` different from the service port.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
clientVersion:
  buildDate: "2024-10-16T15:15:29Z"
  compiler: gc
  gitCommit: cbb86e0d7f4a049666fac0551e8b02ef3d6c3d9a
  gitTreeState: clean
  gitVersion: v1.27.16
  goVersion: go1.22.5
  major: "1"
  minor: "27"
  platform: linux/amd64
kustomizeVersion: v5.0.1
serverVersion:
  buildDate: "2024-10-16T15:16:32Z"
  compiler: gc
  gitCommit: cbb86e0d7f4a049666fac0551e8b02ef3d6c3d9a
  gitTreeState: clean
  gitVersion: v1.27.16
  goVersion: go1.22.5
  major: "1"
  minor: "27"
  platform: linux/amd64
```

</details>


#### Cloud provider

<details>
Local microk8s
</details>


#### OS version

<details>

```console
$ uname -a
Linux khteh-p17-2i 6.11.0-21-generic #21-Ubuntu SMP PREEMPT_DYNAMIC Wed Feb 19 16:50:40 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
$ lsb_release -a
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 24.10
Release:	24.10
Codename:	oracular
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该 issue 的描述来看，问题集中在 Kubernetes Headless Service 的网络配置和端口映射使用上。用户在访问时如果省略了端口，就无法正常访问；当显式指定 TargetPort（即 8000）后能够访问。这属于 Kubernetes Service 配置和使用方式的常规差异所导致的访问行为，与安全性漏洞无关。  
1. 该问题并未涉及敏感信息的泄露、未导致命令或代码执行、提权等安全风险。  
2. 也未造成数据越权访问或可利用的攻击面，更多是 Kubernetes 中 Headless Service 的典型使用问题（使用 ClusterIP=None 及 Service Port 与 TargetPort 不一致时需要明确指定端口）。  
3. 因此，不属于安全漏洞范畴，按照风险判断标准应归为“不涉及”安全风险。

**复现过程：**

```python
# 该问题不涉及安全风险，因此无需复现脚本
```


---


## Issue #131257 Pause image 3.10.1 not available in staging, production

- Issue 链接：[#131257](https://github.com/kubernetes/kubernetes/issues/131257)

### Issue 内容

#### What happened?

Pause image was bumped as a part of PR: https://github.com/kubernetes/kubernetes/pull/130102
But the build process of windows pause failed due to `tag@digest`: https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/post-kubernetes-push-image-pause/1889748429312102400
Dropping the `digest` should work as a stop-gap.

Ref conversation: https://kubernetes.slack.com/archives/CCK68P2Q2/p1742839354591189?thread_ts=1742832454.449449&cid=CCK68P2Q2

#### What did you expect to happen?

Pause image to get built and promoted as expected so that it can be updated in k/k src to use it.

#### How can we reproduce it (as minimally and precisely as possible)?

```
arkas1@arkas1-ubuntu-vm:~/kubernetes/build/pause$ docker buildx build --provenance=false --sbom=false --pull --output=type=registry --platform windows/amd64 \
        -t gcr.io/k8s-staging-kubernetes/pause:3.10.1-windows-amd64-1809 --build-arg BASE=mcr.microsoft.com/oss/kubernetes/windows-pause-image-base:v0.4.1@sha256:37cc10768383b55611d724a05eb18564cb5184c89b0c2faa7d4eff63475092df-windows-1809-amd64 --build-arg ARCH=amd64 -f Dockerfile_windows .
[+] Building 0.1s (1/1) FINISHED                                                                                                                                                            docker-container:k-release-multiarch
 => [internal] load build definition from Dockerfile_windows                                                                                                                                                                0.0s
 => => transferring dockerfile: 795B                                                                                                                                                                                        0.0s
 => WARN: InvalidDefaultArgInFrom: Default value for ARG ${BASE} results in empty or invalid base image name (line 16)                                                                                                      0.0s

 1 warning found (use docker --debug to expand):
 - InvalidDefaultArgInFrom: Default value for ARG ${BASE} results in empty or invalid base image name (line 16)
Dockerfile_windows:16
--------------------
  14 |     
  15 |     ARG BASE
  16 | >>> FROM ${BASE}
  17 |     ARG ARCH
  18 |     ADD bin/pause-windows-${ARCH}.exe /pause.exe
--------------------
ERROR: failed to solve: failed to parse stage name "mcr.microsoft.com/oss/kubernetes/windows-pause-image-base:v0.4.1@sha256:37cc10768383b55611d724a05eb18564cb5184c89b0c2faa7d4eff63475092df-windows-1809-amd64": invalid reference format
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

Branches having the improper `tag@digest` : 

- `master`
- `release-1.33`


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue描述来看，该问题主要与Windows平台的Pause镜像构建失败有关，原因是镜像名称中使用了“tag@digest”的格式导致无效引用。这属于构建和发布流程中的技术问题，并未提及任何可能导致权限提升、凭据泄露或其他能够被攻击者利用的安全风险点，也没有暴露项目核心配置或敏感信息。因此，该问题不涉及安全漏洞或安全风险。

**复现过程：**

```python
# 该Issue为构建配置问题，不涉及安全风险，因此无需编写复现脚本。
# 以下仅是符合回答格式的空脚本，不执行任何操作。

def main():
    print("此问题为构建流程问题，不涉及安全风险，无需POC。")

main()
```


---


## Issue #131253 Kubelet failed to start on reboot with memory manager's `Static` policy

- Issue 链接：[#131253](https://github.com/kubernetes/kubernetes/issues/131253)

### Issue 内容

#### What happened?

Memory manager's `Static` policy will cause kubelet fail to start on reboot, when total memory of each numa node changes after each reboot, while they still sum up to a fixed number, just as https://github.com/kubernetes/kubernetes/issues/109996 describes.
 
Though Kubernetes v1.32 [improves memory manager reliability and consistency](https://kubernetes.io/blog/2024/12/13/memory-manager-goes-ga/#improving-memory-manager-reliability-and-consistency) with https://github.com/kubernetes/kubernetes/pull/127074 by @Tal-or, the issue is still not fixed because it still checks whether numa node's total memory is unchanged:

https://github.com/kubernetes/kubernetes/blob/d64f34eb2cf57ca017368bd94dc577952c2647d9/pkg/kubelet/cm/memorymanager/policy_static.go#L710-L714

Thus following error happened in kubelet on reboot:
<details>

```
Apr 11 09:15:09 master0 kubelet[13019]: E0411 09:15:09.451647   13019 policy_static.go:718] "Memory states for the NUMA node and resource are different" node=2 resource="memory" field="TotalMemSize" TotalMemSize1=135233794048 TotalMemSize2=135277490176 memoryState1={"total":135233794048,"systemReserved":4429185024,"allocatable":130804609024,"reserved":0,"free":130804609024} memoryState2={"total":135277490176,"systemReserved":4429185024,"allocatable":130848305152,"reserved":0,"free":130848305152}
Apr 11 09:15:09 master0 kubelet[13019]: E0411 09:15:09.451671   13019 policy_static.go:90] "Invalid state, please drain node and remove policy state file" err="[memorymanager] the expected machine state is different from the real one"
Apr 11 09:15:09 master0 kubelet[13019]: E0411 09:15:09.451680   13019 memory_manager.go:202] "Policy start error" err="[memorymanager] the expected machine state is different from the real one"
Apr 11 09:15:09 master0 kubelet[13019]: E0411 09:15:09.451690   13019 kubelet.go:1643] "Failed to start ContainerManager" err="start memory manager error: [memorymanager] the expected machine state is different from the real one"
Apr 11 09:15:09 master0 systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
```

</details>

1. Memory state json before reboot:
<details>

```json
{
  "policyName": "Static",
  "machineState": {
    "0": {
      "numberOfAssignments": 0,
      "memoryMap": {
        "hugepages-1Gi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "hugepages-2Mi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "memory": {
          "total": 134607716352,
          "systemReserved": 4429185024,
          "allocatable": 130178531328,
          "reserved": 0,
          "free": 130178531328
        }
      },
      "cells": [
        0
      ]
    },
    "1": {
      "numberOfAssignments": 0,
      "memoryMap": {
        "hugepages-1Gi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "hugepages-2Mi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "memory": {
          "total": 135277490176,
          "systemReserved": 4429185024,
          "allocatable": 130848305152,
          "reserved": 0,
          "free": 130848305152
        }
      },
      "cells": [
        1
      ]
    },
    "2": {
      "numberOfAssignments": 0,
      "memoryMap": {
        "hugepages-1Gi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "hugepages-2Mi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "memory": {
          "total": 135233794048,
          "systemReserved": 4429185024,
          "allocatable": 130804609024,
          "reserved": 0,
          "free": 130804609024
        }
      },
      "cells": [
        2
      ]
    },
    "3": {
      "numberOfAssignments": 0,
      "memoryMap": {
        "hugepages-1Gi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "hugepages-2Mi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "memory": {
          "total": 134193364992,
          "systemReserved": 4429185024,
          "allocatable": 129764179968,
          "reserved": 0,
          "free": 129764179968
        }
      },
      "cells": [
        3
      ]
    }
  },
  "checksum": 1040760267
}
```

</details>

2. Memory state json after reboot:
<details>

```json
{
  "policyName": "Static",
  "machineState": {
    "0": {
      "numberOfAssignments": 0,
      "memoryMap": {
        "hugepages-1Gi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "hugepages-2Mi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "memory": {
          "total": 134607716352,
          "systemReserved": 4429185024,
          "allocatable": 130178531328,
          "reserved": 0,
          "free": 130178531328
        }
      },
      "cells": [
        0
      ]
    },
    "1": {
      "numberOfAssignments": 0,
      "memoryMap": {
        "hugepages-1Gi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "hugepages-2Mi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "memory": {
          "total": 135277490176,
          "systemReserved": 4429185024,
          "allocatable": 130848305152,
          "reserved": 0,
          "free": 130848305152
        }
      },
      "cells": [
        1
      ]
    },
    "2": {
      "numberOfAssignments": 0,
      "memoryMap": {
        "hugepages-1Gi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "hugepages-2Mi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "memory": {
          "total": 135277490176,
          "systemReserved": 4429185024,
          "allocatable": 130848305152,
          "reserved": 0,
          "free": 130848305152
        }
      },
      "cells": [
        2
      ]
    },
    "3": {
      "numberOfAssignments": 0,
      "memoryMap": {
        "hugepages-1Gi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "hugepages-2Mi": {
          "total": 0,
          "systemReserved": 0,
          "allocatable": 0,
          "reserved": 0,
          "free": 0
        },
        "memory": {
          "total": 134149668864,
          "systemReserved": 4429185024,
          "allocatable": 129720483840,
          "reserved": 0,
          "free": 129720483840
        }
      },
      "cells": [
        3
      ]
    }
  },
  "checksum": 3935320266
}
```

</details>

We can see total memory of node 0, 1 is unchanged, but node 2, 3 changed while they sum up to a same count.

#### What did you expect to happen?

Improve the implementation of `areMemoryStatesEqual`, to allow for numa node total memory change as long as the total summed up memory is consistent (and the current memory state can serve allocated pods?).

#### How can we reproduce it (as minimally and precisely as possible)?

1. Prepare a multi numa-node machine where memory of each numa node varies on reboot.
2. Start kubernetes cluster with kubelet memory manager policy to be `Static`.
3. Reboot to see the failure of kubelet.

#### Anything else we need to know?

https://github.com/kubernetes/kubernetes/issues/109996#issuecomment-1145929818 says issue described there would be classified as a feature request instead of bug. But I think this is a bug given that Kubernetes v1.32 is [improving memory manager reliability and consistency](https://kubernetes.io/blog/2024/12/13/memory-manager-goes-ga/#improving-memory-manager-reliability-and-consistency):

> The kubelet does not guarantee pod ordering when admitting pods after a restart or reboot.
>
> In certain edge cases, this behavior could cause the memory manager to reject some pods, and in more extreme cases, it may cause kubelet to fail upon restart.
> 
> Previously, the beta implementation lacked certain checks and logic to prevent these issues.
> 
> To stabilize the memory manager for general availability (GA) readiness, small but critical refinements have been made to the algorithm, improving its robustness and handling of edge cases.

And this issue is exactly the edge case that `cause kubelet to fail upon restart`, which should be handed to improve its robustness.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.3
Kustomize Version: v5.5.0
Server Version: v1.32.3
```

</details>


#### Cloud provider

Self-host

#### OS version

<details>

```console
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.2 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.2 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux gpu164-master0 5.15.0-84-generic #93-Ubuntu SMP Tue Sep 5 17:16:10 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

`kubeadm`, configuration as follows:
<details>

```
kind: InitConfiguration
apiVersion: kubeadm.k8s.io/v1beta3

nodeRegistration:
  name: master0
  criSocket: unix:///var/run/containerd/containerd.sock
  imagePullPolicy: IfNotPresent
  taints: [] # Set taints to empty list to avoid control-plane node from being tainted
  kubeletExtraArgs:
    resolv-conf: /run/systemd/resolve/resolv.conf
    system-reserved: cpu=100m,memory=100Mi
    kube-reserved: cpu=607.5m,memory=16696Mi
    cpu-manager-policy: static
    memory-manager-policy: Static
    reserved-memory: 0:memory=4224Mi;1:memory=4224Mi;2:memory=4224Mi;3:memory=4224Mi

localAPIEndpoint:
  advertiseAddress: 10.194.3.15
  bindPort: 6443

bootstrapTokens:
  - token: xvq0og.vyaim43ag7mw45r9

---
kind: ClusterConfiguration
apiVersion: kubeadm.k8s.io/v1beta3

kubernetesVersion: 1.32.3
clusterName: kubernetes
controlPlaneEndpoint: 10.194.3.14

imageRepository: registry.k8s.io
certificatesDir: /etc/kubernetes/pki
networking:
  podSubnet: 10.240.0.0/12
  serviceSubnet: 10.96.0.0/12
  dnsDomain: cluster.local
apiServer:
  extraArgs:
    service-node-port-range: 10000-65530
    event-ttl: 24h0m0s
  extraVolumes: []
  certSANs: []
  timeoutForControlPlane: 4m0s
controllerManager:
  extraArgs:
    node-cidr-mask-size: "20"
  extraVolumes: []
scheduler:
  extraArgs: {}
  extraVolumes: []
dns: {}
etcd:
  local:
    dataDir: /var/lib/etcd
    extraArgs: {}
    serverCertSANs: []
    peerCertSANs: []

---
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1

cgroupDriver: systemd

clusterDomain: cluster.local
clusterDNS: [10.96.0.10]

featureGates:
  CPUManager: true
  MemoryManager: true
```

</details>


#### Container runtime (CRI) and version (if applicable)




#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从描述来看，该 Issue 主要是由于在多 NUMA 节点场景下，重启后各节点的可用内存总量可能发生变化，导致 kubelet 使用内存管理（MemoryManager）的 Static 策略时，检查到“期望内存状态”和“实际内存状态”不一致，从而无法正常启动。这类问题不会造成信息泄露、权限提升，也不涉及越权或侧信道攻击等安全风险，仅仅是 kubelet 启动过程中功能性逻辑的不足。  
即使将其纳入拒绝服务（DoS）的范畴，也需要具有相当高的权限（可以修改NUMA节点内存分配策略或访问机器启动配置），并没有说明此问题可被低权限攻击者利用，因此不构成典型的安全漏洞。综合判断，该 Issue 不涉及安全风险。

**解释说明：**

该问题属于内存管理策略的功能性缺陷，不属于安全漏洞，因此无需提供复现脚本。若在实际生产环境中遭遇，可通过修正 `areMemoryStatesEqual` 函数或暂时禁用 Static 策略等方式缓解或解决。

---


## Issue #131244 Go modules are not tagged for v0.33.0-rc.0

- Issue 链接：[#131244](https://github.com/kubernetes/kubernetes/issues/131244)

### Issue 内容

#### What happened?

See e.g. https://pkg.go.dev/k8s.io/api?tab=versions

#### What did you expect to happen?

v0.33.0-rc.0 tag to show up

#### How can we reproduce it (as minimally and precisely as possible)?

Look into https://github.com/kubernetes/api/tags

#### Anything else we need to know?

Previous issue #116912


#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 仅描述了 Go 模块在某个版本上缺少相应 tag 的问题，主要属于版本管理或发布流程方面的事项，并未提及任何可能导致代码泄露、漏洞利用、权限提升或其他安全方面的问题，故不涉及安全风险。

**复现过程：**

```python
# 该 Issue 不涉及安全风险，无需提供复现脚本
# 下方仅示例说明如何检查仓库标签，不代表存在安全风险

import subprocess
import sys
import time

def main():
    try:
        # 假设本地已有 GitHub 登录信息，直接执行 git fetch 并查看标签
        subprocess.run(["git", "clone", "https://github.com/kubernetes/api.git", "kubernetes_api_repo"], check=True)
        time.sleep(1)  # 等待片刻，避免仓库信息未下载完全
        subprocess.run(["git", "-C", "kubernetes_api_repo", "fetch", "--tags"], check=True)
        tag_list = subprocess.check_output(["git", "-C", "kubernetes_api_repo", "tag"], text=True)
        print("当前仓库标签:\n", tag_list)
    except subprocess.CalledProcessError as e:
        print("执行 Git 命令失败:", e)
        sys.exit(1)

    # 脚本执行完成
    print("Tag 检查结束。")

main()
```


---


## Issue #131241 kubectl drain no longer works with node accounts in Kubernetes 1.32

- Issue 链接：[#131241](https://github.com/kubernetes/kubernetes/issues/131241)

### Issue 内容

#### What happened?

Since Kubernetes 1.32, the AuthorizeNodeWithSelectors feature gate defaults to true. This leads to Nodes only being able to query Pods that are associated with that node. kubectl drain does not handle this gracefully; when run with node credentials, it will evict or terminate all pods, but then exit on a permission error as those pods, once successfully terminated or evicted, no longer have a relationship with that node and are thus not accessible to it via API.

#### What did you expect to happen?

kubectl drain finishes cleanly after all Pods have been evicted or terminated, as it does on 1.31 or with the AuthorizeNodeWithSelectors feature gate set to false.

#### How can we reproduce it (as minimally and precisely as possible)?

Run kubectl drain on a node running Kubernetes 1.32 with default feature gate settings, using the node's own account for authorization.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.3
Kustomize Version: v5.5.0
Server Version: v1.32.3
```

</details>


#### Cloud provider

none


#### OS version

<details>
Debian GNU/Linux 12 (bookworm)
</details>


#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)




### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue描述来看，该问题是由于Kubernetes 1.32默认开启了AuthorizeNodeWithSelectors功能后，节点凭据只能查询与自身节点关联的Pod，在执行kubectl drain时，当Pod被驱逐或删除后，与节点失去关联，导致节点凭据无法继续查询到这些Pod而产生权限错误。此问题属于Kubernetes对节点访问Pod权限策略改变所引发的功能性兼容或设计问题，并不涉及任意越权、敏感信息泄露、提权或其他安全攻击场景，因此不属于安全漏洞。

**复现过程：**

```python
# 由于该Issue与安全漏洞无关，这里不提供安全PoC。
# 如果仅想重现kubectl drain的权限错误，可使用python的kubernetes库模拟 drain 操作，
# 但此处展示的脚本仅演示如何与Kubernetes交互，并不会造成安全风险的利用。
#
# 注意：该脚本仅为功能测试范例，需在有权限访问集群的环境运行。

import time
from kubernetes import client, config
from kubernetes.stream import stream

def main():
    # 加载本地kubeconfig
    config.load_kube_config()

    # 创建CoreV1Api客户端
    v1 = client.CoreV1Api()

    # 假设要对某个node执行 drain，示例node名称请根据实际情况替换
    node_name = "your-node-name"

    # 1. 获取该node上运行的Pod列表
    field_selector = f"spec.nodeName={node_name}"
    pods = v1.list_pod_for_all_namespaces(field_selector=field_selector).items

    print(f"Node {node_name} 上的Pods：")
    for pod in pods:
        print(f"- {pod.metadata.namespace}/{pod.metadata.name}")
    
    # 2. 模拟对这些Pod执行驱逐或删除操作（此处仅简单调用delete）
    #   注意：真实 drain 需要依赖高级逻辑处理，如daemonset等特殊Pod的跳过策略，这里仅做简单演示
    for pod in pods:
        try:
            v1.delete_namespaced_pod(
                name=pod.metadata.name,
                namespace=pod.metadata.namespace
            )
            print(f"已删除Pod: {pod.metadata.namespace}/{pod.metadata.name}")
        except Exception as e:
            print(f"删除Pod时出错: {e}")

    # 3. 等待一下，让Pod完成删除
    time.sleep(3)

    # 4. 删除后尝试再次查询已删除的Pod，会因为AuthorizeNodeWithSelectors而出现无权限或找不到资源等错误
    for pod in pods:
        try:
            _ = v1.read_namespaced_pod(pod.metadata.name, pod.metadata.namespace)
            print(f"仍能访问到Pod: {pod.metadata.namespace}/{pod.metadata.name}")
        except Exception as e:
            print(f"无法访问已删除的Pod(预期权限问题或NotFound): {e}")

    print("脚本执行完毕，若出现权限错误或无法查询已删除Pod即与Issue描述现象相符。")

main()
```


**解释说明：**

1. 该脚本通过python的kubernetes客户端加载默认kubeconfig，并获取指定节点上的Pod列表，演示性地删除这些Pod，再次访问被删除的Pod时，若已开启AuthorizeNodeWithSelectors且使用节点账号授权，可能会出现无权限或找不到资源的错误，模拟了Issue提及的情况。  
2. 该问题并非源自越权访问或敏感信息泄露等安全风险，而是一种与Kubernetes策略调整相关的功能异常，因此归类为“不涉及”安全风险。  
3. 脚本仅展示了如何与Kubernetes进行交互，并无任何高风险的漏洞利用行为，可在本地测试环境使用。  
4. 实际排障或兼容处理时，可根据需求关闭AuthorizeNodeWithSelectors（不推荐在生产环境中随意变更集群默认安全策略）或使用更高级别凭据执行drain操作，以避免出现权限错误。

---


## Issue #131232 kubernetes.io/csi: unmounter failed, because of the vol_data.json: no such file or directory

- Issue 链接：[#131232](https://github.com/kubernetes/kubernetes/issues/131232)

### Issue 内容

#### What happened?

When the csi interface is invoked to attach a volume, if the volume fails to be attached, the removeMountDir method is invoked to clear the directory where the volume is attached. If the pod is deleted, the csi volume fails to be attached, and a message is displayed indicating that the volume fails to be detached. The cause is that the vol_data.json file does not exist.
The code for mount a volume is as follows:
https://github.com/kubernetes/kubernetes/blob/88dfcb225d41326113990e87b11137641c121a32/pkg/volume/csi/csi_mounter.go#L300-L323

The NewUnmounter method is invoked to detach a volume. The vol_data.json file needs to be read. The code is as follows:
https://github.com/kubernetes/kubernetes/blob/88dfcb225d41326113990e87b11137641c121a32/pkg/volume/csi/csi_plugin.go#L556-L559

The log is as follows:
E0120 10:53:49.870240 4083505 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/31c052e2-63f5-4127-95cb-0923db30dd5a-dump podName:31c052e2-63f5-4127-95cb-0923db30dd5a nodeName:}" failed. No retries permitted until 2025-01-20 10:53:50.370182329 +0000 UTC m=+5.022372153 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "dump" (UniqueName: "kubernetes.io/csi/31c052e2-63f5-4127-95cb-0923db30dd5a-dump") pod "ndp-spark-history-0" (UID: "31c052e2-63f5-4127-95cb-0923db30dd5a") : rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing: dial unix /var/lib/kubelet/plugins_registry/sop-csi-driver-reg.sock: connect: resource temporarily unavailable"

E0120 13:16:00.518419 4083505 reconciler_common.go:169] "operationExecutor.UnmountVolume failed (controllerAttachDetachEnabled true) for volume \"dump\" (UniqueName: \"kubernetes.io/csi/31c052e2-63f5-4127-95cb-0923db30dd5a-dump\") pod \"31c052e2-63f5-4127-95cb-0923db30dd5a\" (UID: \"31c052e2-63f5-4127-95cb-0923db30dd5a\") : UnmountVolume.NewUnmounter failed for volume \"dump\" (UniqueName: \"kubernetes.io/csi/31c052e2-63f5-4127-95cb-0923db30dd5a-dump\") pod \"31c052e2-63f5-4127-95cb-0923db30dd5a\" (UID: \"31c052e2-63f5-4127-95cb-0923db30dd5a\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/31c052e2-63f5-4127-95cb-0923db30dd5a/volumes/kubernetes.io~csi/dump/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/31c052e2-63f5-4127-95cb-0923db30dd5a/volumes/kubernetes.io~csi/dump/vol_data.json]: open /var/lib/kubelet/pods/31c052e2-63f5-4127-95cb-0923db30dd5a/volumes/kubernetes.io~csi/dump/vol_data.json: no such file or directory" err="UnmountVolume.NewUnmounter failed for volume \"dump\" (UniqueName: \"kubernetes.io/csi/31c052e2-63f5-4127-95cb-0923db30dd5a-dump\") pod \"31c052e2-63f5-4127-95cb-0923db30dd5a\" (UID: \"31c052e2-63f5-4127-95cb-0923db30dd5a\") : kubernetes.io/csi: unmounter failed to load volume data file [/var/lib/kubelet/pods/31c052e2-63f5-4127-95cb-0923db30dd5a/volumes/kubernetes.io~csi/dump/mount]: kubernetes.io/csi: failed to open volume data file [/var/lib/kubelet/pods/31c052e2-63f5-4127-95cb-0923db30dd5a/volumes/kubernetes.io~csi/dump/vol_data.json]: open /var/lib/kubelet/pods/31c052e2-63f5-4127-95cb-0923db30dd5a/volumes/kubernetes.io~csi/dump/vol_data.json: no such file or directory"

#### What did you expect to happen?

Volumes that fail to be mounted can also be unmounted successfully.

#### How can we reproduce it (as minimally and precisely as possible)?

Simulate a scenario in which a volume fails to be attached to the CSI and the vol_data.json file is deleted. After the volume fails to be mounted, the volume is unmounted.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.31
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该Issue描述的情况来看，问题主要是CSI在卸载（unmount）流程中，由于vol_data.json文件缺失，导致卸载失败。整个流程并未涉及越权访问、敏感信息泄漏、任意命令执行等安全风险，也未提供可被攻击者利用的途径。该问题属于资源管理或容器编排的逻辑错误，无法被用来进一步发动攻击或提升权限。因此，本Issue不涉及任何安全风险。

**复现过程：**

```python
# 由于该Issue不涉及安全风险，故无需提供安全漏洞复现脚本。
# 如需测试CSI在丢失vol_data.json文件后的行为，可在kubernetes集群上
# 手动或通过脚本移除vol_data.json后观察unmount失败的现象。
# 以下示例演示如何在集群环境中检测卷文件是否存在，并展示可能的报错日志。
# 该脚本仅用于演示如何定位和查看CSI相关逻辑，非漏洞利用POC。

import time
import os
from kubernetes import client, config

def main():
    # 假设本地已配置kubeconfig文件，可正常读取
    config.load_kube_config()

    v1 = client.CoreV1Api()

    # 以下逻辑仅用于演示对CSI挂载点进行简单排查和日志检测的过程
    # 实际操作中可结合自身部署场景完成CSI驱动测试
    print("开始检测目标Pod的vol_data.json文件是否存在...")

    # 演示先找到某个Pod（此处仅举例，可根据自身集群修改）
    pods = v1.list_pod_for_all_namespaces()
    for pod in pods.items:
        # 仅示例打印Pod名称，不作实际删除操作
        print(f"检测 Pod: {pod.metadata.name}")
    
    print("如需进一步测试，请在真实环境中删除/var/lib/kubelet/.../vol_data.json文件后，观察unmount过程。")
    print("脚本结束。")
    time.sleep(2)
    return
```


---


## Issue #131228 Defined nodePort same for UDP/TCP -- service update does not sync ports.

- Issue 链接：[#131228](https://github.com/kubernetes/kubernetes/issues/131228)

### Issue 内容

#### What happened?

If you change a service manifest to have the same NodePort: and "Update/Apply" to an existing service it will only change 1 of the nodePorts that is on the same port, seems to be the first `ports:` in the list.

If you delete and recreate the service, the duplicate TCP/UDP service accepts the same nodePort.

Server Version: v1.32.1

Ref #91590 

```
[manifests] % kubectl  apply -f k8s-external-coredns-service.yaml
service/k8s-external-coredns configured
[manifests] % kubectl  get svc k8s-external-coredns
NAME                   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)                                    AGE
k8s-external-coredns   NodePort   10.4.215.193   <none>        53:30054/UDP,53:30054/TCP,9153:30828/TCP   14m
[manifests] % kubectl  apply -f k8s-external-coredns-service.yaml
service/k8s-external-coredns configured
[manifests] % kubectl  get svc k8s-external-coredns
NAME                   TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)                                    AGE
k8s-external-coredns   NodePort   10.4.215.193   <none>        53:30154/UDP,53:30054/TCP,9153:30828/TCP   15m
```

FYI, Additionally while testing the order issue, I noted that if you change the order of the ports: attribute, it also does not accept an update due to the duplicates:
```
% kubectl  apply -f k8s-external-coredns-service.yaml
The Service "k8s-external-coredns" is invalid:
* spec.ports[1].name: Duplicate value: "dns"
* spec.ports[1]: Duplicate value: core.ServicePort{Name:"", Protocol:"UDP", AppProtocol:(*string)(nil), Port:53, TargetPort:intstr.IntOrString{Type:0, IntVal:0, StrVal:""}, NodePort:0}
```

#### What did you expect to happen?

The apply/update should take the same as a new creation.

#### How can we reproduce it (as minimally and precisely as possible)?

```
apiVersion: v1
kind: Service
metadata:
  labels:
    k8s-app: kube-dns-k8s-external
    kubernetes.io/name: CoreDNS
  name: k8s-external-coredns
  namespace: kube-system
spec:
  type: NodePort
  externalTrafficPolicy: Cluster
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: dns
    port: 53
    protocol: UDP
    targetPort: 53
    nodePort: 30054
  - name: dns-tcp
    port: 53
    protocol: TCP
    targetPort: 53
    nodePort: 30054
  - name: metrics
    port: 9153
    protocol: TCP
    targetPort: 9153
  selector:
    k8s-app: k8s-external-coredns
  sessionAffinity: None

```

#### Anything else we need to know?

Non-Critcal, the workaround is just to delete/recreate.  Just reporting for completness.

#### Kubernetes version

<details>

```console
Client Version: v1.32.3
Server Version: v1.32.1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
CNI is FLannel
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据Issue的描述，这是一个关于Kubernetes Service在使用相同的NodePort（UDP/TCP）时更新不生效的功能性问题，不能算作安全漏洞。该问题未涉及访问控制绕过、敏感信息泄露、命令执行、容器逃逸或提权等安全场景，也没有对多用户环境产生越权影响。因此不属于安全风险范畴。

**解释说明：**

该Issue本质上是Kubernetes在处理同一NodePort下不同协议时更新行为有缺陷的功能性问题，无法通过此问题直接造成安全威胁或漏洞利用。故判定为“不涉及”安全风险。仅需在项目层面修复或规避此特性缺陷即可，无需额外安全防护措施。

---


## Issue #131223 Wrong file system running multiple Windows pods

- Issue 链接：[#131223](https://github.com/kubernetes/kubernetes/issues/131223)

### Issue 内容

#### What happened?

Hi,

we have update the following components in our environment:

Jenkins - 2.462.2 -> 2.479.2 

AKS - 1.29.7 -> 1.31.5

With the current status our Jenkins pipeline fails (randomly) when we execute multiple stages on a Windows node pool. We use the same pod definition for all the stages in the pipeline. 
Checking the file system we have seen some differences.
When the stage is executed correctly the C:\Jenkins folder content is:
![Image](https://github.com/user-attachments/assets/6444f899-c9ca-4ad9-834a-3c1d306f3c9a)

while in case of a failure the content is: 

![Image](https://github.com/user-attachments/assets/5d14b6a2-7352-4b2b-9dbf-e2f13998c853)

 
We are struggling to understand the cause. The pod - cloud-node-manager-windows-xxx which it should be managed by Kubernetes have the exact file system posted in the failure scenario.

Our Windows pods mount 2 volumes:

![Image](https://github.com/user-attachments/assets/2e46a5d1-2a94-4409-94ee-4485475500a7)


We have tested also the old working version of Jenkins with the new version of Kubernetes but the issue persist.

#### What did you expect to happen?

I'm expect to be able to run multiple pods on a Windows node pool without running into this problem, which causes our pipelines to fail.

#### How can we reproduce it (as minimally and precisely as possible)?

Tools:
Jenkins - 2.462.2 or 2.479.2 
AKS - 1.31.5

---
Jenkinsfile with parallel stages which involve Windows pods:
...
matrix {
                axes {
                    axis {
                        name 'AXE_PLATFORM'
                        values 'Win32_x64', 'Win32_x86'
                    }
                    axis {
                        name 'AXE_TYPE'
                        values 'Debug', 'Release'
                    }
                }
                stages {
                    stage('Build') {
                        agent { kubernetes ( 'pod_windows.yaml' }
                        steps {
                            script {
                                bat 'dir C:\\jenkins'
                                ...
                            }
                        }
                    }
                }
            }
....

#### Anything else we need to know?

We use groovy to merge to pods definitions, one which contains the jenkins remote agent and the other one which contains our pod used to execute our instructions.


#### Kubernetes version

1.31.5


#### Cloud provider

Azure


#### OS version

Here the base image we use mcr.microsoft.com/windows:10.0.17763.6293
On top of that we install other tools (msbuild, git, ...)


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该 Issue 的描述来看，主要问题是多个并行运行的 Windows Pod 在文件系统上产生了不一致的现象，导致 Jenkins Pipeline 随机失败。该问题更像是一个环境配置或容器存储的使用方式所导致的功能性故障，并未提及任何敏感信息泄露、命令执行、提权或多用户情况下导致越权等安全风险点。  
因此，根据当前内容分析，本问题并不涉及安全漏洞或安全风险，而是属于环境和配置层面的功能问题。

**复现过程：**

```python
# 该问题不涉及安全风险，无需提供复现脚本
```


---


## Issue #131210 [BUG] Windows node reports less CPU usage than its pods with Kubernetes version 1.30.0

- Issue 链接：[#131210](https://github.com/kubernetes/kubernetes/issues/131210)

### Issue 内容

#### What happened?

In AKS clusters (k8s version 1.30.0), kubectl top node is showing less cpu load for windows nodes than expected. For example we have observed: <html>
<body>
<!--StartFragment--><p data-start="137" data-end="154" class="">You’re observing:</p>
<div class="overflow-x-auto contain-inline-size">
Pod | Node | Pod CPU (millicores) | Node CPU (millicores)
-- | -- | -- | --
-4g6vg | akswwin020001wd | 4335m | 628m (8%)
-7xcb4 | akswwin020001zz | 3496m | 552m (7%)
-q6x5x | akswwin020001t0 | 4893m | 579m (7%)
-qcsq8 | akswwin020001y4 | 3435m | 512m (6%)
-wdlc4 | akswwin020001ye | 3708m | 570m (7%)
-zxtmd | akswwin020001y5 | 4495m | 641m (8%)

</div><!--EndFragment-->
</body>
</html>




#### What did you expect to happen?

Expect to see windows node cpu metrics bigger than pods and is the summerize of all pods cpu usage.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create an AKS cluster with k8s version 1.30.0
2. Create a windows nodepool with 1 Windows2022/2019 node
3. Create simple cpu-consuming deployment.
4. Wait until metrics are collected (~1 minute)
5. Execute command kubectl top pod and kubectl top node
6. Pods have in total more cpu usage than node (It seems that the CPU metric is a factor 10 off).

#### Anything else we need to know?

The issue was reported with previous version https://github.com/kubernetes/kubernetes/issues/122382, looks like the issue still exists.

#### Kubernetes version

<details>

1.30.0

</details>


#### Cloud provider

<details>
AKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue描述来看，该问题集中在Windows节点上报的CPU使用率与实际Pod使用率不一致，属于监控或度量数据的偏差或统计BUG，并未涉及任意未授权访问、提权、命令执行、敏感信息泄露等安全风险。仅是度量数据不准确而引发的问题。因此，根据Issue内容判断为功能或性能Bug，而非安全漏洞。

**复现过程：**

```python
# 该Issue为非安全问题，不涉及安全风险，故无需编写复现POC。
# 如果确需复现此监控数据不一致的问题，可在本地搭建测试集群后对指标进行比对，
# 但此过程仅与度量和监控实现有关，并不涉及安全性。
```


---


## Issue #131209 plugin execution metric buckets are not useful for debugging high latency plugins

- Issue 链接：[#131209](https://github.com/kubernetes/kubernetes/issues/131209)

### Issue 内容

#### What happened?

Understanding scheduling latency for pods with CSI-PVC is hard because the metric `scheduler_plugin_execution_duration_seconds_bucket` does not report any buckets after 0.022s.

In the following data, it could be see that a large majority of observations (around 91k) are in +Inf bucket as compared to other observations in buckets (around 1807)   

```
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="1e-05"} 1798
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="1.5000000000000002e-05"} 1800
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="2.2500000000000005e-05"} 1806
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="3.375000000000001e-05"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="5.062500000000001e-05"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="7.593750000000002e-05"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.00011390625000000003"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.00017085937500000006"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.0002562890625000001"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.00038443359375000017"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.0005766503906250003"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.0008649755859375004"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.0012974633789062506"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.0019461950683593758"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.0029192926025390638"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.004378938903808595"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.006568408355712893"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.009852612533569338"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.014778918800354007"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="0.02216837820053101"} 1807
scheduler_plugin_execution_duration_seconds_bucket{extension_point="PreBind",plugin="VolumeBinding",status="Success",le="+Inf"} 93685
```

#### What did you expect to happen?

If scheduling latency of pod goes high, there should a way to debug which plugin and extension point is causing the highest latency

![Image](https://github.com/user-attachments/assets/4f92a7e9-3cfb-46d0-a849-650b65e31f7a)

#### How can we reproduce it (as minimally and precisely as possible)?

1. install a csi with non-trivial volume translations
2. start a deployment with 1000 pods and pvcs
3. observe the metrics

#### Anything else we need to know?

_No response_

#### Kubernetes version


```console
$ kubectl  version
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.30.5
```

</details>


#### Cloud provider

baremetal


#### OS version

NA

#### Install tools

NA

#### Container runtime (CRI) and version (if applicable)

NA

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

downstream CSI

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该 Issue 的描述和内容来看，主要问题是 Metric 的统计分档区间不够精细，导致在调试高延迟时无法区分更高时间范围内不同插件的执行时间，进而难以定位高延迟插件。这只是一个度量指标与调试能力不足的问题，本身并未暴露任何权限提升、越权访问、敏感信息泄露或命令执行等安全风险。

根据给出的风险判断标准：
1. 该问题未涉及任何可能被利用进行攻击、提权或信息泄露的情形；
2. 未有敏感信息或凭据泄露；
3. 未导致潜在的容器逃逸或命令执行等高危漏洞；
4. 仅是监控与度量信息的不足，无法被用来实施拒绝服务攻击或造成安全威胁。

因此，该 Issue 与安全问题无关，风险评级为“不涉及”。

**解释说明：**

该问题并非安全漏洞，因此不需要提供复现脚本。上文的分析中已阐明该问题不涉及安全风险，故不做进一步复现。

---


## Issue #131205 kubectl does not honor  "insecure-skip-tls-verify" flag in in-cluster mode

- Issue 链接：[#131205](https://github.com/kubernetes/kubernetes/issues/131205)

### Issue 内容

#### What happened?

I am trying to execute a kubectl command within a pod. The pod does not have a kube-config file; but it has a service account token and ca-cert.
I think running kubectl this way (within the pod) runs it in "in-cluster" mode.
I see certificate validation errors even when "insecure-skip-tls-verify" flag is specified. Please see **Case1** output below.
However the same command works when a kubeconfig file is loaded within the pod and specified in the kubectl command. Please see **Case2** output below

The server-certificate presented by my apiserver does not contain the IP address 192.168.0.1 (kubernetes clusterIP service) in the SAN. This is the reason for specifying `--insecure-skip-tls-verify=true`

The kubeconfig file used in Case2 below uses 192.168.0.1  (kubernetes clusterIP service) as the "server" (apiserver) to connect to. It should thus have the same certificate validation issue ... but it works !

**kubectl.conf file copied within the pod**
```
apiVersion: v1
kind: Config
clusters:
- cluster:
    server: https://192.168.0.1:443
```

**Case1 -- no kubeconfig file (in-cluster mode) **

```
root@fsm-eqx-chi-kubenode14-staging-karmada-st:/usr/local/pod/rackman# ./kubectl get po --insecure-skip-tls-verify=true  -v=10
I0408 12:49:00.949567      66 merged_client_builder.go:163] Using in-cluster namespace
I0408 12:49:00.949776      66 merged_client_builder.go:121] Using in-cluster configuration
I0408 12:49:00.950200      66 envvar.go:172] "Feature gate default state" feature="ClientsPreferCBOR" enabled=false
I0408 12:49:00.950219      66 envvar.go:172] "Feature gate default state" feature="InformerResourceVersion" enabled=false
I0408 12:49:00.950227      66 envvar.go:172] "Feature gate default state" feature="WatchListClient" enabled=false
I0408 12:49:00.950233      66 envvar.go:172] "Feature gate default state" feature="ClientsAllowCBOR" enabled=false
I0408 12:49:00.950271      66 discovery_client.go:253] "Request Body" body=""
I0408 12:49:00.950377      66 round_trippers.go:473] curl -v -XGET  -H "Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json" -H "User-Agent: kubectl/v1.32.3 (linux/amd64) kubernetes/32cc146" -H "Authorization: Bearer <masked>" 'https://192.168.0.1:443/api?timeout=32s'
I0408 12:49:00.963871      66 round_trippers.go:517] HTTP Trace: Dial to tcp:192.168.0.1:443 succeed
I0408 12:49:00.979655      66 round_trippers.go:560] GET https://192.168.0.1:443/api?timeout=32s  in 29 milliseconds
I0408 12:49:00.979670      66 round_trippers.go:577] HTTP Statistics: DNSLookup 0 ms Dial 13 ms TLSHandshake 15 ms Duration 29 ms
I0408 12:49:00.979679      66 round_trippers.go:584] Response Headers:
E0408 12:49:00.979742      66 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://192.168.0.1:443/api?timeout=32s\": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs"
I0408 12:49:00.980876      66 cached_discovery.go:120] skipped caching discovery info due to Get "https://192.168.0.1:443/api?timeout=32s": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs
I0408 12:49:00.980991      66 discovery_client.go:253] "Request Body" body=""
I0408 12:49:00.981084      66 round_trippers.go:473] curl -v -XGET  -H "Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json" -H "User-Agent: kubectl/v1.32.3 (linux/amd64) kubernetes/32cc146" -H "Authorization: Bearer <masked>" 'https://192.168.0.1:443/api?timeout=32s'
I0408 12:49:00.992617      66 round_trippers.go:517] HTTP Trace: Dial to tcp:192.168.0.1:443 succeed
I0408 12:49:01.008172      66 round_trippers.go:560] GET https://192.168.0.1:443/api?timeout=32s  in 27 milliseconds
I0408 12:49:01.008233      66 round_trippers.go:577] HTTP Statistics: DNSLookup 0 ms Dial 11 ms TLSHandshake 15 ms Duration 27 ms
I0408 12:49:01.008253      66 round_trippers.go:584] Response Headers:
E0408 12:49:01.008369      66 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://192.168.0.1:443/api?timeout=32s\": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs"
I0408 12:49:01.009578      66 cached_discovery.go:120] skipped caching discovery info due to Get "https://192.168.0.1:443/api?timeout=32s": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs
I0408 12:49:01.009649      66 shortcut.go:103] Error loading discovery information: Get "https://192.168.0.1:443/api?timeout=32s": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs
I0408 12:49:01.009724      66 discovery_client.go:253] "Request Body" body=""
I0408 12:49:01.009882      66 round_trippers.go:473] curl -v -XGET  -H "Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json" -H "User-Agent: kubectl/v1.32.3 (linux/amd64) kubernetes/32cc146" -H "Authorization: Bearer <masked>" 'https://192.168.0.1:443/api?timeout=32s'
I0408 12:49:01.021283      66 round_trippers.go:517] HTTP Trace: Dial to tcp:192.168.0.1:443 succeed
I0408 12:49:01.036799      66 round_trippers.go:560] GET https://192.168.0.1:443/api?timeout=32s  in 26 milliseconds
I0408 12:49:01.036834      66 round_trippers.go:577] HTTP Statistics: DNSLookup 0 ms Dial 11 ms TLSHandshake 15 ms Duration 26 ms
I0408 12:49:01.036852      66 round_trippers.go:584] Response Headers:
E0408 12:49:01.036939      66 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://192.168.0.1:443/api?timeout=32s\": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs"
I0408 12:49:01.038067      66 cached_discovery.go:120] skipped caching discovery info due to Get "https://192.168.0.1:443/api?timeout=32s": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs
I0408 12:49:01.038174      66 discovery_client.go:253] "Request Body" body=""
I0408 12:49:01.038275      66 round_trippers.go:473] curl -v -XGET  -H "User-Agent: kubectl/v1.32.3 (linux/amd64) kubernetes/32cc146" -H "Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json" -H "Authorization: Bearer <masked>" 'https://192.168.0.1:443/api?timeout=32s'
I0408 12:49:01.049681      66 round_trippers.go:517] HTTP Trace: Dial to tcp:192.168.0.1:443 succeed
I0408 12:49:01.064773      66 round_trippers.go:560] GET https://192.168.0.1:443/api?timeout=32s  in 26 milliseconds
I0408 12:49:01.064804      66 round_trippers.go:577] HTTP Statistics: DNSLookup 0 ms Dial 11 ms TLSHandshake 15 ms Duration 26 ms
I0408 12:49:01.064821      66 round_trippers.go:584] Response Headers:
E0408 12:49:01.064901      66 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://192.168.0.1:443/api?timeout=32s\": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs"
I0408 12:49:01.066026      66 cached_discovery.go:120] skipped caching discovery info due to Get "https://192.168.0.1:443/api?timeout=32s": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs
I0408 12:49:01.066104      66 discovery_client.go:253] "Request Body" body=""
I0408 12:49:01.066206      66 round_trippers.go:473] curl -v -XGET  -H "User-Agent: kubectl/v1.32.3 (linux/amd64) kubernetes/32cc146" -H "Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json" -H "Authorization: Bearer <masked>" 'https://192.168.0.1:443/api?timeout=32s'
I0408 12:49:01.078300      66 round_trippers.go:517] HTTP Trace: Dial to tcp:192.168.0.1:443 succeed
I0408 12:49:01.094105      66 round_trippers.go:560] GET https://192.168.0.1:443/api?timeout=32s  in 27 milliseconds
I0408 12:49:01.094137      66 round_trippers.go:577] HTTP Statistics: DNSLookup 0 ms Dial 11 ms TLSHandshake 15 ms Duration 27 ms
I0408 12:49:01.094155      66 round_trippers.go:584] Response Headers:
E0408 12:49:01.094230      66 memcache.go:265] "Unhandled Error" err="couldn't get current server API group list: Get \"https://192.168.0.1:443/api?timeout=32s\": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs"
I0408 12:49:01.095352      66 cached_discovery.go:120] skipped caching discovery info due to Get "https://192.168.0.1:443/api?timeout=32s": tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs
I0408 12:49:01.095428      66 helpers.go:264] Connection error: Get https://192.168.0.1:443/api?timeout=32s: tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs
Unable to connect to the server: tls: failed to verify certificate: x509: cannot validate certificate for 192.168.0.1 because it doesn't contain any IP SANs
root@fsm-eqx-chi-kubenode14-staging-karmada-st:/usr/local/pod/rackman# 

```

**Case2 -- with kube-config file**

```
root@fsm-eqx-chi-kubenode14-staging-karmada-st:/usr/local/pod/rackman# ./kubectl get po --insecure-skip-tls-verify=true  --kubeconfig=./k.conf -v=10
I0408 12:51:05.912499     115 loader.go:402] Config loaded from file:  ./k.conf
I0408 12:51:05.912634     115 merged_client_builder.go:163] Using in-cluster namespace
I0408 12:51:05.912865     115 envvar.go:172] "Feature gate default state" feature="ClientsAllowCBOR" enabled=false
I0408 12:51:05.912883     115 envvar.go:172] "Feature gate default state" feature="ClientsPreferCBOR" enabled=false
I0408 12:51:05.912890     115 envvar.go:172] "Feature gate default state" feature="InformerResourceVersion" enabled=false
I0408 12:51:05.912897     115 envvar.go:172] "Feature gate default state" feature="WatchListClient" enabled=false
I0408 12:51:05.912934     115 discovery_client.go:253] "Request Body" body=""
I0408 12:51:05.913006     115 round_trippers.go:473] curl -v -XGET  -H "User-Agent: kubectl/v1.32.3 (linux/amd64) kubernetes/32cc146" -H "Authorization: Bearer <masked>" -H "Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json" 'https://192.168.0.1:443/api?timeout=32s'
I0408 12:51:05.926274     115 round_trippers.go:517] HTTP Trace: Dial to tcp:192.168.0.1:443 succeed
I0408 12:51:05.955768     115 round_trippers.go:560] GET https://192.168.0.1:443/api?timeout=32s 200 OK in 42 milliseconds
I0408 12:51:05.955811     115 round_trippers.go:577] HTTP Statistics: DNSLookup 0 ms Dial 13 ms TLSHandshake 15 ms ServerProcessing 12 ms Duration 42 ms
I0408 12:51:05.955841     115 round_trippers.go:584] Response Headers:
I0408 12:51:05.955861     115 round_trippers.go:587]     Cache-Control: public
I0408 12:51:05.955875     115 round_trippers.go:587]     Vary: Accept
I0408 12:51:05.955886     115 round_trippers.go:587]     X-Kubernetes-Pf-Prioritylevel-Uid: 7d9c616a-a9f7-4cae-8abc-3b48f7e25d27
I0408 12:51:05.955896     115 round_trippers.go:587]     X-Varied-Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json
I0408 12:51:05.955907     115 round_trippers.go:587]     Audit-Id: ce75520e-a997-4d1e-87c7-6d4dca17a25d
I0408 12:51:05.955921     115 round_trippers.go:587]     Content-Type: application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList
I0408 12:51:05.955943     115 round_trippers.go:587]     Etag: "7E2E4FBD8CDC884130EBFEF64757F6BC507A8EA6A2EA8F94E05F1B6F207B4E183C63D6D6EA5513FDDC10FC16233248F933DC787C288493D52745A3FAFD68CCD8"
I0408 12:51:05.955956     115 round_trippers.go:587]     X-Kubernetes-Pf-Flowschema-Uid: 278b1097-7b09-48fd-8f57-2cae1444965c
I0408 12:51:05.955969     115 round_trippers.go:587]     Date: Tue, 08 Apr 2025 12:51:05 GMT
I0408 12:51:05.957867     115 discovery_client.go:253] "Response Body" body=<
```

#### What did you expect to happen?

I expected the `--insecure-skip-tls-verify=true` flag to work the same (& succeed) in both cases.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Configure an apiserver ith a certificate that does not have 192.168.0.1 (the clusterIP) in the SAN field.
2. Create a pod
3. Install kubectl within the pod 
4. Copy a valid kubeconfig within the pod
5. Execute both cases as shown above.


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version

Client Version: v1.29.14
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.14

```

</details>


#### Cloud provider

<details>
none -- Baremetal cluster
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# PRETTY_NAME="Debian GNU/Linux 11 (bullseye)"
NAME="Debian GNU/Linux"
VERSION_ID="11"
VERSION="11 (bullseye)"
VERSION_CODENAME=bullseye
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ uname -a
Linux gce-usc-kubemaster11-staging 5.10.0-34-cloud-amd64 #1 SMP Debian 5.10.234-1 (2025-02-24) x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
none -- installed using ansible
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd v1.7.17 
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
CNI - kube-router
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue的描述来看，该问题的核心是“在集群内运行kubectl时，即使指定了--insecure-skip-tls-verify=true选项，依然会因为证书不包含目标IP地址的SAN字段而导致验证失败”，属于kubectl在“in-cluster”模式下对insecure-skip-tls-verify配置的处理逻辑缺陷或不一致，与真正的安全漏洞无关。  
这一问题并不能被攻击者利用来造成信息泄露、提权、注入或其他安全风险，也不会导致拒绝服务，由于Issue本身并未体现出可被利用的攻击面或造成安全影响的可能性，因此不属于安全漏洞范畴。

**解释说明：**

本Issue反映的是一个功能/配置缺陷，并未包含可能被恶意利用的安全风险点，故将风险评级判断为“不涉及”，无需提供漏洞复现脚本。若后续该问题在其他场景下被证明会导致安全威胁，则需要另行评估。

---


## Issue #131197 ValidatingAdmissionPolicy CEL Type Checking fails using `all` macro with notAComprehensionRange

- Issue 链接：[#131197](https://github.com/kubernetes/kubernetes/issues/131197)

### Issue 内容

#### What happened?

Attempting to use ValidatingAdmissionPolicy to provide a layer of authz for our Kubernetes cluster to allow some edits but not others to the Pod/Deployment spec.

With an (admittedly complicated) expression such as:
```
!has(object.spec) && !has(oldObject.spec) || // skip validation if spec is not present
// allow only if the pod spec is not changed, or if the changes are only the container resources
object.spec.all(k, k == "containers" ?  
  object.spec.containers.all(c, 
    oldObject.spec.containers.exists(o, o.name == c.name &&
    c.all(k, k == "resources" || (k in o && c[k] == o[k])))) 
: (k in oldObject.spec && object.spec[k] == oldObject.spec[k])) &&
oldObject.spec.all(k, k == "containers" ? 
  oldObject.spec.containers.all(c, 
    object.spec.containers.exists(o, o.name == c.name &&
      c.all(k, k == "resources" || (k in o && c[k] == o[k])))) 
: (k in object.spec && object.spec[k] == oldObject.spec[k]))
```
(which intends to allow metadata changes such as labels/ownerReferences/annotations, and InPlacePodVerticalScaling resource changes)

The CEL type checker complains (via the ValidatingAdmissionPolicy status field) with:
```
      Warning:    /v1, Kind=Pod: ERROR: <input>:5:7: expression of type 'Pod127885267.spec' cannot be range of a comprehension (must be list, map, or dynamic)
 | object.spec.all(k, k == "containers" ?
 | ......^
```

#### What did you expect to happen?

The type checker passes the expression, as the object spec should be a Map.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a ValidatingAdmissionPolicy with the above expression, view the status.

#### Anything else we need to know?

/sig api-machinery

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.3
Kustomize Version: v5.4.2
Server Version: v1.31.7
```

</details>


#### Cloud provider

<details>
AWS, self-hosted on EC2. 
</details>




### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据Issue描述可知，这属于在使用ValidatingAdmissionPolicy的CEL表达式时，类型检查无法通过所导致的功能性或逻辑性问题，并未描述任何可被利用的安全漏洞或不当访问场景。问题核心在于CEL对spec字段类型识别不正确，导致表达式无法通过校验，而非安全风险点。因此，该Issue不属于安全风险范畴。

**复现过程：**

```python
# 由于该Issue并不涉及安全风险，因此无需复现脚本
# 如需测试CEL表达式，请在本地Kubernetes集群中创建相应的ValidatingAdmissionPolicy资源
# 并将Issue中给出的表达式配置到policy中进行验证。
```


---


## Issue #131189 kubectl exec report error Unknown stream id 1, discarding message

- Issue 链接：[#131189](https://github.com/kubernetes/kubernetes/issues/131189)

### Issue 内容

#### What happened?

When I run the kubectl exec command, an error log is printed. When does this error occur?
![Image](https://github.com/user-attachments/assets/7002dc4b-775e-4837-b90c-fe04d3613697)

#### What did you expect to happen?

Whether no error can be reported

#### How can we reproduce it (as minimally and precisely as possible)?

Run the kubectl exec command.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.31
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue描述来看，这只是关于在使用 kubectl exec 命令时出现了 "Unknown stream id 1, discarding message" 这样一条错误日志。该日志一般与网络传输、客户端与服务端版本兼容性或工具本身的Bug等因素有关，并无迹象表明其可导致越权、命令执行、数据泄露等安全风险，也没有涉及到敏感信息暴露或不当配置的问题。因此，本Issue不属于安全漏洞或风险的范畴。

**解释说明：**

本Issue的内容仅为错误日志问题，与安全风险无关，因此无需编写POC脚本。该错误通常是正常的调试或错误提示，不影响系统整体安全。若欲进一步排查，可从网络环境、kubectl与集群的版本兼容性等方面着手。

---


## Issue #131186 Severe file copy slowdown in pod using Local PV after node reboot (resolved by pod recreation)

- Issue 链接：[#131186](https://github.com/kubernetes/kubernetes/issues/131186)

### Issue 内容

#### What happened?

We are using Local PersistentVolume (local PV) to mount a local directory to a pod. After a node reboot, the pod is still running, but performing file copy operations inside the pod (e.g., using `cp`) becomes very slow.

Using `strace -f -tt -T -p <PID>` to trace the cp process shows that `sendfile()` system calls are taking 0.7+ seconds per 16MB chunk:
```
15:02:53.949729 sendfile(4, 3, NULL, 16777216) = 16777216 <0.715818>
15:02:54.665935 sendfile(4, 3, NULL, 16777216) = 16777216 <0.731067>
15:02:55.397318 sendfile(4, 3, NULL, 16777216) = 16777216 <0.737302>
...

```

However, if we delete the pod and let it be recreated, the file copy speed returns to normal. sendfile() calls then complete in ~0.015 seconds:
```
15:04:38.006504 sendfile(4, 3, NULL, 16777216) = 16777216 <0.015368>
15:04:38.022146 sendfile(4, 3, NULL, 16777216) = 16777216 <0.015512>
...
```



#### What did you expect to happen?

After node reboot, the pod using local PV should continue to perform file copy operations at normal speed without requiring a pod delete and recreate.

#### How can we reproduce it (as minimally and precisely as possible)?

1.Create a pod that mounts a local directory using local PersistentVolume.
2.Perform a file copy operation inside the pod (e.g., cp largefile /tmp/), observe performance is good.
3.Reboot the node where the pod is scheduled.
4.After the node is back and the pod is still running, repeat the file copy operation.
5.Observe that file copy is very slow.
6.Delete the pod, let it be recreated (same node), and copy files again — performance is back to normal.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
1.28.3
```

</details>


#### Cloud provider

<details>
none
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述的是在使用 Local PV 的容器化应用节点重启后出现文件复制性能严重下降的问题，从描述来看并没有涉及潜在的安全漏洞或敏感信息泄露，也未涉及到跨权限操作、漏洞利用、提权等问题。该问题更像是一个与 Kubernetes 或底层存储重连机制相关的性能或可靠性缺陷，并不属于安全漏洞范畴。

**解释说明：**

Issue 所述的问题仅与节点重启后本地存储性能表现不佳有关，不涉及任何可被攻击者利用的安全缺陷，故不涉及安全风险。这个问题可以通过 Kubernetes 对本地节点重启后对 PV 的重新挂载或容器重建策略进行优化来解决，也可与存储驱动本身的实现细节关联，但无论何种情形都未体现安全风险。故最终判断：不涉及安全风险。

---


## Issue #131178 Deleting a namespace doesn't switches to default namespace

- Issue 链接：[#131178](https://github.com/kubernetes/kubernetes/issues/131178)

### Issue 内容

#### What happened?

I created a namespace `04--pod namespace` & then I created couple of pods in this namespace(Irrelevant to the issue but still describing the actual flow). I then deleted the namespace & this does 2 things:
 - Deletes the namespace
 - Deletes all resources such as pods associated in the namespace

Now that I run `kubectl get pods`, I get the following result
```
No resources found in 04--pod namespace.
```

<img width="431" alt="Image" src="https://github.com/user-attachments/assets/491ea6ce-e604-40cf-bd15-8466ddfec8cd" />

#### What did you expect to happen?

Deleting a namespace should switch it back to the `default` namespace. As the namespace is deleted, the `get pods` lookup shouldn't be looking into the deleted namespace. This feels like a data inconsistency

<img width="495" alt="Image" src="https://github.com/user-attachments/assets/64ac88e8-cbd7-4dbc-873f-30ee14202e69" />

#### How can we reproduce it (as minimally and precisely as possible)?

- Create a namespace
- Switch to that namespace
- Create a pod(Not required)
- Delete the namespace
- Run `kubectl get pods` . This will show no pods present in the deleted namespace

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.7+k3s1
WARNING: version difference between client (1.30) and server (1.28) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 仅描述了在删除 Namespace 后，kubectl 查询命名空间仍默认停留在已被删除的命名空间，从而导致查不到资源的问题。此问题属于功能/使用层面的不便，不涉及敏感信息泄露、权限提升、执行任意命令等安全风险。因此根据Issue描述可判断此问题不属于安全漏洞。

**解释说明：**

该问题并不涉及对系统权限、数据或资源的非预期访问，也不会导致拒绝服务、敏感信息泄露或任意代码执行等安全风险。因此无需提供复现脚本。该 Issue 主要是一个体验或功能一致性的问题，未涉及安全漏洞范畴。

---


## Issue #131175 No-op Server-Side Apply (SSA) calls updates resourceVersion and metadata.managedFields[].time

- Issue 链接：[#131175](https://github.com/kubernetes/kubernetes/issues/131175)

### Issue 内容

#### What happened?

I have a controller-runtime based controller that creates various resources using Server-Side Apply (SSA) using `Client.Patch(ctx, obj, client.Apply)` where `obj` is of Unstructured type. In the case of ClusterRoleBindings with an empty array for `subjects` (subjects: []`, I see resourceVersion increment on SSA using the Client even when the entire object is the same as before. 

The resulting object applied to the cluster is as follows:
```
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    app: gloo
    app.kubernetes.io/managed-by: gloo-operator
    gloo: rbac
    operator.gloo.solo.io/managed-by: gatewaycontroller.gloo-gateway
  name: gloo-upstream-mutator-binding-gloo-system
  ownerReferences:
  - apiVersion: operator.gloo.solo.io/v1
    blockOwnerDeletion: true
    controller: true
    kind: GatewayController
    name: gloo-gateway
    uid: 428928a5-e3b0-4b44-bdb9-846213aedc14
  uid: 8851a60d-2577-4618-a824-7c39d7a179d6
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: gloo-upstream-mutator-gloo-system
subjects: []
```

The constant resourceVersion bumps cause infinite reconciliation loops. 

Logs from my reconciler that reconciles GatewayController CRs, which is an OwnerReference for the ClusterRoleBinding created above.

<details>

<summary>Controller logs</summary>

```
{"time":"2025-04-04T11:05:19.849442305-07:00","level":"INFO","msg":"reconciling","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"60ca6e8d-8cbe-4414-ad19-92ce093a0b47"}
{"time":"2025-04-04T11:05:21.039420497-07:00","level":"DEBUG","msg":"DBG before patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":""}
{"time":"2025-04-04T11:05:21.039541053-07:00","level":"TRACE","msg":"reconciling","controller":"GatewayController","name":"gloo-gateway","kind":"ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:21.039822559-07:00","level":"DEBUG-4","msg":"Request Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"60ca6e8d-8cbe-4414-ad19-92ce093a0b47","body":"{\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"kind\":\"ClusterRoleBinding\",\"metadata\":{\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"blockOwnerDeletion\":true,\"controller\":true,\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\"}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"},\"subjects\":[]}"}
{"time":"2025-04-04T11:05:21.134339032-07:00","level":"DEBUG","msg":"update event","component":"k8s-events","controller":"GatewayController","kind":"*v1.ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:21.134334874-07:00","level":"DEBUG-4","msg":"Response Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"60ca6e8d-8cbe-4414-ad19-92ce093a0b47","body":"{\"kind\":\"ClusterRoleBinding\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"metadata\":{\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"uid\":\"8851a60d-2577-4618-a824-7c39d7a179d6\",\"resourceVersion\":\"715344\",\"creationTimestamp\":\"2025-04-03T19:34:24Z\",\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\",\"controller\":true,\"blockOwnerDeletion\":true}],\"managedFields\":[{\"manager\":\"test\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{}}},{\"manager\":\"solo.io/gloo-operator\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"time\":\"2025-04-04T18:05:21Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{},\"f:subjects\":{}}}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"}}\n"}
{"time":"2025-04-04T11:05:21.134744019-07:00","level":"DEBUG","msg":"DBG after patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":"715344"}
{"time":"2025-04-04T11:05:21.134764397-07:00","level":"INFO","msg":"reconcile done","controller":"GatewayController","name":"gloo-gateway","requeue":false}
{"time":"2025-04-04T11:05:21.134802328-07:00","level":"INFO","msg":"Starting EventSource","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","source":{"Type":{"metadata":{"creationTimestamp":null},"spec":{"controllerName":""},"status":{}},"Cache":{"Scheme":{},"RESTMapper":{}},"Handler":{},"Predicates":[{}]}}
{"time":"2025-04-04T11:05:21.13527954-07:00","level":"INFO","msg":"reconciling","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"1c2eb894-a1a9-4890-b5b0-4033c000b3db"}
{"time":"2025-04-04T11:05:22.167487732-07:00","level":"DEBUG","msg":"DBG before patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":""}
{"time":"2025-04-04T11:05:22.167526584-07:00","level":"TRACE","msg":"reconciling","controller":"GatewayController","name":"gloo-gateway","kind":"ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:22.167585054-07:00","level":"DEBUG-4","msg":"Request Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"1c2eb894-a1a9-4890-b5b0-4033c000b3db","body":"{\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"kind\":\"ClusterRoleBinding\",\"metadata\":{\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"blockOwnerDeletion\":true,\"controller\":true,\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\"}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"},\"subjects\":[]}"}
{"time":"2025-04-04T11:05:22.259725328-07:00","level":"DEBUG","msg":"update event","component":"k8s-events","controller":"GatewayController","kind":"*v1.ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:22.260006613-07:00","level":"DEBUG-4","msg":"Response Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"1c2eb894-a1a9-4890-b5b0-4033c000b3db","body":"{\"kind\":\"ClusterRoleBinding\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"metadata\":{\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"uid\":\"8851a60d-2577-4618-a824-7c39d7a179d6\",\"resourceVersion\":\"715350\",\"creationTimestamp\":\"2025-04-03T19:34:24Z\",\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\",\"controller\":true,\"blockOwnerDeletion\":true}],\"managedFields\":[{\"manager\":\"test\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{}}},{\"manager\":\"solo.io/gloo-operator\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"time\":\"2025-04-04T18:05:22Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{},\"f:subjects\":{}}}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"}}\n"}
{"time":"2025-04-04T11:05:22.260227096-07:00","level":"DEBUG","msg":"DBG after patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":"715350"}
{"time":"2025-04-04T11:05:22.260241252-07:00","level":"INFO","msg":"reconcile done","controller":"GatewayController","name":"gloo-gateway","requeue":false}
{"time":"2025-04-04T11:05:22.260306745-07:00","level":"INFO","msg":"reconciling","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"bd02ebf6-fc01-44db-8b1e-d1dd5dac695e"}
{"time":"2025-04-04T11:05:23.18185363-07:00","level":"DEBUG","msg":"DBG before patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":""}
{"time":"2025-04-04T11:05:23.181900417-07:00","level":"TRACE","msg":"reconciling","controller":"GatewayController","name":"gloo-gateway","kind":"ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:23.181959288-07:00","level":"DEBUG-4","msg":"Request Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"bd02ebf6-fc01-44db-8b1e-d1dd5dac695e","body":"{\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"kind\":\"ClusterRoleBinding\",\"metadata\":{\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"blockOwnerDeletion\":true,\"controller\":true,\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\"}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"},\"subjects\":[]}"}
{"time":"2025-04-04T11:05:23.26796157-07:00","level":"DEBUG","msg":"update event","component":"k8s-events","controller":"GatewayController","kind":"*v1.ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:23.269135375-07:00","level":"DEBUG-4","msg":"Response Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"bd02ebf6-fc01-44db-8b1e-d1dd5dac695e","body":"{\"kind\":\"ClusterRoleBinding\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"metadata\":{\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"uid\":\"8851a60d-2577-4618-a824-7c39d7a179d6\",\"resourceVersion\":\"715355\",\"creationTimestamp\":\"2025-04-03T19:34:24Z\",\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\",\"controller\":true,\"blockOwnerDeletion\":true}],\"managedFields\":[{\"manager\":\"test\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{}}},{\"manager\":\"solo.io/gloo-operator\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"time\":\"2025-04-04T18:05:23Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{},\"f:subjects\":{}}}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"}}\n"}
{"time":"2025-04-04T11:05:23.269257834-07:00","level":"DEBUG","msg":"DBG after patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":"715355"}
{"time":"2025-04-04T11:05:23.269265027-07:00","level":"INFO","msg":"reconcile done","controller":"GatewayController","name":"gloo-gateway","requeue":false}
{"time":"2025-04-04T11:05:23.269321613-07:00","level":"INFO","msg":"reconciling","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"159cf501-7635-46ac-a648-07d7e7714379"}
{"time":"2025-04-04T11:05:24.110189038-07:00","level":"DEBUG","msg":"DBG before patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":""}
{"time":"2025-04-04T11:05:24.110232379-07:00","level":"TRACE","msg":"reconciling","controller":"GatewayController","name":"gloo-gateway","kind":"ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:24.110287492-07:00","level":"DEBUG-4","msg":"Request Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"159cf501-7635-46ac-a648-07d7e7714379","body":"{\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"kind\":\"ClusterRoleBinding\",\"metadata\":{\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"blockOwnerDeletion\":true,\"controller\":true,\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\"}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"},\"subjects\":[]}"}
{"time":"2025-04-04T11:05:24.21306735-07:00","level":"DEBUG","msg":"update event","component":"k8s-events","controller":"GatewayController","kind":"*v1.ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:24.221289459-07:00","level":"DEBUG-4","msg":"Response Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"159cf501-7635-46ac-a648-07d7e7714379","body":"{\"kind\":\"ClusterRoleBinding\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"metadata\":{\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"uid\":\"8851a60d-2577-4618-a824-7c39d7a179d6\",\"resourceVersion\":\"715361\",\"creationTimestamp\":\"2025-04-03T19:34:24Z\",\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\",\"controller\":true,\"blockOwnerDeletion\":true}],\"managedFields\":[{\"manager\":\"test\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{}}},{\"manager\":\"solo.io/gloo-operator\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"time\":\"2025-04-04T18:05:24Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{},\"f:subjects\":{}}}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"}}\n"}
{"time":"2025-04-04T11:05:24.221590482-07:00","level":"DEBUG","msg":"DBG after patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":"715361"}
{"time":"2025-04-04T11:05:24.221610059-07:00","level":"INFO","msg":"reconcile done","controller":"GatewayController","name":"gloo-gateway","requeue":false}
{"time":"2025-04-04T11:05:24.221716838-07:00","level":"INFO","msg":"reconciling","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"e9aa3054-66ac-4f75-8198-0e36f97c3f63"}
{"time":"2025-04-04T11:05:25.038961847-07:00","level":"DEBUG","msg":"DBG before patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":""}
{"time":"2025-04-04T11:05:25.039003995-07:00","level":"TRACE","msg":"reconciling","controller":"GatewayController","name":"gloo-gateway","kind":"ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:25.039085538-07:00","level":"DEBUG-4","msg":"Request Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"e9aa3054-66ac-4f75-8198-0e36f97c3f63","body":"{\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"kind\":\"ClusterRoleBinding\",\"metadata\":{\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"blockOwnerDeletion\":true,\"controller\":true,\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\"}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"},\"subjects\":[]}"}
{"time":"2025-04-04T11:05:25.129147224-07:00","level":"DEBUG","msg":"update event","component":"k8s-events","controller":"GatewayController","kind":"*v1.ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:25.147824027-07:00","level":"DEBUG-4","msg":"Response Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"e9aa3054-66ac-4f75-8198-0e36f97c3f63","body":"{\"kind\":\"ClusterRoleBinding\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"metadata\":{\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"uid\":\"8851a60d-2577-4618-a824-7c39d7a179d6\",\"resourceVersion\":\"715368\",\"creationTimestamp\":\"2025-04-03T19:34:24Z\",\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\",\"controller\":true,\"blockOwnerDeletion\":true}],\"managedFields\":[{\"manager\":\"test\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{}}},{\"manager\":\"solo.io/gloo-operator\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"time\":\"2025-04-04T18:05:25Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{},\"f:subjects\":{}}}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"}}\n"}
{"time":"2025-04-04T11:05:25.148016978-07:00","level":"DEBUG","msg":"DBG after patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":"715368"}
{"time":"2025-04-04T11:05:25.148030082-07:00","level":"INFO","msg":"reconcile done","controller":"GatewayController","name":"gloo-gateway","requeue":false}
{"time":"2025-04-04T11:05:25.14812521-07:00","level":"INFO","msg":"reconciling","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"ec78a951-2894-4360-9e73-5a75a670b8a9"}
{"time":"2025-04-04T11:05:25.977047899-07:00","level":"DEBUG","msg":"DBG before patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":""}
{"time":"2025-04-04T11:05:25.977096409-07:00","level":"TRACE","msg":"reconciling","controller":"GatewayController","name":"gloo-gateway","kind":"ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:25.977157443-07:00","level":"DEBUG-4","msg":"Request Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"ec78a951-2894-4360-9e73-5a75a670b8a9","body":"{\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"kind\":\"ClusterRoleBinding\",\"metadata\":{\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"blockOwnerDeletion\":true,\"controller\":true,\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\"}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"},\"subjects\":[]}"}
{"time":"2025-04-04T11:05:26.064699551-07:00","level":"DEBUG","msg":"update event","component":"k8s-events","controller":"GatewayController","kind":"*v1.ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:26.06539972-07:00","level":"DEBUG-4","msg":"Response Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"ec78a951-2894-4360-9e73-5a75a670b8a9","body":"{\"kind\":\"ClusterRoleBinding\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"metadata\":{\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"uid\":\"8851a60d-2577-4618-a824-7c39d7a179d6\",\"resourceVersion\":\"715373\",\"creationTimestamp\":\"2025-04-03T19:34:24Z\",\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\",\"controller\":true,\"blockOwnerDeletion\":true}],\"managedFields\":[{\"manager\":\"test\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{}}},{\"manager\":\"solo.io/gloo-operator\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"time\":\"2025-04-04T18:05:26Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{},\"f:subjects\":{}}}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"}}\n"}
{"time":"2025-04-04T11:05:26.065578204-07:00","level":"DEBUG","msg":"DBG after patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":"715373"}
{"time":"2025-04-04T11:05:26.065589615-07:00","level":"INFO","msg":"reconcile done","controller":"GatewayController","name":"gloo-gateway","requeue":false}
{"time":"2025-04-04T11:05:26.065674955-07:00","level":"INFO","msg":"reconciling","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"6c5350d2-aa90-45e6-ad17-5358e52c76df"}
{"time":"2025-04-04T11:05:26.964328778-07:00","level":"DEBUG","msg":"DBG before patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":""}
{"time":"2025-04-04T11:05:26.964370346-07:00","level":"TRACE","msg":"reconciling","controller":"GatewayController","name":"gloo-gateway","kind":"ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:26.964424768-07:00","level":"DEBUG-4","msg":"Request Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"6c5350d2-aa90-45e6-ad17-5358e52c76df","body":"{\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"kind\":\"ClusterRoleBinding\",\"metadata\":{\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"blockOwnerDeletion\":true,\"controller\":true,\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\"}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"},\"subjects\":[]}"}
{"time":"2025-04-04T11:05:27.067780249-07:00","level":"DEBUG-4","msg":"Response Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"6c5350d2-aa90-45e6-ad17-5358e52c76df","body":"{\"kind\":\"ClusterRoleBinding\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"metadata\":{\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"uid\":\"8851a60d-2577-4618-a824-7c39d7a179d6\",\"resourceVersion\":\"715377\",\"creationTimestamp\":\"2025-04-03T19:34:24Z\",\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\",\"controller\":true,\"blockOwnerDeletion\":true}],\"managedFields\":[{\"manager\":\"test\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{}}},{\"manager\":\"solo.io/gloo-operator\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"time\":\"2025-04-04T18:05:27Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{},\"f:subjects\":{}}}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"}}\n"}
{"time":"2025-04-04T11:05:27.067785829-07:00","level":"DEBUG","msg":"update event","component":"k8s-events","controller":"GatewayController","kind":"*v1.ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:27.067987757-07:00","level":"DEBUG","msg":"DBG after patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":"715377"}
{"time":"2025-04-04T11:05:27.068000801-07:00","level":"INFO","msg":"reconcile done","controller":"GatewayController","name":"gloo-gateway","requeue":false}
{"time":"2025-04-04T11:05:27.068082835-07:00","level":"INFO","msg":"reconciling","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"b3a79bfe-a5a3-4727-b398-9f1e5be7bab8"}
{"time":"2025-04-04T11:05:27.937475733-07:00","level":"DEBUG","msg":"DBG before patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":""}
{"time":"2025-04-04T11:05:27.937514335-07:00","level":"TRACE","msg":"reconciling","controller":"GatewayController","name":"gloo-gateway","kind":"ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
{"time":"2025-04-04T11:05:27.937581009-07:00","level":"DEBUG-4","msg":"Request Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"b3a79bfe-a5a3-4727-b398-9f1e5be7bab8","body":"{\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"kind\":\"ClusterRoleBinding\",\"metadata\":{\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"blockOwnerDeletion\":true,\"controller\":true,\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\"}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"},\"subjects\":[]}"}
{"time":"2025-04-04T11:05:28.034370458-07:00","level":"DEBUG-4","msg":"Response Body","controller":"gatewaycontroller","controllerGroup":"operator.gloo.solo.io","controllerKind":"GatewayController","GatewayController":{"name":"gloo-gateway"},"namespace":"","name":"gloo-gateway","reconcileID":"b3a79bfe-a5a3-4727-b398-9f1e5be7bab8","body":"{\"kind\":\"ClusterRoleBinding\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"metadata\":{\"name\":\"gloo-upstream-mutator-binding-gloo-system\",\"uid\":\"8851a60d-2577-4618-a824-7c39d7a179d6\",\"resourceVersion\":\"715377\",\"creationTimestamp\":\"2025-04-03T19:34:24Z\",\"labels\":{\"app\":\"gloo\",\"app.kubernetes.io/managed-by\":\"gloo-operator\",\"gloo\":\"rbac\",\"operator.gloo.solo.io/managed-by\":\"gatewaycontroller.gloo-gateway\"},\"ownerReferences\":[{\"apiVersion\":\"operator.gloo.solo.io/v1\",\"kind\":\"GatewayController\",\"name\":\"gloo-gateway\",\"uid\":\"428928a5-e3b0-4b44-bdb9-846213aedc14\",\"controller\":true,\"blockOwnerDeletion\":true}],\"managedFields\":[{\"manager\":\"test\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{}}},{\"manager\":\"solo.io/gloo-operator\",\"operation\":\"Apply\",\"apiVersion\":\"rbac.authorization.k8s.io/v1\",\"time\":\"2025-04-04T18:05:27Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:metadata\":{\"f:labels\":{\"f:app\":{},\"f:app.kubernetes.io/managed-by\":{},\"f:gloo\":{},\"f:operator.gloo.solo.io/managed-by\":{}},\"f:ownerReferences\":{\"k:{\\\"uid\\\":\\\"428928a5-e3b0-4b44-bdb9-846213aedc14\\\"}\":{}}},\"f:roleRef\":{},\"f:subjects\":{}}}]},\"roleRef\":{\"apiGroup\":\"rbac.authorization.k8s.io\",\"kind\":\"ClusterRole\",\"name\":\"gloo-upstream-mutator-gloo-system\"}}\n"}
{"time":"2025-04-04T11:05:28.034599266-07:00","level":"DEBUG","msg":"DBG after patch ==========","controller":"GatewayController","name":"gloo-gateway","resourceVersion":"715377"}
{"time":"2025-04-04T11:05:28.03461255-07:00","level":"INFO","msg":"reconcile done","controller":"GatewayController","name":"gloo-gateway","requeue":false}
```
</details>

As seen from the logs, the request body is the same always, the responde body only differs in the `resourceVersion`  and `metadata.managedFields[].time` fields. 

A `--watch` on the ClusterRoleBinding indicates that it is not being removed (age doesn't change), but I see updates on every SSA call that my controller makes:
```
gloo-upstream-mutator-binding-gloo-system                       ClusterRole/gloo-upstream-mutator-gloo-system                               22h
gloo-upstream-mutator-binding-gloo-system                       ClusterRole/gloo-upstream-mutator-gloo-system                               22h
gloo-upstream-mutator-binding-gloo-system                       ClusterRole/gloo-upstream-mutator-gloo-system                               22h
gloo-upstream-mutator-binding-gloo-system                       ClusterRole/gloo-upstream-mutator-gloo-system                               22h
gloo-upstream-mutator-binding-gloo-system                       ClusterRole/gloo-upstream-mutator-gloo-system                               22h
gloo-upstream-mutator-binding-gloo-system                       ClusterRole/gloo-upstream-mutator-gloo-system                               22h
gloo-upstream-mutator-binding-gloo-system                       ClusterRole/gloo-upstream-mutator-gloo-system                               22h
```

I have a Predicate to log events within the reconciler that trigger reconciliation, and the only event I see is an Update event corresponding to the Patched ClusterRoleBinding after every SSA call:
```
{"time":"2025-04-04T11:05:27.067785829-07:00","level":"DEBUG","msg":"update event","component":"k8s-events","controller":"GatewayController","kind":"*v1.ClusterRoleBinding","ref":{"Namespace":"","Name":"gloo-upstream-mutator-binding-gloo-system"}}
```

#### What did you expect to happen?

I expect repeated SSA calls for the same object to not bump the resourceVersion. 

#### How can we reproduce it (as minimally and precisely as possible)?


```
func TestDbg(t *testing.T) {
	a := assert.New(t)

	yamlToUnstructured := func(yamlData string) (unstructured.Unstructured, error) {
		decoder := yaml.NewYAMLOrJSONDecoder(strings.NewReader(yamlData), 4096)
		var obj unstructured.Unstructured
		err := decoder.Decode(&obj)
		return obj, err
	}

	mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{
		Scheme: scheme.Scheme,
		Client: client.Options{
			Cache: &client.CacheOptions{
				// The Client creates a ListWatch for all resource kinds that are requested with Get().
				// Disable the cache for Objects that can consume massive amounts of memory.
				DisableFor: []client.Object{
					&rbacv1.ClusterRoleBinding{},
				},
			},
		},
	})
	a.NoError(err)
	a.NotNil(mgr)

	c := mgr.GetClient()
	a.NotNil(c)

	r1 := `apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: test-bug
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: test-bug
subjects: []
`
	origObj, err := yamlToUnstructured(r1)
	a.NoError(err)
	a.NotNil(origObj)

	for i := range 5 {
		obj := origObj
		t.Logf("iteration: %d", i)
		obj.SetManagedFields(nil)
		err := c.Patch(context.TODO(), &obj, client.Apply, client.FieldOwner("test"), client.ForceOwnership)
		a.NoError(err)
		out := &rbacv1.ClusterRoleBinding{}
		err = c.Get(context.TODO(), client.ObjectKeyFromObject(&obj), out)
		a.NoError(err)
		t.Logf("resourceVersion: %s", out.GetResourceVersion())
	}
}
```
`resourceVersion` increments:
```
obj_test.go:417: iteration: 0
obj_test.go:424: resourceVersion: 739527
obj_test.go:417: iteration: 1
obj_test.go:424: resourceVersion: 739527
obj_test.go:417: iteration: 2
obj_test.go:424: resourceVersion: 739529
obj_test.go:417: iteration: 3
obj_test.go:424: resourceVersion: 739529
obj_test.go:417: iteration: 4
obj_test.go:424: resourceVersion: 739529
```

With `subjects` not set, `resourceVersion` does not increment:
```
obj_test.go:416: iteration: 0
obj_test.go:423: resourceVersion: 739026
obj_test.go:416: iteration: 1
obj_test.go:423: resourceVersion: 739026
obj_test.go:416: iteration: 2
obj_test.go:423: resourceVersion: 739026
obj_test.go:416: iteration: 3
obj_test.go:423: resourceVersion: 739026
obj_test.go:416: iteration: 4
obj_test.go:423: resourceVersion: 739026
```


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.30.11-eks-bcf3d70
```

</details>


#### Cloud provider

EKS


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该 Issue 描述内容来看，问题的根源在于 Server-Side Apply (SSA) 对同一对象反复执行 Patch 时，导致 resourceVersion 以及 managedFields 中的时间字段被更新，从而触发无限循环的 Reconcile。该问题是一个重复更新逻辑的缺陷或使用方式不当，并不涉及越权访问、敏感信息泄露、命令执行、提权或容器逃逸等安全漏洞。因此，从安全角度判断，这是一个功能/逻辑问题而非安全风险。

**解释说明：**

该 Issue 中描述的问题并不会带来未授权访问、敏感信息泄露或潜在的攻击面扩张，属于项目使用中的逻辑/功能性缺陷，不视为安全漏洞。故风险评级为“不涉及”，无需提供复现脚本。若要解决该问题，可在业务逻辑或 SSA 调用策略上进行优化，确保在没有实际变更时不重复调用 Patch 方法。

---


## Issue #131171 DaemonSets should be scheduled before Deployments on new Nodes

- Issue 链接：[#131171](https://github.com/kubernetes/kubernetes/issues/131171)

### Issue 内容

#### What happened?

We're trying to reduce AWS EC2 costs by reducing the number of nodes to 0 automatically during times the enter cluster is not needed using an autoscaling_schedule (see e.g. https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/autoscaling_schedule). We have observed that, when a single node is started after no nodes at all have been available, the kube-scheduler may start a large number of Deployments on that node, leaving insufficient resources (in our case memory) for the pods of all DaemonSets to be started on that node. That situation was not rectified automatically after adding more nodes to the cluster, instead, I had to cordon the server and terminate some pods on the first node for them to be moved to another node.

It would be preferable if resources for DaemonSets were reserved before Workloads that are not tied to nodes (Deployments, ReplicaSets, ...) are scheduled.



#### What did you expect to happen?

Resources for DaemonSets were reserved before Workloads that are not tied to nodes (Deployments, ReplicaSets, ...) are scheduled.


#### How can we reproduce it (as minimally and precisely as possible)?

1. configure a cluster with 0 worker nodes
2. create daemonset that use a significant amount of resources
3. create Deployments and Replicasets to use all the resources (of at least one type) of one of the worker nodes to be added to the cluster
4. add one worker node
5. sometimes, Deployments and Replicasets are scheduled on the single worker node, leaving too few or no resources for Daemonsets
6. if all Daemonsets were scheduled, remove the worker node and re-add it
7. add more nodes

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据Issue描述，该问题主要与Kubernetes调度机制及资源不足时DaemonSet和Deployment之间的先后调度顺序有关，并非存在潜在的安全漏洞，也未涉及容器逃逸、命令执行、权限提升、或敏感信息泄露等情况。该Issue本质上是一个对Kubernetes调度策略的改进或特性需求，无法用于构成攻击行为或未经授权的操作，不属于安全风险范畴。

**复现过程：**

```python
# 该Issue并非安全问题，因此无需安全相关POC。
# 如需测试Kubernetes调度顺序，可以在本地搭建一个测试集群，
# 将节点数降为0后再新增节点并创建占用大量资源的DaemonSet和Deployment，
# 以重现Issue中描述的情况。
# 下面示例脚本仅示范如何创建资源，不涉及任何安全漏洞利用，
# 不可视为安全测试POC，仅用于演示调度顺序问题。

import time
from kubernetes import client, config

def main():
    # 读取默认kubeconfig
    config.load_kube_config()

    v1_apps = client.AppsV1Api()

    # 示例：创建一个占用较多资源的DaemonSet
    daemonset_manifest = {
        "apiVersion": "apps/v1",
        "kind": "DaemonSet",
        "metadata": {
            "name": "test-daemonset",
            "labels": {
                "app": "test-daemonset"
            }
        },
        "spec": {
            "selector": {
                "matchLabels": {
                    "app": "test-daemonset"
                }
            },
            "template": {
                "metadata": {
                    "labels": {
                        "app": "test-daemonset"
                    }
                },
                "spec": {
                    "containers": [
                        {
                            "name": "test-container",
                            "image": "busybox",
                            "resources": {
                                "requests": {
                                    "memory": "512Mi",
                                    "cpu": "250m"
                                },
                                "limits": {
                                    "memory": "512Mi",
                                    "cpu": "500m"
                                }
                            },
                            "command": ["sh", "-c", "while true; do sleep 10; done"]
                        }
                    ]
                }
            },
            "updateStrategy": {
                "type": "RollingUpdate"
            }
        }
    }

    # 创建DaemonSet
    try:
        v1_apps.create_namespaced_daemon_set(
            namespace="default",
            body=daemonset_manifest
        )
        print("Created DaemonSet: test-daemonset")
    except client.exceptions.ApiException as e:
        print("Error creating DaemonSet:", e)

    # 示例：创建一个消耗大量资源的Deployment
    deployment_manifest = {
        "apiVersion": "apps/v1",
        "kind": "Deployment",
        "metadata": {
            "name": "test-deployment",
            "labels": {
                "app": "test-deployment"
            }
        },
        "spec": {
            "replicas": 3,
            "selector": {
                "matchLabels": {
                    "app": "test-deployment"
                }
            },
            "template": {
                "metadata": {
                    "labels": {
                        "app": "test-deployment"
                    }
                },
                "spec": {
                    "containers": [
                        {
                            "name": "test-container",
                            "image": "busybox",
                            "resources": {
                                "requests": {
                                    "memory": "512Mi",
                                    "cpu": "250m"
                                },
                                "limits": {
                                    "memory": "512Mi",
                                    "cpu": "500m"
                                }
                            },
                            "command": ["sh", "-c", "while true; do sleep 10; done"]
                        }
                    ]
                }
            }
        }
    }

    # 创建Deployment
    try:
        v1_apps.create_namespaced_deployment(
            namespace="default",
            body=deployment_manifest
        )
        print("Created Deployment: test-deployment")
    except client.exceptions.ApiException as e:
        print("Error creating Deployment:", e)

    # 等待片刻让Kubernetes开始调度
    time.sleep(30)

    print("调度演示脚本执行完毕。请在Kubernetes集群中验证DaemonSet和Deployment的实际调度情况。")

main()
```


---


## Issue #131163 Deployment may gets stuck after GracefulNodeShutdown

- Issue 链接：[#131163](https://github.com/kubernetes/kubernetes/issues/131163)

### Issue 内容

#### What happened?

In our cluster, a deployment was created in the d8-system namespace that successfully generated a replicaset and started one pod. Later, a graceful shutdown was executed using shutdown -r -t 5, and after the server restarted, the deployment created a new replicaset and a new pod.

When the deployment reaches its progress deadline (600 seconds by default), it begins deleting pods from the current replicaset. However, logs show that even terminated pods (in an Error or Completed state) from the previous replicaset are still detected. 
As a result (that's my guess), the deployment does nothing for about 10 minutes before creating a new replicaset. If the `Error` pod is manually deleted, the deployment immediately creates a new replicaset. This suggests that the presence of a terminated pod is blocking progress.


Logs:

~13:39 Deployment timed out, then I see that the deployment removes pods from the current RS, then I see messages about “Found related pods” and then for 10 minutes nothing happens. I just see the pods in Error status and nothing else

```
deckhouse-58b8bbfcb6-9shvv         0/1     Error               0          28m
deckhouse-7b7bd6c9bc-mv94c         1/2     Terminating         0          26s
```

```
I0402 13:39:11.545056       1 deployment_util.go:776] "Deployment timed out from last progress check" deployment="d8-system/deckhouse" timeout=false from="2025-04-02 13:38:46 +0000 UTC" now="2025-04-02 13:39:11.544986046 +0000 UTC m=+937.359430398"

I0402 13:39:11.545759       1 replica_set.go:621] "Too many replicas" replicaSet="d8-system/deckhouse-7b7bd6c9bc" need=0 deleting=1

I0402 13:39:11.545809       1 replica_set.go:248] "Found related ReplicaSets" replicaSet="d8-system/deckhouse-7b7bd6c9bc" relatedReplicaSets=["d8-system/deckhouse-55b654774c","d8-system/deckhouse-58b8bbfcb6","d8-system/deckhouse-7b7bd6c9bc"]
I0402 13:39:11.545885       1 replica_set.go:818] "Found related pods" kind="ReplicaSet" replicaSet="d8-system/deckhouse-7b7bd6c9bc" pods=["d8-system/deckhouse-58b8bbfcb6-9shvv","d8-system/deckhouse-7b7bd6c9bc-mv94c"]

```

No further deployment syncing occurs until about 10 minutes later. At 13:48:47, a new syncing event is logged:

I0402 13:48:47.000653       1 deployment_controller.go:594] "Started syncing deployment" deployment="d8-system/deckhouse" startTime="2025-04-02 13:48:47.000520919 +0000 UTC m=+1512.814965267"

This 10-minute pause indicates that the deployment controller is still considering the terminated/error pod as active.


#### What did you expect to happen?

I expect the deployment to immediately attempt to create a new replicas

#### How can we reproduce it (as minimally and precisely as possible)?

- Create a deployment (e.g., deckhouse) in the d8-system namespace with one replica.

- The deployment creates a replicaset and starts one pod.

- Execute a graceful shutdown using `shutdown -r -t 5`

- After the server restarts, observe that the deployment creates a new RS and a new pod.

- When one pod fails (enters an Error/Completed state) and the deployment reaches its 600-second progress deadline, the deployment stalls for about 10 minutes.

Manually delete the terminated/error pod and note that the deployment immediately creates a new replicaset and pod.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.30.10
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.10
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
PRETTY_NAME="Ubuntu 22.04.5 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

Linux zykov-master-0 5.15.0-131-generic #141-Ubuntu SMP Fri Jan 10 21:18:28 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue内容来看，该问题主要是因为Kubernetes Deployment在节点重启后，遇到已经处于Error或Completed状态的旧Pod，导致Deployment控制器在后续调度时陷入约10分钟的等待期。该现象本质上是Deployment控制器对旧副本集和旧Pod状态处理出现的逻辑延迟，阻塞后续ReplicaSet的创建。  
通篇并未体现出信息泄露、命令执行、提权、容器逃逸等可被恶意利用的安全风险，也不涉及凭据泄露或者非预期的多用户环境下的权限滥用。因此，该问题是一个正常的功能或可靠性Bug，不涉及安全漏洞场景。

**复现过程：**

```python
# 该Issue并非安全问题，因此无需编写安全漏洞复现脚本。
# 如需演示Issue现象，可使用Python调用kubernetes客户端创建Deployment并手动重启节点，
# 观察Deployment对象在Pod状态为Error/Completed时的调度逻辑，但这并非安全风险POC示例。
import time
from kubernetes import client, config
from kubernetes.client.rest import ApiException

def main():
    """
    演示：部署一个Deployment后，模拟节点重启，观察Deployment状态卡住的现象。
    仅供功能测试，不涉及安全风险。
    """

    # 假设已在本地配置 KUBECONFIG
    config.load_kube_config()

    apps_v1 = client.AppsV1Api()
    core_v1 = client.CoreV1Api()

    namespace = "d8-system"
    deployment_name = "demo-deployment"

    # 1. 创建一个最小化的 Deployment
    deployment_manifest = {
        "metadata": {"name": deployment_name},
        "spec": {
            "replicas": 1,
            "selector": {"matchLabels": {"app": "demo"}},
            "template": {
                "metadata": {"labels": {"app": "demo"}},
                "spec": {
                    "containers": [{
                        "name": "nginx",
                        "image": "nginx:latest",
                    }]
                },
            },
        },
    }

    try:
        print("创建Deployment...")
        apps_v1.create_namespaced_deployment(
            body=deployment_manifest,
            namespace=namespace
        )
    except ApiException as e:
        print(f"创建Deployment失败: {e}")

    print("Deployment已创建，等待数秒后模拟节点重启并查看Deployment状态...")
    time.sleep(30)

    # 此处仅打印Deployment状态，实际环境中需要在外部执行节点重启操作
    # 然后等待Pod进入Error/Completed并重新启动观察
    try:
        dep = apps_v1.read_namespaced_deployment(name=deployment_name, namespace=namespace)
        print(f"Deployment状态: {dep.status}")
    except ApiException as e:
        print(f"读取Deployment状态失败: {e}")

    print("脚本结束，若需重现Issue，请手动进行节点重启并观察Deployment在Error Pod状态下的调度逻辑。")
```


**解释说明：**

1. 此脚本仅演示如何创建一个最小化Deployment，并提示在实际环境中进行节点重启后，观察Deployment在Pod处于Error/Completed状态时的表现。  
2. 由于本Issue不是安全风险，不会造成可被利用的漏洞，也不涉及高风险或低风险的安全问题，因此不需要编写专门的安全POC脚本。  
3. 该示例通过kubernetes客户端库调用集群API完成对象创建，无需外部命令或凭据明文。脚本执行后可在2分钟内正常结束。  
4. 本示例仅供对部署卡住现象进行功能性验证，不涉及任何安全漏洞的复现。

---


## Issue #131152 Same RWO PVC ends up on different nodes

- Issue 链接：[#131152](https://github.com/kubernetes/kubernetes/issues/131152)

### Issue 内容

#### What happened?

I applied the following configuration that has two pods using the same RWO PVC.

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: vol1
  labels:
    type: local
spec:
  storageClassName: manual
  capacity:
    storage: 10Mi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim1
spec:
  storageClassName: manual
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Mi
---
apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: data
      resources:
        limits:
          cpu: "1"
          memory: "200Mi"
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: claim1
---
apiVersion: v1
kind: Pod
metadata:
  name: pod2
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: data
      resources:
        limits:
          cpu: "1"
          memory: "200Mi"
  volumes:
    - name: data
      persistentVolumeClaim:
        claimName: claim1

```

According to #103305 and #26567, pods could be scheduled on different nodes, and actually were. But, contrary to these issues, both pods were able to attach to the volume (s?) and run just fine.

It really seems that the PVC "used" both nodes because data written by both pods were present on each corresponding node, and this happened **without any notice**.

Is this expected behavior?



#### What did you expect to happen?

Volume attachment should fail for at least one of the two pods.

#### How can we reproduce it (as minimally and precisely as possible)?

By applying the provided configuration.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.30.4+rke2r1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.5 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux ubuntu-nic 6.8.0-57-generic #59~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Mar 19 17:07:41 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述了一个使用了 ReadWriteOnce (RWO) 类型 PVC 的配置，但当两个 Pod 分别被调度到不同节点时，它们都能够正常写入本地目录并运行。该问题更多是由于对 Kubernetes RWO 语义或底层 Volume/StorageClass 行为的困惑或不当使用导致的资源状态不一致或可能的数据同步问题，并不涉及到敏感信息泄露、权限提升、任意代码执行、容器逃逸等安全风险场景。

从描述看，该行为并没有直接造成越权访问、拒绝服务或其他可以被利用的安全威胁，且未涉及到敏感数据泄露，因此不属于安全漏洞范畴，应将风险评级判定为“不涉及”。

**解释说明：**

此 Issue 仅反映了在使用 RWO PVC 时出现的配置或使用问题，其风险不在安全层面，而在数据一致性和预期使用语义方面，不构成安全漏洞。故不需要提供安全复现脚本。

---


## Issue #131147 Duplicate value error when editing a port/protocol with Server Side Apply

- Issue 链接：[#131147](https://github.com/kubernetes/kubernetes/issues/131147)

### Issue 内容

#### What happened?

When using `kubectl apply --server-side` on a deployment or service yaml file to edit a port number, it will result in this error:
`
The Service "test-service" is invalid: spec.ports[1].name: Duplicate value: "http"
`
The reason why we see 'duplicate' error when modifying port number is due to the protocol and port number being a composite key in managed fields:

```
managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      'f:spec':
        'f:ports':
          'k:{"port":1935,"protocol":"TCP"}':  # Port is part of the key
            .: {}
            'f:name': {}
            'f:port': {}
            'f:targetPort': {}
```
It uses a composite key {"port":1935,"protocol":"TCP"} to identify unique ports
When you try to change the port number while keeping the name the same, SSA sees this as a conflict because:
The old port entry is still managed by the previous apply
The new port entry with the same name but different port number is trying to be applied
SSA treats this as a duplicate port name conflict

#### What did you expect to happen?

server side applies without errors. The service/deployment reflects the edited port number.

#### How can we reproduce it (as minimally and precisely as possible)?

1.
```
cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Service
metadata:
  name: test-service
  namespace: default
spec:
  ports:
    - name: http
      port: 80
      targetPort: 80
EOF
service/test-service created
```

2.
```
cat <<EOF | kubectl apply --server-side -f -
apiVersion: v1
kind: Service
metadata:
  name: test-service
  namespace: default
spec:
  ports:
    - name: http
      port: 8080  # Changed from 80 to 8080
      targetPort: 8080
EOF
The Service "test-service" is invalid: spec.ports[1].name: Duplicate value: "http"
```

#### Anything else we need to know?

This was originally found as part of an ArgoCD issue: (https://github.com/argoproj/argo-cd/issues/17717)
The issue uses a feature called Server Side Diff which depends on `kubectl apply --server-side [--dry-run=server]`

#### Kubernetes version

<details>

```console
kubectl version
Client Version: v1.32.1
Kustomize Version: v5.5.0
Server Version: v1.32.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
uname -a
Darwin macos-JK700GFY27 23.6.0 Darwin Kernel Version 23.6.0: Wed Jul 31 20:50:00 PDT 2024; root:xnu-10063.141.1.700.5~1/RELEASE_ARM64_T6031 arm64ersion, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue描述来看，这是一次在Kubernetes中使用Server Side Apply时出现的配置/使用冲突，根本原因是服务端在解析Service配置时把"port"和"protocol"作为组合键，导致在修改端口号但保留相同名称时触发了“Duplicate value”校验错误。  
该问题仅涉及Server Side Apply机制下的资源管理字段冲突，并未透露任何异常敏感信息或允许攻击者利用的安全漏洞，也没有涉及提权、执行任意命令、信息泄露或其他高危操作，因此不属于安全风险。

**解释说明：**

该问题仅为Kubernetes资源管理的使用或逻辑冲突，与安全无关，因此不存在安全风险，也无需额外的安全测试脚本来复现。  
如需在开发或测试环境中模拟此报错，可直接在Kubernetes集群中用kubectl或kubernetes客户端库进行相同操作，即先创建一个Service，然后通过Server Side Apply修改port值即可复现。  
对项目本身而言，只需参考官方文档或Issue讨论，利用正确的操作方式或资源配置来避免该冲突。

---


## Issue #131134 Potential incorrect RealServer weight update logic in IPVS proxier initial sync

- Issue 链接：[#131134](https://github.com/kubernetes/kubernetes/issues/131134)

### Issue 内容

#### What happened?

In the `syncProxyRules` logic of kube-proxy's IPVS mode, during the initial sync phase, the weight of each RealServer is updated if **any** of the existing destinations (`curDests`) has a different weight from the newly created `newDest`. This logic may lead to unnecessary or even incorrect updates.

##### Where the code is:
File: `pkg/proxy/ipvs/proxier.go`  
Function: `syncProxyRules`  
Related snippet:
```go
for _, dest := range curDests {
    if dest.Weight != newDest.Weight {
        err = proxier.ipvs.UpdateRealServer(appliedVirtualServer, newDest)
    }
}
```

##### Why it's problematic:
The current code updates `newDest` without checking if the existing `dest` in `curDests` actually matches the IP and port of `newDest`. This can result in:
- Unnecessary repeated updates to the same RealServer.
- Updates triggered by unrelated `dest` entries.
- Inefficient syncing behavior during startup.



#### What did you expect to happen?

The logic should compare the `Address` and `Port` of `dest` and `newDest`, and only perform an update if:
```go
dest.Address.Equal(newDest.Address) && dest.Port == newDest.Port && dest.Weight != newDest.Weight
```

This ensures only the correct RealServer is updated when needed.

#### How can we reproduce it (as minimally and precisely as possible)?

This logic runs during the initial sync phase. In environments with multiple RealServers with varying weights, the current code may repeatedly trigger `UpdateRealServer` on unrelated destinations.

#### Anything else we need to know?

The logic should compare the `Address` and `Port` of `dest` and `newDest`, and only perform an update if:
```go
dest.Address.Equal(newDest.Address) && dest.Port == newDest.Port && dest.Weight != newDest.Weight
```

This ensures only the correct RealServer is updated when needed.

#### Kubernetes version

- Kubernetes version: [ v1.29.0]
- IPVS mode enabled


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该 Issue 的描述来看，问题主要是 IPVS 模式下的 kube-proxy 在初始同步环节可能错误地比较 RealServer 的 weight，从而导致不必要或错误的更新操作。该问题更多地影响到系统在启动时的同步效率，容易出现重复或无关的 RealServer 更新请求，但从描述中并未体现出可被攻击者利用以发起拒绝服务攻击、提升权限、执行命令或其他安全后果的可能性。该问题更像是功能/性能逻辑层面的缺陷，而非安全漏洞，因此不涉及实际的安全风险。

**复现过程：**

```python
# 本Issue并未描述安全风险，因此无需提供针对安全漏洞的复现脚本。
# 若仅需验证重复或无关的 RealServer 更新，可在测试环境中部署
# 对应版本的kube-proxy并观察其日志与更新行为。
print("当前Issue未涉及安全风险，无需复现POC")
```


---


## Issue #131133 Duplicate "Transfer-Encoding: chunked" header

- Issue 链接：[#131133](https://github.com/kubernetes/kubernetes/issues/131133)

### Issue 内容

#### What happened?

After upgrading ngress-nginx-controller to v1.11.5 and kube webhook version v20231226-1a7112e06, getting- upstream sent duplicate header line: "Transfer-Encoding: chunked", previous value: "Transfer-Encoding: chunked" while reading response header from upstream.

#### What did you expect to happen?

From our gui, when i am trying to hit search api, but everytime i hit it i am getting upstream sent duplicate header line: "Transfer-Encoding: chunked", previous value: "Transfer-Encoding: chunked" while reading response header from upstream and 502 as response. If i roll back to ingress controller version v1.9.4 its working fine.

#### How can we reproduce it (as minimally and precisely as possible)?

I have upgraded from v1.9.4 to v.11.5 and kubehook from v20231011-8b53cabe0 to v20231226-1a7112e06, than i am getting this

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.30.3
```

</details>


#### Cloud provider

<details>
Azure
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从问题描述来看，产生“Transfer-Encoding: chunked”头被重复写入，是由于升级到指定版本后代理或后端服务在响应头中重复添加了该字段，导致 502 错误。该问题主要是请求-响应处理上的配置或兼容性Bug，并未给出可利用的安全漏洞场景，也不涉及凭据泄露、代码执行、容器逃逸、提权等安全风险要素，更不涉及因多重头导致的请求拆分攻击（HTTP Request Smuggling）等高级安全风险。因此，本Issue并不构成安全风险。

**复现过程：**

```python
# 本Issue不涉及安全风险，因此无需提供复现脚本
```


---


