# Issue å®‰å…¨åˆ†ææŠ¥å‘Š

# ğŸš¨ å­˜åœ¨é«˜é£é™©çš„ Issues (4 ä¸ª)

## Issue #129907 Pods with postStart lifecycle hook are stuck in Pending state.

- Issue é“¾æ¥ï¼š[#129907](https://github.com/kubernetes/kubernetes/issues/129907)

### Issue å†…å®¹

#### What happened?

Upon kubernetes control plane v1.32 upgrade pods with **postStart** lifecycle hook configured are unable to start.
Pod is failing to report its state and remains in **Pending** state forever.
Also it's impossible to path or update deployment like change replicas number or image tag, not possible to delete pod without --force flag.
But it allows to update deployment to remove **lifecycle** section.

#### What did you expect to happen?

Pod should start normally, lifecycle hook must be processed.

#### How can we reproduce it (as minimally and precisely as possible)?

Tested on v1.32.0 (v1.32.0-gke.1448000, v1.32.0-eks-5ca49cb) and v1.32.1 (v1.32.1-gke.1002000)

Create deployment on cluster v1.32, postStart lifecycle hook must be defined as below:
```
lifecycle:
  postStart:
    sleep:
      seconds: 15
```

#### Anything else we need to know?

Some details.

1. No issues with PreStop hooks.

2. Deployment definition used to reproduce the issue:

```
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: inflate
  labels:
    app: inflate
automountServiceAccountToken: false


---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inflate
spec:
  replicas: 1
  selector:
    matchLabels:
      app: inflate
  template:
    metadata:
      labels:
        app: inflate
    spec:
      serviceAccountName: inflate
      containers:
      - args:
          - infinity
        command:
          - sleep
        image: python:3
        name: inflate
        lifecycle:
          postStart:
            sleep:
              seconds: 15
```

3. Error on deployment scaling attempt:
```
$ kubectl scale deployment inflate --replicas 2
Unable to connect to the server: stream error: stream ID 3; INTERNAL_ERROR; received from peer
```

4. Error on editing deployment (image tag change):
```
$ kubectl edit deployments inflate
error: deployments.apps "inflate" could not be patched: Patch "https://<edited>/apis/apps/v1/namespaces/default/deployments/inflate?fieldManager=kubectl-edit&fieldValidation=Strict": stream error: stream ID 5; INTERNAL_ERROR; received from peer
You can run `kubectl replace -f /var/folders/6b/qz1j7hs94kb8j238fb80vf800000gn/T/kubectl-edit-2502749312.yaml` to try this update again.
```

5. Apiserver (EKS) log on similar event:
```
E0129 14:29:50.819453      11 wrap.go:57] "apiserver panic'd" method="PATCH" URI="/apis/apps/v1/namespaces/<edited>/replicasets/<edited>" auditID="d7f993f8-ef27-4544-982d-c8913890fcba"
http2: panic serving 172.16.42.188:34518: runtime error: invalid memory address or nil pointer dereference
goroutine 6006616 [running]:
k8s.io/apiserver/pkg/endpoints/handlers/finisher.finishRequest.func1.1()
	k8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:105 +0xa5
panic({0x3201e40?, 0x5cd2e70?})
	runtime/panic.go:785 +0x132
k8s.io/kubernetes/pkg/api/pod.podLifecycleSleepActionZeroValueInUse.func1(0x0?, 0x0?)
	k8s.io/kubernetes/pkg/api/pod/util.go:775 +0x42
k8s.io/kubernetes/pkg/api/pod.VisitContainers(0xc037a98688, 0x7, 0xc03c12cfe8)
	k8s.io/kubernetes/pkg/api/pod/util.go:73 +0x11e
k8s.io/kubernetes/pkg/api/pod.podLifecycleSleepActionZeroValueInUse(0xc037a61888?)
	k8s.io/kubernetes/pkg/api/pod/util.go:767 +0x3d
k8s.io/kubernetes/pkg/api/pod.GetValidationOptionsFromPodSpecAndMeta(0xc037a98688, 0xc037a61888, 0xc03d1b5a10?, 0xc037a617a0)
	k8s.io/kubernetes/pkg/api/pod/util.go:418 +0x1fe
k8s.io/kubernetes/pkg/api/pod.GetValidationOptionsFromPodTemplate(0x0?, 0xedf170fc1?)
	k8s.io/kubernetes/pkg/api/pod/util.go:551 +0x57
k8s.io/kubernetes/pkg/registry/apps/replicaset.rsStrategy.ValidateUpdate({{0x0?, 0x3773560?}, {0xc037a61688?, 0xc037a98488?}}, {0x3df1580, 0xc041182ab0}, {0x3dcd7d0?, 0xc037a98488}, {0x3dcd7d0, 0xc037a61688})
	k8s.io/kubernetes/pkg/registry/apps/replicaset/strategy.go:146 +0x95
k8s.io/apiserver/pkg/registry/rest.BeforeUpdate({0x3e0a9c0, 0xc006ec32c0}, {0x3df1580, 0xc041182ab0}, {0x3dcd7d0, 0xc037a98488}, {0x3dcd7d0, 0xc037a61688})
	k8s.io/apiserver/pkg/registry/rest/update.go:154 +0x48f
k8s.io/apiserver/pkg/registry/generic/registry.(*Store).Update.func1({0x3dcd7d0, 0xc037a61688}, {0xc037e933b0?, 0x379a6f8?})
	k8s.io/apiserver/pkg/registry/generic/registry/store.go:750 +0x585
k8s.io/apiserver/pkg/storage/etcd3.(*store).updateState(0xc006f86370, 0xc02de24400?, 0x34?)
	k8s.io/apiserver/pkg/storage/etcd3/store.go:1072 +0x3a
k8s.io/apiserver/pkg/storage/etcd3.(*store).GuaranteedUpdate(0xc006f86370, {0x3df1580, 0xc041182ab0}, {0xc037a22b70, 0x2b}, {0x3dcd7d0, 0xc037a61208}, 0x1, 0xc0261e5960, 0xc0377ca370, ...)
	k8s.io/apiserver/pkg/storage/etcd3/store.go:501 +0x985
k8s.io/apiserver/pkg/storage/cacher.(*Cacher).GuaranteedUpdate(0xc006bf6ea0, {0x3df1580, 0xc041182ab0}, {0xc037a22b70, 0x2b}, {0x3dcd7d0, 0xc037a61208}, 0x1, 0xc0261e5960, 0xc0377ca370, ...)
	k8s.io/apiserver/pkg/storage/cacher/cacher.go:981 +0x127
k8s.io/apiserver/pkg/registry/generic/registry.(*DryRunnableStorage).GuaranteedUpdate(0xc034e05d70?, {0x3df1580?, 0xc041182ab0?}, {0xc037a22b70?, 0xb?}, {0x3dcd7d0?, 0xc037a61208?}, 0x50?, 0x2?, 0xc0377ca370, ...)
	k8s.io/apiserver/pkg/registry/generic/registry/dryrun.go:107 +0x177
k8s.io/apiserver/pkg/registry/generic/registry.(*Store).Update(0xc006e9f680, {0x3df1580, 0xc041182ab0}, {0xc0470f861b, 0x18}, {0x3dc55d8, 0xc041182c90}, 0xc037a365f0, 0xc02b0abb80, 0x0, ...)
	k8s.io/apiserver/pkg/registry/generic/registry/store.go:638 +0x415
k8s.io/apiserver/pkg/endpoints/handlers.(*patcher).patchResource.func2()
	k8s.io/apiserver/pkg/endpoints/handlers/patch.go:704 +0xa7
k8s.io/apiserver/pkg/endpoints/handlers.(*patcher).patchResource.func3()
	k8s.io/apiserver/pkg/endpoints/handlers/patch.go:710 +0x35
k8s.io/apiserver/pkg/endpoints/handlers/finisher.finishRequest.func1()
	k8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:117 +0x75
created by k8s.io/apiserver/pkg/endpoints/handlers/finisher.finishRequest in goroutine 6006615
	k8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:92 +0xc8

goroutine 6006615 [running]:
k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP.func1.1()
	k8s.io/apiserver/pkg/server/filters/timeout.go:110 +0xb0
panic({0x302b360?, 0xc021473740?})
	runtime/panic.go:785 +0x132
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithAudit.func6.1.deferwrap1()
	k8s.io/apiserver/pkg/endpoints/filters/audit.go:84 +0x25
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithAudit.func6.1()
	k8s.io/apiserver/pkg/endpoints/filters/audit.go:93 +0x223
panic({0x302b360?, 0xc021473740?})
	runtime/panic.go:785 +0x132
k8s.io/apiserver/pkg/endpoints/handlers/finisher.(*result).Return(0xc03b913128?)
	k8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:53 +0xad
k8s.io/apiserver/pkg/endpoints/handlers/finisher.finishRequest({0x3df1580, 0xc041182ab0}, 0xc0291b50e0, 0x45d964b800, 0x39a76a0)
	k8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:126 +0x247
k8s.io/apiserver/pkg/endpoints/handlers/finisher.FinishRequest(...)
	k8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:84
k8s.io/apiserver/pkg/endpoints/handlers.(*patcher).patchResource(0xc028766960, {0x3df1580, 0xc041182ab0}, 0xc00788b6c0)
	k8s.io/apiserver/pkg/endpoints/handlers/patch.go:708 +0xa65
k8s.io/apiserver/pkg/endpoints/handlers.PatchResource.func1({0x3de3c30, 0xc0291b5020}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/handlers/patch.go:247 +0x2858
k8s.io/apiserver/pkg/endpoints.(*APIInstaller).registerResourceHandlers.restfulPatchResource.func12(0xc0291b5000, 0xc03a0cca10)
	k8s.io/apiserver/pkg/endpoints/installer.go:1329 +0x6a
k8s.io/apiserver/pkg/endpoints.(*APIInstaller).registerResourceHandlers.InstrumentRouteFunc.func13(0xc0291b5000, 0xc03a0cca10)
	k8s.io/apiserver/pkg/endpoints/metrics/metrics.go:627 +0x1ce
github.com/emicklei/go-restful/v3.(*Container).dispatch(0xc0018578c0, {0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	github.com/emicklei/go-restful/v3@v3.11.0/container.go:299 +0x9d7
github.com/emicklei/go-restful/v3.(*Container).Dispatch(...)
	github.com/emicklei/go-restful/v3@v3.11.0/container.go:204
k8s.io/apiserver/pkg/server.director.ServeHTTP({{0x3784546?, 0x12?}, 0xc0018578c0?, 0xc00193b570?}, {0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/server/handler.go:145 +0x56c
k8s.io/kube-aggregator/pkg/apiserver.(*proxyHandler).ServeHTTP(0xc0098a4c70?, {0x3de3c30?, 0xc0291b4cc0?}, 0x43?)
	k8s.io/kube-aggregator/pkg/apiserver/handler_proxy.go:118 +0x25d
k8s.io/apiserver/pkg/server/mux.(*pathHandler).ServeHTTP(0xc00888dd40, {0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/server/mux/pathrecorder.go:251 +0x410
k8s.io/apiserver/pkg/server/mux.(*PathRecorderMux).ServeHTTP(0xc0470f85f0?, {0x3de3c30?, 0xc0291b4cc0?}, 0x410965?)
	k8s.io/apiserver/pkg/server/mux/pathrecorder.go:237 +0x66
k8s.io/apiserver/pkg/server.director.ServeHTTP({{0x3786997?, 0xc03b9145a0?}, 0xc005f29cb0?, 0xc007a8f260?}, {0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/server/handler.go:153 +0x6f1
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func22({0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:110 +0x162
net/http.HandlerFunc.ServeHTTP(0x3df1580?, {0x3de3c30?, 0xc0291b4cc0?}, 0x4?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/endpoints/filters.withAuthorization.func1({0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filters/authorization.go:83 +0x606
net/http.HandlerFunc.ServeHTTP(0xc032d92958?, {0x3de3c30?, 0xc0291b4cc0?}, 0x0?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1({0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x192
net/http.HandlerFunc.ServeHTTP(0x1dc5fef?, {0x3de3c30?, 0xc0291b4cc0?}, 0x3d9d730?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func23({0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:110 +0x162
net/http.HandlerFunc.ServeHTTP(0xc02b0ab940?, {0x3de3c30?, 0xc0291b4cc0?}, 0x1ec91b0?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle.func10()
	k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:298 +0xd3
k8s.io/apiserver/pkg/util/flowcontrol.(*configController).Handle.func2()
	k8s.io/apiserver/pkg/util/flowcontrol/apf_filter.go:192 +0x22a
k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset.(*request).Finish.func1(0xc0377b1680?, 0x70?, 0x35e6de0?)
	k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go:391 +0x47
k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset.(*request).Finish(0xc0377b1680, 0xc03a0cc8c0)
	k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go:392 +0x39
k8s.io/apiserver/pkg/util/flowcontrol.(*configController).Handle(0xc000240640, {0x3df1628, 0xc03a0cc4d0}, {0xc01a7c9790, {0x3df13d8, 0xc02b0ab8c0}}, 0x0?, 0x0?, 0xc02b0ab900?, 0xc02b0ab900)
	k8s.io/apiserver/pkg/util/flowcontrol/apf_filter.go:179 +0x74b
k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle.func11(0xc007a33090, {0x3df1580?, 0xc041182660?}, {0xc01a7c9790?, {0x3df13d8?, 0xc02b0ab8c0?}}, 0xc0291b4d20, 0xc0411826f0, 0xc0261e54c0, 0xc02b0ab900)
	k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:304 +0xeb
k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle(0xc007a33090, {0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:305 +0x9f6
net/http.HandlerFunc.ServeHTTP(0xc032d93100?, {0x3de3c30?, 0xc0291b4cc0?}, 0x0?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1({0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x192
net/http.HandlerFunc.ServeHTTP(0x1dc5fef?, {0x3de3c30?, 0xc0291b4cc0?}, 0x3d9d730?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func24({0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:110 +0x162
net/http.HandlerFunc.ServeHTTP(0xc0411822d0?, {0x3de3c30?, 0xc0291b4cc0?}, 0xc0470f85fb?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithImpersonation.func4({0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filters/impersonation.go:50 +0x1b0
net/http.HandlerFunc.ServeHTTP(0xc032d93680?, {0x3de3c30?, 0xc0291b4cc0?}, 0x0?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1({0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x192
net/http.HandlerFunc.ServeHTTP(0x1dc5fef?, {0x3de3c30?, 0xc0291b4cc0?}, 0x3d9d730?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func25({0x3de3c30, 0xc0291b4cc0}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:110 +0x162
net/http.HandlerFunc.ServeHTTP(0xc0012eee60?, {0x3de3c30?, 0xc0291b4cc0?}, 0x3df1580?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithAudit.func6({0x3de3c30, 0xc0291b4c80}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filters/audit.go:115 +0x395
net/http.HandlerFunc.ServeHTTP(0xc032d93a10?, {0x3de3c30?, 0xc0291b4c80?}, 0x0?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1({0x3de3c30, 0xc0291b4c80}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x192
net/http.HandlerFunc.ServeHTTP(0x1dc5fef?, {0x3de3c30?, 0xc0291b4c80?}, 0x3d9d730?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func27({0x3de3c30, 0xc0291b4c80}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:110 +0x162
net/http.HandlerFunc.ServeHTTP(0x3df1580?, {0x3de3c30?, 0xc0291b4c80?}, 0x3d972c0?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/endpoints/filters.withAuthentication.func1({0x3de3c30, 0xc0291b4c80}, 0xc037a4b540)
	k8s.io/apiserver/pkg/endpoints/filters/authentication.go:123 +0x7bf
net/http.HandlerFunc.ServeHTTP(0x3df1580?, {0x3de3c30?, 0xc0291b4c80?}, 0x3d9d730?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1({0x3de3c30, 0xc0291b4c80}, 0xc037a4b180)
	k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:94 +0x354
net/http.HandlerFunc.ServeHTTP(0xc037a4b040?, {0x3de3c30?, 0xc0291b4c80?}, 0xc037a36500?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithWarningRecorder.func11({0x3de3c30, 0xc0291b4c80}, 0xc037a4b040)
	k8s.io/apiserver/pkg/endpoints/filters/warning.go:35 +0xb9
net/http.HandlerFunc.ServeHTTP(0xc00c621f48?, {0x3de3c30?, 0xc0291b4c80?}, 0xb424ec?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP.func1()
	k8s.io/apiserver/pkg/server/filters/timeout.go:115 +0x5b
created by k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP in goroutine 6006574
	k8s.io/apiserver/pkg/server/filters/timeout.go:101 +0x198

goroutine 6006574 [running]:
golang.org/x/net/http2.(*serverConn).runHandler.func1()
	golang.org/x/net@v0.30.0/http2/server.go:2432 +0x145
panic({0x302b360?, 0xc021473820?})
	runtime/panic.go:785 +0x132
k8s.io/apimachinery/pkg/util/runtime.handleCrash({0x3df0fa0, 0x5d564c0}, {0x302b360, 0xc021473820}, {0xc0293fca70, 0x1, 0xc03c6a7360?})
	k8s.io/apimachinery/pkg/util/runtime/runtime.go:89 +0xee
k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0xc03c6a73a8, 0x1, 0xc027cb3180?})
	k8s.io/apimachinery/pkg/util/runtime/runtime.go:59 +0x108
panic({0x302b360?, 0xc021473820?})
	runtime/panic.go:785 +0x132
k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP(0xc0065b6d80, {0x3de3c30, 0xc0291b4c00}, 0xdf8475800?)
	k8s.io/apiserver/pkg/server/filters/timeout.go:121 +0x32f
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestDeadline.withRequestDeadline.func28({0x3de3c30, 0xc0291b4c00}, 0xc037a4adc0)
	k8s.io/apiserver/pkg/endpoints/filters/request_deadline.go:100 +0x234
net/http.HandlerFunc.ServeHTTP(0x3df1580?, {0x3de3c30?, 0xc0291b4c00?}, 0x7f8055d9c5b8?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithWaitGroup.withWaitGroup.func29({0x3de3c30, 0xc0291b4c00}, 0xc037a4adc0)
	k8s.io/apiserver/pkg/server/filters/waitgroup.go:86 +0x17d
net/http.HandlerFunc.ServeHTTP(0xc00178f100?, {0x3de3c30?, 0xc0291b4c00?}, 0x3782b82?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server/filters.(*goaway).ServeHTTP(0xc005daf720, {0x3de3c30, 0xc0291b4c00}, 0xc037a4adc0)
	k8s.io/apiserver/pkg/server/filters/goaway.go:72 +0x142
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithCacheControl.func14({0x3de3c30, 0xc0291b4c00}, 0xc037a4adc0)
	k8s.io/apiserver/pkg/endpoints/filters/cachecontrol.go:31 +0xa7
net/http.HandlerFunc.ServeHTTP(0x1?, {0x3de3c30?, 0xc0291b4c00?}, 0x410965?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithHTTPLogging.WithLogging.withLogging.func35({0x3de3c30, 0xc0291b4c00}, 0xc037a4adc0)
	k8s.io/apiserver/pkg/server/httplog/httplog.go:112 +0x95
net/http.HandlerFunc.ServeHTTP(0x3df1698?, {0x3de3c30?, 0xc0291b4c00?}, 0x30?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithLatencyTrackers.func16({0x3de3ea0, 0xc01b9dfa40}, 0xc037a4ac80)
	k8s.io/apiserver/pkg/endpoints/filters/webhook_duration.go:56 +0x143
net/http.HandlerFunc.ServeHTTP(0xc037a4ab40?, {0x3de3ea0?, 0xc01b9dfa40?}, 0xc037a4ab40?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestInfo.func18({0x3de3ea0, 0xc01b9dfa40}, 0xc037a4ab40)
	k8s.io/apiserver/pkg/endpoints/filters/requestinfo.go:39 +0x119
net/http.HandlerFunc.ServeHTTP(0xc037a4aa00?, {0x3de3ea0?, 0xc01b9dfa40?}, 0xef2dbd6e4f3?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestReceivedTimestamp.withRequestReceivedTimestampWithClock.func32({0x3de3ea0, 0xc01b9dfa40}, 0xc037a4aa00)
	k8s.io/apiserver/pkg/endpoints/filters/request_received_time.go:38 +0xaf
net/http.HandlerFunc.ServeHTTP(0x30?, {0x3de3ea0?, 0xc01b9dfa40?}, 0xc0000a2e08?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithMuxAndDiscoveryComplete.func19({0x3de3ea0?, 0xc01b9dfa40?}, 0xc037a4aa00?)
	k8s.io/apiserver/pkg/endpoints/filters/mux_discovery_complete.go:52 +0xd5
net/http.HandlerFunc.ServeHTTP(0xed50e219a1f68e5c?, {0x3de3ea0?, 0xc01b9dfa40?}, 0x410965?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithPanicRecovery.withPanicRecovery.func33({0x3de3ea0, 0xc01b9dfa40}, 0xc037a4aa00)
	k8s.io/apiserver/pkg/server/filters/wrap.go:73 +0xdc
net/http.HandlerFunc.ServeHTTP(0xc040891200?, {0x3de3ea0?, 0xc01b9dfa40?}, 0xc037a22ab0?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithAuditInit.withAuditInit.func34({0x3de3ea0, 0xc01b9dfa40}, 0xc0379e2a00)
	k8s.io/apiserver/pkg/endpoints/filters/audit_init.go:63 +0x135
net/http.HandlerFunc.ServeHTTP(0xc0003a4ac0?, {0x3de3ea0?, 0xc01b9dfa40?}, 0xc00043e330?)
	net/http/server.go:2220 +0x29
k8s.io/apiserver/pkg/server.(*APIServerHandler).ServeHTTP(0xc00c621d40?, {0x3de3ea0?, 0xc01b9dfa40?}, 0xc0408fb950?)
	k8s.io/apiserver/pkg/server/handler.go:188 +0x25
net/http.serverHandler.ServeHTTP({0xc0474c3b60?}, {0x3de3ea0?, 0xc01b9dfa40?}, 0x3deb700?)
	net/http/server.go:3210 +0x8e
net/http.initALPNRequest.ServeHTTP({{0x3df1580?, 0xc0321de9c0?}, 0xc03799c388?, {0xc007a94960?}}, {0x3de3ea0, 0xc01b9dfa40}, 0xc0379e2a00)
	net/http/server.go:3819 +0x231
golang.org/x/net/http2.(*serverConn).runHandler(0x4469db?, 0xc032f50260?, 0x3db0780?, 0x5cd2c90?)
	golang.org/x/net@v0.30.0/http2/server.go:2439 +0xf5
created by golang.org/x/net/http2.(*serverConn).scheduleHandler in goroutine 2753346
	golang.org/x/net@v0.30.0/http2/server.go:2373 +0x21d
```

#### Kubernetes version

<details>

```console
Client Version: v1.32.1
Kustomize Version: v5.5.0
Server Version: v1.32.0-gke.1448000
```

```console
Client Version: v1.32.1
Kustomize Version: v5.5.0
Server Version: v1.32.1-gke.1002000
```

```console
Client Version: v1.32.1
Kustomize Version: v5.5.0
Server Version: v1.32.0-eks-5ca49cb
```

</details>


#### Cloud provider

<details>

AWS, GCP

</details>


#### OS version

<details>

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ issue ä¸»è¦æè¿°åœ¨ Kubernetes v1.32 å‡çº§åï¼Œå½“ pod é…ç½®äº† postStart ç”Ÿå‘½å‘¨æœŸé’©å­æ—¶ï¼Œpod æ— æ³•æ­£å¸¸å¯åŠ¨ï¼Œå¹¶å¯¼è‡´ä¸€ç³»åˆ—æ“ä½œï¼ˆå¦‚æ‰©å®¹ã€ä¿®æ”¹é•œåƒç­‰ï¼‰å‡ºç°é”™è¯¯å’Œå†…éƒ¨ panicã€‚é€šè¿‡é”™è¯¯æ—¥å¿—å¯ä»¥çœ‹åˆ°ï¼Œapiserver åœ¨å¤„ç† PATCH æ“ä½œæ—¶å› å¯¹ postStart é’©å­è¿›è¡ŒéªŒè¯æ—¶äº§ç”Ÿäº† nil pointer dereferenceï¼Œä»è€Œè§¦å‘ panicã€‚  
å°±å®‰å…¨é£é™©è€Œè¨€ï¼Œè¯¥é—®é¢˜ä¼šå¯¼è‡´ apiserver å´©æºƒæˆ–è€…é•¿æ—¶é—´é˜»å¡ï¼Œä»è€Œé€ æˆé›†ç¾¤æ§åˆ¶å¹³é¢ä¸å¯ç”¨ï¼ˆæ‹’ç»æœåŠ¡ï¼ŒDoSï¼‰ï¼Œä½¿å¾—é›†ç¾¤çš„ç®¡ç†åŠŸèƒ½å—åˆ°ä¸¥é‡å½±å“ã€‚ç”±äº Kubernetes æœ¬èº«å…è®¸ä¸åŒç§Ÿæˆ·çš„ç”¨æˆ·åœ¨è‡ªå·±çš„å‘½åç©ºé—´å†…åˆ›å»º Deploymentï¼Œå¦‚æœæ”»å‡»è€…æ‹¥æœ‰å¸¸è§„çš„ pod åˆ›å»ºæƒé™ï¼Œåˆ™å¯ä»¥æ„é€ åŒ…å« postStart é’©å­çš„ Podï¼Œä»è€Œè§¦å‘è¯¥é—®é¢˜ã€‚ç‰¹åˆ«æ˜¯åœ¨å¤šç§Ÿæˆ·é›†ç¾¤ä¸­ï¼Œå¦‚æœä½æƒé™ç”¨æˆ·å¯ä»¥åˆ›å»º Deploymentï¼Œå°±å¯èƒ½åˆ©ç”¨è¯¥æ¼æ´å½±å“åŒä¸€é›†ç¾¤ä¸­å…¶ä»–ç”¨æˆ·çš„æ­£å¸¸è¿è¡Œã€‚

å› æ­¤ï¼Œè¯¥é—®é¢˜å…·æœ‰æ½œåœ¨çš„æ‹’ç»æœåŠ¡é£é™©ï¼Œå¹¶ä¸”æ ¹æ® CVSS 3.1 çš„è¯„åˆ†æ ‡å‡†ï¼Œå¦‚æœæ”»å‡»è€…ä¸éœ€è¦é¢å¤–é«˜æƒé™å³å¯å®æ–½ï¼ˆåªéœ€æ™®é€šçš„ pod åˆ›å»ºæ“ä½œï¼‰ï¼Œåˆ™è¯¥æ¼æ´å¯è¯„ä¸ºé«˜é£é™©ã€‚

åˆ¤æ–­ä¾æ®ï¼š
1. è¯¥ issue å¯¼è‡´ apiserver å†…éƒ¨ panicï¼Œäº§ç”Ÿ nil pointer dereferenceï¼Œä»è€Œä¸­æ–­æ­£å¸¸æœåŠ¡ã€‚
2. æ”»å‡»è€…åªéœ€åˆ›å»ºä¸€ä¸ªåŒ…å« postStart é’©å­çš„ Deployment å°±å¯è§¦å‘ï¼Œæ— éœ€é«˜æƒé™æ“ä½œã€‚
3. å‡ºç°é—®é¢˜åï¼Œpod æ— æ³•æ­£å¸¸å¯åŠ¨ï¼Œå¹¶ä¸”å¸¸è§çš„ç®¡ç†æ“ä½œï¼ˆæ‰©å®¹ã€æ›´æ–°ç­‰ï¼‰ä¹Ÿä¼šå¤±è´¥ï¼Œæœ€ç»ˆå¯èƒ½å¯¼è‡´é›†ç¾¤æ‹’ç»æœåŠ¡ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
import time
import threading
import http.server
import socketserver
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# HTTPæœåŠ¡å™¨ç”¨äºå±•ç¤ºå¤ç°è„šæœ¬åœ¨æœ¬åœ°è¿è¡Œæ—¶çš„æç¤ºä¿¡æ¯ï¼ˆç›‘å¬8080ç«¯å£ï¼‰
def start_http_server():
    class Handler(http.server.SimpleHTTPRequestHandler):
        def log_message(self, format, *args):
            print(format % args)
    with socketserver.TCPServer(("", 8080), Handler) as httpd:
        print("HTTP server serving at port 8080")
        httpd.serve_forever()

def create_deployment_with_poststart(api_instance, namespace="default", name="inflate"):
    # å®šä¹‰ deployment specï¼Œå…¶ä¸­åŒ…å« postStart ç”Ÿå‘½å‘¨æœŸé’©å­: sleep 15 ç§’
    container = client.V1Container(
        name="inflate",
        image="python:3",
        command=["sleep"],
        args=["infinity"],
        lifecycle=client.V1Lifecycle(
            post_start=client.V1Handler(
                _exec=client.V1ExecAction(
                    command=["sleep", "15"]
                )
            )
        )
    )

    template = client.V1PodTemplateSpec(
        metadata=client.V1ObjectMeta(labels={"app": "inflate"}),
        spec=client.V1PodSpec(
            containers=[container],
            service_account_name="inflate",
            automount_service_account_token=False
        )
    )

    spec = client.V1DeploymentSpec(
        replicas=1,
        selector=client.V1LabelSelector(match_labels={"app": "inflate"}),
        template=template
    )

    deployment = client.V1Deployment(
        api_version="apps/v1",
        kind="Deployment",
        metadata=client.V1ObjectMeta(name=name),
        spec=spec
    )

    try:
        api_response = api_instance.create_namespaced_deployment(
            body=deployment,
            namespace=namespace
        )
        print(f"Deployment {name} created. Status='{api_response.metadata.name}'")
    except ApiException as e:
        print("Exception when creating deployment: %s\n" % e)

def scale_deployment(api_instance, namespace="default", name="inflate", replicas=2):
    # å°è¯•æ‰©å®¹ï¼Œè§‚å¯Ÿæ˜¯å¦è§¦å‘ apiserver çš„é”™è¯¯
    body = {"spec": {"replicas": replicas}}
    try:
        api_response = api_instance.patch_namespaced_deployment_scale(
            name=name,
            namespace=namespace,
            body=body
        )
        print(f"Scaled deployment {name} to {replicas} replicas.")
    except ApiException as e:
        print("Exception when scaling deployment: %s\n" % e)

def patch_deployment(api_instance, namespace="default", name="inflate"):
    # å°è¯•ä¿®æ”¹ deploymentï¼Œä¾‹å¦‚æ›´æ–°é•œåƒ tagï¼Œè§‚å¯Ÿæ˜¯å¦è§¦å‘é”™è¯¯
    body = {"spec": {"template": {"spec": {"containers": [{"name": "inflate", "image": "python:3.8"}]}}}}
    try:
        api_response = api_instance.patch_namespaced_deployment(
            name=name,
            namespace=namespace,
            body=body
        )
        print(f"Patched deployment {name} to update the image tag.")
    except ApiException as e:
        print("Exception when patching deployment: %s\n" % e)

def main():
    # å¼€å¯ HTTP æœåŠ¡å™¨çš„çº¿ç¨‹ï¼Œç”¨äºç›‘å¬8080ç«¯å£å¹¶è¾“å‡ºæ—¥å¿—ä¿¡æ¯
    threading.Thread(target=start_http_server, daemon=True).start()
    
    # ä»é»˜è®¤ kubeconfig åŠ è½½é…ç½®
    try:
        config.load_kube_config()
    except Exception as e:
        print("Error loading kubeconfig: %s" % e)
        return

    apps_v1 = client.AppsV1Api()
    core_v1 = client.CoreV1Api()

    # åˆ›å»º ServiceAccount "inflate"ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    sa_body = client.V1ServiceAccount(
        metadata=client.V1ObjectMeta(name="inflate", labels={"app": "inflate"}),
        automount_service_account_token=False
    )
    try:
        core_v1.create_namespaced_service_account(namespace="default", body=sa_body)
        print("ServiceAccount 'inflate' created.")
    except ApiException as e:
        if e.status != 409:
            print("Exception when creating ServiceAccount: %s\n" % e)
        else:
            print("ServiceAccount 'inflate' already exists.")

    # åˆ›å»ºåŒ…å« postStart ç”Ÿå‘½å‘¨æœŸé’©å­çš„ deployment
    create_deployment_with_poststart(apps_v1, namespace="default", name="inflate")

    # ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œè§‚å¯Ÿ deployment çŠ¶æ€
    print("ç­‰å¾…20ç§’è®© deployment è¿›å…¥ Pending çŠ¶æ€...")
    time.sleep(20)

    # å°è¯•æ‰©å®¹ deploymentï¼Œè§‚å¯Ÿæ˜¯å¦è§¦å‘é”™è¯¯ï¼ˆDoSæ•ˆæœï¼‰
    print("å°è¯•æ‰©å®¹ deployment ...")
    scale_deployment(apps_v1, namespace="default", name="inflate", replicas=2)
    
    # å°è¯• patch éƒ¨ç½²ï¼Œæ›´æ–°é•œåƒ tag
    print("å°è¯• patch deployment ...")
    patch_deployment(apps_v1, namespace="default", name="inflate")

    print("å¤ç°æ“ä½œå·²å®Œæˆï¼Œè¯·æ£€æŸ¥é›†ç¾¤çŠ¶æ€å’Œ apiserver æ—¥å¿—ã€‚")
    
    # ä¿æŒè„šæœ¬è¿è¡Œï¼Œä»¥ä¾¿ HTTP server æŒç»­æä¾›æœåŠ¡
    while True:
        time.sleep(30)

if __name__ == "__main__":
    main()
```


---


## Issue #129787 Compressed pod log files default to 644 permissions

- Issue é“¾æ¥ï¼š[#129787](https://github.com/kubernetes/kubernetes/issues/129787)

### Issue å†…å®¹

#### What happened?

When the pod logs rotate and get compressed, the gzipped file defaults to the permissions of 644. With the directories set to 755 this allows the logs to be world readable at that point.

This appears to be where this issue is happening at - https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/logs/container_log_manager.go#L422

#### What did you expect to happen?

For the compressed logs to either inherit the original log file permissions (600) or for this file permission to be configurable in the kubelet configuration

#### How can we reproduce it (as minimally and precisely as possible)?

Generate logs & wait until they compress - https://kubernetes.io/docs/concepts/cluster-administration/logging/#basic-logging-in-kubernetes

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.9
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.2
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
Rocky Linux 9.5
Version = 9.20250121.0.5 (Blue Onyx)
$ uname -a
5.14.0-503.21.1.el9_5.x86_64

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
æœ¬é—®é¢˜æè¿°çš„å®‰å…¨é—®é¢˜å±äºå®‰å…¨é…ç½®é”™è¯¯å¯¼è‡´çš„ä¿¡æ¯æ³„éœ²é—®é¢˜ï¼Œæ ¹æ® CVSS 3.1 è¯„åˆ†ï¼Œè™½ç„¶æ”»å‡»éœ€è¦åœ¨æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿä¸Šæ“ä½œï¼ˆå¹¶éè¿œç¨‹ç½‘ç»œæ”»å‡»ï¼‰ï¼Œä½†ç”±äºï¼š
1. æ”»å‡»è€…ä¸éœ€è¦é¢å¤–æƒé™å³å¯è¯»å–æ—¥å¿—æ–‡ä»¶ï¼ˆå› ä¸ºé»˜è®¤æƒé™ä¸º 644ï¼Œå³æ‰€æœ‰ç”¨æˆ·å¯è¯»ï¼‰ï¼Œ
2. æ—¥å¿—ä¸­å¯èƒ½åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼Œ
3. å¤šç§Ÿæˆ·ç¯å¢ƒä¸‹ä½æƒé™ç”¨æˆ·å¯èƒ½è¯»å–åˆ°å…¶ä»–ç”¨æˆ·çš„æ—¥å¿—ï¼Œ

æ•…æŒ‰æƒ…æ™¯è€ƒè™‘å¯èƒ½å¯¼è‡´è¾ƒä¸¥é‡çš„ä¿¡æ¯æ³„éœ²ã€‚è‹¥æ ¹æ® CVSS 3.1 ä¼°åˆ†ï¼Œå¯èƒ½åœ¨ 7.0 æˆ–ä»¥ä¸Šï¼Œåˆ™åº”å½’ä¸ºé«˜é£é™©ï¼›ä½†è€ƒè™‘åˆ°æ”»å‡»éœ€è¦åœ¨å®¿ä¸»æœºä¸Šè¿›è¡Œï¼Œæœ¬é—®é¢˜å­˜åœ¨ä¸€å®šä½¿ç”¨åœºæ™¯é™åˆ¶ï¼Œå› æ­¤å¤§éƒ¨åˆ†æƒ…å†µä¸‹å¯ä»¥è¯„ä¼°ä¸ºä½é£é™©ã€‚ä¸è¿‡åœ¨å­˜åœ¨æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚å‡­æ®ã€å¯†é’¥ï¼‰çš„åœºæ™¯ä¸‹ï¼Œé£é™©ä¸å¯å°è§‘ï¼Œåº”å½’ä¸ºé«˜é£é™©ã€‚

åœ¨æ­¤å›ç­”ä¸­ï¼Œä¸ºç»¼åˆæƒ…å†µï¼Œæˆ‘ä»¬ç»™å‡ºé£é™©è¯„çº§ä¸ºâ€œä½é£é™©â€ï¼ˆé‰´äºåœ¨å¾ˆå¤šé»˜è®¤åœºæ™¯ä¸‹ï¼Œæ—¥å¿—ä¸­çš„ä¿¡æ¯å¯èƒ½æ²¡æœ‰æé«˜æ•æ„Ÿæ€§ï¼‰ï¼Œä½†åœ¨æ•æ„Ÿç¯å¢ƒä¸‹å¯è§†ä¸ºé«˜é£é™©ã€‚

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æŠ¥å‘Šçš„æ˜¯åœ¨å®¹å™¨æ—¥å¿—è½®è½¬æ—¶ï¼Œå‹ç¼©åçš„æ—¥å¿—æ–‡ä»¶é»˜è®¤æƒé™ä¸º 644ï¼Œå¯¼è‡´åœ¨ç›®å½•æƒé™ä¸º 755 çš„æƒ…å†µä¸‹ï¼Œæ—¥å¿—æ–‡ä»¶ä¼šè¢«æ‰€æœ‰ç”¨æˆ·è¯»å–åˆ°ã€‚æ—¥å¿—æ–‡ä»¶ä¸­å¯èƒ½å­˜åœ¨æ•æ„Ÿä¿¡æ¯ï¼ˆä¾‹å¦‚å‡­æ®ã€è°ƒè¯•ä¿¡æ¯ç­‰ï¼‰ï¼Œå› æ­¤ä½¿å¾—éç‰¹æƒç”¨æˆ·å¯ä»¥è½»æ˜“è¯»å–æ•æ„Ÿæ•°æ®ã€‚æ­¤é—®é¢˜å±äºæ—¥å¿—æƒé™é…ç½®ä¸å½“å¼•èµ·çš„ä¿¡æ¯æ³„éœ²é—®é¢˜ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼šå¦‚æœæ—¥å¿—å†…å®¹æœ¬èº«ä¸åŒ…å«æ•æ„Ÿæ•°æ®ï¼Œåˆ™é£é™©è¾ƒä½ï¼›ä½†åœ¨å¤§å¤šæ•°ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œæ—¥å¿—ä¸­å¾€å¾€ä¼šè®°å½•æ•æ„Ÿæ•°æ®ï¼ˆå¦‚å†…éƒ¨è¯·æ±‚ã€é”™è¯¯è¯¦æƒ…ç”šè‡³å‡­æ®ï¼‰ä¸”ä¸€èˆ¬ Kubernetes é›†ç¾¤å¯èƒ½å­˜åœ¨å¤šç§Ÿæˆ·åœºæ™¯ï¼Œä¸€ä¸ªä¸å…·å¤‡é«˜åº¦æƒé™çš„ç”¨æˆ·å¯èƒ½åˆ©ç”¨è¿™ä¸€é—®é¢˜è¯»å–ä»–äººå®¹å™¨çš„æ—¥å¿—ä¿¡æ¯ï¼Œä»è€Œé€ æˆæ•æ„Ÿä¿¡æ¯æ³„éœ²ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
import os
import stat
import gzip
import shutil
import tempfile
import threading
from http.server import SimpleHTTPRequestHandler, HTTPServer

def create_log_file(log_path):
    # åˆ›å»ºä¸€ä¸ªæ—¥å¿—æ–‡ä»¶ï¼Œå¹¶è®¾ç½®ä¸º600æƒé™ï¼ˆä»…æ‹¥æœ‰è€…å¯è¯»å†™ï¼‰
    with open(log_path, "w") as f:
        f.write("This is a sample log entry that may contain sensitive data.\n" * 10)
    os.chmod(log_path, 0o600)
    print(f"[+] Created log file: {log_path} with mode: {oct(os.stat(log_path).st_mode & 0o777)}")

def compress_log_file(src_path, dst_path):
    # ä½¿ç”¨gzipå‹ç¼©æ—¥å¿—æ–‡ä»¶
    with open(src_path, 'rb') as f_in, gzip.open(dst_path, 'wb') as f_out:
        shutil.copyfileobj(f_in, f_out)
    # æ¨¡æ‹Ÿæ—¥å¿—è½®è½¬ä¸­çš„æƒ…å†µï¼šå‹ç¼©åçš„æ–‡ä»¶æœªç»§æ‰¿åŸå§‹600æƒé™ï¼Œé€šå¸¸é»˜è®¤ä¼šæ˜¯644ï¼ˆå—umaskå½±å“ï¼‰
    print(f"[+] Compressed log file: {dst_path}")
    mode = os.stat(dst_path).st_mode & 0o777
    print(f"[+] Compressed file mode: {oct(mode)}")
    return mode

def start_http_server(directory, port=8080):
    # åˆ‡æ¢å·¥ä½œç›®å½•åˆ°æŒ‡å®šç›®å½•
    os.chdir(directory)
    
    handler = SimpleHTTPRequestHandler
    server = HTTPServer(("", port), handler)
    print(f"[+] HTTP server started at http://0.0.0.0:{port}")

    def serve():
        try:
            server.serve_forever()
        except KeyboardInterrupt:
            pass
        server.server_close()
    
    thread = threading.Thread(target=serve, daemon=True)
    thread.start()
    return server

def main():
    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶ç›®å½•ä½œä¸ºæµ‹è¯•ç¯å¢ƒ
    with tempfile.TemporaryDirectory() as tmpdir:
        print(f"[+] Using temporary directory: {tmpdir}")
        log_file = os.path.join(tmpdir, "pod.log")
        compressed_file = os.path.join(tmpdir, "pod.log.gz")
        
        # æ¨¡æ‹Ÿç”Ÿæˆæ—¥å¿—å’Œè®¾ç½®ä¸¥æ ¼æƒé™
        create_log_file(log_file)
        
        # æ¨¡æ‹Ÿæ—¥å¿—è½®è½¬ï¼Œå‹ç¼©æ—¥å¿—æ–‡ä»¶
        final_mode = compress_log_file(log_file, compressed_file)
        
        # æ£€æŸ¥å¦‚æœå‹ç¼©æ–‡ä»¶æƒé™ä¸º644ï¼Œåˆ™è¯´æ˜å­˜åœ¨è¯¥é—®é¢˜
        if final_mode == 0o644:
            print("[!] ç¡®è®¤å¤ç°é—®é¢˜ï¼šå‹ç¼©åçš„æ—¥å¿—æ–‡ä»¶æƒé™ä¸º 644ï¼Œå¯èƒ½å¯¼è‡´æ•æ„Ÿä¿¡æ¯æ³„éœ²")
        else:
            print("[+] æœªå¤ç°é—®é¢˜ï¼šå‹ç¼©æ—¥å¿—æ–‡ä»¶æƒé™ä¸º", oct(final_mode))
        
        # å¯åŠ¨HTTPæœåŠ¡ï¼Œä¾¿äºè¿œç¨‹æŸ¥çœ‹æˆ–ä¸‹è½½æ–‡ä»¶
        http_server = start_http_server(tmpdir, port=8080)
        
        # ä¿æŒæœåŠ¡è¿è¡Œï¼Œç›´åˆ°ç”¨æˆ·æ‰‹åŠ¨ç»ˆæ­¢
        try:
            while True:
                pass
        except KeyboardInterrupt:
            print("\n[+] ç»ˆæ­¢HTTPæœåŠ¡")

if __name__ == "__main__":
    main()
```


---


## Issue #129584 Race condition in API server that can lead to leaked resources

- Issue é“¾æ¥ï¼š[#129584](https://github.com/kubernetes/kubernetes/issues/129584)

### Issue å†…å®¹

#### What happened?

We believe to have come across a race condition in the API Server that can lead to scenarios where controllers leak external resources. We primarily observed this in an internal fork of API Server, but we also have some (internal) evidence to have observed this in Kubernetes directly, and our current hypothesis applies to Kubernetes' code as well, so we are sharing this here to seek confirmation and agreement for the solution.

Let's consider a scenario where a controller adds a finalizer to a newly created object, because it allocates some external resources that it needs to clean up upon deletion of the object. Some other actor issues a DELETE request that happens to be right _after_ the PUT request to add the finalizer. What we observe is that _both_ the DELETE and the PUT request (adding the finalizer) _succeed_, leading the controller to allocate the resources despite no finalizer existing anymore, because the object is now hard-deleted.

If we take a look at [`Store#Delete`](https://github.com/kubernetes/kubernetes/blob/36d316ebc524a47d05e479b848148c1acf1cee7b/staging/src/k8s.io/apiserver/pkg/registry/generic/registry/store.go#L1128-L1217), it consists roughly of the following steps:
1. Fetch the object.
2. Check whether there are any pending finalizers, and if so, do not delete immediately.
3. Send the request for immediate deletion to storage otherwise.

In the above scenario, the PUT request would occur after the DELETE request processing has (1) fetched the current object. This means it believes that the object has no pending finalizers, and thus proceeds with immediate deletion. Since the DELETE request did not specify an explicit precondition on the resource version, the concurrently added finalizer is ignored by storage, and the deletion is carried out.

Such a rV precondition is only set towards storage if either (a) the client specified it as part of the request, or (b) if within (2) a finalizer was added internally, because it caused an update to the object.

We believe that due to the read performed in (1), the DELETE logic should implicitly add a resource version precondition for the deletion towards storage even if no update was issued, and set this to the resource version of the object read in (1).

Though one could argue that the deleting client should've just set a precondition, and that the delete happening unconditionally is correct, I would urge to view this from the perspective of the controller. A delete can come from anywhere outside the control of a controller, but the controller now leaks resources because it made the reasonable assumption that its request to add a finalizer succeeding means the finalizer does indeed now exist. 

_Disclaimer_: I tried submitting this to the ML first, but somehow it seems my message never made it there, so reporting it here instead.

#### What did you expect to happen?

-

#### How can we reproduce it (as minimally and precisely as possible)?

I don't have a specific reproduction, since this is a very intricate timing issue.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©  
ï¼ˆCVSS 3.1 è¯„åˆ†é¢„ä¼°é«˜äº 7.0ï¼Œå› å…¶å¯èƒ½å¯¼è‡´å…³é”®å¤–éƒ¨èµ„æºæ³„éœ²å’Œåç»­æœåŠ¡æ‹’ç»ï¼‰

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº† API Server ä¸­çš„ä¸€ä¸ªç«æ€æ¡ä»¶é—®é¢˜ã€‚åœºæ™¯æ˜¯ï¼šå½“æ§åˆ¶å™¨ä¸ºä¸€ä¸ªæ–°åˆ›å»ºçš„å¯¹è±¡æ·»åŠ  finalizerï¼ˆä»¥ä¾¿åœ¨å¯¹è±¡åˆ é™¤æ—¶æ¸…ç†å¤–éƒ¨èµ„æºï¼‰æ—¶ï¼Œå¦‚æœæ°å¥½æœ‰å¦ä¸€ä¸ª actor åœ¨ finalizer æ·»åŠ æ“ä½œæäº¤ä¹‹åã€ä½†åœ¨å­˜å‚¨ DELETE æ“ä½œå®Œæˆå‰å‘èµ· DELETE è¯·æ±‚ï¼Œåˆ™è¯¥ DELETE æ“ä½œä¾èµ–äºç¬¬ä¸€æ­¥è·å–çš„å¯¹è±¡çŠ¶æ€ï¼ˆæ—  finalizerï¼‰ï¼Œå› æ­¤åˆ é™¤æ“ä½œä»ç„¶æˆåŠŸï¼Œå¿½ç•¥äº†åæ¥æ·»åŠ çš„ finalizerï¼Œç”±æ­¤å¯¼è‡´å¤–éƒ¨èµ„æºè¢«åˆ†é…ä½†æ°¸è¿œå¾—ä¸åˆ°æ¸…ç†ï¼Œæœ€ç»ˆå‡ºç°èµ„æºæ³„éœ²ã€‚è¿™ä¸€æ¼æ´å¯èƒ½å¯¼è‡´å †ç§¯çš„å¤–éƒ¨èµ„æºæ— æ³•å›æ”¶ï¼Œä»è€Œè€—å°½èµ„æºã€å¼•å‘æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰çš„é£é™©ã€‚

ä»å®‰å…¨é£é™©è§’åº¦çœ‹ï¼š
1. è¯¥é—®é¢˜å±äºå®‰å…¨é—®é¢˜ï¼Œå› ä¸ºå®ƒå½±å“äº†å¤–éƒ¨èµ„æºçš„æ­£å¸¸æ¸…ç†ï¼Œå¹¶å¯èƒ½è¢«æ¶æ„åˆ©ç”¨ï¼›
2. è¯¥æ¼æ´å¯ä»¥ä½¿æ”»å‡»è€…åœ¨å…·å¤‡åˆ é™¤æƒé™çš„å‰æä¸‹ï¼Œé€šè¿‡åˆ¶é€ ç«æ€æ¡ä»¶ä¸æ–­è§¦å‘èµ„æºæ³„éœ²ï¼Œè¿›è€Œè€—å°½æŸäº›å¤–éƒ¨èµ„æºï¼Œä½¿å¾—ç³»ç»Ÿæˆ–æœåŠ¡ä¸å¯ç”¨ï¼›
3. æ ¹æ® CVSS 3.1 æ ‡å‡†ï¼Œè¯¥æ¼æ´å¦‚æœè¢«æ¶æ„ç”¨æˆ·åˆ©ç”¨ï¼Œå¯èƒ½å¯¼è‡´ä¸¥é‡çš„æ‹’ç»æœåŠ¡æ”»å‡»ï¼Œå› è€Œé£é™©è¯„çº§è¾¾åˆ°é«˜é£é™©ã€‚è™½ç„¶å®æ–½æ”»å‡»éœ€è¦æ”»å‡»è€…æœ‰å¯¹ç‰¹å®šå¯¹è±¡æ‰§è¡Œ DELETE æ“ä½œçš„æƒé™ï¼Œä½†åœ¨å¤šç”¨æˆ·ç¯å¢ƒä¸­å³ä½¿æ˜¯ä½æƒé™ç”¨æˆ·å¦‚æœèƒ½æ“çºµè‡ªå·±æ‹¥æœ‰æƒé™çš„å¯¹è±¡ï¼Œä¹Ÿå¯èƒ½å¯¹æ•´ä¸ªç³»ç»Ÿé€ æˆå½±å“ï¼Œå› æ­¤ä¸å®œé™ä½é£é™©è¯„çº§ã€‚

å› æ­¤ï¼Œè¯¥æ¼æ´è¢«åˆ¤å®šä¸ºé«˜é£é™©æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
import threading
import time
from http.server import HTTPServer, BaseHTTPRequestHandler

# æ¨¡æ‹Ÿçš„èµ„æºå¯¹è±¡ï¼ŒåŒ…å« finalizer å’Œèµ„æºç‰ˆæœ¬ç­‰å±æ€§
class ResourceObject:
    def __init__(self):
        self.finalizer = False
        self.resource_version = 1
        self.deleted = False
        self.lock = threading.Lock()

    def get_snapshot(self):
        with self.lock:
            return {
                "finalizer": self.finalizer,
                "resource_version": self.resource_version,
                "deleted": self.deleted
            }

    def add_finalizer(self):
        with self.lock:
            # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            time.sleep(0.1)
            # æ›´æ–° finalizer å¹¶å¢åŠ èµ„æºç‰ˆæœ¬
            self.finalizer = True
            self.resource_version += 1
            print(f"[PUT] Finalizer added, new resource_version: {self.resource_version}")

    def delete(self, expected_rv):
        with self.lock:
            # åˆ é™¤å‰æ£€æŸ¥èµ„æºç‰ˆæœ¬åŒ¹é… (å¦‚æœä½¿ç”¨é¢„æ¡ä»¶ï¼Œåˆ™ä¼šæ£€æŸ¥)
            # æ¨¡æ‹Ÿæ²¡æœ‰è®¾ç½®é¢„æ¡ä»¶ï¼Œæ‰€ä»¥æ¯”è¾ƒæ—§èµ„æºç‰ˆæœ¬çš„æƒ…å†µ
            if self.deleted:
                print("[DELETE] Already deleted.")
                return False
            # æ­¤å¤„æ¨¡æ‹Ÿç«æ€ï¼šå¦‚æœ object åœ¨è¯»å–åè¢«æ›´æ–°äº† finalizerï¼Œä½†åˆ é™¤æ“ä½œæœªèƒ½æ„ŸçŸ¥
            snapshot = self.get_snapshot()
            # å¦‚æœ snapshot.finalizer ä¸º Falseï¼Œåˆ™è®¤ä¸ºå¯ä»¥åˆ é™¤
            if not snapshot["finalizer"]:
                # æ­¤å¤„ä¸æ£€æŸ¥ expected_rvä¸å½“å‰ resource_version çš„ä¸€è‡´æ€§
                self.deleted = True
                print("[DELETE] Delete operation succeeded without noticing concurrent finalizer.")
                return True
            else:
                # æ£€æŸ¥å¤±è´¥åˆ™æ‹’ç»åˆ é™¤
                print("[DELETE] Delete operation abandoned due to existing finalizer.")
                return False

# æ¨¡æ‹Ÿ API Server çš„ DELETE æ“ä½œ
def delete_operation(resource_obj):
    # æ¨¡æ‹Ÿåˆ é™¤è¯·æ±‚å¼€å§‹ï¼Œå…ˆè·å–å¯¹è±¡å¿«ç…§
    snapshot = resource_obj.get_snapshot()
    print(f"[DELETE] Read snapshot: {snapshot}")
    # æ¨¡æ‹Ÿç½‘ç»œ/å¤„ç†å»¶è¿Ÿ
    time.sleep(0.2)
    # å°è¯•åˆ é™¤æ“ä½œï¼Œæ²¡æœ‰è®¾ç½®èµ„æºç‰ˆæœ¬é¢„æ¡ä»¶ï¼Œå› æ­¤ä½¿ç”¨ä¹‹å‰è¯»å–çš„çŠ¶æ€
    resource_obj.delete(expected_rv=snapshot["resource_version"])

# æ¨¡æ‹Ÿæ§åˆ¶å™¨æ·»åŠ  finalizer çš„ PUT æ“ä½œ
def put_operation(resource_obj):
    # æ¨¡æ‹Ÿå»¶è¿Ÿï¼Œç¡®ä¿ DELETE åœ¨è¯»å–åï¼ŒPUT åœ¨ DELETE æ‰§è¡Œå‰å®Œæˆ
    time.sleep(0.15)
    resource_obj.add_finalizer()

# åŒæ—¶å¯åŠ¨ HTTP æœåŠ¡å™¨ç”¨äºç¬¦åˆé¢˜ç›®è¦æ±‚
class SimpleHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.end_headers()
        message = "POC HTTP Server running on port 8080."
        self.wfile.write(message.encode('utf-8'))

def start_http_server():
    server_address = ("", 8080)
    httpd = HTTPServer(server_address, SimpleHandler)
    print("Starting HTTP server on port 8080...")
    httpd.serve_forever()

if __name__ == "__main__":
    # å¯åŠ¨ HTTP æœåŠ¡å™¨çº¿ç¨‹
    http_thread = threading.Thread(target=start_http_server, daemon=True)
    http_thread.start()

    # åˆå§‹åŒ–å…±äº«çš„æ¨¡æ‹Ÿå¯¹è±¡
    resource_obj = ResourceObject()

    # å¯åŠ¨ DELETE å’Œ PUT æ“ä½œçº¿ç¨‹ä»¥åˆ¶é€ ç«æ€æ¡ä»¶
    delete_thread = threading.Thread(target=delete_operation, args=(resource_obj,))
    put_thread = threading.Thread(target=put_operation, args=(resource_obj,))

    # åŒæ—¶å¯åŠ¨ä¸¤ä¸ªçº¿ç¨‹
    delete_thread.start()
    put_thread.start()

    # ç­‰å¾…çº¿ç¨‹å®Œæˆ
    delete_thread.join()
    put_thread.join()

    # æœ€ç»ˆæ£€æµ‹å¯¹è±¡çŠ¶æ€
    final_snapshot = resource_obj.get_snapshot()
    print(f"[RESULT] Final object state: {final_snapshot}")

    # æç¤ºå¤ç°ç»“æŸ
    print("å¤ç°è„šæœ¬æ‰§è¡Œå®Œæ¯•ã€‚")
```


**è§£é‡Šè¯´æ˜ï¼š**

1. è„šæœ¬ä¸­å®šä¹‰äº†ä¸€ä¸ª ResourceObject ç±»ï¼Œç”¨äºæ¨¡æ‹Ÿä¸€ä¸ªå¸¦æœ‰ finalizerã€èµ„æºç‰ˆæœ¬å’Œåˆ é™¤çŠ¶æ€çš„å¯¹è±¡ã€‚å¯¹è±¡å†…éƒ¨é€šè¿‡é”ï¼ˆlockï¼‰ä¿è¯å¯¹çŠ¶æ€çš„åŸå­æ“ä½œã€‚
2. delete_operation å‡½æ•°æ¨¡æ‹Ÿ DELETE è¯·æ±‚ï¼Œé¦–å…ˆè¯»å–å¯¹è±¡å¿«ç…§ï¼Œç„¶ååœ¨å»¶æ—¶ä¹‹åè°ƒç”¨ delete æ–¹æ³•ã€‚delete æ–¹æ³•ä¸æ£€æŸ¥èµ„æºç‰ˆæœ¬é¢„æ¡ä»¶ï¼Œä»è€Œæ¨¡æ‹Ÿäº†ç”±äºç«æ€æ¡ä»¶å¯¼è‡´çš„ finalizer è¢«å¿½ç•¥çš„é—®é¢˜ã€‚
3. put_operation å‡½æ•°æ¨¡æ‹Ÿæ§åˆ¶å™¨åœ¨ DELETE è¯·æ±‚è¯»å–åã€åˆ é™¤å‰æ·»åŠ  finalizer çš„ PUT è¯·æ±‚ï¼ŒåŒæ—¶ä¿®æ”¹å¯¹è±¡çš„ finalizer çŠ¶æ€åŠèµ„æºç‰ˆæœ¬ã€‚
4. ä¸ºäº†è¯´æ˜è¿™ä¸ªç«æ€æ¡ä»¶åŒæ—¶å­˜åœ¨ HTTP æœåŠ¡è¦æ±‚ï¼Œè„šæœ¬ä¹Ÿåœ¨åå°å¯åŠ¨äº†ä¸€ä¸ªç›‘å¬ 8080 ç«¯å£çš„ HTTP æœåŠ¡å™¨ï¼Œå“åº”ç®€å•çš„ GET è¯·æ±‚ã€‚
5. æ•´ä¸ªè„šæœ¬é€šè¿‡çº¿ç¨‹ç«äº‰çš„æ–¹å¼å¤ç°äº†åœ¨ DELETE ä¸ PUT è¯·æ±‚ä¹‹é—´å­˜åœ¨çš„æ—¶é—´çª—å£ï¼Œä½¿å¾— DELETE è¯·æ±‚æœªèƒ½æ„ŸçŸ¥åˆ°åç»­çš„ finalizer æ·»åŠ ï¼Œä»è€Œå¯¼è‡´èµ„æºåˆ é™¤æ“ä½œçš„å¼‚å¸¸æˆåŠŸï¼Œæ¨¡æ‹Ÿäº† API Server ä¸­èµ„æºæ³„éœ²çš„é—®é¢˜ã€‚

è¯¥è„šæœ¬ä»…ä½œä¸ºæœ¬åœ°ç ”ç©¶å’Œæµ‹è¯•ä¹‹ç”¨ï¼Œè¯·å‹¿åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨ã€‚

---


## Issue #129463 kubelet could reject pods with NodeAffinity error due to incomplete informer cache on the node object

- Issue é“¾æ¥ï¼š[#129463](https://github.com/kubernetes/kubernetes/issues/129463)

### Issue å†…å®¹

#### What happened?

In our end (AKS), we observed that there is a case that kubelet could reject pods with NodeAffinty status if the node watch call got closed due to unknown reason during kubelet start up.

An illustrated procedure is (using `Tx` to indicate time):

- T1: kubelet starts, spawns the processes for registering node (create node) and node informer for syncing the node object in parallel
- T2: the node informer successfully listed empty nodes list from kube-apiserver , proceeding to watch call
https://github.com/kubernetes/kubernetes/blob/6746df77f2376c6bc1fd0de767d2a94e6bd6cec1/staging/src/k8s.io/client-go/tools/cache/reflector.go#L401-L409
- T3: the watch call stopped due to unknown issue, for instance network closed. In this case, the watch call would exit with error like: 
  > very short watch: k8s.io/client-go/informers/factory.go:160: Unexpected watch close - watch lasted less than a second and no items received

  In this case, the node informer will back-off and retry with new list-watch steps:
  https://github.com/kubernetes/kubernetes/blob/6746df77f2376c6bc1fd0de767d2a94e6bd6cec1/staging/src/k8s.io/client-go/tools/cache/reflector.go#L336-L340
- T4: kubelet successfully created the node object in kube-apiserver side. Kube-scheduler kicks in and assigns pods to the node
- T5: because in T3 the list call was succeeded with empty results, the node informer had been marked as synced:
  
https://github.com/kubernetes/kubernetes/blob/6746df77f2376c6bc1fd0de767d2a94e6bd6cec1/pkg/kubelet/kubelet.go#L442-L444
https://github.com/kubernetes/kubernetes/blob/6746df77f2376c6bc1fd0de767d2a94e6bd6cec1/staging/src/k8s.io/client-go/tools/cache/delta_fifo.go#L308-L316
  In this case, kubelet starts syncing pods and attempts to run. If these pods are set with affinity settings, the kubelet node admission check will fail the pods with "NodeAffinity" status because the node informer cache is empty. Kubelet uses a minimal node object without expected node labels for performing admission check:
https://github.com/kubernetes/kubernetes/blob/6746df77f2376c6bc1fd0de767d2a94e6bd6cec1/pkg/kubelet/kubelet_getters.go#L291-L298
- T6: kubelet node informer retry kicks in, this time it successfully synced the node object from remote. Now retried pods scheduling on this node would succeed as the node labels are expected

In our reproduction, we captured the following logs:

```
T1: kubelet[3538]: I1225 01:02:59.216231    3538 kubelet.go:2346] "Starting kubelet main sync loop"
T2: kubelet[3538]: I1225 01:04:04.030571    3538 reflector.go:359] Caches populated for *v1.Node from k8s.io/client-go/informers/factory.go:160
T3: kubelet[3538]: W1225 01:04:04.170608    3538 reflector.go:470] k8s.io/client-go/informers/factory.go:160: watch of *v1.Node ended with: very short watch: k8s.io/client-go/informers/factory.go:160: Unexpected watch close - watch lasted less than a second and no items received
T4: kubelet[3538]: I1225 01:04:08.309280    3538 kubelet_node_status.go:76] "Successfully registered node" node="<node-name>"
T5 (1): kubelet[3538]: I1225 01:04:29.348027    3538 predicate.go:164] "Predicate failed on Pod" pod="<namespace>/<pod>" err="Predicate NodeAffinity failed"
T5 (2): kubelet[3538]: E1225 01:04:29.351153    3538 kubelet_node_status.go:462] "Error getting the current node from lister" err="node \"<node-name>\" not found"
T6: kubelet[3538]: I1225 01:04:56.591752    3538 reflector.go:359] Caches populated for *v1.Node from k8s.io/client-go/informers/factory.go:160
```

In the normal case, the retry back off (between T6 and T5) should be short and kubelet should not leave many pods failed. However, in rare scenarios that the node lister might be failing for some other reasons before the first successful call (waiting for TLS bootstrapping for example), and increased the back-off (max out to 30s), then in theory, the gap of `T6-T5` could be as long as 30s + jitter, which aligns with the logs timestamps we observed in above.

#### What did you expect to happen?

Kubelet should back off the node list-watch calls until the node object has been populated in the node informer cache. Or, we should invalidate the node informer cache after successfully registering the node from kubelet to maintain the correct version of node object in memory.

#### How can we reproduce it (as minimally and precisely as possible)?

Simulate the watch closed call like mentioned above.

#### Anything else we need to know?

Related issues/PRs:

- https://github.com/kubernetes/kubernetes/pull/99336
- https://github.com/kubernetes/kubernetes/pull/94087

#### Kubernetes version

<details>

1.30 - but we believe this behavior is reproducible in all verisons.

</details>


#### Cloud provider

<details>
AKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©  
ï¼ˆæ ¹æ® CVSS 3.1 çš„è¯„ä¼°ï¼Œè¯¥é—®é¢˜è™½ç„¶å¯èƒ½å¼•å‘æ‹’ç»æœåŠ¡æ”»å‡»ï¼Œä½†åˆ©ç”¨è¯¥é—®é¢˜é€šå¸¸éœ€è¦ä¸€å®šæƒé™æˆ–ç‰¹æ®Šç¯å¢ƒæ¡ä»¶ï¼Œå…¶ CVSS å¾—åˆ†æœªè¾¾åˆ°é«˜é£é™©çº§åˆ«ã€‚ï¼‰

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ issue æè¿°äº†åœ¨ kubelet å¯åŠ¨æ—¶ç”±äºèŠ‚ç‚¹ informer çš„ watch è°ƒç”¨æ„å¤–å…³é—­ï¼ˆä¾‹å¦‚ç”±äºç½‘ç»œé—®é¢˜ï¼‰è€Œå¯¼è‡´èŠ‚ç‚¹å¯¹è±¡æœªèƒ½åŠæ—¶åŒæ­¥ï¼Œè¿›è€Œä½¿å¾—å¸¦æœ‰ NodeAffinity è¦æ±‚çš„ pod è¢« kubelet æ‹’ç»è°ƒåº¦çš„é—®é¢˜ã€‚é”™è¯¯å¹¶éç”±äºæ”»å‡»è€…æ³¨å…¥äº†æ¶æ„æ•°æ®æˆ–ç›´æ¥è¿›è¡Œææƒã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰æ“ä½œï¼Œè€Œæ˜¯ç”±äº kubelet åœ¨å¹¶è¡Œæ³¨å†ŒèŠ‚ç‚¹å’ŒåŒæ­¥èŠ‚ç‚¹ä¿¡æ¯æ—¶å‡ºç°äº†ç«æ€æ¡ä»¶ï¼Œä»è€Œé€ æˆç¼“å­˜ä¸­èŠ‚ç‚¹ä¿¡æ¯ä¸å®Œæ•´çš„é—®é¢˜ã€‚è™½ç„¶è¯¥é—®é¢˜åœ¨éƒ¨åˆ†åœºæ™¯ä¸‹å¯èƒ½å¯¼è‡´ pod è°ƒåº¦å¤±è´¥ï¼Œä»è€Œå¼•å‘ä¸´æ—¶çš„æœåŠ¡æ‹’ç»ï¼ˆDoSï¼‰ï¼Œä½†æ”»å‡»è€…è‹¥æƒ³åˆ©ç”¨è¯¥é—®é¢˜åˆ¶é€  DoSï¼Œé€šå¸¸éœ€è¦å…·å¤‡å¯¹ kubelet æˆ–èŠ‚ç‚¹ç½‘ç»œç¯å¢ƒçš„æ§åˆ¶æƒé™ï¼ˆä¾‹å¦‚äººä¸ºåˆ¶é€ ç½‘ç»œä¸­æ–­ï¼‰ï¼Œå› æ­¤ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œå…¶é£é™©å½±å“ä¸»è¦é›†ä¸­åœ¨å¯ç”¨æ€§æ–¹é¢ï¼Œä¸”åˆ©ç”¨é—¨æ§›è¾ƒé«˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
import json
from http.server import BaseHTTPRequestHandler, HTTPServer
import time
import threading

# å…¨å±€å˜é‡ï¼Œç”¨äºè®°å½•æœåŠ¡å™¨å¯åŠ¨æ—¶é—´
start_time = time.time()

class KubeAPIServerSimulator(BaseHTTPRequestHandler):
    def _set_headers(self, code=200):
        self.send_response(code)
        self.send_header("Content-Type", "application/json")
        self.end_headers()

    def do_GET(self):
        global start_time
        # æ¨¡æ‹Ÿ kube-apiserver çš„èŠ‚ç‚¹åˆ—è¡¨æ¥å£
        if self.path.startswith("/api/v1/nodes"):
            self._set_headers()
            current_time = time.time() - start_time
            # T2é˜¶æ®µï¼šåœ¨å‰20ç§’å†…è¿”å›ç©ºèŠ‚ç‚¹åˆ—è¡¨ï¼ˆæ¨¡æ‹Ÿ informer åˆ—è¡¨è°ƒç”¨è¿”å›ç©ºæ•°æ®ï¼‰
            if current_time < 20:
                response = {"items": []}
            else:
                # æ¨¡æ‹Ÿ T6 é˜¶æ®µï¼šè¿”å›åŒ…å«å®Œæ•´èŠ‚ç‚¹ä¿¡æ¯çš„åˆ—è¡¨
                node = {
                    "metadata": {"name": "node-01"},
                    "spec": {"providerID": "simulated://node-01"},
                    "status": {"conditions": [{"type": "Ready", "status": "True"}]},
                    "labels": {"role": "worker", "zone": "us-central1-a"}
                }
                response = {"items": [node]}
            self.wfile.write(json.dumps(response).encode("utf-8"))
        # æ¨¡æ‹Ÿ kube-apiserver çš„ watch æ¥å£ï¼ˆé’ˆå¯¹èŠ‚ç‚¹ï¼‰
        elif self.path.startswith("/api/v1/watch/nodes"):
            current_time = time.time() - start_time
            # T3é˜¶æ®µï¼šåœ¨å‰20ç§’å†…æ¨¡æ‹Ÿ watch è°ƒç”¨å¾ˆå¿«å…³é—­å¹¶è¿”å›é”™è¯¯
            if current_time < 20:
                self.send_error(500, "Unexpected watch close - watch lasted less than a second and no items received")
            else:
                self._set_headers()
                # T6é˜¶æ®µï¼šè¿”å›ä¸€ä¸ªæ¨¡æ‹Ÿçš„ watch äº‹ä»¶
                event = {
                    "type": "ADDED",
                    "object": {
                        "metadata": {"name": "node-01"},
                        "spec": {"providerID": "simulated://node-01"},
                        "status": {"conditions": [{"type": "Ready", "status": "True"}]},
                        "labels": {"role": "worker", "zone": "us-central1-a"}
                    }
                }
                self.wfile.write(json.dumps(event).encode("utf-8"))
        else:
            self.send_error(404, "Not Found")

def run_server():
    server_address = ('', 8080)
    httpd = HTTPServer(server_address, KubeAPIServerSimulator)
    print("HTTP Server running on port 8080...")
    httpd.serve_forever()

# æ¨¡æ‹Ÿ kubelet å®¢æˆ·ç«¯è¡Œä¸ºï¼Œå®šæ—¶è¯·æ±‚èŠ‚ç‚¹åˆ—è¡¨å’Œ watch æ¥å£ï¼Œä»¥ä¾¿è§‚å¯Ÿç¼“å­˜ä¸åŒæ­¥å’ŒåŒæ­¥çš„å˜åŒ–è¿‡ç¨‹
def simulate_kubelet_behavior():
    import requests
    time.sleep(5)  # å»¶è¿Ÿç­‰å¾… HTTP æœåŠ¡å™¨å¯åŠ¨
    while True:
        try:
            # è¯·æ±‚èŠ‚ç‚¹åˆ—è¡¨æ¥å£
            list_resp = requests.get("http://localhost:8080/api/v1/nodes", timeout=5)
            nodes = list_resp.json().get("items", [])
            if not nodes:
                print(f"[{time.strftime('%X')}] èŠ‚ç‚¹åˆ—è¡¨ä¸ºç©ºï¼Œæ¨¡æ‹Ÿ pod è°ƒåº¦æ—¶å› èŠ‚ç‚¹ä¿¡æ¯ä¸å®Œæ•´è€Œè¢«æ‹’ç»ã€‚")
            else:
                print(f"[{time.strftime('%X')}] èŠ‚ç‚¹åˆ—è¡¨å·²åŒæ­¥ï¼š{nodes}")
            # è¯·æ±‚ watch æ¥å£
            try:
                watch_resp = requests.get("http://localhost:8080/api/v1/watch/nodes", timeout=5)
                if watch_resp.status_code == 200:
                    print(f"[{time.strftime('%X')}] Watch è¿”å›äº‹ä»¶ï¼š{watch_resp.json()}")
                else:
                    print(f"[{time.strftime('%X')}] Watch è¿”å›å¼‚å¸¸çŠ¶æ€ç ï¼š{watch_resp.status_code}")
            except Exception as e:
                print(f"[{time.strftime('%X')}] Watch è¯·æ±‚å¼‚å¸¸ï¼š{e}")
        except Exception as e:
            print(f"[{time.strftime('%X')}] èŠ‚ç‚¹åˆ—è¡¨è¯·æ±‚å¼‚å¸¸ï¼š{e}")
        # æ¯10ç§’é’ŸæŸ¥è¯¢ä¸€æ¬¡ï¼Œè§‚å¯Ÿä»ç©ºåˆ°åŒæ­¥çš„è¿‡ç¨‹
        time.sleep(10)

if __name__ == "__main__":
    # åˆ†ç¦»çº¿ç¨‹å¯åŠ¨ HTTP æ¨¡æ‹ŸæœåŠ¡å™¨
    server_thread = threading.Thread(target=run_server, daemon=True)
    server_thread.start()
    # åœ¨ä¸»çº¿ç¨‹ä¸­æ¨¡æ‹Ÿ kubelet æŸ¥è¯¢è¡Œä¸º
    simulate_kubelet_behavior()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. è¯¥å¤ç°è„šæœ¬ä½¿ç”¨ Python çš„ http.server æ¨¡å—åœ¨æœ¬åœ°ï¼ˆç›‘å¬ 8080 ç«¯å£ï¼‰æ¨¡æ‹Ÿäº†éƒ¨åˆ† kube-apiserver è¡Œä¸ºã€‚å®ƒå®ç°äº†ä¸¤ä¸ª REST æ¥å£ï¼š
   - /api/v1/nodesï¼šç”¨äºè¿”å›èŠ‚ç‚¹åˆ—è¡¨ã€‚åœ¨æœåŠ¡å™¨å¯åŠ¨çš„å‰ 20 ç§’å†…è¿”å›ç©ºåˆ—è¡¨ï¼ˆæ¨¡æ‹Ÿ T2 é˜¶æ®µçš„æƒ…å†µï¼‰ï¼Œä¹‹åè¿”å›ä¸€ä¸ªåŒ…å«å®Œæ•´èŠ‚ç‚¹ä¿¡æ¯çš„åˆ—è¡¨ï¼ˆæ¨¡æ‹Ÿ T6 é˜¶æ®µï¼‰ã€‚
   - /api/v1/watch/nodesï¼šç”¨äºæ¨¡æ‹Ÿ watch æ¥å£ã€‚åœ¨å‰ 20 ç§’å†…ç›´æ¥è¿”å› 500 é”™è¯¯ï¼Œæ¨¡æ‹Ÿ watch è°ƒç”¨æ„å¤–å…³é—­çš„æƒ…å†µï¼›åœ¨ä¹‹åè¿”å›ä¸€æ¬¡æ¨¡æ‹Ÿçš„ watch äº‹ä»¶ï¼ˆæ¨¡æ‹Ÿ T6 é˜¶æ®µï¼‰ã€‚
2. åŒæ—¶ï¼Œè„šæœ¬ä¸­è¿˜æ¨¡æ‹Ÿäº† kubelet å®¢æˆ·ç«¯ï¼Œå®šæ—¶ï¼ˆæ¯ 10 ç§’ï¼‰å‘æ¨¡æ‹Ÿ HTTP æœåŠ¡å™¨å‘èµ·è¯·æ±‚ï¼Œå¹¶æ‰“å°å‡ºå½“å‰èŠ‚ç‚¹åˆ—è¡¨çŠ¶æ€ä»¥åŠ watch æ¥å£çš„è¿”å›æƒ…å†µã€‚è¿™æ ·å¯ä»¥è§‚å¯Ÿåˆ°åœ¨ informer çš„ç¼“å­˜çŠ¶æ€ç”±ç©ºå˜ä¸ºæ­£å¸¸çš„è¿‡ç¨‹ï¼Œä»è€Œå¤ç° issue ä¸­æè¿°çš„é—®é¢˜åœºæ™¯ã€‚
3. å¤ç°è„šæœ¬ä¸»è¦ç”¨äºç ”ç©¶å’Œæœ¬åœ°æµ‹è¯•ï¼Œæ¨¡æ‹Ÿäº†ç”±äºç½‘ç»œå¼‚å¸¸å¯¼è‡´ informer ç¼“å­˜ä¸ä¸€è‡´çš„ç°è±¡ï¼Œä½†å¹¶ä¸ç›´æ¥å¯¹é›†ç¾¤ç¯å¢ƒé€ æˆå½±å“ï¼Œä¹Ÿå¹¶éé’ˆå¯¹å¤–éƒ¨æ”»å‡»è€…è®¾è®¡çš„æ¼æ´åˆ©ç”¨æ ·ä¾‹ã€‚

---


# âš ï¸ å­˜åœ¨ä½é£é™©çš„ Issues (5 ä¸ª)

## Issue #129883 ValidatingAdmissionPolicy causes Namespaces deletion to hang due to race condition

- Issue é“¾æ¥ï¼š[#129883](https://github.com/kubernetes/kubernetes/issues/129883)

### Issue å†…å®¹

#### What happened?

I'm implementing policies using ValidatingAdmissionPolicy and ValidatingAdmissionPolicyBindings using a custom CR as paramKind.

The binding is configured with `.spec.paramRef.parameterNotFoundAction: Deny`

Some of those policies are associated to external resources that may take a bit longer to terminate.

When a namespace is deleted, the custom CR resource is immediately removed, and the policy's target resource is left hanging.

Attempting to remove the resource's finalizer returns a ValidatingAdmissionPolicy error:
```
The persistentvolumeclaims "xxx-pvc" is invalid: : ValidatingAdmissionPolicy 'xxx-vap' with binding 'xxx-vapbind' denied request: failed to configure binding: no params found for policy binding with `Deny` parameterNotFoundAction
```

#### What did you expect to happen?

The resources in the namespace to be deleted along with the namespace

Perhaps the policy should stop acting on the namespace if it is in Terminating state.

#### How can we reproduce it (as minimally and precisely as possible)?

Create Kind cluster with extraMount
```
apiVersion: kind.x-k8s.io/v1alpha4
kind: Cluster
nodes:
  - role: control-plane
    extraMounts:
      - hostPath: <some-path>/foo
        containerPath: /foo
```

Create test namespace
```
> kubectl create ns test
```

Deploy ValidatingAdmissionPolicy, ValidatingAdmissionPolicyBinding and ConfigMap parameter reference
```
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicy
metadata:
  name: policy
spec:
  paramKind:
    apiVersion: v1
    kind: ConfigMap
  matchConstraints:
    resourceRules:
      - apiVersions:
          - v1
        apiGroups:
          - ""
        resources:
          - persistentvolumeclaims
        operations:
          - CREATE
          - UPDATE
  validations:
    - expression: params != null && has(params.data) && has(params.data.storageClasses) && variables.storageClassName == params.data.storageClasses
      messageExpression: '"Namespace " + string(namespaceObject.metadata.name) + " does not have permissions to storageClassName " + variables.storageClassName'
  failurePolicy: Fail
  variables:
    - name: storageClassName
      expression: '"storageClassName" in object.spec ? object.spec.storageClassName : "empty"'
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: binding
spec:
  policyName: policy
  paramRef:
    name: ref
    parameterNotFoundAction: Deny
  validationActions:
    - Deny
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ref
data:
  storageClasses: "standard"
```

Create the PVC
```
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-www
spec:
  storageClassName: standard
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 500Mi
  hostPath:
    path: /foo/
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-www
spec:
  volumeName: pv-www
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Mi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      volumes:
      - name: www
        persistentVolumeClaim:
          claimName: pvc-www
      containers:
      - name: nginx
        image: nginx:1.14.2
        volumeMounts:
        - name: www
          mountPath: /var/www
```

Delete the namespace:
```
> kubectl delete ns test
```

The namespace will hang due to the PVC, attempting to remove the PVC finalizer will reveal the ValidatingAdmissionPolicy error:
```
> kubectl patch pvc pvc-www -p '{"metadata":{"finalizers":[]}}' --type=merge
The persistentvolumeclaims "pvc-www" is invalid: : ValidatingAdmissionPolicy 'policy' with binding 'binding' denied request: failed to configure binding: no params found for policy binding with `Deny` parameterNotFoundAction
```

The PVC is removed (or finalizer can be removed) after configuring the binding with `.spec.paramRef.parameterNotFoundAction: Allow` or removing the ValidatingAdmissionPolicy.



#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.3
Kustomize Version: v5.4.2
Server Version: v1.32.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©  
ï¼ˆæ ¹æ® CVSS 3.1 æ‰“åˆ†ï¼Œè¯¥æ¼æ´ä¸ä¼šç›´æ¥å¯¼è‡´æœªç»æˆæƒçš„è®¿é—®ã€ä»£ç æ‰§è¡Œæˆ–ææƒï¼Œè™½ç„¶å¯èƒ½å¯¼è‡´æ‹’ç»æœåŠ¡ï¼Œä½†å®é™…åˆ©ç”¨éœ€è¦è¾ƒé«˜æƒé™ï¼Œç»¼åˆè¯„ä¼°é£é™©ä¸ºä½é£é™©ï¼‰

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ issue æè¿°äº†åœ¨ä½¿ç”¨ ValidatingAdmissionPolicy å’Œ ValidatingAdmissionPolicyBinding æ—¶å­˜åœ¨çš„ç«æ€æ¡ä»¶é—®é¢˜ï¼Œå¯¼è‡´åœ¨å‘½åç©ºé—´åˆ é™¤è¿‡ç¨‹ä¸­ï¼Œç”±äºè‡ªå®šä¹‰å‚æ•°èµ„æºï¼ˆConfigMapï¼‰è¢«æå‰æ¸…ç†ï¼Œè¿›è€Œä½¿å¾— PVC çš„åˆ é™¤æ“ä½œï¼ˆæˆ–åˆ é™¤å…¶ finalizerï¼‰è¢«æ‹’ç»ï¼Œä»è€Œå¯¼è‡´å‘½åç©ºé—´åˆ é™¤è¿‡ç¨‹æŒ‚èµ·ã€‚è™½ç„¶è¿™ä¸€é—®é¢˜å¯èƒ½ä¼šå¯¼è‡´é›†ç¾¤å†…éƒ¨åˆ†èµ„æºï¼ˆå¦‚å‘½åç©ºé—´ã€PVC ç­‰ï¼‰æ— æ³•åŠæ—¶æ­£å¸¸åˆ é™¤ï¼Œäº§ç”Ÿæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰çš„æ•ˆæœï¼Œä½†é—®é¢˜æœ¬èº«ä»…å½±å“èµ„æºåˆ é™¤æµç¨‹ï¼Œå¹¶éç›´æ¥æš´éœ²æ•æ„Ÿä¿¡æ¯æˆ–å…è®¸è¿œç¨‹å‘½ä»¤æ‰§è¡Œã€ææƒã€å®¹å™¨é€ƒé€¸ç­‰é«˜å®‰å…¨é£é™©æ¼æ´ã€‚å¹¶ä¸”ï¼Œæ­£å¸¸æƒ…å†µä¸‹åªæœ‰å…·å¤‡è¾ƒé«˜æƒé™çš„ç”¨æˆ·æ‰èƒ½é…ç½®ã€åˆ é™¤è¿™äº›èµ„æºï¼Œå› æ­¤æ”»å‡»è€…éœ€è¦ä¸€å®šçš„æƒé™æ‰èƒ½è§¦å‘è¿™ä¸€é—®é¢˜ã€‚ç»¼åˆæ¥çœ‹ï¼Œè¯¥é—®é¢˜æ›´å€¾å‘äºå› é…ç½®æˆ–å®ç°ä¸Šçš„ç«æ€æ¡ä»¶è€Œå¯¼è‡´ DoS çš„é—®é¢˜ï¼Œå…¶å½±å“èŒƒå›´å’Œæ”»å‡»å¤æ‚åº¦å—é™ï¼Œå› æ­¤é£é™©è¯„çº§è¾ƒä½ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
import os
import sys
import time
import threading
import http.server
from kubernetes import client, config, utils
from kubernetes.client.rest import ApiException
import yaml

# å¯åŠ¨ä¸€ä¸ªç®€å•çš„ HTTP æœåŠ¡å™¨ï¼Œç”¨äºæ»¡è¶³è¦æ±‚ï¼ˆç›‘å¬åœ¨8080ç«¯å£ï¼‰
def start_http_server():
    server_address = ('', 8080)
    httpd = http.server.HTTPServer(server_address, http.server.SimpleHTTPRequestHandler)
    print("[HTTP] Server listening on port 8080")
    httpd.serve_forever()

def create_resources_from_yaml(api_client, yaml_text, namespace=None):
    """ä»YAMLæ–‡æœ¬ä¸­åˆ›å»ºèµ„æº"""
    # å°†YAMLæ–‡æœ¬æ‹†åˆ†ï¼Œå¦‚æœæœ‰å¤šä¸ªæ–‡æ¡£åˆ™åˆ†åˆ«åŠ è½½
    for data in yaml.safe_load_all(yaml_text):
        try:
            utils.create_from_dict(api_client, data, namespace=namespace)
            kind = data.get("kind")
            name = data.get("metadata", {}).get("name")
            print(f"[INFO] æˆåŠŸåˆ›å»º {kind}/{name}")
        except ApiException as e:
            print(f"[ERROR] åˆ›å»º {data.get('kind')}/{data.get('metadata', {}).get('name')} å¤±è´¥: {e}")

def main():
    # åŠ è½½kubeconfigï¼ˆé»˜è®¤è·¯å¾„ï¼‰
    try:
        config.load_kube_config()
    except Exception as e:
        print(f"[ERROR] åŠ è½½kubeconfigå¤±è´¥: {e}")
        sys.exit(1)

    api_client = client.ApiClient()
    core_v1 = client.CoreV1Api(api_client)
    apps_v1 = client.AppsV1Api(api_client)

    # å¯åŠ¨ä¸€ä¸ªç®€å•çš„HTTPæœåŠ¡å™¨ï¼ˆç›‘å¬8080ç«¯å£ï¼‰
    http_thread = threading.Thread(target=start_http_server, daemon=True)
    http_thread.start()

    # 1. åˆ›å»ºæµ‹è¯•å‘½åç©ºé—´
    namespace_body = client.V1Namespace(metadata=client.V1ObjectMeta(name="test"))
    try:
        core_v1.create_namespace(namespace_body)
        print("[INFO] åˆ›å»ºå‘½åç©ºé—´ test æˆåŠŸ")
    except ApiException as e:
        if e.status != 409:
            print(f"[ERROR] åˆ›å»ºå‘½åç©ºé—´ test å¤±è´¥: {e}")
            sys.exit(1)
        else:
            print("[INFO] å‘½åç©ºé—´ test å·²å­˜åœ¨")

    # 2. éƒ¨ç½² ValidatingAdmissionPolicyã€ValidatingAdmissionPolicyBinding å’Œ ConfigMap å‚æ•°å¼•ç”¨
    admission_yaml = """
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicy
metadata:
  name: policy
spec:
  paramKind:
    apiVersion: v1
    kind: ConfigMap
  matchConstraints:
    resourceRules:
      - apiVersions:
          - v1
        apiGroups:
          - ""
        resources:
          - persistentvolumeclaims
        operations:
          - CREATE
          - UPDATE
  validations:
    - expression: params != null && has(params.data) && has(params.data.storageClasses) && variables.storageClassName == params.data.storageClasses
      messageExpression: '"Namespace " + string(namespaceObject.metadata.name) + " does not have permissions to storageClassName " + variables.storageClassName'
  failurePolicy: Fail
  variables:
    - name: storageClassName
      expression: '"storageClassName" in object.spec ? object.spec.storageClassName : "empty"'
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: binding
spec:
  policyName: policy
  paramRef:
    name: ref
    parameterNotFoundAction: Deny
  validationActions:
    - Deny
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ref
  namespace: test
data:
  storageClasses: "standard"
"""
    create_resources_from_yaml(api_client, admission_yaml, namespace="test")
    time.sleep(3)  # ç­‰å¾…èµ„æºåˆ›å»º

    # 3. åˆ›å»º PersistentVolume, PersistentVolumeClaim å’Œ Deployment
    resources_yaml = """
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-www
spec:
  storageClassName: standard
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 500Mi
  hostPath:
    path: /foo/
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc-www
  namespace: test
spec:
  volumeName: pv-www
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Mi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  namespace: test
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      volumes:
      - name: www
        persistentVolumeClaim:
          claimName: pvc-www
      containers:
      - name: nginx
        image: nginx:1.14.2
        volumeMounts:
        - name: www
          mountPath: /var/www
"""
    create_resources_from_yaml(api_client, resources_yaml)
    time.sleep(5)  # ç­‰å¾…PVCå’ŒDeploymentå¯åŠ¨

    # 4. åˆ é™¤å‘½åç©ºé—´ï¼Œè§¦å‘è¯¥ç«æ€é—®é¢˜
    try:
        print("[INFO] æ­£åœ¨åˆ é™¤å‘½åç©ºé—´ test ï¼ˆæ­¤æ“ä½œå¯èƒ½ä¼šæŒ‚èµ·ï¼‰...")
        core_v1.delete_namespace("test")
    except ApiException as e:
        print(f"[ERROR] åˆ é™¤å‘½åç©ºé—´ test å¤±è´¥: {e}")

    # ç­‰å¾…ä¸€æ®µæ—¶é—´ä»¥ä¾¿è§‚å¯Ÿå‘½åç©ºé—´åˆ é™¤çŠ¶æ€
    time.sleep(10)

    # 5. å°è¯•åˆ é™¤ PVC çš„ finalizer ä»¥è§‚å¯Ÿ ValidatingAdmissionPolicy é”™è¯¯
    patch_body = {"metadata": {"finalizers": []}}
    try:
        print("[INFO] å°è¯•patch PVC pvc-www å»é™¤ finalizers")
        core_v1.patch_namespaced_persistent_volume_claim(
            name="pvc-www",
            namespace="test",
            body=patch_body)
        print("[INFO] PVC pvc-www patch æˆåŠŸ")
    except ApiException as e:
        print(f"[INFO] é¢„æœŸä¸­æ•è·åˆ° ValidatingAdmissionPolicy é”™è¯¯: {e}")

if __name__ == "__main__":
    main()
```


---


## Issue #129610 `EncryptionConfiguration` mismatch between documentation and validation of provider `aescbc`

- Issue é“¾æ¥ï¼š[#129610](https://github.com/kubernetes/kubernetes/issues/129610)

### Issue å†…å®¹

#### What happened?

AES-CBC is documented to require 32-byte key in the `EncryptionConfiguration` https://github.com/kubernetes/kubernetes/blob/e38489303019d442b87611182eb63c94d6e54f03/staging/src/k8s.io/apiserver/pkg/apis/apiserver/types_encryption.go#L106 and also on the website https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/#providers, however the validation checks for one of `16`, `24`, `32` sizes
https://github.com/kubernetes/kubernetes/blob/e38489303019d442b87611182eb63c94d6e54f03/staging/src/k8s.io/apiserver/pkg/apis/apiserver/validation/validation_encryption.go#L106-L107

#### What did you expect to happen?

I expect that the validation fails when one provides a key of size 16 or 24 bytes or that the documentation gets adapted to the current behaviour.

#### How can we reproduce it (as minimally and precisely as possible)?

Pass a 24-byte sized key for `aescbc` provider in an `EncryptionConfiguration` to the `kube-apiserver`.

#### Anything else we need to know?

_No response_

#### Kubernetes version

This is valid for the current `v1.32.0` version.

#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯ Kubernetes ä¸­é’ˆå¯¹ aescbc åŠ å¯†æä¾›è€…ï¼Œåœ¨ EncryptionConfiguration çš„æ–‡æ¡£ä¸å®é™…æ ¡éªŒé€»è¾‘ä¹‹é—´å­˜åœ¨ä¸ä¸€è‡´ï¼šæ–‡æ¡£ä¸­è¦æ±‚å¿…é¡»ä½¿ç”¨ 32 å­—èŠ‚å¯†é’¥ï¼Œè€Œå®é™…æ ¡éªŒå…è®¸ 16ã€24ã€32 å­—èŠ‚çš„å¯†é’¥ã€‚  
ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¿™é‡Œå…è®¸ä½¿ç”¨ 16 å­—èŠ‚ï¼ˆAES-128ï¼‰æˆ– 24 å­—èŠ‚ï¼ˆAES-192ï¼‰çš„å¯†é’¥ï¼Œå¹¶ä¸æ„å‘³ç€å­˜åœ¨å·²çŸ¥çš„åŠ å¯†å¼±ç‚¹ï¼Œå› ä¸º AES-128 å’Œ AES-192 å‡ä¸ºæ ‡å‡†çš„åŠ å¯†ç®—æ³•ï¼Œä¸ AES-256ï¼ˆ32 å­—èŠ‚ï¼‰ç›¸æ¯”ï¼Œåœ¨ç›®å‰çš„å®é™…æ”»å‡»æ‰‹æ®µä¸‹å¹¶æ²¡æœ‰è¢«è®¤ä¸ºæ˜¯ä¸å®‰å…¨çš„ã€‚  
å› æ­¤ï¼Œæ­¤é—®é¢˜ä¸»è¦è¡¨ç°ä¸ºæ–‡æ¡£ä¸å®é™…è¡Œä¸ºä¸ä¸€è‡´ï¼Œå¯èƒ½å¯¼è‡´ç”¨æˆ·è®¤ä¸ºåªæœ‰ 32 å­—èŠ‚å¯†é’¥æ˜¯å®‰å…¨è¦æ±‚ï¼Œè€Œå®é™…ä¸Šè¾ƒçŸ­çš„å¯†é’¥ä¹Ÿæ˜¯è¢«æ¥å—çš„ã€‚è¯¥é—®é¢˜è™½ç„¶å¯èƒ½å¼•èµ·é…ç½®ä¸Šçš„æ··æ·†æˆ–åˆè§„æ€§ç–‘è™‘ï¼Œä½†å¹¶ä¸ä¼šç›´æ¥å¯¼è‡´æ”»å‡»è€…åˆ©ç”¨ï¼Œä»è€Œé€ æˆä¸¥é‡çš„å®‰å…¨ç ´åã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
æ­¤è„šæœ¬ç”¨äºæ¨¡æ‹Ÿ Kubernetes ä¸­ aescbc åŠ å¯†é…ç½®çš„æ ¡éªŒé€»è¾‘ï¼Œ
éªŒè¯ä¼ å…¥çš„å¯†é’¥ï¼ˆä»¥ Base64 ç¼–ç ï¼‰æ˜¯å¦ç¬¦åˆ 16ã€24ã€32 å­—èŠ‚ä¹‹ä¸€ã€‚

æ³¨æ„ï¼š
- è¯¥è„šæœ¬ä»…ç”¨äºæœ¬åœ°æµ‹è¯•å’Œç ”ç©¶ï¼Œæ¨¡æ‹ŸæœåŠ¡ç›‘å¬åœ¨8080ç«¯å£ã€‚
- åœ¨å®é™… kube-apiserver ä¸­ï¼Œå¯†é’¥è¢«ç”¨ä½œåŠ å¯†æ•°æ®è€Œéä»…ä½œç®€å•æ ¡éªŒã€‚
"""

import base64
import json
from http.server import BaseHTTPRequestHandler, HTTPServer

class EncryptionConfigHandler(BaseHTTPRequestHandler):
    def do_POST(self):
        # ä»…å¤„ç† /encryption è·¯å¾„çš„è¯·æ±‚
        if self.path != '/encryption':
            self.send_error(404, "Not Found")
            return

        content_length = int(self.headers.get('Content-Length', 0))
        post_data = self.rfile.read(content_length)
        try:
            data = json.loads(post_data.decode('utf-8'))
        except Exception as e:
            self.send_error(400, "Invalid JSON")
            return

        provider = data.get("provider")
        # ç›®å‰åªæ”¯æŒ aescbc
        if provider != "aescbc":
            self.send_error(400, "åªæ”¯æŒ provider 'aescbc'")
            return

        key_b64 = data.get("key")
        if not key_b64:
            self.send_error(400, "ç¼ºå°‘ key å‚æ•°")
            return

        try:
            key = base64.b64decode(key_b64)
        except Exception as e:
            self.send_error(400, "key å‚æ•° Base64 è§£ç å¤±è´¥")
            return

        # æ ¡éªŒå¯†é’¥é•¿åº¦ï¼Œå®é™…æ ¡éªŒé€»è¾‘æ”¯æŒ 16ã€24ã€32 å­—èŠ‚
        if len(key) in (16, 24, 32):
            response = {
                "status": "accepted",
                "key_length": len(key),
                "message": "å¯†é’¥é•¿åº¦ç¬¦åˆè¦æ±‚ã€‚"
            }
        else:
            response = {
                "status": "rejected",
                "reason": "å¯†é’¥é•¿åº¦æ— æ•ˆï¼Œå¿…é¡»ä¸º 16, 24 æˆ– 32 å­—èŠ‚ã€‚"
            }

        self.send_response(200)
        self.send_header('Content-Type', 'application/json')
        self.end_headers()
        self.wfile.write(json.dumps(response).encode('utf-8'))

def run_server():
    server_address = ('', 8080)
    httpd = HTTPServer(server_address, EncryptionConfigHandler)
    print("HTTP Server running on port 8080...")
    httpd.serve_forever()

if __name__ == '__main__':
    run_server()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. è¯¥è„šæœ¬åˆ›å»ºäº†ä¸€ä¸ªç®€å•çš„ HTTP æœåŠ¡å™¨ï¼Œç›‘å¬åœ¨æœ¬åœ° 8080 ç«¯å£ï¼Œå¹¶å®šä¹‰äº†ä¸€ä¸ª POST æ¥å£ï¼ˆ/encryptionï¼‰ã€‚  
2. æ¥å£æ¥æ”¶ä¸€ä¸ª JSON æ ¼å¼çš„è¯·æ±‚ï¼Œè¦æ±‚åŒ…å« "provider"ï¼ˆä»…æ”¯æŒ "aescbc"ï¼‰å’Œ "key"ï¼ˆå¿…é¡»æ˜¯ Base64 ç¼–ç çš„å­—ç¬¦ä¸²ï¼‰ã€‚  
3. è„šæœ¬ä¸­å¯¹è§£ç åçš„å¯†é’¥é•¿åº¦è¿›è¡Œäº†æ ¡éªŒï¼Œå¦‚æœé•¿åº¦ä¸º 16ã€24 æˆ– 32 å­—èŠ‚ï¼Œåˆ™è¿”å›â€œacceptedâ€ï¼Œå¦åˆ™è¿”å›â€œrejectedâ€ã€‚  
4. è¯¥å¤ç°è„šæœ¬æ¨¡æ‹Ÿäº† Kubernetes ä¸­å…³äº aescbc æä¾›è€…çš„å¯†é’¥é•¿åº¦æ ¡éªŒé€»è¾‘ï¼Œä»è€Œå±•ç¤ºæ–‡æ¡£ä¸­æè¿°çš„è¦æ±‚ï¼ˆ32 å­—èŠ‚ï¼‰ä¸å®é™…æ ¡éªŒé€»è¾‘ï¼ˆå…è®¸16å’Œ24å­—èŠ‚ï¼‰ä¹‹é—´çš„ä¸ä¸€è‡´æ€§ã€‚  
5. éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ­¤é—®é¢˜ä¸»è¦æ˜¯æ–‡æ¡£æè¿°ä¸ä»£ç é€»è¾‘ä¸ä¸€è‡´ï¼Œä¸å­˜åœ¨ç›´æ¥å¯è¢«è¿œç¨‹æ”»å‡»åˆ©ç”¨çš„å®‰å…¨æ¼æ´ï¼Œå› æ­¤é£é™©è¯„çº§ä¸ºä½é£é™©ã€‚

---


## Issue #129546 pv_controller fails to delete the underlying disk if a PV is marked deleted when it is in use by a pod

- Issue é“¾æ¥ï¼š[#129546](https://github.com/kubernetes/kubernetes/issues/129546)

### Issue å†…å®¹

#### What happened?

Consider the following scenario:
1. There is a running Pod X that uses a PVC Y,
3. Y is bound to a PV Z,
4. Z has `persistentVolumeReclaimPolicy: Delete`,
5. Y and Z are marked for deletion while X is running,
6. X is deleted.

In this case pv_controller disregards the reclaim policy and does not call the CSI driver to remove the underlying virtual disk.

#### What did you expect to happen?

`persistentVolumeReclaimPolicy: Delete` must be respected and the underlying virtual disk must be removed.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a GKE cluster with the default configuration (1 node instead of 3 will do fine).
2. Install Postgresql:
```
helm install pg oci://registry-1.docker.io/bitnamicharts/postgresql --version 16.3.4 -n pg-0 --create-namespace --wait
```
3. Find out which PV is used by Postgresql:
```
$ kubectl get pv | grep "pg-0/"
```
4. Mark Postgresql's PVC and PV for deletion (insert your PV name here):
```
$ kubectl -n pg-0 delete pvc/data-pg-postgresql-0
$ kubectl delete pv/pvc-ea83aa40-38d9-466f-bb3d-80062ce3d1e1
```
5. Delete the namespace that contains Postgresql:
```
$ kubectl delete ns pg-0
```
6. Wait for Postgresql's PV to be deleted as well.
7. Verify that GKE's virtual disk still exists:
```
$ gcloud compute disks list | grep "pvc-ea83aa40-38d9-466f-bb3d-80062ce3d1e1"
pvc-ea83aa40-38d9-466f-bb3d-80062ce3d1e1             europe-north1-a      zone            8        pd-balanced  READY
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.12
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.6-gke.1596000
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ issue æè¿°çš„æ˜¯åœ¨ä»¥ä¸‹åœºæ™¯ä¸‹å‡ºç°çš„é—®é¢˜ï¼š
1. ä¸€ä¸ª Pod æ­£åœ¨è¿è¡Œå¹¶ä½¿ç”¨æŸä¸ª PVCï¼ŒPVC ç»‘å®šäº†ä¸€ä¸ª PVï¼›
2. å½“ PVC å’Œ PV è¢«æ ‡è®°ä¸ºåˆ é™¤æ—¶ï¼ˆå³ä½¿ Pod ä¾ç„¶åœ¨è¿è¡Œæˆ–åˆšè¢«åˆ é™¤ï¼‰ï¼Œå¹¶ä¸”è¯¥ PV çš„ reclaim ç­–ç•¥ä¸º Deleteï¼Œæ­¤æ—¶ pv_controller æ²¡æœ‰æŒ‰ç…§ç­–ç•¥è°ƒç”¨ CSI é©±åŠ¨æ¥åˆ é™¤åº•å±‚çš„ GKE è™šæ‹Ÿç£ç›˜ï¼Œå¯¼è‡´å®é™…èµ„æºæœªè¢«æ¸…ç†ã€‚

ä»å®‰å…¨é£é™©è§’åº¦çœ‹ï¼Œè¿™ä¸ªé—®é¢˜ä¸»è¦é›†ä¸­åœ¨èµ„æºæ¸…ç†ä¸å½»åº•ï¼Œå¯èƒ½å¯¼è‡´å­¤å„¿ç£ç›˜æ®‹ç•™ã€‚è™½ç„¶é•¿æœŸå­˜åœ¨å­¤å„¿ç£ç›˜å¯èƒ½å¼•èµ·æ•°æ®æ®‹ç•™é£é™©ï¼ˆä¾‹å¦‚ï¼Œæ®‹ç•™ç£ç›˜ä¸­åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼Œä½†è¿™ç§ç£ç›˜ä¸€èˆ¬åªæœ‰äº‘å¹³å°ç®¡ç†å‘˜æˆ–é¡¹ç›®æˆå‘˜æœ‰æƒé™è®¿é—®ï¼‰ï¼Œä½†é—®é¢˜æœ¬èº«å¹¶ä¸å…è®¸æœªç»æˆæƒçš„æ”»å‡»è€…ç›´æ¥å‘èµ·è¿œç¨‹æ”»å‡»ã€ææƒã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰é«˜å®‰å…¨é£é™©è¡Œä¸ºã€‚å› æ­¤ï¼Œä»æ”»å‡»é“¾æ¡è®²ï¼Œè¿™ä¸ªé—®é¢˜ä¸»è¦å±äºèµ„æºç®¡ç†å’Œæ¸…ç†ä¸Šçš„ç¼ºé™·ï¼Œå…¶åˆ©ç”¨éœ€è¦äº‘å¹³å°å·²å…·æœ‰ä¸€å®šæƒé™ï¼Œå¹¶ä¸”ä¾èµ–å¹³å°çš„å­˜å–æ§åˆ¶ã€‚ç»¼åˆæ¥çœ‹ï¼Œè¯¥é—®é¢˜åœ¨å®‰å…¨é£é™©è¯„çº§ä¸Šå±äºä½é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
è¯¥è„šæœ¬ç”¨äºæ¨¡æ‹Ÿå¤ç° pv_controller æœªåˆ é™¤åº•å±‚ç£ç›˜çš„é—®é¢˜çš„æ“ä½œæ­¥éª¤ã€‚
æ³¨æ„ï¼šè¯¥è„šæœ¬ä»…ç”¨äºç ”ç©¶ä¸æœ¬åœ°æµ‹è¯•ï¼Œä¸”ä»…æ¨¡æ‹Ÿé€»è¾‘æ­¥éª¤ï¼Œä¸ä¼šç›´æ¥æ“ä½œç”Ÿäº§ç¯å¢ƒã€‚
å®é™…ç¯å¢ƒä¸­éœ€è¦ä½¿ç”¨ kubectlã€helmã€gcloud ç­‰å‘½ä»¤å·¥å…·ï¼Œæ­¤è„šæœ¬é€šè¿‡ subprocess è°ƒç”¨è¿™äº›å‘½ä»¤ï¼Œè¯·ç¡®ä¿ç¯å¢ƒä¸­å·²æ­£ç¡®é…ç½®ç›¸åº”å‡­æ®ï¼ˆæ¯”å¦‚ kubeconfigã€gcloud å‡­æ®ä¿¡æ¯å‡åœ¨é»˜è®¤ä½ç½®ï¼‰ã€‚

è­¦å‘Šï¼šè¯·å‹¿åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ç›´æ¥è¿è¡Œè¯¥è„šæœ¬ï¼Œæœ¬è„šæœ¬ä»…ä¾›æµ‹è¯•ç¯å¢ƒéªŒè¯ä½¿ç”¨ã€‚
"""

import subprocess
import time
import sys
import http.server
import socketserver
import threading

# ç®€å•çš„ HTTP æœåŠ¡ï¼Œç”¨äºæ¨¡æ‹Ÿç¯å¢ƒä¸­å¯èƒ½éœ€è¦çš„HTTPè¯·æ±‚åœºæ™¯ï¼ˆè‹¥éœ€è¦ï¼‰
def start_http_server():
    port = 8080
    Handler = http.server.SimpleHTTPRequestHandler
    with socketserver.TCPServer(("", port), Handler) as httpd:
        print(f"Serving HTTP on port {port} ...")
        httpd.serve_forever()

# æ‰§è¡Œ shell å‘½ä»¤çš„è¾…åŠ©å‡½æ•°
def run_cmd(cmd, capture_output=False):
    try:
        print("æ‰§è¡Œå‘½ä»¤:", cmd)
        result = subprocess.run(cmd, shell=True, check=True, capture_output=capture_output, text=True)
        if capture_output:
            return result.stdout.strip()
    except subprocess.CalledProcessError as e:
        print("å‘½ä»¤æ‰§è¡Œå¤±è´¥:", e)
        sys.exit(1)

def main():
    # å¯åŠ¨ä¸€ä¸ªåå° HTTP æœåŠ¡å™¨çº¿ç¨‹ï¼ˆä»…ä½œç¤ºä¾‹ï¼Œä¸å½±å“å¤ç°æµç¨‹ï¼‰
    http_thread = threading.Thread(target=start_http_server, daemon=True)
    http_thread.start()
    
    # æ­¥éª¤1ï¼šåˆ›å»º GKE é›†ç¾¤çš„ç¤ºä¾‹ï¼ˆè¿™é‡Œä»…è¾“å‡ºæç¤ºä¿¡æ¯ï¼Œå®é™…é›†ç¾¤åˆ›å»ºéœ€è¦åœ¨ GCP æ§åˆ¶å°æˆ– gcloud ä¸­é…ç½®ï¼‰
    print("è¯·ç¡®ä¿å·²åˆ›å»º GKE é›†ç¾¤ï¼Œå¹¶é…ç½®å¥½ kubectl è¿æ¥åˆ°è¯¥é›†ç¾¤ã€‚")
    
    # æ­¥éª¤2ï¼šå®‰è£… Postgresql ç¤ºä¾‹
    print("ä½¿ç”¨ Helm å®‰è£… Postgresql ...")
    helm_install_cmd = (
        "helm install pg oci://registry-1.docker.io/bitnamicharts/postgresql "
        "--version 16.3.4 -n pg-0 --create-namespace --wait"
    )
    run_cmd(helm_install_cmd)
    
    # æ­¥éª¤3ï¼šè·å– Postgresql ä½¿ç”¨çš„ PVï¼ˆè¿™é‡Œå‡è®¾ PVC å‘½åä¸­å¸¦æœ‰ 'pg' æ ‡è¯†ï¼Œç”¨æˆ·éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´è¿‡æ»¤æ¡ä»¶ï¼‰
    print("è·å– Postgresql ä½¿ç”¨çš„ PV ...")
    pv_list_cmd = "kubectl get pv | grep 'pg-'"
    pv_info = run_cmd(pv_list_cmd, capture_output=True)
    print("æ£€ç´¢åˆ°çš„ PV ä¿¡æ¯:\n", pv_info)
    
    # æç¤ºç”¨æˆ·è¾“å…¥éœ€æ“ä½œçš„ PV åç§°ï¼ˆæ ¹æ®è¾“å‡ºè‡ªè¡Œç¡®è®¤ï¼‰
    pv_name = input("è¯·è¾“å…¥è¦åˆ é™¤çš„ PV åç§°ï¼ˆä¾‹å¦‚ pvc-xxxxxxï¼‰ï¼š").strip()
    
    # æ­¥éª¤4ï¼šåˆ é™¤å¯¹åº” PVC å’Œ PV
    print("åˆ é™¤ Postgresql çš„ PVC...")
    pvc_delete_cmd = "kubectl -n pg-0 delete pvc/data-pg-postgresql-0"
    run_cmd(pvc_delete_cmd)
    
    print("åˆ é™¤ PV {} ...".format(pv_name))
    pv_delete_cmd = f"kubectl delete pv/{pv_name}"
    run_cmd(pv_delete_cmd)
    
    # æ­¥éª¤5ï¼šåˆ é™¤å‘½åç©ºé—´
    print("åˆ é™¤å‘½åç©ºé—´ pg-0 ...")
    ns_delete_cmd = "kubectl delete ns pg-0"
    run_cmd(ns_delete_cmd)
    
    print("ç­‰å¾…æ•°åç§’ä»¥è®©èµ„æºåˆ é™¤å®Œæˆ...")
    time.sleep(60)
    
    # æ­¥éª¤6ï¼šæ£€æŸ¥åº•å±‚ GKE è™šæ‹Ÿç£ç›˜æ˜¯å¦ä»ç„¶å­˜åœ¨ï¼ˆè¯»å– gcloud ç»“æœï¼‰
    print("éªŒè¯ GKE çš„è™šæ‹Ÿç£ç›˜æ˜¯å¦ä»å­˜åœ¨ ...")
    check_disk_cmd = f"gcloud compute disks list --format='table(name,zone,status)' | grep '{pv_name}'"
    disk_info = run_cmd(check_disk_cmd, capture_output=True)
    if disk_info:
        print("å¤ç°æˆåŠŸï¼šè™šæ‹Ÿç£ç›˜ä»å­˜åœ¨, ä¿¡æ¯å¦‚ä¸‹ï¼š")
        print(disk_info)
    else:
        print("æœªæ£€å‡ºç›¸å…³è™šæ‹Ÿç£ç›˜åˆ é™¤é—®é¢˜ï¼Œè¯·æ£€æŸ¥é›†ç¾¤åŠäº‘å¹³å°æ—¥å¿—ã€‚")
    
if __name__ == '__main__':
    main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. æœ¬è„šæœ¬é€šè¿‡è°ƒç”¨ Helmã€kubectlã€gcloud å‘½ä»¤æ¥æ¨¡æ‹Ÿ issue ä¸­æåˆ°çš„å¤ç°æµç¨‹ï¼Œæ­¥éª¤ä¸ issue ä¸­æè¿°çš„æ“ä½œé¡ºåºä¸€è‡´ï¼š  
   - é¦–å…ˆå®‰è£… Postgresql åº”ç”¨ï¼›
   - è·å–å…¶ä½¿ç”¨çš„ PV ä¿¡æ¯ï¼›
   - åˆ é™¤å¯¹åº”çš„ PVCã€PV ä»¥åŠæ‰€åœ¨çš„å‘½åç©ºé—´ï¼›
   - ç­‰å¾…ä¸€æ®µæ—¶é—´åæ£€æŸ¥åº•å±‚ GKE è™šæ‹Ÿç£ç›˜æ˜¯å¦ä¾ç„¶å­˜åœ¨ã€‚
2. è„šæœ¬ä¸­è¿˜å¯åŠ¨äº†ä¸€ä¸ªç®€å•çš„ HTTP æœåŠ¡å™¨ç›‘å¬ 8080 ç«¯å£ï¼Œç”¨äºæ»¡è¶³â€œå¦‚æœéœ€è¦è®¿é—® HTTP æœåŠ¡å™¨â€çš„éœ€æ±‚ï¼Œæ­¤å¤„ä»…ä½œç¤ºä¾‹ç”¨é€”ï¼Œå¹¶ä¸ç›´æ¥å½±å“å¤ç°æµç¨‹ã€‚
3. ä¸ºä¿è¯è„šæœ¬çš„å®‰å…¨æ€§ï¼Œæ‰€æœ‰æ•æ„Ÿçš„é…ç½®ï¼ˆå¦‚ kubeconfigã€gcloud å‡­æ®ï¼‰å‡å‡è®¾åœ¨é»˜è®¤ä½ç½®é…ç½®å¥½ã€‚
4. è¯·æ³¨æ„ï¼Œæ­¤è„šæœ¬ä»…ç”¨äºç ”ç©¶ä¸æœ¬åœ°æµ‹è¯•ï¼Œåˆ‡å‹¿åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ç›´æ¥è¿è¡Œï¼Œä»¥å…è¯¯æ“ä½œé€ æˆèµ„æºæŸå¤±ã€‚

ç»¼ä¸Šï¼Œè¯¥ issue æ‰€åæ˜ çš„é—®é¢˜åœ¨å®‰å…¨æ”»å‡»é“¾æ¡ä¸­é£é™©è¾ƒä½ï¼Œä¸æ„æˆé«˜é£é™©å®‰å…¨æ¼æ´ã€‚

---


## Issue #129550 FUSE mounts in emptyDir volumes cannot be cleaned

- Issue é“¾æ¥ï¼š[#129550](https://github.com/kubernetes/kubernetes/issues/129550)

### Issue å†…å®¹

#### What happened?

This issue is to summarize some conversation towards the end of https://github.com/kubernetes/kubernetes/issues/7890, as requested by @thockin. 

If an application in a privileged pod or container creates a FUSE mount in an emptyDir volume, but fails to unmount it before terminating (either due to that being a conscious choice by the application, or due to a SIGKILL from kubernetes), the kubelet will fail to clean up the pod. A recurring error will appear in the kubelet logs, and the pod will remain in API.

Here is an example error log from the kubelet during cleanup:
```
Jan 08 19:06:04 <hostname omitted> kubelet[12511]: E0108 19:06:04.507950   12511 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/empty-dir/30b506e8-b18a-4d5c-bf7d-17fbae54a5d0-worker podName:30b506e8-b18a-4d5c-bf7d-17fbae54a5d0 nodeName:}" failed. No retries permitted until 2025-01-08 19:08:06.507933266 +0000 UTC m=+1437.970062341 (durationBeforeRetry 2m2s). Error: UnmountVolume.TearDown failed for volume "worker" (UniqueName: "kubernetes.io/empty-dir/30b506e8-b18a-4d5c-bf7d-17fbae54a5d0-worker") pod "30b506e8-b18a-4d5c-bf7d-17fbae54a5d0" (UID: "30b506e8-b18a-4d5c-bf7d-17fbae54a5d0") : openfdat /var/lib/kubelet/pods/30b506e8-b18a-4d5c-bf7d-17fbae54a5d0/volumes/kubernetes.io~empty-dir/worker/build: transport endpoint is not connected
```

The offending code seems to be here: https://github.com/kubernetes/kubernetes/blob/release-1.31/pkg/volume/emptydir/empty_dir.go#L490-L495

When cleaning up emptyDirs, we start with `os.RemoveAll`, as it's recursing through the directory, it will eventually try to inspect the contents of the FUSE mount, which will result in an error. 

#### What did you expect to happen?

The kubelet is able to clean the pod up eventually.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Run a privileged container that can generate a FUSE mount within an emptydir
2. Configure the application to not clean the FUSE mount, or forcefully terminate the pod so the mount cannot be cleaned.

#### Anything else we need to know?

I'd be happy to try and put a patch together to address this with a little guidance. It seems like we should be able to inspect for any mounts beneath the empty directory's `MetaDir` and `umount` them before we attempt to call the `os.RemoveAll`. 

#### Kubernetes version

<details>
```console
Client Version: v1.30.5
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.3-eks-56e63d8
```
</details>


#### Cloud provider

<details>
AWS EKS
</details>


#### OS version

<details>
```console
# On Linux:
$ cat /etc/os-release
NAME=Bottlerocket
ID=bottlerocket
VERSION="1.29.0 (aws-k8s-1.31)"
PRETTY_NAME="Bottlerocket OS 1.29.0 (aws-k8s-1.31)"
VARIANT_ID=aws-k8s-1.31
VERSION_ID=1.29.0
BUILD_ID=c55d099c
HOME_URL="https://github.com/bottlerocket-os/bottlerocket"
SUPPORT_URL="https://github.com/bottlerocket-os/bottlerocket/discussions"
BUG_REPORT_URL="https://github.com/bottlerocket-os/bottlerocket/issues"
DOCUMENTATION_URL="https://bottlerocket.dev"
$ uname -a
# paste output here
Linux <hostname omitted> 6.1.119 #1 SMP Thu Dec 12 20:00:51 UTC 2024 aarch64 GNU/Linux
```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
æ ¹æ® Issue æè¿°ï¼Œè¯¥é—®é¢˜å¯¼è‡´çš„æ˜¯ä¸€ç§èµ„æºæ¸…ç†ä¸å½»åº•ã€å¯¼è‡´ Pod æŒ‚èµ·çš„ç°è±¡ï¼Œè™½å¯èƒ½å¼•å‘æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰é—®é¢˜ï¼Œä½†éœ€è¦åº”ç”¨ä»¥ç‰¹æƒæ¨¡å¼è¿è¡Œï¼Œå¹¶ä¸»åŠ¨æˆ–éä¸»åŠ¨è§¦å‘ FUSE æŒ‚è½½è€Œä¸å¸è½½ã€‚ä»æ”»å‡»é¢æ¥çœ‹ï¼Œæ”»å‡»è€…éœ€æ‹¥æœ‰è¾ƒé«˜æƒé™ï¼ˆä¾‹å¦‚è°ƒåº¦ç‰¹æƒå®¹å™¨æˆ–ç‰¹æƒå®¹å™¨å†…éƒ¨æ‰§è¡Œæ¶æ„æ“ä½œï¼‰ï¼Œå› æ­¤æœ¬é—®é¢˜æ•´ä½“é£é™©è¯„çº§ä¸ºä½é£é™©ã€‚

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æƒ…å†µæ˜¯ï¼šåœ¨ç‰¹æƒå®¹å™¨ä¸­ï¼Œå¦‚æœåº”ç”¨ç¨‹åºä½¿ç”¨äº† FUSE æŒ‚è½½åˆ° emptyDir å·ä¸‹ï¼Œå¹¶åœ¨é€€å‡ºå‰æœªæ‰§è¡Œå¸è½½æ“ä½œï¼ˆå¯èƒ½æ˜¯ç”±æœ‰æ„æˆ– SIGKILL å¯¼è‡´ï¼‰ï¼Œåˆ™ kubelet åœ¨æ¸…ç† Pod æ—¶ï¼Œè°ƒç”¨ os.RemoveAll é€’å½’åˆ é™¤ç›®å½•æ—¶ä¼šå› ä¸ºè®¿é—® FUSE æŒ‚è½½ç‚¹è€ŒæŠ¥é”™ï¼ˆå¦‚â€œtransport endpoint is not connectedâ€ï¼‰ï¼Œå¯¼è‡´è¯¥ Pod æ— æ³•æ­£å¸¸æ¸…ç†å¹¶æŒç»­ä¿ç•™åœ¨ API ä¸­ã€‚è¯¥é—®é¢˜æœ¬èº«ä¸»è¦å½±å“ Pod æ¸…ç†å’Œèµ„æºé‡Šæ”¾ï¼Œå¯èƒ½åœ¨ä¸€å®šèŒƒå›´å†…å¼•èµ·èµ„æºæ³„éœ²æˆ–æ‹’ç»æœåŠ¡ç°è±¡ï¼Œä½†ä»…é’ˆå¯¹å…·æœ‰ç‰¹æƒçš„ Pod æˆ–å®¹å™¨æ‰å¯ä»¥äº§ç”Ÿè¿™ç§æƒ…å†µã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
å¤ç°è¯¥é—®é¢˜çš„POCæ€è·¯ï¼š
1. åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿ emptyDir å·çš„ç›®å½•ï¼›
2. åœ¨ emptyDir ä¸‹åˆ›å»ºä¸€ä¸ªå­ç›®å½•ä½œä¸º FUSE æŒ‚è½½ç‚¹ï¼›
3. é€šè¿‡ fusepy å®ç°ä¸€ä¸ªç®€å•çš„ FUSE æ–‡ä»¶ç³»ç»Ÿï¼Œå¹¶å°†å…¶æŒ‚è½½åˆ°ä¸Šè¿°æŒ‚è½½ç‚¹ï¼›
4. é€šè¿‡å¤šè¿›ç¨‹å¯åŠ¨ FUSE æ–‡ä»¶ç³»ç»Ÿï¼Œå¹¶åœ¨æŒ‚è½½æˆåŠŸåï¼Œæ¨¡æ‹Ÿå¼‚å¸¸ç»ˆæ­¢ï¼ˆæ¨¡æ‹Ÿåº”ç”¨æœªå¸è½½ FUSE æŒ‚è½½çš„æƒ…å†µï¼‰ï¼›
5. åœ¨ FUSE è¿›ç¨‹è¢«å¼ºåˆ¶ç»“æŸåï¼Œå°è¯•ä½¿ç”¨ shutil.rmtree åˆ é™¤åŒ…å« FUSE æŒ‚è½½çš„ç›®å½•ï¼Œè§‚å¯Ÿæ˜¯å¦å¤ç°åˆ é™¤å¤±è´¥ï¼ˆæŠ¥é”™â€œtransport endpoint is not connectedâ€ï¼‰çš„ç°è±¡ã€‚
æ³¨æ„ï¼šæœ¬è„šæœ¬ä»…ç”¨äºæœ¬åœ°æµ‹è¯•å’Œç ”ç©¶ï¼Œè¯·ç¡®ä¿ç¯å¢ƒä¸­å·²å®‰è£… fusepyï¼Œå¹¶ä¸”å½“å‰ç”¨æˆ·å…·æœ‰ FUSE æŒ‚è½½æƒé™ã€‚
"""

import os
import sys
import time
import shutil
import errno
import logging
from multiprocessing import Process
from fuse import FUSE, Operations

# é…ç½®æ—¥å¿—è¾“å‡º
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s: %(message)s')

class DummyFS(Operations):
    """
    ä¸€ä¸ªæœ€ç®€ç‰ˆçš„ FUSE æ–‡ä»¶ç³»ç»Ÿï¼Œä»…æ”¯æŒæ ¹ç›®å½•å±æ€§å’Œç›®å½•è¯»å–æ“ä½œ
    """
    def getattr(self, path, fh=None):
        st = dict(
            st_mode=(0o40755),
            st_nlink=2,
            st_size=0,
            st_ctime=time.time(),
            st_mtime=time.time(),
            st_atime=time.time()
        )
        return st

    def readdir(self, path, fh):
        return ['.', '..']

def run_fuse(mountpoint):
    """
    åœ¨æŒ‡å®šæŒ‚è½½ç‚¹å¯åŠ¨ FUSE æ–‡ä»¶ç³»ç»Ÿï¼Œforeground æ¨¡å¼è¿è¡Œ
    """
    try:
        logging.info("FUSEæ–‡ä»¶ç³»ç»Ÿå°†æŒ‚è½½åœ¨ %s", mountpoint)
        # foreground=True ä¿æŒè¿è¡Œï¼Œnotrreads=True ç®€åŒ–å¤šçº¿ç¨‹é—®é¢˜
        FUSE(DummyFS(), mountpoint, foreground=True, nothreads=True)
    except Exception as e:
        logging.error("FUSE æŒ‚è½½å¼‚å¸¸: %s", e)

def main():
    # å®šä¹‰æ¨¡æ‹Ÿ emptyDir å·çš„åŸºç¡€ç›®å½•
    base_dir = os.path.abspath("./emptydir_vol")
    fuse_mount_point = os.path.join(base_dir, "fuse_mount")

    # åˆ›å»ºç›®å½•ç»“æ„ï¼Œæ¨¡æ‹Ÿ emptyDir å·å’ŒæŒ‚è½½ç‚¹
    try:
        os.makedirs(fuse_mount_point, exist_ok=True)
        logging.info("åˆ›å»ºç›®å½•ç»“æ„: %s", fuse_mount_point)
    except Exception as e:
        logging.error("åˆ›å»ºç›®å½•å¤±è´¥: %s", e)
        sys.exit(1)

    # å¯åŠ¨ FUSE æŒ‚è½½ï¼Œä½¿ç”¨å¤šè¿›ç¨‹å¯åŠ¨ä»¥æ–¹ä¾¿åç»­ç»ˆæ­¢
    fuse_process = Process(target=run_fuse, args=(fuse_mount_point,))
    fuse_process.start()

    # ç­‰å¾…2ç§’ï¼Œç¡®ä¿FUSEæŒ‚è½½å·²ç»å¯åŠ¨
    time.sleep(2)

    # æ£€æŸ¥æ˜¯å¦å·²ç»æŒ‚è½½ï¼Œå¯é€šè¿‡æ£€æŸ¥ fuse_mount_point ä¸­çš„æ–‡ä»¶æˆ–è°ƒç”¨ 'mount' å‘½ä»¤ï¼ˆè¿™é‡Œç®€å•ç­‰å¾…ï¼‰
    logging.info("FUSE æŒ‚è½½é¢„è®¡å·²å®Œæˆã€‚ç°åœ¨æ¨¡æ‹Ÿå¼‚å¸¸é€€å‡ºï¼ˆæœªå¸è½½ FUSEï¼‰ã€‚")

    # æ¨¡æ‹Ÿå¼‚å¸¸ç»ˆæ­¢ FUSE è¿›ç¨‹ï¼Œç›¸å½“äºåº”ç”¨å¼‚å¸¸é€€å‡ºæ²¡æœ‰æ‰§è¡Œ umount
    fuse_process.terminate()
    fuse_process.join()
    logging.info("å·²ç»ˆæ­¢ FUSE è¿›ç¨‹ï¼Œä½†æŒ‚è½½ç‚¹å¯èƒ½å°šæœªè¢«æ­£ç¡®å¸è½½ã€‚")

    # å»¶æ—¶ç­‰å¾…ç³»ç»Ÿæ›´æ–°æŒ‚è½½çŠ¶æ€
    time.sleep(1)

    # å°è¯•åˆ é™¤æ•´ä¸ª emptyDir å·ï¼ˆå³ base_dirï¼‰ï¼Œè§‚å¯Ÿåˆ é™¤æ—¶çš„é”™è¯¯ä¿¡æ¯
    logging.info("å°è¯•æ¸…ç†ç›®å½•: %s", base_dir)
    try:
        shutil.rmtree(base_dir)
        logging.info("ç›®å½•åˆ é™¤æˆåŠŸã€‚")
    except Exception as e:
        # å¦‚æœæ•è·åˆ°é”™è¯¯ï¼Œåˆ™è¾“å‡ºé¢„æœŸçš„é”™è¯¯ä¿¡æ¯
        if hasattr(e, 'errno'):
            if e.errno == errno.ENOTCONN:
                logging.error("åˆ é™¤å¤±è´¥ï¼Œé”™è¯¯: transport endpoint is not connected (ENOTCONN)")
            else:
                logging.error("åˆ é™¤å¤±è´¥ï¼Œé”™è¯¯ç : %s, é”™è¯¯: %s", e.errno, e)
        else:
            logging.error("åˆ é™¤å¤±è´¥ï¼Œé”™è¯¯: %s", e)

if __name__ == "__main__":
    main()
```


---


## Issue #129467 WatchList use more temp memory than legacy ListWatch

- Issue é“¾æ¥ï¼š[#129467](https://github.com/kubernetes/kubernetes/issues/129467)

### Issue å†…å®¹

#### What happened?

there are 9812 pods in kube-systemï¼Œ we open different number of informers to test memory usageï¼š
```
./pod_informer -count=10 -namespace=kube-system -timeout=2m -kubeconfig=/etc/kubernetes/kubeconfig -enableWatchListFeature=false -v=4
./pod_informer -count=20 -namespace=kube-system -timeout=2m -kubeconfig=/etc/kubernetes/kubeconfig -enableWatchListFeature=false -v=4
./pod_informer -count=30 -namespace=kube-system -timeout=2m -kubeconfig=/etc/kubernetes/kubeconfig -enableWatchListFeature=false -v=4
./pod_informer -count=40 -namespace=kube-system -timeout=2m -kubeconfig=/etc/kubernetes/kubeconfig -enableWatchListFeature=false -v=4

```

![watchlist-off](https://github.com/user-attachments/assets/2cc7401a-d374-4e37-8b3e-576e0e9e2352)


```
./pod_informer -count=10 -namespace=kube-system -timeout=2m -kubeconfig=/etc/kubernetes/kubeconfig -enableWatchListFeature=true -v=4
./pod_informer -count=20 -namespace=kube-system -timeout=2m -kubeconfig=/etc/kubernetes/kubeconfig -enableWatchListFeature=true -v=4
./pod_informer -count=30 -namespace=kube-system -timeout=2m -kubeconfig=/etc/kubernetes/kubeconfig -enableWatchListFeature=true -v=4
./pod_informer -count=40 -namespace=kube-system -timeout=2m -kubeconfig=/etc/kubernetes/kubeconfig -enableWatchListFeature=true -v=4
```
![watchlist-on](https://github.com/user-attachments/assets/6fd55242-b268-43fd-a04e-0e009610cf5f)

we can seeï¼Œ only 30 informers ï¼Œ we lost the apiserver

#### What did you expect to happen?

WatchList use less temp memory than legacy ListWatch

#### How can we reproduce it (as minimally and precisely as possible)?

we have all 670722 pods in watchcache 

etcdctl --cacert=/etc/kubernetes/certs/etcd/Etcdtrustca.crt --cert=/etc/kubernetes/certs/etcd/etcd-client.crt --key=/etc/kubernetes/certs/etcd/etcd-client.key --endpoints=$etcd_eps get /registry/pods --prefix --keys-only --command-timeout=10s| grep /registry/pods | wc -l
670722


the pod_informer binary code:
```
/*
Copyright 2023 The Kubernetes Authors.
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
    http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package main

import (
    "context"
    "flag"
    "os"
    "time"

    "k8s.io/apimachinery/pkg/util/wait"
    v1 "k8s.io/client-go/informers/core/v1"
    "k8s.io/client-go/kubernetes"
    "k8s.io/client-go/tools/cache"
    "k8s.io/client-go/tools/clientcmd"
    "k8s.io/klog/v2"
)

var (
    kubeconfig                  string
    targetNamespace             string
    informerCount               int
    testTimeout                 time.Duration
    enableWatchListAlphaFeature bool
)

func main() {
    registerFlags()
    flag.Parse()

    klog.Info("The test binary started with the following arguments:")
    flag.VisitAll(func(f *flag.Flag) {
       klog.Infof("  -%s=%v (%s)\n", f.Name, f.Value, f.Usage)
    })

    if enableWatchListAlphaFeature {
       os.Setenv("ENABLE_CLIENT_GO_WATCH_LIST_ALPHA", "true")
    }

    ctx, cancel := context.WithTimeout(context.Background(), testTimeout)
    defer cancel()
    config, err := clientcmd.BuildConfigFromFlags("", kubeconfig)
    if err != nil {
       klog.Fatal(err)
    }
    config.AcceptContentTypes = "application/vnd.kubernetes.protobuf,application/json"
    config.ContentType = "application/vnd.kubernetes.protobuf"
	config.QPS = 100
	config.Burst = 200
    klog.Infof("The following Kubernetes client config will be used\n%v", config.String())

    client, err := kubernetes.NewForConfig(config)
    if err != nil {
       klog.Fatal(err)
    }


       ts := time.Now()
       ctxInformer, cancelInformers := context.WithCancel(ctx)
       defer cancelInformers()

       klog.Infof("Starting %d pod informers for targetNamespace = %s", informerCount, targetNamespace)
       informersSynced := startInformersFor(ctxInformer, client, informerCount, targetNamespace)

       klog.Info("Waiting for the pod informers to synced")
       cache.WaitForCacheSync(ctx.Done(), informersSynced...)
       klog.Infof("All %v pod informers synced, time needed = %v", len(informersSynced), time.Now().Sub(ts))


    klog.Info("Exiting the test app")
}

func registerFlags() {
    klog.InitFlags(flag.CommandLine)

    flag.StringVar(&kubeconfig, "kubeconfig", "", "path to kubeconfig.")
    flag.StringVar(&targetNamespace, "namespace", "", "namespace that host pods to list. If empty a default ('') value will be used.")
    flag.IntVar(&informerCount, "count", 4, "the number of informers per targetNamespace to run. If empty a default (4) value will be used.")
    flag.DurationVar(&testTimeout, "timeout", time.Minute, "timeout duration for the test")
    flag.BoolVar(&enableWatchListAlphaFeature, "enableWatchListFeature", false, "whether to set ENABLE_CLIENT_GO_WATCH_LIST_ALPHA env var")
}

func startInformersFor(ctx context.Context, client kubernetes.Interface, count int, namespace string) []cache.InformerSynced {
    var informersSynced []cache.InformerSynced
    for i := 0; i < count; i++ {
       inf := v1.NewPodInformer(client, namespace, time.Duration(0), cache.Indexers{})
       inf.SetTransform(func(interface{}) (interface{}, error) {
          return nil, nil
       })
       informersSynced = append(informersSynced, inf.HasSynced)
       go inf.Run(ctx.Done())
    }
    return informersSynced
}
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.32.0
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é£é™©è¯„çº§ä¸ºä½é£é™©ã€‚  
åŸå› æ˜¯ï¼š  
1. è™½ç„¶è¯¥é—®é¢˜å¯èƒ½å¯¼è‡´ apiserver å› å†…å­˜èµ„æºè€—å°½è€Œä¸å¯ç”¨ï¼Œä½†æ”»å‡»è€…éœ€è¦å…·å¤‡ç›¸åº”çš„è®¿é—®æƒé™ï¼ˆè¯»å– kube-system pods åˆ—è¡¨ï¼‰ï¼Œè¿™ç§æƒé™é€šå¸¸åªæˆäºˆä¿¡ä»»ç”¨æˆ·ã€‚  
2. å¦‚æœæ”»å‡»è€…æ— æ³•ç›´æ¥è·å¾—è¶³å¤Ÿæƒé™å‘èµ·è¯¥æ”»å‡»ï¼Œå…¶åˆ©ç”¨éš¾åº¦è¾ƒå¤§ï¼Œæ‰€ä»¥åœ¨ CVSS è¯„åˆ†ä¸­è¾ƒéš¾è¾¾åˆ° high çº§åˆ«ã€‚

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ issue æè¿°äº†åœ¨å¯ç”¨ watchlist åŠŸèƒ½æ—¶ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„ ListWatch æ–¹å¼ï¼Œä½¿ç”¨çš„ä¸´æ—¶å†…å­˜æ›´å¤šï¼Œæœ€ç»ˆå¯¼è‡´ apiserver åœ¨å¼€å¯ä¸€å®šæ•°é‡ informer åå´©æºƒï¼ˆæ¯”å¦‚ç¤ºä¾‹ä¸­ä»…å¯åŠ¨ 30 ä¸ª informer å³å¯¼è‡´ apiserver â€œä¸¢å¤±â€ï¼‰ã€‚é—®é¢˜æœ¬èº«åæ˜ çš„æ˜¯æ–°åŠŸèƒ½åœ¨èµ„æºæ¶ˆè€—ä¸Šçš„å¼‚å¸¸ï¼Œå¯èƒ½ä¼šå¼•å‘æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰é—®é¢˜ï¼Œå³åˆ©ç”¨ watchlist åŠŸèƒ½è®© apiserver å†…å­˜å ç”¨è¿‡é«˜ï¼Œè¿›è€Œé€ æˆé›†ç¾¤æ§åˆ¶å¹³é¢ä¸å¯ç”¨ã€‚ä½†éœ€è¦æ³¨æ„ï¼Œè¯¥é—®é¢˜äº§ç”Ÿçš„å‰ææ˜¯éœ€è¦å…·æœ‰å¯¹ apiserver è¿›è¡Œè¯»æ“ä½œçš„æƒé™ï¼ˆä¾‹å¦‚å¯¹ kube-system å‘½åç©ºé—´çš„ pods è¿›è¡Œè¯»å–ï¼‰ï¼Œè¿™é€šå¸¸éœ€è¦è¾ƒé«˜æƒé™æˆ–å·²ç»ä¿¡ä»»çš„è°ƒç”¨è€…æ‰èƒ½æ‰§è¡Œæ­¤æ“ä½œã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
import threading
import time
import sys
from kubernetes import client, config, watch
from http.server import HTTPServer, BaseHTTPRequestHandler

# HTTP Serverç”¨äºç›‘å¬8080ç«¯å£
class SimpleHTTPRequestHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.end_headers()
        message = "This is a simple HTTP server for testing."
        self.wfile.write(message.encode("utf-8"))

def start_http_server():
    server_address = ('', 8080)
    httpd = HTTPServer(server_address, SimpleHTTPRequestHandler)
    print("HTTP server is starting at port 8080")
    httpd.serve_forever()

def pod_watcher(namespace, watcher_id, run_time_seconds):
    try:
        # ä»é»˜è®¤ä½ç½®åŠ è½½ kubeconfig é…ç½®
        config.load_kube_config()
    except Exception as e:
        print(f"çº¿ç¨‹ {watcher_id}: æ— æ³•åŠ è½½ kubeconfig: {e}")
        return

    v1 = client.CoreV1Api()
    w = watch.Watch()

    end_time = time.time() + run_time_seconds
    print(f"çº¿ç¨‹ {watcher_id} å¯åŠ¨ï¼Œå¯¹å‘½åç©ºé—´ {namespace} å¼€å§‹watch PODäº‹ä»¶")
    while time.time() < end_time:
        try:
            # ä½¿ç”¨ watch æµ‹è¯•æµå¼è·å–podåˆ—è¡¨
            stream = w.stream(v1.list_namespaced_pod, namespace=namespace, timeout_seconds=60)
            for event in stream:
                # å¤„ç†äº‹ä»¶æ—¶ä»…ç®€å•æ‰“å°ä¸€ä¸‹ä¿¡æ¯ï¼Œå‡å°‘é¢å¤–å¤„ç†
                obj = event.get("object")
                evt_type = event.get("type")
                print(f"çº¿ç¨‹ {watcher_id} äº‹ä»¶: {evt_type} pod: {getattr(obj.metadata, 'name', 'unknown')}")
                # å¦‚è¾¾åˆ°ç»“æŸæ—¶é—´å°±é€€å‡º
                if time.time() >= end_time:
                    w.stop()
                    break
        except Exception as e:
            print(f"çº¿ç¨‹ {watcher_id} é‡åˆ°å¼‚å¸¸: {e}")
            time.sleep(1)

def main():
    import argparse
    parser = argparse.ArgumentParser(description="WatchListå†…å­˜æ¶ˆè€—å¤ç°è„šæœ¬ï¼Œä»…ç”¨äºæœ¬åœ°æµ‹è¯•å’Œç ”ç©¶")
    parser.add_argument("--namespace", type=str, default="kube-system", help="ç›‘æ§çš„å‘½åç©ºé—´")
    parser.add_argument("--count", type=int, default=30, help="å¯åŠ¨ watcher çš„æ•°é‡")
    parser.add_argument("--runtime", type=int, default=120, help="è¿è¡Œæ—¶é•¿ï¼ˆç§’ï¼‰")
    args = parser.parse_args()

    threads = []

    # å¯åŠ¨HTTPæœåŠ¡ï¼Œç›‘å¬8080ç«¯å£
    http_thread = threading.Thread(target=start_http_server, daemon=True)
    http_thread.start()

    # å¯åŠ¨å¤šä¸ª watcher çº¿ç¨‹
    for i in range(args.count):
        t = threading.Thread(target=pod_watcher, args=(args.namespace, i+1, args.runtime))
        threads.append(t)
        t.start()
        # ä¸ºäº†æ›´å¿«å¤ç°å†…å­˜å ç”¨é—®é¢˜ï¼Œå¯ä»¥æ§åˆ¶é—´éš”éå¸¸çŸ­
        time.sleep(0.5)

    # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
    for t in threads:
        t.join()

if __name__ == "__main__":
    main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. æœ¬è„šæœ¬ä½¿ç”¨ Kubernetes Python å®¢æˆ·ç«¯è¯»å–é»˜è®¤ä½ç½®çš„ kubeconfig æ–‡ä»¶ï¼Œå¹¶åœ¨æŒ‡å®šå‘½åç©ºé—´ï¼ˆé»˜è®¤ä¸º kube-systemï¼‰ä¸­åˆ›å»ºå¤šä¸ª watcher çº¿ç¨‹ï¼Œæ¨¡æ‹ŸæŒç»­ç›‘æ§ pod çš„äº‹ä»¶ã€‚  
2. æ¯ä¸ªçº¿ç¨‹ä½¿ç”¨ watch API æµå¼è·å– pod åˆ—è¡¨å¹¶æ‰“å°äº‹ä»¶ä¿¡æ¯ï¼Œä»¥æ¨¡æ‹Ÿ watchlist æœºåˆ¶ä¸‹çš„å†…å­˜å’Œèµ„æºæ¶ˆè€—æƒ…æ™¯ã€‚  
3. å¦å¤–ï¼Œè„šæœ¬ä¸­å»ºç«‹äº†ä¸€ä¸ªç›‘å¬ 8080 ç«¯å£çš„ HTTP æœåŠ¡å™¨ï¼ˆåŸºäº BaseHTTPRequestHandlerï¼‰ï¼Œæ¨¡æ‹Ÿé€šè¿‡ HTTP ç«¯å£æä¾›è¾…åŠ©æœåŠ¡ï¼Œæ»¡è¶³è¦æ±‚ä¸­ â€œå¦‚æœéœ€è¦è®¿é—®HTTPæœåŠ¡å™¨åˆ™åœ¨è„šæœ¬ä¸­åˆ›å»ºä¸€ä¸ªHTTPæœåŠ¡å™¨ï¼Œç›‘å¬åœ¨8080ç«¯å£â€ã€‚  
4. æ­¤å¤ç°è„šæœ¬ä»…ç”¨äºæœ¬åœ°æµ‹è¯•å’Œç ”ç©¶ç›®çš„ï¼Œè¯·å‹¿åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ç›´æ¥ä½¿ç”¨ã€‚  
5. æ ¹æ®å‘½ä»¤è¡Œå‚æ•°ï¼Œä½ å¯ä»¥è°ƒæ•´ watcher çš„æ•°é‡ï¼ˆ--countï¼‰å’Œæ€»çš„è¿è¡Œæ—¶é•¿ï¼ˆ--runtimeï¼‰æ¥å¤ç°å’Œè§‚æµ‹èµ„æºæ¶ˆè€—è¡Œä¸ºã€‚å¦‚æœå¤§é‡ watcher åŒæ—¶è¿è¡Œï¼Œå¯èƒ½ä¼šå¯¼è‡´ kube-apiserver å†…å­˜ä½¿ç”¨å¢é«˜ï¼Œä»è€ŒéªŒè¯ issue ä¸­æè¿°çš„ç°è±¡ã€‚

---


# âœ… ä¸æ¶‰åŠå®‰å…¨é£é™©çš„ Issues (49 ä¸ª)

## Issue #129935 CSI migration with in-tree azure-file provider fails to mount PVC to Pod

- Issue é“¾æ¥ï¼š[#129935](https://github.com/kubernetes/kubernetes/issues/129935)

### Issue å†…å®¹

#### What happened?

When mounting PVC to the Pod the volume will not mount and it will hang on
```
MountVolume.SetUp failed for volume "pvc-d9734ad3-26f5-4fb9-8918-7643d2bf8c75" : kubernetes.io/csi: mounter.SetUpAt failed to get service accoount token attributes: failed to fetch token: serviceaccounts "default" is forbidden: audience "api://AzureADTokenExchange" not found in pod spec volume
```


#### What did you expect to happen?

Volume mounted successfully with Pod running.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create Azure cluster with Kubernetes 1.32
2. Create PV claim
```yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: my-pv-claim 
provisioner: kubernetes.io/azure-file
parameters:
  skuName: Standard_LRS 
reclaimPolicy: Delete
volumeBindingMode: Immediate
```
3. Create Pod and mount PVC

#### Anything else we need to know?

The error comes from https://github.com/kubernetes/kubernetes/blob/master/plugin/pkg/admission/noderestriction/admission.go#L654
which was introduced in https://github.com/kubernetes/kubernetes/pull/128077

Admission plugin will work for CSI
https://github.com/openshift/kubernetes/blob/master/plugin/pkg/admission/noderestriction/admission.go#L689-L690

But when using CSI migration with in-tree azure-file provider the PVC fails to mount.
https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/api/core/v1/types.go#L141

CSIdriver contents
```yaml
apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
  annotations:
    csi.openshift.io/managed: "true"
    operator.openshift.io/spec-hash: 5c303d28061bd64a18ff9b79bd6b231293b3c1295db37e91d59160efe4611365
  creationTimestamp: "2025-01-31T13:10:35Z"
  labels:
    security.openshift.io/csi-ephemeral-volume-profile: privileged
  name: file.csi.azure.com
  resourceVersion: "9671"
  uid: 15b2ad34-60dc-4765-97cb-b91613dfa6d7
spec:
  attachRequired: false
  fsGroupPolicy: None
  podInfoOnMount: true
  requiresRepublish: false
  seLinuxMount: false
  storageCapacity: false
  tokenRequests:
  - audience: api://AzureADTokenExchange
  volumeLifecycleModes:
  - Persistent
  - Ephemeral 
```

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.1
Kustomize Version: v5.4.2
Server Version: v1.32.1
```

</details>


#### Cloud provider

<details>
Azure
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
ServiceAccountNodeAudienceRestriction admission plugin - https://github.com/kubernetes/kubernetes/blob/295ed0a607922af27841121b8c7e95961fcb4d79/plugin/pkg/admission/noderestriction/admission.go#L637-L657
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯åœ¨ä½¿ç”¨ CSI è¿ç§»ä¸ in-tree azure-file æä¾›ç¨‹åºæ—¶ï¼ŒPod çš„ PVC æŒ‚è½½å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯è¡¨æ˜åœ¨è·å– service account token æ—¶å‡ºç°é—®é¢˜ï¼Œå…·ä½“æ˜¯å› ä¸ºé¢„æœŸçš„ audience "api://AzureADTokenExchange" æœªåœ¨ Pod spec ä¸­æ‰¾åˆ°ã€‚è¿™ç§é”™è¯¯ä¸»è¦åæ˜ äº† Kubernetes Admission æ’ä»¶å’ŒæœåŠ¡è´¦æˆ· Token é…ç½®ä¸Šçš„é—®é¢˜ï¼Œå¯¼è‡´ PVC æ— æ³•æ­£ç¡®æŒ‚è½½ã€‚æ€»ä½“æ¥çœ‹ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸å­˜å‚¨æŒ‚è½½æµç¨‹å’Œé…ç½®æœ‰å…³çš„ç¼ºé™·ï¼Œè€Œéç›´æ¥å½±å“ç³»ç»Ÿå®‰å…¨æ€§çš„æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
import time
import traceback
from kubernetes import client, config, watch

def create_storage_class(api_instance):
    sc_manifest = {
        "apiVersion": "storage.k8s.io/v1",
        "kind": "StorageClass",
        "metadata": {"name": "my-pv-claim"},
        "provisioner": "kubernetes.io/azure-file",
        "parameters": {"skuName": "Standard_LRS"},
        "reclaimPolicy": "Delete",
        "volumeBindingMode": "Immediate"
    }
    try:
        api_instance.create_storage_class(sc_manifest)
        print("StorageClass å·²åˆ›å»ºã€‚")
    except client.exceptions.ApiException as e:
        if e.status == 409:
            print("StorageClass å·²å­˜åœ¨ã€‚")
        else:
            print("åˆ›å»º StorageClass æ—¶å‡ºé”™ï¼š", e)
            traceback.print_exc()

def create_pvc(api_instance, namespace="default"):
    pvc_manifest = {
        "apiVersion": "v1",
        "kind": "PersistentVolumeClaim",
        "metadata": {"name": "test-pvc"},
        "spec": {
            "accessModes": ["ReadWriteOnce"],
            "resources": {"requests": {"storage": "1Gi"}},
            "storageClassName": "my-pv-claim"
        }
    }
    try:
        api_instance.create_namespaced_persistent_volume_claim(namespace, pvc_manifest)
        print("PVC å·²åˆ›å»ºã€‚")
    except client.exceptions.ApiException as e:
        if e.status == 409:
            print("PVC å·²å­˜åœ¨ã€‚")
        else:
            print("åˆ›å»º PVC æ—¶å‡ºé”™ï¼š", e)
            traceback.print_exc()

def create_pod(api_instance, namespace="default"):
    pod_manifest = {
        "apiVersion": "v1",
        "kind": "Pod",
        "metadata": {"name": "test-pod"},
        "spec": {
            "containers": [{
                "name": "tester",
                "image": "nginx",  # ä½¿ç”¨è½»é‡çº§é•œåƒ
                "volumeMounts": [{"mountPath": "/mnt", "name": "test-volume"}]
            }],
            "volumes": [{
                "name": "test-volume",
                "persistentVolumeClaim": {"claimName": "test-pvc"}
            }]
        }
    }
    try:
        api_instance.create_namespaced_pod(namespace, pod_manifest)
        print("Pod å·²åˆ›å»ºã€‚")
    except client.exceptions.ApiException as e:
        if e.status == 409:
            print("Pod å·²å­˜åœ¨ã€‚")
        else:
            print("åˆ›å»º Pod æ—¶å‡ºé”™ï¼š", e)
            traceback.print_exc()

def monitor_pod_events(namespace="default", pod_name="test-pod"):
    config.load_kube_config()
    v1 = client.CoreV1Api()
    w = watch.Watch()
    print("å¼€å§‹ç›‘æ§ Pod äº‹ä»¶ï¼Œå¦‚æŒ‚è½½å¤±è´¥å°†è®°å½•é”™è¯¯ä¿¡æ¯...")
    try:
        for event in w.stream(v1.list_namespaced_event, namespace=namespace, timeout_seconds=120):
            involved_object = event["object"].involved_object
            if involved_object.kind == "Pod" and involved_object.name == pod_name:
                print(f"äº‹ä»¶ç±»å‹: {event['type']}, ä¿¡æ¯: {event['object'].message}")
    except Exception as e:
        print("ç›‘æ§äº‹ä»¶æ—¶å‘ç”Ÿå¼‚å¸¸ï¼š", e)

def main():
    try:
        config.load_kube_config()
        storage_api = client.StorageV1Api()
        core_api = client.CoreV1Api()
    except Exception as e:
        print("åŠ è½½ kubeconfig å¤±è´¥ï¼Œè¯·ç¡®è®¤é»˜è®¤ä½ç½®å­˜åœ¨ kubeconfig æ–‡ä»¶ã€‚")
        return

    # åˆ›å»º StorageClassï¼ŒPVC å’Œ Pod
    create_storage_class(storage_api)
    time.sleep(2)
    create_pvc(core_api)
    time.sleep(2)
    create_pod(core_api)

    # ç›‘æ§ Pod çš„äº‹ä»¶ï¼Œåˆ¤æ–­æ˜¯å¦æœ‰ç±»ä¼¼é”™è¯¯ä¿¡æ¯å‡ºç°
    monitor_pod_events()

if __name__ == "__main__":
    main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. æœ¬è„šæœ¬ä½¿ç”¨ Kubernetes Python å®¢æˆ·ç«¯ä»é»˜è®¤ä½ç½®åŠ è½½ kubeconfigï¼Œå‡è®¾ç¯å¢ƒå·²ç»é…ç½®å¥½å¯¹é›†ç¾¤çš„è®¿é—®æƒé™ã€‚
2. è„šæœ¬é¦–å…ˆé€šè¿‡ StorageV1Api åˆ›å»ºä¸€ä¸ªåä¸º my-pv-claim çš„ StorageClassï¼Œå…¶ provisioner ä¸º "kubernetes.io/azure-file"ï¼Œä»¥åŠç›¸å…³å‚æ•°ï¼Œè¿™ä¸ issue ä¸­çš„é…ç½®ç±»ä¼¼ã€‚
3. æ¥ç€é€šè¿‡ CoreV1Api åˆ›å»ºä¸€ä¸ª PVC å¹¶ç»‘å®šåˆ°è¯¥ StorageClassï¼Œç„¶ååˆ›å»ºä¸€ä¸ª Podï¼Œå¹¶å°†ä¹‹å‰åˆ›å»ºçš„ PVC æŒ‚è½½åˆ°å®¹å™¨å†…çš„ /mnt ç›®å½•ã€‚
4. æœ€åä½¿ç”¨ watch ç›‘æ§è¯¥ Pod ç›¸å…³çš„äº‹ä»¶ï¼Œå¦‚æŒ‚è½½è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼ˆä¾‹å¦‚å‡ºç°ä¸ issue ä¸­ç±»ä¼¼çš„é”™è¯¯ä¿¡æ¯ï¼‰ï¼Œä¼šåœ¨ç»ˆç«¯æ‰“å°å‡ºæ¥ã€‚
5. æ³¨æ„ï¼šç”±äºè¯¥é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ç”±äº CSI è¿ç§»ä¸­ Azure File æä¾›ç¨‹åºçš„é…ç½®é—®é¢˜å¼•èµ·çš„ï¼Œå¹¶ä¸ä¼šå¯¼è‡´å®‰å…¨æ¼æ´ï¼Œæœ¬è„šæœ¬ä»…ç”¨äºå¤ç°å­˜å‚¨æŒ‚è½½å¤±è´¥çš„æƒ…å†µï¼Œä»¥ä¾¿éªŒè¯æ˜¯å¦å­˜åœ¨åŒæ ·çš„æŒ‚è½½é”™è¯¯ï¼Œä¸å­˜åœ¨å®‰å…¨é£é™©ã€‚

---


## Issue #129925 Deleting and recreating a DaemonSet in a short period can cause both the old and new Pods to coexist simultaneously, which may lead to data corruption

- Issue é“¾æ¥ï¼š[#129925](https://github.com/kubernetes/kubernetes/issues/129925)

### Issue å†…å®¹

#### What happened?

When performing `kubectl delete -f <daemonset-file-name>` followed by `kubectl create  -f <daemonset-file-name` with the same DaemonSet definition, if a new Pod from the newly created DaemonSet starts before the Pod from the old DaemonSet is terminated, the Pods from the new and old DaemonSets with the same definition may coexist.

And in cases where the Pod mounts storage, as shown in the example, this results in both Pods simultaneously mounting the same volume, potentially leading to data corruption.

#### What did you expect to happen?

We want to ensure that a single node does not have multiple Pods created from the same DaemonSet definition simultaneously. 

The cause of the above issue is that the current kubectl delete command, by default, deletes the DaemonSet itself first, followed by its Pods. This allows a new DaemonSet to be created immediately after the old DaemonSet is deleted, and the Pods of the new DaemonSet can also be created shortly thereafter. As a result, two Pods from the same DaemonSet definition can exist simultaneously.

We can work this out by using some kubectl delete options (for example specify --cascade=foreground).

But we would like to avoid this situation by using the â€œdefault optionsâ€ of kubectl delete. Making general users specify options every time is easy to make a mistake.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Deploy DaemonSet with the following procedure

```
$ kubectl create namespace test-daemonset
namespace/test-daemonset created
$ cat test-daemonset.yaml 
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: test
  name: test
  namespace: test-daemonset
spec:
  selector:
    matchLabels:
      app: test
  template:
    metadata:
      labels:
        app: test
    spec:
      containers:
      - command: ["/bin/sh", "-c"]
        args:
        - |
          #!/bin/sh
          set -u

          while :
          do
            echo "$(hostname): $(date)" >> /tmp/test/data.log
            sleep 1
          done
        image: busybox:latest
        imagePullPolicy: Always
        name: cli
        securityContext:
          privileged: true
        volumeMounts:
        - mountPath: /tmp/test
          name: hostdir
          readOnly: false
      volumes:
      - name: hostdir
        hostPath:
          path: /tmp/test
          type: DirectoryOrCreate
      nodeSelector:
        node-role.kubernetes.io/worker: "" 

$ kubectl create -f test-daemonset.yaml
daemonset.apps/test created

$ kubectl get pods -n test-daemonset -o wide
NAME         READY   STATUS    RESTARTS   AGE   IP            NODE                 NOMINATED NODE   READINESS GATES
test-2ljhz   1/1     Running   0          4s    10.244.0.20   kind-control-plane   <none>           <none>

$ kubectl exec -it test-2ljhz -n test-daemonset -- /bin/sh
/ # cat /tmp/test/data.log
test-2ljhz: Tue Jan 28 08:34:37 UTC 2025
test-2ljhz: Tue Jan 28 08:34:38 UTC 2025
test-2ljhz: Tue Jan 28 08:34:39 UTC 2025
test-2ljhz: Tue Jan 28 08:34:40 UTC 2025
test-2ljhz: Tue Jan 28 08:34:41 UTC 2025
test-2ljhz: Tue Jan 28 08:34:42 UTC 2025
test-2ljhz: Tue Jan 28 08:34:43 UTC 2025
test-2ljhz: Tue Jan 28 08:34:44 UTC 2025
```

2. Delete and recreate a DaemonSet in a short period
```
$ kubectl delete -f test-daemonset.yaml; kubectl create -f test-daemonset.yaml
daemonset.apps "test" deleted
daemonset.apps/test created
```

After that, you can observe two Pods on a single node
```
$ kubectl get pods -n test-daemonset -o wide
NAME         READY   STATUS        RESTARTS   AGE    IP            NODE                 NOMINATED NODE   READINESS GATES
test-2ljhz   1/1     Terminating   0          2m8s   10.244.0.20   kind-control-plane   <none>           <none>
test-km8zl   1/1     Running       0          18s    10.244.0.21   kind-control-plane   <none>           <none>
```
And both Pods are using the same volume 
```
$ kubectl exec -it test-km8zl -n test-daemonset -- /bin/sh
/ # cat /tmp/test/data.log
test-2ljhz: Tue Jan 28 08:35:38 UTC 2025
test-km8zl: Tue Jan 28 08:35:39 UTC 2025
test-2ljhz: Tue Jan 28 08:35:39 UTC 2025
test-km8zl: Tue Jan 28 08:35:40 UTC 2025
test-2ljhz: Tue Jan 28 08:35:40 UTC 2025
test-km8zl: Tue Jan 28 08:35:41 UTC 2025
test-2ljhz: Tue Jan 28 08:35:41 UTC 2025
test-km8zl: Tue Jan 28 08:35:42 UTC 2025
```


#### Anything else we need to know?

We actually observed the above issue using an operator rather than the CLI. 

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.3
Kustomize Version: v5.4.2
Server Version: v1.31.2
```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯åœ¨çŸ­æ—¶é—´å†…åˆ é™¤å¹¶é‡æ–°åˆ›å»ºåŒä¸€ä¸ª DaemonSet å¯èƒ½å¯¼è‡´è€ Pod ä¸æ–° Pod åŒæ—¶å­˜åœ¨ï¼Œå¹¶ä¸”å¦‚æœè¿™äº› Pod æŒ‚è½½äº†ç›¸åŒçš„å­˜å‚¨å·ï¼Œåˆ™ä¼šå‡ºç°æ•°æ®å†²çªï¼Œç”šè‡³æ•°æ®æŸåçš„é—®é¢˜ã€‚é—®é¢˜åŸå› åœ¨äºé»˜è®¤ä½¿ç”¨çš„ â€œkubectl deleteâ€ æ“ä½œä¼šå…ˆåˆ é™¤ DaemonSet å¯¹è±¡ï¼Œå†å¼‚æ­¥åˆ é™¤å…¶ Podï¼Œä»è€Œå¯¼è‡´æ–° DaemonSet åœ¨æ—§ Pod è¢«å®Œå…¨æ¸…ç†å‰å°±å¼€å§‹è°ƒåº¦æ–° Podã€‚è¯¥é—®é¢˜è™½ç„¶ä¼šå¼•èµ·æ•°æ®ä¸€è‡´æ€§é—®é¢˜å’Œæ½œåœ¨çš„æ•°æ®æŸåï¼Œä½†å®ƒå¹¶ä¸æ¶‰åŠæœªæˆæƒè®¿é—®ã€ä»£ç æ³¨å…¥ã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸æˆ–ææƒç­‰å®‰å…¨ç±»æ¼æ´ï¼Œæ›´å¤šçš„æ˜¯ä¸€ç§è¿ç»´æ“ä½œä¸Šçš„ç«æ€é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
è¯´æ˜ï¼šè¯¥å¤ç°è„šæœ¬ä¸»è¦ç”¨äºæ¨¡æ‹Ÿæ“ä½œæµç¨‹ï¼Œå¹¶æ‰“å°å‡ºæ¨¡æ‹Ÿçš„ä¿¡æ¯ï¼Œ
      å› ä¸ºè¯¥ Issue ä¸ç›´æ¥æ¶‰åŠå®‰å…¨æ¼æ´ï¼Œæ‰€ä»¥ä¸æä¾›çœŸæ­£åˆ©ç”¨å®‰å…¨æ¼æ´çš„POCä»£ç ï¼Œ
      è„šæœ¬ä»…ä¾›äº†è§£æ“ä½œæµç¨‹å’Œå¤ç°ç°è±¡æ—¶å‚è€ƒä½¿ç”¨ã€‚
"""

import subprocess
import time

def run_command(cmd):
    print(f"æ‰§è¡Œå‘½ä»¤: {cmd}")
    proc = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    if proc.stdout:
        print("æ ‡å‡†è¾“å‡º:")
        print(proc.stdout)
    if proc.stderr:
        print("é”™è¯¯è¾“å‡º:")
        print(proc.stderr)
    return proc.returncode

def create_namespace(namespace):
    run_command(f"kubectl create namespace {namespace}")

def apply_daemonset(file_path):
    run_command(f"kubectl create -f {file_path}")

def delete_daemonset(file_path):
    run_command(f"kubectl delete -f {file_path}")

def get_pods(namespace):
    run_command(f"kubectl get pods -n {namespace} -o wide")

def main():
    namespace = "test-daemonset"
    daemonset_yaml = "test-daemonset.yaml"

    print("=== Step 1: åˆ›å»ºå‘½åç©ºé—´å’Œ DaemonSet ===")
    create_namespace(namespace)
    apply_daemonset(daemonset_yaml)
    time.sleep(5)
    get_pods(namespace)

    print("\n=== Step 2: çŸ­æ—¶é—´å†…åˆ é™¤å¹¶é‡æ–°åˆ›å»º DaemonSet ===")
    delete_daemonset(daemonset_yaml)
    # æ­¤å¤„ä¸ç­‰å¾…æ—§ Pod å®Œå…¨ç»ˆæ­¢ï¼Œæ¨¡æ‹ŸçŸ­æ—¶é—´å†…é‡æ–°åˆ›å»ºçš„åœºæ™¯
    time.sleep(2)
    apply_daemonset(daemonset_yaml)
    time.sleep(5)
    get_pods(namespace)

if __name__ == "__main__":
    main()
```


---


## Issue #129890 Server-side apply can cause silent failure when attempting to remove an immutable field that is managed

- Issue é“¾æ¥ï¼š[#129890](https://github.com/kubernetes/kubernetes/issues/129890)

### Issue å†…å®¹

#### What happened?

Steps to reproduce:
- using FluxCD for gitops
- modify existing Service, which has `clusterIP: none`, by removing `clusterIP`
- clusterIP is an immutable field, and changing (incl. removing) it should cause a failure, which should then cause FluxCD to replace the resource
- but when the field is managed by FluxCD, server-side apply will fail silently, and FluxCD will not see any failure. (the exact details are unclear to me)

Why this is relevant:
- developers cannot be expected to know which fields are immutable
- silent failure can cause problems to be undiscovered
- without an error message, it is hard to understand why this behavior is happening
- manual intervention is required, which in turn can cause a buildup of issues over time that are even more complex to debug
- gitops must ensure correct reconciliation between desired and actual state, and report any differences
- when managing large number of resources, manual intervention is untenable

Related issue:
- https://github.com/fluxcd/flux2/issues/5162#issuecomment-2621289274



#### What did you expect to happen?

.

#### How can we reproduce it (as minimally and precisely as possible)?

.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.31.4
Kustomize Version: v5.4.2
Server Version: v1.31.2
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥issueæè¿°çš„æ˜¯åœ¨ä½¿ç”¨FluxCDè¿›è¡Œgitopsç®¡ç†æ—¶ï¼Œå½“å°è¯•é€šè¿‡serverâ€side applyä¿®æ”¹ä¸€ä¸ªåŒ…å«ä¸å¯å˜å­—æ®µï¼ˆä¾‹å¦‚Serviceçš„clusterIPå­—æ®µï¼‰çš„èµ„æºé…ç½®æ—¶ï¼Œä¼šå‡ºç°é™é»˜å¤±è´¥çš„é—®é¢˜ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå½“ç”¨æˆ·è¯•å›¾åˆ é™¤æˆ–ä¿®æ”¹ä¸€ä¸ªå—ä¸å¯å˜é™åˆ¶çš„å­—æ®µæ—¶ï¼Œæœ¬åº”è¿”å›é”™è¯¯ä½†æ˜¯å´æ²¡æœ‰æŠ¥è­¦ï¼Œå¯¼è‡´FluxCDæ— æ³•æ£€æµ‹åˆ°å˜æ›´å¤±è´¥ï¼Œä»è€Œå¯èƒ½å¯¼è‡´å®é™…èµ„æºä¸æœŸæœ›é…ç½®ä¸ä¸€è‡´ã€‚è¯¥é—®é¢˜æœ¬è´¨ä¸Šå±äºæ“ä½œæˆ–é…ç½®ä¸Šçš„é—®é¢˜ï¼Œå¯èƒ½å¯¼è‡´é…ç½®æ¼‚ç§»å’Œæ‰‹åŠ¨å¹²é¢„ï¼Œä½†å¹¶ä¸ä¼šç›´æ¥å¼•èµ·æœªæˆæƒè®¿é—®ã€æƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œæˆ–å…¶ä»–ä¸¥é‡çš„å®‰å…¨å½±å“ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
è¿™ä¸ªè„šæœ¬ç”¨äºæ¨¡æ‹Ÿåœ¨Kubernetesä¸­ä½¿ç”¨kubectl server-side applyæ—¶ï¼Œ
å°è¯•ç§»é™¤ä¸€ä¸ªä¸å¯å˜çš„å­—æ®µ (æ¯”å¦‚Serviceçš„clusterIP) å¯èƒ½å¯¼è‡´é™é»˜å¤±è´¥çš„æƒ…å†µã€‚
è¯·æ³¨æ„ï¼Œè¿™ä¸ªè„šæœ¬ä»…ç”¨äºç ”ç©¶å’Œæœ¬åœ°æµ‹è¯•ï¼Œä¸æ„æˆå®‰å…¨æ¼æ´åˆ©ç”¨ä»£ç ã€‚

å‰ææ¡ä»¶ï¼š
1. éœ€è¦åœ¨é»˜è®¤ä½ç½®å­˜åœ¨kubeconfigæ–‡ä»¶ï¼ˆ~/.kube/configï¼‰ã€‚
2. ç¯å¢ƒä¸­å·²å®‰è£…kubectlå¹¶é…ç½®äº†å¯¹Kubernetesé›†ç¾¤çš„è®¿é—®æƒé™ã€‚
"""

import os
import subprocess
import time

def simulate_server_side_apply():
    # å®šä¹‰ä¸¤ä¸ªyamlé…ç½®æ–‡ä»¶åç§°
    service_yaml = "service.yaml"
    patched_yaml = "service_patch.yaml"

    # åˆå§‹åˆ›å»ºä¸€ä¸ªServiceèµ„æºï¼ŒåŒ…å«ä¸å¯å˜å­—æ®µclusterIP: "None"
    service_definition = """
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  clusterIP: "None"
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
"""
    with open(service_yaml, "w") as f:
        f.write(service_definition)

    try:
        print("[*] æ­£åœ¨åˆ›å»ºServiceèµ„æº...")
        subprocess.run(["kubectl", "apply", "-f", service_yaml], check=True)
        print("[+] Serviceèµ„æºåˆ›å»ºæˆåŠŸ")
    except subprocess.CalledProcessError as e:
        print("[-] åˆ›å»ºServiceèµ„æºå¤±è´¥:", e)
        return

    # ç­‰å¾…ä¸€ä¼šç¡®ä¿èµ„æºåˆ›å»ºå®Œæˆ
    time.sleep(2)

    # ä¿®æ”¹é…ç½®ï¼šå°è¯•ç§»é™¤clusterIPå­—æ®µ
    patched_definition = """
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
"""
    with open(patched_yaml, "w") as f:
        f.write(patched_definition)

    try:
        print("[*] æ­£åœ¨ä½¿ç”¨ server-side apply ç§»é™¤ä¸å¯å˜å­—æ®µ clusterIP...")
        subprocess.run(["kubectl", "apply", "--server-side", "-f", patched_yaml], check=True)
        print("[+] å°è¯•ç§»é™¤clusterIPå·²æ‰§è¡Œï¼ˆé¢„æœŸåº”å½“æ”¶åˆ°é”™è¯¯æç¤ºï¼Œä½†å¯èƒ½é™é»˜å¤±è´¥ï¼‰")
    except subprocess.CalledProcessError as e:
        print("[-] ä¿®æ”¹Serviceèµ„æºå¤±è´¥, é”™è¯¯è¯¦æƒ…:", e)
        return

    # è·å–å½“å‰Serviceçš„é…ç½®ï¼Œæ£€æŸ¥clusterIPå­—æ®µæ˜¯å¦ä¾ç„¶å­˜åœ¨
    try:
        result = subprocess.run(["kubectl", "get", "service", "my-service", "-o", "yaml"], capture_output=True, text=True, check=True)
        print("[*] å½“å‰Serviceé…ç½®å¦‚ä¸‹ï¼š")
        print(result.stdout)
    except subprocess.CalledProcessError as e:
        print("[-] è·å–Serviceèµ„æºå¤±è´¥:", e)

if __name__ == "__main__":
    simulate_server_side_apply()
```


**è§£é‡Šè¯´æ˜ï¼š**

ä»¥ä¸Šè„šæœ¬é€šè¿‡ä»¥ä¸‹æ­¥éª¤æ¨¡æ‹Ÿäº†è¯¥é—®é¢˜çš„æƒ…å†µï¼š
1. ä½¿ç”¨kubectlåˆ›å»ºä¸€ä¸ªåä¸ºâ€œmy-serviceâ€çš„Serviceèµ„æºï¼Œå…¶é…ç½®ä¸­åŒ…å«ä¸å¯å˜å­—æ®µclusterIPï¼Œå…¶å€¼ä¸º"None"ï¼›
2. æ„é€ ä¸€ä¸ªä¿®æ”¹åçš„é…ç½®æ–‡ä»¶ï¼Œè¯¥æ–‡ä»¶å°è¯•ç§»é™¤clusterIPå­—æ®µï¼Œå¹¶é€šè¿‡kubectlçš„server-side applyæäº¤è¯¥å˜æ›´ï¼›
3. ç†è®ºä¸Šï¼Œç”±äºclusterIPä¸ºä¸å¯å˜å­—æ®µï¼ŒKubernetesåº”å½“è¿”å›é”™è¯¯ï¼Œä½†åœ¨fluxcdç®¡ç†ä¸‹å¯èƒ½å‡ºç°é™é»˜å¤±è´¥çš„æƒ…å†µï¼›
4. æœ€åå†æ¬¡è·å–Serviceèµ„æºçš„é…ç½®ä»¥æ£€æŸ¥clusterIPå­—æ®µæ˜¯å¦ä¾ç„¶å­˜åœ¨ã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¯¥è„šæœ¬ä»…ç”¨äºç ”ç©¶å’Œæœ¬åœ°æµ‹è¯•ç¯å¢ƒï¼ŒéªŒè¯åœ¨ä¿®æ”¹ä¸å¯å˜å­—æ®µæ—¶æ˜¯å¦å­˜åœ¨é™é»˜å¤±è´¥çš„é—®é¢˜ï¼Œè€Œè¯¥é—®é¢˜æœ¬èº«å¹¶ä¸æ„æˆå®‰å…¨æ¼æ´ï¼Œä¹Ÿä¸ä¼šå¯¼è‡´æœªç»æˆæƒçš„è®¿é—®æˆ–ææƒç­‰é£é™©ã€‚

---


## Issue #129880 Pods Pending when loaded via file, omit resources in the spec and LimitRange is present

- Issue é“¾æ¥ï¼š[#129880](https://github.com/kubernetes/kubernetes/issues/129880)

### Issue å†…å®¹

#### What happened?

We deploy control-plane components as Pod manifests for Kubelet to load. Here is an example of our APIServer Pod manifest: https://github.com/utilitywarehouse/tf_kube_ignition/blob/master/resources/kube-apiserver.yaml

As you can see it *does not* specify any resource requests/limits.

All our Namespaces have LimitRange like this:

```
apiVersion: v1
kind: LimitRange
metadata:
  name: kube-system
spec:
  limits:
    - default:
        cpu: 7000m
        ephemeral-storage: 1Gi
      defaultRequest:
        cpu: 0m
        ephemeral-storage: 0
      type: Container
```

Starting with v1.32 - these Pods no longer start, they stay `Pending` and the error is:

```
failed to patch status
...
Pod "kube-scheduler-master-1.exp-1.merit.uw.systems" is invalid: status.qosClass: Invalid value: "BestEffort": field is immutable
```

If we add resource requests+limits to the Pod definition, Pods successfully start. If we remove LimitRange from the Namespace, the Pods successfully start.

We suspect this issue is related to https://github.com/kubernetes/kubernetes/pull/127744 and https://github.com/kubernetes/kubernetes/pull/128083 .

We are going to specify resources in all our static Pod definitions, its the right thing to do for us, but this seems like incorrect behavior that was worth a Github Issue.

#### What did you expect to happen?

Pods that do not specify resource requests / limits are able to start in a Namespace with a LimitRange.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a static Pod definition that is loaded by Kubelet, omit specifying resources.
Create a LimitRange that specifies cpu and memory.
Running v1.32 Pod should stay `Pending`.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.31.3
Kustomize Version: v5.4.2
Server Version: v1.32.1
```

</details>


#### Cloud provider

<details>
on-prem VM
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Flatcar Container Linux by Kinvolk"
ID=flatcar
ID_LIKE=coreos
VERSION=4081.2.1
VERSION_ID=4081.2.1
BUILD_ID=2024-12-12-2346
SYSEXT_LEVEL=1.0
PRETTY_NAME="Flatcar Container Linux by Kinvolk 4081.2.1 (Oklo)"
ANSI_COLOR="38;5;75"
HOME_URL="https://flatcar.org/"
BUG_REPORT_URL="https://issues.flatcar.org"
FLATCAR_BOARD="amd64-usr"
CPE_NAME="cpe:2.3:o:flatcar-linux:flatcar_linux:4081.2.1:*:*:*:*:*:*:*"

$ uname -a
Linux master-0.exp-1.merit.uw.systems 6.6.65-flatcar #1 SMP PREEMPT_DYNAMIC Thu Dec 12 23:15:00 -00 2024 x86_64 Intel(R) Xeon(R) CPU D-1548 @ 2.00GHz GenuineIntel GNU/Linux
```

</details>


#### Install tools

<details>
Terraform module: https://github.com/utilitywarehouse/tf_kube_ignition/tree/master
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Version:  0.1.0
RuntimeName:  containerd
RuntimeVersion:  v1.7.21
RuntimeApiVersion:  v1
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†åœ¨ Kubernetes v1.32 ä¸­ï¼Œå½“é€šè¿‡æ–‡ä»¶åŠ è½½é™æ€ Pod ä¸”åœ¨ Pod è§„èŒƒä¸­æ²¡æœ‰æŒ‡å®šèµ„æºè¯·æ±‚/é™åˆ¶æ—¶ï¼Œåœ¨å­˜åœ¨ LimitRange çš„å‘½åç©ºé—´ä¸­ Pod æ— æ³•å¯åŠ¨ï¼Œå¹¶æŠ¥é”™â€œstatus.qosClass: Invalid value: 'BestEffort': field is immutableâ€ã€‚æ ¹æ®æè¿°ï¼Œè¯¥é—®é¢˜ä¸ Kubernetes å¯¹ LimitRange è¡Œä¸ºçš„å˜æ›´æœ‰å…³ï¼Œå¹¶éæ¶‰åŠæƒé™ç»•è¿‡ã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸æˆ–ææƒç­‰å®‰å…¨æ¼æ´ã€‚  
æ­¤å¤–ï¼Œè¯¥é—®é¢˜å½±å“çš„æ˜¯é™æ€ Pod çš„è°ƒåº¦å’ŒçŠ¶æ€æ›´æ–°é—®é¢˜ï¼Œä¸å±äºè¿œç¨‹æ”»å‡»è€…å¯åˆ©ç”¨çš„æ¼æ´ï¼›ä¹Ÿä¸å­˜åœ¨æ”»å‡»è€…åˆ©ç”¨æäº¤çš„æ•æ„Ÿä¿¡æ¯ã€ä¸å½“æ“ä½œæˆ–é…ç½®å¤±è¯¯æ¥è¿›è¡Œè¿›ä¸€æ­¥ç ´åçš„é£é™©ã€‚å› æ­¤ï¼Œä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¿™ä¸ª Issue æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªäº§å“ç¼ºé™·æˆ–é…ç½®é—æ¼é—®é¢˜ï¼Œè€Œéå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
è¯´æ˜ï¼š
æ­¤è„šæœ¬ç”¨äºæ¨¡æ‹Ÿé™æ€ Pod éƒ¨ç½²æ—¶é—æ¼èµ„æºé…ç½®ï¼Œå¹¶åœ¨å­˜åœ¨ LimitRange çš„å‘½åç©ºé—´ä¸­æ£€æŸ¥ Pod çŠ¶æ€ï¼Œ
å¤ç° Kubernetes v1.32 ä¸­ç›¸å…³é—®é¢˜çš„ç°è±¡ã€‚ç”±äºè¯¥é—®é¢˜ä¸å±äºå®‰å…¨é£é™©ï¼Œå› æ­¤æ­¤è„šæœ¬ä»…ç”¨äºç ”ç©¶å’Œæœ¬åœ°æµ‹è¯•ï¼Œ
å¹¶ä¸åŒ…å«ä»»ä½•åˆ©ç”¨ä»£ç ã€‚
"""

import subprocess
import time
import yaml
import http.server
import socketserver
import threading

# HTTPæœåŠ¡å™¨ï¼Œç”¨äºæ¨¡æ‹Ÿç¯å¢ƒä¸­å¯¹å¤–æä¾›HTTPæ¥å£ï¼ˆæ ¹æ®éœ€è¦ï¼‰
class SimpleHTTPRequestHandler(http.server.SimpleHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.end_headers()
        self.wfile.write(b"Static Pod test server running on 8080")

def start_http_server():
    handler = SimpleHTTPRequestHandler
    with socketserver.TCPServer(("", 8080), handler) as httpd:
        print("HTTP Server serving at port 8080")
        httpd.serve_forever()

def deploy_limitrange(namespace):
    # åˆ›å»º LimitRange YAML
    limitrange = {
        "apiVersion": "v1",
        "kind": "LimitRange",
        "metadata": {
            "name": "kube-system",
            "namespace": namespace
        },
        "spec": {
            "limits": [
                {
                    "default": {
                        "cpu": "7000m",
                        "ephemeral-storage": "1Gi"
                    },
                    "defaultRequest": {
                        "cpu": "0m",
                        "ephemeral-storage": "0"
                    },
                    "type": "Container"
                }
            ]
        }
    }
    with open("limitrange.yaml", "w") as f:
        yaml.dump(limitrange, f)
    print("Applying LimitRange ...")
    subprocess.run(["kubectl", "apply", "-f", "limitrange.yaml"], check=True)

def deploy_static_pod(pod_manifest_file):
    print(f"Applying static Pod manifest from {pod_manifest_file} ...")
    subprocess.run(["kubectl", "apply", "-f", pod_manifest_file], check=True)

def get_pod_status(namespace, pod_name):
    try:
        out = subprocess.check_output(["kubectl", "get", "pod", pod_name, "-n", namespace, "-o", "yaml"])
        pod = yaml.safe_load(out)
        return pod.get("status", {}).get("phase", "Unknown")
    except subprocess.CalledProcessError as e:
        return f"Error: {str(e)}"

def main():
    # å¯åŠ¨ä¸€ä¸ª HTTP æœåŠ¡å™¨ï¼ˆç›‘å¬åœ¨8080ç«¯å£ï¼‰
    http_thread = threading.Thread(target=start_http_server, daemon=True)
    http_thread.start()

    namespace = "default"
    pod_manifest_file = "static-pod.yaml"
    pod_name = "static-pod-test"

    # å‡†å¤‡ä¸€ä¸ªé™æ€ Pod manifestï¼Œæ•…æ„çœç•¥èµ„æºè¯·æ±‚å’Œé™åˆ¶å­—æ®µ
    pod_manifest = {
        "apiVersion": "v1",
        "kind": "Pod",
        "metadata": {
            "name": pod_name,
            "namespace": namespace
        },
        "spec": {
            "containers": [
                {
                    "name": "pause",
                    "image": "k8s.gcr.io/pause:3.9"
                    # æ³¨æ„ï¼šæ­¤å¤„æ•…æ„ä¸è®¾ç½®resources
                }
            ]
        }
    }
    with open(pod_manifest_file, "w") as f:
        yaml.dump(pod_manifest, f)
    print(f"Static Pod manifest written to {pod_manifest_file}")

    # éƒ¨ç½² LimitRangeï¼Œæ¨¡æ‹Ÿç¯å¢ƒä¸­æœ‰é»˜è®¤èµ„æºé™åˆ¶é…ç½®
    deploy_limitrange(namespace)

    # éƒ¨ç½²é™æ€ Pod
    deploy_static_pod(pod_manifest_file)

    # ç­‰å¾…ä¸€æ®µæ—¶é—´åæ£€æµ‹ Pod çŠ¶æ€
    print("ç­‰å¾… 20 ç§’åæ£€æŸ¥ Pod çŠ¶æ€ ...")
    time.sleep(20)
    status = get_pod_status(namespace, pod_name)
    print(f"Pod {pod_name} çŠ¶æ€ï¼š{status}")

if __name__ == "__main__":
    main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. è„šæœ¬é¦–å…ˆå¯åŠ¨äº†ä¸€ä¸ªæœ¬åœ° HTTP æœåŠ¡å™¨ï¼Œè¯¥æœåŠ¡å™¨ç›‘å¬åœ¨ 8080 ç«¯å£ï¼Œä»…ç”¨äºæ»¡è¶³é¢˜ç›®è¦æ±‚æ¨¡æ‹Ÿ HTTP æœåŠ¡å™¨çš„ç¯å¢ƒï¼ˆå¹¶éæ¼æ´åˆ©ç”¨æ­¥éª¤çš„ä¸€éƒ¨åˆ†ï¼‰ã€‚
2. deploy_limitrange() å‡½æ•°é€šè¿‡ç”Ÿæˆå¹¶åº”ç”¨ä¸€ä¸ª YAML æ–‡ä»¶æ¥åœ¨æŒ‡å®šå‘½åç©ºé—´ä¸­åˆ›å»ºä¸€ä¸ª LimitRange å¯¹è±¡ï¼Œæ¨¡æ‹Ÿ Kubernetes ç¯å¢ƒä¸­å­˜åœ¨é»˜è®¤çš„èµ„æºé™åˆ¶é…ç½®ã€‚
3. deploy_static_pod() å‡½æ•°ç”Ÿæˆä¸€ä¸ªé™æ€ Pod çš„ YAML æ¸…å•ï¼Œè¯¥ Pod æ•…æ„çœç•¥äº†å®¹å™¨çš„èµ„æºè¯·æ±‚å’Œé™åˆ¶å­—æ®µï¼Œç¬¦åˆ Issue ä¸­æè¿°çš„æƒ…å†µã€‚
4. è„šæœ¬åº”ç”¨äº†ç”Ÿæˆçš„ YAML æ¸…å•ï¼Œå¹¶åœ¨ä¸€æ®µç­‰å¾…æ—¶é—´åï¼Œé€šè¿‡è°ƒç”¨ kubectl å‘½ä»¤è·å– Pod çš„çŠ¶æ€ï¼Œä»è€Œè§‚å¯Ÿæ˜¯å¦ç”±äºèµ„æºè®¾ç½®é—®é¢˜å¯¼è‡´ Pod ä¸€ç›´å¤„äº Pending çŠ¶æ€ã€‚
5. ç”±äºè¯¥é—®é¢˜å±äº Kubernetes çš„è¡Œä¸ºå˜åŒ–è€Œéå®‰å…¨æ¼æ´ï¼Œè„šæœ¬ä»…ç”¨äºå¤ç°è¯¥ç°è±¡ï¼Œå¹¶ä¸æä¾›ä»»ä½•é«˜é£é™©åˆ©ç”¨è·¯å¾„ã€‚

---


## Issue #129866 HPA wrongly assumes that terminated pods have an utilization of 100%

- Issue é“¾æ¥ï¼š[#129866](https://github.com/kubernetes/kubernetes/issues/129866)

### Issue å†…å®¹

#### What happened?

A pod that terminated was considered by the HPA controller to be at its target utilization.

The controller logic ([1](https://github.com/kubernetes/kubernetes/blob/ed9572d9c7733602de43979caf886fd4092a7b0f/pkg/controller/podautoscaler/replica_calculator.go#L106-L120), [2](https://github.com/kubernetes/kubernetes/blob/ed9572d9c7733602de43979caf886fd4092a7b0f/pkg/controller/podautoscaler/replica_calculator.go#L211-L223)) is such that, while scaling up, it conservatively considers that a pod for which we couldn't get the utilization metric from a metrics API are at their target utilization. (On scale down, the assumption is conservatively that the utilization is 0.)

#### What did you expect to happen?

I expected the controller to assume that a terminated pod has an utilization of 0.

This is [already correctly handled for pods that terminated with a failure](https://github.com/kubernetes/kubernetes/blob/ed9572d9c7733602de43979caf886fd4092a7b0f/pkg/controller/podautoscaler/replica_calculator.go#L383), but the case where a pod terminated successfully isn't handled.


#### How can we reproduce it (as minimally and precisely as possible)?

Create a Deployment with pods that terminate (without a failure) and observe that an HPA targetting this Deployment will assume that terminated pods are at target utilization.

#### Anything else we need to know?

Handling the case where the pod is terminated normally [here](https://github.com/kubernetes/kubernetes/blob/ed9572d9c7733602de43979caf886fd4092a7b0f/pkg/controller/podautoscaler/replica_calculator.go#L383) will fix this.

#### Kubernetes version

<details>

```console
$ kubectl version
v1.29.10-gke.1280000
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux rodete"
NAME="Debian GNU/Linux rodete"
VERSION_CODENAME=rodete
```

</details>


#### Install tools

<details>
N/A
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
N/A
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
None
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯ Kubernetes HPAï¼ˆHorizontal Pod Autoscalerï¼‰åœ¨å¤„ç†å·²æ­£å¸¸ç»ˆæ­¢ï¼ˆæ²¡æœ‰å¤±è´¥ï¼‰çš„ pod æ—¶ï¼Œé”™è¯¯åœ°å‡è®¾å…¶åˆ©ç”¨ç‡ä¸ºç›®æ ‡åˆ©ç”¨ç‡ï¼ˆ100%ï¼‰ï¼Œä»è€Œå¯èƒ½å¯¼è‡´é”™è¯¯çš„æ‰©ç¼©å®¹å†³ç­–ã€‚é—®é¢˜æœ¬èº«å±äºè®¾è®¡æˆ–é€»è¾‘é”™è¯¯ï¼Œä¸æ¶‰åŠä»£ç æ‰§è¡Œæ¼æ´ã€ææƒã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰å®‰å…¨é—®é¢˜ï¼Œä¹Ÿä¸å­˜åœ¨æ”»å‡»è€…å¯åˆ©ç”¨çš„è§¦å‘æ¼æ´ã€‚å› æ­¤è¯¥ Issue ä¸å®‰å…¨é£é™©æ— å…³ï¼Œå±äºéå®‰å…¨æ€§ç¼ºé™·ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
import time
import sys
from kubernetes import client, config
from kubernetes.client.rest import ApiException

DEPLOYMENT_NAME = "test-terminated-deployment"
NAMESPACE = "default"
HPA_NAME = "test-hpa"

def create_deployment(api_instance):
    # å®šä¹‰ä¸€ä¸ªpodï¼Œåœ¨å¯åŠ¨åé©¬ä¸Šæ­£å¸¸é€€å‡º
    container = client.V1Container(
        name="terminated-container",
        image="busybox",
        command=["/bin/sh", "-c", "echo Hello; sleep 1; exit 0"]
    )
    template = client.V1PodTemplateSpec(
        metadata=client.V1ObjectMeta(labels={"app": DEPLOYMENT_NAME}),
        spec=client.V1PodSpec(restart_policy="Never", containers=[container])
    )
    spec = client.V1DeploymentSpec(
        replicas=3,
        selector=client.V1LabelSelector(match_labels={"app": DEPLOYMENT_NAME}),
        template=template
    )
    deployment = client.V1Deployment(
        metadata=client.V1ObjectMeta(name=DEPLOYMENT_NAME),
        spec=spec
    )
    try:
        api_response = api_instance.create_namespaced_deployment(
            namespace=NAMESPACE,
            body=deployment
        )
        print(f"Deployment {DEPLOYMENT_NAME} created.")
    except ApiException as e:
        print("Exception when creating deployment: %s\n" % e)
        sys.exit(1)

def create_hpa(hpa_api):
    # åˆ›å»ºä¸€ä¸ªHPAå¯¹è±¡ï¼Œç›®æ ‡æ˜¯è¿™ä¸ªDeploymentï¼Œè®¾ç½®target CPUåˆ©ç”¨ç‡ä¸º50%
    min_replicas = 1
    max_replicas = 5
    target_metric = client.V2beta2MetricSpec(
        type="Resource",
        resource=client.V2beta2ResourceMetricSource(
            name="cpu",
            target=client.V2beta2MetricTarget(
                type="Utilization",
                average_utilization=50
            )
        )
    )
    hpa_spec = client.V2beta2HorizontalPodAutoscalerSpec(
        scale_target_ref=client.V2beta2CrossVersionObjectReference(
            api_version="apps/v1",
            kind="Deployment",
            name=DEPLOYMENT_NAME
        ),
        min_replicas=min_replicas,
        max_replicas=max_replicas,
        metrics=[target_metric]
    )
    hpa = client.V2beta2HorizontalPodAutoscaler(
        metadata=client.V1ObjectMeta(name=HPA_NAME),
        spec=hpa_spec
    )
    try:
        hpa_api.create_namespaced_horizontal_pod_autoscaler(
            namespace=NAMESPACE,
            body=hpa
        )
        print(f"HPA {HPA_NAME} created.")
    except ApiException as e:
        print("Exception when creating HPA: %s\n" % e)
        sys.exit(1)

def check_hpa_status(hpa_api):
    # æŸ¥è¯¢HPAå¯¹è±¡çŠ¶æ€ï¼Œå…³æ³¨å½“å‰çš„æŒ‡æ ‡ä¿¡æ¯
    try:
        hpa_status = hpa_api.read_namespaced_horizontal_pod_autoscaler_status(
            name=HPA_NAME, namespace=NAMESPACE
        )
        print("HPA status:")
        print(hpa_status.status)
    except ApiException as e:
        print("Exception when reading HPA status: %s\n" % e)

def cleanup(api_instance, hpa_api):
    # åˆ é™¤ HPA å’Œ Deployment èµ„æº
    try:
        hpa_api.delete_namespaced_horizontal_pod_autoscaler(
            name=HPA_NAME,
            namespace=NAMESPACE
        )
        print(f"HPA {HPA_NAME} deleted.")
    except ApiException as e:
        print("Exception when deleting HPA: %s\n" % e)
    try:
        api_instance.delete_namespaced_deployment(
            name=DEPLOYMENT_NAME,
            namespace=NAMESPACE
        )
        print(f"Deployment {DEPLOYMENT_NAME} deleted.")
    except ApiException as e:
        print("Exception when deleting deployment: %s\n" % e)

def main():
    # ä»é»˜è®¤ kubeconfig åŠ è½½é…ç½®
    try:
        config.load_kube_config()
    except Exception as e:
        print("Error loading kubeconfig: %s" % e)
        sys.exit(1)

    apps_v1 = client.AppsV1Api()
    hpa_api = client.AutoscalingV2beta2Api()

    print("Creating deployment with pods that terminate normally...")
    create_deployment(apps_v1)

    print("Creating HPA targeting the deployment...")
    create_hpa(hpa_api)

    print("Waiting for pods to run and terminate...")
    time.sleep(15)

    print("Checking HPA status to observe utilization assumptions...")
    check_hpa_status(hpa_api)

    input("æŒ‰å›è½¦é”®è¿›è¡Œæ¸…ç†å¹¶é€€å‡º...")
    cleanup(apps_v1, hpa_api)

if __name__ == '__main__':
    main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. è„šæœ¬ä¸­é¦–å…ˆåŠ è½½é»˜è®¤ä½ç½®çš„ kubeconfigï¼Œè¿æ¥åˆ° Kubernetes é›†ç¾¤ã€‚
2. é€šè¿‡ Kubernetes Python å®¢æˆ·ç«¯åˆ›å»ºä¸€ä¸ª Deploymentï¼Œè¯¥ Deployment è¿è¡Œçš„ BusyBox å®¹å™¨åœ¨å¯åŠ¨åä¼šæ‰§è¡Œâ€œecho Hello; sleep 1; exit 0â€ï¼Œå³æ­£å¸¸é€€å‡ºï¼Œä»è€Œæ¨¡æ‹Ÿâ€œæ­£å¸¸ç»ˆæ­¢â€çš„ podã€‚
3. åˆ›å»ºä¸€ä¸ª HPAï¼ˆæ¨ªå‘Podè‡ªåŠ¨æ‰©ç¼©å®¹å™¨ï¼‰å¯¹è±¡ï¼Œç›®æ ‡ä¸ºä¸Šè¿° Deploymentï¼Œç›®æ ‡ CPU åˆ©ç”¨ç‡è®¾ä¸º 50%ã€‚ç”±äº pod æ­£å¸¸ç»ˆæ­¢ä¸”æ— æ³•è·å–åˆ©ç”¨ç‡æ•°æ®ï¼ŒæŒ‰ç…§ Issue æ‰€æè¿°çš„é€»è¾‘ï¼ŒHPA æ§åˆ¶å™¨å¯èƒ½é”™è¯¯åœ°å°†è¿™äº› pod è®¤ä¸ºè¾¾åˆ°äº†ç›®æ ‡åˆ©ç”¨ç‡ã€‚
4. è„šæœ¬ç­‰å¾…ä¸€å®šæ—¶é—´åæŸ¥è¯¢ HPA çš„çŠ¶æ€ï¼Œç”¨æˆ·å¯ä»¥è§‚å¯Ÿ HPA å½“å‰çš„æŒ‡æ ‡ä¿¡æ¯ï¼ˆè™½ç„¶æŒ‡æ ‡æ˜¯å¦é‡‡é›†æˆåŠŸä¾èµ–äºé›†ç¾¤å†…çš„ metrics-serverï¼‰ã€‚
5. æœ€åï¼Œè„šæœ¬æä¾›æ¸…ç†åŠŸèƒ½ï¼Œåˆ é™¤åˆ›å»ºçš„ HPA å’Œ Deployment èµ„æºï¼Œä¿æŒé›†ç¾¤ç¯å¢ƒæ•´æ´ã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ­¤è„šæœ¬ä»…ç”¨äºå¤ç° HPA è¡Œä¸ºçš„BUGï¼Œç”¨äºæµ‹è¯•å’Œç ”ç©¶ï¼Œä¸æ„æˆå®‰å…¨æ”»å‡»ã€‚æ•´ä¸ªé—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸ªé€»è¾‘ç¼ºé™·ï¼Œä¸ç¬¦åˆå®‰å…¨æ¼æ´çš„å®šä¹‰ï¼Œå› æ­¤é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

---


## Issue #129842 hack/local-up-cluster.sh fails with emulation version needs to be greater or equal to 1.31

- Issue é“¾æ¥ï¼š[#129842](https://github.com/kubernetes/kubernetes/issues/129842)

### Issue å†…å®¹

#### What happened?

Tried to create local Kubernetes cluster using hack/local-up-cluster.sh script but the api-server fails to come up.

#### What did you expect to happen?

Successfully create a local Kubernetes cluster.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Clone the Kubernetes repository
```
https://github.com/kubernetes/kubernetes.git
```

2. Run the local cluster create script
```
./hack/local-up-cluster.sh
```
[Install the missing dependencies reported by the script]

3. Error

```
2025/01/27 20:08:25 [INFO] received CSR
2025/01/27 20:08:25 [INFO] generating key: rsa-2048
2025/01/27 20:08:25 [INFO] encoded CSR
2025/01/27 20:08:25 [INFO] signed certificate with serial number xxxxxxxxx
Waiting for apiserver to come up
!!! [0127 20:09:29] Timed out waiting for apiserver:  to answer at https://127.0.0.1:6443/healthz; tried 60 waiting 1 between each
check apiserver logs: /tmp/kube-apiserver.log
Cleaning up...
```

4. API server Log
```
$ cat /tmp/kube-apiserver.log
I0127 20:08:28.017237   63752 flags.go:64] FLAG: --v="3"
I0127 20:08:28.017240   63752 flags.go:64] FLAG: --version="false"
I0127 20:08:28.017243   63752 flags.go:64] FLAG: --vmodule=""
I0127 20:08:28.017246   63752 flags.go:64] FLAG: --watch-cache="true"
I0127 20:08:28.017248   63752 flags.go:64] FLAG: --watch-cache-sizes="[]"
W0127 20:08:28.017278   63752 registry.go:256] calling componentGlobalsRegistry.AddFlags more than once, the registry will be set by the latest flags
E0127 20:08:28.017662   63752 run.go:72] "command failed" err="emulation version needs to be greater or equal to 1.31, got 1.23.0-alpha.1.24689"
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.2
Kustomize Version: v5.4.2
The connection to the server 127.0.0.1:62205 was refused - did you specify the right host or port?
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On mac
Darwin KMacBook-Pro.local 24.1.0 Darwin Kernel Version 24.1.0

```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯åœ¨ä½¿ç”¨ hack/local-up-cluster.sh è„šæœ¬å¯åŠ¨æœ¬åœ° Kubernetes é›†ç¾¤æ—¶ï¼Œç”±äºæ£€æµ‹åˆ° emulation ç‰ˆæœ¬è¿‡ä½ï¼ˆå®é™…ç‰ˆæœ¬ 1.23.0-alpha.1.24689ï¼Œè€Œè¦æ±‚å¤§äºç­‰äº 1.31ï¼‰å¯¼è‡´ API Server æ— æ³•å¯åŠ¨ã€‚ä»æ—¥å¿—å’Œé”™è¯¯ä¿¡æ¯æ¥çœ‹ï¼Œè¿™æ˜¯ä¸€å¤„ç‰ˆæœ¬ä¾èµ–æˆ–é…ç½®é—®é¢˜ï¼Œå¹¶éé’ˆå¯¹ç³»ç»Ÿçš„å®‰å…¨æ¼æ´æˆ–å®‰å…¨æœºåˆ¶ç¼ºé™·ï¼Œä¹Ÿæ²¡æœ‰æ¶‰åŠæ¶æ„æ“ä½œã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€å‘½ä»¤æ‰§è¡Œã€æƒé™æå‡ç­‰é«˜é£é™©å®‰å…¨é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
æ­¤è„šæœ¬ç”¨äºæ¨¡æ‹Ÿå¤ç° hack/local-up-cluster.sh è„šæœ¬å¯åŠ¨æ—¶å›  emulation ç‰ˆæœ¬ä¸åŒ¹é…çš„é—®é¢˜ã€‚
æ³¨æ„ï¼šå½“å‰é—®é¢˜å±äºç‰ˆæœ¬é…ç½®é—®é¢˜ï¼Œä¸æ¶‰åŠå®‰å…¨æ¼æ´ï¼Œå› æ­¤è¯¥è„šæœ¬ä»…ç”¨äºæµ‹è¯•å’ŒéªŒè¯ç‰ˆæœ¬æ£€æµ‹é€»è¾‘ã€‚
"""

import subprocess
import sys
import http.server
import socketserver
import threading

def start_http_server():
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„ HTTP æœåŠ¡å™¨ï¼Œç›‘å¬åœ¨ 8080 ç«¯å£ï¼ˆæ»¡è¶³è¦æ±‚ï¼šåœ¨è„šæœ¬ä¸­åˆ›å»ºä¸€ä¸ªHTTPæœåŠ¡å™¨ï¼‰
    Handler = http.server.SimpleHTTPRequestHandler
    httpd = socketserver.TCPServer(("", 8080), Handler)
    print("HTTP server is serving at port 8080")
    httpd.serve_forever()

def emulate_kube_apiserver():
    # æ¨¡æ‹Ÿ API Server çš„å¯åŠ¨æ—¥å¿—è¾“å‡º
    print("I0127 20:08:28.017237   63752 flags.go:64] FLAG: --v=\"3\"")
    print("I0127 20:08:28.017240   63752 flags.go:64] FLAG: --version=\"false\"")
    print("I0127 20:08:28.017243   63752 flags.go:64] FLAG: --vmodule=\"\"")
    print("I0127 20:08:28.017246   63752 flags.go:64] FLAG: --watch-cache=\"true\"")
    print("I0127 20:08:28.017248   63752 flags.go:64] FLAG: --watch-cache-sizes=\"[]\"")
    print("W0127 20:08:28.017278   63752 registry.go:256] calling componentGlobalsRegistry.AddFlags more than once, the registry will be set by the latest flags")
    print("E0127 20:08:28.017662   63752 run.go:72] \"command failed\" err=\"emulation version needs to be greater or equal to 1.31, got 1.23.0-alpha.1.24689\"")
    print("\nCleaning up...")

def run_local_cluster():
    print("Starting local Kubernetes cluster simulation...")
    # æ¨¡æ‹Ÿå¯åŠ¨ API Server
    emulate_kube_apiserver()
    # æ­¤å¤„å®é™…è„šæœ¬ä¸­è¿˜ä¼šåŒ…å«å¯åŠ¨å„ç»„ä»¶ã€ä¾èµ–æ£€æµ‹ç­‰ï¼Œæœ¬å¤ç°ä»…èšç„¦ emulation ç‰ˆæœ¬æ£€æµ‹

def main():
    # å¯åŠ¨ä¸€ä¸ªåå°çº¿ç¨‹ä½œä¸º HTTP æœåŠ¡å™¨
    server_thread = threading.Thread(target=start_http_server, daemon=True)
    server_thread.start()

    # æ¨¡æ‹Ÿæœ¬åœ°é›†ç¾¤åˆ›å»ºæµç¨‹
    run_local_cluster()

    # ä¿æŒä¸»ç¨‹åºè¿è¡Œä»¥ä¾¿ HTTP æœåŠ¡å™¨æŒç»­æœåŠ¡
    try:
        while True:
            pass
    except KeyboardInterrupt:
        print("\nShutting down simulation...")
        sys.exit(0)

if __name__ == "__main__":
    main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. æœ¬å¤ç°è„šæœ¬ç”¨äºæ¨¡æ‹Ÿ hack/local-up-cluster.sh è„šæœ¬ä¸­å¯åŠ¨æœ¬åœ° Kubernetes é›†ç¾¤æ—¶å›  emulation ç‰ˆæœ¬ä¸åŒ¹é…è€Œå¯¼è‡´ API Server å¯åŠ¨å¤±è´¥çš„æƒ…å½¢ã€‚  
2. è„šæœ¬ä¸­é¦–å…ˆå¯åŠ¨äº†ä¸€ä¸ªç›‘å¬åœ¨ 8080 ç«¯å£çš„ HTTP æœåŠ¡å™¨ï¼Œè¿™æ»¡è¶³é¢˜ç›®è¦æ±‚åœ¨è„šæœ¬ä¸­åˆ›å»º HTTP æœåŠ¡å™¨ã€‚  
3. æ¥ç€æ¨¡æ‹Ÿ API Server çš„å¯åŠ¨è¿‡ç¨‹ï¼Œå¹¶æ‰“å°å‡ºä¸æ—¥å¿—ä¸­æä¸ºç›¸ä¼¼çš„è¾“å‡ºï¼Œå…¶ä¸­åŒ…æ‹¬ç‰ˆæœ¬æ£€æµ‹å¤±è´¥çš„é”™è¯¯æç¤ºã€‚  
4. ç”±äºè¯¥é—®é¢˜æœ¬èº«ä¸ºç‰ˆæœ¬é…ç½®é—®é¢˜ï¼Œä¸”ä¸æ¶‰åŠå®‰å…¨æ¼æ´ï¼Œå› æ­¤æ­¤å¤ç°è„šæœ¬ä»…ç”¨äºç ”ç©¶å’Œæœ¬åœ°æµ‹è¯•ï¼Œä¸ä¼šå¯¹ç³»ç»Ÿå®‰å…¨æ€§äº§ç”Ÿå½±å“ã€‚

---


## Issue #129838 Service with selector does not create all required endpoints, when a POD has multiple containers with a matching named port.

- Issue é“¾æ¥ï¼š[#129838](https://github.com/kubernetes/kubernetes/issues/129838)

### Issue å†…å®¹

#### What happened?

I have a Deployment which creates POD's containing two containers, each container has a named port "grpc-service" on a different port to avoid conflicts.  I want a k8s service to connect to these grpc services in a round-robin fashion and also be able to use the k8s API to retrieve the list of endpoints.  I attempt to do this by creating a simple service, with a label selector that matches the container, and names the target-port.  However when I scale the  deployment only one of the containers is matched to the service,  I was expecting both containers to be matched resulting in two endpoints for each POD instance.

The generated endpoint looks like this, when the deployment is scaled to two replica's (I've trimmed out the targetRef's to keep it brief):

```
kind: Endpoints
apiVersion: v1
metadata:
  name: grpc-test-service
  annotations:
    endpoints.kubernetes.io/last-change-trigger-time: '2025-01-27T11:55:15Z'
subsets:
  - addresses:
      - ip: 10.245.0.71
      - ip: 10.245.2.91
    ports:
      - name: grpc
        port: 8080
        protocol: TCP
```

     


#### What did you expect to happen?

I expected the resultant endpoint to look like this (i.e. giving 4 endpoints):

```
apiVersion: v1
kind: Endpoints
metadata:
  name: grpc-test-service
subsets:
- addresses:
  - ip: 10.245.0.124
  - ip: 10.245.2.121
  ports:
  - port: 8080
    name: grpc
    protocol: TCP
- addresses:
  - ip: 10.245.0.124
  - ip: 10.245.2.121
  ports:
  - port: 8081
    name: grpc
    protocol: TCP    
```

#### How can we reproduce it (as minimally and precisely as possible)?

To create my test set-up the following yaml can be used (note that for the example I'm using nginx, however in practise I have different containers, which provide the same gRPC services).

```
apiVersion: v1
kind: Service
metadata:
  name: grpc-test-service
spec:
  type: LoadBalancer
  ports:
  - port: 5000
    name: grpc
    targetPort: grpc-server
  selector:
    app: grpc-test-server
    
---

apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config-site1
data:
  nginx.conf: |
        events {
        }
        http {
           server {
               listen 8080;
               location / {
                   return 200 "Hello world! I'm site 1";
               }
           }
        }

---

apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config-site2
data:
  nginx.conf: |
        events {
        }
        http {
           server {
               listen 8081;
               location / {
                   return 200 "Hello world! I'm site 2";
               }
           }
        }


---

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: grpc-test-server
    name: grpc-test-server
  name: grpc-test-server
spec:
  replicas: 2
  selector:
    matchLabels:
      app: grpc-test-server
      name: grpc-test-server
  template:
    metadata:
      labels:
        app: grpc-test-server
        name: grpc-test-server
      name: grpc-test-server
    spec:
      containers:
      - name: grpc-server-1
        image: nginx
        imagePullPolicy: Always
        ports:
          - name: grpc-server
            containerPort: 8080
            protocol: TCP
        volumeMounts:
          - name: config-site1
            mountPath: /etc/nginx/
               
      - name: grpc-server-2
        image: nginx
        imagePullPolicy: Always
        ports:
          - name: grpc-server
            containerPort: 8081
            protocol: TCP
        volumeMounts:
          - name: config-site2
            mountPath: /etc/nginx/
      volumes:
        - name: config-site1
          configMap:
            name: nginx-config-site1
            items:
              - key: nginx.conf
                path: nginx.conf
        - name: config-site2
          configMap:
            name: nginx-config-site2
            items:
              - key: nginx.conf
                path: nginx.conf
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Warning: Use tokens from the TokenRequest API or manually created secret-based tokens instead of auto-generated secret-based tokens.
Client Version: version.Info{Major:"1", Minor:"21", GitVersion:"v1.21.5", GitCommit:"aea7bbadd2fc0cd689de94a54e5b7b758869d691", GitTreeState:"clean", BuildDate:"2021-09-15T21:10:45Z", GoVersion:"go1.16.8", Compiler:"gc
", Platform:"windows/amd64"}
Server Version: version.Info{Major:"1", Minor:"32", GitVersion:"v1.32.0", GitCommit:"70d3cc986aa8221cd1dfb1121852688902d3bf53", GitTreeState:"clean", BuildDate:"2024-12-11T17:59:15Z", GoVersion:"go1.23.3", Compiler:"gc
", Platform:"linux/amd64"}
WARNING: version difference between client (1.21) and server (1.32) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
None, an on Premise setup
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 24.04.1 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04.1 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo
$ uname -a
Linux bmk8s-hermes 6.8.0-51-generic #52-Ubuntu SMP PREEMPT_DYNAMIC Thu Dec 5 13:09:44 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯ Kubernetes ä¸­ Service ä¸ Endpoints çš„è¡Œä¸ºä¸ç¬¦åˆé¢„æœŸï¼Œå³å½“ä¸€ä¸ª Pod å†…æœ‰å¤šä¸ªå®¹å™¨éƒ½æš´éœ²äº†åŒåç«¯å£æ—¶ï¼ŒService åªåŒ¹é…åˆ°äº†å…¶ä¸­ä¸€ä¸ªå®¹å™¨çš„ç«¯å£ï¼Œå¯¼è‡´ç”Ÿæˆçš„ Endpoints æ•°é‡å°‘äºé¢„æœŸã€‚è¿™æ˜¯ä¸€ä¸ªå…³äº Kubernetes æœåŠ¡å‘ç°å’Œ Endpoint ç”Ÿæˆçš„ bug æˆ–è®¾è®¡ç¼ºé™·é—®é¢˜ï¼Œå¹¶æœªæ¶‰åŠä¾‹å¦‚å‘½ä»¤æ‰§è¡Œã€å‡­æ®æ³„éœ²ã€æƒé™æå‡ã€å®¹å™¨é€ƒé€¸æˆ–æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ç­‰å®‰å…¨ç›¸å…³çš„é—®é¢˜ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œè¯¥é—®é¢˜å±äºåº”ç”¨å±‚æˆ–è°ƒåº¦é€»è¾‘çš„é—®é¢˜ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚å› æ­¤å¯ä»¥åˆ¤æ–­è¯¥é—®é¢˜ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥é—®é¢˜å¹¶éå®‰å…¨é£é™©ï¼Œå› æ­¤æ²¡æœ‰å®‰å…¨æ€§å¤ç°çš„POCã€‚
# å¦‚æœæ‚¨éœ€è¦éªŒè¯è¯¥ Kubernetes è¡Œä¸ºï¼Œå¯ä»¥ç®€å•è¾“å‡ºæç¤ºä¿¡æ¯ï¼š
if __name__ == "__main__":
    print("è¯¥ Issue æè¿°çš„æ˜¯ Kubernetes Service ä¸ Endpoints çš„ç”Ÿæˆç¼ºé™·ï¼Œä¸å­˜åœ¨å®‰å…¨é£é™©ã€‚")
```


**è§£é‡Šè¯´æ˜ï¼š**

1. åˆ†æä¸­ï¼Œæˆ‘ä»¬ç¡®è®¤è¯¥ Issue ä¸»è¦æè¿°çš„æ˜¯åœ¨å¤šå®¹å™¨ Pod ä¸­ï¼ŒKubernetes Service é€šè¿‡ label selector å’Œ targetPort åç§°åŒ¹é…æ—¶æœªèƒ½è¯†åˆ«æ‰€æœ‰å…·å¤‡åŒ¹é…ç«¯å£çš„å®¹å™¨ï¼Œå¯¼è‡´ Endpoints æ•°é‡ä¸ç¬¦åˆé¢„æœŸçš„é—®é¢˜ã€‚è¿™æ˜¯ Kubernetes æœåŠ¡å‘ç°çš„ä¸€ä¸ª bugï¼Œè€Œéå®‰å…¨é—®é¢˜ã€‚
2. æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥é—®é¢˜æ—¢ä¸æ¶‰åŠæ•æ„Ÿä¿¡æ¯æ³„éœ²ã€å‘½ä»¤æ‰§è¡Œã€æƒé™æå‡ç­‰å®‰å…¨é—®é¢˜ï¼Œä¹Ÿä¸ä¼šå¯¼è‡´æ‹’ç»æœåŠ¡ç­‰é«˜å®‰å…¨äº‹ä»¶ï¼Œå› æ­¤æ•´ä½“é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€å®‰å…¨é£é™©ã€‚
3. ç”±äºä»…åœ¨é«˜é£é™©é—®é¢˜ä¸­æ‰è¦æ±‚æä¾›ä½¿ç”¨ Python ç¼–å†™çš„å¤ç°è„šæœ¬ï¼Œæ­¤å¤„ç”±äºé—®é¢˜æœ¬èº«ä¸æ„æˆå®‰å…¨é£é™©ï¼Œå› æ­¤å¤ç°è„šæœ¬åªåšç®€å•è¯´æ˜ï¼Œå¹¶ä¸æ¶‰åŠå®é™…çš„æ¼æ´åˆ©ç”¨ä»£ç ï¼Œä»…ç”¨äºè¯´æ˜è¯¥é—®é¢˜çš„æ€§è´¨ã€‚

---


## Issue #129833 k8s deployment remove duplicate `tolerations` but not `affinity`

- Issue é“¾æ¥ï¼š[#129833](https://github.com/kubernetes/kubernetes/issues/129833)

### Issue å†…å®¹

#### What happened?

If we create a deployment with duplicate toleration the pod spec doesn't reflect the duplicate values, but the same doesn't happen to duplicate node affinities,

Deployment tolerations:
```
      tolerations:
      - effect: NoSchedule
        key: "key1"
        operator: Equal
        value: "true"
      - effect: NoSchedule
        key: "key1"
        operator: Equal
        value: "true"

```

Pod tolerations:
```
  tolerations:
  - effect: NoSchedule
    key: "key1"
    operator: Equal
    value: "true"
```


Deployment Affinity == Pod affinity :
```
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: "key1"
                operator: Exists
              - key: "key1"
                operator: Exists
            - matchExpressions:
              - key: "key1"
                operator: Exists
              - key: "key1"
                operator: Exists
```


#### What did you expect to happen?

Desired affinity:
```
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: "key1"
                operator: Exists
```

#### How can we reproduce it (as minimally and precisely as possible)?

Create a normal deployment with placements(tolerations and affinity) 

#### Anything else we need to know?

_No response_

#### Kubernetes version
k8s.io/api v0.31.2
<details>

```console
$ kubectl version
# paste output here

```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯ Kubernetes éƒ¨ç½²ï¼ˆDeploymentï¼‰åœ¨å¤„ç†é‡å¤é…ç½®æ—¶çš„ä¸ä¸€è‡´è¡Œä¸ºï¼šå¯¹äºé‡å¤çš„ tolerationsï¼Œä¼šè‡ªåŠ¨å»é‡ï¼Œè€Œå¯¹äºé‡å¤çš„ node affinityï¼Œåˆ™ä¸ä¼šå»é‡ã€‚è¯¥é—®é¢˜åæ˜ äº† Kubernetes åœ¨å°†éƒ¨ç½²é…ç½®è½¬æ¢ä¸º Pod è§„èŒƒæ—¶å­˜åœ¨çš„é€»è¾‘ä¸ä¸€è‡´ï¼Œä½†å®ƒä»…å±äºå®ç°ä¸Šçš„é—®é¢˜ï¼Œä¸æ¶‰åŠæœªæˆæƒè®¿é—®ã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€æƒé™æå‡æˆ–å…¶ä»–å®‰å…¨æ¼æ´ã€‚åŒæ—¶ï¼ŒIssue ä¸­æœªæ¶‰åŠæ•æ„Ÿä¿¡æ¯æ³„éœ²ã€ä¸å½“å‡­è¯é…ç½®ã€æˆ–è€…å› ä½æƒé™ç”¨æˆ·èƒ½å¤Ÿå½±å“é«˜æƒé™ç”¨æˆ·ç­‰å®‰å…¨é£é™©ï¼Œå› æ­¤ä¸æ„æˆå®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
æœ¬è„šæœ¬ç”¨äºæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ kubernetes Python å®¢æˆ·ç«¯åˆ›å»ºä¸€ä¸ª Deploymentï¼Œ
è¯¥ Deployment ä¸­åŒ…å«é‡å¤çš„ tolerations ä¸é‡å¤çš„ node affinity é…ç½®ï¼Œä»è€Œé‡ç° Issue ä¸­æè¿°çš„é—®é¢˜è¡¨ç°ã€‚
æ³¨æ„ï¼šè¯¥è„šæœ¬ä»…ç”¨äºç ”ç©¶å’Œæœ¬åœ°æµ‹è¯•ï¼Œä¸ä¼šå¯¹é›†ç¾¤é€ æˆç ´åï¼›åŒæ—¶ï¼Œæœ¬é—®é¢˜æœ¬èº«ä¸å±äºå®‰å…¨é£é™©é—®é¢˜ã€‚
"""

import yaml
import time
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# åŠ è½½ kubeconfig é…ç½®ï¼Œå‡è®¾åœ¨é»˜è®¤ä½ç½®
config.load_kube_config()

apps_v1 = client.AppsV1Api()

# å®šä¹‰åŒ…å«é‡å¤ tolerations å’Œé‡å¤ affinity çš„ Deployment é…ç½®
deployment_manifest = {
    "apiVersion": "apps/v1",
    "kind": "Deployment",
    "metadata": {
        "name": "duplicate-config-demo",
        "labels": {"app": "duplicate-demo"}
    },
    "spec": {
        "replicas": 1,
        "selector": {"matchLabels": {"app": "duplicate-demo"}},
        "template": {
            "metadata": {"labels": {"app": "duplicate-demo"}},
            "spec": {
                "containers": [{
                    "name": "nginx",
                    "image": "nginx:latest",
                    "ports": [{"containerPort": 80}]
                }],
                "tolerations": [
                    {
                        "key": "key1",
                        "operator": "Equal",
                        "value": "true",
                        "effect": "NoSchedule"
                    },
                    {
                        "key": "key1",
                        "operator": "Equal",
                        "value": "true",
                        "effect": "NoSchedule"
                    }
                ],
                "affinity": {
                    "nodeAffinity": {
                        "requiredDuringSchedulingIgnoredDuringExecution": {
                            "nodeSelectorTerms": [
                                {
                                    "matchExpressions": [
                                        {"key": "key1", "operator": "Exists"},
                                        {"key": "key1", "operator": "Exists"}
                                    ]
                                },
                                {
                                    "matchExpressions": [
                                        {"key": "key1", "operator": "Exists"},
                                        {"key": "key1", "operator": "Exists"}
                                    ]
                                }
                            ]
                        }
                    }
                }
            }
        }
    }
}

def create_deployment():
    try:
        resp = apps_v1.create_namespaced_deployment(
            body=deployment_manifest,
            namespace="default"
        )
        print("Deployment created. Status='%s'" % str(resp.metadata.name))
    except ApiException as e:
        print("Exception when creating deployment: %s\n" % e)

def get_pod_spec():
    # ç­‰å¾… Pod åˆ›å»ºå®Œæˆåè·å– Pod è§„èŒƒä¸­çš„ tolerations å’Œ affinity è¡¨ç°
    core_v1 = client.CoreV1Api()
    pods = []
    for _ in range(10):
        resp = core_v1.list_namespaced_pod(namespace="default", label_selector="app=duplicate-demo")
        pods = resp.items
        if pods:
            break
        time.sleep(2)
    if not pods:
        print("æœªæ‰¾åˆ°ç›¸å…³ Podã€‚")
        return
    pod = pods[0]
    print("Pod name:", pod.metadata.name)
    print("Pod tolerations:")
    for tol in pod.spec.tolerations or []:
        print(yaml.dump(tol))
    print("Pod affinity:")
    if pod.spec.affinity and pod.spec.affinity.node_affinity:
        print(yaml.dump(pod.spec.affinity.node_affinity))
    else:
        print("æ—  affinity é…ç½®")

def delete_deployment():
    try:
        apps_v1.delete_namespaced_deployment(
            name="duplicate-config-demo",
            namespace="default",
            body=client.V1DeleteOptions(
                propagation_policy='Foreground'
            )
        )
        print("Deployment deleted.")
    except ApiException as e:
        print("Exception when deleting deployment: %s\n" % e)

if __name__ == "__main__":
    print("åˆ›å»º Deployment ...")
    create_deployment()
    print("ç­‰å¾… Pod å¯åŠ¨ ...")
    time.sleep(10)
    print("è·å– Pod é…ç½® ...")
    get_pod_spec()
    print("æ¸…ç†æµ‹è¯• Deployment ...")
    delete_deployment()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. æœ¬è„šæœ¬ä½¿ç”¨ Kubernetes å®˜æ–¹çš„ Python å®¢æˆ·ç«¯ä»é»˜è®¤ä½ç½®åŠ è½½ kubeconfig å¹¶è¿æ¥é›†ç¾¤ã€‚
2. è„šæœ¬å®šä¹‰äº†ä¸€ä¸ª Deploymentï¼Œè¯¥ Deployment åœ¨ Pod è§„èŒƒä¸­æ—¢é…ç½®äº†é‡å¤çš„ tolerationsï¼Œä¹Ÿé…ç½®äº†é‡å¤çš„ node affinityï¼ˆé€šè¿‡ä¸¤ç»„é‡å¤çš„ matchExpressionsï¼‰ã€‚
3. åˆ›å»º Deployment åï¼Œè„šæœ¬ç­‰å¾… Pod å¯åŠ¨ï¼Œå¹¶è°ƒç”¨ get_pod_spec() å‡½æ•°è·å–å·²ç”Ÿæˆçš„ Pod ä¸­çš„ tolerations å’Œ affinity ä¿¡æ¯ã€‚é€šè¿‡å¯¹æ¯” Deployment ä¸­çš„é…ç½®å’Œ Pod ä¸­å®é™…çš„é…ç½®ï¼Œå¯ä»¥è§‚å¯Ÿåˆ° tolerations é‡å¤é¡¹è¢«å»é‡ï¼Œè€Œ affinity éƒ¨åˆ†åˆ™ä¿ç•™äº†æ‰€æœ‰é…ç½®ã€‚
4. æœ€åï¼Œè„šæœ¬åˆ é™¤äº†åˆ›å»ºçš„ Deploymentï¼Œä»¥ä¾¿æµ‹è¯•å®Œæˆåæ¸…ç†é›†ç¾¤èµ„æºã€‚
5. éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¯¥ Issue æ‰€æè¿°çš„é—®é¢˜å±äºé…ç½®é€»è¾‘ä¸ä¸€è‡´é—®é¢˜ï¼Œæ²¡æœ‰å¼•å…¥å®‰å…¨é£é™©ï¼Œå› æ­¤æœ¬å¤ç°è„šæœ¬ä»…ç”¨äºéªŒè¯è¡Œä¸ºå·®å¼‚ï¼Œä¸æ¶‰åŠä»»ä½•å®‰å…¨æ¼æ´åˆ©ç”¨ã€‚

---


## Issue #129829 [EventedPLEG] EventedPLEG should detect resize completion rapidly

- Issue é“¾æ¥ï¼š[#129829](https://github.com/kubernetes/kubernetes/issues/129829)

### Issue å†…å®¹

#### What would you like to be added?

EventedPLEG should detect completion of pod resizing in the runtime as rapidly as GenericPLEG.

Since v1.32 (#128518), GenericPLEG polls container resources in the runtime at resizing a pod so that it detects resize completion rapidly. This poll is actuated when GenericPLEG relists pods regularly:
https://github.com/kubernetes/kubernetes/blob/d36322f8d76c8e2a456e381bcc6bb43e4bbe602c/pkg/kubelet/pleg/generic.go#L314-L327

EventedPLEG depends on GenericPLEG for this polling:
https://github.com/kubernetes/kubernetes/blob/d36322f8d76c8e2a456e381bcc6bb43e4bbe602c/pkg/kubelet/pleg/evented.go#L430-L432

When EventedPLEG is enabled, GenericPLEG relists pods every five minutes:
https://github.com/kubernetes/kubernetes/blob/d36322f8d76c8e2a456e381bcc6bb43e4bbe602c/pkg/kubelet/kubelet.go#L197

Then, it takes at most five minutes till pod resize appears to be completed in the API.


#### Why is this needed?

Pod resize should complete as quickly as if EventedPLEG is not enabled.

Considering maturity of features, I donâ€™t think this issue blocks promotion of InPlacePodVerticalScaling. This issue should be solved for promoting EventedPLEG.


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue ä¸»è¦æè¿°çš„æ˜¯ EventedPLEG åœ¨æ£€æµ‹ Pod é‡ç½®ï¼ˆresizeï¼‰å®ŒæˆçŠ¶æ€æ—¶å­˜åœ¨å»¶æ—¶é—®é¢˜ï¼ŒæŒ‡å‡ºç›¸æ¯” GenericPLEGï¼ŒEventedPLEG æœªèƒ½â€œè¿…é€Ÿâ€æ£€æµ‹åˆ° Pod resize å®Œæˆã€‚æ•´ä¸ªæè¿°å…³æ³¨çš„æ˜¯åŠŸèƒ½å’Œæ€§èƒ½ä¸Šçš„æ”¹è¿›ä¼˜åŒ–ï¼Œè€Œä¸æ˜¯åŠŸèƒ½å®ç°ä¸­å­˜åœ¨çš„å®‰å…¨æ¼æ´æˆ–å®‰å…¨é£é™©é—®é¢˜ã€‚Issue ä¸­å¹¶æœªæåŠä»»ä½•å¯èƒ½å¯¼è‡´æœªæˆæƒè®¿é—®ã€ä¿¡æ¯æ³„æ¼ã€å‘½ä»¤æ‰§è¡Œã€ææƒã€å®¹å™¨é€ƒé€¸ç­‰å…¸å‹å®‰å…¨é—®é¢˜ã€‚æè¿°ä¸­å¼•ç”¨çš„ä»£ç é“¾æ¥ä¹Ÿéƒ½æ˜¯ç”¨äºè¯´æ˜äº‹ä»¶æ£€æµ‹ä¸è½®è¯¢å®ç°çš„é€»è¾‘ï¼Œæ²¡æœ‰æ¶‰åŠæ•æ„Ÿä¿¡æ¯æš´éœ²ã€ä¸å½“æ“ä½œæˆ–æ•æ„Ÿé…ç½®é”™è¯¯çš„é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
è¯´æ˜ï¼š
æ­¤è„šæœ¬ä»…ä¸ºæ¼”ç¤ºç”¨é€”ï¼Œç”¨äºæ¨¡æ‹Ÿä¸€ä¸ªHTTPæœåŠ¡ï¼Œè¯¥æœåŠ¡åœ¨8080ç«¯å£ç›‘å¬ï¼Œ
ç”¨äºè¯æ˜æœ¬Issueæœ¬è´¨ä¸Šæ˜¯åŠŸèƒ½ä¼˜åŒ–é—®é¢˜ï¼Œå¹¶ä¸å­˜åœ¨å®‰å…¨æ¼æ´ï¼Œå› æ­¤æ— éœ€å¯¹æ¼æ´é£é™©è¿›è¡Œå¤ç°ã€‚
"""

import http.server
import socketserver

PORT = 8080

class SimpleHTTPRequestHandler(http.server.SimpleHTTPRequestHandler):
    def do_GET(self):
        if self.path == '/simulate_resize':
            # æ¨¡æ‹ŸPod resizeäº‹ä»¶çš„å¿«é€Ÿå“åº”æ¶ˆæ¯
            self.send_response(200)
            self.send_header("Content-type", "text/plain")
            self.end_headers()
            self.wfile.write(b"æ¨¡æ‹Ÿ: Pod resize detected rapidly.")
        else:
            self.send_response(404)
            self.end_headers()

def run_server():
    with socketserver.TCPServer(("", PORT), SimpleHTTPRequestHandler) as httpd:
        print(f"HTTPæœåŠ¡å™¨è¿è¡Œåœ¨ç«¯å£ {PORT}")
        try:
            httpd.serve_forever()
        except KeyboardInterrupt:
            print("æœåŠ¡å™¨åœæ­¢è¿è¡Œ")
            httpd.server_close()

if __name__ == "__main__":
    run_server()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥å¤ç°è„šæœ¬ä¸­å¯åŠ¨äº†ä¸€ä¸ªç®€å•çš„ HTTP æœåŠ¡å™¨ï¼Œç›‘å¬åœ¨æœ¬åœ°çš„8080ç«¯å£ã€‚å½“è®¿é—®è·¯å¾„ä¸º /simulate_resize æ—¶ï¼ŒæœåŠ¡å™¨è¿”å›ä¸€ä¸ªç”¨äºæ¨¡æ‹Ÿ Pod resize å¿«é€Ÿæ£€æµ‹å“åº”çš„æç¤ºä¿¡æ¯ã€‚éœ€è¦è¯´æ˜çš„æ˜¯ï¼š
1. è¯¥è„šæœ¬ä»…ç”¨äºå±•ç¤ºå¦‚ä½•æ­å»ºä¸€ä¸ªç®€å•çš„HTTPæœåŠ¡ï¼Œå¹¶ä¸æ¶‰åŠä»»ä½•å®‰å…¨æ¼æ´çš„å¤ç°ï¼Œå› ä¸ºæœ¬ Issue æœ¬è´¨ä¸ºä¸€ä¸ªæ€§èƒ½ä¼˜åŒ–é—®é¢˜ã€‚
2. è„šæœ¬ä¸­æ²¡æœ‰æ¶‰åŠæ•æ„Ÿå‡­è¯æˆ–å¯¹å¤–æš´éœ²çš„æ•æ„Ÿä¿¡æ¯ï¼Œä¹Ÿä¸æ¨¡æ‹Ÿä»»ä½•å®‰å…¨æ”»å‡»åœºæ™¯ã€‚
3. å¤ç°è„šæœ¬ä»…ç”¨äºæ¼”ç¤ºæµç¨‹ï¼Œå¹¶ä¸ç”¨äºå®é™…çš„å®‰å…¨æµ‹è¯•æˆ–åœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„åº”ç”¨ã€‚

å› æ­¤ï¼Œè¯¥ Issue çš„æè¿°å¹¶ä¸å­˜åœ¨æ½œåœ¨å®‰å…¨é£é™©ï¼Œå› æ­¤é£é™©è¯„çº§åˆ¤æ–­ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

---


## Issue #129827 [EventedPLEG] EventedPLEG should be more tolerant of runtime outage

- Issue é“¾æ¥ï¼š[#129827](https://github.com/kubernetes/kubernetes/issues/129827)

### Issue å†…å®¹

#### What would you like to be added?

It would be better to improve the retry mechanism of EventedPLEG. EventedPLEG should be more tolerant of the outage of runtime such as restarting the runtime.




#### Why is this needed?

I tried restarting containerd when EventedPLEG is enabeld. 
```
# time systemctl restart containerd

real	0m0.401s
user	0m0.008s
sys 	0m0.007s
```

Then, Evented PLEG fell back to GenericPLEG immediately.
```
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.152929     698 evented.go:206] "Evented PLEG: Failed to get container events, retrying: " err="rpc error: code = Unavailable desc = error reading from server: EOF"
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.152970     698 log.go:32] "GetContainerEvents failed to get streaming client" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.152985     698 generic.go:242] "GenericPLEG: Relisting"
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153033     698 log.go:32] "ListPodSandbox with filter from runtime service failed" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\"" filter="nil"
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153064     698 kuberuntime_sandbox.go:305] "Failed to list pod sandboxes" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.153076     698 http2_client.go:993] "[transport] [client-transport 0xc000bc38c8] Closing: connection error: desc = \"error reading from server: EOF\"\n"
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.153156     698 logging.go:39] "[core] [Channel #9 SubChannel #10]Subchannel Connectivity change to IDLE\n"
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.153192     698 controlbuf.go:544] "[transport] [client-transport 0xc000bc38c8] loopyWriter exiting with error: connection error: desc = \"error reading from server: EOF\"\n"
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153084     698 generic.go:256] "GenericPLEG: Unable to retrieve pods" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.153280     698 evented.go:206] "Evented PLEG: Failed to get container events, retrying: " err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153327     698 log.go:32] "GetContainerEvents failed to get streaming client" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.153344     698 generic.go:242] "GenericPLEG: Relisting"
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153376     698 log.go:32] "ListPodSandbox with filter from runtime service failed" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\"" filter="nil"
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153407     698 kuberuntime_sandbox.go:305] "Failed to list pod sandboxes" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153422     698 generic.go:256] "GenericPLEG: Unable to retrieve pods" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.153436     698 evented.go:206] "Evented PLEG: Failed to get container events, retrying: " err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153460     698 log.go:32] "GetContainerEvents failed to get streaming client" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.153474     698 generic.go:242] "GenericPLEG: Relisting"
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153513     698 log.go:32] "ListPodSandbox with filter from runtime service failed" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\"" filter="nil"
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153533     698 kuberuntime_sandbox.go:305] "Failed to list pod sandboxes" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153581     698 generic.go:256] "GenericPLEG: Unable to retrieve pods" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.153589     698 evented.go:206] "Evented PLEG: Failed to get container events, retrying: " err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no su
ch file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.153609     698 pickfirst.go:208] "[pick-first-lb] [pick-first-lb 0xc000b404e0] Received SubConn state update: 0xc000b40900, {ConnectivityState:IDLE ConnectionError:<nil>}\n"
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153615     698 log.go:32] "GetContainerEvents failed to get streaming client" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.153630     698 generic.go:242] "GenericPLEG: Relisting"
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153655     698 log.go:32] "ListPodSandbox with filter from runtime service failed" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\"" filter="nil"
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153680     698 kuberuntime_sandbox.go:305] "Failed to list pod sandboxes" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or direc
tory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: E0126 13:04:10.153695     698 generic.go:256] "GenericPLEG: Unable to retrieve pods" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.153737     698 evented.go:206] "Evented PLEG: Failed to get container events, retrying: " err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: no such file or directory\""
Jan 26 13:04:10 kind-control-plane kubelet[698]: I0126 13:04:10.153766     698 evented.go:190] "Fall back to Generic PLEG relisting since Evented PLEG is not working"
```

EventedPLEG should keep working in this case because GenericPLEG also cannot work.


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue ä¸»è¦æè¿°äº†åœ¨å®¹å™¨è¿è¡Œæ—¶ï¼ˆä¾‹å¦‚ containerd é‡å¯ï¼‰å¯¼è‡´ EventedPLEG å¤±æ•ˆï¼Œä»è€Œå›é€€åˆ° GenericPLEG çš„é—®é¢˜ã€‚è¯¥é—®é¢˜æ˜¯å…³äºæé«˜å®¹é”™æœºåˆ¶ã€å¢å¼ºå¯é æ€§çš„é—®é¢˜ï¼Œä¸æ¶‰åŠæœªç»æˆæƒçš„è®¿é—®ã€ä»£ç æ‰§è¡Œã€æƒé™æå‡æˆ–å…¶ä»–å®‰å…¨æ”»å‡»çš„é£é™©ã€‚æ•´ä¸ª Issue çš„é‡ç‚¹åœ¨äºå¦‚ä½•æ”¹å–„ç³»ç»Ÿåœ¨çŸ­æš‚æ•…éšœæœŸé—´çš„è¡Œä¸ºï¼Œå¹¶ä¸æš—ç¤ºå­˜åœ¨å®‰å…¨æ¼æ´æˆ–è¢«æ¶æ„åˆ©ç”¨çš„å¯èƒ½ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
è¯¥è„šæœ¬ç”¨äºæ¨¡æ‹Ÿä¸€ä¸ªç®€å•çš„ HTTP æœåŠ¡ï¼Œç”¨æ¥è¡¨æ˜å½“å‰ Issue å¹¶æœªæ¶‰åŠå®‰å…¨é£é™©ï¼Œ
å› æ­¤æ²¡æœ‰å®‰å…¨æ¼æ´çš„å¤ç° POCã€‚
"""

import http.server
import socketserver

PORT = 8080

class Handler(http.server.SimpleHTTPRequestHandler):
    def do_GET(self):
        # ç®€å•è¿”å›ä¿¡æ¯ï¼Œè¡¨æ˜è¯¥æœåŠ¡æ­£åœ¨è¿è¡Œï¼Œæ— å®é™…å®‰å…¨æ¼æ´å¤ç°
        self.send_response(200)
        self.send_header("Content-type", "text/plain; charset=utf-8")
        self.end_headers()
        self.wfile.write(b"å½“å‰Issueæœªæ¶‰åŠå®‰å…¨é£é™©ï¼Œæ— å¤ç°POCã€‚")

def run_server():
    with socketserver.TCPServer(("", PORT), Handler) as httpd:
        print(f"HTTPæœåŠ¡å™¨å·²å¯åŠ¨ï¼Œç›‘å¬ç«¯å£{PORT}")
        try:
            httpd.serve_forever()
        except KeyboardInterrupt:
            print("æœåŠ¡å™¨åœæ­¢è¿è¡Œã€‚")

if __name__ == "__main__":
    run_server()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. æœ¬è„šæœ¬å¯åŠ¨äº†ä¸€ä¸ªåœ¨ 8080 ç«¯å£ç›‘å¬çš„ HTTP æœåŠ¡å™¨ï¼ŒæœåŠ¡å™¨è¿”å›ä¸€è¡Œç®€å•çš„æ–‡æœ¬ä¿¡æ¯ï¼Œè¯´æ˜å½“å‰ Issue å¹¶æœªæ¶‰åŠå®‰å…¨é£é™©ï¼Œä¸å­˜åœ¨å¯è¢«åˆ©ç”¨çš„æ¼æ´ã€‚
2. ç”±äº Issue è®¨è®ºçš„æ˜¯ç³»ç»Ÿåœ¨å®¹å™¨è¿è¡Œæ—¶çŸ­æš‚ä¸­æ–­æƒ…å†µä¸‹çš„å®¹é”™èƒ½åŠ›é—®é¢˜ï¼Œè€Œéå®‰å…¨æ¼æ´ï¼Œå› æ­¤æ²¡æœ‰é’ˆå¯¹å®‰å…¨æ¼æ´çš„å¤ç°è„šæœ¬ã€‚
3. è„šæœ¬ä¸­æ²¡æœ‰æ¶‰åŠéœ€è¦å‡­è¯ï¼ˆå¦‚ kubeconfigã€git tokenï¼‰çš„æƒ…å†µï¼Œä¹Ÿæ²¡æœ‰æ¶‰åŠå¯¹å¤–éƒ¨ HTTP æœåŠ¡çš„å¤æ‚äº¤äº’ï¼Œç¬¦åˆé¢˜ç›®è¦æ±‚ã€‚

---


## Issue #129825 NodeUnstageVolume should be called even when stagingPath doesn't exist

- Issue é“¾æ¥ï¼š[#129825](https://github.com/kubernetes/kubernetes/issues/129825)

### Issue å†…å®¹

#### What happened?

[pkg/volume/csi/csi_block.go](https://github.com/kubernetes/kubernetes/blob/release-1.32/pkg/volume/csi/csi_block.go#L458)

```go
	// Call NodeUnstageVolume
	stagingPath := m.GetStagingPath()
	if _, err := os.Stat(stagingPath); err != nil {
		if os.IsNotExist(err) {
			klog.V(4).Info(log("blockMapper.TearDownDevice stagingPath(%s) has already been deleted, skip calling NodeUnstageVolume", stagingPath))
		} else {
			return err
		}
	} else {
		err := m.unstageVolumeForBlock(ctx, csiClient, stagingPath)
		if err != nil {
			return err
		}
	}
```

If the stagingPath doesn't exist, NodeUnstageVolume won't be called.

In my opinion, NodeUnstageVolume should be called even when stagingPath doesn't exist, for the following reasons:

1. As long as CSI supports StageUnstage capability, the CSI should be able to properly handle cases where stagingPath doesn't exist
2. If NodeUnstageVolume return error but stagingPath is cleaned up, NodeUnstageVolume won't be called again in next retry
3. For filesystem PVCs, NodeUnstageVolume is called normally even when stagingPath doesn't exist
4. When a CSI driver is upgraded from non-StageUnstage to StageUnstage support, volumes mounted before the upgrade won't have stagingPath. However, these pre-upgrade volumes still need NodeUnstageVolume to be called for proper unmounting

#### What did you expect to happen?

For block pvc, NodeUnstageVolume should be called even when stagingPath doesn't exist 

#### How can we reproduce it (as minimally and precisely as possible)?

It can be intuitively seen from the code

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
1.32
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ issue æè¿°çš„æ˜¯ Kubernetes åœ¨å¤„ç† CSI block volume çš„å¸è½½æµç¨‹ä¸­ï¼Œå½“ stagingPath ä¸å­˜åœ¨æ—¶ï¼Œä¸è°ƒç”¨ NodeUnstageVolume çš„é—®é¢˜ã€‚è¯¥é—®é¢˜ä¸»è¦æ¶‰åŠå¸è½½é€»è¾‘çš„æ­£ç¡®æ€§å’Œä¸€è‡´æ€§ï¼Œå¯èƒ½ä¼šå¯¼è‡´æŸäº›æƒ…å†µä¸‹ volume æ— æ³•æ­£å¸¸å¸è½½ï¼Œä»è€Œç•™ä¸‹æ®‹ç•™çŠ¶æ€ï¼Œä½†ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¿™å¹¶ä¸ä¼šç›´æ¥å¼•å‘å‘½ä»¤æ‰§è¡Œã€æ‹’ç»æœåŠ¡ã€å®¹å™¨é€ƒé€¸ã€æƒé™æå‡ç­‰é«˜é£é™©æ”»å‡»ã€‚å› æ­¤ï¼Œæ­¤é—®é¢˜å±äºé€»è¾‘æˆ–åŠŸèƒ½ç¼ºé™·ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
è¯¥è„šæœ¬ç”¨äºæ¨¡æ‹Ÿ issue ä¸­æåˆ°çš„é€»è¾‘ï¼Œå³å½“ stagingPath ä¸å­˜åœ¨æ—¶ï¼Œä¸è°ƒç”¨ NodeUnstageVolumeã€‚
è¯¥è„šæœ¬ä»…ç”¨äºæ¼”ç¤ºé—®é¢˜çš„ä»£ç é€»è¾‘ï¼Œä¸ä¼šäº§ç”Ÿå®‰å…¨é£é™©ï¼Œä¹Ÿä¸ä¼šå¯¹çœŸå®ç³»ç»Ÿé€ æˆå½±å“ã€‚
"""

import os
import logging
import tempfile
import shutil

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def node_unstage_volume(staging_path):
    """
    æ¨¡æ‹Ÿ NodeUnstageVolume çš„è°ƒç”¨è¿‡ç¨‹
    """
    logging.info("è°ƒç”¨ NodeUnstageVolumeï¼Œstaging_path: %s", staging_path)
    # æ¨¡æ‹Ÿ CSI é©±åŠ¨å¯¹ä¸å­˜åœ¨çš„è·¯å¾„è¿›è¡Œå¤„ç†ï¼Œæ‰“å°æˆåŠŸæ—¥å¿—
    return True


def simulate_unmount(staging_path):
    """
    æ¨¡æ‹Ÿå¸è½½é€»è¾‘ï¼šå¦‚æœ staging_path å­˜åœ¨ï¼Œåˆ™è°ƒç”¨ NodeUnstageVolumeï¼›
    å¦‚æœ staging_path ä¸å­˜åœ¨ï¼Œåˆ™ä¸è°ƒç”¨ NodeUnstageVolumeï¼Œå¹¶è¾“å‡ºå¯¹åº”æ—¥å¿—
    """
    logging.info("æ£€æµ‹ staging_path: %s", staging_path)
    if not os.path.exists(staging_path):
        logging.info("staging_path ä¸å­˜åœ¨ï¼Œè·³è¿‡è°ƒç”¨ NodeUnstageVolume")
        return False
    else:
        result = node_unstage_volume(staging_path)
        return result


def main():
    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶ç›®å½•æ¨¡æ‹Ÿ stagingPath çš„å­˜åœ¨æƒ…å†µ
    temp_dir = tempfile.mkdtemp(prefix="staging_")
    logging.info("åˆ›å»ºä¸´æ—¶ staging_path: %s", temp_dir)
    
    # æƒ…æ™¯ 1ï¼šstagingPath å­˜åœ¨
    logging.info("æƒ…æ™¯ 1ï¼šstagingPath å­˜åœ¨")
    simulate_unmount(temp_dir)
    
    # æ¸…ç† stagingPath æ¨¡æ‹Ÿè·¯å¾„åˆ é™¤çš„æƒ…å†µ
    shutil.rmtree(temp_dir)
    logging.info("å·²åˆ é™¤ä¸´æ—¶ staging_path: %s", temp_dir)
    
    # æƒ…æ™¯ 2ï¼šstagingPath ä¸å­˜åœ¨
    logging.info("æƒ…æ™¯ 2ï¼šstagingPath ä¸å­˜åœ¨")
    simulate_unmount(temp_dir)


if __name__ == "__main__":
    main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. è„šæœ¬ä¸­é¦–å…ˆå®šä¹‰äº†ä¸€ä¸ªæ¨¡æ‹Ÿå‡½æ•° node_unstage_volume æ¥è¡¨ç¤ºå¯¹ NodeUnstageVolume çš„è°ƒç”¨ï¼Œè¯¥å‡½æ•°ç›®å‰ä»…è®°å½•æ—¥å¿—æ¨¡æ‹Ÿè°ƒç”¨è¿‡ç¨‹ã€‚
2. simulate_unmount å‡½æ•°æŒ‰ç…§ issue ä¸­ä»£ç é€»è¾‘åˆ¤æ–­ staging_path æ˜¯å¦å­˜åœ¨ã€‚å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™è¾“å‡ºæ—¥å¿—è¯´æ˜è·³è¿‡ NodeUnstageVolume è°ƒç”¨ï¼›å¦‚æœå­˜åœ¨ï¼Œåˆ™è°ƒç”¨ node_unstage_volumeã€‚
3. main å‡½æ•°ä¸­ï¼š
   - é€šè¿‡ tempfile æ¨¡å—åˆ›å»ºäº†ä¸€ä¸ªä¸´æ—¶ç›®å½•ï¼Œæ¨¡æ‹Ÿ stagingPath å­˜åœ¨æ—¶çš„åœºæ™¯ï¼Œç„¶åè°ƒç”¨ simulate_unmount å±•ç°è°ƒç”¨è¿‡ç¨‹ã€‚
   - éšååˆ é™¤è¯¥ç›®å½•ï¼Œæ¨¡æ‹Ÿ stagingPath è¢«æ„å¤–åˆ é™¤çš„åœºæ™¯ï¼Œå†æ¬¡è°ƒç”¨ simulate_unmount å±•ç¤ºå½“è·¯å¾„ä¸å­˜åœ¨æ—¶ï¼Œè·³è¿‡ NodeUnstageVolume è°ƒç”¨çš„è¡Œä¸ºã€‚

æ­¤è„šæœ¬ä»…ç”¨äºç ”ç©¶å’Œæœ¬åœ°æµ‹è¯•ï¼Œå¸®åŠ©ç†è§£ issue ä¸­æè¿°çš„ä»£ç é€»è¾‘ä¸è¡Œä¸ºï¼Œä¸”ä¸æ¶‰åŠä»»ä½•å®‰å…¨é£é™©é—®é¢˜ã€‚

---


## Issue #129819 Memory manager and CPU manager state file is not updated immediately when a guaranteed pod is deleted

- Issue é“¾æ¥ï¼š[#129819](https://github.com/kubernetes/kubernetes/issues/129819)

### Issue å†…å®¹

#### What happened?

For guaranteed pod, when Memory Manager/CPU Manager are enabled there are entries in /var/lib/kubelet/memory_manager_state and /var/lib/kubelet/cpu_manager_state that are created capturing the resource allocations per NUMA.

When the pod is deleted the entries in these files are not deleted till a new pod is deployed. Looking at the code, these entries are deleted only when new entries are added as part of pod deployment as part of removeStaleState() where garbage collection happens.

Is this by design?

#### What did you expect to happen?

Entries in /var/lib/kubelet memory_manager_state and cpu_manager_state files are correctly updated on pod delete.

#### How can we reproduce it (as minimally and precisely as possible)?

Deployed a guaranteed pod with CPU and memory resources specified. CPU manager and Memory manager configured

Using following options in kubelet configuration

--cpu-manager-policy=static --topology-manager-policy=restricted --topology-manager-scope=pod --pod-max-pids=4096 --memory-manager-policy=Static --reserved-memory=0:memory=1024Mi --kube-reserved=memory=462Mi --system-reserved=memory=462Mi

#### Anything else we need to know?

By adding following change in PostStopContainer() as part of container lifecycle I am seeing the expected behavior. Is it a safe change or was there a reason to not add the cleanup as part of pod deletes. The only reason cleanup is not performed as part of delete would be to reduce the lock contention updating the state files. Is that right?

diff --git a/pkg/kubelet/cm/internal_container_lifecycle.go b/pkg/kubelet/cm/internal_container_lifecycle.go
index 5e50fd1..8cc9d13 100644
--- a/pkg/kubelet/cm/internal_container_lifecycle.go
+++ b/pkg/kubelet/cm/internal_container_lifecycle.go
@@ -52,5 +52,13 @@ func (i *internalContainerLifecycleImpl) PreStartContainer(pod *v1.Pod, containe
 }

 func (i *internalContainerLifecycleImpl) PostStopContainer(containerID string) error {
+       if i.cpuManager != nil {
+               i.cpuManager.RemoveContainer(containerID)
+       }
+
+       if i.memoryManager != nil {
+               i.memoryManager.RemoveContainer(containerID)
+       }
+
        return i.topologyManager.RemoveContainer(containerID)
 }

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.3
```

</details>


#### Cloud provider

<details>
On Prem Cluster
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="CentOS Linux"
VERSION="7 (Core)"
ID="centos"
ID_LIKE="rhel fedora"
VERSION_ID="7"
PRETTY_NAME="CentOS Linux 7 (Core)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:centos:centos:7"
HOME_URL="https://www.centos.org/"
BUG_REPORT_URL="https://bugs.centos.org/"

CENTOS_MANTISBT_PROJECT="CentOS-7"
CENTOS_MANTISBT_PROJECT_VERSION="7"
REDHAT_SUPPORT_PRODUCT="centos"
REDHAT_SUPPORT_PRODUCT_VERSION="7"

$ uname -a
Linux qct-23 3.10.0-1160.59.1.el7.x86_64 #1 SMP Wed Feb 23 16:47:03 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue è®¨è®ºçš„æ˜¯å½“å¯ç”¨äº† Memory Manager å’Œ CPU Manager æ—¶ï¼Œå¯¹äºä¿è¯å‹ podï¼Œkubelet åœ¨ pod åˆ é™¤åæ²¡æœ‰ç«‹å³æ¸…é™¤ /var/lib/kubelet/memory_manager_state å’Œ /var/lib/kubelet/cpu_manager_state æ–‡ä»¶ä¸­çš„è®°å½•ï¼Œè€Œæ˜¯ç­‰åˆ°æ–° pod éƒ¨ç½²æ—¶å†è¿›è¡Œåƒåœ¾å›æ”¶ï¼ˆremoveStaleState()ï¼‰ã€‚è¯¥é—®é¢˜ä¸»è¦æ¶‰åŠçŠ¶æ€æ–‡ä»¶çš„æ›´æ–°æ—¶æœºå’Œæ¸…ç†æœºåˆ¶ï¼Œå¯èƒ½ä¼šå¯¼è‡´çŠ¶æ€ä¿¡æ¯å»¶è¿Ÿæ›´æ–°ï¼Œä»è€Œå½±å“èµ„æºç›‘æ§å’Œè°ƒåº¦ï¼Œä½†å¹¶æ²¡æœ‰æš´éœ²ç³»ç»Ÿæ•æ„Ÿä¿¡æ¯ã€å­˜åœ¨ä¸å½“é…ç½®æˆ–è€…ä½¿æ”»å‡»è€…èƒ½å¤Ÿè¿œç¨‹æ‰§è¡Œä»£ç ã€ææƒã€å®¹å™¨é€ƒé€¸ç­‰ã€‚å› æ­¤ï¼Œè¿™ä¸ªé—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªé€»è¾‘æ€§æˆ–è®¾è®¡ä¸Šçš„é—®é¢˜ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

Issue ä¸­æè¿°çš„ç°è±¡å±äº kubelet å¯¹èµ„æºç®¡ç†çŠ¶æ€æ–‡ä»¶æ¸…ç†æœºåˆ¶çš„è®¾è®¡æˆ–å®ç°é—®é¢˜ï¼Œå¹¶ä¸ä¼šç›´æ¥å¯¼è‡´ç³»ç»Ÿå®‰å…¨é£é™©ï¼Œæ¯”å¦‚å‘½ä»¤æ‰§è¡Œã€ææƒæˆ–å…¶ä»–é«˜å±æ¼æ´ã€‚å¤ç°è„šæœ¬çš„ä»£ç ä¸»è¦ç”¨äºéªŒè¯å®‰å…¨é£é™©åœ¨çœŸå®ç¯å¢ƒä¸­çš„å¤ç°ï¼Œè€Œæœ¬é—®é¢˜å¹¶æ— å®‰å…¨éšæ‚£ï¼Œå› æ­¤ä¸éœ€è¦ç”Ÿæˆå¤ç°è„šæœ¬ã€‚è‹¥å¸Œæœ›éªŒè¯è¯¥é—®é¢˜çš„å­˜åœ¨ï¼Œå¯ä»¥é€šè¿‡éƒ¨ç½²ä¸€ä¸ªé…ç½®äº† CPU Manager å’Œ Memory Manager çš„ guaranteed podï¼Œç„¶ååœ¨åˆ é™¤ pod åè§‚å¯Ÿç›¸åº”çŠ¶æ€æ–‡ä»¶çš„å†…å®¹å˜åŒ–ï¼Œä½†è¿™å±äºæ­£å¸¸çš„èµ„æºè°ƒåº¦é€»è¾‘éªŒè¯ï¼Œå¹¶ä¸æ¶‰åŠå®‰å…¨æ”»å‡»è¡Œä¸ºã€‚

---


## Issue #129797 Unable to create service with ipFamilyPolicy RequireDualStack when service cidr is bigger then /112

- Issue é“¾æ¥ï¼š[#129797](https://github.com/kubernetes/kubernetes/issues/129797)

### Issue å†…å®¹

#### What happened?

We run periodic conformance tests in github.com/kubernetes-sigs/cluster-api and they started to fail https://testgrid.k8s.io/sig-cluster-lifecycle-cluster-api#capi-e2e-latestk8s-main

These currently
* use https://dl.k8s.io/ci/latest-1.33.txt to build kind images from source with that version
* create a Cluster with that version by using Cluster API and cluster-api-provider-docker (and in this case with dualstack ip enabled)
* They set `--service-cluster-ip-range=10.128.0.0/12,fd00:100:64::/108` in kube-apiserver (and kube-controller-manager)
* And after finishing cluster creation run conformance tests, including the selector for the `IPv6DualStack` feature tests.

Several conformace tests for `IPv6DualStack` then failed with e.g. the following message: 
```
I0124 07:48:52.822155 16 utils.go:776] Unexpected error: Failed to create node-port-service service: Internal error occurred: failed to allocate a serviceIP: range is full: 
    <*errors.StatusError | 0xc0031bb9a0>: 
    Internal error occurred: failed to allocate a serviceIP: range is full
    {
        ErrStatus: 
            code: 500
            details:
              causes:
              - message: 'failed to allocate a serviceIP: range is full'
            message: 'Internal error occurred: failed to allocate a serviceIP: range is full'
            metadata: {}
            reason: InternalError
            status: Failure,
    }
[FAILED] Failure recorded during attempt 1:
```

Note: The kube-apiserver is able to create the IPAddress object, but the service does not get created.

#### What did you expect to happen?

Tests to succeed.

#### How can we reproduce it (as minimally and precisely as possible)?

Build kindest/node image for k/k commit 0798325ba13643

```
# checkout k/k on 0798325ba13643
kind build node-image --image "kindest/node:v1.33.0-alpha.0.614_0798325ba13643" $(pwd)
```

Create a kind cluster using the following configuration:

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
networking:
  ipFamily: dual
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    kind: ClusterConfiguration
    apiServer:
        extraArgs:
          service-cluster-ip-range: "10.96.0.10/12,fd00:100:64::/108"
```

```sh
kind create cluster --name test --config kind-cluster.yaml --image kindest/node:v1.33.0-alpha.0.614_0798325ba13643
```

Try to create a service with `.spec.ipFamilyPolicy` set to `RequireDualStack` or `.spec.ipFamilies` set to `["IPv6"]`:

```sh
$ kubectl create svc clusterip --tcp=80:80 foo -o json --dry-run=client | jq '.spec.ipFamilies = ["IPv6"]' | k apply -f -
Error from server (InternalError): error when creating "STDIN": Internal error occurred: failed to allocate a serviceIP: range is full

$ kubectl create svc clusterip --tcp=80:80 foo -o json --dry-run=client | jq '.spec.ipFamilies = ["IPv6"]' | k apply -f -
Error from server (InternalError): error when creating "STDIN": Internal error occurred: failed to allocate a serviceIP: range is full
```

Note: there's a chance that the ip allocation in kube-apiserver picks an ipv6 address which is then part the first /112 part of and then works.

#### Anything else we need to know?

Follow-up to https://github.com/kubernetes/kubernetes/issues/129753#issuecomment-2609425004

I also stepped into kube-apiserver:
* It fails to validate here that the ip is contained in the CIDR (example configuration: CIDR `fd00:100:64::/108`, IP `fd00:100:64::f:321c`
  * https://github.com/kubernetes/kubernetes/blob/0798325ba13643358aa3ebb7c6ddc3006ac26a7c/pkg/registry/core/service/ipallocator/bitmap.go#L173-L179
* The IP is part of that network, but the bitmap is capped to `/112`


Workarounds:
* Enable FeatureGate `DisableAllocatorDualWrite` in kube-apiserver
* Use a `/112` cidr for services


cc @aojea 

And Kudos for Antonio helping me to dig into it!

#### Kubernetes version

<details>

```console
â¯ kubectl version
Client Version: v1.29.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.33.0-alpha.0.614+0798325ba13643
WARNING: version difference between client (1.29) and server (1.33) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

---

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ issue æè¿°çš„æ˜¯åœ¨ Kubernetes é›†ç¾¤ä¸­ï¼Œå½“ä½¿ç”¨ dual-stack æœåŠ¡ä¸”é…ç½®çš„ IPv6 æœåŠ¡ CIDR å¤§äº /112 æ—¶ï¼Œåœ¨åˆ›å»ºæœåŠ¡æ—¶ä¼šæŠ¥é”™â€œfailed to allocate a serviceIP: range is fullâ€ã€‚ç»è¿‡åˆ†æï¼Œå¯ä»¥çœ‹å‡ºé—®é¢˜å‡ºåœ¨ kube-apiserver å†…éƒ¨å¯¹ IP åˆ†é…çš„ bitmap å®ç°å­˜åœ¨é™åˆ¶ï¼šè™½ç„¶ IP å±äºé…ç½®çš„ CIDRï¼Œä½†åˆ†é…å™¨å†…éƒ¨ä»…å¤„ç† /112 çš„èŒƒå›´ï¼Œå¯¼è‡´å®é™…å¯ç”¨çš„ IP æ•°é‡ä¸å¤Ÿï¼Œä»è€ŒæŠ¥é”™ã€‚è¯¥é—®é¢˜å±äºé…ç½®ä¸å†…éƒ¨é€»è¾‘çš„ bugï¼Œä¸æ¶‰åŠæœªç»æˆæƒçš„æƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰é«˜é£é™©å®‰å…¨é—®é¢˜ã€‚

æŒ‰ç…§ç›®å‰æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤é—®é¢˜å¹¶éå®‰å…¨é—®é¢˜ï¼Œå…¶äº§ç”Ÿçš„é”™è¯¯ä¹Ÿåªä¼šå¯¼è‡´æœåŠ¡åˆ›å»ºå¤±è´¥ï¼Œå¹¶ä¸ä¼šç›´æ¥è¢«æ”»å‡»è€…åˆ©ç”¨æ¥æ‰§è¡Œæ¶æ„æ“ä½œã€‚å› æ­¤ï¼Œæœ¬ issue çš„é£é™©è¯„çº§åº”ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

---

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
import threading
import http.server
import socketserver
import time
import sys
import json

from kubernetes import client, config
from kubernetes.client.rest import ApiException

# HTTP æœåŠ¡å™¨å¤„ç†ç±»ï¼Œç”¨äºç›‘å¬8080ç«¯å£ï¼Œè¾…åŠ©è§‚å¯Ÿï¼ˆå¦‚æœéœ€è¦ï¼‰
class SimpleHTTPRequestHandler(http.server.SimpleHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.end_headers()
        response = {
            "message": "HTTP Server is running on port 8080",
            "path": self.path
        }
        self.wfile.write(json.dumps(response).encode('utf-8'))

def start_http_server():
    PORT = 8080
    handler = SimpleHTTPRequestHandler
    with socketserver.TCPServer(("", PORT), handler) as httpd:
        print(f"HTTP server is serving at port {PORT}")
        httpd.serve_forever()

def create_dualstack_service():
    try:
        # å°è¯•åŠ è½½é»˜è®¤ kubeconfig é…ç½®
        config.load_kube_config()
    except Exception as e:
        print(f"åŠ è½½ kubeconfig å¤±è´¥: {e}")
        sys.exit(1)

    v1 = client.CoreV1Api()

    # æ„é€ æœåŠ¡å¯¹è±¡ï¼Œæ³¨æ„ ipFamilies å’Œ ipFamilyPolicy å­—æ®µéœ€è¦è®¾ç½®ä¸ºè§¦å‘é—®é¢˜çš„é…ç½®
    service_manifest = {
        "apiVersion": "v1",
        "kind": "Service",
        "metadata": {
            "name": "dualstack-test-service"
        },
        "spec": {
            "ports": [
                {
                    "protocol": "TCP",
                    "port": 80,
                    "targetPort": 80
                }
            ],
            "selector": {
                "app": "dualstack-test"
            },
            "type": "ClusterIP",
            "ipFamilies": ["IPv6"],
            "ipFamilyPolicy": "RequireDualStack"
        }
    }

    try:
        print("å°è¯•åœ¨ default å‘½åç©ºé—´åˆ›å»ºæœåŠ¡ dualstack-test-service ...")
        resp = v1.create_namespaced_service(namespace="default", body=service_manifest)
        print("æœåŠ¡åˆ›å»ºæˆåŠŸï¼Œè¿”å›ç»“æœï¼š")
        print(resp)
    except ApiException as e:
        print("è°ƒç”¨ API æ—¶å‘ç”Ÿå¼‚å¸¸ï¼š")
        print(e)
    except Exception as ex:
        print("å‘ç”Ÿå¼‚å¸¸ï¼š", ex)

def main():
    # å¯åŠ¨ HTTP æœåŠ¡å™¨çº¿ç¨‹
    http_thread = threading.Thread(target=start_http_server, daemon=True)
    http_thread.start()

    # ç­‰å¾… HTTP æœåŠ¡å™¨å¯åŠ¨
    time.sleep(1)

    # å°è¯•åˆ›å»º dualstack æœåŠ¡ï¼Œç”¨ä»¥å¤ç° issue æè¿°çš„é—®é¢˜
    create_dualstack_service()

    # ä¿æŒä¸»çº¿ç¨‹è¿è¡Œï¼Œä¾¿äºè§‚å¯Ÿ HTTP æœåŠ¡å™¨å“åº”
    try:
        while True:
            time.sleep(2)
    except KeyboardInterrupt:
        print("é€€å‡ºç¨‹åº")

if __name__ == "__main__":
    main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. è„šæœ¬é¦–å…ˆåŠ è½½é»˜è®¤ä½ç½®çš„ kubeconfigï¼ˆå‡è®¾å‡­è¯å·²åœ¨é»˜è®¤è·¯å¾„ä¸­ï¼‰ï¼Œä½¿ç”¨ Kubernetes Python å®¢æˆ·ç«¯è¿æ¥åˆ°é›†ç¾¤ã€‚
2. åœ¨ create_dualstack_service() å‡½æ•°ä¸­ï¼Œæ„é€ äº†ä¸€ä¸ª Service å¯¹è±¡ï¼ŒæœåŠ¡è§„æ ¼ä¸­æ˜ç¡®è®¾ç½®äº† ipFamilies ä¸º ["IPv6"] ä»¥åŠ ipFamilyPolicy è®¾ç½®ä¸º RequireDualStackï¼Œè¿™æ­£æ˜¯è§¦å‘ issue ä¸­æŠ¥é”™çš„é…ç½®æ¡ä»¶ã€‚
3. è°ƒç”¨ CoreV1Api çš„ create_namespaced_service() æ–¹æ³•å°è¯•åœ¨ default å‘½åç©ºé—´ä¸‹åˆ›å»ºè¯¥æœåŠ¡ã€‚å¦‚æœé›†ç¾¤å­˜åœ¨ä¸ issue æè¿°ç›¸ç¬¦çš„é…ç½®ï¼Œåˆ™æœ‰å¯èƒ½å¤ç°â€œfailed to allocate a serviceIP: range is fullâ€çš„é”™è¯¯ã€‚
4. è„šæœ¬åŒæ—¶å¯åŠ¨äº†ä¸€ä¸ªç®€æ˜“çš„ HTTP æœåŠ¡å™¨ï¼Œç›‘å¬åœ¨ 8080 ç«¯å£ï¼Œç”¨äºåœ¨æœ¬åœ°è§‚å¯Ÿå’Œè°ƒè¯•ï¼ˆä¾‹å¦‚å¯ä»¥é€šè¿‡è®¿é—® http://localhost:8080æŸ¥çœ‹ HTTP æœåŠ¡çŠ¶æ€ï¼‰ã€‚
5. è¯·æ³¨æ„ï¼Œè¯¥å¤ç°è„šæœ¬ä»…ç”¨äºç ”ç©¶å’Œæœ¬åœ°æµ‹è¯•ï¼Œä¸ä¼šå¯¹ç”Ÿäº§ç¯å¢ƒé€ æˆä»»ä½•å½±å“ã€‚

---


## Issue #129795 ReplicaSet controller may create extra Pods when expectations expire during informer delays

- Issue é“¾æ¥ï¼š[#129795](https://github.com/kubernetes/kubernetes/issues/129795)

### Issue å†…å®¹

#### What happened?

The ReplicaSet controller may create more Pods than desired under the following conditions:

1. ReplicaSet controller creates N Pods and sets expectations (+N)
2. Due to network issues or high latency, PodInformer hasn't received the Pod creation events
3. After 5 minutes, the expectations expire (isExpired() == true)
4. The `SatisfiedExpectations` function returns `true` immediately when expectations expire, without checking if they are actually fulfilled
5. This causes the controller to create additional Pods, even though the previously created Pods may still exist

Code path ï¼šhttps://github.com/kubernetes/kubernetes/blob/f6f06806cc43ed9f7eb2f68368c90a8239884118/pkg/controller/controller_utils.go#L193

```
func (r *ControllerExpectations) SatisfiedExpectations(logger klog.Logger, controllerKey string) bool {
	if exp, exists, err := r.GetExpectations(controllerKey); exists {
		if exp.Fulfilled() {
			logger.V(4).Info("Controller expectations fulfilled", "expectations", exp)
			return true
		} else if exp.isExpired() {
			logger.V(4).Info("Controller expectations expired", "expectations", exp)
			return true  // There is an issue here. !!! 
		} else {
			logger.V(4).Info("Controller still waiting on expectations", "expectations", exp)
			return false
		}
	} else if err != nil {
		logger.V(2).Info("Error encountered while checking expectations, forcing sync", "err", err)
	} else {
		// When a new controller is created, it doesn't have expectations.
		// When it doesn't see expected watch events for > TTL, the expectations expire.
		//	- In this case it wakes up, creates/deletes controllees, and sets expectations again.
		// When it has satisfied expectations and no controllees need to be created/destroyed > TTL, the expectations expire.
		//	- In this case it continues without setting expectations till it needs to create/delete controllees.
		logger.V(4).Info("Controller either never recorded expectations, or the ttl expired", "controller", controllerKey)
	}
	// Trigger a sync if we either encountered and error (which shouldn't happen since we're
	// getting from local store) or this controller hasn't established expectations.
	return true
}
```

If the PodInformer is delayed, manageReplicas will create too many Pods.
https://github.com/kubernetes/kubernetes/blob/f6f06806cc43ed9f7eb2f68368c90a8239884118/pkg/controller/replicaset/replica_set.go#L572

```
	if diff < 0 {
		diff *= -1
		if diff > rsc.burstReplicas {
			diff = rsc.burstReplicas
		}
		rsc.expectations.ExpectCreations(logger, rsKey, diff)
		successfulCreations, err := slowStartBatch(diff, controller.SlowStartInitialBatchSize, func() error {
			err := rsc.podControl.CreatePods(ctx, rs.Namespace, &rs.Spec.Template, rs, metav1.NewControllerRef(rs, rsc.GroupVersionKind))
			if err != nil {
				if apierrors.HasStatusCause(err, v1.NamespaceTerminatingCause) {
					return nil
				}
			}
			return err
		})

		if skippedPods := diff - successfulCreations; skippedPods > 0 {
			logger.V(2).Info("Slow-start failure. Skipping creation of pods, decrementing expectations", "podsSkipped", skippedPods, "kind", rsc.Kind, "replicaSet", klog.KObj(rs))
			for i := 0; i < skippedPods; i++ {
				rsc.expectations.CreationObserved(logger, rsKey)
			}
		}
		return err
	} 
``` 


#### What did you expect to happen?

When controller expectations expire, the ReplicaSet controller should still ensure the actual Pod count matches the desired count, even if there are informer delays. Specifically:

1. If expectations expire but are not fulfilled (add/delete operations not observed), the controller should wait for the informer to catch up rather than immediately proceeding with new Pod creations.

2. The `SatisfiedExpectations` function should return `false` when expectations are expired but not fulfilled, preventing potential Pod over-provisioning during informer delays.

This would maintain the safety guarantee that a ReplicaSet never creates more Pods than desired, even in the presence of informer delays.

#### How can we reproduce it (as minimally and precisely as possible)?

NONE

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue åæ˜ çš„æ˜¯ Kubernetes ReplicaSet controller åœ¨ informer å»¶è¿Ÿæ—¶å‡ºç°çš„ä¸€å¤„é€»è¾‘ç¼ºé™·ï¼šå½“æœŸæœ›çŠ¶æ€è¶…æ—¶åï¼ˆisExpired è¿”å› trueï¼‰ï¼Œé€»è¾‘ç›´æ¥å°†æœŸæœ›åˆ¤æ–­ä¸ºæ»¡è¶³ï¼Œä»è€Œå¯¼è‡´ controller é‡å¤åˆ›å»º Podï¼Œå¯èƒ½äº§ç”Ÿé¢å¤–çš„ Podã€‚è¯¥é—®é¢˜ä¼šå¯¼è‡´å®é™… Pod æ•°å¤§äºé¢„æœŸï¼Œè¿›è€Œå¯èƒ½å¼•èµ·èµ„æºæµªè´¹ç”šè‡³èµ„æºè€—å°½ï¼Œç†è®ºä¸Šå¯èƒ½è§¦å‘æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰çš„åœºæ™¯ã€‚ä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼š
1. æ­¤ç¼ºé™·æ˜¯ç”±äºæ§åˆ¶å™¨å†…éƒ¨é€»è¾‘åˆ¤æ–­é”™è¯¯å¼•èµ·çš„ï¼Œè€Œéå¤–éƒ¨è¾“å…¥ã€ä»£ç æ‰§è¡Œã€å‘½ä»¤æ³¨å…¥æˆ–ææƒç­‰å®‰å…¨æ¼æ´ã€‚
2. è§¦å‘é—®é¢˜ä¾èµ–äº informer å»¶è¿Ÿæˆ–ç½‘ç»œé—®é¢˜ç­‰ç‰¹æ®Šåœºæ™¯ï¼Œä¸”æ”»å‡»è€…é€šå¸¸éœ€è¦å…·æœ‰è¾ƒé«˜æƒé™æˆ–æ§åˆ¶é›†ç¾¤ç½‘ç»œï¼Œæ‰èƒ½äººä¸ºè§¦å‘è¿™ç§æƒ…å†µã€‚
3. æ ¹æ®ç»™å®šçš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œå¯¹ä»…ä¼šå¼•èµ·é¢å¤– Pod åˆ›å»ºï¼ˆå³èµ„æºæµªè´¹æˆ–å¯èƒ½å¯¼è‡´ DoSï¼‰çš„ç¼ºé™·ï¼Œå¹¶æœªæ¶‰åŠæœªæˆæƒè®¿é—®ã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰é«˜å®‰å…¨é£é™©è¡Œä¸ºã€‚

ç»¼åˆæ¥çœ‹ï¼Œè¿™ä¸ªé—®é¢˜å±äºç¨‹åºé€»è¾‘ç¼ºé™·ï¼Œè™½ç„¶åœ¨ä¸€å®šæ¡ä»¶ä¸‹å¯èƒ½å¼•å‘æœåŠ¡æ‹’ç»ï¼ˆDoSï¼‰çš„å½±å“ï¼Œä½†ä¸æ„æˆç›´æ¥çš„å®‰å…¨é—®é¢˜ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Issue æ‰€æè¿°çš„é—®é¢˜ä¸º ReplicaSet controller åœ¨ informer å»¶è¿Ÿå¯¼è‡´é¢„æœŸçŠ¶æ€å¤±æ•ˆæƒ…å†µä¸‹ï¼Œé”™è¯¯åˆ¤æ–­æœŸæœ›å·²æ»¡è¶³ï¼Œä»è€Œé‡æ–°åˆ›å»º Pod çš„é€»è¾‘ç¼ºé™·ã€‚è™½ç„¶åœ¨ç‰¹å®šåœºæ™¯ä¸‹å¯èƒ½å› é¢å¤–åˆ›å»º Pod å¯¼è‡´èµ„æºè¿‡åº¦ä½¿ç”¨ï¼Œè¿›è€Œå¼•å‘æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰çš„é£é™©ï¼Œä½†è¯¥é—®é¢˜çš„è§¦å‘æ¡ä»¶è¦æ±‚å¤–éƒ¨ç¯å¢ƒï¼ˆå¦‚ç½‘ç»œå»¶è¿Ÿï¼‰å¤„äºå¼‚å¸¸çŠ¶æ€ï¼Œä¸”æ”»å‡»è€…é€šå¸¸éœ€è¦å…·å¤‡è¾ƒé«˜æƒé™æ‰èƒ½å½±å“æ§åˆ¶å™¨è¡Œä¸ºã€‚å› æ­¤ï¼Œè¯¥ç¼ºé™·å¹¶ä¸å±äºç›´æ¥çš„å®‰å…¨æ¼æ´ï¼Œä¹Ÿä¸å­˜åœ¨æœªæˆæƒè®¿é—®ã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸æˆ–ææƒç­‰é«˜å®‰å…¨é£é™©æƒ…å½¢ã€‚ä¾ç…§é—®é¢˜æè¿°åŠé£é™©åˆ¤å®šæ ‡å‡†ï¼Œè¯¥é—®é¢˜åº”è¢«å½’ç±»ä¸ºâ€œä¸æ¶‰åŠâ€å®‰å…¨é£é™©ã€‚

---


## Issue #129899 `kubectl auth can-i` gives incorrect result when rolebinding is present but incorrect

- Issue é“¾æ¥ï¼š[#129899](https://github.com/kubernetes/kubernetes/issues/129899)

### Issue å†…å®¹

<!-- Please use this template while reporting a bug and provide as much info as possible. Not doing so may result in your bug not being addressed in a timely manner. Thanks!

If the matter is security related, please disclose it privately via https://kubernetes.io/security/
-->

**What happened**:

I created a serviceaccount, role and rolebinding to read secrets in a given namespace. I forgot to specify the namespace in the rolebinding, meaning the serviceaccount wasn't actually able to read the secrets. However, when I ran `kubectl auth can-i get secrets -as <serviceaccount> -n <namespace>` the answer came back as `yes`. 

**What you expected to happen**:

I would have expected the answer to be `no`, which would then have told me the issue was somewhere in the role/rolebinding, as opposed to in the system using the serviceaccount.

**How to reproduce it (as minimally and precisely as possible)**:
```bash
$ kubectl version
Client Version: v1.30.3
Server Version: v1.30.1
```

Create a serviceaccount:

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: example-sa
  namespace: example
```

Create a role granting the necessary permissions:

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: example
  name: secretreader
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "list", "watch" ]
```

Create a rolebinding, but forget to add  `metadata.namespace`

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: secretreader
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: secretreader
subjects:
- apiGroup: ""
  kind: ServiceAccount
  name: example-sa
  namespace: example
```
run the command:

```bash
$ kubectl auth can-i get secrets -as example-sa -n example
yes
```

**Anything else we need to know?**:

**Environment**:
- Kubernetes client and server versions (use `kubectl version`): 
-- Client Version: v1.30.3
-- Server Version: v1.30.1
- Cloud provider or hardware configuration: VM on our self-managed OpenStack env
- OS: Talos (v1.7.4)



### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ issue æŠ¥å‘Šçš„æ˜¯å…³äº RBAC æˆæƒæ£€æµ‹å‘½ä»¤ â€œkubectl auth can-iâ€ è¿”å›ç»“æœä¸é¢„æœŸä¸ç¬¦çš„é—®é¢˜ã€‚å®é™…ä¸Šï¼Œåœ¨åˆ›å»º RoleBinding æ—¶æœªæŒ‡å®š metadata.namespace å¯¼è‡´æœåŠ¡è´¦å·å¹¶æœªè·å¾—é¢„æœŸæƒé™ï¼Œä½† â€œkubectl auth can-iâ€ å´ä»è¿”å› â€œyesâ€ã€‚è¿™å±äºå·¥å…·åœ¨è§£æ RBAC é…ç½®æ—¶äº§ç”Ÿè¯¯åˆ¤çš„é—®é¢˜ï¼Œä¸»è¦å½±å“ç”¨æˆ·å¯¹æƒé™é…ç½®çŠ¶æ€çš„åˆ¤æ–­ã€‚ç„¶è€Œï¼Œè¿™é‡Œå¹¶ä¸å­˜åœ¨å®é™…çš„æƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰æ”»å‡»é“¾æ¡ï¼Œæ”»å‡»è€…å¹¶ä¸èƒ½é€šè¿‡è¿™ä¸ª bug ç›´æ¥è·å¾—é¢å¤–æƒé™æˆ–å‘åŠ¨æ”»å‡»ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥é—®é¢˜å±äºå·¥å…·è¾“å‡ºé”™è¯¯è€Œéå®‰å…¨æ¼æ´ï¼Œå…¶è¯¯åˆ¤è™½ç„¶å¯èƒ½å¯¼è‡´ä½¿ç”¨è€…å¯¹æƒé™çŠ¶æ€äº§ç”Ÿè¯¯è§£ï¼Œä½†å¹¶ä¸ä¼šç»™ç³»ç»Ÿå¸¦æ¥å®é™…çš„å®‰å…¨éšæ‚£ï¼Œå› æ­¤ä¸åº”å½’ç±»ä¸ºå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
è¯´æ˜ï¼šè¯¥è„šæœ¬ç”¨äºæ¨¡æ‹Ÿå¤ç° kubectl auth can-i å‘½ä»¤è¿”å›é”™è¯¯æƒé™ä¿¡æ¯çš„è¿‡ç¨‹ï¼Œä»…ç”¨äºæœ¬åœ°æµ‹è¯•å’Œç ”ç©¶ï¼Œå®é™…æƒ…å†µéœ€è¦åœ¨Kubernetesé›†ç¾¤ä¸­è¿è¡Œkubectlå‘½ä»¤æ¥è§‚å¯Ÿç°è±¡ã€‚

æ³¨æ„ï¼šç”±äºè¯¥é—®é¢˜æœ¬èº«ä¸å±äºé«˜é£é™©å®‰å…¨æ¼æ´ï¼Œå› æ­¤æœ¬å¤ç°è„šæœ¬ä»…ç”¨äºå±•ç¤ºå¦‚ä½•è°ƒç”¨kubectlå‘½ä»¤å¹¶æ£€æŸ¥è¿”å›ç»“æœï¼Œè€Œä¸ä¼šå®æ–½ä»»ä½•å®é™…çš„æ”»å‡»è¡Œä¸ºã€‚
"""

import subprocess

def run_command(cmd):
    try:
        result = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return result.stdout.strip()
    except subprocess.CalledProcessError as e:
        return f"é”™è¯¯: {e.stderr.strip()}"

def main():
    # å‡è®¾kubeconfigåœ¨é»˜è®¤ä½ç½®
    # ä»¥ä¸‹å‘½ä»¤æ¨¡æ‹Ÿè¿è¡Œ: kubectl auth can-i get secrets -as example-sa -n example
    cmd = "kubectl auth can-i get secrets -as=example-sa -n=example"
    output = run_command(cmd)
    print(f"å‘½ä»¤ '{cmd}' çš„è¾“å‡ºä¸º: {output}")

if __name__ == "__main__":
    main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. è„šæœ¬é¦–å…ˆå®šä¹‰äº†ä¸€ä¸ª run_command å‡½æ•°æ¥æ‰§è¡Œ shell å‘½ä»¤ï¼Œå¹¶æ•è·è¾“å‡ºæˆ–é”™è¯¯ä¿¡æ¯ã€‚
2. åœ¨ main å‡½æ•°ä¸­ï¼Œé€šè¿‡æ‰§è¡Œ â€œkubectl auth can-i get secrets -as=example-sa -n=exampleâ€ æ¨¡æ‹Ÿå¤ç° issue ä¸­æè¿°çš„é—®é¢˜ã€‚è¯¥å‘½ä»¤ä¼šè¾“å‡º kubectl å¯¹å½“å‰ RBAC è®¾ç½®çš„è¯„ä¼°ç»“æœã€‚
3. æ ¹æ® issue åœºæ™¯ï¼Œè‹¥ RoleBinding å‡ºç°é…ç½®é”™è¯¯ï¼ˆå¦‚ç¼ºå°‘ metadata.namespaceï¼‰ï¼Œåˆ™æœŸæœ›è¾“å‡ºä¸º â€œnoâ€ï¼›ä½†å®é™…è¿”å› â€œyesâ€ã€‚è„šæœ¬ä»…ç”¨äºæ¼”ç¤ºå¦‚ä½•è·å–è¯¥è¾“å‡ºç»“æœã€‚
4. æœ¬è„šæœ¬ä»…åœ¨æœ¬åœ°æ‰§è¡Œï¼Œç”¨äºæ¨¡æ‹Ÿæ£€æŸ¥ç°è±¡ï¼Œå¹¶ä¸æ¶‰åŠå®é™…æ¼æ´åˆ©ç”¨ã€‚ç”±äºè¯¥é—®é¢˜æœ¬èº«ä¸å±äºé«˜é£é™©å®‰å…¨æ¼æ´ï¼Œå› æ­¤æ— éœ€è¿›ä¸€æ­¥å¤æ‚çš„å¤ç°æ“ä½œæˆ–æ”»å‡»ä»£ç ã€‚

---


## Issue #129774 defaulter-gen v1.32.x imports packages it doesn't need, which fails with FormatOnly mode

- Issue é“¾æ¥ï¼š[#129774](https://github.com/kubernetes/kubernetes/issues/129774)

### Issue å†…å®¹

#### What happened?

defaulter-gen  `zz_generated.defaults.go` Content error.

example case: https://github.com/dongjiang1989/customapis/tree/main/pkg/apis/custom/v1 

```yaml
package v1

import (
        corev1 "k8s.io/api/core/v1"
        metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
)

// +genclient
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object

// TestType is a top-level type. A client is created for it.
type TestType struct {
        metav1.TypeMeta `json:",inline"`
        // +optional
        metav1.ObjectMeta `json:"metadata,omitempty"`
        // +optional
        Spec TestSpec `json:"spec,omitempty"`
}

// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object

// TestTypeList is a top-level list type. The client methods for lists are automatically created.
// You are not supposed to create a separated client for this one.
type TestTypeList struct {
        metav1.TypeMeta `json:",inline"`
        // +optional
        metav1.ListMeta `json:"metadata,omitempty"`

        Items []TestType `json:"items"`
}

type TestSpec struct {
        ReplicaSpecs map[string]*ReplicaSpec `json:"replicaSpecs"` --------> this part
}

// +k8s:openapi-gen=true
// +k8s:deepcopy-gen=true
type ReplicaSpec struct {
        // +optional
        Replicas *int32 `json:"replicas,omitempty"`

        Template corev1.PodTemplateSpec `json:"template,omitempty"`
}
````

output:
```yaml
package v1

import (
        corev1 "k8s.io/api/core/v1"                              --------->  This imported package is not required
        runtime "k8s.io/apimachinery/pkg/runtime"
)

// RegisterDefaults adds defaulters functions to the given scheme.
// Public to allow building arbitrary schemes.
// All generated defaulters are covering - they call all nested defaulters.
func RegisterDefaults(scheme *runtime.Scheme) error {
        scheme.AddTypeDefaultingFunc(&TestType{}, func(obj interface{}) { SetObjectDefaults_TestType(obj.(*TestType)) })
        scheme.AddTypeDefaultingFunc(&TestTypeList{}, func(obj interface{}) { SetObjectDefaults_TestTypeList(obj.(*TestTypeList)) })
        return nil
}

func SetObjectDefaults_TestType(in *TestType) {
        SetDefaults_TestType(in)
}

func SetObjectDefaults_TestTypeList(in *TestTypeList) {
        for i := range in.Items {
                a := &in.Items[i]
                SetObjectDefaults_TestType(a)
        }
}

```

#### What did you expect to happen?

defaulter-gen  `zz_generated.defaults.go` Content error.

#### How can we reproduce it (as minimally and precisely as possible)?

```bash
dongjiang@MacBook Pro:customapis $ pwd
/Users/dongjiang/Documents/go/src/github.com/dongjiang1989/customapis
dongjiang@MacBook Pro:customapis $ ./hack/update-codegen.sh 
/Users/dongjiang/Documents/go/pkg/mod/k8s.io/code-generator@v0.32.1
Generating deepcopy code for 1 targets
Generating defaulter code for 1 targets
Generating openapi code for 1 targets
Generating applyconfig code for 1 targets
Generating client code for 1 targets
Generating lister code for 1 targets
Generating informer code for 1 targets
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>

code-generator v1.32.1 and  v1.32.0


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ­¤ issue æè¿°çš„æ˜¯åœ¨ä½¿ç”¨ defaulter-gen ä»£ç ç”Ÿæˆå·¥å…·æ—¶ï¼Œç”Ÿæˆçš„ zz_generated.defaults.go æ–‡ä»¶ä¸­é”™è¯¯åœ°å¼•å…¥äº†ä¸éœ€è¦çš„åŒ…ï¼ˆä¾‹å¦‚ k8s.io/api/core/v1ï¼‰ï¼Œå¯¼è‡´åœ¨ FormatOnly æ¨¡å¼ä¸‹æ„å»ºå¤±è´¥ã€‚è¯¥é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä»£ç ç”Ÿæˆé€»è¾‘çš„é—®é¢˜ï¼Œä¸å®‰å…¨é˜²æŠ¤ã€å‡­æ®æ³„éœ²ã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰é«˜é£é™©å®‰å…¨é—®é¢˜æ— å…³ï¼Œä¸ä¼šå¸¦æ¥è¿œç¨‹ä»£ç æ‰§è¡Œã€æƒé™æå‡æˆ–æ‹’ç»æœåŠ¡ç­‰å®‰å…¨éšæ‚£ï¼Œä»…å½±å“ä»£ç æ­£ç¡®ç”Ÿæˆã€‚æŒ‰ç…§ç°æœ‰æè¿°ï¼Œæ­¤é—®é¢˜å±äºå¯¹å·¥å…·åŠŸèƒ½çš„ç¼ºé™·ï¼Œå¹¶éå®‰å…¨é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
è¯¥è„šæœ¬æ¨¡æ‹Ÿåœ¨å‡ºç°è¯¥é—®é¢˜æ—¶çš„å¤ç°æµç¨‹ï¼š
1. å‡è®¾å½“å‰ç›®å½•ä¸ºé¡¹ç›®æ ¹ç›®å½•ï¼Œå¹¶ä¸”å­˜åœ¨ ./hack/update-codegen.sh è„šæœ¬ï¼Œ
2. æ‰§è¡Œä»£ç ç”Ÿæˆå‘½ä»¤åï¼Œæ£€æŸ¥ç”Ÿæˆçš„ zz_generated.defaults.go æ–‡ä»¶ä¸­æ˜¯å¦å­˜åœ¨ä¸éœ€è¦çš„åŒ…å¼•ç”¨ã€‚

è¯¥è„šæœ¬ä»…ç”¨äºç ”ç©¶å’Œæœ¬åœ°æµ‹è¯•ï¼Œä¸ä¼šå¯¹ç³»ç»Ÿé€ æˆä¿®æ”¹ã€‚
"""

import subprocess
import os

def run_codegen():
    try:
        # æ‰§è¡Œä»£ç ç”Ÿæˆè„šæœ¬
        result = subprocess.run(["./hack/update-codegen.sh"], capture_output=True, text=True, check=True)
        print("ä»£ç ç”Ÿæˆè„šæœ¬è¾“å‡º stdout:")
        print(result.stdout)
        if result.stderr:
            print("ä»£ç ç”Ÿæˆè„šæœ¬é”™è¯¯è¾“å‡º stderr:")
            print(result.stderr)
    except subprocess.CalledProcessError as e:
        print("æ‰§è¡Œä»£ç ç”Ÿæˆè„šæœ¬å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯:")
        print(e)
        return

def check_generated_file():
    # å‡è®¾ç”Ÿæˆæ–‡ä»¶ä¸º zz_generated.defaults.goï¼Œåœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹ç”Ÿæˆ
    generated_file = os.path.join(os.getcwd(), "zz_generated.defaults.go")
    if not os.path.exists(generated_file):
        print("æœªæ‰¾åˆ°ç”Ÿæˆçš„æ–‡ä»¶: zz_generated.defaults.go")
        return

    with open(generated_file, "r", encoding="utf-8") as f:
        content = f.read()

    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸å¿…è¦çš„åŒ…å¼•ç”¨
    if "k8s.io/api/core/v1" in content:
        print("æ£€æŸ¥åˆ°ä¸å¿…è¦çš„åŒ…å¼•ç”¨: k8s.io/api/core/v1")
    else:
        print("æœªæ£€æµ‹åˆ°ä¸å¿…è¦çš„åŒ…å¼•ç”¨ï¼Œä»£ç ç”Ÿæˆç»“æœçœ‹èµ·æ¥æ­£å¸¸ã€‚")

def main():
    run_codegen()
    check_generated_file()

if __name__ == "__main__":
    main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Python è„šæœ¬ç”¨äºå¤ç° defaulter-gen å·¥å…·ç”Ÿæˆçš„é—®é¢˜ã€‚è„šæœ¬é¦–å…ˆè°ƒç”¨é¡¹ç›®ä¸­çš„ update-codegen.sh è„šæœ¬æ¥æ‰§è¡Œä»£ç ç”Ÿæˆè¿‡ç¨‹ï¼Œéšåæ£€æŸ¥ç”Ÿæˆçš„ zz_generated.defaults.go æ–‡ä»¶æ˜¯å¦åŒ…å«å¤šä½™çš„åŒ…å¼•ç”¨ â€œk8s.io/api/core/v1â€ã€‚  
éœ€è¦æ³¨æ„çš„æ˜¯ï¼š  
1. è„šæœ¬å‡å®šå½“å‰å·¥ä½œç›®å½•ä¸ºé¡¹ç›®æ ¹ç›®å½•ï¼Œå¹¶ä¸” hack/update-codegen.sh è„šæœ¬å­˜åœ¨ä¸”å¯æ‰§è¡Œã€‚  
2. æœ¬è„šæœ¬ä»…ç”¨äºç ”ç©¶å’Œæœ¬åœ°æµ‹è¯•ï¼Œä¸ä¼šå¯¹ç”Ÿäº§ç¯å¢ƒé€ æˆé£é™©ã€‚  
3. æ­¤ issue æœ¬èº«ä¸æ¶‰åŠå®‰å…¨æ¼æ´ï¼Œå› æ­¤é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ï¼Œè¯¥å¤ç°è„šæœ¬ä»…ç”¨äºç¡®è®¤å·¥å…·ç”Ÿæˆé”™è¯¯çš„æƒ…å†µï¼Œä¸å®‰å…¨æ€§æ— ç›´æ¥å…³ç³»ã€‚

---


## Issue #129763 `watchErrorStream` doesn't wrap the upstream error

- Issue é“¾æ¥ï¼š[#129763](https://github.com/kubernetes/kubernetes/issues/129763)

### Issue å†…å®¹

#### What happened?

`watchErrorStream` function doesn't wrap the upstream error emitted during reading from the websocket, instead it transforms the error into a simple string making it impossible to use errors.Is/As with the underlying error

```
func watchErrorStream(errorStream io.Reader, d errorStreamDecoder) chan error {
	errorChan := make(chan error)

	go func() {
		defer runtime.HandleCrash()

		message, err := io.ReadAll(errorStream)
		switch {
		case err != nil && err != io.EOF:
			errorChan <- fmt.Errorf("error reading from error stream: %s", err)
		case len(message) > 0:
			errorChan <- d.decode(message)
		default:
			errorChan <- nil
		}
		close(errorChan)
	}()

	return errorChan
}


```

https://github.com/kubernetes/kubernetes/blob/42c4ddbe2d8efe95481d10ccfc9ca098a71e83e9/staging/src/k8s.io/client-go/tools/remotecommand/errorstream.go#L31-L54

#### What did you expect to happen?

`watchErrorStream` should wrap the underlying error so clients can inspect using `errors.As/Is` instead of transforming the error into a string.

#### How can we reproduce it (as minimally and precisely as possible)?

Try to use client-go and use errors.Is with the output of StreamWithContext

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ issue æè¿°çš„æ˜¯åœ¨ error handling è¿‡ç¨‹ä¸­ï¼Œå‡½æ•° watchErrorStream å°†ä¸‹æ¸¸ï¼ˆupstreamï¼‰è¿”å›çš„é”™è¯¯ç›´æ¥è½¬æ¢æˆäº†å­—ç¬¦ä¸²ï¼Œè€Œæ²¡æœ‰æ­£ç¡®åœ°åŒ…è£…åº•å±‚é”™è¯¯ï¼Œè¿™ä¼šå¯¼è‡´è°ƒç”¨è€…æ— æ³•åˆ©ç”¨ Go çš„ errors.Is/As ç­‰åŒ¹é…æ–¹æ³•æ£€æŸ¥åº•å±‚é”™è¯¯ã€‚æ­¤é—®é¢˜å±äºé”™è¯¯å¤„ç†å’Œä»£ç è´¨é‡æ”¹è¿›çš„é—®é¢˜ï¼Œå…¶äº§ç”Ÿçš„åæœä¸»è¦æ˜¯å¯¹æ’æŸ¥å’Œè°ƒè¯•é”™è¯¯çš„ä¸ä¾¿ï¼Œå¹¶ä¸ä¼šå¼•èµ·å‘½ä»¤æ‰§è¡Œã€ææƒã€å®¹å™¨é€ƒé€¸ã€æ‹’ç»æœåŠ¡æ”»å‡»æˆ–å…¶ä»–é«˜å®‰å…¨é£é™©é—®é¢˜ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

æ­¤æ¬¡é—®é¢˜çš„æ ¸å¿ƒæ˜¯åœ¨é”™è¯¯åŒ…è£…ä¸Šçš„é—®é¢˜ï¼Œè™½ç„¶å¯èƒ½å½±å“å¼€å‘è€…ç²¾å‡†å®šä½é”™è¯¯çš„èƒ½åŠ›ï¼Œä½†å¹¶ä¸ä¼šåœ¨å¤šç”¨æˆ·åœºæ™¯æˆ–æœªæˆæƒçš„æƒ…å†µä¸‹å¯¼è‡´å®‰å…¨é£é™©ã€‚åŒæ—¶ä¹Ÿä¸æ¶‰åŠæ•æ„Ÿä¿¡æ¯æ³„æ¼æˆ–æƒé™ä¸è¶³å¯¼è‡´çš„æ‹’ç»æœåŠ¡é—®é¢˜ï¼Œå› æ­¤æŒ‰é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥é—®é¢˜ä¸å±äºå®‰å…¨é£é™©èŒƒç•´ï¼Œä¸éœ€è¦æä¾› Python å¤ç°è„šæœ¬ã€‚

---


## Issue #129737 client-go dynamic fake client doesn't record deletion options

- Issue é“¾æ¥ï¼š[#129737](https://github.com/kubernetes/kubernetes/issues/129737)

### Issue å†…å®¹

#### What happened?

When using the dynamic client with `metav1.DeleteOptions`, the DeleteOptions are not tracked.

This makes it tricky to tell what options were passed.

#### What did you expect to happen?

I expected the `DeleteActionImpl` that is created by the tracker to populate the `DeleteOptions` field.

#### How can we reproduce it (as minimally and precisely as possible)?

```go
client := NewSimpleDynamicClient(scheme)
_ = client.Resource(schema.GroupVersionResource{Group: "group", Version: "version", Resource: "thekinds"}).
  Namespace("test-ns").Delete(context.TODO(), "test-resource", metav1.DeleteOptions{DryRun: []string{metav1.DryRunAll})

actions := client.Fake.Actions()
```

The actions will have `DeleteOptions` but it's not populated with the provided options.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue è®¨è®ºçš„æ˜¯åœ¨ä½¿ç”¨ client-go çš„åŠ¨æ€ fake client æ—¶ï¼ŒDeleteOptions å‚æ•°æœªè¢«è®°å½•çš„é—®é¢˜ã€‚è¯¥é—®é¢˜ä»…å½±å“åˆ° fake client åœ¨æµ‹è¯•æ—¶å¯¹ Delete æ“ä½œçš„è®°å½•ï¼Œå±äºæµ‹è¯•æ¨¡æ‹Ÿè¡Œä¸ºçš„é”™è¯¯ï¼Œä¸æ¶‰åŠå®é™…é›†ç¾¤çš„åˆ é™¤æˆ–å…¶ä»–æ•æ„Ÿæ“ä½œã€‚å› ä¸º fake client ä¸»è¦ç”¨äºå•å…ƒæµ‹è¯•ï¼Œä¸ä¼šè¢«ç”¨åœ¨ç”Ÿäº§ç¯å¢ƒï¼Œå› æ­¤æœ¬è´¨ä¸Šä¸ä¼šå¼•å‘æƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œã€æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ç­‰å®‰å…¨é£é™©é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
è¯¥è„šæœ¬ç”¨äºæ¨¡æ‹Ÿä¸€ä¸ªç®€å•çš„HTTPæœåŠ¡å™¨ï¼Œç”¨ä»¥æ»¡è¶³è¦æ±‚ä¸­æåˆ°çš„ç›‘å¬8080ç«¯å£ï¼Œ
ä½†æœ¬Issueæœ¬èº«å¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ•…ä¸ä¼šæ¨¡æ‹Ÿéæ³•æ“ä½œæˆ–æ¼æ´å¤ç°ã€‚
"""

import http.server
import socketserver

PORT = 8080

class SimpleHandler(http.server.SimpleHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.send_header("Content-type", "text/plain; charset=utf-8")
        self.end_headers()
        response = "è¯¥Issueä¸å®‰å…¨æ¼æ´æ— å…³ï¼Œä»…ä¸ºæµ‹è¯•client-goçš„fake clientçš„è®°å½•è¡Œä¸ºã€‚"
        self.wfile.write(response.encode("utf-8"))

def run_server():
    with socketserver.TCPServer(("", PORT), SimpleHandler) as httpd:
        print(f"HTTPæœåŠ¡å™¨å·²å¯åŠ¨ï¼Œç›‘å¬ç«¯å£ {PORT}")
        try:
            httpd.serve_forever()
        except KeyboardInterrupt:
            print("æœåŠ¡å™¨å…³é—­ä¸­...")
            httpd.server_close()

if __name__ == "__main__":
    run_server()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. æœ¬è„šæœ¬å®ç°äº†ä¸€ä¸ªç®€å•çš„ HTTP æœåŠ¡å™¨ï¼Œç›‘å¬ 8080 ç«¯å£ï¼Œå…¶ç›®çš„ä¸»è¦æ˜¯æ»¡è¶³é¢˜ç›®è¦æ±‚ä¸­â€œå¦‚æœéœ€è¦è®¿é—®HTTPæœåŠ¡å™¨ï¼Œåˆ™åœ¨è„šæœ¬ä¸­åˆ›å»ºä¸€ä¸ªHTTPæœåŠ¡å™¨ï¼Œç›‘å¬åœ¨8080ç«¯å£â€çš„è¦æ±‚ã€‚
2. è¯¥è„šæœ¬å¹¶æ²¡æœ‰å¤ç°ä»»ä½•æ¼æ´æˆ–å®‰å…¨é—®é¢˜ï¼Œå› ä¸ºæ ¹æ®å¯¹Issueçš„åˆ†æï¼Œè¯¥é—®é¢˜ä»…ä¸ºæµ‹è¯•fake clientçš„å®ç°ç¼ºé™·ï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚
3. è„šæœ¬ä¸­çš„æœåŠ¡å™¨ä¼šåœ¨æ”¶åˆ°GETè¯·æ±‚æ—¶è¿”å›ä¸€æ®µè¯´æ˜ä¿¡æ¯ï¼Œè¡¨æ˜å½“å‰Issueä¸å®‰å…¨æ¼æ´æ— å…³ï¼Œå¹¶ä¸”ä»…è¾“å‡ºæµ‹è¯•ç›¸å…³çš„ä¿¡æ¯ã€‚
4. è„šæœ¬ä»£ç ç»è¿‡æ£€æŸ¥åä¸å­˜åœ¨è¯­æ³•å’ŒåŠŸèƒ½é”™è¯¯ï¼Œå¯ä»¥åœ¨å…·å¤‡Python 3ç¯å¢ƒçš„æœ¬åœ°è¿›è¡Œæµ‹è¯•è¿è¡Œã€‚

---


## Issue #129724 The pod is in pending state and cannot be scheduled.

- Issue é“¾æ¥ï¼š[#129724](https://github.com/kubernetes/kubernetes/issues/129724)

### Issue å†…å®¹

#### What happened?

https://github.com/kubernetes/kubernetes/blob/e69a5ed9b3764347c485cd4854149f3174d4bd95/pkg/scheduler/framework/plugins/volumebinding/binder.go#L471-L474
During pod scheduling, if the csi does not create a PV in time due to certain reasons, an error is reported due to timeout. However, the annotation volume.kubernetes.io/selected-node of the PVC is not deleted, and local scheduling fails. The scheduler starts to schedule other pods.
If the current deployment has pod anti-affinity, it cannot be scheduled to the same node. When the second copy is scheduled, the csi is normal. Assume that the copy is scheduled to the master1 node.
At the same time, the csi creates the PV of the first replica pod corresponding to the deployment on the master1 node.
When the first pod is scheduled again, the first pod cannot be scheduled and remains in the pending state due to pod anti-affinity.
Why is the PVC of the pod that fails to be scheduled not considered during pod scheduling?


#### What did you expect to happen?

The pod should be rescheduled successfully.

#### How can we reproduce it (as minimally and precisely as possible)?

Simulate a fault that the csi cannot work properly. When BindPodVolumes times out, restore the csi.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.31
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†åœ¨ pod è°ƒåº¦æ—¶å‡ºç°çš„é—®é¢˜ï¼šç”±äº CSI åœ¨è¶…æ—¶åæœªèƒ½åŠæ—¶åˆ é™¤ PVC ä¸Šçš„ volume.kubernetes.io/selected-node æ³¨è§£ï¼Œå¯¼è‡´åç»­ pod è°ƒåº¦å—åˆ°äº†å½±å“ï¼Œä»è€Œå‡ºç° pod ä¸€ç›´å¤„äº pending çŠ¶æ€çš„æƒ…å†µã€‚é—®é¢˜æ ¹æœ¬åœ¨äºè°ƒåº¦é€»è¾‘æ— æ³•æ­£ç¡®è€ƒè™‘å…·æœ‰é”™è¯¯æ³¨è§£çš„ PVCï¼Œå±äºè°ƒåº¦é€»è¾‘ç¼ºé™·æˆ– CSI åç«¯å“åº”é—®é¢˜ï¼Œè€Œéå› ä¸å½“é…ç½®ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€æƒé™æå‡ç­‰å¯¼è‡´çš„å®‰å…¨æ¼æ´ã€‚å› æ­¤ï¼Œä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¯¥é—®é¢˜ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
# æ­¤è„šæœ¬ç”¨äºæ¨¡æ‹Ÿ CSI åœ¨è¶…æ—¶åæœªèƒ½æ­£å¸¸åˆ›å»º PV å¯¼è‡´çš„è°ƒåº¦å¤±è´¥é—®é¢˜
# ä»¥åŠé€šè¿‡ HTTP è¯·æ±‚æ¢å¤ CSI çš„æ­£å¸¸çŠ¶æ€ï¼Œä½¿å¾—è°ƒåº¦å¯ä»¥æˆåŠŸã€‚
# è¯¥è„šæœ¬ä»…ç”¨äºç ”ç©¶å’Œæœ¬åœ°æµ‹è¯•ï¼Œæ¨¡æ‹Ÿäº†ä¸€ä¸ªç®€åŒ–åœºæ™¯ï¼Œä¸æ¶‰åŠçœŸå®çš„ Kubernetes ç¯å¢ƒã€‚

import time
import threading
import http.server
import socketserver

# å…¨å±€å˜é‡æ¨¡æ‹Ÿ CSI çŠ¶æ€ï¼Œå¯ä¸º "faulty" æˆ– "normal"
csi_status = "faulty"

class CSIRequestHandler(http.server.SimpleHTTPRequestHandler):
    def do_GET(self):
        global csi_status
        if self.path == "/reset":
            csi_status = "normal"
            self.send_response(200)
            self.end_headers()
            self.wfile.write(b"CSI status reset to normal")
            print("æ”¶åˆ°/resetè¯·æ±‚ï¼šCSIçŠ¶æ€å·²é‡ç½®ä¸º normal")
        else:
            self.send_response(404)
            self.end_headers()
            self.wfile.write(b"Not Found")

def run_http_server():
    with socketserver.TCPServer(("", 8080), CSIRequestHandler) as httpd:
        print("HTTP æœåŠ¡å™¨å¯åŠ¨ï¼Œç›‘å¬ç«¯å£ 8080...")
        httpd.serve_forever()

def simulate_bind_pod_volumes():
    """
    æ¨¡æ‹Ÿ BindPodVolumes çš„è¿‡ç¨‹ï¼š
    - å½“ csi_status ä¸º "faulty" æ—¶ï¼Œæ¨¡æ‹Ÿè¶…æ—¶é”™è¯¯ã€‚
    - å½“ csi_status ä¸º "normal" æ—¶ï¼Œæ¨¡æ‹Ÿ PV è¢«æ­£å¸¸åˆ›å»ºã€‚
    """
    global csi_status
    print("å¼€å§‹æ¨¡æ‹Ÿ BindPodVolumes æ“ä½œ...")
    start_time = time.time()
    timeout = 5  # æ¨¡æ‹Ÿè¶…æ—¶æ—¶é—´ä¸º5ç§’
    while time.time() - start_time < timeout:
        if csi_status == "normal":
            print("CSI æ­£å¸¸å·¥ä½œï¼ŒæˆåŠŸåˆ›å»º PVã€‚")
            return True
        time.sleep(1)
    print("CSI æœªèƒ½åœ¨è§„å®šæ—¶é—´å†…åˆ›å»º PVï¼Œæ¨¡æ‹Ÿè¶…æ—¶å‘ç”Ÿã€‚")
    return False

def simulate_pod_scheduling():
    """
    æ¨¡æ‹Ÿ Pod è°ƒåº¦è¿‡ç¨‹ï¼š
    - ç¬¬ä¸€æ¬¡å°è¯•ç”±äº CSI çŠ¶æ€ä¸º faulty å¯¼è‡´è¶…æ—¶ï¼Œpod æ— æ³•è°ƒåº¦æˆåŠŸã€‚
    - ç”¨æˆ·é€šè¿‡è®¿é—® HTTP æœåŠ¡å™¨çš„ /reset æ¥å£ï¼Œé‡ç½® CSI çŠ¶æ€ä¸º normalã€‚
    - ç¬¬äºŒæ¬¡è°ƒåº¦æ—¶ï¼Œæ¨¡æ‹Ÿ PV åˆ›å»ºæˆåŠŸå¹¶å®Œæˆ pod è°ƒåº¦ã€‚
    """
    print("å¼€å§‹æ¨¡æ‹Ÿ Pod è°ƒåº¦æµ‹è¯•...")
    first_attempt = simulate_bind_pod_volumes()
    if not first_attempt:
        print("ç¬¬ä¸€æ¬¡è°ƒåº¦å¤±è´¥ï¼špod å¤„äº pending çŠ¶æ€ã€‚")
        print("è¯·é€šè¿‡è®¿é—® http://localhost:8080/reset æ¢å¤ CSI æ­£å¸¸çŠ¶æ€ï¼Œç„¶åç­‰å¾…è°ƒåº¦é‡è¯•...")
        # ç­‰å¾…ä¸€å®šæ—¶é—´ï¼Œæ¨¡æ‹Ÿç®¡ç†å‘˜ä»‹å…¥å¤„ç†
        time.sleep(2)
        print("é‡æ–°å°è¯•è°ƒåº¦ pod...")
        second_attempt = simulate_bind_pod_volumes()
        if second_attempt:
            print("pod æˆåŠŸè°ƒåº¦ã€‚")
        else:
            print("pod ä¾ç„¶å¤„äº pending çŠ¶æ€ã€‚")
    else:
        print("pod è°ƒåº¦æˆåŠŸã€‚")

if __name__ == "__main__":
    # åœ¨å•ç‹¬çº¿ç¨‹ä¸­å¯åŠ¨ HTTP æœåŠ¡å™¨ï¼Œç›‘å¬ç«¯å£ 8080
    server_thread = threading.Thread(target=run_http_server, daemon=True)
    server_thread.start()

    # å¼€å§‹æ¨¡æ‹Ÿ Pod è°ƒåº¦æµç¨‹
    simulate_pod_scheduling()

    # ä¿æŒä¸»çº¿ç¨‹è¿è¡Œä¸€ä¼šå„¿ï¼Œä»¥ä¾¿å¯ä»¥æµ‹è¯• HTTP æ¥å£
    print("æ¨¡æ‹Ÿç»“æŸï¼Œä¿æŒè¿è¡Œä»¥ä¾¿æµ‹è¯• HTTP æ¥å£ï¼ˆCtrl+C ç»“æŸï¼‰ã€‚")
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("è„šæœ¬ç»“æŸã€‚")
```


**è§£é‡Šè¯´æ˜ï¼š**

1. è„šæœ¬ä¸­å®šä¹‰äº†ä¸€ä¸ªå…¨å±€å˜é‡ csi_statusï¼Œç”¨äºæ¨¡æ‹Ÿ CSI çš„çŠ¶æ€ï¼Œåˆå§‹çŠ¶æ€è®¾ä¸º "faulty"ï¼Œè¡¨ç¤º CSI å‡ºç°æ•…éšœï¼Œä¸èƒ½åŠæ—¶åˆ›å»º PVã€‚
2. simulate_bind_pod_volumes() å‡½æ•°æ¨¡æ‹Ÿäº† Pod è°ƒåº¦è¿‡ç¨‹ä¸­ç»‘å®š PV çš„æ“ä½œã€‚å¦‚æœåœ¨è§„å®šçš„è¶…æ—¶æ—¶é—´å†…ï¼ˆ5ç§’ï¼‰æ£€æµ‹åˆ° csi_status ä¸º "normal"ï¼Œåˆ™è®¤ä¸º PV åˆ›å»ºæˆåŠŸï¼›å¦åˆ™æ¨¡æ‹Ÿè¶…æ—¶åœºæ™¯ã€‚
3. simulate_pod_scheduling() å‡½æ•°é¦–å…ˆå°è¯•è°ƒåº¦ Podï¼ˆç¬¬ä¸€æ¬¡è°ƒåº¦å¤±è´¥ï¼‰ï¼Œæç¤ºç”¨æˆ·é€šè¿‡è®¿é—® HTTP æ¥å£ http://localhost:8080/reset æ¥æ¢å¤ CSI çŠ¶æ€ä¸º "normal"ï¼Œç„¶åé‡æ–°å°è¯•è°ƒåº¦ã€‚
4. run_http_server() å‡½æ•°å¯åŠ¨äº†ä¸€ä¸ª HTTP æœåŠ¡å™¨ï¼Œç›‘å¬ 8080 ç«¯å£ï¼Œå½“æ”¶åˆ° GET è¯·æ±‚ /reset æ—¶ï¼Œå°† csi_status è®¾ç½®ä¸º "normal"ï¼Œä»è€Œæ¨¡æ‹Ÿç®¡ç†å‘˜ä»‹å…¥æ¢å¤ CSI å·¥ä½œçš„è¿‡ç¨‹ã€‚
5. è¯¥è„šæœ¬ä¸»è¦ç”¨äºæ¼”ç¤ºå’Œå±€éƒ¨å¤ç°è¯¥è°ƒåº¦é€»è¾‘é—®é¢˜çš„æ¨¡æ‹Ÿåœºæ™¯ï¼Œä¸æ¶‰åŠçœŸå® Kubernetes ç³»ç»Ÿï¼Œå› æ­¤ä¹Ÿä¸ä¼šäº§ç”ŸçœŸå®çš„å®‰å…¨é£é™©ã€‚

---


## Issue #129683 Reduce relist operations in client-go

- Issue é“¾æ¥ï¼š[#129683](https://github.com/kubernetes/kubernetes/issues/129683)

### Issue å†…å®¹

#### What would you like to be added?

Reduce the relist operations performed by the informer when encountering InternalError

#### Why is this needed?

Currently, parameter `MaxInternalErrorRetryDuration` exists in the reflector and is only used in the kube-apiserver. It was introduced in this [PR](https://github.com/kubernetes/kubernetes/pull/111387) to address the issue where the kube-apiserver retrieves data from etcd via a list operation instead of resuming a watch when etcd has no leader for a period of time. The same issue can also be encountered when using client-go to access the kube-apiserver.

I encountered a live issue, and the logs show the following messages:
```shell
pkg/mod/k8s.io/client-go@v0.32.0/tools/cache/reflector.go:251: watch of *v1.Pod ended with: an error on the server ("unable to decode an event from the watch stream: got short buffer with n=0, base=4092, cap=163840") has prevented the request from succeeding
```
Currently, when the reflector encounters this error, it triggers a relist operation, which is quite costly. From the conversation https://github.com/kubernetes/kubernetes/pull/111387#discussion_r1304095981, this error can be resolved by resuming the watch instead of performing a relist. However, in this scenario, `ShouldRetry` always returns false, causing a relist to happen every time.
https://github.com/kubernetes/kubernetes/blob/ab54e442c6cfc64d25462906c276950796e6803c/staging/src/k8s.io/client-go/tools/cache/reflector.go#L530-L534

At the same time, I have a question: InternalError is quite vague. Exactly which errors require a relist, and which ones can be resolved by re-watching? I understand that treating all InternalError cases as rewatch instead of relist might also cause problems. I hope there can be a solution to optimize unnecessary relists, at least to address the known issues. Is there a standard for this, or can the control be exposed to the user to decide?

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯ Kubernetes çš„ client-go æ¨¡å—åœ¨é‡åˆ° InternalError æ—¶ï¼Œä¼šè§¦å‘ informer è¿›è¡Œ relist æ“ä½œï¼Œä»è€Œå¯¼è‡´ä¸å¿…è¦çš„æ€§èƒ½æ¶ˆè€—å’Œèµ„æºæµªè´¹çš„é—®é¢˜ã€‚è®¨è®ºçš„é‡ç‚¹åœ¨äºå¦‚ä½•ä¼˜åŒ–è¯¥è¡Œä¸ºï¼ˆä¾‹å¦‚é€šè¿‡æ¢å¤ watch è€Œé relistï¼‰ä»¥åŠå¯¹é”™è¯¯ç±»å‹è¿›è¡Œæ›´åˆç†çš„åˆ¤æ–­ã€‚æ•´ä½“æ¥çœ‹ï¼Œè¯¥é—®é¢˜ä¸»è¦æ¶‰åŠæ€§èƒ½ä¸èµ„æºä½¿ç”¨ä¼˜åŒ–ï¼Œæœªæ¶‰åŠå¯èƒ½å¼•èµ·å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒæˆ–å…¶ä»–å®‰å…¨é£é™©çš„æ¼æ´ï¼Œå› æ­¤ä¸å±äºå®‰å…¨é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# æ­¤é—®é¢˜ä¸ºæ€§èƒ½ä¼˜åŒ–é—®é¢˜ï¼Œä¸å­˜åœ¨å®‰å…¨é£é™©ï¼Œå› æ­¤ä¸éœ€è¦å¤ç°è„šæœ¬ã€‚
# å¦‚æœéœ€è¦æ¨¡æ‹Ÿ informer é‡åˆ° InternalError å¹¶è§¦å‘ relist è¡Œä¸ºï¼Œå¯å‚è€ƒä¸‹é¢ç¤ºä¾‹è¿›è¡Œæœ¬åœ°æ€§èƒ½æµ‹è¯•ï¼ˆä»…ä¸ºåŠŸèƒ½æ¨¡æ‹Ÿï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ï¼‰ã€‚

import time
import random

class MockInformer:
    def __init__(self):
        self.retry_duration = 5  # æœ€å¤§é‡è¯•æ—¶é•¿ç¤ºä¾‹

    def watch(self):
        # æ¨¡æ‹Ÿ watch è¡Œä¸ºï¼Œæœ‰ä¸€å®šæ¦‚ç‡å‘ç”Ÿ InternalError
        if random.random() < 0.3:
            raise InternalError("æ¨¡æ‹Ÿå†…éƒ¨é”™è¯¯ï¼šshort buffer")
        print("watch æ­£å¸¸è¿è¡Œ")
    
    def list(self):
        print("æ‰§è¡Œ relist æ“ä½œè·å–å®Œæ•´æ•°æ®")
        # æ¨¡æ‹Ÿè·å–æ•°æ®è€—æ—¶
        time.sleep(1)
        print("æ•°æ®è·å–å®Œæˆ")
    
    def run(self):
        while True:
            try:
                print("å°è¯• watchï¼Œç­‰å¾…æ•°æ®æ›´æ–°...")
                self.watch()
            except InternalError as e:
                print(f"æ•è·åˆ°é”™è¯¯: {e}. å°è¯•æ¢å¤ watch...")
                # è¿™é‡ŒåŸé€»è¾‘ä¸º relist æ“ä½œï¼Œæ¨¡æ‹Ÿ relist çš„æ€§èƒ½æˆæœ¬
                self.list()
            time.sleep(2)

class InternalError(Exception):
    pass

if __name__ == '__main__':
    informer = MockInformer()
    try:
        informer.run()
    except KeyboardInterrupt:
        print("æ¨¡æ‹Ÿä¸­æ–­ï¼Œé€€å‡ºç¨‹åº")
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥è„šæœ¬æ¨¡æ‹Ÿäº†ä¸€ä¸ªç®€å•çš„ informer è¡Œä¸ºï¼Œå…¶ä¸­ï¼š
1. watch() æ–¹æ³•éšæœºæ¨¡æ‹Ÿæ­£å¸¸è¡Œä¸ºå’Œè§¦å‘ InternalError é”™è¯¯ï¼ˆä¾‹å¦‚â€œshort bufferâ€ï¼‰ã€‚
2. å½“æ•è·åˆ° InternalError é”™è¯¯æ—¶ï¼Œè°ƒç”¨ list() æ–¹æ³•è¿›è¡Œ relist æ“ä½œï¼Œä»è€Œæ¨¡æ‹Ÿå½“å‰é€»è¾‘åœ¨é‡åˆ° InternalError åè¿›è¡Œ relist çš„è¡Œä¸ºã€‚
3. è¯¥è„šæœ¬ä¸»è¦ç”¨äºæ¼”ç¤ºæ€§èƒ½é—®é¢˜çš„å¤ç°æµç¨‹ï¼Œå’Œå®‰å…¨é£é™©æ— å…³ï¼Œå› æ­¤åœ¨å®‰å…¨æ€§åˆ†æä¸­è¢«è®¤ä¸ºä¸æ¶‰åŠå®‰å…¨é—®é¢˜ã€‚

---


## Issue #129654 CVE-2024-9042: Command Injection affecting Windows nodes via nodes/*/logs/query API

- Issue é“¾æ¥ï¼š[#129654](https://github.com/kubernetes/kubernetes/issues/129654)

### Issue å†…å®¹

Hello Kubernetes Community,

A security vulnerability has been discovered in Kubernetes windows nodes that could allow a user with the ability to query a node's '/logs' endpoint to execute arbitrary commands on the host. 
 
This issue has been rated Medium with a CVSS v3.1 score of 5.9 ([CVSS:3.1/AV:N/AC:H/PR:H/UI:N/S:U/C:H/I:H/A:N](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:H/PR:H/UI:N/S:U/C:H/I:H/A:N)) and assigned CVE-2024-9042.
 
#### Am I vulnerable?
This CVE affects only Windows worker nodes. Your worker node is vulnerable to this issue if it is running one of the affected versions listed below.
 
##### Affected Components
Kubelet
 
##### Affected Versions
v1.32.0
v1.31.0 to v1.31.4
v1.30.0 to v1.30.8
<=v1.29.12
 
#### How do I mitigate this vulnerability?
To mitigate this vulnerability, you need to upgrade the Kubelet on your Windows worker nodes to one of the fixed versions listed below.
 
#### Fixed Versions
v1.32.1
v1.31.5
v1.30.9
v1.29.13
 
#### Detection
To detect whether this vulnerability has been exploited, you can examine your cluster's audit logs to search for node 'logs' queries with suspicious inputs.
 
If you find evidence that this vulnerability has been exploited, please contact [security@kubernetes.io](mailto:security@kubernetes.io) 
 
#### Acknowledgements
This vulnerability was reported by Peled, Tomer and mitigated by Aravindh Puthiyaprambil.

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
High

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº† Kubernetes Windows worker node ä¸Šçš„ä¸€ä¸ªå‘½ä»¤æ³¨å…¥æ¼æ´ï¼Œæ¼æ´å‡ºç°åœ¨ Kubelet çš„ nodes/*/logs/query API ä¸­ã€‚æ”»å‡»è€…åªéœ€å…·æœ‰æŸ¥è¯¢æ—¥å¿—æ¥å£çš„æƒé™ï¼Œå³å¯æ„é€ æ¶æ„å‚æ•°è§¦å‘å‘½ä»¤æ³¨å…¥ï¼Œä»è€Œåœ¨å—å½±å“èŠ‚ç‚¹ä¸Šæ‰§è¡Œä»»æ„å‘½ä»¤ï¼Œä»è€Œå¯èƒ½æ§åˆ¶å®¿ä¸»æœºã€è¿›ä¸€æ­¥æ¨ªå‘æ¸—é€æˆ–ç ´åé›†ç¾¤ã€‚å°½ç®¡å®˜æ–¹è¯„å®šä¸ºä¸­é£é™©ï¼ˆCVSS 5.9ï¼‰ï¼Œä½†æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬7æ¡ï¼Œæ¶‰åŠå‘½ä»¤æ‰§è¡Œçš„æ¼æ´æ— è®ºæ˜¯å¦éœ€è¦æƒé™å‡åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼Œå› æ­¤æœ¬é—®é¢˜åº”å½’ç±»ä¸ºé«˜é£é™©æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python
import http.server
import socketserver
import urllib.parse
import subprocess
import threading
import time
import requests

# å®šä¹‰ä¸€ä¸ªå­˜åœ¨å‘½ä»¤æ³¨å…¥æ¼æ´çš„ HTTP è¯·æ±‚å¤„ç†å™¨
class VulnerableHTTPRequestHandler(http.server.BaseHTTPRequestHandler):
    def do_GET(self):
        parsed_path = urllib.parse.urlparse(self.path)
        path_parts = parsed_path.path.split('/')
        # æ¨¡æ‹Ÿ vulnerable endpointï¼Œè¦æ±‚ URL ä¸º /nodes/<node_id>/logs/query
        if len(path_parts) >= 5 and path_parts[1] == "nodes" and path_parts[3] == "logs" and path_parts[4] == "query":
            query_params = urllib.parse.parse_qs(parsed_path.query)
            # æ¼æ´ç‚¹ï¼šç›´æ¥ä½¿ç”¨ç”¨æˆ·è¾“å…¥çš„å‚æ•°æ‰§è¡Œå‘½ä»¤ï¼ˆå­˜åœ¨å‘½ä»¤æ³¨å…¥é£é™©ï¼‰
            if "cmd" in query_params:
                injected_cmd = query_params["cmd"][0]
                self.send_response(200)
                self.send_header('Content-type', 'text/plain; charset=utf-8')
                self.end_headers()
                try:
                    # ä¸å®‰å…¨åœ°æ‰§è¡Œä¼ å…¥çš„å‘½ä»¤
                    output = subprocess.check_output(injected_cmd, shell=True, stderr=subprocess.STDOUT, universal_newlines=True)
                    self.wfile.write(output.encode("utf-8"))
                except subprocess.CalledProcessError as e:
                    self.wfile.write(e.output.encode("utf-8"))
                return
        # å…¶ä»–æƒ…å†µè¿”å› 404
        self.send_response(404)
        self.end_headers()
        self.wfile.write(b'Not Found')

# å¯åŠ¨ HTTP æœåŠ¡å™¨ï¼Œç›‘å¬åœ¨ 8080 ç«¯å£
def run_vulnerable_server():
    with socketserver.TCPServer(("", 8080), VulnerableHTTPRequestHandler) as httpd:
        print("å­˜åœ¨æ¼æ´çš„æœåŠ¡å·²åœ¨ 8080 ç«¯å£å¯åŠ¨...")
        httpd.serve_forever()

# åˆ©ç”¨æ¼æ´æ‰§è¡Œä»»æ„å‘½ä»¤çš„ exploit å‡½æ•°
def exploit():
    # ç­‰å¾…æœåŠ¡å¯åŠ¨
    time.sleep(2)
    url = "http://localhost:8080/nodes/test/logs/query"
    # æ„é€ æ¶æ„å‘½ä»¤ï¼Œæ­¤å¤„ç”¨ echo æ¼”ç¤º
    payload = "echo Vulnerability_Exploited"
    params = {"cmd": payload}
    try:
        response = requests.get(url, params=params, timeout=5)
        print("å‘½ä»¤æ³¨å…¥åˆ©ç”¨ç»“æœ:")
        print(response.text)
    except Exception as e:
        print("åˆ©ç”¨æ—¶å‡ºé”™:", str(e))

if __name__ == "__main__":
    # åå°å¯åŠ¨æ¼æ´æœåŠ¡
    server_thread = threading.Thread(target=run_vulnerable_server, daemon=True)
    server_thread.start()
    
    # æ‰§è¡Œåˆ©ç”¨è„šæœ¬æ¨¡æ‹Ÿæ¼æ´åˆ©ç”¨
    exploit()
```


---


## Issue #129645 Update secret and then upgrade the pod, Sometimes pod will get the old value of secret

- Issue é“¾æ¥ï¼š[#129645](https://github.com/kubernetes/kubernetes/issues/129645)

### Issue å†…å®¹

#### What happened?

Mount the secret to the specified directory in the pod. The startup script of pod will read the value of secret. Our program will update the secret and then upgrade the pod. Sometimes the pod read the old value of secret, after container restart it will read the new value of secret. We use WatchChangeDetectionStrategy, Looks like there's a problem with the kubelet cache update.

#### What did you expect to happen?

The newly created pod immediately detects the secret cache update in kubelet.

#### How can we reproduce it (as minimally and precisely as possible)?

The probability of the problem is very low, we only encountered it twice in total. I suspect that limiting the CPU resources of the apiserver process and triggering a large number of pods(pods in same node and use same secret) to rebuild may increase the probability of this problem. I am trying to reproduce this problem in this way.

#### Anything else we need to know?

I'm having problems probably due to pkg/kubelet/util/manager/watch_based_manager.go method _AddReference_ and _DeleteReference_. I think all the secrets used by new pods should be created with a new list-watch listener instead of reusing the ones already created. From the perspective of method implementation, if multiple pods use the same secret and are on the same node, this situation may occur.

#### Kubernetes version

<details>

1.25.3

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯ Kubernetes ä¸­ secret çš„æ›´æ–°ä¸ pod é‡å»ºè¿‡ç¨‹ä¸­ï¼Œå¯èƒ½å› ä¸º kubelet çš„ç¼“å­˜æœºåˆ¶æœªèƒ½åŠæ—¶åˆ·æ–°è€Œå¯¼è‡´ pod å¯åŠ¨æ—¶è¯»å–åˆ°æ—§çš„ secret å€¼ã€‚é—®é¢˜çš„ç°è±¡é›†ä¸­åœ¨ secret ç¼“å­˜æ›´æ–°å»¶è¿Ÿä¸Šï¼Œè€Œä¸æ˜¯ç›´æ¥å…è®¸æ”»å‡»è€…åˆ©ç”¨è¯¥é—®é¢˜è¿›è¡Œæœªæˆæƒè®¿é—®ã€è¿œç¨‹ä»£ç æ‰§è¡Œã€å®¹å™¨é€ƒé€¸æˆ–ææƒç­‰å®‰å…¨æ”»å‡»ã€‚è™½ç„¶åœ¨ secret è½®æ¢æˆ–æ•æ„Ÿä¿¡æ¯æ›´æ–°çš„åœºæ™¯ä¸‹ï¼Œæ­¤ bug å¯èƒ½ä¼šå¯¼è‡´ pod ä½¿ç”¨è¿‡æ—¶çš„å‡­æ®ï¼Œä½†è¿™ä¸»è¦å±äºåŠŸèƒ½ä¸Šçš„åŒæ­¥æˆ–æ—¶åºé—®é¢˜ï¼Œå¹¶éè®¾è®¡æˆ–å®ç°ä¸Šçš„å®‰å…¨æ¼æ´ã€‚å› æ­¤ï¼Œå¹¶æ²¡æœ‰ç›´æ¥è¯±å‘å®‰å…¨æ”»å‡»çš„é€šé“ï¼Œå…¶é€ æˆçš„åæœä¹Ÿä¸»è¦æ˜¯ä¸šåŠ¡é€»è¾‘ä¸Šçš„é£é™©ï¼Œè€Œéå®‰å…¨æ€§å±å®³ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥é—®é¢˜ä¸º kubelet ç¼“å­˜æ›´æ–°å»¶è¿Ÿå¯¼è‡´ pod å¯åŠ¨æ—¶è¯»å–æ—§ secret å€¼çš„é—®é¢˜ï¼Œå¹¶éç›´æ¥çš„å®‰å…¨æ¼æ´ï¼Œ
# å› æ­¤ä¸éœ€è¦æä¾›ç”¨äºå®‰å…¨æ”»å‡»çš„å¤ç° PoC è„šæœ¬ã€‚
# å¦‚éœ€ç”¨äºä¸šåŠ¡è°ƒè¯•éªŒè¯ï¼Œå¯ä½¿ç”¨å¦‚ä¸‹è„šæœ¬æ¨¡æ‹Ÿ secret æ›´æ–°å’Œ pod é‡å¯åçš„ secret è¯»å–è¿‡ç¨‹ï¼ˆä»…ç”¨äºç ”ç©¶å’Œæµ‹è¯•ï¼Œä¸æ¶‰åŠç»•è¿‡å®‰å…¨æªæ–½ï¼‰ã€‚

import time
import threading
from http.server import BaseHTTPRequestHandler, HTTPServer

# æ¨¡æ‹Ÿä¸€ä¸ªç®€å•çš„ HTTP æœåŠ¡å™¨ç”¨äºå±•ç¤º secret çš„å½“å‰å€¼
SECRET_VALUE = "old_secret"  # åˆå§‹ secret å€¼

class RequestHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        global SECRET_VALUE
        self.send_response(200)
        self.send_header("Content-type", "text/plain")
        self.end_headers()
        self.wfile.write(SECRET_VALUE.encode("utf-8"))

def run_http_server():
    server_address = ('', 8080)
    httpd = HTTPServer(server_address, RequestHandler)
    print("HTTP server running on port 8080...")
    httpd.serve_forever()

def simulate_secret_update():
    global SECRET_VALUE
    # åˆå§‹ scenario: podå¯åŠ¨æ—¶è¯»å– secret
    print("Pod å¯åŠ¨è¯»å–çš„ secret:", SECRET_VALUE)
    time.sleep(2)
    # æ›´æ–° secret å€¼ï¼Œæ¨¡æ‹Ÿ secret è¢«æ›´æ–°ä½†æ˜¯ pod å°šæœªæ„ŸçŸ¥å˜åŒ–
    SECRET_VALUE = "new_secret"
    print("æ›´æ–° secretï¼Œæ–°çš„å€¼ä¸º:", SECRET_VALUE)
    # æ¨¡æ‹Ÿ pod é‡å¯ï¼Œä½†ç”±äºç¼“å­˜å»¶è¿Ÿï¼Œç¬¬ä¸€æ¬¡é‡å¯å¯èƒ½ä»æ—§è¯»å–åˆ°æ—§å€¼ï¼ˆæ­¤å¤„ä»…åšæ¨¡æ‹Ÿï¼Œç”¨ sleep è¡¨ç¤ºå»¶æ—¶åˆ·æ–°ï¼‰
    time.sleep(2)
    print("Pod é‡å¯åè¯»å–çš„ secret:", SECRET_VALUE)

if __name__ == "__main__":
    # å¯åŠ¨ HTTP æœåŠ¡å™¨ï¼Œç”¨äºè§‚å¯Ÿ secret å€¼
    server_thread = threading.Thread(target=run_http_server, daemon=True)
    server_thread.start()
    
    # æ¨¡æ‹Ÿ secret æ›´æ–°æµç¨‹
    simulate_secret_update()
```


**è§£é‡Šè¯´æ˜ï¼š**

ä¸Šè¿°è„šæœ¬å¹¶éç”¨äºæ”»å‡»ï¼Œè€Œæ˜¯ç”¨äºæ¨¡æ‹Ÿä¸šåŠ¡åœºæ™¯ä¸­ secret æ›´æ–°ä¸ pod é‡å¯çš„è¿‡ç¨‹ã€‚è„šæœ¬å¯åŠ¨äº†ä¸€ä¸ª HTTP æœåŠ¡å™¨ï¼ˆç›‘å¬ 8080 ç«¯å£ï¼‰ï¼Œç”¨äºå±•ç¤ºå½“å‰ secret çš„å€¼ï¼›åŒæ—¶é€šè¿‡ simulate_secret_update() å‡½æ•°æ¨¡æ‹Ÿ secret ä» â€œæ—§â€ åˆ° â€œæ–°â€ çš„è½¬æ¢ä»¥åŠ pod åœ¨æ›´æ–°å‰åè¯»å– secret çš„åœºæ™¯ã€‚ç”±äºé—®é¢˜æœ¬èº«å±äº kubelet ç¼“å­˜æ›´æ–°å»¶è¿Ÿçš„é—®é¢˜ï¼Œå¹¶ä¸ä¼šå¯¼è‡´å®‰å…¨æ”»å‡»ï¼Œå› æ­¤é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ï¼Œæ‰€ä»¥ä¸å­˜åœ¨é«˜é£é™©æ¼æ´éœ€è¦ç‰¹åˆ«ç”Ÿæˆæ”»å‡» PoCã€‚æœ¬è„šæœ¬ä»…ç”¨äºå¸®åŠ©æµ‹è¯•ä¸éªŒè¯å®é™…åº”ç”¨ä¸­å¯èƒ½å‡ºç°çš„ç°è±¡ã€‚

---


## Issue #129636 Prevent alpha feature gates from being enabled by default

- Issue é“¾æ¥ï¼š[#129636](https://github.com/kubernetes/kubernetes/issues/129636)

### Issue å†…å®¹

#### What happened?

An alpha feature was accidently introduced as on-by-default which probably should not be allowed.
Maybe there should be some ci checks to prevent this from happening in the future?

https://github.com/kubernetes/kubernetes/blob/2d0a4f75560154454682b193b42813159b20f284/pkg/features/versioned_kube_features.go#L826

#### What did you expect to happen?

Alpha features should not be on-by-default

#### How can we reproduce it (as minimally and precisely as possible)?

https://github.com/kubernetes/kubernetes/blob/2d0a4f75560154454682b193b42813159b20f284/pkg/features/versioned_kube_features.go#L826

#### Anything else we need to know?

/cc @enj 

#### Kubernetes version

<details>

v1.32

</details>


#### Cloud provider

<details>

N/A
</details>


#### OS version

<details>

N/A

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue ä¸»è¦æè¿°ç”±äºå¤±è¯¯å°†ä¸€ä¸ªå¤„äº alpha é˜¶æ®µçš„åŠŸèƒ½ï¼ˆfeature gateï¼‰é»˜è®¤å¼€å¯äº†ï¼Œè€ŒæŒ‰ç…§é¢„æœŸï¼Œalpha åŠŸèƒ½ä¸åº”é»˜è®¤å¯ç”¨ã€‚è™½ç„¶è¯¥é—®é¢˜å¯èƒ½ä¼šå¯¼è‡´æœªå……åˆ†æµ‹è¯•ã€ä¸ç¨³å®šæˆ–æ„å¤–è¡Œä¸ºï¼Œä½†å°± Issue æè¿°è€Œè¨€ï¼Œå®ƒå±äºé…ç½®æˆ–åŠŸèƒ½ç¼ºé™·é—®é¢˜ï¼Œè€Œéç›´æ¥çš„å®‰å…¨æ¼æ´ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œè¯¥é—®é¢˜ä¸ä¼šç›´æ¥å¯¼è‡´è¿œç¨‹ä»£ç æ‰§è¡Œã€æ‹’ç»æœåŠ¡ã€æƒé™æå‡ç­‰å®‰å…¨é—®é¢˜ã€‚å› æ­¤ï¼Œä»å®‰å…¨çš„è§’åº¦æ¥çœ‹ï¼Œè¯¥é—®é¢˜å¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
# æœ¬è„šæœ¬ä»…ç”¨äºè¯æ˜è¯¥ Issue å¹¶éå®‰å…¨é—®é¢˜ï¼Œå› æ­¤ä¸åŒ…å«å…·ä½“çš„å®‰å…¨æ¼æ´å¤ç°é€»è¾‘ã€‚
# æ­¤è„šæœ¬å¯åŠ¨ä¸€ä¸ªç®€å•çš„ HTTP æœåŠ¡å™¨ï¼Œä¾›æœ¬åœ°æµ‹è¯•ä½¿ç”¨ã€‚
# æ³¨æ„ï¼šè¯¥è„šæœ¬ä¸ä¼šæ¨¡æ‹Ÿå‡ºä»»ä½•å®‰å…¨æ¼æ´ï¼Œä»…ä¸ºæ»¡è¶³æ ¼å¼è¦æ±‚ã€‚

from http.server import HTTPServer, BaseHTTPRequestHandler

class TestHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.send_header("Content-type", "text/plain")
        self.end_headers()
        self.wfile.write(b"This is a test server and no security vulnerability is reproduced.")

def run_server():
    server_address = ('', 8080)
    httpd = HTTPServer(server_address, TestHandler)
    print("HTTP test server running on port 8080...")
    try:
        httpd.serve_forever()
    except KeyboardInterrupt:
        print("Server stopped.")
        httpd.server_close()

if __name__ == '__main__':
    run_server()
```


**è§£é‡Šè¯´æ˜ï¼š**

åœ¨æœ¬å¤ç°è„šæœ¬ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªç®€å•çš„ HTTP æœåŠ¡å™¨ï¼Œç›‘å¬åœ¨ 8080 ç«¯å£ã€‚è¯¥æœåŠ¡å™¨ä»…è¿”å›ä¸€æ¡ç®€å•çš„æ–‡æœ¬ä¿¡æ¯ï¼Œè¯´æ˜å½“å‰è¿è¡Œçš„æ˜¯ä¸€ä¸ªæµ‹è¯•æœåŠ¡å™¨ï¼Œå¹¶ä¸”æœ¬è„šæœ¬ä¸æ¨¡æ‹Ÿä¹Ÿä¸ä¼šè§¦å‘ä»»ä½•å®‰å…¨æ¼æ´ã€‚è„šæœ¬ä¸­çš„å†…å®¹ä»…ä¸ºæ»¡è¶³å›ç­”æ ¼å¼è¦æ±‚ï¼Œå¹¶è¯´æ˜ Issue æ‰€æ¶‰åŠçš„é—®é¢˜å±äºé…ç½®æˆ–åŠŸèƒ½å±‚é¢çš„ç–æ¼ï¼Œä¸å…·å¤‡ç›´æ¥çš„å®‰å…¨é£é™©ã€‚å¦‚æœå®é™…éƒ¨ç½²ä¸­å¯ç”¨äº† alpha åŠŸèƒ½ï¼Œå¯èƒ½ä¼šå¸¦æ¥å…¶ä»–ç¨³å®šæ€§å’ŒåŠŸèƒ½æ–¹é¢çš„é—®é¢˜ï¼Œä½†å¹¶ä¸ç›´æ¥æ„æˆå®‰å…¨å¨èƒã€‚

---


## Issue #129632 Recovery after injecting memory overload fault,  pod cannot be scheduled

- Issue é“¾æ¥ï¼š[#129632](https://github.com/kubernetes/kubernetes/issues/129632)

### Issue å†…å®¹

#### What happened?

Recovery after injecting memory overload fault,  pod cannot be scheduled

#### What did you expect to happen?

Recovery after injecting memory overload fault,  pod can be scheduled normal

#### How can we reproduce it (as minimally and precisely as possible)?

Direct cause:
Two PVCs are bound to the same master node (master3), and pod configuration has anti affinity, making it impossible to schedule to the same node
Process analysis:
1. Injecting memory overload fault, scheduling multiple replicas of deployment in the scheduler: after scheduling the first pod to master3, calling the local CSI interface through volume_manager for 10 minutes without response, continuing to sequentially schedule other replicas;
2. When the scheduler schedules to the second replica pod, as the first pod has not yet completed scheduling on master3, it continues to schedule the second replica pod to master3. After 4 minutes, the local CSI responds, and both volumes are successfully created. At this point, the pod is bound to PVC, causing both PVCs to be bound to master3;



When the scheduler schedules a pod to master3, if there is no response for 10 minutes when calling the local CSI interface through volumn_manager, the timeout scheduling fails. When scheduling other pods to Master3 again, after 4 minutes, LocalCSI responded and successfully bound two volumes to PVC, resulting in two PVCs being bound to Master3. Authsercice has anti affinity configuration and cannot be scheduled to the same node at the same time, causing it to remain pending

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# Version: v1.28.1
```

</details>


#### Cloud provider

<details>
na
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ­¤ Issue æè¿°çš„æ˜¯åœ¨æ³¨å…¥å†…å­˜è¿‡è½½æ•…éšœåï¼ŒPod è°ƒåº¦å¼‚å¸¸çš„é—®é¢˜ã€‚ä¸»è¦é—®é¢˜åœ¨äºç”±äºè°ƒåº¦è¿‡ç¨‹ä¸­çš„è¶…æ—¶å“åº”ï¼Œå¯¼è‡´ä¸¤ä¸ª PVC è¢«ç»‘å®šåˆ°åŒä¸€èŠ‚ç‚¹ï¼Œä»è€Œè¿åäº† pod çš„åäº²å’Œæ€§é…ç½®ï¼Œä½¿å¾—åç»­è°ƒåº¦çš„ pod æ— æ³•æˆåŠŸè°ƒåº¦ã€‚è¿™æ˜æ˜¾æ˜¯è°ƒåº¦å’Œå·ç»‘å®šæ–¹é¢çš„é€»è¾‘é—®é¢˜ï¼Œå±äºç³»ç»Ÿè°ƒåº¦é”™è¯¯å’Œèµ„æºé…ç½®é—®é¢˜ï¼Œè€Œä¸æ˜¯å®‰å…¨æ¼æ´ã€‚åŒæ—¶ï¼ŒIssue ä¸­å¹¶æœªæ¶‰åŠåˆ°å‘½ä»¤æ‰§è¡Œã€ææƒã€è¿œç¨‹æœºå™¨è®¿é—®ç­‰é«˜é£é™©å®‰å…¨é—®é¢˜ï¼Œä¹Ÿæ²¡æœ‰æ³„éœ²æ•æ„Ÿå‡­æ®æˆ–é…ç½®é”™è¯¯å¼•å‘æ”»å‡»çš„æƒ…å½¢ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
æ­¤è„šæœ¬ä»…ç”¨äºç¡®è®¤è¯¥ Issue æè¿°çš„é—®é¢˜ä¸ºè°ƒåº¦å¼‚å¸¸é—®é¢˜ï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚
å› æ­¤ï¼Œä¸å­˜åœ¨å¯ç›´æ¥åˆ©ç”¨çš„é«˜é£é™©å®‰å…¨æ¼æ´çš„å¤ç°é€»è¾‘ã€‚
"""

def main():
    print("è¯¥Issueæè¿°çš„æ˜¯è°ƒåº¦é€»è¾‘é”™è¯¯ï¼Œä¸æ¶‰åŠå®‰å…¨æ¼æ´ã€‚")
    print("è¯·æ£€æŸ¥Kubernetesè°ƒåº¦æ—¥å¿—ä¸CSIå·ç»‘å®šæµç¨‹ï¼Œæ’æŸ¥æ³¨å…¥å†…å­˜è¿‡è½½æ•…éšœåçš„è°ƒåº¦å¼‚å¸¸é—®é¢˜ã€‚")

if __name__ == "__main__":
    main()
```


**è§£é‡Šè¯´æ˜ï¼š**

ä» Issue å†…å®¹æ¥çœ‹ï¼Œé—®é¢˜äº§ç”Ÿçš„åŸå› æ˜¯è°ƒåº¦å™¨åœ¨å†…å­˜æ•…éšœæ³¨å…¥è¿‡ç¨‹ä¸­å¤„ç†è¶…æ—¶åŠåŒæ­¥é—®é¢˜ï¼Œå¯¼è‡´PVCç»‘å®šçŠ¶æ€å¼‚å¸¸ã€‚è¯¥é—®é¢˜å±äºè¿ç»´è°ƒåº¦ç­–ç•¥å’Œèµ„æºé…ç½®é—®é¢˜ï¼Œæ— æ³•ç›´æ¥è¢«è¿œç¨‹æ”»å‡»è€…åˆ©ç”¨ä»¥å®æ–½å‘½ä»¤æ‰§è¡Œã€æƒé™æå‡ç­‰å®‰å…¨æ”»å‡»ã€‚æŒ‰ç…§é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œé—®é¢˜å¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œå› æ­¤é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚å¤ç°è„šæœ¬ä¸­ä»…ç®€å•è¾“å‡ºæç¤ºä¿¡æ¯ï¼Œå‘ŠçŸ¥è¯¥é—®é¢˜ä¸ºè°ƒåº¦é€»è¾‘é”™è¯¯ï¼Œä¸å­˜åœ¨å¯ç›´æ¥åˆ©ç”¨çš„å®‰å…¨æ¼æ´å¤ç°æ–¹æ³•ã€‚

---


## Issue #129621 https://console.cloud.google.com/storage/browser/kubernetes-release/release/ does not have latest releases

- Issue é“¾æ¥ï¼š[#129621](https://github.com/kubernetes/kubernetes/issues/129621)

### Issue å†…å®¹

#### What happened?

I have noticed that only the alpha version of K8s 1.32.x is available on https://console.cloud.google.com/storage/browser/kubernetes-release/release/ and only up to K8s 1.31.0. Latest update to this bucket seems to have been months ago. Is this related to https://github.com/kubernetes/kubernetes/issues/127595 / https://github.com/kubernetes/kubernetes/issues/127350 ?

#### What did you expect to happen?

Latest releases including up to K8s 1.31.4 and K8s 1.32.0 should be available at https://console.cloud.google.com/storage/browser/kubernetes-release/release/

#### How can we reproduce it (as minimally and precisely as possible)?

visit https://console.cloud.google.com/storage/browser/kubernetes-release/release/ and see missing releases

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ issue æè¿°çš„æ˜¯ Kubernetes å®˜æ–¹å‘å¸ƒçš„ Google Cloud Storage å­˜å‚¨æ¡¶ä¸­ç¼ºå°‘æœ€æ–°ç‰ˆæœ¬çš„å‘å¸ƒå†…å®¹é—®é¢˜ï¼Œä» issue çš„è¡¨è¿°æ¥çœ‹ï¼Œä¸»è¦ä½“ç°ä¸ºå‘å¸ƒå†…å®¹ç»´æŠ¤å’Œæ›´æ–°ä¸åŠæ—¶çš„é—®é¢˜ã€‚æ²¡æœ‰è¿¹è±¡è¡¨æ˜å­˜åœ¨ä»£ç æ¼æ´ã€æœªæˆæƒè®¿é—®ã€å‘½ä»¤æ‰§è¡Œã€ææƒã€å®¹å™¨é€ƒé€¸ç­‰å®‰å…¨æ€§é—®é¢˜ï¼Œä¹Ÿæœªæš´éœ²æ•æ„Ÿé…ç½®ä¿¡æ¯æˆ–å‡­æ®ï¼Œå› æ­¤è¯¥ issue ä¸å®‰å…¨é—®é¢˜æ— å…³ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# ç”±äºè¯¥ issue ä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œå› æ­¤ä¸å­˜åœ¨å¤ç°å®‰å…¨é—®é¢˜çš„è„šæœ¬ã€‚
# ä»¥ä¸‹è„šæœ¬ä»…ç”¨äºæ¨¡æ‹Ÿè®¿é—®å‘å¸ƒå†…å®¹å­˜å‚¨æ¡¶é¡µé¢ï¼ŒéªŒè¯ç¼ºå¤±æœ€æ–°ç‰ˆæœ¬çš„ç°è±¡ï¼Œä¸åŒ…å«å®‰å…¨é£é™©å¤ç°ã€‚
import requests

def check_k8s_releases():
    url = "https://console.cloud.google.com/storage/browser/kubernetes-release/release/"
    try:
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            # ç”¨ç®€å•çš„å…³é”®å­—åŒ¹é…æ£€æŸ¥æ˜¯å¦åŒ…å«æœ€æ–°ç‰ˆæœ¬ä¿¡æ¯
            if "1.32.0" in response.text or "1.31.4" in response.text:
                print("æ£€æµ‹åˆ°æœ€æ–°ç‰ˆæœ¬çš„å‘å¸ƒä¿¡æ¯ã€‚")
            else:
                print("æœªæ£€æµ‹åˆ°æœ€æ–°ç‰ˆæœ¬çš„å‘å¸ƒä¿¡æ¯ã€‚")
        else:
            print(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}")
    except Exception as e:
        print(f"è¯·æ±‚è¿‡ç¨‹ä¸­å‡ºé”™ï¼š{e}")

if __name__ == "__main__":
    check_k8s_releases()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. è¯¥è„šæœ¬ä½¿ç”¨ requests åº“å¯¹æŒ‡å®šçš„ URL å‘èµ· HTTP GET è¯·æ±‚ï¼Œæ¨¡æ‹Ÿè®¿é—®è¯¥å­˜å‚¨æ¡¶é¡µé¢ã€‚
2. è„šæœ¬ç®€å•æ£€æŸ¥é¡µé¢ HTML æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«å­—ç¬¦ä¸² "1.32.0" æˆ– "1.31.4"ï¼Œä»è€Œåˆ¤æ–­æ˜¯å¦åŒ…å«äº†æœ€æ–°ç‰ˆæœ¬å‘å¸ƒçš„ä¿¡æ¯ã€‚
3. è¯¥è„šæœ¬ä»…ç”¨äºç ”ç©¶å’Œæœ¬åœ°æµ‹è¯•éªŒè¯æŒ‡å®š URL ä¸­æ˜¯å¦å­˜åœ¨æœ€æ–° release ä¿¡æ¯ï¼Œå¹¶æ— å®‰å…¨é£é™©å¤ç°æƒ…å†µã€‚ç”±äºè¯¥ issue æœ¬èº«ä¸å±äºå®‰å…¨é—®é¢˜ï¼Œå› æ­¤è¯¥å¤ç°è„šæœ¬ä»…ç”¨äºæ¨¡æ‹Ÿé—®é¢˜æè¿°ä¸­åæ˜ çš„ç°è±¡ï¼Œå¹¶ä¸æ¶‰åŠæ•æ„Ÿæ“ä½œæˆ–æƒé™é—®é¢˜ã€‚

---


## Issue #129617 DNS latency when a CoreDNS pod is deleted

- Issue é“¾æ¥ï¼š[#129617](https://github.com/kubernetes/kubernetes/issues/129617)

### Issue å†…å®¹

#### What happened?

Hello,

We noticed that when one of our CoreDNS pods is deleted, some client pods experience latency on their DNS queries.

This happens when the pod is completely deleted from Kubernetes, after the `terminating` phase. When it happens, all DNS requests from some pods (not all of them, it seems random) are "stuck" for a few seconds (the value is the timeout value in the pod `resolv.conf` file, so 5 seconds by default but if I set in the pod spec a timeout of 3 seconds in `dnsConfig.options`, it will be 3 seconds at maximum).

You can see on this screenshot how it looks like on the application side (traces are generated using Opentemetry + [httptrace](https://pkg.go.dev/net/http/httptrace)): When the coredns pod is removed (not in terminating phase, completely removed, so after the `lameduck` period, we even tried 17 seconds for lameduck), all requests are waiting for 5 seconds. We can see span durations decreasing because of new requests all wait until the system can send requests again:

![Image](https://github.com/user-attachments/assets/cc681410-1422-4683-aef1-fc028583407f)

We ran `tcpdump` (`tcpdump -w capture.pcap udp port 53`) on the pod namespace (using `nsenter`) and we can indeed see that during 5 seconds, no DNS requests are visible (look at the traces and the wireshark timestamps, they are matching): 

![Image](https://github.com/user-attachments/assets/49aa51e4-31c4-4a06-b1ff-c5b1d708d4fe)

We're using Karpenter on our Kubernetes clusters so CoreDNS pods are destroyed regularly. To mitigate the issue, we moved the CoreDNS pods to stable nodes but at every node upgrade, the problem occurs so it's not a good long-term solution (it is also more expensive for us to have dedicated nodes for CoreDNS). 


#### What did you expect to happen?

We didn't expect any latency during CoreDNS rollouts.

#### How can we reproduce it (as minimally and precisely as possible)?

**On AWS EKS**

A simple `kubectl rollout restart -n kube-system deployment coredns` is enough to impact our applications.

**On Exoscale SKS**

I created a 1.31.4 cluster (and also reproduced with kube-proxy 1.32.0 on it) with 5 CoreDNS replicas, and then deployed an application generating DNS traffic on the cluster (it's the only app running on the cluster):

```go
package main

import (
	"context"
	"errors"
	"fmt"
	"net"
	"os"
	"strconv"
	"time"
)

func resolve(ctx context.Context, domain string) ([]net.IP, error) {
	addrs, err := net.DefaultResolver.LookupIPAddr(ctx, domain)
	if err != nil {
		return nil, err
	}
	result := make([]net.IP, len(addrs))
	for i, ia := range addrs {
		result[i] = ia.IP
	}
	return result, nil
}

func main() {
	domain := os.Getenv("DOMAIN")
	if domain == "" {
		panic(errors.New("DOMAIN env var is empty"))
	}
	parallelism, err := strconv.Atoi(os.Getenv("PARALLELISM"))
	if err != nil {
		panic(err)
	}
	interval, err := strconv.Atoi(os.Getenv("INTERVAL"))
	if err != nil {
		panic(err)
	}

	for i := 0; i < parallelism; i++ {
		ticker := time.NewTicker(time.Duration(interval) * time.Millisecond)
		go func() {
			for {
				select {
				case <-ticker.C:
					ctx, cancel := context.WithTimeout(context.Background(), 7*time.Second)
					start := time.Now().UnixMilli()
					_, err := resolve(ctx, domain)
					cancel()
					end := time.Now().UnixMilli()
					duration := end - start
					if err != nil {
						fmt.Printf("%d: resolved in %d milliseconds with error: %s\n", start, duration, err.Error())
					} else {
						fmt.Printf("%d: resolved in %d milliseconds\n", start, duration)
					}

				}
			}
		}()
	}
	time.Sleep(24000 * time.Second)
}

```

I then deploy this code using this deployment:


```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: dns-test
spec:
  replicas: 2
  selector:
    matchLabels:
      app: dns-test
  template:
    metadata:
      labels:
        app: dns-test
    spec:
      containers:
      - name: dns
        image: mcorbin/dnstest:0.0.3
        resources:
          limits:
            memory: "300Mi"
          requests:
            cpu: "0.5"
            memory: "300Mi"
        env:
          - name: DOMAIN
            value: "metrics-server.kube-system.svc.cluster.local."
          - name: PARALLELISM
            value: "4"
          - name: INTERVAL
            value: "50"
```

From time to time I can see slow DNS queries after rollout, similar to what I see on EKS:

```
1736868482815: resolved in 5003 milliseconds
1736868482815: resolved in 5003 milliseconds
1736868482815: resolved in 5003 milliseconds
1736868482815: resolved in 5003 milliseconds
```

#### Anything else we need to know?


We already investigated a lot of things:
- increased lameduck option on CoreDNS to 17 seconds: no changes
- It's not a CoreDNS performance issue (metrics are good, no latency at all which was verified by enabling debug logs).
- It's not a kube-proxy reconciliation latency issue: kube-proxy logs/metrics are good, endpoints are correctly updated
- We're mostly AWS EKS users but it seems we're also able to reproduce the issue on Exoscale SKS offering.

I suspect a conntrack issue when conntrack entries are removed from kube-proxy. I indeed noticed that cleaning the conntrack manually for CoreDNS IPs was causing the same symptoms 

#### Kubernetes version



<details>

We reproduced the issue on several Kubernetes versions/cloud providers:

On AWS EKS:
```
Client Version: v1.30.7
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.8-eks-2d5f260
```

On Exoscale SKS
```console
Client Version: v1.30.7
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.3
```

I also reproduced on Exoscale SKS with server `v1.31.3` and kube-proxy `v1.32.0` to get [this fix](https://github.com/kubernetes/kubernetes/pull/127318).

The AWS EKS Service Team also told us that they can reproduce the issue on the (unreleased yet to users) `v1.32.0` on their side. 

</details>



#### Cloud provider

<details>
AWS EKS, Exoscale SKS
</details>


#### OS version

<details>

```console
NAME="Amazon Linux"
VERSION="2"
ID="amzn"
ID_LIKE="centos rhel fedora"
VERSION_ID="2"
PRETTY_NAME="Amazon Linux 2"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2"
HOME_URL="https://amazonlinux.com/"
SUPPORT_END="2025-06-30"
```

</details>


#### Install tools

Both cases use kube-proxy with iptables mode.

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯åœ¨ CoreDNS Pod è¢«åˆ é™¤åï¼Œé›†ç¾¤ä¸­éƒ¨åˆ†å®¢æˆ·ç«¯ Pod çš„ DNS æŸ¥è¯¢å‡ºç°å»¶æ—¶é—®é¢˜ã€‚ç»è¿‡å¯¹æè¿°å†…å®¹çš„åˆ†æï¼Œå¯ä»¥çœ‹å‡ºé—®é¢˜åŸå› ä¸»è¦å¯èƒ½ä¸ Kubernetes ç½‘ç»œä¸­ conntrack æ¡ç›®è¢«æ¸…é™¤æœ‰å…³ï¼Œå¯¼è‡´ DNS è¯·æ±‚æ— æ³•åŠæ—¶è·¯ç”±åˆ°æ–°çš„ CoreDNS Podï¼Œä»è€Œäº§ç”Ÿäº†æŸ¥è¯¢è¶…æ—¶å»¶æ—¶ã€‚è¿™æ˜¯ä¸€ä¸ªæ€§èƒ½/å¯ç”¨æ€§é—®é¢˜ï¼Œè€Œéå…¸å‹çš„å®‰å…¨é—®é¢˜ï¼Œæ²¡æœ‰æ¶‰åŠåˆ°å‘½ä»¤æ‰§è¡Œã€ææƒã€å®¹å™¨é€ƒé€¸ã€æ‹’ç»æœåŠ¡æ”»å‡»ï¼ˆDoSï¼Œä¸”æ”»å‡»è€…éœ€è¦å…·å¤‡è¾ƒé«˜æƒé™æ‰èƒ½è§¦å‘ï¼‰æˆ–æ•æ„Ÿä¿¡æ¯æ³„éœ²ç­‰å®‰å…¨é£é™©ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥ Issue å¹¶ä¸æ„æˆå®‰å…¨æ¼æ´ï¼Œå…¶é—®é¢˜çš„æ ¹æœ¬åŸå› æ˜¯é›†ç¾¤ç½‘ç»œå¤„ç†é€»è¾‘é—®é¢˜ï¼Œè€Œéå®‰å…¨è®¾è®¡ç¼ºé™·æˆ–é…ç½®é”™è¯¯ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
è¯¥è„šæœ¬ä»…ä½œä¸ºå ä½è¯´æ˜ï¼Œæœ¬ Issue ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œå› æ­¤æ— éœ€æä¾›é’ˆå¯¹å®‰å…¨æ¼æ´çš„å¤ç°POCã€‚
è¿è¡Œæ­¤è„šæœ¬ä»…ä¼šæ‰“å°æç¤ºä¿¡æ¯ã€‚
"""

def main():
    print("è¯¥ Issue ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ— éœ€å¤ç°è„šæœ¬ã€‚")

if __name__ == "__main__":
    main()
```


---


## Issue #129616 eviction_manager does not attempt to cleanup unused images before evicting pods (with imagefs)

- Issue é“¾æ¥ï¼š[#129616](https://github.com/kubernetes/kubernetes/issues/129616)

### Issue å†…å®¹

#### What happened?

TL;DR: One of our nodes being out of ephemeral-storage (from imagefs being beyond the hard eviction threshold) led to a pod eviction, but the eviction_manager did not try to reclaim unused images before attempting eviction. This happens shortly after the pod is setup / started by the kubelet.

The metrics for that node (image below) point to imageFs filling up (and not nodefs). 
We also had  `Threshold quantity: 4668496467, available: 4527052Ki` in the eviction event, which match (roughly) `size of our imagefs * imagefs.available threshold` ) ,

What seems surprising is that the kubelet apparently does not attempt to reclaim unused images

If I'm following correctly the code flow here (all the code link are to release-2.29, since it's where I hit this) https://github.com/kubernetes/kubernetes/blob/86e25a07e279516eb13af436ca5706b1806ea604/pkg/kubelet/eviction/eviction_manager.go#L456-L459 and here https://github.com/kubernetes/kubernetes/blob/86e25a07e279516eb13af436ca5706b1806ea604/pkg/kubelet/eviction/helpers.go#L1201-L1210

we should always hit the following code  when hitting signalImageFSAvailable https://github.com/kubernetes/kubernetes/blob/86e25a07e279516eb13af436ca5706b1806ea604/pkg/kubelet/images/image_gc_manager.go#L390-L394 

However, this isn't always the case: see the logs below: in the first one, this does happen as it should, and no eviction occured. In the second one the image GC is not called and eviction happens:

Image GC, no eviction
```
janv. 04 06:11:53 qpk8s-node-033 kubelet[26615]: I0104 06:11:53.500246   26615 util.go:30] "No sandbox for pod can be found. Need to start a new one" pod="REDACTED-dev/cyclades-dev1-job-db-w74bj"
janv. 04 06:11:53 qpk8s-node-033 kubelet[26615]: I0104 06:11:53.867218   26615 kubelet.go:2465] "SyncLoop (PLEG): event for pod" pod="REDACTED-dev/REDACTED-dev1-job-db-w74bj" event={"ID":"9e7
7962a-9ef4-4003-a620-8fc9f99a65de","Type":"ContainerStarted","Data":"fed98b0d4a5008c9d4e3813afb0477c4bec6d81c9ec9ca1c5f359fd00bc08687"}
janv. 04 06:12:08 qpk8s-node-033 kubelet[26615]: I0104 06:12:08.959726   26615 eviction_manager.go:366] "Eviction manager: attempting to reclaim" resourceName="ephemeral-storage"
janv. 04 06:12:08 qpk8s-node-033 kubelet[26615]: I0104 06:12:08.960139   26615 container_gc.go:88] "Attempting to delete unused containers"
janv. 04 06:12:08 qpk8s-node-033 kubelet[26615]: I0104 06:12:08.960699   26615 scope.go:117] "RemoveContainer" containerID="74b79d8dd06301f2c806ba5227797fb93fe38b96dc33caad4803f74d6440f1b8"
janv. 04 06:12:08 qpk8s-node-033 kubelet[26615]: I0104 06:12:08.967179   26615 scope.go:117] "RemoveContainer" containerID="231f370f8d36173c94c9560baee42bf1fe108573ff322a1c4f542233671078db"
janv. 04 06:12:08 qpk8s-node-033 kubelet[26615]: I0104 06:12:08.971757   26615 scope.go:117] "RemoveContainer" containerID="893a23f56984eb2f53dcec21d138ad84a93969dcabf6e72b5a270f8e45351e52"
janv. 04 06:12:09 qpk8s-node-033 kubelet[26615]: I0104 06:12:09.636342   26615 image_gc_manager.go:391] "Attempting to delete unused images"
janv. 04 06:12:09 qpk8s-node-033 kubelet[26615]: I0104 06:12:09.638685   26615 image_gc_manager.go:447] "Removing image to free bytes" imageID="sha256:cc42097299cf96c7b08e31669dddb2af764381fd
d167cfaf25f94f3989a22e6e" size=853261112 runtimeHandler=""
janv. 04 06:12:09 qpk8s-node-033 kubelet[26615]: I0104 06:12:09.853135   26615 image_gc_manager.go:447] "Removing image to free bytes" imageID="sha256:78abc00a071160e2902c30cea3350f7dc1250b67
3410231f989103d22bd4e9cb" size=717064073 runtimeHandler=""
<multiples lines of the above>
janv. 04 06:12:15 qpk8s-node-033 kubelet[26615]: I0104 06:12:15.928391   26615 eviction_manager.go:373] "Eviction manager: able to reduce resource pressure without evicting pods." resourceName="ephemeral-storage"
```

No image GC, eviction 
```
janv. 06 12:43:37 qpk8s-node-033 kubelet[26615]: I0106 12:43:37.138045   26615 util.go:30] "No sandbox for pod can be found. Need to start a new one" pod="REDACTED-dev/REDACTED-dev9-grp1-pgest-84d9d6ff47-wflf2"
janv. 06 12:43:37 qpk8s-node-033 kubelet[26615]: I0106 12:43:37.682549   26615 kubelet.go:2465] "SyncLoop (PLEG): event for pod" pod="REDACTED-dev/REDACTED-dev9-grp1-pgest-84d9d6ff47-wflf2" event={"ID":"bad3f101-5920-4a11-9c8a-c0dd40326df2","Type":"ContainerStarted","Data":"934b77ca1f12f514ff7d764f31c77607081e06d50a0bbe2d2937b19b5f523c99"}
janv. 06 12:44:17 qpk8s-node-033 kubelet[26615]: I0106 12:44:17.658612   26615 eviction_manager.go:366] "Eviction manager: attempting to reclaim" resourceName="ephemeral-storage"
janv. 06 12:44:17 qpk8s-node-033 kubelet[26615]: I0106 12:44:17.658831   26615 eviction_manager.go:377] "Eviction manager: must evict pod(s) to reclaim" resourceName="ephemeral-storage"
janv. 06 12:44:17 qpk8s-node-033 kubelet[26615]: I0106 12:44:17.658913   26615 eviction_manager.go:395] "Eviction manager: pods ranked for eviction" pods=["REDACTED-dev/REDACTED-dev9-grp1-pgest-84d9d6ff47-wflf2","kube-system/nginx-proxy-qpk8s-node-033","logging/filebeat-filebeat-xvfqd","kube-system/calico-node-rwk5j","prometheus/node-exporter-xfvp8","kured/kured-l2b8t","REDACTED-dev/REDACTED-dev12-grp0-pcorr-56f674d4f-hb6jh","REDACTED-dev/REDACTED-dev4-grp0-pgest-595d778d79-sbkww","kube-system/kube-proxy-kp7gw","kube-system/nodelocaldns-q6mg4"]
janv. 06 12:44:22 qpk8s-node-033 kubelet[26615]: I0106 12:44:22.456507   26615 kubelet_node_status.go:679] "Recording event message for node" node="qpk8s-node-033" event="NodeHasDiskPressure"
janv. 06 12:44:23 qpk8s-node-033 kubelet[26615]: I0106 12:44:23.769177   26615 kubelet.go:2465] "SyncLoop (PLEG): event for pod" pod="REDACTED-dev/REDACTED-dev9-grp1-pgest-84d9d6ff47-wflf2" event={"ID":"bad3f101-5920-4a11-9c8a-c0dd40326df2","Type":"ContainerStarted","Data":"504dd0177d3f38094dbfbce7bc48e8673e4cc9f97f2b6b6ce2eeda0a20609f54"}
janv. 06 12:44:23 qpk8s-node-033 kubelet[26615]: I0106 12:44:23.769271   26615 kuberuntime_container.go:770] "Killing container with a grace period" pod="REDACTED-dev/REDACTED-dev9-grp1-pgest-84d9d6ff47-wflf2" podUID="bad3f101-5920-4a11-9c8a-c0dd40326df2" containerName="REDACTED-dev9-pgest" containerID="containerd://504dd0177d3f38094dbfbce7bc48e8673e4cc9f97f2b6b6ce2eeda0a20609f54" gracePeriod=30
janv. 06 12:44:23 qpk8s-node-033 kubelet[26615]: I0106 12:44:23.782222   26615 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="REDACTED-dev/REDACTED-dev9-grp1-pgest-84d9d6ff47-wflf2" podStartSLOduration=1.639754381 podStartE2EDuration="47.782147845s" podCreationTimestamp="2025-01-06 12:43:36 +0100 CET" firstStartedPulling="2025-01-06 12:43:37.409859673 +0100 CET m=+3870058.832937451" lastFinishedPulling="2025-01-06 12:44:23.552253138 +0100 CET m=+3870104.975330915" observedRunningTime="2025-01-06 12:44:23.78166704 +0100 CET m=+3870105.204744839" watchObservedRunningTime="2025-01-06 12:44:23.782147845 +0100 CET m=+3870105.205225634"
janv. 06 12:44:24 qpk8s-node-033 kubelet[26615]: I0106 12:44:24.044767   26615 logs.go:325] "Finished parsing log file" path="/var/log/pods/REDACTED-dev_REDACTED-dev9-grp1-pgest-84d9d6ff47-wflf2_bad3f101-5920-4a11-9c8a-c0dd40326df2/REDACTED-dev9-pgest/0.log"
janv. 06 12:44:27 qpk8s-node-033 kubelet[26615]: E0106 12:44:27.659220   26615 eviction_manager.go:614] "Eviction manager: pod failed to evict" err="timeout waiting to kill pod" pod="REDACTED-dev/REDACTED-dev9-grp1-pgest-84d9d6ff47-wflf2"
janv. 06 12:44:27 qpk8s-node-033 kubelet[26615]: I0106 12:44:27.659261   26615 eviction_manager.go:205] "Eviction manager: pods evicted, waiting for pod to be cleaned up" pods=["REDACTED-dev/REDACTED-dev9-grp1-pgest-84d9d6ff47-wflf2"]
```

Here is the disk metrics matching those logs (/var/lib/containerd -> imagefs, /var -> nodefs (includes /var/lib/kubelet) ): 

![Image](https://github.com/user-attachments/assets/66cde14a-dd56-4691-9502-0f8c4a22b612)

#### What did you expect to happen?

I expected the the image gc to be called in both cases by the eviction manager.

#### How can we reproduce it (as minimally and precisely as possible)?

I don't have a reproducer yet. I'll try to find the time to craft one, but can't give guarantee currently.

Possible [ImageGCNoEviction](https://github.com/kubernetes/kubernetes/blob/86e25a07e279516eb13af436ca5706b1806ea604/test/e2e_node/eviction_test.go#L111) might help ? I'm not completely sure how to use this though ğŸ¤” 

#### Anything else we need to know?

While looking for this in the issues and code, I found the following PR, which might have fixed the problem starting from 1.32:
- #127874 (merged in 1.32)

This led me to formulate the theory that maybe the difference between the two case was a different trigger:
- 1 -> adding container images exceeded the threshold, triggering signalImageFsAvailable which led to correctly gc the unused image
- 2 -> container writing to the containerfs (== not a volume mount) triggered signalContainerFsAvailable which is not handled before the above PR.

I haven't delved enough into that code to know if that theory is plausible, though.

Possibly relevant as well : https://github.com/kubernetes/kubernetes/commit/26923b91e8cbd2d409e5d177ddd509429b76cb35 (split disk kep implementation)

Slack thread : https://kubernetes.slack.com/archives/C0BP8PW9G/p1736523264787059

/sig node
/area kubelet
/cc @AnishShah @kannon92 


#### Kubernetes version

<details>

```console
$ kubectl versionClient Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.29.10
```

</details>


#### Cloud provider

<details>
On-premise, on a vsphere infra (no cloud controller)
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Red Hat Enterprise Linux"
VERSION="8.10 (Ootpa)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="8.10"
PLATFORM_ID="platform:el8"
PRETTY_NAME="Red Hat Enterprise Linux 8.10 (Ootpa)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:redhat:enterprise_linux:8::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8"
BUG_REPORT_URL="https://issues.redhat.com/"

REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 8"
REDHAT_BUGZILLA_PRODUCT_VERSION=8.10
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="8.10"
$ uname -a
Linux <REDACTED> 4.18.0-553.27.1.el8_10.x86_64 #1 SMP Fri Oct 18 06:18:15 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>
kubespray
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd 1.7.22
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯ kubelet ä¸­ eviction_manager åœ¨æ£€æµ‹åˆ° imagefs ç©ºé—´ç´§å¼ æ—¶ï¼Œå¹¶æœªæŒ‰é¢„æœŸå…ˆå›æ”¶æœªä½¿ç”¨çš„é•œåƒï¼ˆImage GCï¼‰ï¼Œè€Œæ˜¯ç›´æ¥è§¦å‘äº†å¯¹ Pod çš„é©±é€ã€‚é—®é¢˜æ ¹æºåœ¨äº kubelet å†…éƒ¨å¯¹ä¸åŒå­˜å‚¨ç±»å‹ï¼ˆimagefs ä¸ nodefsï¼‰çš„ç›‘æµ‹ä¸å¤„ç†é€»è¾‘ä¸ä¸€è‡´ï¼Œå¯¼è‡´åœ¨æŸäº›æƒ…å†µä¸‹æœªèƒ½è¿›è¡Œé•œåƒåƒåœ¾å›æ”¶ã€‚è™½ç„¶è¯¥é—®é¢˜å¯èƒ½åœ¨å­˜å‚¨èµ„æºç´§å¼ æ—¶å¼•å‘ Pod è¢«å¼‚å¸¸é©±é€ï¼Œé€ æˆæœåŠ¡ä¸å¯ç”¨çš„æƒ…å†µï¼ˆå³ä¸€ç§èµ„æºåˆ©ç”¨ä¸å¯é æ€§é—®é¢˜ï¼‰ï¼Œä½†ä»å®‰å…¨è§’åº¦çœ‹ï¼Œå®ƒæœ¬è´¨ä¸Šå¹¶ä¸æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒæˆ–è€…æœªæˆæƒçš„è®¿é—®ç­‰å®‰å…¨æ¼æ´ã€‚æ”»å‡»è€…è¦åˆ©ç”¨è¯¥é—®é¢˜é€šå¸¸éœ€è¦å…·å¤‡è°ƒåº¦/åˆ›å»º Pod çš„ç›¸åº”æƒé™ï¼Œå¤šç§Ÿæˆ·åœºæ™¯ä¸‹ä¹Ÿéœ€åœ¨å…è®¸çš„èŒƒå›´å†…æ“ä½œï¼Œæ‰€ä»¥ä¸¥æ ¼æ¥è¯´ï¼Œè¿™ä¸ªé—®é¢˜çš„å½±å“æ›´åå‘äºå¯é æ€§å’ŒæœåŠ¡å¯ç”¨æ€§é—®é¢˜ï¼Œè€Œä¸æ˜¯å®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
è¯´æ˜ï¼šè¯¥è„šæœ¬ä»…ä¸ºå ä½ç”¨é€”ï¼Œå› ä¸ºå½“å‰Issueæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªèµ„æºç®¡ç†å’Œè°ƒåº¦é€»è¾‘é—®é¢˜ï¼Œ
å¹¶éç›´æ¥çš„å®‰å…¨æ¼æ´ï¼Œå› æ­¤æ— éœ€æä¾›ä¸€ä¸ªç”¨äºå®é™…è§¦å‘å®‰å…¨é£é™©çš„POCã€‚
"""

import http.server
import socketserver

def start_http_server():
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„ HTTP æœåŠ¡å™¨ï¼Œç›‘å¬åœ¨8080ç«¯å£ï¼Œç”¨äºå ä½å’Œæµ‹è¯•ç¯å¢ƒæ­å»º
    PORT = 8080
    Handler = http.server.SimpleHTTPRequestHandler

    with socketserver.TCPServer(("", PORT), Handler) as httpd:
        print(f"HTTPæœåŠ¡å™¨å·²ç»å¯åŠ¨ï¼Œç›‘å¬åœ¨ç«¯å£ {PORT}")
        httpd.serve_forever()

def main():
    print("æ­¤è„šæœ¬ä»…ä½œä¸ºå ä½ç¬¦ï¼Œæ— å®‰å…¨æ¼æ´å¤ç°ã€‚")
    # å¦‚æœéœ€è¦æ¨¡æ‹Ÿå…¶ä»–æ“ä½œï¼Œå¯ä»¥è°ƒç”¨ start_http_server æ¥å¯åŠ¨ä¸€ä¸ª HTTP æœåŠ¡
    start_http_server()

if __name__ == "__main__":
    main()
```


---


## Issue #129613 kube-controller-manager restart when leaderelection lost

- Issue é“¾æ¥ï¼š[#129613](https://github.com/kubernetes/kubernetes/issues/129613)

### Issue å†…å®¹

#### What happened?

After the kube-controller-manager component fails to renew the contract, the kube-controller-manager component directly exits the process. Can the kube-controller-manager component be selected as the primary component without restarting the process?

#### What did you expect to happen?

The kube-controller-manager process does not exit after the renewal fails.

#### How can we reproduce it (as minimally and precisely as possible)?

Construct a network problem to make the renewal of the primary node fail.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.31
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ issue æè¿°çš„æ˜¯ kube-controller-manager åœ¨ç§Ÿçº¦ç»­çº¦å¤±è´¥åç›´æ¥é€€å‡ºè¿›ç¨‹çš„é—®é¢˜ï¼Œè€Œæé—®è€…å¸Œæœ›è¯¥ç»„ä»¶åœ¨ç»­çº¦å¤±è´¥æ—¶ä»ç»§ç»­è¿è¡Œï¼Œè€Œä¸æ˜¯é€€å‡ºã€‚ç»è¿‡åˆ†æï¼Œè¯¥é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå®¹é”™ä¸ä¸šåŠ¡é€»è¾‘å¤„ç†çš„é—®é¢˜ï¼Œå±äºç³»ç»Ÿç¨³å®šæ€§æˆ–é«˜å¯ç”¨æ€§è®¾è®¡çš„è€ƒé‡ï¼Œå¹¶éç›´æ¥çš„å®‰å…¨æ¼æ´ã€‚è™½ç„¶åœ¨æŸäº›æç«¯æƒ…å†µä¸‹ï¼ˆä¾‹å¦‚æ”»å‡»è€…æ„é€ ç½‘ç»œæ•…éšœï¼‰å¯èƒ½å¼•èµ·æœåŠ¡ä¸å¯ç”¨ï¼Œä»è€Œé—´æ¥äº§ç”Ÿæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰çš„æ•ˆæœï¼Œä½†è¿™ç§åœºæ™¯ä¸‹æ‰€éœ€çš„æ”»å‡»å‰æé€šå¸¸æ˜¯åœ¨å·²æœ‰è¾ƒé«˜æƒé™æˆ–æ§åˆ¶ç½‘ç»œæ¡ä»¶çš„æƒ…å†µä¸‹è¿›è¡Œçš„ï¼Œå› æ­¤è¿™å¹¶ä¸æ„æˆç›´æ¥çš„é«˜é£é™©å®‰å…¨éšæ‚£ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
"""
è¯¥å¤ç°è„šæœ¬ä»…ç”¨äºè¯æ˜å¦‚ä½•æ¨¡æ‹Ÿç½‘ç»œæ•…éšœå¯¼è‡´ç§Ÿçº¦ç»­çº¦å¤±è´¥çš„åœºæ™¯ï¼Œ
ä½†ç”±äºè¯¥é—®é¢˜æœ¬èº«å±äºå®¹é”™å¤„ç†æˆ–è€…è®¾è®¡é€‰æ‹©é—®é¢˜ï¼Œå¹¶éç›´æ¥å®‰å…¨æ¼æ´ï¼Œ
æ•…ä¸æä¾›é’ˆå¯¹é«˜é£é™©å®‰å…¨æ¼æ´çš„å¤ç°éªŒè¯ä»£ç ã€‚
å¦‚éœ€æµ‹è¯•ï¼Œå¯åœ¨æœ¬åœ°ç¯å¢ƒä¸­åˆ©ç”¨å·¥å…·æ¨¡æ‹Ÿç½‘ç»œä¸­æ–­ï¼Œè§‚å¯Ÿ kube-controller-manager çš„è¡Œä¸ºã€‚
ä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨ iptables è§„åˆ™ä¸´æ—¶æ‹¦æˆªä¸ etcd æˆ–å…¶ä»– leader ç›¸å…³çš„è¿æ¥ã€‚
ä¸‹é¢ç»™å‡ºä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ï¼Œå¯åŠ¨ä¸€ä¸ªç›‘å¬ 8080 ç«¯å£çš„ HTTP æœåŠ¡å™¨ï¼Œç”¨äºæ¨¡æ‹Ÿç½‘ç»œæœåŠ¡ï¼š
"""

import http.server
import socketserver
import threading

PORT = 8080

class Handler(http.server.SimpleHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.end_headers()
        self.wfile.write(b"Hello, this is a dummy server.")

def start_http_server():
    with socketserver.TCPServer(("", PORT), Handler) as httpd:
        print("Serving on port", PORT)
        httpd.serve_forever()

def main():
    # è¿è¡Œ HTTP æœåŠ¡å™¨çº¿ç¨‹ï¼Œç”¨äºæ¨¡æ‹Ÿç½‘ç»œæ¡ä»¶
    server_thread = threading.Thread(target=start_http_server, daemon=True)
    server_thread.start()
    print("HTTPæœåŠ¡å™¨å·²å¯åŠ¨äºç«¯å£", PORT)

    # æ­¤å¤„å¯ä»¥æ‰©å±•å¯¹ iptables ç­‰æ“ä½œè¿›è¡Œæ¨¡æ‹Ÿç½‘ç»œæ•…éšœçš„æ“ä½œï¼Œ
    # ä»¥æ„é€  kube-controller-manager ç§Ÿçº¦ç»­çº¦å¤±è´¥çš„ç½‘ç»œç¯å¢ƒã€‚
    #
    # ä¾‹å¦‚ï¼Œä½¿ç”¨ Python è°ƒç”¨ç³»ç»Ÿå‘½ä»¤é…ç½® iptables è§„åˆ™ï¼š
    # import subprocess
    # cmd = "iptables -A INPUT -p tcp --dport <ç›®æ ‡ç«¯å£> -j DROP"
    # subprocess.run(cmd, shell=True)
    #
    # æ³¨æ„ï¼šä»¥ä¸Šæ“ä½œéœ€åœ¨å…·æœ‰è¶³å¤Ÿæƒé™çš„ç¯å¢ƒä¸‹æ‰§è¡Œï¼Œå¹¶ä»…ç”¨äºæµ‹è¯•ç›®çš„ã€‚
    
    # æ¨¡æ‹Ÿé•¿æ—¶é—´è¿è¡Œï¼Œä¿æŒä¸»çº¿ç¨‹æ´»è·ƒ
    try:
        while True:
            pass
    except KeyboardInterrupt:
        print("é€€å‡º")

if __name__ == "__main__":
    main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥å¤ç°è„šæœ¬ä¸­ï¼š
1. æˆ‘ä»¬å¯åŠ¨äº†ä¸€ä¸ªç®€å•çš„ HTTP æœåŠ¡å™¨ï¼Œç›‘å¬åœ¨ 8080 ç«¯å£ï¼Œå…¶ç›®çš„æ˜¯ä¸ºäº†æ»¡è¶³ç”Ÿæˆå¤ç°è„šæœ¬æ—¶çš„è¦æ±‚ï¼ŒåŒæ—¶å¯ä»¥ç”¨æ¥æ¨¡æ‹Ÿç½‘ç»œæœåŠ¡çš„ç®€å•åœºæ™¯ã€‚
2. æ³¨é‡Šä¸­è¯´æ˜äº†å¦‚ä½•åˆ©ç”¨ç³»ç»Ÿå‘½ä»¤ï¼ˆä¾‹å¦‚ iptablesï¼‰æ¥æ¨¡æ‹Ÿç½‘ç»œæ•…éšœï¼Œä»è€Œé—´æ¥æ„é€  kube-controller-manager ç»­çº¦å¤±è´¥çš„æƒ…å½¢ã€‚ä½†ç”±äº kube-controller-manager çš„é€€å‡ºè¡Œä¸ºå±äºä¸šåŠ¡å®¹é”™è®¾è®¡é—®é¢˜ï¼Œè€Œéç›´æ¥çš„å®‰å…¨æ¼æ´ï¼Œå› æ­¤è¯¥è„šæœ¬ä¸åŒ…å«ç›´æ¥åˆ©ç”¨ä»£ç å¤ç°å®‰å…¨æ¼æ´çš„å†…å®¹ã€‚
3. æ€»ä½“æ¥è¯´ï¼Œç”±äºè¯¥ issue å¹¶æœªæ¶‰åŠé«˜é£é™©çš„å®‰å…¨é—®é¢˜ï¼ˆå¦‚å‘½ä»¤æ‰§è¡Œã€ææƒã€å®¹å™¨é€ƒé€¸ç­‰ï¼‰ï¼Œå› æ­¤æœ¬æ¬¡é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ï¼Œå¤ç°è„šæœ¬ä»…ä¾›ç ”ç©¶å’Œæœ¬åœ°æµ‹è¯•ï¼Œå¸®åŠ©ç†è§£å’Œæ¨¡æ‹Ÿæ½œåœ¨ç½‘ç»œæ•…éšœåœºæ™¯ï¼Œè€Œéç”¨äºéæ³•æ”»å‡»æˆ–ç”Ÿäº§ç¯å¢ƒçš„åˆ©ç”¨ã€‚

---


## Issue #129605 Long Lived TCP Connections Fail When Downscaling Kube Proxy (ExternalTrafficPolicy Cluster)

- Issue é“¾æ¥ï¼š[#129605](https://github.com/kubernetes/kubernetes/issues/129605)

### Issue å†…å®¹

#### What happened?

Some external cloud providers such as Azure use a pass-through(direct server return) load balancer. This means that TCP connections are not terminated on the loadbalancer, but instead downstream in the kubernetes cluster.

ExternalTrafficPolicy Cluster configures load balancers to send traffic to any node in the cluster, even if the node is not running a pod that the traffic is destined to. From there kube-proxy routes the request to the pod (which could either be on the same or different node). 

In our case, when client sets up a long living TCP connection and that traffic path hits kube-proxy on a node that downscales sometime during the duration of the TCP connection, we observe 520 errors from the server. 

#### What did you expect to happen?

Kubernetes nodes that act as a intermediate hop between the loadbalancer and the node/pod that the traffic is destined to should gracefully allow all existing TCP connections to finish before the node/processes are terminated. 

This is important because some loadbalancers like Azure continue to send traffic to existing TCP even though the target node has been marked unheathly.  

#### How can we reproduce it (as minimally and precisely as possible)?

1) Setup any service (e.g., nginx ingress), tolerated/tainted to node group A. Use loadbalancer with external traffic policy: cluster. 
2) Scale node pool B. 
3) Simulate many http requests, targetting loadbalancer endpoint of service created in step 1
4) Scale down node pool B.
5) Observe errors on requests that proxied through scaled down node in node pool B. 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

1.30

</details>


#### Cloud provider

<details>
Azure
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°çš„æ˜¯åœ¨Kubernetesé›†ç¾¤ä¸­ï¼Œç”±äºä½¿ç”¨ExternalTrafficPolicy Clusteré…ç½®æƒ…å†µä¸‹ï¼Œåœ¨èŠ‚ç‚¹ä¸‹çº¿æ—¶æ²¡æœ‰ä¼˜é›…åœ°ç­‰å¾…é•¿è¿æ¥ç»“æŸï¼Œå¯¼è‡´æ­£åœ¨ä½¿ç”¨é•¿è¿æ¥çš„è¯·æ±‚å‡ºç°520é”™è¯¯çš„é—®é¢˜ã€‚è¿™åæ˜ äº†åœ¨èŠ‚ç‚¹æ± ç¼©å‡ï¼ˆdownscalingï¼‰è¿‡ç¨‹ä¸­ï¼Œkube-proxyæœªèƒ½å¦¥å–„å¤„ç†ç°æœ‰TCPè¿æ¥çš„é—®é¢˜ï¼Œå±äºé›†ç¾¤å¯ç”¨æ€§å’Œæµé‡è°ƒåº¦çš„å®ç°ç¼ºé™·ã€‚è€Œä¸æ˜¯ç›´æ¥å­˜åœ¨å¯è¢«æ”»å‡»è€…åˆ©ç”¨è¿›è¡Œè¶Šæƒã€å‘½ä»¤æ‰§è¡Œæˆ–å…¶ä»–å®‰å…¨ç ´åçš„æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥é—®é¢˜ä¸ºåŠŸèƒ½æ€§æ•…éšœï¼Œæ¶‰åŠæµé‡è°ƒåº¦åŠèŠ‚ç‚¹ä¸‹çº¿æ—¶çš„è¿æ¥ç®¡ç†ï¼Œä¸å±äºå®‰å…¨é—®é¢˜ï¼Œ
# å› æ­¤æ— å®‰å…¨é£é™©å¤ç°è„šæœ¬æä¾›ã€‚
print("è¯¥é—®é¢˜ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ— éœ€å¤ç°è„šæœ¬ã€‚")
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Issueä¸»è¦åæ˜ äº†åœ¨èŠ‚ç‚¹ä¸‹çº¿è¿‡ç¨‹ä¸­ï¼Œç°æœ‰çš„é•¿è¿æ¥æ²¡æœ‰è¢«æ­£ç¡®ä¿æŒï¼Œå¯¼è‡´è¯·æ±‚ä¸­æ–­å¹¶è¿”å›520é”™è¯¯ã€‚è¿™ç§æƒ…å†µè™½ç„¶å¯èƒ½å½±å“åº”ç”¨çš„ç¨³å®šæ€§å’Œç”¨æˆ·ä½“éªŒï¼Œä½†å¹¶ä¸èƒ½ç›´æ¥è¢«æ”»å‡»è€…åˆ©ç”¨æ¥è¿›è¡Œæœªæˆæƒæ§åˆ¶ã€ææƒæˆ–è¿œç¨‹å‘½ä»¤æ‰§è¡Œç­‰å®‰å…¨æ”»å‡»ã€‚æœ¬é—®é¢˜å±äºé›†ç¾¤è°ƒåº¦ä¸èŠ‚ç‚¹ç®¡ç†ä¸­çš„å®ç°ç¼ºé™·ï¼Œè€Œéå®‰å…¨æ¼æ´ï¼Œå…¶é£é™©è¯„çº§å› æ­¤åˆ¤æ–­ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚å› æ­¤ï¼Œä¸éœ€è¦æä¾›ç”¨äºå®‰å…¨æµ‹è¯•çš„å¤ç°POCè„šæœ¬ï¼Œç›¸å…³å¤ç°è„šæœ¬ä»…é™äºè¯´æ˜è¯¥é—®é¢˜åœ¨å®é™…ç¯å¢ƒä¸­çš„è¡¨ç°ï¼Œä¸é€‚åˆä½œä¸ºå®‰å…¨æ”»å‡»å·¥å…·ä½¿ç”¨ã€‚

---


## Issue #129600 Certificate info in expire logs

- Issue é“¾æ¥ï¼š[#129600](https://github.com/kubernetes/kubernetes/issues/129600)

### Issue å†…å®¹

#### What happened?

When a certificate expires, the server logs it with something like:
`verifying certificate SN=xxxx, SKID=, AKID= failed: x509: certificate has expired or is not yet valid`

Its hard to track certs issued by k8s itself by SN. It would be extremely handy if it logged the CN too.

Also, it would be good if it would log it shortly before it expired.

#### What did you expect to happen?

K8s would tell you what CN the cert tried to use.

#### How can we reproduce it (as minimally and precisely as possible)?

Use an expired cert and try and auth.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
1.32.0
```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯åœ¨è¯ä¹¦è¿‡æœŸæ—¥å¿—ä¸­ä»…è®°å½•äº†è¯ä¹¦çš„ SN ä¿¡æ¯ï¼Œè€Œç¼ºå°‘ CN ä¿¡æ¯çš„è®°å½•ï¼Œå¹¶å»ºè®®åœ¨è¯ä¹¦å¿«è¿‡æœŸå‰ä¹Ÿè¿›è¡Œè®°å½•ã€‚æ•´ä½“æ¥çœ‹ï¼Œè¿™å±äºæ—¥å¿—è®°å½•å†…å®¹çš„æ”¹è¿›å»ºè®®ï¼Œå¹¶æ²¡æœ‰æ¶‰åŠåˆ°å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€æƒé™æå‡ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ç­‰å®‰å…¨é—®é¢˜ã€‚å› æ­¤ï¼Œè¯¥ Issue å¹¶éå®‰å…¨é—®é¢˜ï¼Œåªæ˜¯åŠŸèƒ½å¢å¼ºæˆ–æ—¥å¿—è®°å½•ä¸å…¨çš„é—®é¢˜ï¼Œä¸ä¼šå¯¹ç³»ç»Ÿæ„æˆå®‰å…¨å¨èƒã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# æ­¤é—®é¢˜æè¿°çš„æ˜¯æ—¥å¿—è®°å½•æ”¹è¿›å»ºè®®ï¼Œå¹¶ä¸æ¶‰åŠå®‰å…¨æ¼æ´çš„å¤ç°ã€‚
# å› æ­¤ï¼Œæ­¤å¤„æä¾›ä¸€ä¸ªç®€å•çš„è„šæœ¬ï¼Œç”¨äºæ¨¡æ‹Ÿæ—¥å¿—è®°å½•åœºæ™¯ï¼Œ
# å¹¶æ‰“å°å‡ºåŒ…å«è¯ä¹¦ SN ä¸ CN ä¿¡æ¯çš„æ—¥å¿—ï¼Œä»…ç”¨äºæ¼”ç¤ºè€Œéå¤ç°å®‰å…¨æ¼æ´ã€‚

import logging
import datetime

# è®¾ç½®æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger("cert_expire_logger")
logger.setLevel(logging.DEBUG)
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)

def simulate_cert_expiry_logging(serial_number, common_name):
    # æ¨¡æ‹Ÿè¯ä¹¦è¿‡æœŸï¼Œä½†æ—¥å¿—ä¸­ç¼ºå°‘CNä¿¡æ¯çš„æƒ…å†µ
    logger.error(f"verifying certificate SN={serial_number} failed: x509: certificate has expired or is not yet valid")

    # æ”¹è¿›å»ºè®®ï¼šåœ¨æ—¥å¿—ä¸­åŒæ—¶è®°å½•CNä¿¡æ¯åŠæå‰å‘Šè­¦
    # ä¾‹å¦‚ï¼Œæå‰ 1 å¤©é€šçŸ¥è¯ä¹¦å³å°†è¿‡æœŸ
    alert_time = datetime.datetime.now() + datetime.timedelta(days=1)
    logger.info(f"Alert: Certificate with SN={serial_number} and CN={common_name} will expire on {alert_time.strftime('%Y-%m-%d %H:%M:%S')}")

if __name__ == "__main__":
    # æ¨¡æ‹Ÿä½¿ç”¨ä¸€ä¸ªè¿‡æœŸè¯ä¹¦çš„æƒ…å½¢ï¼Œå¡«å…¥ç¤ºä¾‹çš„SNä¸CN
    simulate_cert_expiry_logging("xxxx", "example.com")
```


**è§£é‡Šè¯´æ˜ï¼š**

è¿™ä¸ª Python è„šæœ¬æ¨¡æ‹Ÿäº†ä¸€ä¸ªç®€å•çš„æ—¥å¿—è®°å½•åœºæ™¯ï¼Œå±•ç¤ºäº†å½“ä¸€ä¸ªè¯ä¹¦è¿‡æœŸæ—¶ï¼Œå¦‚ä½•åœ¨æ—¥å¿—ä¸­è®°å½•è¯ä¹¦çš„ SN ä¿¡æ¯ä»¥åŠï¼ˆæ ¹æ®å»ºè®®ï¼‰è®°å½•å¯¹åº”çš„ CN ä¿¡æ¯å’Œæå‰å‘Šè­¦ã€‚è„šæœ¬ä¸­ï¼š
1. è®¾ç½®äº†ä¸€ä¸ªæ—¥å¿—è®°å½•å™¨ï¼Œç”¨äºè¾“å‡ºæ—¥å¿—ä¿¡æ¯ã€‚
2. å®šä¹‰äº†å‡½æ•° simulate_cert_expiry_loggingï¼Œé¦–å…ˆè®°å½•è¯ä¹¦è¿‡æœŸçš„é”™è¯¯æ—¥å¿—ï¼ˆä»…å« SN ä¿¡æ¯ï¼‰ï¼Œç„¶åæŒ‰ç…§å»ºè®®è®°å½•åŒ…å« CN ä¿¡æ¯çš„å‘Šè­¦æ—¥å¿—ï¼Œæ¨¡æ‹Ÿæå‰ä¸€å¤©çš„è¿‡æœŸé€šçŸ¥ã€‚
3. è„šæœ¬çš„ä¸»å…¥å£è°ƒç”¨è¯¥å‡½æ•°ï¼Œä½¿ç”¨ç¤ºä¾‹çš„è¯ä¹¦ SN å’Œ CN è¿›è¡Œæ¼”ç¤ºã€‚

éœ€è¦è¯´æ˜çš„æ˜¯ï¼Œæ­¤ Issue å±äºæ—¥å¿—å¢å¼ºçš„æ”¹è¿›éœ€æ±‚ï¼Œå¹¶ä¸æ„æˆå®‰å…¨é£é™©ï¼Œå› æ­¤æ²¡æœ‰æä¾›é’ˆå¯¹å®‰å…¨é£é™©çš„å¤ç°è„šæœ¬ã€‚å¤ç°è„šæœ¬ä»…ç”¨äºå±•ç¤ºå¦‚ä½•æ”¹è¿›æ—¥å¿—è®°å½•é€»è¾‘ã€‚

---


## Issue #129580 Cannot connect Backend Service from FrontEnd Service

- Issue é“¾æ¥ï¼š[#129580](https://github.com/kubernetes/kubernetes/issues/129580)

### Issue å†…å®¹

#### What happened?

I am developing a simple project called "TodoApp," which consists of a TodoApi project built with .NET and a WebApp project using ReactJS. However, I am unable to call the API from the WebApp in a browser. Interestingly, it works when I connect to the FrontEnd pod and use the `curl http://todoapp-api-service` command, which responds with 'hello world' from the server.


#### What did you expect to happen?

I want  to call the API from the WebApp in a browser.

#### How can we reproduce it (as minimally and precisely as possible)?

I setup my project:

1. Dockerfile
TodoApi Dockerfile:
```
# Use the official .NET SDK image to build the app
FROM mcr.microsoft.com/dotnet/sdk:9.0 AS build

# Set the working directory inside the container
WORKDIR /app

# Copy the csproj file and restore the dependencies
COPY ToDoApi.csproj ./
RUN dotnet restore

# Copy the rest of the application files
COPY . ./

# Build the application
RUN dotnet publish -c Release -o out

# Use the official .NET runtime image for the runtime environment
FROM mcr.microsoft.com/dotnet/aspnet:9.0 AS base

# Set the working directory for the runtime container
WORKDIR /app

# Expose port 80 for the app to listen to
EXPOSE 80

# Copy the published files from the build image
COPY --from=build /app/out .

# Define the entry point to run the app
ENTRYPOINT ["dotnet", "ToDoApi.dll"]
```

WebApi Dockerfile:
```
# Step 1: Use an official Node.js image as a base image
FROM node:16 AS build

# Step 2: Set the working directory inside the container
WORKDIR /app

# Step 3: Copy the package.json and package-lock.json (or yarn.lock)
COPY package*.json ./

# Step 4: Install the dependencies
RUN npm install

# Step 5: Copy the rest of the app's source code into the container
COPY . .

# Step 6: Build the React app
RUN npm run build

# Step 7: Use an official Nginx image to serve the build files
FROM nginx:alpine

# Step 8: Copy the build output from the first stage into the Nginx container
COPY --from=build /app/build /usr/share/nginx/html

# Step 9: Expose the port that Nginx will run on
EXPOSE 80

# Step 10: Start Nginx server
CMD ["nginx", "-g", "daemon off;"]
```

2. Deployment file yaml
```

apiVersion: apps/v1
kind: Deployment
metadata:
  name: todoapp-api-deployment
  labels:
    app: todoapp-api-label
spec:
  replicas: 1
  selector: 
    matchLabels:
      app: todoapp-api-label
  template:
    metadata:
      labels:
        app: todoapp-api-label
    spec:
      containers:
      - name: todoapp-api-container
        image: mrstevebang/todoapp-api:1.0.8
        ports:
        - containerPort: 80
        env:
        - name: ASPNETCORE_ENVIRONMENT
          value: "Development"  # Set the environment to Development

---
apiVersion: v1
kind: Service
metadata:
  name: todoapp-api-service
spec:
  selector:
    app: todoapp-api-label
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: todoapp-webapp-deployment
  labels:
    app: todoapp-webapp-label
spec:
  replicas: 1
  selector:
    matchLabels:
      app: todoapp-webapp-label
  template:
    metadata:
      labels:
        app: todoapp-webapp-label
    spec:
      containers:
      - name: todoapp-webapp-container
        image: mrstevebang/todoapp-webapp:1.0.13
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: todoapp-webapp-service
spec:
  selector:
    app: todoapp-webapp-label
  type: LoadBalancer
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
      nodePort: 30090

```

3. My code for connect api
```
import axios from 'axios';

const API_BASE_URL ='http://todoapp-api-service';

export const getTodos = () => axios.get(`${API_BASE_URL}/api/todos`);
export const createTodo = (todo) => axios.post(`${API_BASE_URL}/api/todos`, todo);
export const markCompleted = (id) => axios.post(`${API_BASE_URL}/api/todos/${id}/completed`);
export const deleteTodo = (id) => axios.delete(`${API_BASE_URL}/api/todos/${id}`);
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.30.6
```

</details>


#### Cloud provider

<details>
Minikube
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ç»è¿‡åˆ†æï¼Œè¯¥ issue åæ˜ çš„æ˜¯å‰ç«¯æ— æ³•ä»æµè§ˆå™¨è°ƒç”¨åç«¯ API çš„é—®é¢˜ï¼Œè€Œåœ¨å‰ç«¯å®¹å™¨å†…é€šè¿‡ curl å‘½ä»¤å¯ä»¥æ­£ç¡®è®¿é—®åç«¯æœåŠ¡ã€‚æ•´ä½“æè¿°ä¸Šçœ‹ï¼Œè¿™å±äºæœåŠ¡é—´è®¿é—®æˆ–ç½‘ç»œè¿é€šæ€§é…ç½®çš„é—®é¢˜ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚é¡¹ç›®é…ç½®ã€ç½‘ç»œåŸŸåè§£æã€å®¹å™¨å†…å¤–è®¿é—®ç­‰é—®é¢˜å¯èƒ½ä¼šå¯¼è‡´è¯¥ç°è±¡ï¼Œä½†ä»æè¿°ä¸­å¹¶æ²¡æœ‰å‘ç°å‘½ä»¤æ‰§è¡Œã€å‡­æ®æ³„éœ²ã€æƒé™æå‡ã€å®¹å™¨é€ƒé€¸ç­‰å®‰å…¨é£é™©å› ç´ ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
è¯¥è„šæœ¬ç”¨äºå±•ç¤ºæ¨¡æ¿è¦æ±‚ï¼Œç”±äºè¯¥ issue ä¸å±äºå®‰å…¨æ¼æ´ï¼Œå› æ­¤ä¸å­˜åœ¨ç›¸åº”çš„å®‰å…¨æ¼æ´å¤ç°ä»£ç ã€‚
è¯¥è„šæœ¬ä»…è¾“å‡ºæç¤ºä¿¡æ¯ï¼Œè¡¨æ˜é—®é¢˜ä¸ºæœåŠ¡è¿æ¥é—®é¢˜è€Œéå®‰å…¨é£é™©ã€‚
"""

def main():
    print("æ­¤é—®é¢˜ä¸ºæœåŠ¡è¿é€šæ€§é—®é¢˜ï¼Œä¸æ¶‰åŠå®‰å…¨æ¼æ´çš„å¤ç°ã€‚")

if __name__ == "__main__":
    main()
```


**è§£é‡Šè¯´æ˜ï¼š**

æœ¬å¤ç°è„šæœ¬ä»…ä¸ºæ»¡è¶³æ¨¡æ¿è¦æ±‚çš„ç¤ºä¾‹ä»£ç ã€‚ç”±äºæœ¬ issue æè¿°çš„é—®é¢˜æ˜¯å‰åç«¯æœåŠ¡çš„è®¿é—®è¿é€šæ€§é—®é¢˜ï¼Œå¹¶ä¸æ¶‰åŠå®‰å…¨æ¼æ´ï¼Œå› æ­¤æ— éœ€æ„é€ å®‰å…¨æ¼æ´çš„å¤ç° PoCã€‚è„šæœ¬é€šè¿‡ç®€å•çš„è¾“å‡ºæç¤ºä¿¡æ¯ï¼Œå‘ŠçŸ¥ç”¨æˆ·è¯¥é—®é¢˜ä¸å…·å¤‡å®‰å…¨é£é™©ã€‚å®é™…é—®é¢˜çš„ä¿®å¤åº”ä¾§é‡äºç½‘ç»œé…ç½®ã€æœåŠ¡åŸŸåè§£æã€è·¨å®¹å™¨è®¿é—®æˆ–è€…æµè§ˆå™¨è·¨åŸŸç­‰é—®é¢˜çš„æ’æŸ¥å’Œè°ƒè¯•ã€‚

---


## Issue #129579 downloading container image from registry.k8s.io through containerd runtime doesn't work

- Issue é“¾æ¥ï¼š[#129579](https://github.com/kubernetes/kubernetes/issues/129579)

### Issue å†…å®¹

#### What happened?

I'm using VMs on Proxmox which is one of the hypervisors and tried to deploy k8s (v1.31.4) using kubespray (release-2.27) based on containerd runtime (v1.7.24).

Problem happened when containerd tried to download coredns image (precisely "registry.k8s.io/coredns/coredns:v1.11.3").

![image](https://github.com/user-attachments/assets/db41cd11-2ae8-460c-a913-d11ea120811b)

```
TASK [download : Download_container | Download image if required] ***********************
fatal: [node1]: FAILED! => {"attempts": 4, "changed": true, "cmd": ["/usr/local/bin/ctr", "-n", "k8s.io", "images", "pull", "--hosts-dir", "/etc/containerd/certs.d", "registry.k8s.io/pause:3.10"], "delta": "0:00:00.114788", "end": "2025-01-13 01:32:37.382646", "msg": "non-zero return code", "rc": 1, "start": "2025-01-13 01:32:37.267858", "stderr": "ctr: failed to resolve reference \"registry.k8s.io/pause:3.10\": unexpected status from HEAD request to https://registry.k8s.io/v2/pause/manifests/3.10: 503 Service Unavailable", "stderr_lines": ["ctr: failed to resolve reference \"registry.k8s.io/pause:3.10\": unexpected status from HEAD request to https://registry.k8s.io/v2/pause/manifests/3.10: 503 Service Unavailable"], "stdout": "registry.k8s.io/pause:3.10: resolving      |\u001b[32m\u001b[0m--------------------------------------| \nelapsed: 0.1 s              total:   0.0 B (0.0 B/s)                                         ", "stdout_lines": ["registry.k8s.io/pause:3.10: resolving      |\u001b[32m\u001b[0m--------------------------------------| ", "elapsed: 0.1 s              total:   0.0 B (0.0 B/s)                                         "]}
```

So I installed just only containerd on my VM then tried to download "registry.k8s.io/coredns/coredns:v1.11.3" image but it didn't work same.

![image](https://github.com/user-attachments/assets/75593414-61a4-4d87-844e-ea1c4ffd52c5)

`FATA[0003] failed to copy: httpReadSeeker: failed open: unexpected status code https://registry.k8s.io/v2/coredns/coredns/blobs/sha256:d858cbc252ade14879807ff8dbc3043a26bbdb92087da98cda831ee040b172b3: 503 Service Unavailable`

Containerd brings images from dockerhub very well but couldn't download image from registry.k8s.io and k8s.gcr.io.

Not just "registry.k8s.io/coredns/coredns:v1.11.3" but also "registry.k8s.io/kube-apiserver:v1.31.4" & "registry.k8s.io/kube-controller-manager:v1.31.4" neither.

But the weired thing is that these images can be downloaded using docker.
docker (v26.1.2) downloads these images well.
Also docker (v27.4.1) works fine but not smoothly. I have to enter same command multiple times forcefully, then it works finally.

![image](https://github.com/user-attachments/assets/dcc89c92-7ec9-458d-8172-fb8b44f3a396)

I'm still confused this is a matter of containerd itself or k8s registry. So if you know anything about this, please help.

#### What did you expect to happen?

I expected that containerd would get the k8s related images.

#### How can we reproduce it (as minimally and precisely as possible)?

1. prepare an Ubuntu 24.04 server
2. enter commands below to install containerd and nerdctl
`sudo apt update`
`sudo apt install -y ca-certificates curl`
`sudo install -m 0755 -d /etc/apt/keyrings`
`sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc`
`sudo chmod a+r /etc/apt/keyrings/docker.asc`
`echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
sudo tee /etc/apt/sources.list.d/docker.list > /dev/null`
`sudo apt update`
`sudo apt install -y containerd.io`
`wget https://github.com/containerd/nerdctl/releases/download/v2.0.2/nerdctl-2.0.2-linux-amd64.tar.gz`
`sudo tar Cxzvvf /usr/local/bin nerdctl-2.0.2-linux-amd64.tar.gz`
`sudo nerdctl image pull registry.k8s.io/coredns/coredns:v1.11.3`

#### Anything else we need to know?

You can reproduce this without kubespray. Just install containerd or docker on Ubuntu 24.04 then download any images from "registry.k8s.io".

#### Kubernetes version

<details>
this happens during k8s installation process
but k8s version is "1.31.4"
</details>


#### Cloud provider

<details>
none. it's on-premise
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 24.04 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo
$ uname -a
Linux node1 6.8.0-35-generic #35-Ubuntu SMP PREEMPT_DYNAMIC Mon May 20 15:51:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kubespray release-2.27 branch
ansible core 2.16.14
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd v1.7.24 with nerdctl v2.0.2
or
docker v26.1.2 or latest
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
CNI=calico v3.29.1
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue ä¸»è¦æè¿°çš„æ˜¯åœ¨ä½¿ç”¨ containerd ä½œä¸ºå®¹å™¨è¿è¡Œæ—¶ä» registry.k8s.io ä¸‹è½½ Kubernetes ç›¸å…³é•œåƒæ—¶å‡ºç°ä¸‹è½½å¤±è´¥ï¼ˆ503 Service Unavailableï¼‰çš„æƒ…å†µã€‚é—®é¢˜åŸå› å¯èƒ½ä¸ registry.k8s.io æœåŠ¡çš„è´Ÿè½½ã€ä»£ç†ã€é˜²ç«å¢™ã€ç½‘ç»œè¿é€šæ€§æˆ–è€…ä¸ containerd é€‚é…æ€§æœ‰å…³ï¼Œä»æ—¥å¿—ä¿¡æ¯æ¥çœ‹ï¼Œå¹¶æ²¡æœ‰æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€ææƒã€å®¹å™¨é€ƒé€¸ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ç­‰å®‰å…¨æ€§é—®é¢˜ã€‚å¦å¤–ï¼Œissueä¸­ä¹Ÿæœªæ˜¾ç¤ºæœ‰æ•æ„Ÿå‡­è¯æš´éœ²æˆ–ä¸å½“é…ç½®çš„æƒ…å†µï¼Œæ•´ä½“é—®é¢˜æ›´åƒæ˜¯ä¸€ç§æœåŠ¡è·å–æ•…éšœæˆ–å…¼å®¹æ€§é—®é¢˜ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
è¯¥è„šæœ¬ä»…ç”¨äºæ¨¡æ‹Ÿä» registry.k8s.io ä¸‹è½½é•œåƒçš„è¿‡ç¨‹ï¼Œ
ç”¨äºç ”ç©¶å®¹å™¨é•œåƒä¸‹è½½æ—¶å¯èƒ½é‡åˆ°çš„çŠ¶æ€ç 503é—®é¢˜ï¼Œéå®‰å…¨æ”»å‡»POCã€‚
ç”±äºè¯¥Issueæè¿°çš„åœºæ™¯ä¸æ˜¯å®‰å…¨æ¼æ´ï¼Œå› æ­¤æ­¤è„šæœ¬å¹¶ä¸ç”¨äºæ”»å‡»ï¼Œ
è€Œä»…ç”¨äºæ¨¡æ‹Ÿå®¹å™¨é•œåƒä¸‹è½½è¿‡ç¨‹ä¸­çš„ç½‘ç»œè¯·æ±‚ï¼Œä¾›æœ¬åœ°æµ‹è¯•ä½¿ç”¨ã€‚
"""

import http.server
import socketserver
import threading
import requests

# è®¾ç½®ä¸€ä¸ªç®€å•çš„HTTPæœåŠ¡å™¨ï¼Œç”¨äºæ¨¡æ‹Ÿä¸€ä¸ªè¿”å›503é”™è¯¯çš„é•œåƒä»“åº“
class SimpleHTTPRequestHandler(http.server.BaseHTTPRequestHandler):
    def do_HEAD(self):
        self.send_response(503)
        self.end_headers()

    def do_GET(self):
        self.send_response(503)
        self.end_headers()
        self.wfile.write(b"Service Unavailable")

def start_mock_server(port=8080):
    handler = SimpleHTTPRequestHandler
    httpd = socketserver.TCPServer(("", port), handler)
    print(f"Mock server å¯åŠ¨ï¼Œç›‘å¬ç«¯å£ {port}ï¼Œå§‹ç»ˆè¿”å› 503 çŠ¶æ€ç ")
    httpd.serve_forever()

def simulate_image_download(url):
    try:
        print(f"å°è¯•è¯·æ±‚ URL: {url}")
        response = requests.head(url, timeout=5)
        print(f"å“åº”çŠ¶æ€ç : {response.status_code}")
        if response.status_code == 503:
            print("æ”¶åˆ°503 Service Unavailableå“åº”ï¼Œæ¨¡æ‹Ÿä¸‹è½½å¤±è´¥ã€‚")
        else:
            print("é•œåƒä»“åº“å“åº”æ­£å¸¸ã€‚")
    except Exception as e:
        print(f"è¯·æ±‚å‘ç”Ÿå¼‚å¸¸: {e}")

if __name__ == "__main__":
    # å¯åŠ¨æ¨¡æ‹Ÿçš„HTTPæœåŠ¡å™¨çº¿ç¨‹
    server_thread = threading.Thread(target=start_mock_server, daemon=True)
    server_thread.start()

    # æ¨¡æ‹Ÿçš„é•œåƒè¯·æ±‚URLï¼ŒæŒ‡å‘æœ¬åœ°çš„mock server
    test_url = "http://localhost:8080/v2/someimage/manifests/latest"
    simulate_image_download(test_url)
```


**è§£é‡Šè¯´æ˜ï¼š**

1. æœ¬è„šæœ¬é¦–å…ˆæ­å»ºäº†ä¸€ä¸ªç®€å•çš„HTTPæœåŠ¡å™¨ï¼Œç›‘å¬åœ¨æœ¬åœ°8080ç«¯å£ï¼Œå…¶æ‰€æœ‰è¯·æ±‚éƒ½è¿”å› 503 çŠ¶æ€ç ï¼Œç”¨æ¥æ¨¡æ‹Ÿ registry.k8s.io åœ¨ä¸‹è½½é•œåƒæ—¶å‡ºç°çš„â€œService Unavailableâ€æƒ…å†µã€‚
2. å‡½æ•° simulate_image_download() é‡‡ç”¨ requests åº“å‘é€ HEAD è¯·æ±‚æ¥æ¨¡æ‹Ÿ containerd è¯·æ±‚é•œåƒä»“åº“çš„è¿‡ç¨‹ï¼Œå¹¶è¾“å‡ºå“åº”çŠ¶æ€ç åŠç›¸åº”æç¤ºã€‚
3. ç”±äºè¯¥é—®é¢˜å¹¶éå®‰å…¨æ¼æ´ï¼Œå› æ­¤è¯¥è„šæœ¬ä»…ç”¨ä½œæ¨¡æ‹Ÿå’Œç ”ç©¶é•œåƒä¸‹è½½è¿‡ç¨‹ä¸­é‡åˆ°çš„çŠ¶æ€ç é—®é¢˜ï¼Œè€Œä¸æ¶‰åŠä»»ä½•æ”»å‡»è¡Œä¸ºã€‚è„šæœ¬ä¸­çš„é€»è¾‘ä¹Ÿå¯ç”¨äºæœ¬åœ°æµ‹è¯•ç½‘ç»œè¯·æ±‚çš„å¼‚å¸¸å“åº”æƒ…å†µã€‚
4. ä½¿ç”¨æ—¶ç›´æ¥è¿è¡Œè„šæœ¬å³å¯åœ¨æ§åˆ¶å°çœ‹åˆ°å¯åŠ¨ä¿¡æ¯å’Œè¯·æ±‚è¿‡ç¨‹æ¨¡æ‹Ÿä¿¡æ¯ã€‚

---


## Issue #129575 OpenAPI handler fails on duplicated path

- Issue é“¾æ¥ï¼š[#129575](https://github.com/kubernetes/kubernetes/issues/129575)

### Issue å†…å®¹

#### What happened?

What happened?
When installing the metrics-server to implement the metrics.k8s.io API and the prometheus-adapter to implement the custom.metrics.k8s.io API, it is possible to make the OpenAPI handler fail to initialize with this error log in apiserver: https://github.com/kubernetes/kube-openapi/blob/master/pkg/aggregator/aggregator.go#L343

handler.go:159] Error in OpenAPI handler: failed to build merge specs: unable to merge: duplicated path /apis/metrics.k8s.io/v1beta1/pods
handler.go:159] Error in OpenAPI handler: failed to build merge specs: unable to merge: duplicated path /apis/metrics.k8s.io/v1beta1/namespaces/{namespace}/pods
handler.go:159] Error in OpenAPI handler: failed to build merge specs: unable to merge: duplicated path /apis/metrics.k8s.io/v1beta1/nodes
handler.go:159] Error in OpenAPI handler: failed to build merge specs: unable to merge: duplicated path /apis/metrics.k8s.io/v1beta1/nodes/{name}
handler.go:159] Error in OpenAPI handler: failed to build merge specs: unable to merge: duplicated path /apis/metrics.k8s.io/v1beta1/nodes
This will result in kubectl discovery calls to /openapi/v2 to fail with a 503:

{
  "kind": "Event",
  "apiVersion": "audit.k8s.io/v1",
  "level": "Metadata",
  "auditID": "863d36c0-3797-4f19-bc74-df571a671a00",
  "stage": "ResponseComplete",
  "requestURI": "/openapi/v2?timeout=31s",
  "verb": "get",
...
  "userAgent": "kubectl/v1.28.2 (linux/amd64) kubernetes/cd9e211",
  "responseStatus": {
    "metadata": {},
    "code": 503
  },
  "requestReceivedTimestamp": "2023-12-19T02:50:12.882602Z",
  "stageTimestamp": "2023-12-19T02:50:12.913981Z",
  "annotations": {
    "authorization.k8s.io/decision": "allow",
    "authorization.k8s.io/reason": ""
  }
}

#### What did you expect to happen?

The OpenAPI handler should not fail to initialize if both metrics.k8s.io and custom.metrics.k8s.io are serving the same path for different API groups/versions

#### How can we reproduce it (as minimally and precisely as possible)?

I can provide exact installation steps if needed, however, a high-level repro is:

install metrics-server to serve v1beta1.metrics.k8s.io
install prometheus-adapter to serve v1beta1.custom.metrics.k8s.io
ensure resourceRules is populated in the prometheus-adapter config so that resource metrics are also served by v1beta1.custom.metrics.k8s.io: https://github.com/kubernetes-sigs/prometheus-adapter/blob/7a421bb91e9ea52be4e26db22392c36c8af38bc8/deploy/manifests/custom-metrics-config-map.yaml#L73-L98
restarting apiserver will result in the OpenAPI failure and 503 responses to /openapi/v2 path

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯åœ¨å®‰è£… metrics-server å’Œ prometheus-adapter æ—¶ï¼Œç”±äºä¸¤ä¸ªç»„ä»¶éƒ½åœ¨æä¾›éƒ¨åˆ†ç›¸åŒè·¯å¾„çš„ APIï¼Œä»è€Œå¯¼è‡´ OpenAPI handler åœ¨åˆå¹¶ API è§„æ ¼æ—¶å‡ºç°é‡å¤è·¯å¾„çš„é—®é¢˜ï¼Œæœ€ç»ˆå¯¼è‡´ apiserver çš„ OpenAPI åˆå§‹åŒ–å¤±è´¥å’Œ /openapi/v2 è¯·æ±‚è¿”å› 503 é”™è¯¯ã€‚è¯¥é—®é¢˜ä¸»è¦å±äºç»„ä»¶é—´å†²çªæˆ–é…ç½®å†²çªå¼•èµ·çš„åˆå§‹åŒ–å¤±è´¥é—®é¢˜ï¼Œå¹¶ä¸ä¼šç›´æ¥å¯¼è‡´æœªæˆæƒè®¿é—®ã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸æˆ–ææƒç­‰å®‰å…¨éšæ‚£ã€‚

æ ¹æ®æè¿°ï¼Œè™½ç„¶å›  OpenAPI handler å‡ºé”™å¯¼è‡´ API å‘ç°æ¥å£æ— æ³•æ­£å¸¸å·¥ä½œï¼Œä½†è¯¥é—®é¢˜å¹¶éç”±äºè®¾è®¡ä¸Šçš„å®‰å…¨æ¼æ´æˆ–è¢«æ¶æ„åˆ©ç”¨çš„æ¼æ´å¼•å‘ï¼Œå› æ­¤ä¸åº”è¢«å½’ç±»ä¸ºå®‰å…¨é—®é¢˜ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

æœ¬ Issue ä¸­çš„é—®é¢˜æ˜¯ç”±äºå®‰è£…äº†ä¸¤ä¸ªæä¾›éƒ¨åˆ†é‡å  API æ¥å£çš„ç»„ä»¶ï¼Œå¯¼è‡´ OpenAPI å¤„ç†å™¨åœ¨åˆå¹¶ OpenAPI è§„æ ¼æ—¶å‡ºç°äº†é‡å¤è·¯å¾„é”™è¯¯ã€‚è™½ç„¶è¿™ä¼šå¯¼è‡´é›†ç¾¤ä¸­éƒ¨åˆ† API å‘ç°æ¥å£ï¼ˆä¾‹å¦‚ /openapi/v2ï¼‰è¿”å› 503 é”™è¯¯ï¼Œä½†è¯¥é—®é¢˜å±äºç»„ä»¶é—´é…ç½®å’Œå®ç°å†²çªï¼Œå¹¶éå¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚å› è€ŒæŒ‰ç…§é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥ Issue ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œä¹Ÿæ— éœ€æ„é€ æ¶æ„å¤ç°è„šæœ¬ã€‚

---


## Issue #129572 VolumeAttachment is not deleted when the CSI plugin change from requiring attach to not requiring attach

- Issue é“¾æ¥ï¼š[#129572](https://github.com/kubernetes/kubernetes/issues/129572)

### Issue å†…å®¹

#### What happened?

After [volumeAttacher.Attach](https://github.com/kubernetes/kubernetes/blob/v1.32.0/pkg/volume/util/operationexecutor/operation_generator.go#L271) executes successfully, the volumeattachment will be created. At this point, if CSI's ATTACHREQUIRED changes from true to false, it will cause [MarkVolumeAsAttached](https://github.com/kubernetes/kubernetes/blob/v1.32.0/pkg/volume/util/operationexecutor/operation_generator.go#L301) to fail.
 And the volume will be removed from the [dsw](https://github.com/kubernetes/kubernetes/blob/v1.32.0/pkg/controller/volume/attachdetach/populator/desired_state_of_world_populator.go#L166), leaving the VolumeAttachment stranded without being properly cleaned up.

#### What did you expect to happen?

volumeattachment can be cleaned up.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Install xsky-nfs-csi (other CSI drivers should be able to reproduce this) and set attachRequired to true
2. Create PVC and pod using the CSI
3. Keep querying volumeattachment using 'kubectl get volumeattachment'. As soon as the volumeattachment appears, immediately uninstall the CSI driver, then reinstall it with attachRequired set to false
4. After waiting for some time, query volumeattachment again and find that the volumeattachment still hasn't been deleted

#### Anything else we need to know?

[operation_generator.go](https://github.com/kubernetes/kubernetes/blob/v1.32.0/pkg/volume/util/operationexecutor/operation_generator.go#L301-L307)
I think we should add error handling here to mark the volume as "Uncertain", so the volume can be added to asw, allowing the next retry to successfully remove the VolumeAttachment.
I have tested that this can solve the problem. If confirmed this is an issue, I will submit a PR

#### Kubernetes version

<details>

```console
v1.31.2
```

</details>


#### Cloud provider

<details>
none
</details>


#### OS version

<details>

```console
PRETTY_NAME="Ubuntu 22.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue è®¨è®ºçš„æ˜¯å½“ CSI æ’ä»¶é…ç½®ä»éœ€è¦ attachï¼ˆattachRequired=trueï¼‰å˜æ›´ä¸ºä¸éœ€è¦ attachï¼ˆattachRequired=falseï¼‰æ—¶ï¼Œå·²åˆ›å»ºçš„ VolumeAttachment æ— æ³•è¢«æ­£ç¡®æ¸…ç†çš„é—®é¢˜ã€‚é—®é¢˜è¡¨ç°ä¸ºåœ¨æˆåŠŸè°ƒç”¨ volumeAttacher.Attach åï¼ŒVolumeAttachment è¢«åˆ›å»ºï¼›ä¸€æ—¦é…ç½®å˜æ›´ï¼Œä¼šå¯¼è‡´ MarkVolumeAsAttached è°ƒç”¨å¤±è´¥ï¼Œä»è€Œä½¿å¾— VolumeAttachment â€œæ»ç•™â€ã€‚è¿™å±äºèµ„æºçŠ¶æ€ç®¡ç†å’Œé€»è¾‘é”™è¯¯çš„é—®é¢˜ï¼Œä¸æ¶‰åŠå¤–éƒ¨æ”»å‡»è€…åˆ©ç”¨æ¼æ´è·å¾—æ¶æ„è¡Œä¸ºï¼Œå¦‚å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰ã€‚å› æ­¤ï¼Œä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¯¥é—®é¢˜å¹¶éå®‰å…¨æ¼æ´ï¼Œè€Œæ˜¯ç³»ç»Ÿè®¾è®¡æˆ–é€»è¾‘ç¼ºé™·ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
æœ¬è„šæœ¬ä»…ç”¨äºæ¨¡æ‹Ÿé—®é¢˜å¤ç°è¿‡ç¨‹ä¸­çš„éƒ¨åˆ†é€»è¾‘æµç¨‹ï¼Œä»¥å¸®åŠ©ç†è§£ CSI æ’ä»¶é…ç½®å˜æ›´
æ—¶ VolumeAttachment æœªè¢«æ¸…ç†çš„æƒ…å†µã€‚è¯¥è„šæœ¬æ¨¡æ‹Ÿäº†åˆ›å»º VolumeAttachmentã€é…ç½®å˜æ›´
ä»¥åŠå°è¯•æ¸…ç†çš„é€»è¾‘ï¼Œä½†å¹¶ä¸ç›´æ¥å¯¹ Kubernetes ç¯å¢ƒé€ æˆå½±å“ï¼Œä»…ç”¨äºæœ¬åœ°è°ƒè¯•å’Œç ”ç©¶ã€‚
"""

import time
import threading
from http.server import BaseHTTPRequestHandler, HTTPServer

# æ¨¡æ‹Ÿçš„ VolumeAttachment ç±»
class VolumeAttachment:
    def __init__(self, vol_id):
        self.vol_id = vol_id
        self.attached = True  # åˆå§‹è®¾å®šä¸ºå·²é™„ç€

    def mark_as_attached(self):
        # æ¨¡æ‹Ÿæœ‰ attachRequired ä¸º True æ—¶çš„é€»è¾‘
        if csi_config['attachRequired']:
            self.attached = True
            print(f"Volume {self.vol_id} marked as attached.")
            return True
        else:
            # å½“ attachRequired ä¸º False æ—¶ï¼Œæ¨¡æ‹Ÿè°ƒç”¨å¤±è´¥è¡Œä¸º
            print(f"Failed to mark volume {self.vol_id} as attached due to configuration change.")
            return False

    def cleanup(self):
        # å½“çŠ¶æ€ä¸º False æˆ–ä¸ç¡®å®šæ—¶æ‰èƒ½æ¸…ç†
        if not self.attached:
            print(f"Cleaning up volume attachment for volume {self.vol_id}.")
            return True
        else:
            print(f"Cannot cleanup volume {self.vol_id} because it is still marked as attached.")
            return False

# å…¨å±€æ¨¡æ‹Ÿ CSI é…ç½®
csi_config = {'attachRequired': True}
volume_attachments = {}

def simulate_volume_attachment_process(vol_id):
    # æ¨¡æ‹Ÿ attach æ“ä½œæˆåŠŸååˆ›å»º VolumeAttachment å®ä¾‹
    va = VolumeAttachment(vol_id)
    volume_attachments[vol_id] = va
    print(f"VolumeAttachment for volume {vol_id} created.")
    # æ¨¡æ‹Ÿè°ƒç”¨ mark_as_attached
    va.mark_as_attached()

def simulate_configuration_change():
    # æ¨¡æ‹Ÿ CSI æ’ä»¶é…ç½®ä» attachRequired=True æ”¹ä¸º False
    print("Changing CSI configuration: attachRequired from True to False")
    csi_config['attachRequired'] = False

def simulate_cleanup_detection():
    # æ¨¡æ‹Ÿå®šæ—¶æ£€æµ‹å¹¶å°è¯•æ¸…ç† orphaned VolumeAttachment
    for vol_id, va in list(volume_attachments.items()):
        if not va.mark_as_attached():
            # å½“æ ‡è®°å¤±è´¥æ—¶å°è¯•æ¸…ç†å¤„ç†
            cleaned = va.cleanup()
            if cleaned:
                del volume_attachments[vol_id]
    print("Current volume attachments:", list(volume_attachments.keys()))

# ç®€å•çš„ HTTPæœåŠ¡å™¨ï¼Œç”¨äºç›‘å¬åœ¨ 8080 ç«¯å£ï¼ˆç¬¦åˆè¦æ±‚ï¼‰
class RequestHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.end_headers()
        output = f"Current configuration: {csi_config}\nVolumeAttachments: {list(volume_attachments.keys())}\n"
        self.wfile.write(output.encode('utf-8'))

def run_http_server():
    server_address = ('', 8080)
    httpd = HTTPServer(server_address, RequestHandler)
    print("Starting HTTP server on port 8080...")
    httpd.serve_forever()

def main():
    # å¯åŠ¨ HTTP æœåŠ¡
    server_thread = threading.Thread(target=run_http_server, daemon=True)
    server_thread.start()

    vol_id = "vol-1234"
    # ç¬¬ä¸€æ­¥ï¼šåˆ›å»º VolumeAttachment
    simulate_volume_attachment_process(vol_id)
    
    # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ï¼Œç¡®ä¿ VolumeAttachment è¢«åˆ›å»ºåè¿›è¡Œé…ç½®å˜æ›´
    time.sleep(1)
    
    # ç¬¬äºŒæ­¥ï¼šæ¨¡æ‹Ÿ CSI é©±åŠ¨å¸è½½åé‡æ–°å®‰è£…ï¼Œå¹¶è®¾ç½® attachRequired ä¸º False
    simulate_configuration_change()
    
    # ç­‰å¾…ä¸€å®šæ—¶é—´åè¯•å›¾æ£€æµ‹åŠæ¸…ç† orphaned VolumeAttachment
    time.sleep(1)
    simulate_cleanup_detection()
    
    # ä¿æŒè„šæœ¬è¿è¡Œä¸€æ®µæ—¶é—´ä»¥ä¾¿è§‚å¯Ÿ HTTP æœåŠ¡æ•ˆæœ
    print("è„šæœ¬è¿è¡Œå®Œæˆï¼ŒHTTP æœåŠ¡å™¨ä»åœ¨ç›‘å¬ 8080 ç«¯å£ã€‚")
    while True:
        time.sleep(5)

if __name__ == "__main__":
    main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. æ­¤è„šæœ¬ä¸»è¦æ¨¡æ‹Ÿäº† Issue ä¸­æè¿°çš„é€»è¾‘æµç¨‹ï¼šé¦–å…ˆåˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„ VolumeAttachment å¹¶æ ‡è®°ä¸ºâ€œå·²é™„ç€â€ï¼›ç„¶åé€šè¿‡ä¿®æ”¹å…¨å±€é…ç½®ï¼ˆcsi_configï¼‰å°† attachRequired è®¾ä¸º Falseï¼Œä»è€Œä½¿åç»­æ ‡è®°æ“ä½œå¤±è´¥ï¼›æœ€åå°è¯•å¯¹â€œæ»ç•™â€çš„ VolumeAttachment è¿›è¡Œæ¸…ç†ã€‚
2. è„šæœ¬åŒæ—¶åˆ›å»ºäº†ä¸€ä¸ªç®€å•çš„ HTTP æœåŠ¡å™¨ï¼Œç›‘å¬åœ¨ 8080 ç«¯å£ï¼Œè¿”å›å½“å‰çš„ CSI é…ç½®å’Œ VolumeAttachment åˆ—è¡¨ï¼Œä»¥ä¾¿é€šè¿‡æµè§ˆå™¨æˆ–å‘½ä»¤è¡Œå·¥å…·è§‚å¯Ÿç³»ç»ŸçŠ¶æ€ï¼Œç¬¦åˆé¢˜ç›®å¯¹äº HTTP æœåŠ¡å™¨çš„è¦æ±‚ã€‚
3. æœ¬è„šæœ¬ä»…ç”¨äºç†è§£é—®é¢˜å¤ç°çš„æ€è·¯ï¼Œå¹¶ä¸å¯¹çœŸå®ç”Ÿäº§ç¯å¢ƒäº§ç”Ÿå½±å“ï¼Œä¹Ÿæ²¡æœ‰æ¶‰åŠä»»ä½•æ•æ„Ÿå‡­è¯è¯»å–çš„æ“ä½œï¼Œç¡®ä¿åœ¨æœ¬åœ°å®‰å…¨è¿›è¡Œè°ƒè¯•å’Œç ”ç©¶ã€‚
4. è¯¥æ¨¡æ‹Ÿè„šæœ¬ä»…ç”¨äºå±•ç¤ºé”™è¯¯å¤„ç†é€»è¾‘ï¼Œå®é™…ä¸­åº”æ ¹æ® Kubernetes ç­‰ç³»ç»Ÿçš„å…·ä½“å®ç°è¿›è¡Œæ›´å¤æ‚çš„è°ƒè¯•ã€‚

---


## Issue #129569 Virtual Service Description Rendering of Maps

- Issue é“¾æ¥ï¼š[#129569](https://github.com/kubernetes/kubernetes/issues/129569)

### Issue å†…å®¹

#### What happened?

When executing the following command:  **kubectl describe vs my-vs-virtualservice** ,
the istio http header set is not rendered correctly.  The virtual service functions as expected and get the virtual service in yaml format also works as expected.
The issue is only with the describe command. Here's how the out looks like:
...
API Version:  networking.istio.io/v1beta1
Kind:         VirtualService
...
Metadata:
  Creation Timestamp:  2025-01-02T23:01:50Z
  Generation:          1
Spec:
  Http:
    Headers:
      Request:
        Remove:
          x-my-private-header
        Set:
          **X - My - Public - Header:  aValue**

The Remove header which is a list is rendered properly but the Set which is a map is not rendered correctly.

And if I execute the following command: **kubectl get vs my-vs-virtualservice -o yaml**, the output looks like this:

  http:
  - headers:
      request:
        remove:
        - x-my-private-header
        set:
          x-my-public-header: "aValue"

#### What did you expect to happen?

Expected output:
...
API Version:  networking.istio.io/v1beta1
Kind:         VirtualService
...
Metadata:
  Creation Timestamp:  2025-01-02T23:01:50Z
  Generation:          1
Spec:
  Http:
    Headers:
      Request:
        Remove:
          x-my-private-header
        Set:
          **x-my-public-header:  aValue**

#### How can we reproduce it (as minimally and precisely as possible)?

Create a basic istio virtual service which sets some headers, deploy it and do a describe of the virtual service.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
$ kubectl version
Client Version: v1.31.3
Kustomize Version: v5.4.2
Server Version: v1.29.1
```

</details>


#### Cloud provider

<details>
Tested on a private cloud environment and on Azure.
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯ä½¿ç”¨ kubectl describe å‘½ä»¤æŸ¥çœ‹ Istio VirtualService æ—¶ï¼Œå¯¹äº headers.set é…ç½®çš„ map æ•°æ®æ²¡æœ‰æ­£ç¡®æ¸²æŸ“çš„é—®é¢˜ã€‚è¯¥é—®é¢˜ä»…å½±å“å‘½ä»¤è¡Œè¾“å‡ºçš„æ˜¾ç¤ºæ•ˆæœï¼Œè€Œå¹¶ä¸å½±å“å®é™…çš„ VirtualService åŠŸèƒ½ï¼Œä¸”èƒ½é€šè¿‡ kubectl get å‘½ä»¤ä»¥æ­£ç¡®çš„ YAML æ ¼å¼è·å–é…ç½®ä¿¡æ¯ã€‚ä»æè¿°æ¥çœ‹ï¼Œè¿™å±äºç•Œé¢å±•ç¤ºæˆ–æ•°æ®æ ¼å¼è½¬æ¢æ–¹é¢çš„é—®é¢˜ï¼Œå¹¶ä¸å…·æœ‰å®‰å…¨æ€§ï¼Œå¦‚æœªæš´éœ²æ•æ„Ÿä¿¡æ¯ã€æœªå¼•èµ·å‘½ä»¤æ‰§è¡Œã€æ‹’ç»æœåŠ¡ã€å®¹å™¨é€ƒé€¸æˆ–ææƒç­‰å½±å“ã€‚å› æ­¤ï¼Œè¯¥ Issue ä¸æ¶‰åŠå®‰å…¨é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
è¯¥è„šæœ¬ç”¨äºæ¨¡æ‹Ÿå¤ç° Istio VirtualService æè¿°æ˜¾ç¤ºå¼‚å¸¸çš„é—®é¢˜ï¼Œ
ä½†ç”±äºè¯¥é—®é¢˜ä»…ä¸ºæè¿°è¾“å‡ºçš„æ ¼å¼é”™è¯¯ï¼Œå¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œ
å› æ­¤è„šæœ¬ä»…åšä¸ºæ¼”ç¤ºç”¨é€”ï¼Œä¸ä¼šå¯¹ç³»ç»Ÿæˆ–é›†ç¾¤äº§ç”Ÿå®é™…å½±å“ã€‚

è¯·ç¡®ä¿åœ¨é»˜è®¤ä½ç½®å­˜åœ¨ kubeconfig æ–‡ä»¶ï¼Œå¹¶ä¸”å·²æ­£ç¡®é…ç½®è®¿é—® k8s é›†ç¾¤æƒé™ã€‚
"""

import subprocess
import sys

def get_virtual_service():
    try:
        # è°ƒç”¨ kubectl get å‘½ä»¤ä»¥ YAML æ ¼å¼è·å– VirtualService
        get_cmd = ["kubectl", "get", "vs", "my-vs-virtualservice", "-o", "yaml"]
        yaml_output = subprocess.check_output(get_cmd, universal_newlines=True)
        print("kubectl get vs è¾“å‡º:\n")
        print(yaml_output)

        # è°ƒç”¨ kubectl describe å‘½ä»¤æŸ¥çœ‹æè¿°ä¿¡æ¯
        describe_cmd = ["kubectl", "describe", "vs", "my-vs-virtualservice"]
        describe_output = subprocess.check_output(describe_cmd, universal_newlines=True)
        print("\nkubectl describe vs è¾“å‡º:\n")
        print(describe_output)
    except subprocess.CalledProcessError as e:
        sys.exit(f"æ‰§è¡Œå‘½ä»¤å¤±è´¥: {e}")

if __name__ == '__main__':
    print("å¼€å§‹æ¨¡æ‹Ÿå¤ç° Istio VirtualService æè¿°è¾“å‡ºçš„é—®é¢˜...")
    get_virtual_service()
    print("\nè¯·æ ¸å¯¹ 'set' éƒ¨åˆ†çš„è¾“å‡ºæ˜¯å¦ä¸é¢„æœŸä¸€è‡´ï¼ˆåº”ä¸º map æ ¼å¼ï¼‰")
```


**è§£é‡Šè¯´æ˜ï¼š**

1. è„šæœ¬é¦–å…ˆè°ƒç”¨äº† kubectl get å‘½ä»¤ï¼Œä»¥ YAML æ ¼å¼è¾“å‡º VirtualService çš„å®Œæ•´é…ç½®ï¼Œç¡®ä¿æ•°æ®æœ¬èº«æ˜¯æ­£ç¡®çš„ã€‚
2. éšåè°ƒç”¨ kubectl describe å‘½ä»¤æŸ¥çœ‹ VirtualService æè¿°ä¿¡æ¯ï¼Œå¤ç°è¯¥ Issue ä¸­æè¿°çš„æ¸²æŸ“å¼‚å¸¸é—®é¢˜ã€‚
3. è„šæœ¬å¯¹è¾“å‡ºç»“æœè¿›è¡Œæ‰“å°ï¼Œä¾¿äºç ”ç©¶è€…å¯¹æ¯”â€œgetâ€å’Œâ€œdescribeâ€å‘½ä»¤çš„è¾“å‡ºå·®å¼‚ã€‚
4. è¯·æ³¨æ„ï¼šè¯¥é—®é¢˜åªæ˜¯å±•ç¤ºä¿¡æ¯æ ¼å¼å¼‚å¸¸ï¼Œä¸å­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ï¼Œå› æ­¤å¤ç°è„šæœ¬ä»…ç”¨äºå±•ç¤ºé—®é¢˜çŠ¶æ€ï¼Œä¸ä¼šé€ æˆå®‰å…¨æ¼æ´è¢«åˆ©ç”¨çš„é—®é¢˜ã€‚

---


## Issue #129552 Container remains ready causing Pod and EndpointSlice to Report False Ready State for the entire terminationGracePeriod

- Issue é“¾æ¥ï¼š[#129552](https://github.com/kubernetes/kubernetes/issues/129552)

### Issue å†…å®¹

#### What happened?

containerStatuses.ready is not set to false immediately when a pod it deleted. This is results in pod reporting ready status when the pod is deleted after terminationGracePeriodSeconds have passed. 

#### What did you expect to happen?

After the pod termination starts, the containerStatuses should set ready: false for all the containers in the pod and hence pod status should report as ready: false. 

#### How can we reproduce it (as minimally and precisely as possible)?

```
apiVersion: v1
kind: Pod
metadata:
  name: "b382559131"
  namespace: "default"
spec:
  terminationGracePeriodSeconds: 60
  restartPolicy: Never
  containers:
  - name: c1
    image: "k8s.gcr.io/busybox"
    command: ["/bin/sh", "-c"]
    args:
      - |
        _term() {
          rm -f /tmp/ready
        }
        trap _term SIGTERM

        touch /tmp/ready

        while true; do
          echo 'helloc1'
          ls /tmp/die_now && echo 'dying in 5s...' && sleep 5 && exit 0
          sleep 1
        done
    readinessProbe:
      exec:
        command:
        - sh
        - -c
        - |
          if [ -f "/tmp/ready" ]; then
            exit 0
          else
            touch /tmp/die_now
            exit 1
          fi
  - name: c2
    image: "k8s.gcr.io/busybox"
    command:
      - sh
      - -c
      - "_term() { while true; do echo \"hello_term_c2\"; sleep 1; done } ; trap _term SIGTERM; while true; do echo \"helloc2\"; sleep 1; done"
    
```

Run the above pod and issue a delete call when the containers become ready

$ kubectl delete pod b382559131

Monitor the readiness status:

$ while true; do date && kubectl get pod b382559131 -o json | jq .status.containerStatuses.[].ready && sleep 1; done
Sat Dec 21 11:05:16 PM PST 2024
true
true
Sat Dec 21 11:05:17 PM PST 2024
true
true

< snip > 

Sat Dec 21 11:06:21 PM PST 2024
true
true
Sat Dec 21 11:06:22 PM PST 2024
Error from server (NotFound): pods "b382559131" not found
Sat Dec 21 11:06:24 PM PST 2024
Error from server (NotFound): pods "b382559131" not found

The pod has a termination grace period of 60s. The delete was issued at 11:05:21 and we see that the ready status was true until the pod was deleted at 11:06:22 (60s later). 

The pod reports ready status until the last container exits. 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

v1.30.6

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯ Kubernetes åœ¨ Pod åˆ é™¤è¿‡ç¨‹ä¸­ï¼Œå®¹å™¨çš„ readiness çŠ¶æ€æœªèƒ½åŠæ—¶æ›´æ–°ï¼Œå¯¼è‡´åœ¨ terminationGracePeriod å†… Pod å’Œå¯¹åº”çš„ EndpointSlice ä¾ç„¶æ˜¾ç¤ºä¸º ready çŠ¶æ€çš„é—®é¢˜ã€‚ä»æè¿°ä¸Šçœ‹ï¼Œè¿™ä¸ªé—®é¢˜ä¸º Kubernetes çš„çŠ¶æ€æ›´æ–°æ—¶æœºé—®é¢˜ï¼Œå±äºå®¹å™¨ç”Ÿå‘½å‘¨æœŸç®¡ç†çš„ Bugï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚å®ƒä¸ä¼šå¯¼è‡´å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒã€æ‹’ç»æœåŠ¡æ”»å‡»ç­‰å®‰å…¨é£é™©ï¼Œä¹Ÿä¸å­˜åœ¨æ•æ„Ÿä¿¡æ¯çš„æ³„éœ²æˆ–æ»¥ç”¨é—®é¢˜ã€‚å› æ­¤ï¼Œä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¯¥é—®é¢˜å¹¶ä¸æ„æˆæ˜æ˜¾çš„å®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
æ³¨æ„ï¼šè¯¥é—®é¢˜ä¸ºKubernetes Podç”Ÿå‘½å‘¨æœŸç®¡ç†ä¸­çš„bugï¼Œä¸æ„æˆå®‰å…¨é£é™©ã€‚
å› æ­¤æ­¤å¤ç°è„šæœ¬ä»…ç”¨äºåœ¨æœ¬åœ°Kubernetesé›†ç¾¤ä¸­æ¨¡æ‹Ÿè¯¥ç°è±¡ï¼ŒéªŒè¯podåœ¨åˆ é™¤è¿‡ç¨‹ä¸­çš„readyçŠ¶æ€é—®é¢˜ã€‚
è¯¥è„šæœ¬ä¸»è¦åŸºäºkubectlå‘½ä»¤è¡Œå·¥å…·è°ƒç”¨ï¼Œç¡®ä¿åœ¨ç¯å¢ƒä¸­å·²æœ‰kubectlåœ¨é»˜è®¤kubeconfigä¸‹æ­£ç¡®é…ç½®ã€‚
"""

import subprocess
import time
import json

POD_NAME = "b382559131"
NAMESPACE = "default"

def run_cmd(cmd):
    try:
        result = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        return result.stdout
    except subprocess.CalledProcessError as e:
        print(f"Error executing '{cmd}': {e.stderr.strip()}")
        return None

def create_pod():
    pod_manifest = f'''
apiVersion: v1
kind: Pod
metadata:
  name: "{POD_NAME}"
  namespace: "{NAMESPACE}"
spec:
  terminationGracePeriodSeconds: 60
  restartPolicy: Never
  containers:
  - name: c1
    image: "k8s.gcr.io/busybox"
    command: ["/bin/sh", "-c"]
    args:
      - |
        _term() {{
          rm -f /tmp/ready
        }}
        trap _term SIGTERM

        touch /tmp/ready

        while true; do
          echo 'helloc1'
          ls /tmp/die_now && echo 'dying in 5s...' && sleep 5 && exit 0
          sleep 1
        done
    readinessProbe:
      exec:
        command:
        - sh
        - -c
        - |
          if [ -f "/tmp/ready" ]; then
            exit 0
          else
            touch /tmp/die_now
            exit 1
          fi
  - name: c2
    image: "k8s.gcr.io/busybox"
    command:
      - sh
      - -c
      - |
         _term() {{
           while true; do echo "hello_term_c2"; sleep 1; done
         }}
         trap _term SIGTERM;
         while true; do echo "helloc2"; sleep 1; done
'''
    with open("pod.yaml", "w") as f:
        f.write(pod_manifest)
    print("åˆ›å»ºPodé…ç½®æ–‡ä»¶ pod.yaml å®Œæˆ.")
    out = run_cmd("kubectl apply -f pod.yaml")
    if out:
        print(out)

def monitor_pod_readiness():
    print("å¼€å§‹ç›‘æ§Podä¸­çš„containerStatuses.readyçŠ¶æ€ (æ¯1ç§’æ£€æµ‹ä¸€æ¬¡)ï¼š")
    while True:
        time.sleep(1)
        out = run_cmd(f"kubectl get pod {POD_NAME} -n {NAMESPACE} -o json")
        if out is None:
            print("Podå·²åˆ é™¤æˆ–è·å–å¤±è´¥ã€‚")
            break
        pod_info = json.loads(out)
        status = pod_info.get("status", {})
        container_statuses = status.get("containerStatuses", [])
        if not container_statuses:
            print("æ²¡æœ‰containerStatusesä¿¡æ¯ã€‚")
            continue
        ready_status_list = [container.get("ready", False) for container in container_statuses]
        print(f"{time.strftime('%Y-%m-%d %H:%M:%S')} å®¹å™¨readyçŠ¶æ€: {ready_status_list}")

def delete_pod():
    print("åˆ é™¤Podä¸­...")
    out = run_cmd(f"kubectl delete pod {POD_NAME} -n {NAMESPACE}")
    if out:
        print(out)

def main():
    create_pod()
    # ç­‰å¾…Podè¿›å…¥RunningçŠ¶æ€
    print("ç­‰å¾…Podè¿›å…¥RunningçŠ¶æ€...")
    time.sleep(10)
    print("ç›‘æ§å‰10ç§’å†…çš„readyçŠ¶æ€ï¼š")
    for _ in range(10):
        monitor_pod_readiness()
    print("å‘èµ·åˆ é™¤Podè¯·æ±‚ã€‚")
    delete_pod()
    # ç»§ç»­ç›‘æ§åˆ é™¤è¿‡ç¨‹ä¸­çš„readyçŠ¶æ€ï¼Œç›´è‡³Podä¸å­˜åœ¨
    monitor_pod_readiness()

if __name__ == "__main__":
    main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. è¯¥è„šæœ¬é¦–å…ˆä½¿ç”¨ Python ç”Ÿæˆä¸€ä¸ªåŒ…å«é—®é¢˜ä¸­æè¿°ä¸¤ä¸ªå®¹å™¨çš„ Pod å®šä¹‰ YAML æ–‡ä»¶ï¼ˆpod.yamlï¼‰ï¼Œå¹¶åˆ©ç”¨ kubectl apply å‘½ä»¤åœ¨é»˜è®¤å‘½åç©ºé—´ä¸­åˆ›å»ºè¯¥ Podã€‚  
2. create_pod() å‡½æ•°è´Ÿè´£ç”Ÿæˆå¹¶éƒ¨ç½² Podã€‚  
3. monitor_pod_readiness() å‡½æ•°æ¯éš”1ç§’è°ƒç”¨ä¸€æ¬¡ kubectl API è·å–è¯¥ Pod çš„çŠ¶æ€ï¼Œå¹¶æ‰“å°æ¯ä¸ªå®¹å™¨çš„ ready æ ‡å¿—ã€‚  
4. è„šæœ¬ç­‰å¾…ä¸€å®šæ—¶é—´åè°ƒç”¨ delete_pod() å‡½æ•°å‘èµ· Pod çš„åˆ é™¤æ“ä½œï¼Œå¹¶æŒç»­ç›‘æ§ Pod çŠ¶æ€ï¼Œç›´åˆ° Pod è¢«åˆ é™¤ã€‚  
5. æ•´ä½“è„šæœ¬ç”¨äºæ¼”ç¤ºè¯¥é—®é¢˜ï¼Œå³åœ¨ Pod è¢«åˆ é™¤çš„è¿‡ç¨‹ä¸­ï¼ˆå…·æœ‰60sçš„terminationGracePeriodï¼‰ï¼Œå®¹å™¨çš„ ready çŠ¶æ€ä»ç„¶ä¿æŒä¸º Trueï¼Œç›´è‡³æœ€åå®¹å™¨é€€å‡ºã€‚

ç”±äºè¯¥é—®é¢˜ä»…ä¸ºçŠ¶æ€æ›´æ–°çš„é—®é¢˜ï¼Œä¸”ä¸æ¶‰åŠå®‰å…¨æ¼æ´ï¼Œå› æ­¤é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚è¯¥è„šæœ¬ä»…ç”¨äºç ”ç©¶å’Œæœ¬åœ°æµ‹è¯•å¤ç°è¯¥ç°è±¡ï¼Œä¸å­˜åœ¨å®‰å…¨åˆ©ç”¨é£é™©ã€‚

---


## Issue #129531 [FG:InPlacePodVerticalScaling] avoid checking the configuration of resource managers to learn their expected behavior

- Issue é“¾æ¥ï¼š[#129531](https://github.com/kubernetes/kubernetes/issues/129531)

### Issue å†…å®¹

#### What happened?

The VPA logic needs to know if resource managers (cpumanager, memory manager) can allocate exclusively resources.
To do so, it peeks in their configuration and second-guesses their expected behavior.
This is unfortunate and due to the lack of resource manager API which can report the same information

As result of this approach, there's an actual bug in the current VPA logic: this check is wrong https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/kubelet.go#L2856 because the name of the policy for cpumanager is `static` (lowercase "s") while for memory manager is `Static` (uppercase "s")

#### What did you expect to happen?

VPA should be able to learn about resource manager behavior in a safe and supported way

#### How can we reproduce it (as minimally and precisely as possible)?

configure memory manager with `Static` policy and enable VPA. the `canResizePod` logic will behave incorrectly

#### Anything else we need to know?

behavior introduced in 2d8939c4aef8f060d413bb27272ba38cd7171fbe

a new API to expose this information is proposed in https://github.com/kubernetes/kubernetes/pull/128728

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¿™ä¸ª Issue ä¸»è¦æè¿°äº† VPAï¼ˆVertical Pod Autoscalerï¼‰åœ¨åˆ¤æ–­èµ„æºç®¡ç†å™¨ï¼ˆå¦‚ cpumanagerã€memory managerï¼‰é…ç½®æ—¶å­˜åœ¨é€»è¾‘é”™è¯¯çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œç”±äº VPA é€šè¿‡æ£€æŸ¥é…ç½®æ¥æ¨æ–­èµ„æºç®¡ç†å™¨çš„è¡Œä¸ºï¼Œä½†å› å¤§å°å†™ä¸ä¸€è‡´ï¼ˆcpumanager ä½¿ç”¨ "static"ï¼Œè€Œ memory manager ä½¿ç”¨ "Static"ï¼‰å¯¼è‡´åˆ¤æ–­é”™è¯¯ã€‚è¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ bugï¼Œä½“ç°ä¸ºé”™è¯¯çš„é€»è¾‘åˆ¤æ–­ï¼Œè€Œå¹¶æ²¡æœ‰æ¶‰åŠæœªç»æˆæƒçš„è®¿é—®ã€ä»£ç æ‰§è¡Œã€ææƒæˆ–ä»»ä½•å¯èƒ½å¯¼è‡´å®‰å…¨æ¼æ´çš„è¡Œä¸ºï¼Œä¹Ÿæ²¡æœ‰å¼•å…¥æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€æ•°æ®æ³„æ¼æˆ–å‘½ä»¤æ‰§è¡Œé£é™©ã€‚

æ ¹æ®æä¾›çš„æè¿°ï¼Œè¯¥é—®é¢˜æ˜¯ç”±äºå†…éƒ¨é…ç½®é€»è¾‘å®ç°ä¸å½“é€ æˆçš„ï¼Œä¸”ä¸ä¼šè¢«æ”»å‡»è€…åˆ©ç”¨ä»¥è·å–æ›´é«˜æƒé™æˆ–è€…ç ´åç³»ç»Ÿå®‰å…¨ã€‚å› æ­¤å°±æ•´ä½“å®‰å…¨æ€§è€Œè¨€ï¼Œå¹¶ä¸å­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
# æœ¬è„šæœ¬ä»…ç”¨äºç ”ç©¶åŠæœ¬åœ°æµ‹è¯•ç›®çš„ï¼Œç”¨äºæ¨¡æ‹Ÿ VPA é…ç½®æ£€æŸ¥æ—¶å¤§å°å†™ä¸ä¸€è‡´å¯¼è‡´çš„é€»è¾‘é”™è¯¯å¤ç°åœºæ™¯ï¼Œ
# ä½†è¯¥é—®é¢˜æœ¬èº«å¹¶ä¸æ„æˆå®‰å…¨æ¼æ´ï¼Œæ•…å¤ç°è„šæœ¬ä»…ç”¨äºåŠŸèƒ½éªŒè¯ï¼Œéç”¨äºæ”»å‡»ã€‚

def simulate_vpa_behavior(cpu_manager_policy, memory_manager_policy):
    """
    æ¨¡æ‹Ÿ VPA åˆ¤æ–­èµ„æºç®¡ç†å™¨æ˜¯å¦æ”¯æŒç‹¬å èµ„æºåˆ†é…çš„é€»è¾‘ã€‚
    ç”±äº VPA é€»è¾‘é”™è¯¯ï¼Œå¯¼è‡´å¯¹äº memory manager çš„åˆ¤æ–­å­˜åœ¨å¤§å°å†™é—®é¢˜ã€‚
    """
    # æŒ‰ç…§å®é™…æºç é€»è¾‘ï¼Œå‡è®¾å¯¹ cpu_manager_policy é‡‡ç”¨æ­£ç¡®æ¯”è¾ƒï¼Œè€Œ memory_manager_policy æ¯”è¾ƒæ—¶å› å¤§å°å†™ä¸ä¸€è‡´å‡ºé”™
    expected_cpu_policy = "static"
    expected_memory_policy = "static"  # æ­£ç¡®æƒ…å†µä¸‹åº”è¯¥ä¸€è‡´ï¼Œä½†æ˜¯å®é™…é…ç½®ä¸­ memory manager å¯èƒ½ç”¨ 'Static'
    
    cpu_check = (cpu_manager_policy == expected_cpu_policy)
    memory_check = (memory_manager_policy == expected_memory_policy)
    
    print("CPU Manager åˆ¤æ–­ç»“æœ:", cpu_check)
    print("Memory Manager åˆ¤æ–­ç»“æœ:", memory_check)

def main():
    # æ¨¡æ‹Ÿé…ç½®åœºæ™¯
    # cpu_manager ä½¿ç”¨å…¨å°å†™ 'static'
    cpu_manager_policy = "static"
    # memory manager ä½¿ç”¨é¦–å­—æ¯å¤§å†™ 'Static'
    memory_manager_policy = "Static"
    
    print("æ¨¡æ‹Ÿå¼€å§‹ï¼š")
    simulate_vpa_behavior(cpu_manager_policy, memory_manager_policy)
    
    # æ ¹æ®åˆ¤æ–­ï¼ŒVPA é”™è¯¯åœ°è®¤ä¸º memory manager ä¸æ”¯æŒç‹¬å èµ„æºåˆ†é…
    if not simulate_vpa_behavior(memory_manager_policy, "static"): 
        print("VPA é”™è¯¯åœ°åˆ¤æ–­ memory manager ä¸æ”¯æŒç‹¬å èµ„æºåˆ†é…ã€‚")
    else:
        print("é…ç½®åˆ¤æ–­æ­£ç¡®ã€‚")
    
if __name__ == "__main__":
    main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥å¤ç°è„šæœ¬æ¨¡æ‹Ÿäº† VPA åœ¨æ£€æŸ¥èµ„æºç®¡ç†å™¨é…ç½®æ—¶çš„ç®€å•é€»è¾‘ã€‚è„šæœ¬å®šä¹‰äº†ä¸€ä¸ªå‡½æ•° simulate_vpa_behaviorï¼Œç”¨äºæ¯”è¾ƒä¼ å…¥çš„ cpumanager å’Œ memory manager çš„ç­–ç•¥å€¼ä¸é¢„æœŸå€¼ï¼ˆå…¨éƒ¨å°å†™ "static"ï¼‰ã€‚ç”±äºå†…å­˜ç®¡ç†å™¨å¯èƒ½é…ç½®ä¸ºé¦–å­—æ¯å¤§å†™ "Static"ï¼Œå¯¼è‡´æ¯”è¾ƒç»“æœä¸º Falseï¼Œä»è€Œæ¨¡æ‹Ÿäº† ISSUE ä¸­æè¿°çš„é€»è¾‘é”™è¯¯æƒ…å†µã€‚  
éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¯¥é—®é¢˜è™½ä¼šå¯¼è‡´ VPA è¡Œä¸ºä¸æ­£ç¡®ï¼Œä½†ä¸ä¼šå¯¼è‡´ç³»ç»Ÿå®‰å…¨é—®é¢˜ï¼Œæ•…åœ¨é£é™©è¯„çº§ä¸Šåˆ¤æ–­ä¸ºâ€œä¸æ¶‰åŠâ€ï¼Œä¹Ÿä¸ä¼šæä¾›å®é™…å±å®³çš„é«˜é£é™©åˆ©ç”¨ PoCã€‚æœ¬è„šæœ¬ä»…ç”¨äºæ¼”ç¤ºåŠŸèƒ½æ€§é—®é¢˜çš„å¤ç°ï¼Œä¸å…·å¤‡å®‰å…¨é£é™©ã€‚

---


## Issue #129523 DRA: Increase CEL expression complexity limit in resourceClaimTemplate

- Issue é“¾æ¥ï¼š[#129523](https://github.com/kubernetes/kubernetes/issues/129523)

### Issue å†…å®¹

#### What happened?

Currently, smiple expressions does not pass the cost validation because of cel expression complexity limit though cel expression is not really complicated.

Case: https://github.com/intel/intel-resource-drivers-for-kubernetes/blob/80d57856956343e457a79b2cfe9f0486884a4765/deployments/qat/tests/resource-claim-template.yaml#L14

In addition, `exists_one`, `split`, `filter` and `map` also exceed.


#### What did you expect to happen?

It should pass the cost validation for simple enough expressions.

#### How can we reproduce it (as minimally and precisely as possible)?

create a resourceClaimTemplate with CEL expression that with the aforementioned expressions.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.32.0

```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ­¤ issue æè¿°çš„æ˜¯åœ¨ resourceClaimTemplate ä¸­ä½¿ç”¨ CEL è¡¨è¾¾å¼æ—¶ï¼Œç”±äºè¡¨è¾¾å¼å¤æ‚åº¦é™åˆ¶çš„é—®é¢˜å¯¼è‡´ä¸€äº›ç®€å•è¡¨è¾¾å¼ï¼ˆä»¥åŠ exists_oneã€splitã€filter å’Œ map ç­‰å‡½æ•°çš„è°ƒç”¨ï¼‰æ— æ³•é€šè¿‡æˆæœ¬éªŒè¯ã€‚è¿™å±äºåŠŸèƒ½ä¸Šçš„é™åˆ¶æˆ–éªŒè¯ä¸è¶³çš„é—®é¢˜ï¼Œå¹¶éæ¶‰åŠå®‰å…¨æ•æ„Ÿçš„æ¼æ´ã€‚å› æ­¤ï¼Œè¯¥ issue æ›´å¤šæ˜¯åŠŸèƒ½æ”¹è¿›æˆ– bug ä¿®æ­£ï¼Œè€Œä¸æ˜¯å®‰å…¨é£é™©é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
ç”±äºæœ¬ issue ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œä¸éœ€è¦å¤ç°å®‰å…¨é—®é¢˜ã€‚
è¯¥è„šæœ¬ä»…ç”¨äºæç¤ºæ­¤ issue ä¸å®‰å…¨æ¼æ´æ— å…³ã€‚
"""

def main():
    print("è¯¥ issue ä»…æ¶‰åŠ CEL è¡¨è¾¾å¼å¤æ‚åº¦é™åˆ¶çš„åŠŸèƒ½éªŒè¯é—®é¢˜ï¼Œä¸å­˜åœ¨å®‰å…¨é£é™©ã€‚")

if __name__ == "__main__":
    main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è„šæœ¬ä»…ç”¨äºå±•ç¤ºå½“å‰ issue å¹¶ä¸æ„æˆå®‰å…¨æ¼æ´çš„äº‹å®ã€‚æ ¹æ®åˆ†æï¼Œæ­¤åŠŸèƒ½é—®é¢˜å±äºéªŒè¯ç­–ç•¥é—®é¢˜ï¼Œä¸ä¼šè¢«æ”»å‡»è€…åˆ©ç”¨æ¥æ‰§è¡Œä»£ç ã€é€ƒé€¸å®¹å™¨ã€è¶Šæƒæˆ–æ»¥ç”¨ç³»ç»Ÿèµ„æºï¼Œå› æ­¤ä¸éœ€è¦æä¾›é«˜é£é™©æ¼æ´çš„å¤ç°ä»£ç ã€‚è„šæœ¬è¿è¡Œåä¼šè¾“å‡ºè¯´æ˜ä¿¡æ¯ï¼Œè¡¨ç¤ºè¯¥ issue ä¸å®‰å…¨é£é™©æ— å…³ã€‚

---


## Issue #129503 Ratcheting validation missing for CRD status subresources

- Issue é“¾æ¥ï¼š[#129503](https://github.com/kubernetes/kubernetes/issues/129503)

### Issue å†…å®¹

#### What happened?

Testing the new CRD [ratcheting validation](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-ratcheting) feature, I implemented a change of validation from `pattern` to `enum` in both a `spec` and `status` field on a CRD. Note, this CRD includes a status subresource.

I created the CRD with the `pattern` validation on the fields initially and then created a CR with field values in spec and status that matched the pattern, but did not match my intended enum.

Editing the CRD now to move to the `enum`, this would invalidate the stored data in the two fields, meaning ratcheting is required.

Updating fields such as metadata (adding an annotation) or other spec fields worked correctly, and no issues were returned.

Trying to update any status field using the status subresource path failed, complaining that my field did not match the expected enum.

#### What did you expect to happen?

Values to the status should be ratcheted in the same way that the based endpoint is ratcheted.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a minimal CRD with a status subresource and a field with a pattern validation (and another field for testing)
2. Persist a CR with a value that does not match the pattern.
3. Update the CRD schema to move from `pattern` to `enum` with a value that does not match the persisted CR.
4. Update an annotation, observe success
5. Update the other CRD status field with a new value, observe that it errors complaining about the enum value being incorrect.

#### Anything else we need to know?

Neither the [documentation](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-ratcheting) nor the [KEP](https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/4008-crd-ratcheting) suggest that there should be any difference in behaviour.

I raised this [on slack](https://kubernetes.slack.com/archives/C0EG7JC6T/p1736187102623779) and it was suggested that this was a bug.

Looking at the code, I can see [ratcheting options](https://github.com/kubernetes/apiextensions-apiserver/blob/d72b6f721f9b3cf655cb545e59cfb5f4bc615b8f/pkg/registry/customresource/strategy.go#L287-L291) on the storage for the main path, but not in the [status strategy](https://github.com/kubernetes/apiextensions-apiserver/blob/d72b6f721f9b3cf655cb545e59cfb5f4bc615b8f/pkg/registry/customresource/status_strategy.go#L96)

#### Kubernetes version

<details>
Still in the main development branch AFAICT, tested on 1.31.3
```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯ Kubernetes CRDï¼ˆè‡ªå®šä¹‰èµ„æºï¼‰ä¸­ ratcheting éªŒè¯çš„ä¸€ä¸ªé€»è¾‘ç¼ºé™·ï¼šåœ¨æ›´æ–° CRD çš„ schemaï¼ˆå°†éªŒè¯æ–¹å¼ç”± pattern ä¿®æ”¹ä¸º enumï¼‰åï¼Œä½¿ç”¨ status å­èµ„æºè¿›è¡Œæ›´æ–°æ—¶æ²¡æœ‰åº”ç”¨ä¸»èµ„æºè·¯å¾„é‚£è¾¹çš„ ratcheting é€»è¾‘ï¼Œå¯¼è‡´æ›´æ–°çŠ¶æ€å­—æ®µæ—¶æŠ¥é”™ã€‚ç»è¿‡åˆ†æå¯ä»¥çœ‹å‡ºï¼Œæ­¤é—®é¢˜å±äºéªŒè¯é€»è¾‘å®ç°ä¸Šçš„ bugï¼Œè€Œéè®¾è®¡æˆ–å®ç°ä¸Šçš„å®‰å…¨æ¼æ´ï¼Œä¸æ¶‰åŠæ•æ„Ÿæ•°æ®æ³„éœ²ã€å‘½ä»¤æ‰§è¡Œã€ææƒã€å®¹å™¨é€ƒé€¸ç­‰å®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥é—®é¢˜ä¸å±äºå®‰å…¨é£é™©ï¼Œå› æ­¤ä¸éœ€è¦æä¾›å®‰å…¨å¤ç°POCã€‚
print("æ­¤Issueæè¿°çš„é—®é¢˜ä¸ºCRD ratchetingéªŒè¯çš„é€»è¾‘ç¼ºé™·ï¼Œå¹¶ä¸æ„æˆå®‰å…¨é£é™©ï¼Œæ— éœ€æ„é€ å¤ç°POCã€‚")
```


**è§£é‡Šè¯´æ˜ï¼š**

æ ¹æ® Issue çš„æè¿°ï¼Œç¼ºå¤± ratcheting éªŒè¯çš„è¡Œä¸ºä»…å½±å“äº† CRD status å­èµ„æºçš„æ­£å¸¸æ›´æ–°æµç¨‹ï¼Œå¹¶æœªå¼•å…¥å®‰å…¨æ¼æ´ï¼ˆä¾‹å¦‚ï¼Œæ”»å‡»è€…æ— æ³•åˆ©ç”¨è¯¥é—®é¢˜æ¥æ‰§è¡Œæœªæˆæƒæ“ä½œã€ç»•è¿‡å®‰å…¨æ§åˆ¶æˆ–å¯¼è‡´æœåŠ¡æ‹’ç»ç­‰å®‰å…¨é£é™©ï¼‰ã€‚å› æ­¤ï¼Œæœ¬é—®é¢˜çš„é£é™©è¯„çº§åˆ¤æ–­ä¸ºâ€œä¸æ¶‰åŠâ€ï¼Œå¹¶ä¸ç¬¦åˆé«˜é£é™©é—®é¢˜çš„æƒ…å†µï¼Œä¹Ÿä¸éœ€è¦æä¾›å®‰å…¨æ¼æ´çš„å¤ç°è„šæœ¬ã€‚

---


## Issue #129499 OpenAPI handler fails on duplicated path

- Issue é“¾æ¥ï¼š[#129499](https://github.com/kubernetes/kubernetes/issues/129499)

### Issue å†…å®¹

#### What happened?

After installing kyverno and kyverno reports-server on Kubernetes 1.28.15 or 1.30.6 OpenAPI handler fail to initialize with this error log in apiserver:
`handler.go:160] Error in OpenAPI handler: failed to build merge specs: unable to merge: duplicated path /apis/reports.kyverno.io/v1/namespaces/{namespace}/ephemeralreports/{name}
` 
It works fine with K8s 1.27


#### What did you expect to happen?

The OpenAPI handler should not fail to initialize. 

#### How can we reproduce it (as minimally and precisely as possible)?

On k8s 1.28+ , install kyverno and kyverno reports-server and then restart kube-apiserver then you will see error while running `kubectl get --raw /openapi/v2` 
`Error from server (ServiceUnavailable): the server is currently unable to handle the request` 

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.28.15/1.30.6

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ issue æè¿°äº†åœ¨ Kubernetes 1.28.15 æˆ– 1.30.6 ç¯å¢ƒä¸­ï¼Œå®‰è£… kyverno å’Œ kyverno reports-server åï¼Œå› é‡å¤å®šä¹‰ OpenAPI è·¯å¾„è€Œå¯¼è‡´ OpenAPI handler æ— æ³•æ­£å¸¸åˆå§‹åŒ–çš„é—®é¢˜ã€‚æŠ¥é”™ä¿¡æ¯æ˜¾ç¤º OpenAPI åˆå¹¶è§„æ ¼å¤±è´¥ï¼Œå¯¼è‡´é€šè¿‡ /openapi/v2 æ¥å£è·å–è§„èŒƒæ—¶è¿”å› ServiceUnavailable é”™è¯¯ã€‚è¿™ä¸ªé—®é¢˜å±äºç”±äºé‡å¤çš„ API è·¯å¾„å®šä¹‰å¼•èµ·çš„é…ç½®æˆ–å…¼å®¹æ€§ bugï¼Œä»é—®é¢˜æè¿°æ¥çœ‹å¹¶æ²¡æœ‰æ¶‰åŠä»»ä½•å®‰å…¨æ•æ„Ÿçš„æ•°æ®æ³„éœ²ã€æƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸æˆ–æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ç­‰å®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥å¤ç°è„šæœ¬ä»…ç”¨äºæ¨¡æ‹Ÿé—®é¢˜ç¯å¢ƒä¸­çš„è¡Œä¸ºï¼Œä»…ä¾›æœ¬åœ°æµ‹è¯•å’Œç ”ç©¶è§‚å¯Ÿã€‚
# æ³¨æ„ï¼šæ­¤é—®é¢˜ä¸º OpenAPI è§„èŒƒåˆå¹¶è¿‡ç¨‹ä¸­çš„ bugï¼Œå¹¶éå®‰å…¨æ¼æ´ï¼Œ
# å› æ­¤è¯¥è„šæœ¬ä»…ç”¨äºæ¨¡æ‹Ÿé”™è¯¯æ—¥å¿—æ‰“å°ï¼Œä¸æ¶‰åŠå¯¹çœŸå® Kubernetes é›†ç¾¤çš„æ”»å‡»è¡Œä¸ºã€‚

import http.server
import socketserver

class OpenAPIHandler(http.server.SimpleHTTPRequestHandler):
    def do_GET(self):
        # æ¨¡æ‹Ÿ /openapi/v2 æ¥å£ï¼Œå¦‚æœæ£€æµ‹åˆ°é‡å¤è·¯å¾„åˆ™è¿”å›é”™è¯¯
        if self.path == "/openapi/v2":
            # æ¨¡æ‹Ÿé‡å¤è·¯å¾„æ£€æµ‹é”™è¯¯
            self.send_response(503)
            self.send_header("Content-type", "text/plain")
            self.end_headers()
            self.wfile.write(b"Error in OpenAPI handler: failed to build merge specs: duplicated path detected")
        else:
            self.send_response(200)
            self.send_header("Content-type", "text/plain")
            self.end_headers()
            self.wfile.write(b"Hello, this is a simulated Kubernetes API server.")

def run_server(port=8080):
    with socketserver.TCPServer(("", port), OpenAPIHandler) as httpd:
        print(f"æ¨¡æ‹Ÿ HTTP æœåŠ¡å™¨å·²å¯åŠ¨ï¼Œç›‘å¬åœ¨ç«¯å£ {port}")
        try:
            httpd.serve_forever()
        except KeyboardInterrupt:
            print("æœåŠ¡å™¨å…³é—­ã€‚")
            httpd.shutdown()

if __name__ == '__main__':
    run_server()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. æœ¬è„šæœ¬æ¨¡æ‹Ÿäº†ä¸€ä¸ª HTTP æœåŠ¡å™¨ï¼Œç›‘å¬åœ¨ 8080 ç«¯å£ã€‚æœåŠ¡å™¨ä¸­å®šä¹‰äº†ä¸€ä¸ª OpenAPIHandlerï¼Œå½“è®¿é—®è·¯å¾„ä¸º â€œ/openapi/v2â€ æ—¶ï¼Œè¿”å› 503 çŠ¶æ€ç å¹¶è¾“å‡ºé”™è¯¯æç¤ºä¿¡æ¯ï¼Œæ¨¡æ‹Ÿç”±äºé‡å¤è·¯å¾„å¯¼è‡´ OpenAPI handler æ„å»ºå¤±è´¥çš„æƒ…å†µã€‚
2. å…¶ä»–é â€œ/openapi/v2â€ è¯·æ±‚ï¼Œåˆ™è¿”å› 200 çŠ¶æ€ç åŠç®€å•çš„æ¬¢è¿ä¿¡æ¯ã€‚
3. éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ­¤é—®é¢˜æœ¬èº«æ˜¯ç”± API è·¯å¾„é‡å¤å¼•èµ·çš„ bugï¼Œä¸å®‰å…¨æ”»å‡»æ— å…³ï¼Œå› æ­¤è„šæœ¬ä¸­çš„å¤ç°é€»è¾‘ä»…ç”¨äºæ¼”ç¤ºé—®é¢˜äº§ç”Ÿæ—¶è¿”å›ç›¸åº”é”™è¯¯ä¿¡æ¯çš„åœºæ™¯ï¼Œå¹¶éç”¨äºå®‰å…¨æµ‹è¯•æˆ–åˆ©ç”¨ã€‚
4. æ‰€æœ‰å‡­è¯ã€é…ç½®ç­‰å‡å‡è®¾åœ¨é»˜è®¤ä½ç½®ï¼Œæœ¬è„šæœ¬ä¸æ¶‰åŠçœŸå®çš„ Kubernetes é›†ç¾¤æ“ä½œï¼Œä»…åœ¨æœ¬åœ°ç¯å¢ƒä¸­æ¨¡æ‹Ÿé—®é¢˜ç°è±¡è¿›è¡ŒéªŒè¯ã€‚

---


## Issue #129496 Fix message formatting in AttachDetach controller on VA status error

- Issue é“¾æ¥ï¼š[#129496](https://github.com/kubernetes/kubernetes/issues/129496)

### Issue å†…å®¹

#### What happened?

Attachdetach Controller records events on a PVC when errors occur:
 * https://github.com/kubernetes/kubernetes/blob/c3f3fdc1aa62002a58bec1141fe69e86bbb27491/pkg/volume/util/operationexecutor/operation_generator.go#L315

The message is logged with `Eventf`, however a single string is passed to the `messageFmt` argument. This treats a message (which may potentially contain format characters that should be treated literally) as a format string. This can result in incorrectly formatted messages, depending on the error message. This is problematic for CSI, as error messages are propagated up from the CSI driver, and presented on a VolumeAttachment resource as a status message. This can result in an event with special characters being replaced by `!!(MISSING)`.

#### What did you expect to happen?

Expected error message to be printed as string literal, not format string.

#### How can we reproduce it (as minimally and precisely as possible)?

Reproduction requires configuring a CSI driver to emit a particular error message that includes special characters interpreted as golang string format characters on `ControllerPublish` RPC.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
1.30
```

</details>


#### Cloud provider

<details>
GCP
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯ AttachDetach æ§åˆ¶å™¨åœ¨å¤„ç† PVC é”™è¯¯äº‹ä»¶æ—¶ï¼Œä½¿ç”¨ Eventf è®°å½•æ—¥å¿—æ—¶é”™è¯¯åœ°å°†æ•´ä¸ªé”™è¯¯æ¶ˆæ¯ä½œä¸ºæ ¼å¼åŒ–å­—ç¬¦ä¸²ä¼ å…¥ï¼Œå¯èƒ½å¯¼è‡´æ—¥å¿—ä¸­çš„æ ¼å¼å ä½ç¬¦è¢«é”™è¯¯æ›¿æ¢æˆ "!!(MISSING)" çš„ç°è±¡ã€‚è¿™æ˜¯ä¸€ä¸ªå…³äºæ—¥å¿—ä¿¡æ¯æ ¼å¼åŒ–çš„é—®é¢˜ï¼Œä½¿å¾—é”™è¯¯ä¿¡æ¯æ˜¾ç¤ºä¸æ­£ç¡®ï¼›å¹¶ä¸æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€ææƒã€å®¹å™¨é€ƒé€¸ã€æ‹’ç»æœåŠ¡æ”»å‡»ç­‰å®‰å…¨é£é™©ï¼Œä¹Ÿä¸ä¼šä½¿é”™è¯¯ä¿¡æ¯æ³„éœ²å‡ºæ•æ„Ÿæ•°æ®ã€‚å› æ­¤ï¼Œè¿™ä¸ªé—®é¢˜ä»…å±äºæ—¥å¿—æ˜¾ç¤ºé”™è¯¯çš„é—®é¢˜ï¼Œè€Œéå®‰å…¨æ¼æ´ï¼Œä¸å­˜åœ¨å®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# ç”±äºæœ¬Issueä¸æ˜¯å®‰å…¨é£é™©é—®é¢˜ï¼Œæ‰€ä»¥æ²¡æœ‰é«˜é£é™©çš„å¤ç°è„šæœ¬ã€‚
# ä¸‹é¢æä¾›ä¸€ä¸ªç®€å•çš„ç¤ºä¾‹ï¼Œæ¨¡æ‹Ÿè¯¥æ—¥å¿—æ ¼å¼åŒ–é—®é¢˜ï¼Œä»…ç”¨äºå±•ç¤ºé”™è¯¯çš„æ ¼å¼åŒ–ç°è±¡

import logging

def simulate_eventf(message_fmt, *args):
    """
    æ¨¡æ‹Ÿ Kubernetes ä¸­ Eventf è¡Œä¸ºï¼š
    å°† message_fmt ä½œä¸ºæ ¼å¼åŒ–å­—ç¬¦ä¸²æ¥å¤„ç†ï¼Œå¦‚æœå…¶ä¸­åŒ…å«æ ¼å¼åŒ–å ä½ç¬¦ï¼Œ
    åˆ™ä¼šæ›¿æ¢å¯¹åº”å‚æ•°ï¼›å¦åˆ™åŸæ ·è¾“å‡ºã€‚
    """
    try:
        # å°è¯•æ ¼å¼åŒ–å­—ç¬¦ä¸²
        message = message_fmt % args
    except Exception as e:
        message = f"æ ¼å¼åŒ–é”™è¯¯ï¼š{str(e)}ï¼ŒåŸæ¶ˆæ¯ï¼š{message_fmt}"
    return message

if __name__ == '__main__':
    # é”™è¯¯ç¤ºä¾‹ï¼šé”™è¯¯æ¶ˆæ¯ä¸­åŒ…å«æ ¼å¼åŒ–å­—ç¬¦ä¸²çš„å ä½ç¬¦ï¼Œä¾‹å¦‚ %s ï¼Œä½†æ²¡æœ‰ç›¸åº”å‚æ•°
    error_message = "CSI driver error: unexpected token '%s' in response"
    
    # æ­¤å¤„é”™è¯¯åœ°ä¼ å…¥æ•´ä¸ªé”™è¯¯æ¶ˆæ¯ä½œä¸ºæ ¼å¼åŒ–å­—ç¬¦ä¸²å‚æ•°ï¼Œé¢„æœŸä¸è¿›è¡Œæ ¼å¼åŒ–
    # å®é™…ä¸Š simulate_eventf å°†å°è¯•æŒ‰ç…§æ ¼å¼ç¬¦å·è¿›è¡Œæ›¿æ¢
    formatted_message = simulate_eventf(error_message, )
    print("å¤ç°çš„æ ¼å¼åŒ–ç»“æœ:")
    print(formatted_message)
    
    # æ­£ç¡®çš„åšæ³•åº”è¯¥æ˜¯å°†é”™è¯¯æ¶ˆæ¯ä½œä¸ºæ™®é€šå­—ç¬¦ä¸²å¤„ç†ï¼Œè€Œä¸æ˜¯æ ¼å¼åŒ–å­—ç¬¦ä¸²ã€‚
    # æ¨¡æ‹Ÿæ­£ç¡®å¤„ç†ï¼š
    print("\næ­£ç¡®å¤„ç†çš„ç»“æœ:")
    print(error_message)
    
    # å¦å¤–ï¼Œå¯¹äºéœ€è¦å°†é”™è¯¯æ¶ˆæ¯ä¸­çš„æ ¼å¼ç¬¦å·åŸæ ·æ˜¾ç¤ºçš„æƒ…å†µï¼Œå¯ä»¥è¿›è¡Œè½¬ä¹‰å¤„ç†ï¼š
    escaped_message = error_message.replace("%", "%%")
    print("\nè½¬ä¹‰åçš„æ­£ç¡®æ˜¾ç¤ºç»“æœ:")
    print(escaped_message)
```


**è§£é‡Šè¯´æ˜ï¼š**

è¿™ä¸ªå¤ç°è„šæœ¬æ¨¡æ‹Ÿäº† Kubernetes ä¸­ä½¿ç”¨ Eventf æ‰“å°æ—¥å¿—æ—¶å¯èƒ½å‡ºç°çš„æ ¼å¼åŒ–é—®é¢˜ã€‚å‡½æ•° simulate_eventf å°è¯•æŒ‰ç…§ä¼ å…¥çš„ message_fmt è¿›è¡Œå­—ç¬¦ä¸²æ ¼å¼åŒ–ï¼›å½“é”™è¯¯æ¶ˆæ¯ä¸­åŒ…å«æœªè¢«è½¬ä¹‰çš„æ ¼å¼åŒ–å ä½ç¬¦ï¼ˆä¾‹å¦‚ %sï¼‰è€Œæ²¡æœ‰æä¾›ç›¸åº”çš„å‚æ•°æ—¶ï¼Œå°±ä¼šå‡ºç°æ ¼å¼åŒ–é”™è¯¯ï¼Œä»è€Œæ¨¡æ‹Ÿ Issue ä¸­æè¿°çš„ç°è±¡ã€‚  
ç”±äºæœ¬ Issue å¹¶éå®‰å…¨é—®é¢˜ï¼Œæ‰€ä»¥å¤ç°è„šæœ¬ä»…ç”¨äºå±•ç¤ºé”™è¯¯çš„æ—¥å¿—æ ¼å¼åŒ–è¡Œä¸ºï¼Œä¸æ¶‰åŠå®‰å…¨æ”»å‡»æˆ–æ¼æ´åˆ©ç”¨ã€‚å®é™…ç¯å¢ƒä¸­ï¼Œä¸ºé˜²æ­¢æ—¥å¿—æ ¼å¼åŒ–é”™è¯¯ï¼Œåº”ç¡®ä¿å¯¹é”™è¯¯æ¶ˆæ¯ä¸­çš„ç‰¹æ®Šå­—ç¬¦è¿›è¡Œé€‚å½“å¤„ç†ï¼Œå¦‚è½¬ä¹‰æˆ–ç›´æ¥å½“åšæ™®é€šå­—ç¬¦ä¸²ã€‚

---


## Issue #129487 Using fake api client to query resources will cause the client-go package memory to continue to grow

- Issue é“¾æ¥ï¼š[#129487](https://github.com/kubernetes/kubernetes/issues/129487)

### Issue å†…å®¹

#### What happened?

When I use fake api client in the simulated scheduler to simulate the resource changes of pods or nodes, the memory of the client-go package continues to increase.For example, after I create a pod, when I query this pod multiple times, the client-go memory continues to grow.

The problem I'm currently facing is that all actions such as query, modify, and delete will be appended to this c.actions slice, causing the client-go package memory to continue to increase until the process is restarted to release the memory.

``` go 
 func (c *Fake) Invokes(action Action, defaultReturnObj runtime.Object) (runtime.Object, error) {

	c.Lock()
	defer c.Unlock()

	actionCopy := action.DeepCopy()
	c.actions = append(c.actions, action.DeepCopy())
	for _, reactor := range c.ReactionChain {
		if !reactor.Handles(actionCopy) {
			continue
		}

		handled, ret, err := reactor.React(actionCopy)
		if !handled {
			continue
		}

		return ret, err
	}

	return defaultReturnObj, nil
}
```

#### What did you expect to happen?


Fix the code here: c.actions = append(c.actions, action.DeepCopy()) , where actions such as add, delete, modify, and query are not continuously appended to a.action, causing the memory to continue to grow.

#### How can we reproduce it (as minimally and precisely as possible)?

``` go
import (
	"context"
	"fmt"
	"net/http"
	"time"

	_ "net/http/pprof"

	"github.com/gofrs/uuid"
	v1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	fakeclientset "k8s.io/client-go/kubernetes/fake"
)

func main() {
	go func() {
		fmt.Println(http.ListenAndServe("localhost:8065", nil))
	}()

	// åˆ›å»ºfakeclient
	fc := fakeclientset.NewSimpleClientset()
	for i := 0; i < 100000; i++ {
		// éšæœºç”Ÿæˆpodçš„name
		uuidName, _ := uuid.NewV4()
		podName := "test-" + uuidName.String()
		// éšæœºåˆ›å»ºpod
		pod := &v1.Pod{ObjectMeta: metav1.ObjectMeta{Name: podName}}
		// åˆ›å»ºpod
		fc.CoreV1().Pods("default").Create(context.TODO(), pod, metav1.CreateOptions{})
		pod.Spec.NodeName = "node1"
		// æ›´æ–°pod
		fc.CoreV1().Pods("default").Update(context.TODO(), pod, metav1.UpdateOptions{})
		// åˆ é™¤pod
		err := fc.CoreV1().Pods("default").Delete(context.TODO(), podName, metav1.DeleteOptions{})
		if err != nil {
			fmt.Println("delete pod failed")
		}
	}
}
```


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.
```

</details>


#### Cloud provider

<details>
Self hosted
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯ä½¿ç”¨ fake API Client æ—¶ï¼Œç”±äºæ¯æ¬¡è°ƒç”¨ï¼ˆå¢ã€åˆ ã€æ”¹ã€æŸ¥ï¼‰éƒ½ä¼šæŠŠæ“ä½œè®°å½•é€šè¿‡ DeepCopy åè¿½åŠ åˆ° c.actions åˆ‡ç‰‡ä¸­ï¼Œå¯¼è‡´ client-go åŒ…çš„å†…å­˜ä¸æ–­å¢é•¿ã€‚è¿™ä¸ªé—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå†…å­˜æ³„éœ² bugï¼Œä¸»è¦å½±å“çš„æ˜¯æµ‹è¯•æ¨¡æ‹Ÿç¯å¢ƒï¼ˆsimulated schedulerï¼‰ï¼Œè€Œéç”Ÿäº§ç¯å¢ƒä¸­çœŸå® API å®¢æˆ·ç«¯çš„è¡Œä¸ºã€‚

ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¯¥é—®é¢˜ä¸ä¼šç›´æ¥å¯¼è‡´å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰æ”»å‡»ï¼Œä¹Ÿä¸ä¼šå¯¼è‡´æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€‚å³ä½¿å‡ºç°å†…å­˜æ³„éœ²ï¼Œå¯¼è‡´è¿›ç¨‹å†…å­˜ä¸æ–­å¢é•¿ï¼Œé€šå¸¸ä¹Ÿåªä¼šå¼•èµ·èµ„æºè€—å°½çš„æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰é—®é¢˜ã€‚è€Œä¸”ç”±äºè¯¥é—®é¢˜å‡ºç°åœ¨ fake client ç”¨äºæµ‹è¯•çš„åœºæ™¯ä¸‹ï¼Œä¸å­˜åœ¨å¤–éƒ¨æ”»å‡»è€…æ— éœ€æƒé™å³è§¦å‘çš„æƒ…å†µã€‚å› æ­¤ï¼Œè¯¥ Issue æè¿°çš„é—®é¢˜ä¸ç›´æ¥æ„æˆå®‰å…¨é—®é¢˜ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

æœ¬è„šæœ¬ï¼ˆæ­¤å¤„ä¸ºç©ºï¼‰åŸæœ¬ç”¨äºå¤ç°å®‰å…¨é—®é¢˜ï¼Œä½†æŒ‰ç…§åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤ Issue å¯¼è‡´çš„å†…å­˜æ³„éœ²å¹¶ä¸ç›´æ¥æ„æˆå®‰å…¨é£é™©ã€‚å†…å­˜æ³„éœ²æ‰€å¼•å‘çš„èµ„æºè€—å°½é—®é¢˜åœ¨è¯¥åœºæ™¯ä¸­ä¸»è¦å½±å“çš„æ˜¯æµ‹è¯•ç¯å¢ƒï¼Œå¹¶ä¸”æ”»å‡»è€…é€šå¸¸æ— æ³•é€šè¿‡è§¦å‘è¯¥é—®é¢˜å¯¹ç”Ÿäº§ç¯å¢ƒé€ æˆå®é™…å®‰å…¨å¨èƒã€‚å› æ­¤ä¸ç”Ÿæˆç›¸å…³å¤ç°ä»£ç ã€‚å¯¹äºå†…å­˜æ³„éœ²é—®é¢˜ï¼Œå¯ä»¥ä»ä»£ç é€»è¾‘è§’åº¦è§£å†³ï¼šä¾‹å¦‚ä¸åœ¨æ¯æ¬¡æ“ä½œæ—¶å°†è¡Œä¸ºæ°¸ä¹…ä¿å­˜ï¼Œæˆ–ä»¥ä¸€å®šæ¡ä»¶æ¸…ç†è¿‡æœŸçš„æ“ä½œè®°å½•ï¼Œä»è€Œé¿å…å†…å­˜ä¸æ–­ç´¯ç§¯ã€‚

---


## Issue #129481 Can't mount iscsi volume with openiscsi from target that does not automatically inform your initiator about changes in that session.

- Issue é“¾æ¥ï¼š[#129481](https://github.com/kubernetes/kubernetes/issues/129481)

### Issue å†…å®¹

#### What happened?

Can't mount iscsi volume in pod. 

I'm creating pod with iscsi volume from NAS using config:
```
---
apiVersion: v1
kind: Pod
metadata:
  name: iscsipd2
spec:
  containers:
  - name: iscsipd-ro
    image: nginx
    volumeMounts:
    - mountPath: "/mnt/iscsipd"
      name: iscsipd-rw
  volumes:
  - name: iscsipd-rw
    iscsi:
      targetPortal: <portalhost/port>
      iqn: <IQN>
      lun: 1
      fsType: ext4
      readOnly: false
```

Pod fails to start reporting:
```
  Warning  FailedMount             2s (x7 over 3m28s)  kubelet                  MountVolume.WaitForAttach failed for volume "iscsipd-rw" : failed to get any path for iscsi disk, last err seen:
Timed out waiting for device at path /dev/disk/by-path/<portal-IQN-Lun> after 30s
```

iSCSI server is Synology NAS which uses open iscsi 2.0-876.

It is possible to mount LUNs onto the host itself manually.

#### What did you expect to happen?

Volume mounted.

#### How can we reproduce it (as minimally and precisely as possible)?

Reproducing could be tricky if you don't have version of iscsi that shows similar behaviour.


When kubelet mounts iSCSI volume it executes the following command sequence
(starts here https://github.com/kubernetes/kubernetes/blob/642efbb595df18bcac54e1e53ab3d1d4df1569aa/pkg/volume/iscsi/iscsi_util.go#L339)

```
iscsiadm -m discoverydb -t sendtargets -p <portal> -I default -o new
iscsiadm -m discoverydb -t sendtargets -p <portal> -I default --discover
iscsiadm -m node -p <portal> -T <target> -I default -o update -n node.session.scan -v manual
iscsiadm -m node -p <portal> -T <target> -I default --login
iscsiadm -m node -p <portal> -T <target> -o update -n node.startup -v manual
```
After this it will wait for device path to appear once iscsid service on the node attaches to LUN. This unfortunately doesn't happen. The reason is `node.session.scan` being set to `manual` which was introduced in #90982 to address different issue. It is happening because my server doesn't notify initiator about new luns in session when `manual` option is set.

The solution is to rescan active iSCSI session to discover LUN which would trigger device file creation.
The test was to just execute
```
sudo iscsiadm -m node -p <...> -T <...> -I default -R
```
while `kubelet` was waiting for device file.
I then added
```
diff --git a/pkg/volume/iscsi/iscsi_util.go b/pkg/volume/iscsi/iscsi_util.go
index b51127aa92f..239cb075aef 100644
--- a/pkg/volume/iscsi/iscsi_util.go
+++ b/pkg/volume/iscsi/iscsi_util.go
@@ -376,6 +376,12 @@ func (util *ISCSIUtil) AttachDisk(b iscsiDiskMounter) (string, error) {
                                        klog.Warningf("Warning: Failed to set iSCSI login mode to manual. Error: %v", err)
                                }
 
+                               // For targets that don't notify initiator about LUNs added to session we need to trigger recans
+                               _, err = execWithLog(b, "iscsiadm", "-m", "node", "-p", tp, "-T", b.Iqn, "-I", b.Iface, "-R")
+                               if err != nil {
+                                       klog.Warningf("Warning: Failed to trigger rescan of LUNs in session. Error: %v", err)
+                               }
+
                                // Rebuild the host map after logging in
                                portalHostMap, err := b.deviceUtil.GetISCSIPortalHostMapForTarget(b.Iqn)
                                if err != nil {

```
to `kubelet` unconditionally after `--login` command to fix the issue. While it works, I'm sure there might be cases where this behaviour is undesirable, some internet people mentioned rescan being slow. Nevertheless it would be nice to have an option to do rescan either based on config or when mount can't be found within certain time threshold.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.3
Kustomize Version: v5.4.2
Server Version: v1.32.0
```

</details>


#### Cloud provider

<details>
Self hosted
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 24.04.1 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04.1 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo
$ uname -a
Linux xxx 6.8.0-51-generic #52-Ubuntu SMP PREEMPT_DYNAMIC Thu Dec  5 13:09:44 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¿™ä¸ª Issue æè¿°äº†åœ¨ Kubernetes ä¸­ä½¿ç”¨ iSCSI å·æ—¶å‡ºç°æŒ‚è½½å¤±è´¥çš„é—®é¢˜ï¼ŒåŸå› åœ¨äº openiscsi 2.0-876 ç‰ˆæœ¬å¯¹ç›®æ ‡è®¾å¤‡æœªèƒ½ä¸»åŠ¨é€šçŸ¥ initiator å…³äº LUN å˜åŒ–çš„é—®é¢˜ã€‚ä¸ºäº†è§£å†³è¯¥é—®é¢˜ï¼Œæå‡ºäº†åœ¨ kubelet æ‰§è¡Œ iscsiadm ç™»å½•åç«‹å³è§¦å‘ä¸€æ¬¡ rescanï¼ˆå³æ‰§è¡Œ iscsiadmÂ -Rï¼‰çš„æ–¹æ¡ˆï¼Œä»¥ä¾¿é‡æ–°æ‰«æä¼šè¯ä¸­çš„ LUNã€‚è¯¥é—®é¢˜åæ˜ çš„æ˜¯å­˜å‚¨æŒ‚è½½å’Œè®¾å¤‡å‘ç°æµç¨‹ä¸­çš„åŠŸèƒ½æ€§ç¼ºé™·ï¼Œè€Œéå®‰å…¨æ¼æ´ï¼Œå…¶æœ¬è´¨ä¸Šä¸ä¼šå¯¼è‡´æ•°æ®æ³„éœ²ã€æœªæˆæƒå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰å®‰å…¨é£é™©ï¼Œä¹Ÿä¸å…·å¤‡æ¶æ„åˆ©ç”¨çš„æ”»å‡»é¢ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# ç”±äºè¯¥ Issue ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œä¸‹é¢çš„è„šæœ¬ä»…ç”¨äºæ¨¡æ‹Ÿ iSCSI æŒ‚è½½è¿‡ç¨‹ä¸­è§¦å‘ rescan çš„åŠŸèƒ½æ€§å¤ç°ï¼Œ
# è¯·åœ¨å®‰å…¨æµ‹è¯•ç¯å¢ƒä¸­ä½¿ç”¨è¯¥è„šæœ¬ï¼Œåˆ‡å‹¿åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ç›´æ¥è¿è¡Œã€‚

import subprocess
import time
import logging
from http.server import BaseHTTPRequestHandler, HTTPServer
import threading

# åˆ›å»ºä¸€ä¸ªç®€å•çš„ HTTP æœåŠ¡å™¨ï¼Œç”¨äºç›‘å¬8080ç«¯å£ï¼ˆæ»¡è¶³é¢˜ç›®è¦æ±‚ï¼‰
class SimpleHTTPRequestHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.end_headers()
        self.wfile.write(b'ISCIS rescan simulation server running')

def start_http_server():
    server_address = ('', 8080)
    httpd = HTTPServer(server_address, SimpleHTTPRequestHandler)
    logging.info("HTTP server starting on port 8080")
    httpd.serve_forever()

def run_command(command):
    logging.info("æ‰§è¡Œå‘½ä»¤: %s", " ".join(command))
    try:
        # åœ¨æµ‹è¯•ç¯å¢ƒä¸­ï¼Œiscsiadm å‘½ä»¤å¯èƒ½æ— æ³•æ­£å¸¸è¿è¡Œï¼Œå› æ­¤ä½¿ç”¨ shell è°ƒç”¨æ¨¡æ‹Ÿæƒ…å†µ
        result = subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, timeout=30)
        if result.stdout:
            logging.info("è¾“å‡º: %s", result.stdout.strip())
        if result.stderr:
            logging.warning("é”™è¯¯: %s", result.stderr.strip())
        return result.stdout, result.stderr
    except subprocess.TimeoutExpired as e:
        logging.error("å‘½ä»¤è¶…æ—¶: %s", e)
        return None, str(e)

def simulate_iscsi_mount(target_portal, iqn, lun, iface="default"):
    # æ¨¡æ‹Ÿ iSCSI å‘ç°å’Œç™»å½•è¿‡ç¨‹
    cmds = [
        ["iscsiadm", "-m", "discoverydb", "-t", "sendtargets", "-p", target_portal, "-I", iface, "-o", "new"],
        ["iscsiadm", "-m", "discoverydb", "-t", "sendtargets", "-p", target_portal, "-I", iface, "--discover"],
        ["iscsiadm", "-m", "node", "-p", target_portal, "-T", iqn, "-I", iface, "-o", "update", "-n", "node.session.scan", "-v", "manual"],
        ["iscsiadm", "-m", "node", "-p", target_portal, "-T", iqn, "-I", iface, "--login"],
        ["iscsiadm", "-m", "node", "-p", target_portal, "-T", iqn, "-o", "update", "-n", "node.startup", "-v", "manual"],
    ]
    for command in cmds:
        run_command(command)
        time.sleep(1)  # æ¨¡æ‹Ÿå‘½ä»¤é—´çš„å»¶æ—¶

    # æ¨¡æ‹Ÿ kubelet ç­‰å¾…è®¾å¤‡èŠ‚ç‚¹ç”Ÿæˆçš„è¿‡ç¨‹ï¼ŒæœŸé—´é€šè¿‡è§¦å‘ rescan è„šæœ¬é€šçŸ¥ LUN çš„æ›´æ–°
    logging.info("ç­‰å¾…è®¾å¤‡èŠ‚ç‚¹ç”Ÿæˆ...")
    device_found = False
    for attempt in range(10):
        # æ­¤å¤„åˆ†æ”¯ä»…ä½œæ¨¡æ‹Ÿæ£€æŸ¥ï¼Œåœ¨çœŸå®åœºæ™¯ä¸­åº”æ£€æµ‹ /dev/disk/by-path/ ä¸‹æ˜¯å¦å­˜åœ¨è®¾å¤‡æ–‡ä»¶
        logging.info("å°è¯• %d: æœªæ£€æµ‹åˆ°å¯¹åº”è®¾å¤‡ï¼Œè§¦å‘ rescan æ“ä½œ...", attempt + 1)
        rescan_cmd = ["iscsiadm", "-m", "node", "-p", target_portal, "-T", iqn, "-I", iface, "-R"]
        run_command(rescan_cmd)
        time.sleep(3)
        # æ¨¡æ‹Ÿè®¾å¤‡åœ¨ç¬¬5æ¬¡å°è¯•åå‡ºç°
        if attempt == 4:
            device_found = True
            logging.info("è®¾å¤‡èŠ‚ç‚¹æ¨¡æ‹Ÿå‡ºç°")
            break

    if not device_found:
        logging.warning("è¶…æ—¶æœªæ£€æµ‹åˆ°è®¾å¤‡èŠ‚ç‚¹")
    else:
        logging.info("iSCSI æŒ‚è½½æµç¨‹æ¨¡æ‹ŸæˆåŠŸ")

def main():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

    # å¯åŠ¨ HTTP æœåŠ¡å™¨çº¿ç¨‹ï¼ˆæ»¡è¶³é¢˜ç›®è¦æ±‚ï¼Œç›‘å¬8080ç«¯å£ï¼‰
    http_thread = threading.Thread(target=start_http_server, daemon=True)
    http_thread.start()

    # é…ç½®æµ‹è¯•å‚æ•°ï¼ˆæ ¹æ®æµ‹è¯•ç¯å¢ƒè¿›è¡Œè°ƒæ•´ï¼‰
    target_portal = "192.168.1.100:3260"         # ç¤ºä¾‹ç›®æ ‡é—¨æˆ·åœ°å€
    iqn = "iqn.2001-04.com.example:storage.disk1"  # ç¤ºä¾‹ IQN
    lun = "1"

    simulate_iscsi_mount(target_portal, iqn, lun)

if __name__ == '__main__':
    main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. è„šæœ¬ä¸­å®šä¹‰äº† simulate_iscsi_mount å‡½æ•°ï¼Œç”¨æ¥æ¨¡æ‹Ÿ Kubernetes ä¸­æŒ‚è½½ iSCSI å·æ—¶æ‰§è¡Œçš„ä¸€ç³»åˆ— iscsiadm å‘½ä»¤ï¼ŒåŒ…æ‹¬å‘ç°ç›®æ ‡ã€æ›´æ–°ä¼šè¯æ‰«ææ¨¡å¼ã€ç™»å½•ä»¥åŠè®¾ç½®å¯åŠ¨æ¨¡å¼ç­‰æ­¥éª¤ã€‚
2. åœ¨ç­‰å¾…è®¾å¤‡ç”Ÿæˆçš„è¿‡ç¨‹ä¸­ï¼Œé€šè¿‡å¾ªç¯æ¯éš”å‡ ç§’è§¦å‘ä¸€æ¬¡ iscsiadm çš„ rescan å‘½ä»¤ï¼ˆ-R å‚æ•°ï¼‰ï¼Œæ¨¡æ‹Ÿè§¦å‘ LUN å˜åŒ–é€šçŸ¥ï¼Œä»è€Œä½¿è®¾å¤‡æ–‡ä»¶ç”Ÿæˆã€‚
3. åŒæ—¶ï¼Œæ»¡è¶³é¢˜ç›®è¦æ±‚ï¼Œè„šæœ¬å¯åŠ¨äº†ä¸€ä¸ªç®€å•çš„ HTTP æœåŠ¡å™¨ï¼Œç›‘å¬ 8080 ç«¯å£ã€‚
4. æ³¨æ„ï¼šè¯¥è„šæœ¬ä»…ç”¨äºæ¼”ç¤ºæŒ‚è½½æµç¨‹çš„åŠŸèƒ½æ€§å¤ç°ï¼Œå¹¶éé’ˆå¯¹å®‰å…¨æ¼æ´ã€‚å®é™…ç¯å¢ƒä¸­è¿è¡Œ iscsiadm å‘½ä»¤éœ€è¦ç®¡ç†å‘˜æƒé™ï¼Œå¹¶ä¸”è¯·åœ¨å®‰å…¨çš„æµ‹è¯•ç¯å¢ƒä¸‹éªŒè¯ï¼Œä¸åº”åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ç›´æ¥ä½¿ç”¨ã€‚

---


## Issue #129478 Cannot start local cluster by local-up-cluster.sh with error: timed out waiting for the condition on pods/coredns-f5bd749cf-q2jj6

- Issue é“¾æ¥ï¼š[#129478](https://github.com/kubernetes/kubernetes/issues/129478)

### Issue å†…å®¹

#### What happened?

I tried to start local cluster with kubernetes source code with command ./hack/local-up-cluster.sh, and below is the log:

root@k8s:~/go/src/k8s.io/kubernetes# ./hack/local-up-cluster.sh -O
skipped the build because GO_OUT was set (/root/go/src/k8s.io/kubernetes/_output/bin)
API SERVER secure port is free, proceeding...
Detected host and ready to start services.  Doing some housekeeping first...
Using GO_OUT /root/go/src/k8s.io/kubernetes/_output/bin
Starting services now!
Starting etcd
etcd --advertise-client-urls http://127.0.0.1:2379 --data-dir /tmp/tmp.BHf3mU3o3Y --listen-client-urls http://127.0.0.1:2379 --log-level=warn 2> "/tmp/etcd.log" >/dev/null
Waiting for etcd to come up.
+++ [0105 13:33:05] On try 2, etcd: : {"health":"true","reason":""}
{"header":{"cluster_id":"14841639068965178418","member_id":"10276657743932975437","revision":"2","raft_term":"2"}}.+...+...+...+.+........+......+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*...+...+..+.+......+........+............+..........+...+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*.........+.+.....+......+.+..+.....................+....+..+.+..+...+.........+..................+......+.+......+..+.+..............+..........+...........+.+...+...+........+....+...+............+........+....+...+......+.....+.+.........+...........+.+..+.+.........+..+................+.....+...+......+..........+........+.+..............+.......+...+..+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
...........+..........+..+....+...+..+.+..+.+......+.....+.........+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*...........+.........+..+.+..+...+.......+...+........+....+..+.+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*...+.................+....+............+..+...+..........+....................+.........+....+.........+...+........+......+...+.+......+........+.+..+...+................+.........+..+.........+.+..+...+....+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
-----
...+.........+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*...........+.....+.............+......+...+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*.....+......+..+.+.................+...+....+............+...+...+.....+...+......+....+...........+...+.+......+......+...+......+......+.........+........+.+..+...+.......+.....+..........+...............+.........+...+.....+......+...............+.......+...+.....+......+...+.+.........+..+.+..+...+...+.+......+...+...........+.+..+.......+...............+..............+................+........+.+..+.........................+.....+.........+....+..+............+.+...............+...........+...............+..........+.....+...+............+.+...+..+...+....+..+...+..........+............+...+...+..+.+.........+...........+.+.....+...+......+....+..+..........+...+.................+.+..................+..............+.+..+.........+.+.........+.....+...+...+...+....+...........+.+...+...+...+.....+....+...+.........+........+......+.......+...+......+.....+.+..+.+...............+.........+.....+..........+..+.............+..+....+..+.........+...................+..+...+.......+...+.....+......+.+............+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
.......+....+......+...+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*....+.......+...+...+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*.+.........+..........+...............+...+...........+......+....+.....+.+......+..+...............+..........+..+....+.........+............+.....+.........+.+......+........+.......+...+............+...+.....+.+...+..+................+...+.........+..+...+.......+...+......+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
-----
..............+...+.+..+.......+...+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*......+...+.+...+..+...+.............+..+...+...+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*.....+........+......+.........+......+....+............+..+.+...........+...+..........+...........+.+..............+.+.....+.+.................+.............+...+.....+......+.+............+..+....+.....+....+.....+......+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
..+......+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*......+........+.+.....+......+...+.......+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++*........+...............+..+...+......+..........+......+.....+.........+....+......+...+..+....+..+.............+..+...+....+..+....+..............+..........+...+.....+..................+....+..+....+..............+...+............+................+...........+.+..+...+....+...+...+.....+.........+....+..+............+............+.........+.+........+...+.........+.+...+.....+...............+....+..................+...+.....+.......+..+.......+.....+....+.........+.....+.+..+...+......+..........+......+..+..........+..+...+...+..........+...+..+.......+..+....+......+...+........+............+.+..+..........+...+.....+.......+..+...+....+......+..+.......+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
-----
2025/01/05 13:33:07 [INFO] generate received request
2025/01/05 13:33:07 [INFO] received CSR
2025/01/05 13:33:07 [INFO] generating key: rsa-2048
2025/01/05 13:33:07 [INFO] encoded CSR
2025/01/05 13:33:07 [INFO] signed certificate with serial number 440423749374281788744993987946487911942798734629
2025/01/05 13:33:07 [INFO] generate received request
2025/01/05 13:33:07 [INFO] received CSR
2025/01/05 13:33:07 [INFO] generating key: rsa-2048
2025/01/05 13:33:07 [INFO] encoded CSR
2025/01/05 13:33:07 [INFO] signed certificate with serial number 177798747956492092670340101307886619962413397079
2025/01/05 13:33:07 [INFO] generate received request
2025/01/05 13:33:07 [INFO] received CSR
2025/01/05 13:33:07 [INFO] generating key: rsa-2048
2025/01/05 13:33:08 [INFO] encoded CSR
2025/01/05 13:33:08 [INFO] signed certificate with serial number 245616362482589881504825093782699871584686084179
2025/01/05 13:33:08 [INFO] generate received request
2025/01/05 13:33:08 [INFO] received CSR
2025/01/05 13:33:08 [INFO] generating key: rsa-2048
2025/01/05 13:33:08 [INFO] encoded CSR
2025/01/05 13:33:08 [INFO] signed certificate with serial number 170369636513810095510638438376919261494891756788
2025/01/05 13:33:08 [INFO] generate received request
2025/01/05 13:33:08 [INFO] received CSR
2025/01/05 13:33:08 [INFO] generating key: rsa-2048
2025/01/05 13:33:09 [INFO] encoded CSR
2025/01/05 13:33:09 [INFO] signed certificate with serial number 603887759708125829878376814401298784287701864340
2025/01/05 13:33:09 [INFO] generate received request
2025/01/05 13:33:09 [INFO] received CSR
2025/01/05 13:33:09 [INFO] generating key: rsa-2048
2025/01/05 13:33:09 [INFO] encoded CSR
2025/01/05 13:33:09 [INFO] signed certificate with serial number 245882740224964498161208602587469908668758382368
2025/01/05 13:33:09 [INFO] generate received request
2025/01/05 13:33:09 [INFO] received CSR
2025/01/05 13:33:09 [INFO] generating key: rsa-2048
2025/01/05 13:33:09 [INFO] encoded CSR
2025/01/05 13:33:09 [INFO] signed certificate with serial number 440315627455551203707791282420989785578941371586
2025/01/05 13:33:09 [INFO] generate received request
2025/01/05 13:33:09 [INFO] received CSR
2025/01/05 13:33:09 [INFO] generating key: rsa-2048
2025/01/05 13:33:09 [INFO] encoded CSR
2025/01/05 13:33:09 [INFO] signed certificate with serial number 203859729277878650212404260018964069399575044648
Waiting for apiserver to come up
+++ [0105 13:33:14] On try 4, apiserver: : ok
clusterrolebinding.rbac.authorization.k8s.io/kube-apiserver-kubelet-admin created
clusterrolebinding.rbac.authorization.k8s.io/kubelet-csr created
Cluster "local-up-cluster" set.
use 'kubectl --kubeconfig=/var/run/kubernetes/admin-kube-aggregator.kubeconfig' to use the aggregated API server
serviceaccount/coredns created
clusterrole.rbac.authorization.k8s.io/system:coredns created
clusterrolebinding.rbac.authorization.k8s.io/system:coredns created
configmap/coredns created
deployment.apps/coredns created
service/kube-dns created
coredns addon successfully deployed.
Checking CNI Installation at /opt/cni/bin
WARNING : The kubelet is configured to not fail even if swap is enabled; production deployments should disable swap unless testing NodeSwap feature.
2025/01/05 13:33:16 [INFO] generate received request
2025/01/05 13:33:16 [INFO] received CSR
2025/01/05 13:33:16 [INFO] generating key: rsa-2048
2025/01/05 13:33:17 [INFO] encoded CSR
2025/01/05 13:33:17 [INFO] signed certificate with serial number 119919369832654757525409624483929365727736165
kubelet ( 51055 ) is running.
wait kubelet ready
No resources found
No resources found
127.0.0.1   NotReady   <none>   1s    v1.33.0-alpha.0.308+7bfdda4696f78f
node/127.0.0.1 condition met
2025/01/05 13:33:26 [INFO] generate received request
2025/01/05 13:33:26 [INFO] received CSR
2025/01/05 13:33:26 [INFO] generating key: rsa-2048
2025/01/05 13:33:26 [INFO] encoded CSR
2025/01/05 13:33:26 [INFO] signed certificate with serial number 39785364796214418872404472513236782332099419659
pod "coredns-f5bd749cf-nbkjk" deleted
error: timed out waiting for the condition on pods/coredns-f5bd749cf-q2jj6
error: timed out waiting for the condition on pods/coredns-f5bd749cf-q2jj6
error: timed out waiting for the condition on pods/coredns-f5bd749cf-q2jj6
error: timed out waiting for the condition on pods/coredns-f5bd749cf-q2jj6
error: timed out waiting for the condition on pods/coredns-f5bd749cf-q2jj6
error: timed out waiting for the condition on pods/coredns-f5bd749cf-q2jj6
error: timed out waiting for the condition on pods/coredns-f5bd749cf-q2jj6
error: timed out waiting for the condition on pods/coredns-f5bd749cf-q2jj6

#### What did you expect to happen?

As others example, start up local cluster successfully

#### How can we reproduce it (as minimally and precisely as possible)?

This experiment is on win10 virtual machine VirtualBox 7.1.4 r164100
OS is Ubuntu 24.04.1 LTS
git describe: v1.33.0-alpha.0-308-g7bfdda4696f
git branche: master

#### Anything else we need to know?

_No response_

#### Kubernetes version

git describe: v1.33.0-alpha.0-308-g7bfdda4696f
git branche: master

#### Cloud provider

VM on local machine
OS is Ubuntu 24.04.1 LTS

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ issue æè¿°çš„é—®é¢˜æ˜¯åœ¨ä½¿ç”¨ Kubernetes æºç å¯åŠ¨æœ¬åœ°é›†ç¾¤æ—¶ï¼Œå‡ºç°äº†è¶…æ—¶ç­‰å¾… podï¼ˆcorednsï¼‰å°±ç»ªçš„é”™è¯¯ã€‚æ—¥å¿—ä¸­æ˜¾ç¤ºçš„å„é¡¹æ“ä½œï¼ˆå¦‚ etcd å¯åŠ¨ã€CSRï¼ˆè¯ä¹¦ç­¾å‘ï¼‰è¯·æ±‚å¤„ç†ç­‰ï¼‰å‡ä¸ºæ­£å¸¸çš„é›†ç¾¤å¯åŠ¨æµç¨‹ä¸­çš„æ—¥å¿—ä¿¡æ¯ï¼Œæ²¡æœ‰å‡ºç°å¼‚å¸¸çš„æ•æ„Ÿæ•°æ®æ³„éœ²æˆ–æœåŠ¡å¼‚å¸¸å¤„ç†ã€‚æ•´ä½“æ¥çœ‹ï¼Œè¿™æ˜¯é›†ç¾¤å¯åŠ¨è¿‡ç¨‹ä¸­é‡åˆ°çš„æ‰§è¡Œå’Œé€šä¿¡è¶…æ—¶é—®é¢˜ï¼Œå±äºè¿ç»´æˆ–ç¯å¢ƒé…ç½®çš„ bugï¼Œè€Œéå®‰å…¨æ¼æ´æˆ–åˆ©ç”¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
è¯¥è„šæœ¬ç”¨äºæ¨¡æ‹Ÿè°ƒç”¨ Kubernetes æºç ä¸­çš„ local-up-cluster.sh è„šæœ¬ï¼Œ
ä»¥å¤ç° issue ä¸­æè¿°çš„å¯åŠ¨è¶…æ—¶é—®é¢˜ã€‚
æ³¨æ„ï¼šè¯¥è„šæœ¬ä»…ç”¨äºæœ¬åœ°æµ‹è¯•ï¼Œä¸ä¼šå¯¹ç³»ç»Ÿå®‰å…¨æ€§æ„æˆå½±å“ï¼Œ
ä¸”é—®é¢˜æœ¬è´¨ä¸ºé›†ç¾¤å¯åŠ¨å¤±è´¥çš„ bugï¼Œè€Œéå®‰å…¨é£é™©ã€‚
"""

import subprocess
import sys

def start_local_cluster():
    try:
        # å‡è®¾å½“å‰å·¥ä½œç›®å½•ä¸º Kubernetes æºç æ ¹ç›®å½•ï¼Œ
        # ä¸” hack/local-up-cluster.sh è„šæœ¬å­˜åœ¨ä¸”å…·å¤‡æ‰§è¡Œæƒé™ã€‚
        # æ­¤å¤„ä¼ å…¥ '-O' å‚æ•°ç”¨äºè·³è¿‡ç¼–è¯‘æ­¥éª¤ã€‚
        print("å¼€å§‹å¯åŠ¨æœ¬åœ°é›†ç¾¤...")
        result = subprocess.run(['./hack/local-up-cluster.sh', '-O'], capture_output=True, text=True, timeout=300)
        print("å¯åŠ¨è„šæœ¬æ ‡å‡†è¾“å‡º:")
        print(result.stdout)
        print("å¯åŠ¨è„šæœ¬é”™è¯¯è¾“å‡º:")
        print(result.stderr)
    except subprocess.TimeoutExpired:
        print("å¯åŠ¨è¿›ç¨‹è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒæˆ–é›†ç¾¤æ—¥å¿—ä»¥è·å–è¯¦ç»†ä¿¡æ¯ã€‚")
    except Exception as e:
        print(f"è¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸ï¼š{e}")

def main():
    start_local_cluster()

if __name__ == '__main__':
    main()
```


**è§£é‡Šè¯´æ˜ï¼š**

æ­¤ Python è„šæœ¬åˆ©ç”¨ subprocess æ¨¡å—è°ƒç”¨ Kubernetes é¡¹ç›®ä¸­çš„ local-up-cluster.sh å¯åŠ¨è„šæœ¬ï¼Œæ¨¡æ‹Ÿ issue ä¸­çš„å¤ç°ç¯å¢ƒã€‚è„šæœ¬ä¸­è®¾ç½®äº†è¶…æ—¶æ—¶é—´ï¼Œæ•è·æ ‡å‡†è¾“å‡ºå’Œé”™è¯¯è¾“å‡ºï¼Œæ–¹ä¾¿è§‚å¯Ÿé›†ç¾¤å¯åŠ¨è¿‡ç¨‹ä¸­æ˜¯å¦å‡ºç°ç±»ä¼¼â€œtimed out waiting for the condition on pods/coredns-...â€çš„é”™è¯¯ã€‚  
éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œissue æè¿°çš„é—®é¢˜å±äºé›†ç¾¤å¯åŠ¨å¤±è´¥ï¼ˆæ“ä½œæˆ–é…ç½®é—®é¢˜ï¼‰ï¼Œè€Œéå®‰å…¨æ¼æ´ï¼Œæ‰€ä»¥é£é™©è¯„çº§å®šä¹‰ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚å› æ­¤ï¼Œæœ¬è„šæœ¬ä»…ç”¨äºæ¨¡æ‹Ÿç¯å¢ƒé—®é¢˜ï¼Œä¸æ¶‰åŠä»»ä½•å®‰å…¨é£é™©æˆ–æ¼æ´å¤ç°ã€‚

---


## Issue #129475 projected serviceAccountToken do not honour defaultMode or readOnly: true (tested in 1.30)

- Issue é“¾æ¥ï¼š[#129475](https://github.com/kubernetes/kubernetes/issues/129475)

### Issue å†…å®¹

#### What happened?

We have a container which needs to start as root today (because we install packages, mount a docker socket and the like). But then we change uid to a lower privilege user for the rest of time. 

That user needs access to a projected serviceAccountToken to access another service. The user cannot read the file because its mode is rw-------.

So, we set defaultMode on the projected volume definition, but this has no effect.

We also had readOnly: true in the mount definition, and that seems to not have effect either, as the mode was still rw------- not r--------.

<details>

<summary>Sample Pod definitions</summary>

```
apiVersion: v1
kind: Pod
metadata:
  name: with-mode
  namespace: robertc-scratch
spec:
      containers:
      - command:
        - sleep
        - "604800"
        image: ubuntu
        imagePullPolicy: IfNotPresent
        name: test
        securityContext:
          allowPrivilegeEscalation: false
          runAsUser: 0
        volumeMounts:
        - mountPath: /secrets/token
          name: token
      volumes:
      - name: token
        projected:
          defaultMode: 292
          sources:
          - serviceAccountToken:
              path: token
---
apiVersion: v1
kind: Pod
metadata:
  name: with-readonly
  namespace: robertc-scratch
spec:
      containers:
      - command:
        - sleep
        - "604800"
        image: ubuntu
        imagePullPolicy: IfNotPresent
        name: test
        securityContext:
          allowPrivilegeEscalation: false
          runAsUser: 0
        volumeMounts:
        - mountPath: /secrets/token
          name: token
          readOnly: true
      volumes:
      - name: token
        projected:
          sources:
          - serviceAccountToken:
              path: token

```

</details>

The test I did in making a SSCCE was to run this:
```
kubectl --namespace robertc-scratch exec -ti with-mode -- ls -l /secrets/token/..data/
total 4
-rw------- 1 root root 1414 Jan  3 17:45 token
@rbtcollins âœ /workspaces/infrastructure (rbt/agent-permissions-3) $ kubectl --namespace robertc-scratch exec -ti with-mode -- ls -l /secrets/token/..data/
total 4
-rw------- 1 root root 1414 Jan  3 17:45 token
```

We checked the [documentation ](https://kubernetes.io/docs/concepts/storage/projected-volumes/#introduction) and it describes defaultMode with no restrictions on its relevance to difference sources. The API reference is no more useful.




#### What did you expect to happen?

I expected the defaultMode setting to apply to the serviceAccountToken. All the same reasons one might want a different mode for a projected secret or config map also apply to a projected serviceAccountToken.

#### How can we reproduce it (as minimally and precisely as possible)?

See above.

#### Anything else we need to know?

Probably not ;) .

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.27.16
Kustomize Version: v5.0.1
Server Version: v1.30.5-gke.1699000
```

</details>


#### Cloud provider

<details>
Google GKE
</details>


#### OS version

<details>

I don't have access to the node OS itself, sorry.

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†ä¸€ä¸ª Kubernetes ä¸­ projected volume çš„é—®é¢˜ï¼Œå…·ä½“æ¥è¯´ï¼šå½“ä½¿ç”¨ serviceAccountToken ä½œä¸º projected volume æ—¶ï¼Œæ— è®ºåœ¨å·å®šä¹‰ä¸­è®¾ç½® defaultMode æˆ–åœ¨ volumeMounts ä¸­è®¾ç½® readOnlyï¼Œéƒ½æ²¡æœ‰æ”¹å˜æ–‡ä»¶çš„å®é™…æƒé™ï¼ˆä¾ç„¶æ˜¯ 600ï¼Œå³ rw-------ï¼‰ã€‚è¿™ä¸ªé—®é¢˜å±äº Kubernetes åŠŸèƒ½ä¸Šçš„ç¼ºé™·æˆ–ä¸ä¸€è‡´æ€§ï¼Œå¯¼è‡´åœ¨é¢„æœŸä¸­å¸Œæœ›ä»¥æ›´å®½æ¾æƒé™ï¼ˆä¾‹å¦‚åªè¯»ï¼‰æŒ‚è½½ token æ–‡ä»¶çš„åœºæ™¯ä¸‹ï¼Œä½æƒé™ç”¨æˆ·æ— æ³•è¯»å–è¯¥ tokenï¼Œä»è€Œä½¿å®¹å™¨å†…é™æƒæ“ä½œå—åˆ°å½±å“ã€‚

ä»å®‰å…¨è§’åº¦çœ‹ï¼Œæœ¬ Issue å¹¶ä¸å­˜åœ¨æ½œåœ¨çš„å®‰å…¨æ”»å‡»é£é™©ã€‚ä¾‹å¦‚ï¼š
1. Issue ä¸­è®¨è®ºçš„é—®é¢˜æ˜¯å…³äºæ–‡ä»¶æƒé™çš„é»˜è®¤è®¾ç½®ä¸ç”Ÿæ•ˆï¼Œå¹¶éå…è®¸æ”»å‡»è€…é€šè¿‡æœªæˆæƒè¯»å–æˆ–å†™å…¥æ•æ„Ÿä¿¡æ¯è¿›è¡Œæ”»å‡»ï¼›
2. æ–‡ä»¶æƒé™è®¾ç½®ä¸º 600 å®é™…ä¸Šæ˜¯æ›´ä¸ºä¸¥æ ¼çš„é™åˆ¶ï¼Œåè€Œé˜²æ­¢äº†ä¸å½“çš„è®¿é—®ï¼›
3. æ­¤é—®é¢˜å¯¼è‡´çš„å½±å“ä¸»è¦æ˜¯åŠŸèƒ½æ€§å’Œä½¿ç”¨ä¸Šçš„ä¸ä¾¿ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚å› æ­¤ï¼Œè¯¥é—®é¢˜ä¸æ„æˆå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰é«˜é£é™©å®‰å…¨é—®é¢˜ã€‚

åŸºäºä¸Šè¿°åˆ†æï¼Œå°½ç®¡è¯¥é—®é¢˜å¯èƒ½ä¼šå½±å“æŸäº›æ­£å¸¸ä½¿ç”¨åœºæ™¯ï¼Œä½†ä»å®‰å…¨é£é™©è§’åº¦åˆ¤æ–­ï¼Œé—®é¢˜æœ¬èº«ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œè€Œå±äºåŠŸèƒ½ Bug æˆ–è¡Œä¸ºä¸ç¬¦åˆé¢„æœŸçš„ç¼ºé™·ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
æ­¤è„šæœ¬ä½¿ç”¨ Kubernetes Python å®¢æˆ·ç«¯ï¼Œé€šè¿‡æ‰§è¡Œ Pod å†…çš„ ls å‘½ä»¤æ£€æµ‹
projected serviceAccountToken æ–‡ä»¶çš„æƒé™æ˜¾ç¤ºæƒ…å†µï¼Œç”¨äºå¤ç° Issue ä¸­æè¿°çš„é—®é¢˜ï¼š
å³è®¾ç½®äº† defaultMode æˆ– readOnly åï¼Œä¾ç„¶æ˜¾ç¤º token æ–‡ä»¶çš„æƒé™ä¸º rw------- (600)ã€‚
æ³¨æ„ï¼šè¯¥è„šæœ¬ä»…ç”¨äºç ”ç©¶å’Œæœ¬åœ°æµ‹è¯•ï¼Œéœ€ç¡®ä¿ kubeconfig åœ¨é»˜è®¤ä½ç½®ã€‚
"""

import sys
import time
from kubernetes import client, config, stream

def main():
    # åŠ è½½é»˜è®¤ kubeconfig
    try:
        config.load_kube_config()
    except Exception as e:
        print("åŠ è½½ kubeconfig å¤±è´¥: {}".format(e))
        sys.exit(1)

    # å®šä¹‰å‘½åç©ºé—´å’Œ Pod åç§°ï¼Œæ ¹æ® issue ç¤ºä¾‹ä½¿ç”¨
    namespace = "robertc-scratch"
    pod_name = "with-mode"  # ä¹Ÿå¯ä»¥å°è¯• with-readonly å¯¹æ¯”

    # åˆ›å»º API å®ä¾‹
    core_v1 = client.CoreV1Api()

    # æ£€æŸ¥ Pod æ˜¯å¦å­˜åœ¨
    try:
        pod = core_v1.read_namespaced_pod(name=pod_name, namespace=namespace)
    except client.exceptions.ApiException as e:
        print("è¯»å–Podæ—¶å‡ºé”™: {}".format(e))
        sys.exit(1)

    # ç­‰å¾… Pod å¤„äº Running çŠ¶æ€
    print("ç­‰å¾… Pod [{}] å¤„äº Running çŠ¶æ€...".format(pod_name))
    for i in range(30):
        pod = core_v1.read_namespaced_pod(name=pod_name, namespace=namespace)
        if pod.status.phase == "Running":
            break
        time.sleep(2)
    else:
        print("Pod æœªèƒ½åœ¨é¢„æœŸæ—¶é—´å†…è¿›å…¥ Running çŠ¶æ€")
        sys.exit(1)

    # ä½¿ç”¨ stream æ‰§è¡Œ exec å‘½ä»¤æŸ¥çœ‹ token æ–‡ä»¶çš„æƒé™
    exec_command = [
        '/bin/sh',
        '-c',
        'ls -l /secrets/token/..data/'
    ]

    try:
        print("å¼€å§‹æ‰§è¡Œ exec å‘½ä»¤ä»¥æŸ¥çœ‹ token æ–‡ä»¶æƒé™...")
        resp = stream.stream(core_v1.connect_get_namespaced_pod_exec,
                             pod_name,
                             namespace,
                             command=exec_command,
                             stderr=True, stdin=False,
                             stdout=True, tty=False)
        print("å‘½ä»¤è¾“å‡º:")
        print(resp)
    except Exception as e:
        print("æ‰§è¡Œ exec å‘½ä»¤æ—¶å‡ºé”™: {}".format(e))
        sys.exit(1)

if __name__ == "__main__":
    main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. è„šæœ¬é¦–å…ˆä½¿ç”¨ Kubernetes Python å®¢æˆ·ç«¯åŠ è½½é»˜è®¤çš„ kubeconfigï¼Œè¿æ¥åˆ° Kubernetes é›†ç¾¤ï¼›
2. æŒ‡å®šäº† issue ä¸­æ ·ä¾‹ Pod çš„å‘½åç©ºé—´ "robertc-scratch" å’Œ Pod åç§° "with-mode"ï¼ˆä¹Ÿå¯ä»¥ä½¿ç”¨ "with-readonly" è¿›è¡Œå¯¹æ¯”æµ‹è¯•ï¼‰ï¼›
3. è„šæœ¬ç­‰å¾… Pod è¿›å…¥ Running çŠ¶æ€ï¼ˆæœ€å¤šç­‰å¾… 30 æ¬¡è½®è¯¢ï¼Œæ¯æ¬¡ 2 ç§’ï¼‰ï¼Œä»¥ç¡®ä¿ Pod å¯ç”¨åç»§ç»­æ‰§è¡Œï¼›
4. ä½¿ç”¨ stream æ¨¡å—æ‰§è¡Œ Pod å†…çš„å‘½ä»¤ï¼Œåˆ—å‡ºæŒ‚è½½è·¯å¾„ /secrets/token/..data/ ä¸‹æ–‡ä»¶çš„è¯¦ç»†æƒé™ä¿¡æ¯ï¼›
5. æ ¹æ®è¾“å‡ºï¼Œå¤ç° issue ä¸­æè¿°çš„é—®é¢˜ï¼šå³ä½¿ç”¨æˆ·åœ¨ Pod å®šä¹‰ä¸­é…ç½®äº† defaultMode æˆ– readOnlyï¼Œå®é™… token æ–‡ä»¶çš„æƒé™ä»æ˜¯ rw

---


## Issue #129473 Pods stuck in Terminating state for >3 days; unable to delete via standard methods on GKE

- Issue é“¾æ¥ï¼š[#129473](https://github.com/kubernetes/kubernetes/issues/129473)

### Issue å†…å®¹

#### What happened?

We encountered an issue where four pods in our GKE cluster have been stuck in the `Terminating` state for over three days. Jobs created these pods have already been deleted. Despite our attempts to delete the pods using standard methods (e.g., `kubectl delete --force --grace-period=0`), they remain in this state.

Here is the pod status:
```bash
kubectl get pods
NAME                                                       READY   STATUS        RESTARTS   AGE
allocation-28910880-tzr87                       0/2     Terminating   0          3d
allocation-28912320-8mn4z                       0/2     Terminating   0          3d
allocation-28913760-4gsxh                       0/2     Terminating   0          3d
allocation-28916640-vwdj2                       0/2     Terminating   0          3d
```

Additional details:
- Each pod has the finalizer `batch.kubernetes.io/job-tracking` and a QoS class of `Burstable`.
- Attempts to remove the finalizers result in the following error:
  ```bash
  The Pod "allocation-event-sync-28912320-8mn4z" is invalid: spec.initContainers: Forbidden: pod updates may not add or remove containers
  ```
- One of the pods (`allocation-28912320-8mn4z`) was running on a node (`gke-primary-pool-bd44533c-aezp`) that has since been drained and deleted. However, the pod still references the deleted node while remaining in the `Terminating` state.

Attempts to resolve:
1. Force deletion with `kubectl delete --force --grace-period=0` and via `kubectl replace`.
2. Deleting via the `kubectl proxy` API (both deletion and replacement attempts).
3. Draining and deleting the node where one of the pods was running.
4. Attempting to modify the finalizers or QoS class, resulting in the above error.


#### What did you expect to happen?

The pods should be removed from the cluster once the Job is deleted, or at least be force-deletable using standard methods like kubectl delete.

#### How can we reproduce it (as minimally and precisely as possible)?

We cannot reproduce this issue at the moment. Creating a Job via the CLI or a YAML file and then deleting it does not result in the same stuck Terminating state. This appears to be a specific edge case that we can't relate to. 

#### Anything else we need to know?

The only thing we had in the cluster was that we had recently updated the nodes from version 1.27 to 1.28, but we think that was before the job deletion.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.13-gke.600
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
```

</details>


#### Cloud provider

<details>
Google cloud
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Container-Optimized OS"
ID=cos
PRETTY_NAME="Container-Optimized OS from Google"
HOME_URL="https://cloud.google.com/container-optimized-os/docs"
BUG_REPORT_URL="https://cloud.google.com/container-optimized-os/docs/resources/support-policy#contact_us"
GOOGLE_CRASH_ID=Lakitu
GOOGLE_METRICS_PRODUCT_ID=26
KERNEL_COMMIT_ID=3e0971e1551e88a5a9e615c239f034fd9fd8a423
VERSION=109
VERSION_ID=109
BUILD_ID=17800.309.13
$ uname -a
Linux gke-primary-pool-bd44533c-138r 6.1.100+ #1 SMP PREEMPT_DYNAMIC Sat Aug 10 14:21:56 UTC 2024 x86_64 Intel(R) Xeon(R) CPU @ 2.20GHz GenuineIntel GNU/Linux
```

</details>


#### Install tools

<details>
kubectl
gcloud
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ issue æè¿°çš„æ˜¯åœ¨ GKE é›†ç¾¤ä¸­ï¼Œéƒ¨åˆ† pod å‡ºç°å¼‚å¸¸çŠ¶æ€ï¼ˆé•¿æ—¶é—´å¤„äº Terminating çŠ¶æ€ï¼‰ï¼Œå¹¶ä¸”åœ¨åˆ é™¤æ—¶é‡åˆ°æœ‰å…³ finalizer çš„é—®é¢˜ã€‚æ•´ä¸ªé—®é¢˜è¿‡ç¨‹ä¸»è¦æ¶‰åŠ Kubernetes å¯¹ job å’Œ pod çš„èµ„æºæ¸…ç†ä¸çŠ¶æ€åŒæ­¥é—®é¢˜ï¼Œä»æè¿°æ¥çœ‹å¹¶ä¸å­˜åœ¨å¤–éƒ¨æ”»å‡»è€…åˆ©ç”¨è¯¥é—®é¢˜è¿›è¡Œæœªæˆæƒæ“ä½œã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸æˆ–ææƒç­‰å®‰å…¨é£é™©ã€‚é—®é¢˜æ›´åƒæ˜¯é›†ç¾¤ç®¡ç†æˆ–å¹³å°ç‰ˆæœ¬æ›´æ–°åé—ç•™çš„ bug æˆ–ä¸å¯é¢„æœŸè¡Œä¸ºï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
æ³¨æ„ï¼šè¯¥å¤ç°è„šæœ¬ç”¨äºè¯´æ˜å¦‚ä½•è§¦å‘ pods é•¿æ—¶é—´å¤„äº Terminating çŠ¶æ€çš„é—®é¢˜æ¨¡æ‹Ÿã€‚
ä½†æ ¹æ® issue æè¿°ï¼Œè¯¥é—®é¢˜ä¸»è¦ä¸ºå¹³å° bugï¼Œå¹¶éå®‰å…¨é£é™©ï¼Œå› æ­¤æœ¬è„šæœ¬ä»…ç”¨äºå±•ç¤ºå¦‚ä½•é€šè¿‡ Kubernetes API æ“ä½œ pod çš„ç”Ÿå‘½å‘¨æœŸï¼Œ
è€Œä¸ä¼šçœŸæ­£æ¨¡æ‹Ÿè¯¥é—®é¢˜ï¼Œå› ä¸ºè¯¥é—®é¢˜ç›®å‰æ— æ³•åœ¨éç‰¹å®šå¹³å°/ç¯å¢ƒå¤ç°ã€‚
"""

import time
from http.server import HTTPServer, BaseHTTPRequestHandler
from kubernetes import client, config
from kubernetes.client.rest import ApiException

def create_job(api_instance, namespace="default"):
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„ job ç¤ºä¾‹ï¼Œæ­¤ job ä¼šåˆ›å»ºä¸€ä¸ª pod
    job = client.V1Job(
        metadata=client.V1ObjectMeta(name="demo-job"),
        spec=client.V1JobSpec(
            template=client.V1PodTemplateSpec(
                metadata=client.V1ObjectMeta(labels={"app": "demo"}),
                spec=client.V1PodSpec(
                    containers=[client.V1Container(
                        name="busybox",
                        image="busybox",
                        command=["/bin/sh", "-c", "sleep 3600"]
                    )],
                    restart_policy="Never"
                )
            ),
            backoff_limit=0
        )
    )
    try:
        api_instance.create_namespaced_job(namespace=namespace, body=job)
        print("Job demo-job åˆ›å»ºæˆåŠŸ")
    except ApiException as e:
        print(f"åˆ›å»º Job æ—¶å‡ºç°å¼‚å¸¸: {e}")

def delete_job(api_instance, namespace="default"):
    # åˆ é™¤ jobï¼Œæ­¤æ“ä½œä¼šå°è¯•åˆ é™¤å¯¹åº”çš„ pod
    try:
        api_instance.delete_namespaced_job(
            name="demo-job",
            namespace=namespace,
            body=client.V1DeleteOptions(propagation_policy="Foreground")
        )
        print("Job demo-job åˆ é™¤è¯·æ±‚å·²å‘é€")
    except ApiException as e:
        print(f"åˆ é™¤ Job æ—¶å‡ºç°å¼‚å¸¸: {e}")

class SimpleHandler(BaseHTTPRequestHandler):
    def do_GET(self):
        self.send_response(200)
        self.end_headers()
        self.wfile.write(b"This is a demo HTTP server for security testing research.")

def run_http_server():
    server_address = ("", 8080)
    httpd = HTTPServer(server_address, SimpleHandler)
    print("HTTP æœåŠ¡å™¨å¯åŠ¨ï¼Œç›‘å¬ç«¯å£ 8080")
    httpd.serve_forever()

def main():
    # åŠ è½½é»˜è®¤ kubeconfig
    config.load_kube_config()
    batch_v1 = client.BatchV1Api()
    core_v1 = client.CoreV1Api()
    namespace = "default"

    # åˆ›å»ºä¸€ä¸ª jobï¼Œæ¨¡æ‹Ÿ pod çš„åˆ›å»º
    create_job(batch_v1, namespace)

    # ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œä½¿ pod æˆä¸º Running çŠ¶æ€
    time.sleep(10)

    # åˆ é™¤ jobï¼Œæ­¤æ—¶æŒ‰ç…§æ­£å¸¸æµç¨‹ï¼Œpod åº”è¯¥èƒ½è¢«åˆ é™¤
    delete_job(batch_v1, namespace)

    # å°è¯•è·å–ç›¸å…³ pod çš„çŠ¶æ€ï¼Œç›‘æ§å…¶åˆ é™¤æƒ…å†µ
    for _ in range(10):
        pods = core_v1.list_namespaced_pod(namespace, label_selector="app=demo")
        if not pods.items:
            print("æ‰€æœ‰ pod å·²åˆ é™¤")
            break
        else:
            for pod in pods.items:
                print(f"Pod {pod.metadata.name} çŠ¶æ€: {pod.status.phase}")
        time.sleep(5)

    # å¯åŠ¨ä¸€ä¸ªç®€å•çš„ HTTP æœåŠ¡å™¨ï¼ˆç›‘å¬ 8080 ç«¯å£ï¼‰
    run_http_server()

if __name__ == "__main__":
    main()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. æœ¬è„šæœ¬é¦–å…ˆä½¿ç”¨ Kubernetes Python Client åŠ è½½é»˜è®¤çš„ kubeconfig é…ç½®ï¼Œè¿æ¥åˆ°é›†ç¾¤ã€‚
2. åˆ›å»ºäº†ä¸€ä¸ªç®€å•çš„ Jobï¼Œè¯¥ Job çš„ pod è¿è¡Œä¸€ä¸ª sleep å‘½ä»¤ï¼Œæ¨¡æ‹Ÿå®é™…è¿è¡Œçš„ pod ç¯å¢ƒã€‚
3. åˆ é™¤è¯¥ Job æ—¶ï¼Œä½¿ç”¨ propagation_policy="Foreground"ï¼Œç¡®ä¿è¢«åˆ›å»ºçš„ pod ä¼šä¸€å¹¶åˆ é™¤ï¼›æ­£å¸¸æƒ…å†µä¸‹ï¼Œpod ä¼šé€€å‡ºå’Œæ¸…ç†ã€‚
4. è„šæœ¬ä¸­é‡‡ç”¨ç®€å•çš„è½®è¯¢æ¥ç›‘æ§ pod çŠ¶æ€å˜åŒ–ï¼ˆæ¨¡æ‹Ÿå¯¹ pod åˆ é™¤è¿‡ç¨‹çš„è§‚å¯Ÿï¼‰ã€‚
5. å¦å¤–ï¼Œæœ¬è„šæœ¬ä¸­è¿˜å¯åŠ¨äº†ä¸€ä¸ª HTTP æœåŠ¡å™¨ï¼Œç›‘å¬åœ¨ 8080 ç«¯å£ï¼›è¿™ä¸€éƒ¨åˆ†ä»…ç¬¦åˆé¢˜ç›®è¦æ±‚ï¼Œå±•ç¤ºå¦‚ä½•åˆ›å»ºä¸€ä¸ª HTTP æœåŠ¡å™¨ï¼Œå¹¶ä¸ä¸ pod å¡åœ¨ Terminating çŠ¶æ€çš„é—®é¢˜ç›´æ¥ç›¸å…³ã€‚
6. æ€»ä½“æ¥è¯´ï¼Œæ­¤è„šæœ¬ä»…ä½œä¸ºç ”ç©¶å’Œæµ‹è¯• Kubernetes èµ„æºæ“ä½œçš„ä¸€éƒ¨åˆ†ï¼Œå¹¶ä¸èƒ½åœ¨éç‰¹å®šç¯å¢ƒä¸‹å¤ç° issue ä¸­æåˆ°çš„ pods é•¿æ—¶é—´å¤„äº Terminating çŠ¶æ€çš„é—®é¢˜ï¼Œå› ä¸ºè¯¥é—®é¢˜å¯èƒ½ä¾èµ–äºç‰¹å®šå¹³å°ï¼ˆGKEï¼‰æˆ–ç¯å¢ƒçš„ bug å¼•èµ·ï¼Œå¹¶éå®‰å…¨æ¼æ´ã€‚

ç»¼ä¸Šï¼ŒåŸºäºä»¥ä¸Šåˆ†æï¼Œè¯¥ issue ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ— éœ€æä¾›é’ˆå¯¹å®‰å…¨æ¼æ´çš„å¤ç° POCï¼Œä»è€Œæœ¬é—®é¢˜ä¸å­˜åœ¨é«˜é£é™©å®‰å…¨é—®é¢˜ã€‚

---


## Issue #129462 OS version reported as not supported, but meets requirements

- Issue é“¾æ¥ï¼š[#129462](https://github.com/kubernetes/kubernetes/issues/129462)

### Issue å†…å®¹

#### What happened?

First, I ensured (and reconfigured grub) to use cgroups V2:
in /etc/default/grub,
GRUB_CMDLINE_LINUX="crashkernel=auto rd.lvm.lv=vg0/lv_root rhgb quiet systemd.unified_cgroup_hierarchy=1"

I then verified it is running cgroups V2:
[root@docktest01 ~]# cd /sys/fs/cgroup/
[root@docktest01 cgroup]# ll
total 0
-r--r--r--   1 root root 0 Dec 19 17:17 cgroup.controllers
-rw-r--r--   1 root root 0 Dec 19 17:18 cgroup.max.depth
-rw-r--r--   1 root root 0 Dec 19 17:18 cgroup.max.descendants
-rw-r--r--   1 root root 0 Dec 19 17:17 cgroup.procs
-r--r--r--   1 root root 0 Dec 19 17:18 cgroup.stat
-rw-r--r--   1 root root 0 Dec 19 17:18 cgroup.subtree_control
-rw-r--r--   1 root root 0 Dec 19 17:18 cgroup.threads
-rw-r--r--   1 root root 0 Dec 19 17:18 cpu.pressure
-r--r--r--   1 root root 0 Dec 19 17:18 cpuset.cpus.effective
-r--r--r--   1 root root 0 Dec 19 17:18 cpuset.mems.effective
-r--r--r--   1 root root 0 Dec 19 17:18 cpu.stat
drwxr-xr-x   2 root root 0 Dec 19 17:17 init.scope
-rw-r--r--   1 root root 0 Dec 19 17:18 io.pressure
-r--r--r--   1 root root 0 Dec 19 17:18 io.stat
-r--r--r--   1 root root 0 Dec 19 17:18 memory.numa_stat
-rw-r--r--   1 root root 0 Dec 19 17:18 memory.pressure
--w-------   1 root root 0 Dec 19 17:18 memory.reclaim
-r--r--r--   1 root root 0 Dec 19 17:18 memory.stat
drwxr-xr-x 120 root root 0 Dec 19 17:17 system.slice
drwxr-xr-x   4 root root 0 Dec 31 15:05 user.slice

And the kernel version is:
[root@docktest01 cgroup]# uname -r
4.18.0-477.58.1.el8_8.x86_64

However, when I run kubadm init it tells me that this kernel version is not supported, but then claims that the version it is running meets the requirments:
[root@docktest01 ~]# kubeadm init --pod-network-cidr 10.62.76.0/24 .
.
error execution phase preflight: [preflight] Some fatal errors occurred:
        [ERROR SystemVerification]: kernel release 4.18.0-477.58.1.el8_8.x86_64 is unsupported. Recommended LTS version from the 4.x series is 4.19. Any 5.x or 6.x versions are also supported. For cgroups v2 support, the minimal version is 4.15 and the recommended version is 5.8+

So, while on one hand it states the minimum kernel version for cgroups V2 support is 4.15+, it is failing with a kernel version of 4.18, and via verification, the system is indeed using cgroups V2. 

So, where is the problem? When I first had this error, I found that the system was indeed using cgroups V1, that's why I reconfigured grub, rebuilt the config and rebooted the system, only to show that it was then using V2. But am getting this incompatibility error. 

Is it finding that the system is still running cgroups V1, not V2, and therefore thinking it needs to be on a 4.19 kernel? Also, since RH has not released a 4.19 kernel, is my only option an upgrade to RHEL 9? 

Please assist. 


#### What did you expect to happen?

I expected kubeadm to initialize the master node. 

#### How can we reproduce it (as minimally and precisely as possible)?

Repeat steps taken on RHEL 8.8, with kernel 4.18 as described above. 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# NAME="Red Hat Enterprise Linux"
VERSION="8.8 (Ootpa)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="8.8"
PLATFORM_ID="platform:el8"
PRETTY_NAME="Red Hat Enterprise Linux 8.8 (Ootpa)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:redhat:enterprise_linux:8::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8"
BUG_REPORT_URL="https://bugzilla.redhat.com/"

REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 8"
REDHAT_BUGZILLA_PRODUCT_VERSION=8.8
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="8.8"
$ uname -a
# Linux docktest01 4.18.0-477.58.1.el8_8.x86_64 #1 SMP Wed May 22 13:46:53 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
yum/dnf
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°çš„æ˜¯åœ¨RHEL 8.8å¹³å°ä¸Šï¼Œç”¨æˆ·åœ¨æŒ‰ç…§æ–‡æ¡£è¦æ±‚åˆ‡æ¢åˆ°cgroups V2åï¼Œè¿è¡Œkubeadm initæ—¶å‡ºç°å†…æ ¸ç‰ˆæœ¬ä¸åŒ¹é…çš„é”™è¯¯ã€‚è™½ç„¶å®é™…ç³»ç»Ÿå·²ç»å¯ç”¨äº†cgroups V2ï¼Œä½†kubeadmä»ç„¶æŠ¥é”™è¯´å½“å‰å†…æ ¸4.18ä¸å—æ”¯æŒï¼Œå¹¶å»ºè®®ä½¿ç”¨4.19æˆ–æ›´é«˜ç‰ˆæœ¬ã€‚æ•´ä½“æ¥çœ‹ï¼Œè¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„å…¼å®¹æ€§æˆ–é…ç½®æ£€æµ‹é—®é¢˜ï¼Œæ¶‰åŠçš„æ˜¯ç‰ˆæœ¬åˆ¤æ–­é€»è¾‘ä¸æ¨èå€¼ä¸ç¬¦çš„é—®é¢˜ï¼Œå¹¶ä¸å­˜åœ¨è¿œç¨‹åˆ©ç”¨ã€æƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€æ‹’ç»æœåŠ¡ç­‰å®‰å…¨éšæ‚£ã€‚

æ ¹æ®æè¿°ï¼Œè¯¥Issueæ²¡æœ‰æš´éœ²æ•æ„Ÿä¿¡æ¯ã€ä¹Ÿä¸å­˜åœ¨ä¸å½“æ“ä½œæˆ–é…ç½®é”™è¯¯å¯¼è‡´çš„å®‰å…¨é£é™©ï¼Œå› æ­¤ä»å®‰å…¨è§’åº¦è¯„ä¼°ï¼Œè¯¥é—®é¢˜ä»…æ˜¯åº”ç”¨çš„é€»è¾‘åˆ¤æ–­é—®é¢˜ï¼Œå¹¶ä¸å±äºå®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
def main():
    # æ­¤è„šæœ¬ä»…ç”¨äºè¯´æ˜è¯¥Issueé—®é¢˜ä¸ºå…¼å®¹æ€§å’Œé…ç½®æ£€æµ‹é—®é¢˜ï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©çš„å¤ç°ã€‚
    print("è¯¥Issueä¸»è¦æ˜¯å†…æ ¸ç‰ˆæœ¬å’Œcgroupsè®¾ç½®çš„å…¼å®¹æ€§é—®é¢˜ï¼Œå¹¶ä¸æ„æˆå®‰å…¨é£é™©ã€‚")
    print("è¯·ç¡®ä¿ç³»ç»Ÿéµå¾ªKubeadmå®˜æ–¹æ–‡æ¡£çš„è¦æ±‚ï¼Œè‹¥é‡åˆ°ç‰ˆæœ¬åˆ¤æ–­é—®é¢˜ï¼Œå¯è€ƒè™‘å‡çº§å†…æ ¸ç‰ˆæœ¬æˆ–å’¨è¯¢ç›¸å…³æ”¯æŒã€‚")
    
if __name__ == "__main__":
    main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥å¤ç°è„šæœ¬å¹¶ä¸æ¨¡æ‹Ÿæˆ–è§¦å‘ä»»ä½•å®‰å…¨æ¼æ´ï¼Œè€Œæ˜¯è¾“å‡ºè¯´æ˜ä¿¡æ¯ï¼Œè¡¨æ˜å½“å‰Issueçš„é—®é¢˜å±äºç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚è„šæœ¬è¿è¡Œåä¼šæç¤ºç”¨æˆ·è¯¥Issueä¸å­˜åœ¨å®‰å…¨éšæ‚£ï¼Œä»è€Œå¼•å¯¼ä½¿ç”¨è€…å…³æ³¨ç³»ç»Ÿå…¼å®¹æ€§å’Œè½¯ä»¶ç‰ˆæœ¬ç­‰é—®é¢˜ï¼Œæ— éœ€æ‹…å¿§å®‰å…¨æ€§é—®é¢˜ã€‚

---


## Issue #129456 Setting properties explicitly to `null` does not work without specifying `--server-side`

- Issue é“¾æ¥ï¼š[#129456](https://github.com/kubernetes/kubernetes/issues/129456)

### Issue å†…å®¹

#### What happened?

When running `kubectl apply` without `--server-side`, it is not possible to explicitly set a resource property to `null`.

Until Kubernetes 1.31 this was possible when a resource was created (not when it was patched), I think this behavior was removed by https://github.com/kubernetes/kubernetes/pull/125646.

My use case:
I have a CRD that specifies a default value for a property but allows it to be null: a value for `default` is specified and `nullable` is set to `true` in the CRD definition. So a user can either omit the property (`default` value should be used) or explicitly set it to `null` (then it should be `null`).
When I create a resource of that kind by running `kubectl apply` without `--server-side`, it is not possible to set the value to `null`, the property will always be set to the default value (which does not match the behavior described in the [documentation](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#defaulting-and-nullable)).

Both for creating and patching, we need to think about whether the property should have the same value after running `kubectl apply`, no matter if `--server-side` is provided or not (currently the value depends on the flag being present).

#### What did you expect to happen?

The property's value should be `null` if the user explcitly sets it to `null` and the CRD allows it.

#### How can we reproduce it (as minimally and precisely as possible)?

Create the CRD:
```
cat <<EOF | kubectl apply -f -
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: fooclusters.example.tech
spec:
  group: example.tech
  names:
    kind: FooCluster
    listKind: FooClusterList
    plural: fooclusters
    shortNames:
    - foo
    singular: foocluster
  scope: Namespaced
  versions:
  - name: v1alpha1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        required:
        - spec
        type: object
        properties:
          spec:
            type: object
            properties:
              bar:
                type: string
                nullable: true
                default: "defaultvalue"
EOF
```

Create the resource:
```
cat <<EOF | kubectl apply -f -
apiVersion: example.tech/v1alpha1
kind: FooCluster
metadata:
  name: myfoocluster
spec:
  bar: null
EOF
```

Try running this with and without `--server-side` and check the property's value with:
```
kubectl get fooclusters.example.tech myfoocluster -o jsonpath='{.spec.bar}'
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.2
Kustomize Version: v5.4.2
Server Version: v1.31.0
```

</details>


#### Cloud provider

<details>
Local Kind cluster
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†ä½¿ç”¨ kubectl apply åˆ›å»ºæˆ–æ›´æ–°è‡ªå®šä¹‰èµ„æºæ—¶ï¼Œå½“æ˜¾å¼å°†æŸä¸ªå±æ€§è®¾ç½®ä¸º nullï¼ˆè€Œè¯¥å±æ€§åŒæ—¶åœ¨ CRD ä¸­é…ç½®äº†é»˜è®¤å€¼ä¸”å…è®¸ä¸º nullï¼‰æ—¶ï¼Œä¸æŒ‡å®š --server-side å‚æ•°ä¼šå¯¼è‡´è¯¥å±æ€§è¢«èµ‹äºˆé»˜è®¤å€¼ï¼Œè€Œéä¿æŒ null çš„é—®é¢˜ã€‚è¿™å±äº Kubernetes å®¢æˆ·ç«¯ä¸æœåŠ¡ç«¯è¡Œä¸ºä¸ä¸€è‡´çš„åŠŸèƒ½ç¼ºé™·é—®é¢˜ï¼Œå¹¶æœªæ¶‰åŠæ•æ„Ÿæ•°æ®æ³„éœ²ã€ä¸å½“é…ç½®ã€å‘½ä»¤æ‰§è¡Œã€ææƒã€å®¹å™¨é€ƒé€¸æˆ–æ‹’ç»æœåŠ¡ç­‰ç›´æ¥çš„å®‰å…¨é£é™©ã€‚æ•´ä¸ªé—®é¢˜åæ˜ çš„æ˜¯èµ„æºé»˜è®¤å€¼å¤„ç†æ–¹é¢çš„ bugï¼Œè€Œéæ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# ç”±äºè¯¥ Issue ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œå› æ­¤ä¸éœ€è¦ç”Ÿæˆé’ˆå¯¹å®‰å…¨æ¼æ´çš„å¤ç°POCè„šæœ¬ã€‚
# æ­¤å¤„æä¾›ä»…ç”¨äºéªŒè¯é—®é¢˜å¤ç°çš„ç¤ºä¾‹è„šæœ¬ï¼Œä¸»è¦ä¸ºäº†åœ¨æœ¬åœ°éªŒè¯è¯¥åŠŸèƒ½ç¼ºé™·çš„å­˜åœ¨ï¼Œ
# ä½†è¯¥è„šæœ¬ä¸ç”¨äºå®‰å…¨æ”»å‡»æˆ–å…¶å®ƒéæ³•ç”¨é€”ã€‚
import subprocess
import time

def create_crd():
    crd_yaml = """
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: fooclusters.example.tech
spec:
  group: example.tech
  names:
    kind: FooCluster
    listKind: FooClusterList
    plural: fooclusters
    shortNames:
    - foo
    singular: foocluster
  scope: Namespaced
  versions:
  - name: v1alpha1
    served: true
    storage: true
    schema:
      openAPIV3Schema:
        required:
        - spec
        type: object
        properties:
          spec:
            type: object
            properties:
              bar:
                type: string
                nullable: true
                default: "defaultvalue"
"""
    with open("crd.yaml", "w") as f:
        f.write(crd_yaml)
    subprocess.run(["kubectl", "apply", "-f", "crd.yaml"], check=True)

def create_resource():
    resource_yaml = """
apiVersion: example.tech/v1alpha1
kind: FooCluster
metadata:
  name: myfoocluster
spec:
  bar: null
"""
    with open("resource.yaml", "w") as f:
        f.write(resource_yaml)
    subprocess.run(["kubectl", "apply", "-f", "resource.yaml"], check=True)

def get_resource_bar(use_server_side=False):
    cmd = ["kubectl", "get", "fooclusters.example.tech", "myfoocluster", "-o", "jsonpath={.spec.bar}"]
    if use_server_side:
        cmd.insert(2, "--server-side")
    result = subprocess.run(cmd, capture_output=True, text=True)
    return result.stdout.strip()

def main():
    print("åˆ›å»º CRD...")
    create_crd()
    # ç­‰å¾…ä¸€ä¼šå„¿ç¡®ä¿ CRD ç”Ÿæ•ˆ
    time.sleep(2)

    print("åˆ›å»ºèµ„æº...")
    create_resource()
    # ç­‰å¾…ä¸€ä¼šå„¿ç¡®ä¿èµ„æºåˆ›å»ºå®Œæ¯•
    time.sleep(2)

    print("æŸ¥è¯¢èµ„æºï¼ˆä¸ä½¿ç”¨ --server-sideï¼‰ï¼š")
    normal_value = get_resource_bar(use_server_side=False)
    print("ç»“æœï¼š", normal_value)

    print("æŸ¥è¯¢èµ„æºï¼ˆä½¿ç”¨ --server-sideï¼‰ï¼š")
    server_side_value = get_resource_bar(use_server_side=True)
    print("ç»“æœï¼š", server_side_value)

if __name__ == "__main__":
    main()
```


**è§£é‡Šè¯´æ˜ï¼š**

æ­¤è„šæœ¬ç”¨äºéªŒè¯ Issue ä¸­æè¿°çš„ç°è±¡ï¼šå½“æ˜¾å¼æŠŠå±æ€§ bar è®¾ç½®ä¸º null æ—¶ï¼Œåœ¨ä¸ä½¿ç”¨ --server-side å‚æ•°çš„æƒ…å†µä¸‹ï¼Œkubectl apply ç«Ÿç„¶å°†è¯¥å±æ€§èµ‹äºˆäº† CRD å®šä¹‰ä¸­çš„é»˜è®¤å€¼ "defaultvalue"ï¼Œè€Œä½¿ç”¨ --server-side å‚æ•°æ—¶å¯èƒ½è¡¨ç°ä¸åŒã€‚è„šæœ¬é¦–å…ˆåˆ›å»º CRDï¼Œç„¶ååˆ›å»ºä¸€ä¸ª FooCluster èµ„æºï¼Œæ¥ç€åˆ†åˆ«é€šè¿‡ä¸å¸¦å’Œå¸¦ --server-side å‚æ•°è°ƒç”¨ kubectl get æ£€æŸ¥èµ„æºä¸­ spec.bar çš„å€¼ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¯¥è„šæœ¬ä»…ç”¨äºéªŒè¯åŠŸèƒ½ç¼ºé™·ï¼Œé—®é¢˜æœ¬èº«ä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œå› æ­¤ä¸æ„æˆå®‰å…¨é£é™©æˆ–æ¼æ´æ”»åˆ©ç”¨ã€‚

---


## Issue #129453 can MountPath contain  ":" ?

- Issue é“¾æ¥ï¼š[#129453](https://github.com/kubernetes/kubernetes/issues/129453)

### Issue å†…å®¹

#### What happened?

https://github.com/kubernetes/kubernetes/blob/master/pkg/apis/core/types.go#L2110-L2113
```
	RecursiveReadOnly *RecursiveReadOnlyMode
	// Required. If the path is not an absolute path (e.g. some/path) it
	// will be prepended with the appropriate root prefix for the operating
	// system.  On Linux this is '/', on Windows this is 'C:\'.
	MountPath string
```
but
https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/api/core/v1/types.go#L2283
```
	// Path within the container at which the volume should be mounted.  Must
	// not contain ':'.
	MountPath string `json:"mountPath" protobuf:"bytes,3,opt,name=mountPath"`
```
some people use mountpath:/etc/localtime:ro (k8s should use readonly:true)  will not work on containerd but work on docker.

#### What did you expect to happen?

mountPath  can't contain  ":" 

#### How can we reproduce it (as minimally and precisely as possible)?

set mountpath:/etc/localtime:ro 

#### Anything else we need to know?

_No response_

#### Kubernetes version

any

#### Cloud provider

no 

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°çš„æ˜¯ Kubernetes ä¸­å…³äº MountPath é…ç½®çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼ŒIssue æŒ‡å‡ºåœ¨ Kubernetes çš„ API å®šä¹‰ä¸­å¯¹äº MountPath çš„è¦æ±‚å­˜åœ¨ä¸ä¸€è‡´ï¼Œåœ¨æŸå¤„å…è®¸åŒ…å«å†’å·ï¼Œè€Œåœ¨å¦å¤„æ˜ç¡®å®šä¹‰ä¸å…è®¸åŒ…å«å†’å·ã€‚è™½ç„¶è¿™å¯èƒ½å¯¼è‡´ç”¨æˆ·åœ¨æŸäº›å®¹å™¨è¿è¡Œæ—¶ï¼ˆä¾‹å¦‚ containerd å’Œ dockerï¼‰ä¸‹é‡åˆ°æŒ‚è½½è·¯å¾„é…ç½®é—®é¢˜ï¼Œä½†è¯¥é—®é¢˜ä¸»è¦å±äºé…ç½®å’Œå…¼å®¹æ€§é—®é¢˜ï¼Œè€Œéå®‰å…¨é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
#!/usr/bin/env python3
"""
è¯¥è„šæœ¬ç”¨äºæ¨¡æ‹Ÿ Kubernetes ä¸­ mountPath åŒ…å«å†’å·çš„é…ç½®é—®é¢˜ï¼Œ
ä½†ç”±äºè¯¥é—®é¢˜å±äºé…ç½®ä¸ä¸€è‡´æ€§é—®é¢˜è€Œéå®‰å…¨é—®é¢˜ï¼Œ
å› æ­¤è¯¥è„šæœ¬ä»…ç”¨äºæ¼”ç¤ºå¦‚ä½•æ£€æµ‹æŒ‚è½½è·¯å¾„ä¸­æ˜¯å¦åŒ…å«éæ³•å†’å·ã€‚
"""

import http.server
import socketserver
import json
import sys

PORT = 8080

def validate_mount_path(mount_path: str) -> bool:
    """
    æ£€æŸ¥ mountPath æ˜¯å¦åŒ…å«éæ³•çš„å†’å·å­—ç¬¦
    æ ¹æ® Kubernetes çš„è¦æ±‚ï¼ŒmountPath ä¸åº”åŒ…å«å†’å· (:)ã€‚
    è¿”å› True è¡¨ç¤ºæŒ‚è½½è·¯å¾„åˆæ³•ï¼ŒFalse è¡¨ç¤ºä¸åˆæ³•ã€‚
    """
    if ":" in mount_path:
        return False
    return True

class RequestHandler(http.server.SimpleHTTPRequestHandler):
    def do_POST(self):
        # è¯»å–è¯·æ±‚ä½“æ•°æ®
        content_length = int(self.headers.get('Content-Length', 0))
        post_data = self.rfile.read(content_length)
        try:
            data = json.loads(post_data)
        except json.JSONDecodeError:
            self.send_response(400)
            self.end_headers()
            self.wfile.write(b'Invalid JSON')
            return

        mount_path = data.get("mountPath", "")
        if not isinstance(mount_path, str):
            self.send_response(400)
            self.end_headers()
            self.wfile.write(b'mountPath must be a string')
            return

        # æ ¡éªŒ mountPath æ˜¯å¦åŒ…å«å†’å·
        if validate_mount_path(mount_path):
            result = {"valid": True, "message": "mountPath is valid"}
        else:
            result = {"valid": False, "message": "mountPath contains invalid character ':'"}

        self.send_response(200)
        self.send_header('Content-Type', 'application/json')
        self.end_headers()
        self.wfile.write(json.dumps(result).encode('utf-8'))

def run_http_server():
    handler = RequestHandler
    with socketserver.TCPServer(("", PORT), handler) as httpd:
        print(f"HTTP server running on port {PORT}")
        try:
            httpd.serve_forever()
        except KeyboardInterrupt:
            print("Server stopped by user.")
            httpd.server_close()

if __name__ == "__main__":
    run_http_server()
```


**è§£é‡Šè¯´æ˜ï¼š**

1. æœ¬è„šæœ¬å¯åŠ¨äº†ä¸€ä¸ªç®€æ˜“çš„ HTTP æœåŠ¡å™¨ï¼Œç›‘å¬åœ¨æœ¬åœ° 8080 ç«¯å£ã€‚
2. å½“æœåŠ¡å™¨æ¥æ”¶åˆ° POST è¯·æ±‚æ—¶ï¼Œè§£æè¯·æ±‚ä½“ä¸­çš„ JSON æ•°æ®ï¼Œå¹¶æå– "mountPath" å­—æ®µã€‚
3. åˆ©ç”¨ validate_mount_path å‡½æ•°åˆ¤æ–­ä¼ å…¥çš„ mountPath æ˜¯å¦åŒ…å«å†’å·å­—ç¬¦ï¼ˆ':'ï¼‰ã€‚è‹¥åŒ…å«åˆ™è¿”å›ä¸åˆæ³•çš„ä¿¡æ¯ï¼Œå¦åˆ™è¿”å›åˆæ³•ä¿¡æ¯ã€‚
4. è„šæœ¬ä»…ç”¨äºæ¨¡æ‹Ÿæ£€æµ‹é…ç½®é¡¹ä¸­çš„æ ¼å¼é—®é¢˜ï¼Œå¹¶éç”¨äºå¤ç°å®‰å…¨æ¼æ´ï¼Œå› ä¸ºè¯¥ Issue çš„é—®é¢˜å±äºé…ç½®ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚
5. ç”¨æˆ·å¯ä»¥é€šè¿‡å‘é€åŒ…å« mountPath å­—æ®µçš„ JSON è¯·æ±‚æ¥æµ‹è¯•è¯¥æ£€æµ‹é€»è¾‘ï¼Œä¾‹å¦‚ï¼š
   POST http://localhost:8080 å¹¶å‘é€ {"mountPath": "mountpath:/etc/localtime:ro"}ï¼ŒæœåŠ¡å™¨ä¼šå›åº”è¯¥æŒ‚è½½è·¯å¾„ä¸åˆæ³•ã€‚

---


