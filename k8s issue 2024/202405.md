# Issue å®‰å…¨åˆ†ææŠ¥å‘Š

# ğŸš¨ å­˜åœ¨å®‰å…¨é£é™©çš„ Issues (7 ä¸ª)

## Issue #125220 AppArmor profile parser error

- Issue é“¾æ¥ï¼š[#125220](https://github.com/kubernetes/kubernetes/issues/125220)

### Issue å†…å®¹

#### What happened?

I0530 12:13:05.231090       1 loader.go:97] Polling /profiles every 30s
W0530 12:13:05.234334       1 loader.go:174] AppArmor parser error for /profiles/k8s-nginx in /etc/apparmor.d/tunables/etc at line 25: Could not open 'if'
W0530 12:13:05.234370       1 loader.go:144] Error reading /profiles/k8s-nginx: error reading profiles from /profiles/k8s-nginx: exit status 1

#### What did you expect to happen?

profile should be loaded without any issue. I am trying in Azure aks

#### How can we reproduce it (as minimally and precisely as possible)?

https://github.com/kubernetes/kubernetes/tree/master/test/images/apparmor-loader
tried above link to setups on Azure aks

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

@:/mnt/c/poc/AppArmor/apparmorAzure$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.0", GitCommit:"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d", GitTreeState:"clean", BuildDate:"2022-12-08T19:58:30Z", GoVersion:"go1.19.4", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.9", GitCommit:"1c9860e7360c3f8147ae068e867eaab73b4a6257", GitTreeState:"clean", BuildDate:"2024-04-12T23:21:51Z", GoVersion:"go1.20.12", Compiler:"gc", Platform:"linux/amd64"}

</details>


#### Cloud provider

<details>
Azure AKS
</details>


#### OS version

<details>

```
root@aks-devapp-25368695-vmss000000:/#  cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
root@aks-devapp-25368695-vmss000000:/# uname -a
Linux aks-devapp-25368695-vmss000000 5.15.0-1061-azure #70-Ubuntu SMP Wed Apr 3 02:05:58 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
root@aks-devapp-25368695-vmss000000:/#

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

å­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

**åŸå› åŠå¯èƒ½çš„å½±å“ï¼š**

åœ¨è¯¥Issueä¸­ï¼ŒAppArmoré…ç½®æ–‡ä»¶è§£æé”™è¯¯å¯¼è‡´é…ç½®æ–‡ä»¶æœªèƒ½æ­£ç¡®åŠ è½½ã€‚è¿™å¯èƒ½å¯¼è‡´åº”ç”¨ç¨‹åºæœªå—åˆ°é¢„æœŸçš„å®‰å…¨ç­–ç•¥ä¿æŠ¤ï¼Œä»¥é»˜è®¤æˆ–â€œunconfinedâ€çš„æ¨¡å¼è¿è¡Œã€‚æ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¿™ä¸€ç‚¹ï¼Œæ‰§è¡Œæœªå—é™åˆ¶çš„æ“ä½œï¼Œå¦‚è®¿é—®æ•æ„Ÿæ•°æ®ã€æ‰§è¡Œä»»æ„ä»£ç æˆ–æå‡æƒé™ç­‰ã€‚

ç”±äºAppArmoræœªèƒ½åº”ç”¨é¢„æœŸçš„é™åˆ¶ï¼Œåº”ç”¨ç¨‹åºçš„æ”»å‡»é¢å¢å¤§ï¼Œå¯èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œä»è€Œå¯¹ç³»ç»Ÿé€ æˆä¸¥é‡å½±å“ã€‚

**Proof of Conceptï¼š**

1. æŒ‰ç…§æä¾›çš„é“¾æ¥ï¼ˆhttps://github.com/kubernetes/kubernetes/tree/master/test/images/apparmor-loaderï¼‰åœ¨Azure AKSä¸Šéƒ¨ç½²AppArmoré…ç½®ã€‚
2. è§‚å¯Ÿåˆ°AppArmorè§£æå™¨åœ¨åŠ è½½`/profiles/k8s-nginx`é…ç½®æ–‡ä»¶æ—¶å‡ºé”™ï¼Œå…·ä½“é”™è¯¯ä¸ºï¼š

   ```
   W0530 12:13:05.234334       1 loader.go:174] AppArmor parser error for /profiles/k8s-nginx in /etc/apparmor.d/tunables/etc at line 25: Could not open 'if'
   ```

3. ç”±äºè§£æé”™è¯¯ï¼ŒAppArmoré…ç½®æœªèƒ½æ­£ç¡®åº”ç”¨ï¼Œç›®æ ‡åº”ç”¨ç¨‹åºå¯èƒ½åœ¨æœªå—é™åˆ¶çš„æ¨¡å¼ä¸‹è¿è¡Œã€‚
4. æ”»å‡»è€…å¯ä»¥åˆ©ç”¨åº”ç”¨ç¨‹åºç¼ºä¹AppArmoré™åˆ¶çš„æ¼æ´ï¼Œæ‰§è¡Œè¶…å‡ºé¢„æœŸæƒé™çš„æ“ä½œï¼Œä¾‹å¦‚è¯»å–ç³»ç»Ÿæ–‡ä»¶ã€è®¿é—®å…¶ä»–å®¹å™¨èµ„æºæˆ–æ‰§è¡Œæ¶æ„ä»£ç ã€‚

**CVSS è¯„åˆ†ï¼š**

æ ¹æ®CVSS 3.1æ ‡å‡†ï¼Œæ­¤æ¼æ´çš„è¯„åˆ†å¯èƒ½åœ¨é«˜é£é™©èŒƒå›´å†…ï¼Œå…·ä½“è¯„ä¼°å¦‚ä¸‹ï¼š

- **æ”»å‡»å‘é‡ï¼ˆAVï¼‰ï¼š** ç½‘ç»œï¼ˆNï¼‰
- **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰ï¼š** ä½ï¼ˆLï¼‰
- **æƒé™è¦æ±‚ï¼ˆPRï¼‰ï¼š** æ— ï¼ˆNï¼‰
- **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰ï¼š** æ— ï¼ˆNï¼‰
- **ä½œç”¨èŒƒå›´ï¼ˆSï¼‰ï¼š** æœªæ”¹å˜ï¼ˆUï¼‰æˆ–å·²æ”¹å˜ï¼ˆCï¼‰ï¼Œå–å†³äºå…·ä½“æƒ…å†µ
- **æœºå¯†æ€§å½±å“ï¼ˆCï¼‰ï¼š** é«˜ï¼ˆHï¼‰
- **å®Œæ•´æ€§å½±å“ï¼ˆIï¼‰ï¼š** é«˜ï¼ˆHï¼‰
- **å¯ç”¨æ€§å½±å“ï¼ˆAï¼‰ï¼š** é«˜ï¼ˆHï¼‰

ç»¼åˆè¯„åˆ†å¯èƒ½ä¸ºé«˜äº7.0ï¼Œå±äºé«˜é£é™©çº§åˆ«ã€‚

**å»ºè®®ï¼š**

- æ£€æŸ¥å¹¶ä¿®å¤AppArmoré…ç½®æ–‡ä»¶ä¸­çš„è¯­æ³•é”™è¯¯ï¼Œç¡®ä¿é…ç½®æ–‡ä»¶èƒ½å¤Ÿæ­£ç¡®è§£æå’ŒåŠ è½½ã€‚
- åœ¨éƒ¨ç½²ä¹‹å‰ï¼Œä½¿ç”¨`apparmor_parser`å·¥å…·å¯¹é…ç½®æ–‡ä»¶è¿›è¡Œè¯­æ³•éªŒè¯ã€‚
- å®æ–½é…ç½®æ–‡ä»¶çš„å®Œæ•´æ€§æ ¡éªŒï¼Œé˜²æ­¢é…ç½®æ–‡ä»¶åœ¨ä¼ è¾“æˆ–éƒ¨ç½²è¿‡ç¨‹ä¸­è¢«ç¯¡æ”¹ã€‚

---

## Issue #125053 ValidatingAdmissionPolicy objects have different runtime type compared to CRDValidationRules

- Issue é“¾æ¥ï¼š[#125053](https://github.com/kubernetes/kubernetes/issues/125053)

### Issue å†…å®¹

#### What happened?

Applied CRD:

```yaml
openAPIV3Schema:
        type: object
        properties:
          duration:
            type: string
            format: duration
            "x-kubernetes-validations":
            - rule: "self >= duration(\"60m\")"
              message: "duration must be at least 1 hour"
```

Created policy with following rule:

```yaml
  validations:
    - expression: "object.duration >= duration(\"2h\")"
      message: "duration must be at least 2 hours"
```

Applied instances of CRD:

```yaml
apiVersion: example.com/v1
kind: Thing
metadata:
  name: example-thing
duration: 2h
---
apiVersion: example.com/v1
kind: Thing
metadata:
  name: bad-thing
duration: 5m
```

Correctly received error from CRD rule for `bad-thing`, incorrectly received error for `good-thing` on policy due to missing overload for `>=` operator:

```console
â¯ kubectl apply -f ./instance.yaml
Error from server (Invalid): error when creating "./instance.yaml": things.example.com "example-thing" is forbidden: ValidatingAdmissionPolicy 'example-policy.example.com' with binding 'example-binding.example.com' denied request: expression 'object.duration >= duration("2h")' resulted in error: no such overload
Error from server (Invalid): error when creating "./instance.yaml": Thing.example.com "bad-thing" is invalid: duration: Invalid value: "string": duration must be at least 1 hour
```


This is because CRDValidationRules consider the schema when representing the object in CEL, while VAP directly exposes the unstructured serialization. i.e.  a `duration`-format `string` in a CRD is represented in CEL using a CEL `duration`, but in VAP it is represented using a `string`

https://github.com/kubernetes/kubernetes/blob/e1b0bc3d0a7fb89a1e60f4ec1ee34b10de22d00a/staging/src/k8s.io/apiserver/pkg/admission/plugin/cel/filter.go#L158-L174

https://github.com/kubernetes/kubernetes/blob/1453085b44ff96b10ab25b68ce34929829114521/staging/src/k8s.io/apiextensions-apiserver/pkg/apiserver/schema/cel/validation.go#L786-L804

#### What did you expect to happen?

VAP expressions and CRD rules to work the same way on the same objects in CEL

#### How can we reproduce it (as minimally and precisely as possible)?

https://gist.github.com/alexzielenski/db4faa6cb26a6b31dc967fa18dc79199

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.30

#### Cloud provider

N/A

#### OS version

Tested using kind 1.30

#### Install tools

N/A

#### Container runtime (CRI) and version (if applicable)

N/A

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

N/A

### åˆ†æç»“æœ

å­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

**åŸå› åˆ†æï¼š**

åœ¨Kubernetesä¸­ï¼ŒValidatingAdmissionPolicyï¼ˆVAPï¼‰å’ŒCRDValidationRulesåœ¨ä½¿ç”¨CELï¼ˆCommon Expression Languageï¼‰è¡¨è¾¾å¼æ—¶ï¼Œå¯¹å¯¹è±¡å±æ€§çš„ç±»å‹å¤„ç†æ–¹å¼å­˜åœ¨ä¸ä¸€è‡´ã€‚

- **CRDValidationRules**ï¼šæ ¹æ®OpenAPI V3 Schemaçš„å®šä¹‰ï¼Œä¼šå°†å…·æœ‰`format`ï¼ˆå¦‚`duration`ï¼‰çš„`string`ç±»å‹å­—æ®µåœ¨CELä¸­è¡¨ç¤ºä¸ºå¯¹åº”çš„ç±»å‹ï¼ˆå¦‚`duration`ï¼‰ã€‚å› æ­¤ï¼Œè¡¨è¾¾å¼`self >= duration("60m")`ä¸­çš„`self`è¢«è§†ä¸º`duration`ç±»å‹ï¼Œå¯ä»¥æ­£ç¡®æ¯”è¾ƒã€‚

- **ValidatingAdmissionPolicyï¼ˆVAPï¼‰**ï¼šç›´æ¥æš´éœ²æœªç»ç±»å‹è½¬æ¢çš„å¯¹è±¡å±æ€§ï¼Œå³ä½¿å±æ€§åœ¨OpenAPI Schemaä¸­æŒ‡å®šäº†`format`ï¼Œåœ¨CELä¸­ä»è¢«è§†ä¸º`string`ç±»å‹ã€‚å› æ­¤ï¼Œ`object.duration`åœ¨CELä¸­æ˜¯`string`ç±»å‹ï¼Œè€Œ`duration("2h")`æ˜¯`duration`ç±»å‹ï¼Œå¯¼è‡´è¡¨è¾¾å¼`object.duration >= duration("2h")`ç±»å‹ä¸åŒ¹é…ï¼Œå‡ºç°é”™è¯¯ã€‚

è¿™ç§ä¸ä¸€è‡´å¯¼è‡´VAPçš„éªŒè¯è§„åˆ™æ— æ³•æŒ‰é¢„æœŸæ‰§è¡Œï¼Œå¯èƒ½å‡ºç°ä»¥ä¸‹å®‰å…¨é—®é¢˜ï¼š

1. **éªŒè¯è§„åˆ™è¢«ç»•è¿‡**ï¼šç”±äºç±»å‹ä¸åŒ¹é…ï¼ŒéªŒè¯è¡¨è¾¾å¼å¯èƒ½æŠ›å‡ºé”™è¯¯æˆ–å§‹ç»ˆè¿”å›`false`ï¼Œå¯¼è‡´æœ¬åº”è¢«æ‹’ç»çš„è¯·æ±‚è¢«æ¥å—ã€‚

2. **æ‹’ç»æœåŠ¡æ”»å‡»**ï¼šæ”»å‡»è€…å¯ä»¥é€šè¿‡æäº¤ç‰¹åˆ¶çš„è¯·æ±‚ï¼Œè§¦å‘éªŒè¯è¡¨è¾¾å¼çš„é”™è¯¯ï¼Œå¯¼è‡´APIæœåŠ¡å™¨æ‹’ç»æœåŠ¡æˆ–æ€§èƒ½ä¸‹é™ã€‚

**å¯èƒ½çš„å½±å“ï¼š**

- **å®‰å…¨ç­–ç•¥å¤±æ•ˆ**ï¼šæ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ­¤æ¼æ´ï¼Œç»•è¿‡VAPçš„éªŒè¯è§„åˆ™ï¼Œåˆ›å»ºæˆ–ä¿®æ”¹ä¸ç¬¦åˆå®‰å…¨è¦æ±‚çš„èµ„æºï¼Œå±åŠé›†ç¾¤çš„å®‰å…¨æ€§å’Œå®Œæ•´æ€§ã€‚

- **æ•°æ®ç¯¡æ”¹å’Œæ³„éœ²**ï¼šæœªç»éªŒè¯æˆ–ä¸åˆè§„çš„èµ„æºå¯èƒ½å¯¼è‡´æ•°æ®è¢«ç¯¡æ”¹æˆ–æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€‚

- **æƒé™æå‡**ï¼šæ”»å‡»è€…å¯èƒ½é€šè¿‡åˆ›å»ºç‰¹å®šèµ„æºï¼Œæå‡è‡ªèº«æƒé™æˆ–è·å–æœªç»æˆæƒçš„è®¿é—®ã€‚

**CVSS 3.1è¯„åˆ†ï¼š**

- æ”»å‡»å‘é‡ï¼ˆAVï¼‰ï¼šç½‘ç»œï¼ˆNï¼‰â€”â€”æ¼æ´å¯é€šè¿‡ç½‘ç»œè¿œç¨‹åˆ©ç”¨ã€‚
- æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰ï¼šä½ï¼ˆLï¼‰â€”â€”åˆ©ç”¨è¯¥æ¼æ´ä¸éœ€è¦ç‰¹æ®Šæ¡ä»¶ã€‚
- å¿…è¦æƒé™ï¼ˆPRï¼‰ï¼šä½ï¼ˆLï¼‰â€”â€”æ”»å‡»è€…éœ€è¦èƒ½å¤Ÿå‘APIæœåŠ¡å™¨æäº¤è¯·æ±‚çš„æƒé™ã€‚
- ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰ï¼šæ— ï¼ˆNï¼‰â€”â€”ä¸éœ€è¦å…¶ä»–ç”¨æˆ·çš„å‚ä¸ã€‚
- ä½œç”¨åŸŸï¼ˆSï¼‰ï¼šæœªæ”¹å˜ï¼ˆUï¼‰â€”â€”ä»…å½±å“Kubernetesè‡ªèº«ç»„ä»¶ã€‚
- æœºå¯†æ€§ï¼ˆCï¼‰ï¼šé«˜ï¼ˆHï¼‰â€”â€”å¯èƒ½å¯¼è‡´æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€‚
- å®Œæ•´æ€§ï¼ˆIï¼‰ï¼šé«˜ï¼ˆHï¼‰â€”â€”å¯èƒ½å¯¼è‡´æ•°æ®è¢«æœªæˆæƒä¿®æ”¹ã€‚
- å¯ç”¨æ€§ï¼ˆAï¼‰ï¼šä¸­ï¼ˆMï¼‰â€”â€”å¯èƒ½å¯¼è‡´æœåŠ¡æ€§èƒ½ä¸‹é™æˆ–æ‹’ç»æœåŠ¡ã€‚

**ç»¼åˆå¾—åˆ†ï¼š9.1ï¼ˆé«˜ï¼‰**

**Proof of Conceptï¼š**

1. **å®šä¹‰CRDå’ŒValidatingAdmissionPolicyï¼š**

   ```yaml
   # CRDå®šä¹‰
   openAPIV3Schema:
     type: object
     properties:
       duration:
         type: string
         format: duration
         x-kubernetes-validations:
         - rule: "self >= duration(\"60m\")"
           message: "duration must be at least 1 hour"

   # ValidatingAdmissionPolicy
   validations:
     - expression: "object.duration >= duration(\"2h\")"
       message: "duration must be at least 2 hours"
   ```

2. **æ”»å‡»è€…åˆ›å»ºä¸åˆè§„çš„èµ„æºï¼š**

   ```yaml
   apiVersion: example.com/v1
   kind: Thing
   metadata:
     name: malicious-thing
   duration: "5m"
   ```

3. **é¢„æœŸç»“æœï¼š**

   - **æŒ‰ç…§å®‰å…¨ç­–ç•¥**ï¼Œè¯¥èµ„æºåº”è¢«æ‹’ç»ï¼Œå› ä¸º`duration`å°äº`2h`ã€‚
   - **å®é™…ç»“æœ**ï¼Œç”±äºç±»å‹ä¸åŒ¹é…ï¼ŒéªŒè¯è¡¨è¾¾å¼æœªæ­£ç¡®æ‰§è¡Œï¼Œå¯¼è‡´èµ„æºè¢«åˆ›å»ºæˆåŠŸã€‚

4. **éªŒè¯æ•ˆæœï¼š**

   - **æ”»å‡»è€…éªŒè¯èµ„æºå·²è¢«åˆ›å»ºï¼š**

     ```shell
     kubectl get thing malicious-thing
     ```

   - **èµ„æºå­˜åœ¨ï¼Œè¯´æ˜éªŒè¯è§„åˆ™è¢«ç»•è¿‡ã€‚**

**æ€»ç»“ï¼š**

ç”±äºVAPå’ŒCRDValidationRulesåœ¨å¤„ç†å¯¹è±¡å±æ€§ç±»å‹ä¸Šçš„ä¸ä¸€è‡´ï¼Œå¯¼è‡´éªŒè¯è§„åˆ™å¯èƒ½è¢«ç»•è¿‡ï¼Œæ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¯¥æ¼æ´åˆ›å»ºä¸ç¬¦åˆå®‰å…¨ç­–ç•¥çš„èµ„æºï¼Œå±åŠé›†ç¾¤å®‰å…¨ã€‚å»ºè®®å¯¹VAPçš„å®ç°è¿›è¡Œä¿®å¤ï¼Œä½¿å…¶ä¸CRDValidationRulesåœ¨ç±»å‹å¤„ç†ä¸Šä¿æŒä¸€è‡´ï¼Œç¡®ä¿éªŒè¯è§„åˆ™èƒ½å¤Ÿæ­£ç¡®æ‰§è¡Œã€‚

---

## Issue #124944 Non existing localhostProfile Seccomp profile is not applied on Kubernetes nodes >= 1.28

- Issue é“¾æ¥ï¼š[#124944](https://github.com/kubernetes/kubernetes/issues/124944)

### Issue å†…å®¹

#### What happened?

I am running 2 clusters, 1 is still on 1.26 and the other is on 1.29.5. I tried to apply a custom seccomp profile to a pod on 1.29.5 and noticed it did not seem to work.  While trying to pin-point the issue, I found out that applying a custom seccomp profile that does not exist (i.e. the file does not exist on the node) to a pod on a kubernetes node below 1.28 fails to start the pod, with an error: 
```
Error: failed to generate security options for container "test-container": failed to generate seccomp security options for container: cannot load seccomp profile "/var/lib/kubelet/seccomp/profiles/audit.json": open /var/lib/kubelet/seccomp/profiles/audit.json: no such file or directory
```

However, when trying to do the same on a kubernetes node >= 1.28 incorrectly starts the pod, with no mention of the seccomp profile file not being found or being invalid. Although I used minikube to reproduce this bug, I observed it first on a bare metal installation. 

#### What did you expect to happen?

I expect a pod not to start when the seccomp profile cannot be loaded.

#### How can we reproduce it (as minimally and precisely as possible)?

```
minikube start --kubernetes-version 1.27
kubectl apply -f audit-pod.yaml
```

Observe that the pod does not start and gives an error: 
```
Error: failed to generate security options for container "test-container": failed to generate seccomp security options for container: cannot load seccomp profile "/var/lib/kubelet/seccomp/profiles/audit.json": open /var/lib/kubelet/seccomp/profiles/audit.json: no such file or directory
```

```
minikube delete
minikube start --kubernetes-version 1.28\
kubectl apply -f audit-pod.yaml\
```

Observe that the pod has started without any errors.



audit-pod.yaml:
```apiVersion: v1
kind: Pod
metadata:
  name: audit-pod
  labels:
    app: audit-pod
spec:
  securityContext:
    seccompProfile:
      type: Localhost
      localhostProfile: profiles/audit.json
  containers:
  - name: test-container
    image: hashicorp/http-echo:1.0
    args:
    - "-text=just made some syscalls!"
    securityContext:
      allowPrivilegeEscalation: false
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

Version that errors on invalid seccomp profile:
<details>

```console
Client Version: v1.29.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.27.7
WARNING: version difference between client (1.29) and server (1.27) exceeds the supported minor version skew of +/-1
```

</details>

Version that does not error:
<details>

```console
$ kubectl version
Client Version: v1.29.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.3
```

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME=Buildroot
VERSION=2021.02.12-1-gb75713b-dirty
ID=buildroot
VERSION_ID=2021.02.12
PRETTY_NAME="Buildroot 2021.02.12"
$ uname -a
Linux minikube 5.10.57 #1 SMP Tue Nov 7 06:51:54 UTC 2023 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ç»è¿‡åˆ†æï¼Œè¯¥Issueæ¶‰åŠæ½œåœ¨çš„å®‰å…¨é£é™©ï¼Œå…·ä½“åŸå› å’Œå¯èƒ½å½±å“å¦‚ä¸‹ï¼š

**é—®é¢˜æè¿°ï¼š**

åœ¨Kubernetes 1.28åŠä»¥ä¸Šç‰ˆæœ¬ï¼Œå½“Podé…ç½®äº†ä¸å­˜åœ¨çš„æœ¬åœ°Seccomp profileï¼ˆ`localhostProfile`ï¼‰ï¼Œå³ä½¿è¯¥Seccomp profileæ–‡ä»¶åœ¨èŠ‚ç‚¹ä¸Šä¸å­˜åœ¨ï¼ŒPodä»ç„¶èƒ½å¤Ÿæ­£å¸¸å¯åŠ¨ä¸”æ²¡æœ‰ä»»ä½•é”™è¯¯æç¤ºã€‚ç„¶è€Œï¼Œåœ¨1.28ä»¥ä¸‹çš„ç‰ˆæœ¬ä¸­ï¼Œå¦‚æœæŒ‡å®šçš„Seccomp profileæ–‡ä»¶ä¸å­˜åœ¨ï¼ŒPodä¼šæ— æ³•å¯åŠ¨å¹¶æŠ¥é”™ã€‚

**å®‰å…¨é£é™©åˆ†æï¼š**

1. **Seccompéš”ç¦»å¤±æ•ˆï¼š** Seccompæ˜¯ä¸€ç§ç”¨äºé™åˆ¶å®¹å™¨å†…è¿›ç¨‹å¯ä»¥è°ƒç”¨çš„ç³»ç»Ÿè°ƒç”¨çš„å®‰å…¨æœºåˆ¶ã€‚å¦‚æœSeccomp profileæœªæ­£ç¡®åŠ è½½ï¼Œæ„å‘³ç€Podç¼ºä¹åº”æœ‰çš„ç³»ç»Ÿè°ƒç”¨é™åˆ¶ï¼Œå¯èƒ½å…è®¸Podå†…çš„è¿›ç¨‹æ‰§è¡Œæ›´å¤šçš„ç³»ç»Ÿè°ƒç”¨ï¼Œä»è€Œå¢åŠ æ”»å‡»é¢ã€‚

2. **ç»•è¿‡å®‰å…¨ç­–ç•¥ï¼š** Kubernetesé›†ç¾¤ç®¡ç†å‘˜æˆ–å®‰å…¨ç­–ç•¥å¯èƒ½ä¾èµ–äºSeccompæ¥å®ç°ç³»ç»Ÿè°ƒç”¨çº§åˆ«çš„é™åˆ¶ã€‚å¦‚æœSeccomp profileæœªè¢«æ­£ç¡®åº”ç”¨ï¼Œæ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¿™ä¸€æ¼æ´ç»•è¿‡å®‰å…¨ç­–ç•¥ï¼Œæ‰§è¡ŒåŸæœ¬è¢«ç¦æ­¢çš„æ“ä½œã€‚

3. **æƒé™æå‡å’Œç³»ç»Ÿç ´åï¼š** æ”»å‡»è€…å¯ä»¥åˆ©ç”¨ç¼ºå¤±çš„Seccompé™åˆ¶ï¼Œå‘èµ·ç³»ç»Ÿè°ƒç”¨çº§åˆ«çš„æ”»å‡»ï¼Œä¾‹å¦‚ææƒã€è®¿é—®æ•æ„Ÿä¿¡æ¯ã€å¹²æ‰°ä¸»æœºç³»ç»Ÿç­‰ï¼Œå¯èƒ½å¯¼è‡´é›†ç¾¤èŠ‚ç‚¹è¢«æ”»é™·ã€‚

**æ”»å‡»åœºæ™¯ï¼š**

- **å¤šç§Ÿæˆ·ç¯å¢ƒä¸‹çš„é£é™©ï¼š** åœ¨å…±äº«çš„Kubernetesé›†ç¾¤ä¸­ï¼Œå…·æœ‰åˆ›å»ºPodæƒé™çš„ç”¨æˆ·å¯èƒ½æ˜¯ä¸åŒçš„å›¢é˜Ÿæˆ–ç§Ÿæˆ·ã€‚å¦‚æœä»–ä»¬çš„Podæœªæ­£ç¡®åº”ç”¨Seccompç­–ç•¥ï¼Œæ¶æ„ç”¨æˆ·å¯ä»¥åˆ©ç”¨è¯¥æ¼æ´å¯¹é›†ç¾¤è¿›è¡Œæ”»å‡»ã€‚

- **ä½æƒé™æ”»å‡»è€…çš„åˆ©ç”¨ï¼š** æ”»å‡»è€…åªéœ€è¦å…·æœ‰åˆ›å»ºPodçš„æƒé™ï¼ˆPrivileges Requiredä¸ºLowï¼‰ï¼Œå³å¯é€šè¿‡æŒ‡å®šä¸å­˜åœ¨çš„Seccomp profileï¼Œä½¿Podåœ¨æœªå—é™çš„æƒ…å†µä¸‹è¿è¡Œã€‚

**CVSS v3.1è¯„åˆ†ï¼š**

æ ¹æ®ä¸Šè¿°åˆ†æï¼ŒæŒ‰ç…§CVSS v3.1æ ‡å‡†ï¼Œè¯„åˆ†å¦‚ä¸‹ï¼š

- **æ”»å‡»å‘é‡ï¼ˆAVï¼‰ï¼š** ç½‘ç»œï¼ˆNï¼‰â€”â€”æ”»å‡»è€…å¯ä»¥é€šè¿‡ç½‘ç»œè®¿é—®é›†ç¾¤APIæœåŠ¡å™¨åˆ›å»ºPodã€‚
- **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰ï¼š** ä½ï¼ˆLï¼‰â€”â€”æ”»å‡»ä¸éœ€è¦ç‰¹æ®Šæ¡ä»¶ï¼Œå®¹æ˜“å®æ–½ã€‚
- **æ‰€éœ€ç‰¹æƒï¼ˆPRï¼‰ï¼š** ä½ï¼ˆLï¼‰â€”â€”ä»…éœ€è¦åˆ›å»ºPodçš„æƒé™ã€‚
- **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰ï¼š** æ— ï¼ˆNï¼‰â€”â€”ä¸éœ€è¦å…¶ä»–ç”¨æˆ·çš„å‚ä¸ã€‚
- **èŒƒå›´ï¼ˆSï¼‰ï¼š** æ”¹å˜ï¼ˆCï¼‰â€”â€”æ”»å‡»å¯èƒ½å½±å“åˆ°è¶…å‡ºæ”»å‡»è€…æƒé™çš„èµ„æºã€‚
- **æœºå¯†æ€§å½±å“ï¼ˆCï¼‰ï¼š** é«˜ï¼ˆHï¼‰â€”â€”å¯èƒ½è®¿é—®åˆ°æ•æ„Ÿä¿¡æ¯ã€‚
- **å®Œæ•´æ€§å½±å“ï¼ˆIï¼‰ï¼š** é«˜ï¼ˆHï¼‰â€”â€”å¯èƒ½ç¯¡æ”¹ç³»ç»Ÿæ•°æ®ã€‚
- **å¯ç”¨æ€§å½±å“ï¼ˆAï¼‰ï¼š** é«˜ï¼ˆHï¼‰â€”â€”å¯èƒ½å¯¼è‡´ç³»ç»ŸæœåŠ¡ä¸­æ–­ã€‚

**CVSS Base Scoreè®¡ç®—ï¼š** 9.6ï¼ˆä¸¥é‡ï¼‰

**PoCï¼ˆæ¦‚å¿µéªŒè¯ï¼‰ï¼š**

æŒ‰ç…§Issueä¸­æä¾›çš„æ­¥éª¤ï¼Œå¯ä»¥é‡ç°è¯¥é—®é¢˜ï¼š

1. å¯åŠ¨Kubernetes 1.28æˆ–æ›´é«˜ç‰ˆæœ¬çš„é›†ç¾¤ï¼ˆä¾‹å¦‚ä½¿ç”¨Minikubeï¼‰ï¼š

   ```bash
   minikube start --kubernetes-version 1.28
   ```

2. åº”ç”¨é…ç½®äº†ä¸å­˜åœ¨çš„Seccomp profileçš„Podï¼š

   ```bash
   kubectl apply -f audit-pod.yaml
   ```

   å…¶ä¸­`audit-pod.yaml`çš„å†…å®¹æŒ‡å®šäº†ä¸å­˜åœ¨çš„Seccomp profileï¼š

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: audit-pod
     labels:
       app: audit-pod
   spec:
     securityContext:
       seccompProfile:
         type: Localhost
         localhostProfile: profiles/audit.json
     containers:
     - name: test-container
       image: hashicorp/http-echo:1.0
       args:
       - "-text=just made some syscalls!"
       securityContext:
         allowPrivilegeEscalation: false
   ```

3. è§‚å¯ŸPodæˆåŠŸå¯åŠ¨ä¸”æ²¡æœ‰é”™è¯¯ä¿¡æ¯ï¼š

   ```bash
   kubectl get pods
   # åº”è¯¥çœ‹åˆ°audit-podå¤„äºRunningçŠ¶æ€
   ```

4. éªŒè¯Seccompæœªæ­£ç¡®åº”ç”¨ï¼Œå¯ä»¥åœ¨Podå†…éƒ¨æ‰§è¡ŒåŸæœ¬åº”è¢«é™åˆ¶çš„ç³»ç»Ÿè°ƒç”¨ã€‚

**å¯èƒ½çš„ç¼“è§£æªæ–½ï¼š**

- **éªŒè¯Seccomp profileå­˜åœ¨æ€§ï¼š** åœ¨Podå¯åŠ¨å‰ï¼Œç¡®ä¿æŒ‡å®šçš„Seccomp profileæ–‡ä»¶å­˜åœ¨äºèŠ‚ç‚¹ä¸Šçš„æ­£ç¡®è·¯å¾„ã€‚
- **å‡çº§æˆ–ä¿®å¤Kubernetesç‰ˆæœ¬ï¼š** å…³æ³¨Kubernetesçš„æ›´æ–°ï¼Œå¦‚æœè¯¥é—®é¢˜è¢«è®¤å®šä¸ºæ¼æ´ï¼Œå®˜æ–¹å¯èƒ½ä¼šå‘å¸ƒä¿®å¤ç‰ˆæœ¬ã€‚
- **å®¡è®¡å’Œç›‘æ§ï¼š** å®æ–½å¯¹Podå’ŒèŠ‚ç‚¹çš„å®‰å…¨å®¡è®¡ï¼Œç›‘æ§å¼‚å¸¸çš„ç³»ç»Ÿè°ƒç”¨è¡Œä¸ºã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueæ¶‰åŠä¸€ä¸ªæ½œåœ¨çš„å®‰å…¨æ¼æ´ï¼Œå¯èƒ½å¯¼è‡´Seccompå®‰å…¨ç­–ç•¥æœªè¢«æ­£ç¡®åº”ç”¨ï¼Œå…è®¸æ”»å‡»è€…ç»•è¿‡ç³»ç»Ÿè°ƒç”¨çº§åˆ«çš„é™åˆ¶ï¼Œé€ æˆä¸¥é‡çš„å®‰å…¨åæœã€‚

---

## Issue #124863 NFS PV mountOptions does not work when Volume SubPath option is set

- Issue é“¾æ¥ï¼š[#124863](https://github.com/kubernetes/kubernetes/issues/124863)

### Issue å†…å®¹

#### What happened?

Setting the Volume subpath option at deployment with an NFS PV with a mount option (e.g. nosuid) does not mount accordingly. If no SubPath is set and the NFS PV is set to the /nfs_share/directory the mount works with the nosuid.

#### What did you expect to happen?

should be mounted with correct mountoptions

#### How can we reproduce it (as minimally and precisely as possible)?

- create KIND cluster and install nfs client packages
- create a PV and PVC with below options: 


```
spec:
  accessModes:
    - ReadWriteOnce
    - ReadOnlyMany
    - ReadWriteMany
  capacity:
    storage: 10Gi
  claimRef:
    apiVersion: v1
    kind: PersistentVolumeClaim
    name: pvc01
    namespace: default
    resourceVersion: '53505'
    uid: 0b3736ac-bbb6-4539-95c1-854ddcb36114
  mountOptions:
    - nfsvers=4.2
    - sec=sys
    - nosuid
  nfs:
    path: /nfs
    server: 172.31.40.82
  persistentVolumeReclaimPolicy: Retain
  volumeMode: Filesystem  
```
-  Update the deployment with the volume and mount information: 

```
          volumeMounts:
            - mountPath: /data
              name: vol-oirnd
              subPath: share01

      volumes:
        - name: vol-oirnd
          persistentVolumeClaim:
            claimName: pvc01
          __newPvc:
            type: persistentvolumeclaim
            metadata:
              namespace: default
            spec:
              storageClassName: ''
              volumeName: ''
              resources:
                requests: {} 
```

#### Anything else we need to know?

```
**Not using SubPath**

root@kind-2-control-plane:/# mount | grep nfs
192.168.100.217:/nfs on /var/lib/kubelet/pods/c29b33d8-3709-4082-892c-def5efbba86f/volumes/kubernetes.io~nfs/pv type nfs4 (rw,nosuid,relatime,vers=4.2,rsize=524288,wsize=524288,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=172.18.0.2,local_lock=none,addr=192.168.100.217)

**When not using SubPath**


192.168.100.217:/nfs/hello/world on /var/lib/kubelet/pods/c29b33d8-3709-4082-892c-def5efbba86f/volume-subpaths/pv/httpd/0 type nfs4 (rw,relatime,vers=4.2,rsize=524288,wsize=524288,namlen=255,hard,proto=tcp,timeo=600,retrans=2,sec=sys,clientaddr=172.18.0.2,local_lock=none,addr=192.168.100.217)
root@kind-2-control-plane:/# 

root@kind-2-control-plane:/# kubectl get nodes
NAME                   STATUS   ROLES           AGE   VERSION
kind-2-control-plane   Ready    control-plane   49m   v1.29.2
root@kind-2-control-plane:/# 
```

#### Kubernetes version

<details>

```console
$ kubectl version
 v1.29.2
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
Ubuntu

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ç»è¿‡åˆ†æï¼Œè¯¥ Issue å­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

**åŸå› ï¼š**

åœ¨ Kubernetes ä¸­ï¼Œå½“ä½¿ç”¨ NFS ç±»å‹çš„ PersistentVolumeï¼ˆPVï¼‰æ—¶ï¼Œç®¡ç†å‘˜å¯ä»¥é€šè¿‡ `mountOptions` æ¥æŒ‡å®šæŒ‚è½½é€‰é¡¹ï¼Œä¾‹å¦‚ `nosuid`ï¼Œä»¥é˜²æ­¢æ‰§è¡Œå…·æœ‰ SUID ä½çš„äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œä»è€Œæé«˜å®‰å…¨æ€§ã€‚

ç„¶è€Œï¼Œè¯¥ Issue æŒ‡å‡ºï¼Œå½“åœ¨ Pod çš„é…ç½®ä¸­ä½¿ç”¨äº† `subPath` é€‰é¡¹æ—¶ï¼ŒPV ä¸ŠæŒ‡å®šçš„ `mountOptions`ï¼ˆå¦‚ `nosuid`ï¼‰æœªè¢«æ­£ç¡®åº”ç”¨ã€‚è¿™æ„å‘³ç€ï¼š

- æŒ‚è½½åˆ°å®¹å™¨å†…çš„å­è·¯å¾„å¯èƒ½æœªç¦ç”¨ SUIDã€‚
- æ”»å‡»è€…å¦‚æœèƒ½åœ¨ NFS å…±äº«ç›®å½•ä¸­æ”¾ç½®æ¶æ„çš„ SUID ç¨‹åºï¼Œå¯èƒ½åˆ©ç”¨æ­¤æ¼æ´åœ¨å®¹å™¨å†…è·å¾—æå‡çš„æƒé™ã€‚
- åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œç”šè‡³å¯èƒ½å½±å“å®¿ä¸»æœºçš„å®‰å…¨ã€‚

**å¯èƒ½çš„å½±å“ï¼š**

- **æƒé™æå‡ï¼š** æ”»å‡»è€…å¯ä»¥åœ¨ NFS å…±äº«ä¸Šæ”¾ç½®æ¶æ„çš„ SUID äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œå®¹å™¨å†…çš„è¿›ç¨‹æ‰§è¡Œè¯¥æ–‡ä»¶åï¼Œå¯è·å¾—æ›´é«˜çš„ç³»ç»Ÿæƒé™ã€‚
- **å®¹å™¨é€ƒé€¸ï¼š** åœ¨æŸäº›é…ç½®ä¸‹ï¼Œæ”»å‡»è€…å¯èƒ½çªç ´å®¹å™¨çš„éš”ç¦»ï¼Œè®¿é—®å®¿ä¸»æœºæˆ–å…¶ä»–å®¹å™¨çš„èµ„æºã€‚
- **æ•°æ®æ³„éœ²æˆ–ç¯¡æ”¹ï¼š** æ”»å‡»è€…å¯èƒ½è®¿é—®æˆ–ä¿®æ”¹æ•æ„Ÿæ•°æ®ã€‚

**ç¬¦åˆé£é™©åˆ¤æ–­æ ‡å‡†ï¼š**

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨ï¼š** æ˜¯ï¼Œæ”»å‡»è€…å¯ä»¥åˆ©ç”¨æœªæ­£ç¡®åº”ç”¨çš„ `nosuid` é€‰é¡¹æ‰§è¡Œæ¶æ„çš„ SUID ç¨‹åºã€‚
2. **å¯èƒ½æˆä¸ºæ¼æ´ï¼ŒCVSS è¯„åˆ†é«˜äº Highï¼š** æ˜¯ï¼Œæ ¹æ®ä»¥ä¸‹è¯„åˆ†ï¼ŒCVSS 3.1 åŸºæœ¬åˆ†ä¸º 9.8ï¼ˆCriticalï¼‰ã€‚
3. **ä¸å±äºæäº¤è€…çš„é—®é¢˜ï¼š** æ˜¯ï¼Œé—®é¢˜æºäº Kubernetes å¯¹ `mountOptions` åœ¨ä½¿ç”¨ `subPath` æ—¶æœªæ­£ç¡®åº”ç”¨ï¼Œæ˜¯é¡¹ç›®çš„é—®é¢˜ã€‚
4. **æ— éœ€é¢å¤–æƒé™å³å¯æ”»å‡»ï¼š** æ”»å‡»è€…å¯èƒ½åªéœ€è¦å¯¹ NFS å…±äº«çš„å†™æƒé™ï¼Œæˆ–è€…åˆ©ç”¨å…¶ä»–æ¼æ´è·å–å†™æƒé™ã€‚

**CVSS 3.1 è¯„åˆ†ï¼š**

- **æ”»å‡»å‘é‡ï¼ˆAVï¼‰ï¼š** ç½‘ç»œï¼ˆNï¼‰â€”â€”æ”»å‡»è€…å¯é€šè¿‡ç½‘ç»œè®¿é—® NFS å…±äº«ã€‚
- **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰ï¼š** ä½ï¼ˆLï¼‰â€”â€”æ”»å‡»ä¸éœ€è¦é«˜å¤æ‚åº¦ã€‚
- **ç‰¹æƒè¦æ±‚ï¼ˆPRï¼‰ï¼š** ä½ï¼ˆLï¼‰â€”â€”éœ€è¦å¯¹ NFS å…±äº«æœ‰å†™æƒé™ã€‚
- **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰ï¼š** æ— ï¼ˆNï¼‰â€”â€”ä¸éœ€è¦ç”¨æˆ·äº¤äº’ã€‚
- **ä½œç”¨èŒƒå›´ï¼ˆSï¼‰ï¼š** æ”¹å˜ï¼ˆCï¼‰â€”â€”å½±å“ä»å®¹å™¨åˆ°å®¿ä¸»æœºçš„å®‰å…¨ã€‚
- **æœºå¯†æ€§ï¼ˆCï¼‰ï¼š** é«˜ï¼ˆHï¼‰â€”â€”å¯èƒ½è®¿é—®æ•æ„Ÿä¿¡æ¯ã€‚
- **å®Œæ•´æ€§ï¼ˆIï¼‰ï¼š** é«˜ï¼ˆHï¼‰â€”â€”å¯èƒ½ä¿®æ”¹ç³»ç»Ÿæ•°æ®ã€‚
- **å¯ç”¨æ€§ï¼ˆAï¼‰ï¼š** é«˜ï¼ˆHï¼‰â€”â€”å¯èƒ½å¯¼è‡´æœåŠ¡ä¸­æ–­ã€‚

**CVSS åŸºæœ¬åˆ†ï¼š** 9.8ï¼ˆä¸¥é‡ï¼‰

**Proof of Conceptï¼š**

1. **ç¯å¢ƒå‡†å¤‡ï¼š**
   - éƒ¨ç½²ä¸€ä¸ªä½¿ç”¨ NFS PV çš„ Kubernetes é›†ç¾¤ã€‚
   - PV é…ç½®äº† `mountOptions`ï¼ŒåŒ…æ‹¬ `nosuid`ã€‚
   - Pod é…ç½®äº† `volumeMounts`ï¼Œä½¿ç”¨äº† `subPath`ã€‚

2. **æ”»å‡»æ­¥éª¤ï¼š**
   - æ”»å‡»è€…åœ¨ NFS å…±äº«çš„å­è·¯å¾„ä¸‹æ”¾ç½®ä¸€ä¸ªæ¶æ„çš„ SUID äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œä¾‹å¦‚ä¸€ä¸ªç®€å•çš„ææƒç¨‹åºï¼Œè®¾ç½® SUID ä½ï¼š
     ```
     #include <unistd.h>
     int main() {
         setuid(0);
         system("/bin/sh");
         return 0;
     }
     ```
     ç¼–è¯‘åï¼Œå°†äºŒè¿›åˆ¶æ–‡ä»¶çš„æƒé™è®¾ç½®ä¸º 4755ï¼ˆè®¾ç½® SUID ä½ï¼‰ï¼š
     ```
     chmod 4755 evil_suid_binary
     ```
   - ç”±äº `nosuid` é€‰é¡¹æœªç”Ÿæ•ˆï¼Œå®¹å™¨å†…çš„ç”¨æˆ·æ‰§è¡Œè¯¥äºŒè¿›åˆ¶æ–‡ä»¶åï¼Œå°†ä»¥ root æƒé™è¿è¡Œï¼Œè·å¾—æå‡çš„æƒé™ã€‚

3. **éªŒè¯ï¼š**
   - åœ¨å®¹å™¨å†…ï¼Œæ‰§è¡Œ `evil_suid_binary`ï¼Œå¦‚æœæˆåŠŸè·å¾— root æƒé™ï¼Œè¯æ˜æ¼æ´å­˜åœ¨ã€‚

**å»ºè®®ï¼š**

- ä¿®å¤ Kubernetes å¯¹ `mountOptions` åœ¨ä½¿ç”¨ `subPath` æ—¶çš„å¤„ç†ï¼Œç¡®ä¿é€‰é¡¹è¢«æ­£ç¡®åº”ç”¨ã€‚
- ä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼šé¿å…åœ¨æ•æ„Ÿç¯å¢ƒä¸‹ä½¿ç”¨ `subPath`ï¼Œæˆ–åœ¨ NFS æœåŠ¡ç«¯é™åˆ¶ SUIDã€‚

---

## Issue #124759 CVE-2024-3744: azure-file-csi-driver discloses service account tokens in logs

- Issue é“¾æ¥ï¼š[#124759](https://github.com/kubernetes/kubernetes/issues/124759)

### Issue å†…å®¹

CVSS Rating: [CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:C/C:H/I:N/A:N](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:C/C:H/I:N/A:N) - **MEDIUM** (6.5)

A security issue was discovered in azure-file-csi-driver where an actor with access to the driver logs could observe service account tokens. These tokens could then potentially be exchanged with external cloud providers to access secrets stored in cloud vault solutions.  Tokens are only logged when [TokenRequests is configured in the CSIDriver object](https://kubernetes-csi.github.io/docs/token-requests.html) and the driver is set to run at log level 2 or greater via the -v flag.

This issue has been rated **MEDIUM** [CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:C/C:H/I:N/A:N](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:L/AC:L/PR:L/UI:N/S:C/C:H/I:N/A:N) (6.5), and assigned **CVE-2024-3744**

#### Am I vulnerable?

You may be vulnerable if [TokenRequests is configured in the CSIDriver object](https://kubernetes-csi.github.io/docs/token-requests.html) and the driver is set to run at log level 2 or greater via the -v flag.

To check if token requests are configured, run the following command:

kubectl get csidriver file.csi.azure.com -o jsonpath="{.spec.tokenRequests}"

To check if tokens are being logged, examine the secrets-store container log:

kubectl logs csi-azurefile-controller-56bfddd689-dh5tk -c azurefile -f | grep --line-buffered "csi.storage.k8s.io/serviceAccount.tokens"

##### Affected Versions

- azure-file-csi-driver <= v1.29.3
- azure-file-csi-driver v1.30.0

#### How do I mitigate this vulnerability?

Prior to upgrading, this vulnerability can be mitigated by running azure-file-csi-driver at log level 0 or 1 via the -v flag.

##### Fixed Versions

- azure-file-csi-driver v1.29.4
- azure-file-csi-driver v1.30.1

To upgrade, refer to the documentation: https://github.com/kubernetes-sigs/azurefile-csi-driver?tab=readme-ov-file#install-driver-on-a-kubernetes-cluster 

#### Detection

Examine cloud provider logs for unexpected token exchanges, as well as unexpected access to cloud resources.

If you find evidence that this vulnerability has been exploited, please contact security@kubernetes.io

##### Acknowledgements

This vulnerability was patched by Weizhi Chen @cvvz from Microsoft.

Thank You,
Rita Zhang on behalf of the Kubernetes Security Response Committee

/triage accepted
/lifecycle frozen
/area security
/kind bug
/committee security-response

### åˆ†æç»“æœ

å­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

**åŸå› å’Œå¯èƒ½çš„å½±å“ï¼š**

åœ¨**azure-file-csi-driver**ä¸­ï¼Œå¦‚æœåœ¨CSIDriverå¯¹è±¡ä¸­é…ç½®äº†`TokenRequests`ï¼Œå¹¶ä¸”é©±åŠ¨ç¨‹åºä»¥`-v`æ ‡å¿—è®¾ç½®äº†æ—¥å¿—çº§åˆ«ä¸º2æˆ–æ›´é«˜ï¼Œé‚£ä¹ˆæœåŠ¡è´¦æˆ·ä»¤ç‰Œå¯èƒ½ä¼šè¢«è®°å½•åœ¨æ—¥å¿—ä¸­ã€‚

è¿™æ„å‘³ç€å…·æœ‰è®¿é—®é©±åŠ¨ç¨‹åºæ—¥å¿—æƒé™çš„æ”»å‡»è€…ï¼Œå¯ä»¥ä»æ—¥å¿—ä¸­è·å–æœåŠ¡è´¦æˆ·ä»¤ç‰Œã€‚è¿™äº›ä»¤ç‰Œå¯èƒ½è¢«ç”¨äºä¸å¤–éƒ¨äº‘æä¾›å•†äº¤äº’ï¼Œä»è€Œè®¿é—®å­˜å‚¨åœ¨äº‘å¯†é’¥åº“ï¼ˆcloud vault solutionsï¼‰ä¸­çš„æœºå¯†ä¿¡æ¯ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬5æ¡ï¼š

- **å¯¹äºæ—¥å¿—ä¸­æ³„éœ²å‡­æ®çš„é£é™©ï¼Œå¦‚æœæ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ¯”æ³„éœ²å‡­æ®æ›´ä½çš„æƒé™ä»æ—¥å¿—ä¸­è¯»å–è¯¥å‡­æ®ï¼Œæˆ–è€…æ³„éœ²çš„å‡­æ®ä¸æ”»å‡»è€…ä½¿ç”¨çš„å‡­æ®ä¸æ˜¯ä¸€ç±»å‡­æ®ï¼Œå¯¼è‡´æ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ³„éœ²å‡­æ®è®¿é—®å…¶ä»–ç³»ç»Ÿï¼Œåˆ™åº”é€‚å½“æé«˜é£é™©è¯„çº§åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚**

å› æ­¤ï¼Œæ”»å‡»è€…å¯èƒ½åˆ©ç”¨è¾ƒä½çš„æƒé™ï¼ˆä»…éœ€è®¿é—®æ—¥å¿—çš„æƒé™ï¼‰ï¼Œè·å–é«˜æƒé™çš„æœåŠ¡è´¦æˆ·ä»¤ç‰Œï¼Œå¹¶è¿›ä¸€æ­¥è®¿é—®æ•æ„Ÿçš„äº‘èµ„æºã€‚

**CVSS 3.1è¯„åˆ†ï¼š**

è™½ç„¶Issueä¸­å°†è¯¥æ¼æ´è¯„åˆ†ä¸º**Mediumï¼ˆ6.5ï¼‰**ï¼Œä½†æ ¹æ®ä¸Šè¿°é£é™©åˆ¤æ–­ï¼Œåº”é‡æ–°è¯„ä¼°ä¸º**High**æˆ–æ›´é«˜ã€‚

**Proof of Conceptï¼š**

1. **å‰ææ¡ä»¶ï¼š**
   - **TokenRequests**å·²åœ¨CSIDriverå¯¹è±¡ä¸­é…ç½®ã€‚
   - é©±åŠ¨ç¨‹åºä»¥`-v`æ ‡å¿—è®¾ç½®äº†æ—¥å¿—çº§åˆ«ä¸º2æˆ–æ›´é«˜ã€‚
   - æ”»å‡»è€…æ‹¥æœ‰å¯¹azure-file-csi-driveræ—¥å¿—çš„è¯»å–æƒé™ï¼ˆå¯èƒ½æ˜¯ä½æƒé™ç”¨æˆ·ï¼‰ã€‚

2. **æ”»å‡»æ­¥éª¤ï¼š**
   - æ”»å‡»è€…è®¿é—®å¹¶è¯»å–azure-file-csi-driverçš„æ—¥å¿—æ–‡ä»¶ã€‚
   - åœ¨æ—¥å¿—ä¸­æœç´¢åŒ…å«`"csi.storage.k8s.io/serviceAccount.tokens"`çš„æ—¥å¿—æ¡ç›®ã€‚
   - æå–æ—¥å¿—ä¸­è®°å½•çš„æœåŠ¡è´¦æˆ·ä»¤ç‰Œä¿¡æ¯ã€‚
   - ä½¿ç”¨æå–çš„æœåŠ¡è´¦æˆ·ä»¤ç‰Œï¼Œè®¿é—®äº‘æä¾›å•†çš„APIæˆ–å…¶ä»–å—ä¿æŠ¤çš„èµ„æºã€‚
   - è·å–å­˜å‚¨åœ¨äº‘å¯†é’¥åº“ç­‰ä½ç½®çš„æœºå¯†ä¿¡æ¯ã€‚

**å¯èƒ½çš„å½±å“ï¼š**

- **æ•°æ®æ³„éœ²ï¼š** æ”»å‡»è€…è·å–æ•æ„Ÿçš„æœåŠ¡è´¦æˆ·ä»¤ç‰Œåï¼Œå¯èƒ½è®¿é—®åˆ°å­˜å‚¨åœ¨äº‘ä¸­çš„æœºå¯†æ•°æ®ã€‚
- **æƒé™æå‡ï¼š** æ”»å‡»è€…åˆ©ç”¨æœåŠ¡è´¦æˆ·ä»¤ç‰Œï¼Œè·å¾—æ¯”åŸæœ‰æƒé™æ›´é«˜çš„è®¿é—®èƒ½åŠ›ã€‚
- **æ¨ªå‘ç§»åŠ¨ï¼š** æ”»å‡»è€…å¯èƒ½åˆ©ç”¨è·å–çš„ä»¤ç‰Œï¼Œè®¿é—®å…¶ä»–ç³»ç»Ÿæˆ–æœåŠ¡ï¼Œæ‰©å¤§æ”»å‡»èŒƒå›´ã€‚

å› æ­¤ï¼Œè¯¥é—®é¢˜ç¡®å®æ¶‰åŠæ½œåœ¨çš„å®‰å…¨é£é™©ï¼Œéœ€è¦å¼•èµ·é‡è§†ï¼Œå¹¶åŠæ—¶é‡‡å–æªæ–½è¿›è¡Œä¿®å¤ã€‚

---

## Issue #124701 kubelet should alway get lastest secret/configmap resource in pod add event

- Issue é“¾æ¥ï¼š[#124701](https://github.com/kubernetes/kubernetes/issues/124701)

### Issue å†…å®¹

#### What happened?

The logic is as follows:
1. Manually update secret resources.
2. kubelet listens to resources and updates the watch cache.
3. Start the pod and query the volume attached to the secret resource.

but probabilistically, especially at lots of operating pressures. The third step is performed before the second step. As a result, the old secret is mounted when the pod is started.

![image](https://github.com/kubernetes/kubernetes/assets/17514799/e72a9968-6467-4436-803d-002df05b5df7)

----------

![image](https://github.com/kubernetes/kubernetes/assets/17514799/b81c5384-4b47-420a-9995-6ce46ce275c5)

https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/



#### What did you expect to happen?

![image](https://github.com/kubernetes/kubernetes/assets/17514799/e8fd8400-b3c4-4072-8399-322548183c3c)

solution: kubelet should alway get lastest secret/configmap resource in pod add event instead of using cache

#### How can we reproduce it (as minimally and precisely as possible)?

see What happend

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

å®‰å…¨é£é™©åˆ†æï¼š

é—®é¢˜æè¿°ä¸­æŒ‡å‡ºï¼Œå½“æ‰‹åŠ¨æ›´æ–°äº† Secret èµ„æºåï¼Œkubelet ç›‘å¬èµ„æºå¹¶æ›´æ–°ç¼“å­˜ï¼ˆwatch cacheï¼‰ã€‚ä½†æ˜¯åœ¨é«˜è´Ÿè½½æƒ…å†µä¸‹ï¼Œå¯èƒ½å‡ºç° Pod å¯åŠ¨å¹¶æŸ¥è¯¢é™„åŠ çš„ Secret èµ„æºçš„æ“ä½œå‘ç”Ÿåœ¨ kubelet æ›´æ–°ç¼“å­˜ä¹‹å‰ï¼Œå¯¼è‡´ Pod å¯åŠ¨æ—¶æŒ‚è½½äº†æ—§çš„ Secretã€‚

**æ½œåœ¨çš„å®‰å…¨é£é™©ï¼š**

1. **æ•æ„Ÿä¿¡æ¯æ³„éœ²ä¸æœªæˆæƒè®¿é—®ï¼š**
   - **åœºæ™¯æè¿°ï¼š** å¦‚æœç®¡ç†å‘˜æ›´æ–°äº† Secretï¼Œä»¥æ’¤é”€æŸäº›å‡­æ®æˆ–æ›´æ¢æ•æ„Ÿä¿¡æ¯ï¼Œä»¥é˜²æ­¢æœªæˆæƒçš„è®¿é—®ã€‚ä½†ç”±äº kubelet æœªåŠæ—¶è·å–æœ€æ–°çš„ Secretï¼Œæ–°çš„ Pod å¯èƒ½ä»ç„¶ä½¿ç”¨æ—§çš„ Secretã€‚
   - **æ”»å‡»è€…åˆ©ç”¨ï¼š**
     - æ”»å‡»è€…å¦‚æœå…·å¤‡å¯åŠ¨ Pod çš„æƒé™ï¼Œå¯ä»¥åœ¨ç¼“å­˜æœªæ›´æ–°å‰è¿…é€Ÿåˆ›å»ºæ–°çš„ Podï¼Œè·å–å¹¶ä½¿ç”¨æ—§çš„ Secretã€‚
     - åˆ©ç”¨æ—§çš„å‡­æ®ï¼Œæ”»å‡»è€…å¯èƒ½ç»§ç»­è®¿é—®åŸæœ¬åº”è¢«æ’¤é”€è®¿é—®æƒé™çš„æœåŠ¡æˆ–æ•°æ®ã€‚

2. **æƒé™ç»´æŒä¸æå‡ï¼š**
   - **åœºæ™¯æè¿°ï¼š** åœ¨å®‰å…¨äº‹ä»¶å‘ç”Ÿåï¼Œç®¡ç†å‘˜æ›´æ–° Secret ä»¥é˜»æ­¢æ”»å‡»è€…çš„è¿›ä¸€æ­¥è¡ŒåŠ¨ã€‚
   - **æ”»å‡»è€…åˆ©ç”¨ï¼š**
     - æ”»å‡»è€…åˆ©ç”¨ kubelet çš„ç¼“å­˜æœºåˆ¶ï¼Œåœ¨ Secret æ›´æ–°åä½†ç¼“å­˜æœªåˆ·æ–°å‰ï¼Œå¯åŠ¨æ–°çš„ Podï¼Œè·å–æ—§çš„ Secretï¼Œå®ç°æƒé™ç»´æŒã€‚
     - é€šè¿‡æ—§çš„ Secretï¼Œæ”»å‡»è€…å¯èƒ½æå‡æƒé™ã€æ¨ªå‘ç§»åŠ¨ï¼Œé€ æˆæ›´å¤§çš„å®‰å…¨å¨èƒã€‚

3. **ä¸€è‡´æ€§ä¸åˆè§„æ€§é£é™©ï¼š**
   - **åœºæ™¯æè¿°ï¼š** ä¼ä¸šå¯èƒ½æœ‰åˆè§„è¦æ±‚ï¼Œç¡®ä¿æ‰€æœ‰ç³»ç»Ÿç»„ä»¶ä½¿ç”¨æœ€æ–°çš„å®‰å…¨é…ç½®å’Œå‡­æ®ã€‚
   - **é£é™©ä½“ç°ï¼š**
     - Pod ä½¿ç”¨æ—§çš„ Secretï¼Œå¯èƒ½è¿ååˆè§„è¦æ±‚ï¼Œå¸¦æ¥æ³•å¾‹å’Œç›‘ç®¡é£é™©ã€‚

**é£é™©è¯„çº§ï¼š**

æ ¹æ® **CVSS 3.1** è¯„åˆ†æ ‡å‡†ï¼š

- **æ”»å‡»å‘é‡ï¼ˆAVï¼‰ï¼š** ç½‘ç»œï¼ˆNï¼‰
- **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰ï¼š** ä½ï¼ˆLï¼‰
- **ç‰¹æƒè¦æ±‚ï¼ˆPRï¼‰ï¼š** ä½ï¼ˆLï¼‰â€”â€”éœ€è¦æœ‰åˆ›å»º Pod çš„æƒé™
- **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰ï¼š** æ— ï¼ˆNï¼‰
- **æœºå¯†æ€§å½±å“ï¼ˆCï¼‰ï¼š** é«˜ï¼ˆHï¼‰
- **å®Œæ•´æ€§å½±å“ï¼ˆIï¼‰ï¼š** é«˜ï¼ˆHï¼‰
- **å¯ç”¨æ€§å½±å“ï¼ˆAï¼‰ï¼š** ä½ï¼ˆLï¼‰

ç»¼åˆè¯„åˆ†å¯èƒ½è¾¾åˆ° **Highï¼ˆ7.5 - 8.8ï¼‰**ã€‚

**æ¦‚å¿µéªŒè¯ï¼ˆProof of Conceptï¼‰ï¼š**

1. **å‰ææ¡ä»¶ï¼š**
   - æ”»å‡»è€…å…·å¤‡åœ¨é›†ç¾¤ä¸­åˆ›å»º Pod çš„æƒé™ï¼ˆå¯èƒ½æ˜¯è¢«æ”»é™·çš„ä½æƒé™è´¦æˆ·ï¼‰ã€‚
   - ç®¡ç†å‘˜æ›´æ–°äº† Secretï¼Œä»¥æ’¤é”€æ—§çš„å‡­æ®ã€‚

2. **æ”»å‡»æ­¥éª¤ï¼š**
   - ç®¡ç†å‘˜æ‰§è¡Œæ›´æ–°æ“ä½œï¼Œæ›´æ¢ Secret ä¸­çš„æ•æ„Ÿä¿¡æ¯ã€‚
   - åœ¨ kubelet ç¼“å­˜å°šæœªæ›´æ–°çš„çŸ­æ—¶é—´çª—å£å†…ï¼Œæ”»å‡»è€…è¿…é€Ÿåˆ›å»ºä¸€ä¸ªæ–°çš„ Podï¼ŒæŒ‡å®šæŒ‚è½½ç›®æ ‡ Secretã€‚
   - ç”±äº kubelet ä½¿ç”¨äº†è¿‡æœŸçš„ç¼“å­˜ï¼ŒPod æŒ‚è½½äº†æ—§çš„ Secretã€‚
   - æ”»å‡»è€…åœ¨ Pod å†…è·å–æ—§çš„ Secretï¼Œç»§ç»­å¯¹æ•æ„Ÿèµ„æºè¿›è¡Œæœªæˆæƒçš„è®¿é—®ã€‚

3. **ç»“æœï¼š**
   - æ”»å‡»è€…æˆåŠŸç»•è¿‡äº† Secret æ›´æ–°å¸¦æ¥çš„æƒé™æ§åˆ¶ï¼Œç»´æŒäº†å¯¹æ•æ„Ÿèµ„æºçš„è®¿é—®ã€‚
   - å®‰å…¨æ§åˆ¶æªæ–½è¢«ç»•è¿‡ï¼Œå¯èƒ½å¯¼è‡´æ•°æ®æ³„éœ²ã€æœåŠ¡ç ´åç­‰ä¸¥é‡åæœã€‚

**å»ºè®®æªæ–½ï¼š**

- **åœ¨ Pod å¯åŠ¨æ—¶ï¼Œkubelet åº”ç›´æ¥è·å–æœ€æ–°çš„ Secret/ConfigMap èµ„æºï¼Œè€Œéä¾èµ–ç¼“å­˜ã€‚**
- **å¢åŠ ç¼“å­˜åˆ·æ–°æœºåˆ¶çš„åŠæ—¶æ€§ï¼Œç¡®ä¿åœ¨èµ„æºæ›´æ–°åï¼Œç¼“å­˜èƒ½å¤Ÿç«‹å³åæ˜ æœ€æ–°çŠ¶æ€ã€‚**
- **åŠ å¼ºæƒé™æ§åˆ¶ï¼Œé™åˆ¶è°å¯ä»¥åˆ›å»º Podï¼Œå‡å°‘æ½œåœ¨æ”»å‡»è€…åˆ©ç”¨è¯¥æ¼æ´çš„æœºä¼šã€‚**

---

## Issue #124680 Watch request for CRs costs about 10-15x more memory in k8s-apiserver than in-tree resource watches

- Issue é“¾æ¥ï¼š[#124680](https://github.com/kubernetes/kubernetes/issues/124680)

### Issue å†…å®¹

#### What happened?

I was running some load testing related to flux. When creating 10.000 kustomization custom resources (about 1KiB), the k8s apiserver consumes about 1GiB of memory. When checking with 100.000k and 300.00k, the k8s apiserver scales linearly.
When doing the same thing for 1KiB conifgmaps, creating 10.000 resources, the k8s apiserver consumes about 100 MiB of memory. 
Memory pprof for 10k kustomizations:
![10k kustomizations](https://github.com/kubernetes/kubernetes/assets/6106093/de508042-fe96-4ccc-a1ef-a6d535e611fa)

Memory pprof for 10k configmaps:
![image](https://github.com/kubernetes/kubernetes/assets/6106093/d710dae6-4818-460e-9384-b319938df92d)

The memory usage stays the same as long as the resources are existing. After looking a bit into what might force this, it seems that the kube-controller-manager sets up a watch for the kustomizations/configmaps resources.
```json
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"Metadata","auditID":"a4187168-a301-4fd1-907a-09da8fc3b587","stage":"RequestReceived","requestURI":"/apis/kustomize.toolkit.fluxcd.io/v1/kustomizations?allowWatchBookmarks=true\u0026resourceVersion=727\u0026timeout=5m37s\u0026timeoutSeconds=337\u0026watch=true","verb":"watch","user":{"username":"system:kube-controller-manager","groups":["system:authenticated"]},"sourceIPs":["172.18.0.2"],"userAgent":"kube-controller-manager/v1.29.2 (linux/amd64) kubernetes/4b8e819/metadata-informers","objectRef":{"resource":"kustomizations","apiGroup":"kustomize.toolkit.fluxcd.io","apiVersion":"v1"},"requestReceivedTimestamp":"2024-04-29T14:33:32.979279Z","stageTimestamp":"2024-04-29T14:33:32.979279Z"}
```

This is needed because garbage collector that runs in kube-controller-manager needs to walk the ownership reference map, and it wants to do that in cache: https://github.com/kubernetes/kubernetes/blob/a9eded097d092350a9f8de80a153b3f96841f012/pkg/controller/garbagecollector/garbagecollector.go#L253

#### What did you expect to happen?

Expectation would've been that there is similar memory usage for in-tree and custom resources.

Also, the current garbage collector seems to force k8s-apiserver to cache the full contents of etcd. Is that a correct implementation?

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a cluster
`kind create cluster`

2. Add the kustomize CRD
`curl -L https://raw.githubusercontent.com/fluxcd/kustomize-controller/main/config/crd/bases/kustomize.toolkit.fluxcd.io_kustomizations.yaml | kubectl apply -f -`

3. Create 10k of the following
```yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: podinfo
spec:
  interval: 10m
  targetNamespace: default
  sourceRef:
    kind: GitRepository
    name: podinfo
  path: "./kustomize"
  prune: true
  timeout: 1m
  patches:
  - patch: |-
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: not-used
      spec:
        template:
          metadata:
            annotations:
              cluster-autoscaler.kubernetes.io/safe-to-evict: "true"        
    target:
      kind: Deployment
      labelSelector: "app.kubernetes.io/part-of=my-app"
  - patch: |
      - op: add
        path: /spec/template/spec/securityContext
        value:
          runAsUser: 10000
          fsGroup: 1337
      - op: add
        path: /spec/template/spec/containers/0/securityContext
        value:
          readOnlyRootFilesystem: true
          allowPrivilegeEscalation: false
          runAsNonRoot: true
          capabilities:
            drop:
              - ALL        
    target:
      kind: Deployment
      name: podinfo
      namespace: apps
```

#### Anything else we need to know?



#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>
kind
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

è¯¥ Issue å­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ï¼Œå…·ä½“åˆ†æå¦‚ä¸‹ï¼š

**1. é£é™©æè¿°ï¼š**
åœ¨ Kubernetes é›†ç¾¤ä¸­ï¼Œæ”»å‡»è€…å¯ä»¥é€šè¿‡åˆ›å»ºå¤§é‡çš„è‡ªå®šä¹‰èµ„æºï¼ˆCRsï¼‰ï¼Œå¦‚ `Kustomization` èµ„æºï¼Œå¯¼è‡´ `kube-apiserver` æ¶ˆè€—å¤§é‡å†…å­˜ã€‚ç”±äºåƒåœ¾æ”¶é›†å™¨éœ€è¦ç›‘è§†è¿™äº›èµ„æºï¼Œ`kube-apiserver` ä¼šç¼“å­˜æ‰€æœ‰è¿™äº›èµ„æºçš„ä¿¡æ¯ï¼Œå†…å­˜ä½¿ç”¨é‡éšä¹‹çº¿æ€§å¢é•¿ã€‚è¿™å¯èƒ½å¯¼è‡´æœåŠ¡å™¨çš„å†…å­˜è€—å°½ï¼Œå¼•å‘æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚

**2. é£é™©å¯åˆ©ç”¨æ€§ï¼š**
æ”»å‡»è€…åªéœ€å…·å¤‡åˆ›å»ºè‡ªå®šä¹‰èµ„æºå®ä¾‹çš„ä½çº§æƒé™å³å¯å®æ–½æ”»å‡»ã€‚å¾ˆå¤šæƒ…å†µä¸‹ï¼Œæ™®é€šç”¨æˆ·å¯èƒ½è¢«æˆäºˆåˆ›å»ºæŸäº›å‘½åç©ºé—´ä¸‹èµ„æºçš„æƒé™ã€‚å¦‚æœæ”»å‡»è€…èƒ½å¤Ÿæ‰¹é‡åˆ›å»ºå¤§é‡çš„è‡ªå®šä¹‰èµ„æºï¼Œå°±èƒ½è§¦å‘è¯¥æ¼æ´ã€‚

**3. å¯èƒ½çš„å½±å“ï¼š**
- **å¯ç”¨æ€§å½±å“ï¼ˆAï¼‰ï¼šé«˜ï¼ˆHï¼‰**  
æ”»å‡»è€…å¯ä»¥å¯¼è‡´ `kube-apiserver` å†…å­˜è€—å°½ï¼Œå¯¼è‡´å…¶å´©æºƒæˆ–æ— æ³•å“åº”è¯·æ±‚ï¼Œå½±å“æ•´ä¸ª Kubernetes é›†ç¾¤çš„å¯ç”¨æ€§ã€‚
- **ä¸šåŠ¡å½±å“ï¼š**  
é›†ç¾¤ç®¡ç†å’Œä¸šåŠ¡åº”ç”¨å¯èƒ½å—åˆ°ä¸¥é‡å½±å“ï¼Œæ— æ³•æ­£å¸¸è°ƒåº¦å’Œç®¡ç†å·¥ä½œè´Ÿè½½ã€‚

**4. CVSS 3.1 è¯„åˆ†ï¼š**

| å‘é‡é¡¹                | è¯„çº§  |
|-----------------------|-------|
| æ”»å‡»å‘é‡ (AV)         | ç½‘ç»œ (N)   |
| æ”»å‡»å¤æ‚åº¦ (AC)       | ä½ (L)     |
| æ‰€éœ€ç‰¹æƒ (PR)         | ä½ (L)     |
| ç”¨æˆ·äº¤äº’ (UI)         | æ—  (N)     |
| ä½œç”¨èŒƒå›´ (S)          | æœªå˜ (U)   |
| æœºå¯†æ€§å½±å“ (C)        | æ—  (N)     |
| å®Œæ•´æ€§å½±å“ (I)        | æ—  (N)     |
| å¯ç”¨æ€§å½±å“ (A)        | é«˜ (H)     |

**ç»¼åˆè¯„åˆ†ï¼š7.5ï¼ˆé«˜ï¼‰**

**5. POCï¼ˆæ¦‚å¿µéªŒè¯ï¼‰ï¼š**

æŒ‰ç…§ Issue ä¸­æä¾›çš„æ­¥éª¤ï¼Œæ”»å‡»è€…å¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

**æ­¥éª¤1ï¼šåˆ›å»º Kubernetes é›†ç¾¤**
```bash
kind create cluster
```

**æ­¥éª¤2ï¼šæ·»åŠ è‡ªå®šä¹‰èµ„æºå®šä¹‰ï¼ˆCRDï¼‰**
```bash
curl -L https://raw.githubusercontent.com/fluxcd/kustomize-controller/main/config/crd/bases/kustomize.toolkit.fluxcd.io_kustomizations.yaml | kubectl apply -f -
```

**æ­¥éª¤3ï¼šæ‰¹é‡åˆ›å»ºå¤§é‡çš„ Kustomization èµ„æº**
ç¼–å†™ä¸€ä¸ªè„šæœ¬ï¼Œåˆ›å»ºä¾‹å¦‚ 10,000 ä¸ª `Kustomization` èµ„æºï¼š

```bash
for i in {1..10000}
do
cat <<EOF | kubectl apply -f -
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: podinfo-$i
spec:
  interval: 10m
  targetNamespace: default
  sourceRef:
    kind: GitRepository
    name: podinfo
  path: "./kustomize"
  prune: true
  timeout: 1m
EOF
done
```

**é¢„æœŸç»“æœï¼š**
- `kube-apiserver` çš„å†…å­˜ä½¿ç”¨é‡å°†æ˜¾è‘—å¢åŠ ï¼Œå¯èƒ½è¾¾åˆ°æ•° GiBã€‚
- å¦‚æœç»§ç»­å¢åŠ èµ„æºæ•°é‡ï¼Œ`kube-apiserver` å¯èƒ½ä¼šå‡ºç°å†…å­˜è€—å°½ï¼Œå¯¼è‡´æœåŠ¡å´©æºƒæˆ–æ— æ³•å“åº”ã€‚

**6. æ€»ç»“ï¼š**
è¯¥æ¼æ´å…è®¸å…·æœ‰ä½æƒé™çš„æ”»å‡»è€…é€šè¿‡åˆ›å»ºå¤§é‡è‡ªå®šä¹‰èµ„æºï¼Œå¯¼è‡´ `kube-apiserver` å†…å­˜è€—å°½ï¼Œä»è€Œå®æ–½æ‹’ç»æœåŠ¡æ”»å‡»ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†å’Œ CVSS è¯„åˆ†ï¼Œé£é™©ç­‰çº§ä¸ºé«˜ï¼ˆHighï¼‰ï¼Œåº”äºˆä»¥é‡è§†ï¼Œå¯èƒ½éœ€è¦åˆ†é… CVE ç¼–å·å¹¶åŠæ—¶ä¿®å¤ã€‚

---

# ğŸ“Œ ä¸æ¶‰åŠå®‰å…¨é£é™©çš„ Issues (58 ä¸ª)

## Issue #125242 runtime: failed to create new OS thread

- Issue é“¾æ¥ï¼š[#125242](https://github.com/kubernetes/kubernetes/issues/125242)

### Issue å†…å®¹

#### What happened?

[root@m01 log]# kubectl   get  pod  -A
runtime: failed to create new OS thread (have 3 already; errno=11)
runtime: may need to increase max user processes (ulimit -u)
fatal error: newosproc

runtime stack:
runtime.throw(0x1c1d651, 0x9)
        /usr/local/go/src/runtime/panic.go:1116 +0x72
runtime.newosproc(0xc0000b6400)
        /usr/local/go/src/runtime/os_linux.go:161 +0x1c5
runtime.newm1(0xc0000b6400)
        /usr/local/go/src/runtime/proc.go:1843 +0xdd
runtime.newm(0x1d07a08, 0xc00004a800, 0x2)
        /usr/local/go/src/runtime/proc.go:1822 +0x9b
runtime.startm(0x0, 0xc000048001)
        /usr/local/go/src/runtime/proc.go:1979 +0xc9
runtime.wakep()
        /usr/local/go/src/runtime/proc.go:2067 +0x66
runtime.newproc.func1()
        /usr/local/go/src/runtime/proc.go:3561 +0x97
runtime.systemstack(0x46c414)
        /usr/local/go/src/runtime/asm_amd64.s:370 +0x66
runtime.mstart()
        /usr/local/go/src/runtime/proc.go:1116

goroutine 1 [running, locked to thread]:
runtime.systemstack_switch()
        /usr/local/go/src/runtime/asm_amd64.s:330 fp=0xc0000b26f0 sp=0xc0000b26e8 pc=0x46c540
runtime.newproc(0x0, 0x1d078c8)
        /usr/local/go/src/runtime/proc.go:3554 +0x6e fp=0xc0000b2738 sp=0xc0000b26f0 pc=0x44214e
runtime.init.6()
        /usr/local/go/src/runtime/proc.go:243 +0x35 fp=0xc0000b2758 sp=0xc0000b2738 pc=0x439e35
runtime.doInit(0x2b17e80)
        /usr/local/go/src/runtime/proc.go:5652 +0x8a fp=0xc0000b2788 sp=0xc0000b2758 pc=0x446fca
runtime.main()
        /usr/local/go/src/runtime/proc.go:151 +0xd9 fp=0xc0000b27e0 sp=0xc0000b2788 pc=0x439b59
runtime.goexit()
        /usr/local/go/src/runtime/asm_amd64.s:1374 +0x1 fp=0xc0000b27e8 sp=0xc0000b27e0 pc=0x46e181

#### What did you expect to happen?

Output Pod information.

#### How can we reproduce it (as minimally and precisely as possible)?

I don't know how to repeat the problem.

#### Anything else we need to know?

$ ulimit -u
655350
$ ulimit -n
655350

systemctl edit kubelet.service
[Service]
LimitNOFILE=infinity
LimitNPROC=infinity

#### Kubernetes version

<details>

```console
$ kubectl version
v 1.19.16
```

</details>


#### Cloud provider

<details>
Physical server
</details>

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
CentOS Linux 7 (Core)

$ uname -a
linux m01 4.20.3-1.el7.elrepo.x86_64  x86_64  GNU/Linux
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Docker v24.0.6

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>

### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125226 Listen tcp :53: bind: permission denied ERROR!!

- Issue é“¾æ¥ï¼š[#125226](https://github.com/kubernetes/kubernetes/issues/125226)

### Issue å†…å®¹

#### What happened?

```
kubectl get po -A
NAMESPACE      NAME                                   READY   STATUS             RESTARTS      AGE
kube-flannel   kube-flannel-ds-962vp                  1/1     Running            0             15m
kube-flannel   kube-flannel-ds-qs6xr                  1/1     Running            0             15m
kube-system    coredns-7db6d8ff4d-6w776               0/1     CrashLoopBackOff   1 (10s ago)   12s
kube-system    coredns-7db6d8ff4d-99tng               0/1     CrashLoopBackOff   1 (9s ago)    12s
kube-system    etcd-controlplane                      1/1     Running            0             12m
kube-system    kube-apiserver-controlplane            1/1     Running            0             12m
kube-system    kube-controller-manager-controlplane   1/1     Running            0             12m
kube-system    kube-proxy-hbt5b                       1/1     Running            0             12m
kube-system    kube-proxy-x96js                       1/1     Running            0             11m
kube-system    kube-scheduler-controlplane            1/1     Running            0             12m
```

I was setting up the single-node k8s cluster (1 controlplane and 1 worker node). After going through all the installation process from the official [k8s](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/) site -> at last, after deploying the network plugin on the k8s cluster. CoreDNS pods went into a Crashloopbackoff state. I did check for the container logs and found the following error message:

```
 listen tcp :53: bind: permission denied
```

Please look into this and provide some insights. I have faced same issue while upgrading the cluster from v1.29 to v1.30. 


Thanks & Regards,
Tej Singh Rana

#### What did you expect to happen?

Both coreDNS pods should be in the running state, after deploying the network plugin. 

#### How can we reproduce it (as minimally and precisely as possible)?

Simply follow the steps from the official k8s site. 

#### Anything else we need to know?

I did some tests, and I used 1024 instead of 53 port, and it started to work. (AFAIK, using below port ~1000 was not working)
 
```console
kubectl logs -n kube-system coredns-7db6d8ff4d-wchnq 
.:1024
[INFO] plugin/reload: Running configuration SHA512 = e20da72760199c1bc59098f3ae16621ae48df8f7756e50bd0dfa5553ccb7be57af61562fff46a43fdcce51ac086b26aa19929386004908ad3afe3aea9b06316a
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
```
**Environment**:

- the version of CoreDNS: registry.k8s.io/coredns/coredns:v1.11.1
- Corefile: The below content is from the `coredns` configMap. 

```console
  Corefile: |
    .:53 {
        errors
        health {
           lameduck 5s
        }
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
           fallthrough in-addr.arpa ip6.arpa
           ttl 30
        }
        prometheus :9153
        forward . /etc/resolv.conf {
           max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance
    }
```
- logs:
```console
 listen tcp :53: bind: permission denied
```

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.0 <br>
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3 <br>
Server Version: v1.30.1

```

</details>


#### Cloud provider


N/A



#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release

PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

$ uname -a
Linux controlplane 5.4.0-1106-gcp #115~18.04.1-Ubuntu SMP Mon May 22 20:46:39 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>

```console
kubectl version 
Client Version: v1.30.0 
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3 
Server Version: v1.30.1
```

```console
kubelet --version

Kubernetes v1.30.0
```

```console
kubeadm version 

kubeadm version: &version.Info{Major:"1", Minor:"30", GitVersion:"v1.30.0",  GitCommit:"7c48c2bd72b9bf5c44d21d7338cc7bea77d0ad2a", GitTreeState:"clean", BuildDate:"2024-04-17T17:34:08Z", GoVersion:"go1.22.2", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd --version <br>
containerd containerd.io 1.6.26 3dd1e886e55dd695541fdcd67420c2888645a495
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
<pre>
   Service Account:  flannel
  Init Containers:
   install-cni-plugin:
    Image:      docker.io/flannel/flannel-cni-plugin:v1.4.1-flannel1
    Port:       <none>
    Host Port:  <none>
    Command:
      cp
    Args:
      -f
      /flannel
      /opt/cni/bin/flannel
    Environment:  <none>
    Mounts:
      /opt/cni/bin from cni-plugin (rw)
   install-cni:
    Image:      docker.io/flannel/flannel:v0.25.2
    Port:       <none>
</pre>
</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125217 Persistent 'Terminating' State for Old Pod after Migration to New Node

- Issue é“¾æ¥ï¼š[#125217](https://github.com/kubernetes/kubernetes/issues/125217)

### Issue å†…å®¹

#### What happened?

After a node is shut down, the pod on it migrates to another node and a new pod is established. However, the old pod remains in the Terminating state, continuously displayed, and has not been deleted promptly.

The screenshot is as followsï¼š

![åŸæ¥èŠ‚ç‚¹è¢«å…³é—­åï¼Œpodè¿ç§»åˆ°èŠ‚ç‚¹åï¼Œæ—§çš„èŠ‚ç‚¹ä¿¡æ¯ä¸€ç›´æ˜¾ç¤ºï¼Œæ²¡æœ‰è¢«åŠæ—¶åˆ é™¤](https://github.com/kubernetes/kubernetes/assets/168079959/7660b781-0fa8-4784-a615-d6ef8bde0694)


#### What did you expect to happen?

Can the terminating pod be  deleted promptly ? 

#### How can we reproduce it (as minimally and precisely as possible)?

no idea

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.2

</details>


#### Cloud provider

<details>
no 
</details>


#### OS version

<details>

NAME="CentOS Linux"
VERSION="8"


</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #125216 ResourceClaimParametersRef.APIGroup allows APIversion values

- Issue é“¾æ¥ï¼š[#125216](https://github.com/kubernetes/kubernetes/issues/125216)

### Issue å†…å®¹

#### What happened?

Following YAML file is accepted without error. Notice the `spec.parametersRef.apiGroup` field, it is an `APIVersion` instead of `APIGroup`, containing `/` character. This can lead to an issue when resource driver generates structured parameters' in-tree `ResourceClaimParameters` object with `generatedFrom.apiGroup` being just `APIGroup` without version. The DRA scheduler plugin will try to match the parameters object based on the reference here: https://github.com/kubernetes/kubernetes/blob/be4afb9ef90b19ccb6f7e595cbdb247e088b2347/pkg/scheduler/framework/plugins/dynamicresources/dynamicresources.go#L1066

In latest master the code appears to use an indexer, and the issue might be hidden, if applicable at all. I did not check the reproducibility in master yet. However, main question is whether the `APIGroup` should allow `APIVersion` value with `/` character in general.

```YAML
apiVersion: gpu.resource.intel.com/v1alpha2
kind: GpuClaimParameters
metadata:
  name: delayed-claim-external-gpu
spec:
  count: 1
  type: "gpu"
---
apiVersion: resource.k8s.io/v1alpha2
kind: ResourceClaim
metadata:
  name: delayed-claim-external-gpu
spec:
  resourceClassName: intel-gpu-structured
  parametersRef:
    apiGroup: gpu.resource.intel.com/v1alpha2
    kind: GpuClaimParameters
    name: delayed-claim-external-gpu
```

#### What did you expect to happen?

An error is expected from the apiserver upon object creation request, reporting that APIGroup containes APIVersion. Maybe.

#### How can we reproduce it (as minimally and precisely as possible)?

Have any resource driver that supports structured parameters, [dra-example-driver](https://github.com/kubernetes-sigs/dra-example-driver/pull/47/files#diff-df3210fefbab9f26fa97ba679b29869e25ce897d0ba713ce461da521f10ff7d8), for instance, also puts `APIGroup` (gpucrd.APIGroup) value into `generatedFrom.apiGroup`.

#### Anything else we need to know?

#### Kubernetes version

v1.30.1

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
scheduler's dynamicresources plugin
</details>


### åˆ†æç»“æœ

è¯¥ Issue ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #125205 [FG:InPlacePodVerticalScaling] Slow reconcile when quickly reverting resize patch

- Issue é“¾æ¥ï¼š[#125205](https://github.com/kubernetes/kubernetes/issues/125205)

### Issue å†…å®¹

#### What happened?

While working on https://github.com/kubernetes/kubernetes/pull/125202 and testing the second case (patching a pod to perform an in-place resize and then quickly reverting the patch before the resize has been actuated), I've discovered some unexpected behavior. In this case the pod eventually reconciles but it takes about 3 minutes, with the following test output:

```
[sig-node] Pod InPlace Resize Container [Feature:InPlacePodVerticalScaling] Burstable QoS pod, three containers - no change for c1, increase c2 resources, decrease c3 (net decrease for pod) [sig-node, Feature:InPlacePodVerticalScaling]
k8s.io/kubernetes/test/e2e/node/pod_resize.go:1281
  STEP: Creating a kubernetes client @ 05/29/24 18:13:28.391
  I0529 18:13:28.391963 3291 util.go:499] >>> kubeConfig: /root/kind-test-config
  I0529 18:13:28.393961 3291 util.go:508] >>> kubeContext: kind-kind
  STEP: Building a namespace api object, basename pod-resize @ 05/29/24 18:13:28.394
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/29/24 18:13:28.404
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/29/24 18:13:28.406
  STEP: Creating a kubernetes client @ 05/29/24 18:13:28.408
  I0529 18:13:28.408474 3291 util.go:499] >>> kubeConfig: /root/kind-test-config
  I0529 18:13:28.409838 3291 util.go:508] >>> kubeContext: kind-kind
  STEP: Building a namespace api object, basename pod-resize-resource-quota @ 05/29/24 18:13:28.41
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/29/24 18:13:28.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/29/24 18:13:28.417
  STEP: Creating a kubernetes client @ 05/29/24 18:13:28.419
  I0529 18:13:28.419504 3291 util.go:499] >>> kubeConfig: /root/kind-test-config
  I0529 18:13:28.420717 3291 util.go:508] >>> kubeContext: kind-kind
  STEP: Building a namespace api object, basename pod-resize-errors @ 05/29/24 18:13:28.42
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/29/24 18:13:28.427                                                                                                                                                                                                                                                                        
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/29/24 18:13:28.428                                                                                                                                                                                                                                                                                 
  STEP: creating pod @ 05/29/24 18:13:28.429                                                                                                                                                                                                                                                                                                                                
  STEP: verifying the pod is in kubernetes @ 05/29/24 18:13:30.448                                                                                                                                                                                                                                                                                                          
  STEP: verifying initial pod resources, allocations, and policy are as expected @ 05/29/24 18:13:30.453                                                                                                                                                                                                                                                                    
  STEP: verifying initial pod status resources and cgroup config are as expected @ 05/29/24 18:13:30.453                                                                                                                                                                                                                                                                    
  STEP: patching pod for resize @ 05/29/24 18:13:30.457                                                                                                                                                                                                                                                                                                                     
  STEP: verifying pod patched for resize @ 05/29/24 18:13:30.47                                                                                                                                                                                                                                                                                                             
  STEP: patching pod for rollback @ 05/29/24 18:13:30.516                                                                                                                                                                                                                                                                                                                   
  STEP: verifying pod patched for rollback @ 05/29/24 18:13:30.529                                                                                                                                                                                                                                                                                                          
  STEP: waiting for rollback to be actuated @ 05/29/24 18:13:30.529                                                                                                                                                                                                                                                                                                         
  I0529 18:13:30.534186 3291 builder.go:121] Running '/usr/local/bin/kubectl --server=https://127.0.0.1:60962 -- 
 kubeconfig=/root/kind-test-config --context=kind-kind --namespace=pod-resize-4426 exec testpod -- ls 
  /sys/fs/cgroup/cgroup.controllers'
  I0529 18:13:30.679258 3291 builder.go:146] stderr: "Defaulted container \"c1\" out of: c1, c2, c3\n"
  I0529 18:13:30.679299 3291 builder.go:147] stdout: "/sys/fs/cgroup/cgroup.controllers\n"
  I0529 18:13:30.679331 3291 pod_resize.go:379] Namespace pod-resize-4426 Pod testpod Container c1 - looking for cgroup value 209715200 in path /sys/fs/cgroup/memory.max
  I0529 18:13:30.679379 3291 builder.go:121] Running '/usr/local/bin/kubectl --server=https://127.0.0.1:60962 --kubeconfig=/root/kind-test-config --context=kind-kind --namespace=pod-resize-4426 exec testpod --namespace=pod-resize-4426 --container=c1 -- head -n 1 /sys/fs/cgroup/memory.max'
  I0529 18:13:30.819936 3291 builder.go:146] stderr: ""
  I0529 18:13:30.820015 3291 builder.go:147] stdout: "209715200\n"
  I0529 18:13:30.820050 3291 pod_resize.go:379] Namespace pod-resize-4426 Pod testpod Container c1 - looking for cgroup value 20000 100000 in path /sys/fs/cgroup/cpu.max
  I0529 18:13:30.820117 3291 builder.go:121] Running '/usr/local/bin/kubectl --server=https://127.0.0.1:60962 --kubeconfig=/root/kind-test-config --context=kind-kind --namespace=pod-resize-4426 exec testpod --namespace=pod-resize-4426 --container=c1 -- head -n 1 /sys/fs/cgroup/cpu.max'
  I0529 18:13:30.907604 3291 builder.go:146] stderr: ""
  I0529 18:13:30.907650 3291 builder.go:147] stdout: "20000 100000\n"
  I0529 18:13:30.907669 3291 pod_resize.go:379] Namespace pod-resize-4426 Pod testpod Container c1 - looking for cgroup value 4 in path /sys/fs/cgroup/cpu.weight
  I0529 18:13:30.907714 3291 builder.go:121] Running '/usr/local/bin/kubectl --server=https://127.0.0.1:60962 --kubeconfig=/root/kind-test-config --context=kind-kind --namespace=pod-resize-4426 exec testpod --namespace=pod-resize-4426 --container=c1 -- head -n 1 /sys/fs/cgroup/cpu.weight'
  I0529 18:13:30.993819 3291 builder.go:146] stderr: ""
  I0529 18:13:30.993869 3291 builder.go:147] stdout: "4\n"
  I0529 18:13:30.993895 3291 pod_resize.go:379] Namespace pod-resize-4426 Pod testpod Container c2 - looking for cgroup value 314572800 in path /sys/fs/cgroup/memory.max
  I0529 18:13:30.993951 3291 builder.go:121] Running '/usr/local/bin/kubectl --server=https://127.0.0.1:60962 --kubeconfig=/root/kind-test-config --context=kind-kind --namespace=pod-resize-4426 exec testpod --namespace=pod-resize-4426 --container=c2 -- head -n 1 /sys/fs/cgroup/memory.max'
  I0529 18:13:31.078830 3291 builder.go:146] stderr: ""
  I0529 18:13:31.078876 3291 builder.go:147] stdout: "367001600\n"
  I0529 18:13:33.080354 3291 builder.go:121] Running '/usr/local/bin/kubectl --server=https://127.0.0.1:60962 --kubeconfig=/root/kind-test-config --context=kind-kind --namespace=pod-resize-4426 exec testpod --namespace=pod-resize-4426 --container=c2 -- head -n 1 /sys/fs/cgroup/memory.max'
  I0529 18:13:33.205904 3291 builder.go:146] stderr: ""
  I0529 18:13:33.205947 3291 builder.go:147] stdout: "367001600\n"
  I0529 18:13:35.207569 3291 builder.go:121] Running '/usr/local/bin/kubectl --server=https://127.0.0.1:60962 --kubeconfig=/root/kind-test-config --context=kind-kind --namespace=pod-resize-4426 exec testpod --namespace=pod-resize-4426 --container=c2 -- head -n 1 /sys/fs/cgroup/memory.max'
  I0529 18:13:35.335993 3291 builder.go:146] stderr: ""
  I0529 18:13:35.336037 3291 builder.go:147] stdout: "367001600\n"
```

With the last couple of lines continuing indefinitely until it reads the "rolled back" value from `memory.max` in `c2` (3 mins in my case). This also happens inconsistently, you may have to rerun the test case a couple of times to hit it. You can also run the full suite (`-ginkgo.focus="Feature:InPlacePodVerticalScaling"`) to hit it instead of the specific case provided below.


#### What did you expect to happen?

After patching forwards and patching backwards, the pod should reach its initial state, and `memory.max` in `c2` should be set to its initial value.

#### How can we reproduce it (as minimally and precisely as possible)?

- Check out https://github.com/kubernetes/kubernetes/pull/125202
- Uncomment [this line](https://github.com/kubernetes/kubernetes/pull/125202/files#diff-44a1da1c31a8cd8913e1b8cfb2893c5436bb7c60a4d51c9707f9368bf755ef8fR1391) - `patchAndVerifyAborted`
- Comment both `patchAndVerify` lines (so we only run the "aborted" case), and comment [this line](https://github.com/kubernetes/kubernetes/pull/125202/files#diff-44a1da1c31a8cd8913e1b8cfb2893c5436bb7c60a4d51c9707f9368bf755ef8fR1362)
- Run `e2e.test` with `-ginkgo.focus="Burstable QoS pod, three containers - decrease c1 resources" -num-nodes 2` (you can also run the full suite (`-ginkgo.focus="Feature:InPlacePodVerticalScaling"`) - you may have to run it a couple of times)

#### Anything else we need to know?

This could also be an issue with my test changes, so please flag anything that seems suspicious. If you leave `patchAndVerify` uncommented (so patch -> wait for resize -> patch back -> wait for resize) and then run `patchAndVerifyAborted` (on the same case), you will hit a different bug, which I will file separately. 

#### Kubernetes version

<details>

```console
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.4", GitCommit:"872a965c6c6526caa949f0c6ac028ef7aff3fb78", GitTreeState:"clean", BuildDate:"2022-11-09T13:36:36Z", GoVersion:"go1.19.3", Compiler:"gc", Platform:"darwin/arm64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"31+", GitVersion:"v1.31.0-alpha.0.947+1ff1207d22ab5c-dirty", GitCommit:"1ff1207d22ab5cf442c8dafdf5bded1e32519873", GitTreeState:"dirty", BuildDate:"2024-05-28T19:28:33Z", GoVersion:"go1.22.3", Compiler:"gc", Platform:"linux/arm64"}
WARNING: version difference between client (1.25) and server (1.31) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 11 (bullseye)"
NAME="Debian GNU/Linux"
VERSION_ID="11"
VERSION="11 (bullseye)"
VERSION_CODENAME=bullseye
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"
$ uname -a
Linux docker-desktop 5.15.49-linuxkit #1 SMP PREEMPT Tue Sep 13 07:51:32 UTC 2022 aarch64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kind version 0.17.0
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

```console
NAME                 STATUS   ROLES           AGE     VERSION                                    INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
kind-control-plane   Ready    control-plane   6h29m   v1.31.0-alpha.0.947+1ff1207d22ab5c-dirty   172.18.0.4    <none>        Ubuntu 22.04.1 LTS   5.15.49-linuxkit   containerd://1.6.9
kind-worker          Ready    <none>          6h29m   v1.31.0-alpha.0.947+1ff1207d22ab5c-dirty   172.18.0.2    <none>        Ubuntu 22.04.1 LTS   5.15.49-linuxkit   containerd://1.6.9
kind-worker2         Ready    <none>          6h29m   v1.31.0-alpha.0.947+1ff1207d22ab5c-dirty   172.18.0.3    <none>        Ubuntu 22.04.1 LTS   5.15.49-linuxkit   containerd://1.6.9
```

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125194 kubelet option ImageMaximumGCAge not accepting value in format day and hour (1d1h)

- Issue é“¾æ¥ï¼š[#125194](https://github.com/kubernetes/kubernetes/issues/125194)

### Issue å†…å®¹

#### What happened?

kubelet option **imageMaximumGCAge** which allows an admin to specify a time after which unused images will be garbage collected by the Kubelet, regardless of disk usage. The value is specified as a Kubernetes duration; for example, you can set the configuration field to 3d12h, which means 3 days and 12 hours.
We set the value and kubelet is failing to start and below error is showing in journalctl logs

**May 29 15:46:05 node-10-120-127-63 kubelet[31112]: E0529 15:46:05.091483   31112 run.go:74] "command failed" err="failed to load kubelet config file, path: /var/lib/kubelet/config.yaml, error: failed to load Kubelet config file /var/lib/kubelet/config.yaml, error failed to decode: time: unknown unit \"d\" in duration \"1d1h\**

This is how option is set in kubelet config
imageMaximumGCAge: 1d1h

If I change the duration to only hour e.g. 1h then it works without any issue

Here is the description about this option

https://kubernetes.io/docs/concepts/architecture/garbage-collection/





#### What did you expect to happen?

option **imageMaximumGCAge** should accept the value as mentioned in the documentation in the format day and hours (1d1h) 

#### How can we reproduce it (as minimally and precisely as possible)?

Edit the kubelet configuration file _/var/lib/kubelet/config.yaml_ and append the option _imageMaximumGCAge: 1d1h_ 
vi /var/lib/kubelet/config.yaml
Append below line
imageMaximumGCAge: 1d1h

Restart the kubelet service
sytemctl restart kubelet


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.1



```
</details>


#### Cloud provider

<details>
Virtual machines are deployed on KVM environment
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="SLES"
VERSION="15-SP3"
VERSION_ID="15.3"
PRETTY_NAME="SUSE Linux Enterprise Server 15 SP3"
ID="sles"
ID_LIKE="suse"
ANSI_COLOR="0;32"
CPE_NAME="cpe:/o:suse:sles:15:sp3"
DOCUMENTATION_URL="https://documentation.suse.com/"

$ uname -a
Linux node-10-120-127-62 5.3.18-57-default #1 SMP Wed Apr 28 10:54:41 UTC 2021 (ba3c2e9) x86_64 x86_64 x86_64 GNU/Linux




```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
#crictl version
Version:  0.1.0
RuntimeName:  containerd
RuntimeVersion:  v1.6.28
RuntimeApiVersion:  v1


</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125191 The APIServer memory usage is high.

- Issue é“¾æ¥ï¼š[#125191](https://github.com/kubernetes/kubernetes/issues/125191)

### Issue å†…å®¹

#### What happened?

![image](https://github.com/kubernetes/kubernetes/assets/54977497/2541c771-b727-4031-8cfd-f093ecbeea3c)
The apiserver memory usage is high. The pprof command output shows that the processEvent method occupies a large amount of memory.The processEvent method takes up about 40% of the memory

#### What did you expect to happen?

The APIServer memory usage is not high when the number of requests is stable.

#### How can we reproduce it (as minimally and precisely as possible)?

I don't know how it happened.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.28
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125188 Resource Quotas does not work correctly for services.nodeports

- Issue é“¾æ¥ï¼š[#125188](https://github.com/kubernetes/kubernetes/issues/125188)

### Issue å†…å®¹

#### What happened?

We have Resources Quotes for **services.nodeports** with an allowed limit of 5 and for **services.loadbalancers** with an allowed limit of 1.

```
apiVersion: v1
kind: ResourceQuota
metadata:
  name: network-resources
spec:
  hard:
    services.nodeports: "5"
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-resources
spec:
  hard:
    services.loadbalancers: "1"
```

We deploy a **NodePort** type service in the amount of 1 with the publication of 4 ports, one **ClusterIP** type service and the use of the services.nodeports 4/5 resource quota.

```
k get Resourcequotas -n test
NAME                AGE   REQUEST                       LIMIT
compute-resources   18m   services.loadbalancers: 0/3   
network-resources   95s   services.nodeports: 4/5       

k get svc -n test
NAME                        TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                                                     AGE
my-service-cip              ClusterIP   10.36.39.25    <none>        80/TCP                                                      31m
my-service-np   NodePort    10.36.36.151   <none>        80:30626/TCP,8081:25225/TCP,8083:28082/TCP,9000:26419/TCP   2m33s
```

We deploy a service of the **LoadBalancers** type in the amount of 1 with the publication of 1 port and get the use of the resource quota services.loadbalancers: 1/3   
services.nodeports: 5/5.

```
k get resourcequotas -n test
NAME                AGE     REQUEST                       LIMIT
compute-resources   20m     services.loadbalancers: 1/3   
network-resources   3m24s   services.nodeports: 5/5       

â¯ k get svc -n test
NAME                        TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                                                     AGE
my-service-lb                  LoadBalancer   10.36.32.236   <pending>     80:32268/TCP                                                16s
my-service-cip              ClusterIP      10.36.39.25    <none>        80/TCP                                                      33m
my-service-np   NodePort       10.36.36.151   <none>        80:30626/TCP,8081:25225/TCP,8083:28082/TCP,9000:26419/TCP   4m17s
```

The described behavior has been tested on versions Kubernetes 1.20, 1.27-1.30, and the result is the same for all.

#### What did you expect to happen?

In  the documentation https://kubernetes.io/docs/concepts/policy/resource-quotas/#object-count-quota the use of the resource quota for **services.nodeports** is described as **"The total number of Services of type NodePort that can exist in the namespace."**

In fact, each published port is considered for the services.nodeports resource quota, regardless of the specified type of Load Balancer or NodePort service.

#### How can we reproduce it (as minimally and precisely as possible)?

Create Resource Quota

```
apiVersion: v1
kind: ResourceQuota
metadata:
  name: network-resources
spec:
  hard:
    services.nodeports: "5"
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-resources
spec:
  hard:
    services.loadbalancers: "1"
```

Create service

```
apiVersion: v1
kind: Service
metadata:
  name: my-service-cip
spec:
  selector:
    app.kubernetes.io/name: MyApp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: my-service-np
spec:
  externalTrafficPolicy: Local
  ports:
  - name: api
    port: 80
    protocol: TCP
    targetPort: 8080
  - name: probe
    port: 8081
    protocol: TCP
    targetPort: 8081
  - name: prof
    port: 8083
    protocol: TCP
    targetPort: 8083
  - name: metrics
    port: 9000
    protocol: TCP
    targetPort: 9000
  selector:
    app.kubernetes.io/name: my-service-np
  sessionAffinity: None
  type: NodePort
---
apiVersion: v1
kind: Service
metadata:
  name: my-service-lb
spec:
  externalTrafficPolicy: Local
  ports:
  - name: api
    port: 80
    protocol: TCP
    targetPort: 8080
  - name: probe
    port: 8081
    protocol: TCP
    targetPort: 8081
  - name: prof
    port: 8083
    protocol: TCP
    targetPort: 8083
  - name: metrics
    port: 9000
    protocol: TCP
    targetPort: 9000
  selector:
    app.kubernetes.io/name: my-service-lb
  sessionAffinity: None
  type: LoadBalancer
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

The described behavior has been tested on versions Kubernetes 1.20, 1.27-1.30, and the result is the same for all.

</details>


#### Cloud provider

<details>
none
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/

$ uname -a
Linux k8s-xc-01.stage.xxx.ru 6.1.0-18-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.76-1 (2024-02-01) x86_64 GNU/Linu
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
conteinerd
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
CNI calico
</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #125185 Job controller reports the count of ready pods with unnecessary delay

- Issue é“¾æ¥ï¼š[#125185](https://github.com/kubernetes/kubernetes/issues/125185)

### Issue å†…å®¹

#### What happened?

When Job controller is deleting pods the counter in the status.ready field does not reflect this properly (there is a delay until the cache is refreshed). This affects the scenarios of terminating job, suspended job, and excess pods deleted.

#### What did you expect to happen?

The ready field is updated as soon as the delete requests are sent successfully, in sync with the status.active field.

#### How can we reproduce it (as minimally and precisely as possible)?

1. To see this you can scale down a job, use the following yaml:
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: simple-job
spec:
  completions: 2
  parallelism: 2
  completionMode: Indexed
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: 'worker'
        image: python
        command:
        - python3
        - -c
        - |
          import time
          time.sleep(300)
```

2. Watch for the job updates with the following watcher:
```
kubectl get jobs -w --output-watch-events -ocustom-columns=EVENT:.type,NAME:.object.metadata.name,READY:.object.status.ready,ACTIVE:.object.status.active,TERMINATING:.object.status.terminating,CONDITIONS:.object.status.conditions | ts "%Y-%m-%d %H:%M:%.S"`
```
3. Scale down the job by `kubectl edit job` and decrease parallelism and completions to 1.

We observe events like the following
```
2024-05-29 12:49:58.790101 ADDED     simple-job   <none>   <none>   <none>        <none>
2024-05-29 12:49:58.810008 MODIFIED   simple-job   0        2        0             <none>
2024-05-29 12:50:02.065710 MODIFIED   simple-job   2        2        0             <none>
2024-05-29 12:50:17.526562 MODIFIED   simple-job   2        2        0             <none>
2024-05-29 12:50:17.542893 MODIFIED   simple-job   2        1        0             <none>
2024-05-29 12:50:18.537404 MODIFIED   simple-job   1        1        1             <none>
2024-05-29 12:50:48.785143 MODIFIED   simple-job   1        1        0             <none>
2024-05-29 12:55:01.515515 MODIFIED   simple-job   0        1        0             <none>
2024-05-29 12:55:02.519571 MODIFIED   simple-job   0        <none>   0             <none>
2024-05-29 12:55:02.529433 MODIFIED   simple-job   0        <none>   0             [map[lastProbeTime:2024-05-29T10:55:02Z lastTransitionTime:2024-05-29T10:55:02Z status:True type:Complete]]
```
Note that for one update at `2024-05-29 12:50:17.542893` ready is `2` while `active` is already `1`. We can set `ready` to `1` at this point already. Note that the pod was terminating for around 30s. 

Also note, that the event at `2024-05-29 12:50:17.542893` has `terminating=0`. This value could be set to `1` already, and this is being addressed in https://github.com/kubernetes/kubernetes/issues/125089.

#### Anything else we need to know?

This is related to https://github.com/kubernetes/kubernetes/issues/123775. We need this fix to be able to re-introduce the validation rule that `ready <= active` which was withdrawn here: https://github.com/kubernetes/kubernetes/pull/123792.


The culprit of the issue is that we determine the count of ready pods by filtering active pods [here](https://github.com/kubernetes/kubernetes/blob/afebfdc5d411859468153e2b9921915f3e4f9ff4/pkg/controller/job/job_controller.go#L832). However, the count of active pods might be changed later. 

**Note that**: This issue is slightly more complex than https://github.com/kubernetes/kubernetes/pull/123792. We cannot unconditionally decrement the counter for ready pods based on the number of successfully deleted pods. The functions deleting pods should probably also return the counters of successfully deleted ready pods (along with the count of successfully deleted pods).

#### Kubernetes version

<details>
1.30

</details>


#### Cloud provider

<details>
Kind
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125184 DNS resolution fails within the cluster, and it can only resolve the Pods deployed on the same host

- Issue é“¾æ¥ï¼š[#125184](https://github.com/kubernetes/kubernetes/issues/125184)

### Issue å†…å®¹

#### What happened?


---

I have set up a cluster and deployed our production application. However, there is an issue with connectivity between microservices. Upon investigation, I found that our services cannot access the Eureka registry service using the internal cluster domain name.

The Eureka service is deployed on the nb-17-8 server. Pods deployed on the same server, nb-17-8, can access Eureka using DNS. For example, the three replicas of Eureka need to communicate with each other for synchronization (which works fine):

```
ndsp          eureka-0                                       1/1     Running                 1          71m     10.244.86.42    nb-17-8     <none>           <none>
ndsp          eureka-1                                       1/1     Running                 1          71m     10.244.86.44    nb-17-8     <none>           <none>
ndsp          eureka-2                                       1/1     Running                 1          71m     10.244.86.45    nb-17-8     <none>           <none>
```

Within the Eureka pods on nb-17-8, they cannot access 10.96.0.1 or 10.96.0.10:

```
kubectl exec -it -n ndsp eureka-0 -- sh
/ # nc -v 10.96.0.1 443
10.96.0.1 (10.96.0.1:443) open
^Cpunt!

/ # nc -v  10.96.0.10 53
10.96.0.10 (10.96.0.10:53) open
^Cpunt!
```

On other hosts, the Service IPs are accessible:

```
[root@nb-17-57 ~]#  nc -v 10.96.0.1 443
Ncat: Version 7.70 ( https://nmap.org/ncat )
Ncat: Connected to 10.96.0.1:443.
^C
[root@nb-17-57 ~]#  nc -v 10.96.0.10 53
Ncat: Version 7.70 ( https://nmap.org/ncat )
Ncat: Connected to 10.96.0.10:53.
```

On the hosts, accessing the Eureka pod via domain name is not possible, but using the Pod IP works (from all hosts and pods):

```
[root@nb-17-57 ~]# curl -I 10.244.86.42:8761
HTTP/1.1 200 
Content-Type: text/html;charset=UTF-8
Content-Language: en-US
Transfer-Encoding: chunked
Date: Wed, 29 May 2024 11:14:14 GMT
```

In the CoreDNS pod, there are the following error messages:

```
[ERROR] plugin/errors: 2 eureka-2.eureka. A: read udp 10.244.0.11:44838->8.8.8.8:53: i/o timeout
[ERROR] plugin/errors: 2 eureka-2.eureka. A: read udp 10.244.0.11:51686->8.8.4.4:53: i/o timeout
[ERROR] plugin/errors: 2 eureka-2.eureka. A: read udp 10.244.0.11:54068->8.8.4.4:53: i/o timeout
```

All hosts have the following `resolv.conf` file content (since it's a closed internal network, domain name resolution uses the hosts file and there is no internal DNS server):

```
cat /etc/resolv.conf
; generated by /usr/sbin/dhclient-script
```

Does anyone have any good solutions?

---

#### What did you expect to happen?

Good DNS resolution



#### How can we reproduce it (as minimally and precisely as possible)?

I donâ€™t know, the environment is a big problem

#### Anything else we need to know?

_No response_

#### Kubernetes version

Kubernetes version: 1.20.0

#### Cloud provider

<details>

</details>


#### OS version

Host OS: BigCloud Enterprise Linux release 8.2.2107 (Core) ï¼ˆLike Centosï¼‰


#### Install tools

Kubeadm

#### Container runtime (CRI) and version (if applicable)

Docker  19.03.5

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

CNI and version: flannel v0.11.0

### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125172 Kube-system pods are in crashlookbackup state when the OS is upgraded from RHEL 7.9 to 9.3.

- Issue é“¾æ¥ï¼š[#125172](https://github.com/kubernetes/kubernetes/issues/125172)

### Issue å†…å®¹

#### What happened?

We have a cluster with 3 master and 2 worker nodes. One the master node went through RHEL OS upgrade from 7.9V to 9.3V. after this upgrade of OS the kubelet packages got upgrade to 1.28.15 automatically and resulted in Node failure for which we had to downgrade the kubelet and kubeadm packages to 1.22.15 which is our current version of kubernetes. On doing so we had the node back in ready state. but after this the KUBE-SYSTEM pods running on this server are failing with crashloopback. 

#### What did you expect to happen?

After the downgrade of the Kubelet and Kubeadm packages we expected the kube-system pods to run fine. 

#### How can we reproduce it (as minimally and precisely as possible)?

1. Run kubernetes 1.22.15 on a RHEL server 7.9v 
2. Upgrade the OS to 9.3 RHEL version
3. You can see the Kubelet and kubeadm along with other kubenetes packages will be upgraded to 1.28V
4. now donwgrade the related k8S packages to 1.22.15V
5. now you can reproduce that Kube-system pods running this particular servers wont come up.
6. You can leave the other2 master nodes running on 1.22.15v of k8s on RHEl 7.9V

#### Anything else we need to know?

I wanted to know if we can get a list of kubernetes packages or its needed dependency packages which can be avoid during the OS upgrade of RHEL from 7.9 to 9.3V.

I believe if you can give the list of packages that will get affected during the RHEL OS upgrade, would be of great help. 

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
``` Client Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.15", GitCommit:"1d79bc3bcccfba7466c44cc2055d6e7442e140ea", GitTreeState:"clean", BuildDate:"2022-09-21T12:18:10Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"linux/amd64"}
The connection to the server localhost:8080 was refused - did you specify the right host or port?


</details>


#### Cloud provider

<details>

</details>
NA

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```
NAME="Red Hat Enterprise Linux"
VERSION="9.3 (Plow)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="9.3"
PLATFORM_ID="platform:el9"
PRETTY_NAME="Red Hat Enterprise Linux 9.3 (Plow)"
ANSI_COLOR="0;31"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:redhat:enterprise_linux:9::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9"
BUG_REPORT_URL="https://issues.redhat.com/"

REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 9"
REDHAT_BUGZILLA_PRODUCT_VERSION=9.3
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.3"

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125170 k8s future build failure with Golang tip (1.23), testDeps interface needs updating

- Issue é“¾æ¥ï¼š[#125170](https://github.com/kubernetes/kubernetes/issues/125170)

### Issue å†…å®¹

#### What happened?

Hello k8s folks:

This code here:

https://github.com/kubernetes/kubernetes/blob/cb9844915686832cf58add8d4b76d2fec9857fd1/pkg/util/coverage/coverage.go#L85

is making a call to the function `testing.MainStart`, which is documented as not maintaining the Go 1 compatibility guarantee. Here is the header comment for MainStart:

```
// MainStart is meant for use by tests generated by 'go test'.
// It is not meant to be called directly and is not subject to the Go 1 compatibility document.
// It may change signature from release to release.
```

Filing this issue to let you know that in the upcoming Go release (1.23) the signature of MainStart is indeed changing; there is a new method on the testDeps interface.

If you could please add this new method to your dummy testDeps interface (as in the previous patch  https://github.com/kubernetes/kubernetes/commit/cb9844915686832cf58add8d4b76d2fec9857fd1 that would be great.

Thanks.

@thanm from the golang team.


#### What did you expect to happen?

N/A

#### How can we reproduce it (as minimally and precisely as possible)?

N/A

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
./kubectl version
Client Version: v0.0.0-master+$Format:%H$
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #125167 ExternalTrafficPolicy: Local does not work for NodePort Services

- Issue é“¾æ¥ï¼š[#125167](https://github.com/kubernetes/kubernetes/issues/125167)

### Issue å†…å®¹

#### What happened?

I have a 3-node Kubernetes cluster. When I create a deployment with 2 nginx pods and a NodePort service with externalTrafficPolicy: Local, I find that one of the servers is not correctly forwarding traffic with ipvsadm.

Here is the nginx.yaml
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    name: nginx
spec:
  externalTrafficPolicy: Local
  type: NodePort
  ports:
    - port: 80
      name: http
  selector:
    app: nginx
```
pods status
```bash
root@qt-10-106-120-216:~# kubectl get po -o wide
NAME                                READY   STATUS    RESTARTS   AGE   IP            NODE                NOMINATED NODE   READINESS GATES
nginx-deployment-77d8468669-5dbrg   1/1     Running   0          13m   10.244.1.28   qt-10-106-120-43    <none>           <none>
nginx-deployment-77d8468669-747gb   1/1     Running   0          20m   10.244.2.2    qt-10-106-120-210   <none>           <none>
```

On vm-1: qt-10-106-120-216

Output of ipvsadm -Ln:
```console
TCP  10.106.120.216:31764 rr
  -> 10.244.1.28:80               Masq    1      0          0
  -> 10.244.2.2:80                Masq    1      0          0
```

On vm-2: qt-10-106-120-210

Output of ipvsadm -Ln:
```console
TCP  10.106.120.43:31764 rr
  -> 10.244.1.28:80               Masq    1      0          0
```

On vm-3: qt-10-106-120-43

Output of ipvsadm -Ln:
```console
TCP  10.106.120.210:31764 rr
  -> 10.244.2.2:80                Masq    1      0          0
```

When I update the nginx deployment to have 3 pods:
```console
root@qt-10-106-120-216:~# kubectl get po -o wide
NAME                                READY   STATUS    RESTARTS   AGE   IP            NODE                     NOMINATED NODE   READINESS GATES
nginx-deployment-77d8468669-5dbrg   1/1     Running   0          21m   10.244.1.28   qt-10-106-120-43         <none>           <none>
nginx-deployment-77d8468669-747gb   1/1     Running   0          28m   10.244.2.2    qt-10-106-120-210        <none>           <none>
nginx-deployment-77d8468669-g68qg   1/1     Running   0          62s   10.244.0.4    qt-core-10-106-120-216   <none>           <none>
```
On vm-1: qt-10-106-120-216

Output of ipvsadm -Ln:
```console
TCP  10.106.120.216:31764 rr
  -> 10.244.0.4:80                Masq    1      0          0
```

On vm-2: qt-10-106-120-210

Output of ipvsadm -Ln:
```console
TCP  10.106.120.43:31764 rr
  -> 10.244.1.28:80               Masq    1      0          0
```

On vm-3: qt-10-106-120-43

Output of ipvsadm -Ln:
```console
TCP  10.106.120.210:31764 rr
  -> 10.244.2.2:80                Masq    1      0          0
```


#### What did you expect to happen?

When the nginx deployment has only 2 pods, the node without a pod should directly drop the traffic to ip:nodeport, rather than forwarding it to other nodes.

vm-1: qt-10-106-120-216

Output of ipvsadm -Ln:
```console
TCP  10.106.120.216:31764 rr
```

#### How can we reproduce it (as minimally and precisely as possible)?
kubeadm.yaml
<details>

```yaml
apiVersion: kubeadm.k8s.io/v1beta3
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: abcdef.0123456789abcdef
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.106.120.216
  bindPort: 6443
nodeRegistration:
  criSocket: /run/containerd/containerd.sock
  imagePullPolicy: IfNotPresent
  name: qt-core-10-106-120-216
  taints: null
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta3
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns: {}
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: registry.aliyuncs.com/google_containers
kind: ClusterConfiguration
kubernetesVersion: v1.30.1
networking:
  dnsDomain: cluster.local
  podSubnet: 10.244.0.0/16
  serviceSubnet: 10.96.0.0/12
scheduler: {}
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: ipvs 
ipvs:
  strictARP: true
---
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
cgroupDriver: systemd
```

</details>

I have an initialization script that runs on vm-1: qt-10-106-120-216.

<details>

```console
#!/bin/bash

# Temporarily disable swap
swapoff -a
# Permanently disable swap
sed -i.bak '/swap/s/^/#/' /etc/fstab

systemctl stop ufw

# Load modules
cat <<EOF> /etc/modules-load.d/k8s.conf
br_netfilter
overlay
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
EOF

modprobe br_netfilter
modprobe overlay
modprobe ip_vs
modprobe ip_vs_rr
modprobe ip_vs_wrr
modprobe ip_vs_sh

cat <<EOF> /etc/sysctl.d/99-kubernetes-cri.conf
arp_ignore=1
arp_announce=2
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

sysctl --system

apt update -y
apt install -y apt-transport-https
mkdir /etc/apt/keyrings
curl -fsSL https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.30/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.30/deb/ /" | tee /etc/apt/sources.list.d/kubernetes.list
apt update -y
apt install -y kubelet kubeadm kubectl containerd

mkdir /etc/containerd -p

containerd config default > /etc/containerd/config.toml

# Change cgroups to systemd
sed -i 's#SystemdCgroup = false#SystemdCgroup = true#' /etc/containerd/config.toml

# Change the base infrastructure image
sed -i 's#sandbox_image = "registry.k8s.io/pause"#sandbox_image = "registry.aliyuncs.com/google_containers/pause"#' /etc/containerd/config.toml

systemctl daemon-reload
systemctl restart containerd
systemctl enable containerd

cat <<EOF> /var/lib/kubelet/config.yaml
KUBELET_KUBEADM_ARGS="--container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9"
EOF
systemctl enable kubelet.service
systemctl restart kubelet.service

# Download required images
kubeadm config images pull --image-repository registry.aliyuncs.com/google_containers
# Check if images are downloaded
kubeadm config images list --config kubeadm.yaml

# Initialize the cluster
kubeadm init --config kubeadm.yaml # kubeadm.yaml needs to be modified with hostname and IP address

# Get the real user's home directory under sudo
ORIGINAL_HOME=""

if [ -n "$SUDO_USER" ]; then
    ORIGINAL_HOME=$(getent passwd "$SUDO_USER" | cut -d: -f6)
    # Get the original user's user ID and group ID
    ORIGINAL_UID=$(getent passwd "$SUDO_USER" | cut -d: -f3)
    ORIGINAL_GID=$(getent passwd "$SUDO_USER" | cut -d: -f4)
else
    ORIGINAL_HOME="$HOME"
    ORIGINAL_UID=$(id -u)
    ORIGINAL_GID=$(id -g)
fi

mkdir -p $ORIGINAL_HOME/.kube
cp -i /etc/kubernetes/admin.conf $ORIGINAL_HOME/.kube/config
# Use the original user's user ID and group ID
chown $ORIGINAL_UID:$ORIGINAL_GID $ORIGINAL_HOME/.kube/config

# Remove taints
kubectl taint nodes --all node-role.kubernetes.io/control-plane-
kubectl taint nodes --all node-role.kubernetes.io/master-
```

</details>

Run the following script on other nodes and execute kubeadm join to join the cluster
vm-2: qt-10-106-120-210.
vm-3: qt-10-106-120-43.

<details>

```console
#!/bin/bash

# Temporarily disable swap
swapoff -a
# Permanently disable swap
sed -i.bak '/swap/s/^/#/' /etc/fstab

systemctl stop ufw

# Load modules
cat <<EOF> /etc/modules-load.d/k8s.conf
br_netfilter
overlay
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
EOF

modprobe br_netfilter
modprobe overlay
modprobe ip_vs
modprobe ip_vs_rr
modprobe ip_vs_wrr
modprobe ip_vs_sh

cat <<EOF> /etc/sysctl.d/99-kubernetes-cri.conf
arp_ignore=1
arp_announce=2
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

sysctl --system

apt update -y
apt install -y apt-transport-https
mkdir /etc/apt/keyrings
curl -fsSL https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.30/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.30/deb/ /" | tee /etc/apt/sources.list.d/kubernetes.list
apt update -y
apt install -y kubelet kubeadm kubectl containerd

mkdir /etc/containerd -p

containerd config default > /etc/containerd/config.toml

# Change cgroups to systemd
sed -i 's#SystemdCgroup = false#SystemdCgroup = true#' /etc/containerd/config.toml

# Change the base infrastructure image
sed -i 's#sandbox_image = "registry.k8s.io/pause"#sandbox_image = "registry.aliyuncs.com/google_containers/pause"#' /etc/containerd/config.toml

systemctl daemon-reload
systemctl restart containerd
systemctl enable containerd

cat <<EOF> /var/lib/kubelet/config.yaml
KUBELET_KUBEADM_ARGS="--container-runtime=remote --runtime-request-timeout=15m --container-runtime-endpoint=unix:///run/containerd/containerd.sock --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.9"
EOF
systemctl enable kubelet.service
systemctl restart kubelet.service

# Download required images
kubeadm config images pull --image-repository registry.aliyuncs.com/google_containers
```

</details>

Install the flannel plugin.

<details>

```yaml
---
kind: Namespace
apiVersion: v1
metadata:
  name: kube-flannel
  labels:
    pod-security.kubernetes.io/enforce: privileged
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: flannel
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes/status
  verbs:
  - patch
- apiGroups:
  - "networking.k8s.io"
  resources:
  - clustercidrs
  verbs:
  - list
  - watch
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: flannel
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: flannel
subjects:
- kind: ServiceAccount
  name: flannel
  namespace: kube-flannel
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flannel
  namespace: kube-flannel
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kube-flannel-cfg
  namespace: kube-flannel
  labels:
    tier: node
    app: flannel
data:
  cni-conf.json: |
    {
      "name": "cbr0",
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "flannel",
          "delegate": {
            "hairpinMode": true,
            "isDefaultGateway": true
          }
        },
        {
          "type": "portmap",
          "capabilities": {
            "portMappings": true
          }
        }
      ]
    }
  net-conf.json: |
    {
      "Network": "10.244.0.0/16",
      "Backend": {
        "Type": "vxlan"
      }
    }
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-flannel-ds
  namespace: kube-flannel
  labels:
    tier: node
    app: flannel
spec:
  selector:
    matchLabels:
      app: flannel
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/os
                operator: In
                values:
                - linux
      hostNetwork: true
      priorityClassName: system-node-critical
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni-plugin
        image: docker.io/flannel/flannel-cni-plugin:v1.1.2
       #image: docker.io/rancher/mirrored-flannelcni-flannel-cni-plugin:v1.1.2
        command:
        - cp
        args:
        - -f
        - /flannel
        - /opt/cni/bin/flannel
        volumeMounts:
        - name: cni-plugin
          mountPath: /opt/cni/bin
      - name: install-cni
        image: docker.io/flannel/flannel:v0.21.2
       #image: docker.io/rancher/mirrored-flannelcni-flannel:v0.21.2
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: docker.io/flannel/flannel:v0.21.2
       #image: docker.io/rancher/mirrored-flannelcni-flannel:v0.21.2
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: false
          capabilities:
            add: ["NET_ADMIN", "NET_RAW"]
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: EVENT_QUEUE_DEPTH
          value: "5000"
        volumeMounts:
        - name: run
          mountPath: /run/flannel
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
        - name: xtables-lock
          mountPath: /run/xtables.lock
      volumes:
      - name: run
        hostPath:
          path: /run/flannel
      - name: cni-plugin
        hostPath:
          path: /opt/cni/bin
      - name: cni
        hostPath:
          path: /etc/cni/net.d
      - name: flannel-cfg
        configMap:
          name: kube-flannel-cfg
      - name: xtables-lock
        hostPath:
          path: /run/xtables.lock
          type: FileOrCreate
```

</details>

Install the nginx.

<details>

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2 # tells deployment to run 2 pods matching the template
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    name: nginx
spec:
  externalTrafficPolicy: Local
  type: NodePort
  ports:
    - port: 80
      name: http
  selector:
    app: nginx
```

</details>

Use the curl command to access the IP:nodeport of a node where the nginx pod does not exist.




#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Ubuntu"
VERSION="20.04.3 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.3 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal
$ uname -a
# Linux qt-10-106-120-216 5.4.0-99-generic #112-Ubuntu SMP Thu Feb 3 13:50:55 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

```console
kubeadm version: &version.Info{Major:"1", Minor:"30", GitVersion:"v1.30.1", GitCommit:"6911225c3f747e1cd9d109c305436d08b668f086", GitTreeState:"clean", BuildDate:"2024-05-14T10:49:05Z", GoVersion:"go1.22.2", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

```console
containerd github.com/containerd/containerd 1.7.2
```
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

flannel:v0.21.2

<details>

```yaml
---
kind: Namespace
apiVersion: v1
metadata:
  name: kube-flannel
  labels:
    pod-security.kubernetes.io/enforce: privileged
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: flannel
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes/status
  verbs:
  - patch
- apiGroups:
  - "networking.k8s.io"
  resources:
  - clustercidrs
  verbs:
  - list
  - watch
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: flannel
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: flannel
subjects:
- kind: ServiceAccount
  name: flannel
  namespace: kube-flannel
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: flannel
  namespace: kube-flannel
---
kind: ConfigMap
apiVersion: v1
metadata:
  name: kube-flannel-cfg
  namespace: kube-flannel
  labels:
    tier: node
    app: flannel
data:
  cni-conf.json: |
    {
      "name": "cbr0",
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "flannel",
          "delegate": {
            "hairpinMode": true,
            "isDefaultGateway": true
          }
        },
        {
          "type": "portmap",
          "capabilities": {
            "portMappings": true
          }
        }
      ]
    }
  net-conf.json: |
    {
      "Network": "10.244.0.0/16",
      "Backend": {
        "Type": "vxlan"
      }
    }
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: kube-flannel-ds
  namespace: kube-flannel
  labels:
    tier: node
    app: flannel
spec:
  selector:
    matchLabels:
      app: flannel
  template:
    metadata:
      labels:
        tier: node
        app: flannel
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/os
                operator: In
                values:
                - linux
      hostNetwork: true
      priorityClassName: system-node-critical
      tolerations:
      - operator: Exists
        effect: NoSchedule
      serviceAccountName: flannel
      initContainers:
      - name: install-cni-plugin
        image: docker.io/flannel/flannel-cni-plugin:v1.1.2
       #image: docker.io/rancher/mirrored-flannelcni-flannel-cni-plugin:v1.1.2
        command:
        - cp
        args:
        - -f
        - /flannel
        - /opt/cni/bin/flannel
        volumeMounts:
        - name: cni-plugin
          mountPath: /opt/cni/bin
      - name: install-cni
        image: docker.io/flannel/flannel:v0.21.2
       #image: docker.io/rancher/mirrored-flannelcni-flannel:v0.21.2
        command:
        - cp
        args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        volumeMounts:
        - name: cni
          mountPath: /etc/cni/net.d
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
      containers:
      - name: kube-flannel
        image: docker.io/flannel/flannel:v0.21.2
       #image: docker.io/rancher/mirrored-flannelcni-flannel:v0.21.2
        command:
        - /opt/bin/flanneld
        args:
        - --ip-masq
        - --kube-subnet-mgr
        resources:
          requests:
            cpu: "100m"
            memory: "50Mi"
        securityContext:
          privileged: false
          capabilities:
            add: ["NET_ADMIN", "NET_RAW"]
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: EVENT_QUEUE_DEPTH
          value: "5000"
        volumeMounts:
        - name: run
          mountPath: /run/flannel
        - name: flannel-cfg
          mountPath: /etc/kube-flannel/
        - name: xtables-lock
          mountPath: /run/xtables.lock
      volumes:
      - name: run
        hostPath:
          path: /run/flannel
      - name: cni-plugin
        hostPath:
          path: /opt/cni/bin
      - name: cni
        hostPath:
          path: /etc/cni/net.d
      - name: flannel-cfg
        configMap:
          name: kube-flannel-cfg
      - name: xtables-lock
        hostPath:
          path: /run/xtables.lock
          type: FileOrCreate
```
</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125164 Parallel image pull should pull unique images only once

- Issue é“¾æ¥ï¼š[#125164](https://github.com/kubernetes/kubernetes/issues/125164)

### Issue å†…å®¹

#### What would you like to be added?

Creating large amount of pods that all use the same image causes the kubelet to start pull per each pod, when parallel pull is enabled. This causes the reigstry QPS to be used, slowing down other pods with other images and also wastes the bandwidth for downloading the same images multiple times.

If same image is being already downloaded in parallel, other pods should just wait for it, not starting new download and not using up registry QPS. 

#### Why is this needed?

We have use case where we start large number (500+) pods at the same time where many pods gets scheduled on single node (100+), and most pods have same init container with the same image (given the image on the container is provided with full SHA digest).

This causes lot of bandwidth on the node to be used during the startup of those pods, and also some pods are delayed due to `ImagePullBackoff` caused by `pull QPS exceeded`.

### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125155 problem when calling the API interface to create or delete  PVC resources

- Issue é“¾æ¥ï¼š[#125155](https://github.com/kubernetes/kubernetes/issues/125155)

### Issue å†…å®¹

#### What happened?

When calling the API interface to create PVC resources, PVC is not yet in a bound state (still in a pending state), The API interface call has returned. When calling the API interface to delete PVC resources, PVC is still in the terminating state and has not been completely removed, but the API interface has returned success

#### What did you expect to happen?

When calling the API interface to create PVC resources, when PVC is in a bound state, The API interface call returned. When calling the API interface to delete PVC resources, PVC  has been completely removed, the API interface call returned success.

#### How can we reproduce it (as minimally and precisely as possible)?

calling the API interface to create or delete  PVC resources

#### Anything else we need to know?

_No response_

#### Kubernetes version

v1.23


#### Cloud provider

none

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125134 ResourceQuota computed used resources for extended resources is wrong

- Issue é“¾æ¥ï¼š[#125134](https://github.com/kubernetes/kubernetes/issues/125134)

### Issue å†…å®¹

#### What happened?

When using the `ResourceQuotas` admission controller for extended resources (`nvidia.com/gpu`) the reported used resources are inconsistently and wrong.

#### What did you expect to happen?

The reported used extended resources should respect the current status in the Namespace.

#### How can we reproduce it (as minimally and precisely as possible)?

Apply the following ResourceQuota

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: capsule-skg00000-2
spec:
  hard:
    requests.nvidia.com/gpu: "10"
```

Check the RQ is applied correctly and reported the expected status:

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"ResourceQuota","metadata":{"annotations":{},"name":"capsule-skg00000-2","namespace":"skg00000-gpu"},"spec":{"hard":{"requests.nvidia.com/gpu":"10"}}}
  creationTimestamp: "2024-05-26T17:00:28Z"
  labels:
    capsule.clastix.io/managed-by: skg00000
  name: capsule-skg00000-2
  namespace: skg00000-gpu
  resourceVersion: "8225530"
  uid: 3badf2c2-9959-4882-9315-7ffff127e08e
spec:
  hard:
    requests.nvidia.com/gpu: "10"
status:
  hard:
    requests.nvidia.com/gpu: "10"
  used:
    requests.nvidia.com/gpu: "0"
```

Create a Deployment with a template consuming the extended resource (GPU).

```yaml
spec:        
  automountServiceAccountToken: false   
  containers:
  - args:    
    - sleep 3600      
    command: 
    - /bin/bash       
    - -c     
    - --     
    image: nvidia/cuda:11.0.3-base-ubuntu20.04   
    imagePullPolicy: Always    
    name: nvidia      
    resources:        
      limits:
        cpu: "1"      
        memory: 1Gi   
        nvidia.com/gpu: "1"    
      requests:       
        cpu: "1"      
        memory: 1Gi   
        nvidia.com/gpu: "1"
```

After the Pod is deployed, the ResourceQuota reports the wrong amount of used resources:

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"ResourceQuota","metadata":{"annotations":{},"name":"capsule-skg00000-2","namespace":"skg00000-gpu"},"spec":{"hard":{"requests.nvidia.com/gpu":"10"}}}
  creationTimestamp: "2024-05-26T17:00:28Z"
  labels:
    capsule.clastix.io/managed-by: skg00000
  name: capsule-skg00000-2
  namespace: skg00000-gpu
  resourceVersion: "8225807"
  uid: 3badf2c2-9959-4882-9315-7ffff127e08e
spec:
  hard:
    requests.nvidia.com/gpu: "10"
status:
  hard:
    requests.nvidia.com/gpu: "10"
  used:
    requests.nvidia.com/gpu: "2"
```

The same applies when performing scale out of the Deployment, with inconsistent reporting.

```
$: kubectl scale deployment gpu-workload --replicas=2
$ RQ reports 4 used instances
$: kubectl scale deployment gpu-workload --replicas=3
$ RQ reports 6 used instances
$: kubectl scale deployment gpu-workload --replicas=4
$ RQ reports 8 used instances
```

Upon scaling down, the used instances are still inconsistent from time to time.

```
$: kubectl scale deployment gpu-workload --replicas=3
$ RQ reports 3 used instances
$: kubectl scale deployment gpu-workload --replicas=2
$ RQ reports 3 used instances
$: kubectl scale deployment gpu-workload --replicas=1
$ RQ reports 1 used instances
$: kubectl scale deployment gpu-workload --replicas=0
$ RQ reports 0 used instances
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.4
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 24.04 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo
$ uname -a
Linux REDACTED 6.8.0-31-generic #31-Ubuntu SMP PREEMPT_DYNAMIC Sat Apr 20 00:40:06 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
N.R.
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
N.R.
</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125133 Watch of a single namespace using /api/v1/watch/namespaces/$name missing all events in 1.27+

- Issue é“¾æ¥ï¼š[#125133](https://github.com/kubernetes/kubernetes/issues/125133)

### Issue å†…å®¹

#### What happened?

We upgraded our test cluster to `1.28.8` (GKE `1.28.8-gke.1095000`) and noticed that some of our tests began to time out when watching for events that never arrived. 

Upon investigation, it appears that there is a regression in the deprecated path-based watch API for at least `namespaces` e.g. `http://localhost:8001/api/v1/watch/namespaces/<name>`. We have not been able to reproduce this with any other resource type.

When creating a watch for a namespace (see `how to reproduce`) we see the initial `added` event, but then we do not see subsequent events if we modify the namespace e.g. add a label or annotation. 

#### What did you expect to happen?

I would expect to see events when the namespace is altered. E.g. adding a label should produce a `MODIFIED` event with the contents.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a new test namespace e.g. `kubectl create namespace test-namespace`
2. Create a proxy to the master `kubectl proxy`
3. Curl the watch path of the namespace that has been created e.g. `http://localhost:8001/api/v1/watch/namespaces/test-namespace`
4. In another shell, modify the namespace to add a label to it.
5. Observe that there is no modified event in the watch.

#### Anything else we need to know?

It is understood that the `/watch` based paths are and have been for some time deprecated, we are working on moving to use the watch query param instead.

#### Kubernetes version

```
K8s Version: v.1.28.8
GKE Version: 1.28.8-gke.1095000
Kubectl Version: v1.28.7
```

#### Cloud provider

<details>
We are using Google Kubernetes Engine, please see version above.
</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125130 Regarding the issue of certificate expiration when accessing links with Https in kubernetes through pod access

- Issue é“¾æ¥ï¼š[#125130](https://github.com/kubernetes/kubernetes/issues/125130)

### Issue å†…å®¹

#### What happened?

![image](https://github.com/kubernetes/kubernetes/assets/46311200/5f7f6d63-c825-47ca-80a0-7784a370f86d)
I curl any link with HTTPS in the ingress nginx controller pod in Kubernetes, and the HTTPS certificate will expire at the same time. However, I have no problem curling on the node, and I have no problem starting an nginx container curl using Docker

#### What did you expect to happen?

I curl any link with HTTPS in the ingress nginx controller pod in Kubernetes, and the HTTPS certificate will expire at the same time. However, I have no problem curling on the node, and I have no problem starting an nginx container curl using Docker

#### How can we reproduce it (as minimally and precisely as possible)?

Use Proxmox Virtual Environment and start running a kubernetes cluster, then run an nginx container curl https link

#### Anything else we need to know?

_No response_

#### Kubernetes version

[root@Rocky ~]#  kubectl version
Client Version: v1.29.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.24.0
WARNING: version difference between client (1.29) and server (1.24) exceeds the supported minor version skew of +/-1



#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>
![image](https://github.com/kubernetes/kubernetes/assets/46311200/6023ab68-775f-487a-a3fb-c84c0c068bc0)
![image](https://github.com/kubernetes/kubernetes/assets/46311200/6c9a2fd8-30f1-4421-9660-7657dcd66ddb)


#### Install tools

<details>
Kind  kubeadm
</details>
I have installed both Kind and Kubeadm and encountered the same problem

#### Container runtime (CRI) and version (if applicable)

<details>
![image](https://github.com/kubernetes/kubernetes/assets/46311200/0b862854-970d-4e82-9eb7-a555187731a8)

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125098 kubernetes-sigs / scheduler-plugins  go.mod Error 

- Issue é“¾æ¥ï¼š[#125098](https://github.com/kubernetes/kubernetes/issues/125098)

### Issue å†…å®¹

#### What happened?

Trying to create scheduler plugin by referring link: https://github.com/kubernetes-sigs/scheduler-plugins.git 
Getting error while executing "go mod tidy"

go: finding module for package sigs.k8s.io/scheduler-plugins/pkg/generated/applyconfiguration/scheduling/v1alpha1
go: finding module for package sigs.k8s.io/scheduler-plugins/pkg/generated/clientset/versioned/typed/scheduling/v1alpha1
go: finding module for package sigs.k8s.io/scheduler-plugins/pkg/generated/clientset/versioned
go: finding module for package sigs.k8s.io/scheduler-plugins/pkg/generated/clientset/versioned/typed/scheduling/v1alpha1/fake
go: finding module for package sigs.k8s.io/scheduler-plugins/pkg/generated/clientset/versioned/scheme
go: finding module for package sigs.k8s.io/scheduler-plugins/pkg/generated/informers/externalversions/internalinterfaces
go: finding module for package sigs.k8s.io/scheduler-plugins/pkg/generated/informers/externalversions/scheduling
go: finding module for package sigs.k8s.io/scheduler-plugins/pkg/generated/informers/externalversions/scheduling/v1alpha1
go: finding module for package sigs.k8s.io/scheduler-plugins/pkg/generated/listers/scheduling/v1alpha1
go: sigs.k8s.io/scheduler-plugins/sigs.k8s.io/scheduler-plugins/pkg/generated/applyconfiguration imports
        sigs.k8s.io/scheduler-plugins/pkg/generated/applyconfiguration/scheduling/v1alpha1: no matching versions for query "latest"
go: sigs.k8s.io/scheduler-plugins/sigs.k8s.io/scheduler-plugins/pkg/generated/clientset/versioned imports
        sigs.k8s.io/scheduler-plugins/pkg/generated/clientset/versioned/typed/scheduling/v1alpha1: no matching versions for query "latest"
go: sigs.k8s.io/scheduler-plugins/sigs.k8s.io/scheduler-plugins/pkg/generated/clientset/versioned/fake imports
        sigs.k8s.io/scheduler-plugins/pkg/generated/clientset/versioned: no matching versions for query "latest"
go: sigs.k8s.io/scheduler-plugins/sigs.k8s.io/scheduler-plugins/pkg/generated/clientset/versioned/fake imports
        sigs.k8s.io/scheduler-plugins/pkg/generated/clientset/versioned/typed/scheduling/v1alpha1/fake: no matching versions for query "latest"
go: sigs.k8s.io/scheduler-plugins/sigs.k8s.io/scheduler-plugins/pkg/generated/clientset/versioned/typed/scheduling/v1alpha1 imports
        sigs.k8s.io/scheduler-plugins/pkg/generated/clientset/versioned/scheme: no matching versions for query "latest"
go: sigs.k8s.io/scheduler-plugins/sigs.k8s.io/scheduler-plugins/pkg/generated/informers/externalversions imports
        sigs.k8s.io/scheduler-plugins/pkg/generated/informers/externalversions/internalinterfaces: no matching versions for query "latest"
go: sigs.k8s.io/scheduler-plugins/sigs.k8s.io/scheduler-plugins/pkg/generated/informers/externalversions imports
        sigs.k8s.io/scheduler-plugins/pkg/generated/informers/externalversions/scheduling: no matching versions for query "latest"
go: sigs.k8s.io/scheduler-plugins/sigs.k8s.io/scheduler-plugins/pkg/generated/informers/externalversions/scheduling imports
        sigs.k8s.io/scheduler-plugins/pkg/generated/informers/externalversions/scheduling/v1alpha1: no matching versions for query "latest"
go: sigs.k8s.io/scheduler-plugins/sigs.k8s.io/scheduler-plugins/pkg/generated/informers/externalversions/scheduling/v1alpha1 imports
        sigs.k8s.io/scheduler-plugins/pkg/generated/listers/scheduling/v1alpha1: no matching versions for query "latest"
  

#### What did you expect to happen?

no error

#### How can we reproduce it (as minimally and precisely as possible)?

How to update mod.go

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>
not applicable

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>
Not applicable

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125089 Job controller reports the count of terminating pods with unnecessary delay

- Issue é“¾æ¥ï¼š[#125089](https://github.com/kubernetes/kubernetes/issues/125089)

### Issue å†…å®¹

#### What happened?

When Job controller is terminating and deletes all pods the counter in the `status.terminating` field does not reflect this properly. 

#### What did you expect to happen?

The counter for the terminating pods is updated as soon as Job controller has this information. Similarly as for the counter of active pods.

#### How can we reproduce it (as minimally and precisely as possible)?

0. watch the job & pod updates with the commands:
`kubectl get jobs -w --output-watch-events -ocustom-columns=EVENT:.type,NAME:.object.metadata.name,STATUS:.object.status | ts "%Y-%m-%d %H:%M:%.S"`
`kubectl get pods -w --output-watch-events -ocustom-columns=EVENT:.type,NAME:.object.metadata.name,PHASE:.object.status.phase,FINALIZERS:.object.metadata.finalizers,DELETION:object.metadata.deletionTimestamp,CONTAINERS:.object.status.containerStatuses | ts "%Y-%m-%d %H:%M:%.S"`
1. Run the Job:
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: indexed-job
  labels:
    jobgroup: indexedjob
spec:
  completions: 2
  parallelism: 2
  backoffLimit: 0
  completionMode: Indexed
  podReplacementPolicy: Failed
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: 'worker'
        image: python
        readinessProbe:
          httpGet:
            path: /non-existent-endpoint
            port: 80
          initialDelaySeconds: 1
          periodSeconds: 10
          failureThreshold: 10
        command:
        - python3
        - -c
        - |
          import os, sys, time
          print("Hello world")
          if int(os.environ.get("JOB_COMPLETION_INDEX")) == 0:
            time.sleep(60)
            sys.exit(1)
          time.sleep(300)
          sys.exit(0)
```
3. When the job transitions you can observe updates like this:
For Job:
```
2024-05-23 13:52:11.053575 EVENT   NAME          STATUS
2024-05-23 13:52:11.053833 ADDED   indexed-job   map[]
2024-05-23 13:52:11.076666 MODIFIED   indexed-job   map[active:2 ready:0 startTime:2024-05-23T11:52:11Z terminating:0 uncountedTerminatedPods:map[]]
2024-05-23 13:53:15.315997 MODIFIED   indexed-job   map[ready:0 startTime:2024-05-23T11:52:11Z terminating:0 uncountedTerminatedPods:map[failed:[b237e4f0-dc57-4349-8aa6-7bc381266aed d4dfe535-4b6d-4374-8e9d-b73c9823cd35]]]
2024-05-23 13:53:15.328668 MODIFIED   indexed-job   map[conditions:[map[lastProbeTime:2024-05-23T11:53:15Z lastTransitionTime:2024-05-23T11:53:15Z message:Job has reached the specified backoff limit reason:BackoffLimitExceeded status:True type:Failed]] failed:2 ready:0 startTime:2024-05-23T11:52:11Z terminating:0 uncountedTerminatedPods:map[]]
```
**Issue**: the job reports `terminating: 0` even though it already deleted the second pod, because it is enumerated in the 
`uncountedTerminatedPods`. Also, it is inconsistent, `active` is reported as `0` and `terminating` is 0 as well, while setting `Failed` condition, the pods are in fact still terminating. This can be visible from the pod updates:
```
2024-05-23 13:52:11.063000 EVENT   NAME                  PHASE     FINALIZERS                           DELETION   CONTAINERS
2024-05-23 13:52:11.063188 ADDED   indexed-job-0-wz49q   Pending   [batch.kubernetes.io/job-tracking]   <none>     <none>
2024-05-23 13:52:11.066376 MODIFIED   indexed-job-0-wz49q   Pending   [batch.kubernetes.io/job-tracking]   <none>     <none>
2024-05-23 13:52:11.069335 ADDED      indexed-job-1-jhl9t   Pending   [batch.kubernetes.io/job-tracking]   <none>     <none>
2024-05-23 13:52:11.076759 MODIFIED   indexed-job-1-jhl9t   Pending   [batch.kubernetes.io/job-tracking]   <none>     <none>
2024-05-23 13:52:11.079168 MODIFIED   indexed-job-0-wz49q   Pending   [batch.kubernetes.io/job-tracking]   <none>     [map[image:python imageID: lastState:map[] name:worker ready:false restartCount:0 started:false state:map[waiting:map[reason:ContainerCreating]]]]
2024-05-23 13:52:11.089661 MODIFIED   indexed-job-1-jhl9t   Pending   [batch.kubernetes.io/job-tracking]   <none>     [map[image:python imageID: lastState:map[] name:worker ready:false restartCount:0 started:false state:map[waiting:map[reason:ContainerCreating]]]]
2024-05-23 13:52:13.031692 MODIFIED   indexed-job-0-wz49q   Running   [batch.kubernetes.io/job-tracking]   <none>     [map[containerID:containerd://d4b0a03a1aef94a10d4397a0d82a79cc6edbf32293de038efc17d16e87d9e75f image:docker.io/library/python:latest imageID:docker.io/library/python@sha256:3966b81808d864099f802080d897cef36c01550472ab3955fdd716d1c665acd6 lastState:map[] name:worker ready:false restartCount:0 started:true state:map[running:map[startedAt:2024-05-23T11:52:12Z]]]]
2024-05-23 13:52:14.034589 MODIFIED   indexed-job-1-jhl9t   Running   [batch.kubernetes.io/job-tracking]   <none>     [map[containerID:containerd://e1f78aec0a1b9b1735f77192f2878afec72aecabb8afc7a40e065f425380d800 image:docker.io/library/python:latest imageID:docker.io/library/python@sha256:3966b81808d864099f802080d897cef36c01550472ab3955fdd716d1c665acd6 lastState:map[] name:worker ready:false restartCount:0 started:true state:map[running:map[startedAt:2024-05-23T11:52:13Z]]]]
2024-05-23 13:53:13.140294 MODIFIED   indexed-job-0-wz49q   Running   [batch.kubernetes.io/job-tracking]   <none>     [map[containerID:containerd://d4b0a03a1aef94a10d4397a0d82a79cc6edbf32293de038efc17d16e87d9e75f image:docker.io/library/python:latest imageID:docker.io/library/python@sha256:3966b81808d864099f802080d897cef36c01550472ab3955fdd716d1c665acd6 lastState:map[] name:worker ready:false restartCount:0 started:false state:map[terminated:map[containerID:containerd://d4b0a03a1aef94a10d4397a0d82a79cc6edbf32293de038efc17d16e87d9e75f exitCode:1 finishedAt:2024-05-23T11:53:12Z reason:Error startedAt:2024-05-23T11:52:12Z]]]]
2024-05-23 13:53:14.300875 MODIFIED   indexed-job-0-wz49q   Failed    [batch.kubernetes.io/job-tracking]   <none>     [map[containerID:containerd://d4b0a03a1aef94a10d4397a0d82a79cc6edbf32293de038efc17d16e87d9e75f image:docker.io/library/python:latest imageID:docker.io/library/python@sha256:3966b81808d864099f802080d897cef36c01550472ab3955fdd716d1c665acd6 lastState:map[] name:worker ready:false restartCount:0 started:false state:map[terminated:map[containerID:containerd://d4b0a03a1aef94a10d4397a0d82a79cc6edbf32293de038efc17d16e87d9e75f exitCode:1 finishedAt:2024-05-23T11:53:12Z reason:Error startedAt:2024-05-23T11:52:12Z]]]]
2024-05-23 13:53:15.142013 MODIFIED   indexed-job-0-wz49q   Failed    [batch.kubernetes.io/job-tracking]   <none>     [map[containerID:containerd://d4b0a03a1aef94a10d4397a0d82a79cc6edbf32293de038efc17d16e87d9e75f image:docker.io/library/python:latest imageID:docker.io/library/python@sha256:3966b81808d864099f802080d897cef36c01550472ab3955fdd716d1c665acd6 lastState:map[] name:worker ready:false restartCount:0 started:false state:map[terminated:map[containerID:containerd://d4b0a03a1aef94a10d4397a0d82a79cc6edbf32293de038efc17d16e87d9e75f exitCode:1 finishedAt:2024-05-23T11:53:12Z reason:Error startedAt:2024-05-23T11:52:12Z]]]]
2024-05-23 13:53:15.308553 MODIFIED   indexed-job-1-jhl9t   Running   [batch.kubernetes.io/job-tracking]   2024-05-23T11:53:45Z   [map[containerID:containerd://e1f78aec0a1b9b1735f77192f2878afec72aecabb8afc7a40e065f425380d800 image:docker.io/library/python:latest imageID:docker.io/library/python@sha256:3966b81808d864099f802080d897cef36c01550472ab3955fdd716d1c665acd6 lastState:map[] name:worker ready:false restartCount:0 started:true state:map[running:map[startedAt:2024-05-23T11:52:13Z]]]]
2024-05-23 13:53:15.324258 MODIFIED   indexed-job-0-wz49q   Failed    <none>                               <none>                 [map[containerID:containerd://d4b0a03a1aef94a10d4397a0d82a79cc6edbf32293de038efc17d16e87d9e75f image:docker.io/library/python:latest imageID:docker.io/library/python@sha256:3966b81808d864099f802080d897cef36c01550472ab3955fdd716d1c665acd6 lastState:map[] name:worker ready:false restartCount:0 started:false state:map[terminated:map[containerID:containerd://d4b0a03a1aef94a10d4397a0d82a79cc6edbf32293de038efc17d16e87d9e75f exitCode:1 finishedAt:2024-05-23T11:53:12Z reason:Error startedAt:2024-05-23T11:52:12Z]]]]
2024-05-23 13:53:15.324497 MODIFIED   indexed-job-1-jhl9t   Running   <none>                               2024-05-23T11:53:45Z   [map[containerID:containerd://e1f78aec0a1b9b1735f77192f2878afec72aecabb8afc7a40e065f425380d800 image:docker.io/library/python:latest imageID:docker.io/library/python@sha256:3966b81808d864099f802080d897cef36c01550472ab3955fdd716d1c665acd6 lastState:map[] name:worker ready:false restartCount:0 started:true state:map[running:map[startedAt:2024-05-23T11:52:13Z]]]]
2024-05-23 13:53:45.569196 MODIFIED   indexed-job-1-jhl9t   Failed    <none>                               2024-05-23T11:53:45Z   [map[containerID:containerd://e1f78aec0a1b9b1735f77192f2878afec72aecabb8afc7a40e065f425380d800 image:docker.io/library/python:latest imageID:docker.io/library/python@sha256:3966b81808d864099f802080d897cef36c01550472ab3955fdd716d1c665acd6 lastState:map[] name:worker ready:false restartCount:0 started:false state:map[terminated:map[containerID:containerd://e1f78aec0a1b9b1735f77192f2878afec72aecabb8afc7a40e065f425380d800 exitCode:137 finishedAt:2024-05-23T11:53:45Z reason:Error startedAt:2024-05-23T11:52:13Z]]]]
2024-05-23 13:53:46.198066 MODIFIED   indexed-job-1-jhl9t   Failed    <none>                               2024-05-23T11:53:45Z   [map[containerID:containerd://e1f78aec0a1b9b1735f77192f2878afec72aecabb8afc7a40e065f425380d800 image:docker.io/library/python:latest imageID:docker.io/library/python@sha256:3966b81808d864099f802080d897cef36c01550472ab3955fdd716d1c665acd6 lastState:map[] name:worker ready:false restartCount:0 started:false state:map[terminated:map[containerID:containerd://e1f78aec0a1b9b1735f77192f2878afec72aecabb8afc7a40e065f425380d800 exitCode:137 finishedAt:2024-05-23T11:53:45Z reason:Error startedAt:2024-05-23T11:52:13Z]]]]
2024-05-23 13:53:46.205463 MODIFIED   indexed-job-1-jhl9t   Failed    <none>                               2024-05-23T11:53:15Z   [map[containerID:containerd://e1f78aec0a1b9b1735f77192f2878afec72aecabb8afc7a40e065f425380d800 image:docker.io/library/python:latest imageID:docker.io/library/python@sha256:3966b81808d864099f802080d897cef36c01550472ab3955fdd716d1c665acd6 lastState:map[] name:worker ready:false restartCount:0 started:false state:map[terminated:map[containerID:containerd://e1f78aec0a1b9b1735f77192f2878afec72aecabb8afc7a40e065f425380d800 exitCode:137 finishedAt:2024-05-23T11:53:45Z reason:Error startedAt:2024-05-23T11:52:13Z]]]]
2024-05-23 13:53:46.207674 DELETED    indexed-job-1-jhl9t   Failed    <none>                               2024-05-23T11:53:15Z   [map[containerID:containerd://e1f78aec0a1b9b1735f77192f2878afec72aecabb8afc7a40e065f425380d800 image:docker.io/library/python:latest imageID:docker.io/library/python@sha256:3966b81808d864099f802080d897cef36c01550472ab3955fdd716d1c665acd6 lastState:map[] name:worker ready:false restartCount:0 started:false state:map[terminated:map[containerID:containerd://e1f78aec0a1b9b1735f77192f2878afec72aecabb8afc7a40e065f425380d800 exitCode:137 finishedAt:2024-05-23T11:53:45Z reason:Error startedAt:2024-05-23T11:52:13Z]]]]
```
Note that the second pod gets the deletionTimestamp at `2024-05-23 13:53:15.324497` which is prior to the Job controller setting the `Failed` condition.

I think when fix we should have the `terminating>0` for the last status updates for the Job controller above.

#### Anything else we need to know?

I think the proper solution is to increment the counter of `terminating` pods [here](https://github.com/kubernetes/kubernetes/blob/c9a1a0a3b8acb70cba30d6f4ea59505b4564b4d9/pkg/controller/job/job_controller.go#L895-L902), where we decrement the `active` pods.

Also, this fix is related to https://github.com/kubernetes/kubernetes/issues/123775. If we have the fix we could delay setting the `Failed` condition until `terminating=0`. Otherwise we may observe `terminating=0` because the update of the counter is delayed.

#### Kubernetes version

<details>
Server Version: v1.30.0
</details>


#### Cloud provider

<details>
Kind
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125079 The old pod log file is not deleted from the /var/log/pods/ directory

- Issue é“¾æ¥ï¼š[#125079](https://github.com/kubernetes/kubernetes/issues/125079)

### Issue å†…å®¹

#### What happened?

In the /var/log/pods/ directory, many old log files are not deleted in a timely manner. As a result, the disk space is used up.
![1709001460262](https://github.com/kubernetes/kubernetes/assets/54977497/e541ec52-d6c4-4b7a-bcb6-f7bc88969d24)


#### What did you expect to happen?

I think there should be only 1 old log file in this path.

#### How can we reproduce it (as minimally and precisely as possible)?

I don't know how this happened.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# 1.28
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125069 Bug: securityContext appArmorProfile unconfined not working with containerd

- Issue é“¾æ¥ï¼š[#125069](https://github.com/kubernetes/kubernetes/issues/125069)

### Issue å†…å®¹

#### What happened?

If I define the podSecurityContext with appArmorProfile unconfined, containerd does not take it into account and use the default cri-containerd.apparmor.d profile.
I don't have the problem if I use the deprecated annotations.

#### What did you expect to happen?

The securityContext should give the same result as the annotations. According to #123811 containerd uses both methods.

#### How can we reproduce it (as minimally and precisely as possible)?

This deployment works with annotations:
```
apiVersion: apps/v1                                                                                                                                          
kind: Deployment                                                                                                                                             
metadata:                                                                                                                                                    
  name: test                                                                                                                                                 
  namespace: test                                                                                                                                            
spec:                                                                                                                                                        
  selector:                                                                                                                                                  
    matchLabels:                                                                                                                                             
      mytest: test
  template:
    metadata:
      labels:
        mytest: test
      annotations:
        container.apparmor.security.beta.kubernetes.io/mount-cgroup: unconfined
    spec:
      initContainers:
      - name: mount-cgroup
        image: quay.io/cilium/cilium:v1.15.4
        env:
        - name: CGROUP_ROOT
          value: /run/cilium/cgroupv2
        - name: BIN_PATH
          value: /opt/cni/bin
        securityContext:
          appArmorProfile:
            type: Unconfined
          capabilities:
            add:
            - SYS_ADMIN
            - SYS_CHROOT
            - SYS_PTRACE
            drop:
            - ALL
          seLinuxOptions:
            level: s0
            type: spc_t
        command:
        - sh
        - -ec
        - ls -la /hostproc/1/ns/cgroup
        volumeMounts:
        - name: hostproc
          mountPath: /hostproc
      containers:
      - name: busybox
        image: busybox
        command:
        - sleep
        - "300"
        securityContext:
          appArmorProfile:
            type: Unconfined
      volumes:
      - name: hostproc
        hostPath:
          path: /proc
          type: Directory
```
But this one doesn't.
```
apiVersion: apps/v1                                                                                                                                          
kind: Deployment                                                                                                                                             
metadata:                                                                                                                                                    
  name: test                                                                                                                                                 
  namespace: test                                                                                                                                            
spec:                                                                                                                                                        
  selector:                                                                                                                                                  
    matchLabels:                                                                                                                                             
      mytest: test
  template:
    metadata:
      labels:
        mytest: test
    spec:
      initContainers:
      - name: mount-cgroup
        image: quay.io/cilium/cilium:v1.15.4
        env:
        - name: CGROUP_ROOT
          value: /run/cilium/cgroupv2
        - name: BIN_PATH
          value: /opt/cni/bin
        securityContext:
          appArmorProfile:
            type: Unconfined
          capabilities:
            add:
            - SYS_ADMIN
            - SYS_CHROOT
            - SYS_PTRACE
            drop:
            - ALL
          seLinuxOptions:
            level: s0
            type: spc_t
        command:
        - sh
        - -ec
        - ls -la /hostproc/1/ns/cgroup
        volumeMounts:
        - name: hostproc
          mountPath: /hostproc
      containers:
      - name: busybox
        image: busybox
        command:
        - sleep
        - "300"
        securityContext:
          appArmorProfile:
            type: Unconfined
      volumes:
      - name: hostproc
        hostPath:
          path: /proc
          type: Directory
```
dmesg on the node where the container is launched gives:
`type=1400 audit(1716404572.354:231): apparmor="DENIED" operation="ptrace" profile="cri-containerd.apparmor.d" pid=1852196 comm="ls" requested_mask="read" denied_mask="read" peer="unconfined"`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.1
```

</details>


#### Cloud provider

<details>
Bare Metal K8s on Ubuntu 22.04 servers
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
# Linux k8s-master-01 5.15.0-88-generic #98-Ubuntu SMP Mon Oct 2 15:18:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Version:  0.1.0
RuntimeName:  containerd
RuntimeVersion:  v1.7.17
RuntimeApiVersion:  v1
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
cilium-cli: v0.16.7 compiled with go1.22.2 on linux/amd64
cilium image (default): v1.15.4
cilium image (stable): v1.15.5
cilium image (running): 1.15.4
</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125011 Failed to create the reconcile looper: failed to list all OverLappingIPs: client rate limiter Wait returned an error: context deadline exceeded

- Issue é“¾æ¥ï¼š[#125011](https://github.com/kubernetes/kubernetes/issues/125011)

### Issue å†…å®¹

#### What happened?

While we tried to scale in/out pods, reconciler failure was reported . Reconciler job scheduled for every 5 minutes but failed to execute with the given error: 
`[error] failed to list all OverLappingIPs: client rate limiter Wait returned an error: context deadline exceeded.`
 `[error] failed to create the reconcile looper: failed to list all OverLappingIPs: client rate limiter Wait returned an error: context deadline exceeded`
`[verbose] reconciler failure: failed to list all OverLappingIPs: client rate limiter Wait returned an error: context deadline exceeded.`

The deployment of pod replicas scaled out to 500, the same for ipam podreferences. But when we scale in replicas to 1, pods are scaled in successfully multiple times but more that 100 podreferences are left. Sometimes pods are staying in terminating state. We did multiple series of scale in/out and then uninstall and redeployment - the same issue faced every time: more than 100 pod references left undeleted after scale in. The number of pod references increase as we do multiple times scale in.

Pod reference left undeleled :
```
kubectl get ippools.whereabouts.cni.cncf.io -n kube-system  197.0.0.0-8 -o yaml | grep -c podref
143
```
Error when pod is in terminating state: 
`Warning  FailedKillPod   46s    kubelet            error killing pod: failed to "KillPodSandbox" for "f48e47e5-a4c7-44df-ad4a-b420844dcacc" with KillPodSandboxError: "rpc error: code = DeadlineExceeded desc = context deadline exceeded"`

Issue reported also in [whereabouts](https://github.com/k8snetworkplumbingwg/whereabouts/issues/389)

#### What did you expect to happen?

We expect all podreferences should be deleted during scale in and it shouldn't generate any error in terminating state or podreference issue.



#### How can we reproduce it (as minimally and precisely as possible)?

Reproduction can be done using the [test](https://github.com/k8snetworkplumbingwg/whereabouts?tab=readme-ov-file#running-whereabouts-cni-in-a-local-kind-cluster) where we have used 8 NetworkAttachmentDefinition and do the following steps:

1. The deployment of pod replicas to 500, the same for ipam pod references.
2. Scale in replicas to 1, pods are scaled successfully but more than 100 pod references are left undeleted.
3. Do these 2 series of scale in/out and then release uninstall and redeployment - the same issue every time: more than 100 pod references left undeleted after a scale in.

#### Anything else we need to know?

Would like to explain the issue here in steps for better view:

1. IPs are stored as the ippools CRD of whereabouts, There is an overlapping IP ranges CRD used to store all IPs.[Which creates object for each IP] 
4. The issue is when reconciler code tries to fetch all the objects in under the overlappingipranges. When the number of IPs grow, the number of overlappingipranges objects grow(one for each IP)
5. Reconciler on the initialization of the the first executor objects tries to list all objects under the overlappingipranges CRD, and hits a context-deadline.

#### Kubernetes version

<details>

```console
Issue started with 1.26.1 version and also reproduced in later versions.
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
Whereabouts version : 0.6.2 and also with upgraded version
</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #125010 `GOTOOLCHAIN=go1.22.1 make verify WHAT=codegen` fails with error deleting tempdir

- Issue é“¾æ¥ï¼š[#125010](https://github.com/kubernetes/kubernetes/issues/125010)

### Issue å†…å®¹

#### What happened?

While debugging https://github.com/kubernetes/kubernetes/issues/124932 I discovered that setting `GOTOOLCHAIN` causes `make verify WHAT=codegen` @ 765e7ef0d21 (recent master branch commit) to fail on cleanup at the end.

This doesn't happen if you don't set `GOTOOLCHAIN`. It's not immediately apparent why those are related.

#### What did you expect to happen?

This make target should not fail on a clean branch with no diff.

#### How can we reproduce it (as minimally and precisely as possible)?

set GOTOOLCHAIN to some valid go version and then run `make verify WHAT=codegen`

#### Anything else we need to know?

/sig testing release
/triage accepted

#### Kubernetes version

765e7ef0d21

#### Cloud provider

n/a

#### OS version

Linux. ~Debian. But this is happening inside our build container.

#### Install tools

n/a

#### Container runtime (CRI) and version (if applicable)

docker 20.10.21

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

n/a

### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124962 DRA: no handler registered for plugin type: DRAPlugin at socket /var/lib/kubelet/plugins_registry/

- Issue é“¾æ¥ï¼š[#124962](https://github.com/kubernetes/kubernetes/issues/124962)

### Issue å†…å®¹

#### What happened?

```
E0520 07:49:57.337809       1 nonblockinggrpcserver.go:125] "handling request failed" err="failed registration process: RegisterPlugin error -- no handler registered for plugin type: DRAPlugin at socket /var/lib/kubelet/plugins_registry/netresources.spidernet.io.sock" logger="registrar" requestID=8 request="&RegistrationStatus{PluginRegistered:false,Error:RegisterPlugin error -- no handler registered for plugin type: DRAPlugin at socket /var/lib/kubelet/plugins_registry/netresources.spidernet.io.sock,}"
```

#### What did you expect to happen?

handling request well

#### How can we reproduce it (as minimally and precisely as possible)?

write a dra kubelet-plugin with `kubeletplugin.Start()`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
root@controller-node-1:~# kubectl version
Client Version: v1.29.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
Linux controller-node-1 5.15.0-94-generic #104-Ubuntu SMP Tue Jan 9 15:25:40 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124940 last applied annotations are not getting updated

- Issue é“¾æ¥ï¼š[#124940](https://github.com/kubernetes/kubernetes/issues/124940)

### Issue å†…å®¹

#### What happened?

i was trying to update configmap and after editing successfully configmap annotation didn't update with the last values. still, it holds 1st-time data only.
basically, I am trying to write one code to capture if someone changes anything in CM and we can identify it.

and also in some configmap I am not able to find last-applied annotation.

#### What did you expect to happen?

annotation should update

#### How can we reproduce it (as minimally and precisely as possible)?

create cm with below 
`apiVersion: v1
kind: ConfigMap
metadata:
  name: game-config-env-file
data:
  allowed: '"true"'
  enemies: aliens
  lives: "31"`

and now edit this cm game-config-env-file and let say change any values last-applied annotation is not getting chnaged

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# Client Version: v1.28.8-eks-ae9a62a
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.8-eks-adc7111
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124938 [Sidecar Containers] Eviction message should account for the sidecar containers

- Issue é“¾æ¥ï¼š[#124938](https://github.com/kubernetes/kubernetes/issues/124938)

### Issue å†…å®¹

#### What happened?

The `evictionMessage` is not accounting for the restartable init containers (sidecars). 

/kind bug

#### What did you expect to happen?

When pod is evicted, requests of the sidecar containers must be added to the annotations.

#### How can we reproduce it (as minimally and precisely as possible)?

Evict pod with the sidecars and check out annotation.

#### Anything else we need to know?

/sig node
/priority backlog
KEP: https://github.com/kubernetes/enhancements/issues/753

### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124937 [Sidecar Containers] Sidecar containers finish time needs to be accounted for in job controller

- Issue é“¾æ¥ï¼š[#124937](https://github.com/kubernetes/kubernetes/issues/124937)

### Issue å†…å®¹

#### What happened?

Today the function `getFinishTimeFromContainers` (pkg/controller/job/backoff_utils.go) only account for the regular containers finish time,
presumably assuming that init containers have completed before before.
However, with the sidecar (restartable init) containers, sidecar containers will always finish later than
regular containers. And those needs to be accounted for the calculation.

/kind bug

#### What did you expect to happen?

The sidecar finish time will be accounted when calculating the job's finish time.

#### How can we reproduce it (as minimally and precisely as possible)?

Sidecar container with prolonged termination logic on a job will result in incorrect finish time reporting.

#### Anything else we need to know?

/sig apps
/priority backlog
KEP: https://github.com/kubernetes/enhancements/issues/753

### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124936 [Sidecar Containers] Pods comparison by maxContainerRestarts should account for sidecar containers

- Issue é“¾æ¥ï¼š[#124936](https://github.com/kubernetes/kubernetes/issues/124936)

### Issue å†…å®¹

#### What happened?

Today, there are a few uses of the function `maxContainerRestarts` - mostly to compare pods to decide which one is
better to delete or which logs to get. This is not a huge issue, mostly a quality of life improvement.

The code only look at Container Statuses, but likely need to look at init container statuses as well.
Especially in case of sidecar containers that may behave exactly as regular containers.

There are 2 implementations and 5 comparison interfaces.

Implementatons in:

- pkg/controller/controller_utils.go
- staging/src/k8s.io/kubectl/pkg/util/podutils/podutils.go

```
func maxContainerRestarts(pod *v1.Pod) int {
	maxRestarts := 0
	for _, c := range pod.Status.ContainerStatuses {
		maxRestarts = max(maxRestarts, int(c.RestartCount))
	}
	return maxRestarts
}
```

We may need to be careful including all init container statuses. If a Pod was failing to start for a while
because of Init container failures and now it is running OK, it is likely not important. However, including
the restartable containers (sidecars) restart count is important.

I think the desireable behavior will be to check regular containers max restart count first. And compare this.
Then compare max restart count for restarteable init containers.

/kind bug

#### What did you expect to happen?

Comparison of max container restarts account for init containers as well as regular containers.

#### How can we reproduce it (as minimally and precisely as possible)?

Two pods - one with the sidecar container in constant restart loop and one is running sucessfully. Random one will be picked to get logs from.

#### Anything else we need to know?

/sig apps
/priority backlog

KEP: https://github.com/kubernetes/enhancements/issues/753

### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124932 containerized protobuf codegen does not handle .go-version / GOTOOLCHAIN properly

- Issue é“¾æ¥ï¼š[#124932](https://github.com/kubernetes/kubernetes/issues/124932)

### Issue å†…å®¹

Seen in https://github.com/kubernetes/kubernetes/pull/124922

```
+++ command: bash "hack/make-rules/../../hack/verify-codegen.sh"
go version go1.22.3 linux/amd64
+++ [0517 06:22:42] Generating protobufs for 67 targets
+++ [0517 06:22:42] protoc 23.4 not found (can install with hack/install-protoc.sh); generating containerized...
+++ [0517 06:22:42] Verifying Prerequisites....
+++ [0517 06:22:43] Building Docker image kube-build:build-71fc6bd7d6-5-v1.28.0-go1.21.10-bullseye.0
+++ [0517 06:24:57] Creating data container kube-build-data-71fc6bd7d6-5-v1.28.0-go1.21.10-bullseye.0
+++ [0517 06:24:58] Syncing sources to container
+++ [0517 06:25:03] Output from this container will be rsynced out upon completion. Set KUBE_RUN_COPY_OUTPUT=n to disable.
+++ [0517 06:25:03] Running build command...
go: downloading go1.22.3 (linux/amd64)
go: download go1.22.3 for linux/amd64: toolchain not available
!!! [0517 06:25:04] Call tree:
!!! [0517 06:25:04]  1: build/../build/common.sh:489 kube::build::run_build_command_ex(...)
!!! [0517 06:25:04]  2: build/run.sh:39 kube::build::run_build_command(...)
!!! [0517 06:25:04] Call tree:
!!! [0517 06:25:04]  1: hack/update-codegen.sh:919 codegen::protobuf(...)
+++ exit code: 1
+++ error: 1
[0;31mFAILED[0m   verify-codegen.sh	148s
```

Something is not working properly between .go-version â†’ GOTOOLCHAIN when the go version inside this specific container doesn't match

/assign 
/cc @BenTheElder @MadhavJivrajani

### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124930 v1.30: kube-scheduler crashes with: Observed a panic: "integer divide by zero"

- Issue é“¾æ¥ï¼š[#124930](https://github.com/kubernetes/kubernetes/issues/124930)

### Issue å†…å®¹

#### What happened?

On Kubernetes v1.30.0 (and v1.30.1), `kube-scheduler` can crash with the following panic if a pod is defined in a certain way:

```
W0514 09:09:41.391780       1 feature_gate.go:246] Setting GA feature gate MinDomainsInPodTopologySpread=true. It will be removed in a future release.
I0514 09:09:43.191448       1 serving.go:380] Generated self-signed cert in-memory
W0514 09:09:43.574824       1 authentication.go:446] failed to read in-cluster kubeconfig for delegated authentication: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory
W0514 09:09:43.574862       1 authentication.go:339] No authentication-kubeconfig provided in order to lookup client-ca-file in configmap/extension-apiserver-authentication in kube-system, so client certificate authentication won't work.
W0514 09:09:43.574871       1 authentication.go:363] No authentication-kubeconfig provided in order to lookup requestheader-client-ca-file in configmap/extension-apiserver-authentication in kube-system, so request-header client certificate authentication won't work.
W0514 09:09:43.574885       1 authorization.go:225] failed to read in-cluster kubeconfig for delegated authorization: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory
W0514 09:09:43.574992       1 authorization.go:193] No authorization-kubeconfig provided, so SubjectAccessReview of authorization tokens won't work.
I0514 09:09:43.586307       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0-zalando-master-117-dirty"
I0514 09:09:43.586327       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0514 09:09:43.587758       1 secure_serving.go:213] Serving securely on [::]:10259
I0514 09:09:43.587944       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0514 09:09:43.688733       1 leaderelection.go:250] attempting to acquire leader lease kube-system/kube-scheduler...
I0514 09:09:59.030033       1 leaderelection.go:260] successfully acquired lease kube-system/kube-scheduler
E0514 09:09:59.122043       1 runtime.go:79] Observed a panic: "integer divide by zero" (runtime error: integer divide by zero)
goroutine 330 [running]:
k8s.io/apimachinery/pkg/util/runtime.logPanic({0x1b68bc0, 0x35e1c40})
	k8s.io/apimachinery/pkg/util/runtime/runtime.go:75 +0x7c
k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0x4000be8e00?})
	k8s.io/apimachinery/pkg/util/runtime/runtime.go:49 +0x78
panic({0x1b68bc0?, 0x35e1c40?})
	runtime/panic.go:770 +0x124
k8s.io/kubernetes/pkg/scheduler.(*Scheduler).findNodesThatFitPod(0x40000d6900, {0x22aae08, 0x40012b9630}, {0x22d2bb8, 0x4000320fc8}, 0x4000347dc0, 0x4000cb4488)
	k8s.io/kubernetes/pkg/scheduler/schedule_one.go:502 +0x88c
k8s.io/kubernetes/pkg/scheduler.(*Scheduler).schedulePod(0x40000d6900, {0x22aae08, 0x40012b9630}, {0x22d2bb8, 0x4000320fc8}, 0x4000347dc0, 0x4000cb4488)
	k8s.io/kubernetes/pkg/scheduler/schedule_one.go:402 +0x25c
k8s.io/kubernetes/pkg/scheduler.(*Scheduler).schedulingCycle(0x40000d6900, {0x22aae08, 0x40012b9630}, 0x4000347dc0, {0x22d2bb8, 0x4000320fc8}, 0x4000c92e10, {0x2?, 0x2?, 0x36352c0?}, ...)
	k8s.io/kubernetes/pkg/scheduler/schedule_one.go:149 +0xb8
k8s.io/kubernetes/pkg/scheduler.(*Scheduler).ScheduleOne(0x40000d6900, {0x22aae08, 0x40012b83c0})
	k8s.io/kubernetes/pkg/scheduler/schedule_one.go:111 +0x4c0
k8s.io/apimachinery/pkg/util/wait.JitterUntilWithContext.func1()
	k8s.io/apimachinery/pkg/util/wait/backoff.go:259 +0x2c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x400161dec8?)
	k8s.io/apimachinery/pkg/util/wait/backoff.go:226 +0x40
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x400161df68, {0x2286fe0, 0x4001398b70}, 0x1, 0x40012bab40)
	k8s.io/apimachinery/pkg/util/wait/backoff.go:227 +0x90
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x4000669f68, 0x0, 0x0, 0x1, 0x40012bab40)
	k8s.io/apimachinery/pkg/util/wait/backoff.go:204 +0x80
k8s.io/apimachinery/pkg/util/wait.JitterUntilWithContext({0x22aae08, 0x40012b83c0}, 0x4001385720, 0x0, 0x0, 0x1)
	k8s.io/apimachinery/pkg/util/wait/backoff.go:259 +0x80
k8s.io/apimachinery/pkg/util/wait.UntilWithContext(...)
	k8s.io/apimachinery/pkg/util/wait/backoff.go:170
created by k8s.io/kubernetes/pkg/scheduler.(*Scheduler).Run in goroutine 352
	k8s.io/kubernetes/pkg/scheduler/scheduler.go:445 +0x104
panic: runtime error: integer divide by zero [recovered]
	panic: runtime error: integer divide by zero

goroutine 330 [running]:
k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0x4000be8e00?})
	k8s.io/apimachinery/pkg/util/runtime/runtime.go:56 +0xe0
panic({0x1b68bc0?, 0x35e1c40?})
	runtime/panic.go:770 +0x124
k8s.io/kubernetes/pkg/scheduler.(*Scheduler).findNodesThatFitPod(0x40000d6900, {0x22aae08, 0x40012b9630}, {0x22d2bb8, 0x4000320fc8}, 0x4000347dc0, 0x4000cb4488)
	k8s.io/kubernetes/pkg/scheduler/schedule_one.go:502 +0x88c
k8s.io/kubernetes/pkg/scheduler.(*Scheduler).schedulePod(0x40000d6900, {0x22aae08, 0x40012b9630}, {0x22d2bb8, 0x4000320fc8}, 0x4000347dc0, 0x4000cb4488)
	k8s.io/kubernetes/pkg/scheduler/schedule_one.go:402 +0x25c
k8s.io/kubernetes/pkg/scheduler.(*Scheduler).schedulingCycle(0x40000d6900, {0x22aae08, 0x40012b9630}, 0x4000347dc0, {0x22d2bb8, 0x4000320fc8}, 0x4000c92e10, {0x2?, 0x2?, 0x36352c0?}, ...)
	k8s.io/kubernetes/pkg/scheduler/schedule_one.go:149 +0xb8
k8s.io/kubernetes/pkg/scheduler.(*Scheduler).ScheduleOne(0x40000d6900, {0x22aae08, 0x40012b83c0})
	k8s.io/kubernetes/pkg/scheduler/schedule_one.go:111 +0x4c0
k8s.io/apimachinery/pkg/util/wait.JitterUntilWithContext.func1()
	k8s.io/apimachinery/pkg/util/wait/backoff.go:259 +0x2c
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0x400161dec8?)
	k8s.io/apimachinery/pkg/util/wait/backoff.go:226 +0x40
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x400161df68, {0x2286fe0, 0x4001398b70}, 0x1, 0x40012bab40)
	k8s.io/apimachinery/pkg/util/wait/backoff.go:227 +0x90
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x4000669f68, 0x0, 0x0, 0x1, 0x40012bab40)
	k8s.io/apimachinery/pkg/util/wait/backoff.go:204 +0x80
k8s.io/apimachinery/pkg/util/wait.JitterUntilWithContext({0x22aae08, 0x40012b83c0}, 0x4001385720, 0x0, 0x0, 0x1)
	k8s.io/apimachinery/pkg/util/wait/backoff.go:259 +0x80
k8s.io/apimachinery/pkg/util/wait.UntilWithContext(...)
	k8s.io/apimachinery/pkg/util/wait/backoff.go:170
created by k8s.io/kubernetes/pkg/scheduler.(*Scheduler).Run in goroutine 352
	k8s.io/kubernetes/pkg/scheduler/scheduler.go:445 +0x104
```

The crash happens [here](https://github.com/kubernetes/kubernetes/blob/7c48c2bd72b9bf5c44d21d7338cc7bea77d0ad2a/pkg/scheduler/schedule_one.go#L502) because the `len(nodes)` is `0` in certain cases. 

#### What did you expect to happen?

`kube-scheduler` should not crash, on v1.29.4 it doesn't happen.

On v1.29.4 the `kube-scheduler` is just printing these error logs, but doesn't crash:

```
E0517 13:19:26.611504       1 schedule_one.go:1003] "Error scheduling pod; retrying" err="nodeinfo not found for node name \"invalid-node\"" pod="default/break-kube-scheduler"
```

#### How can we reproduce it (as minimally and precisely as possible)?

Create a pod like this:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: break-kube-scheduler
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchFields:
          - key: metadata.name
            operator: In
            values:
            - invalid-node # a node that doesn't exist
  containers:
  - name: main
    image: alpine
    command: ["cat"]
    stdin: true
```
The important part is that the affinity doesn't match a real/valid node.

Once this pod is being processed by `kube-scheduler` it will crash and continue to do so until the pod is deleted.

#### Anything else we need to know?

This issue was triggered during the rotation of control-plane nodes. In our setup we update the control plane by scaling from 1 to 2 instances. Once both a ready, the old is being terminated via EC2 API. At this stage the kube-controller-manager _sometimes_ manages to create a replacement daemonset pod once the old node is being deleted. This results in a pod targeting a no longer existing node via `affinity` as illustrated in the example above. For some reason, when the `kube-scheduler` is crashing the `kube-controller-manager` doesn't delete the extra/invalid daemonset pod. Not sure if this is another issue or it has always happened in our setup, but only v1.30 makes `kube-scheduler` crash which causes an actual issue.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.1
```

</details>


#### Cloud provider

<details>
We run a custom Kubernetes setup on AWS EC2 (not EKS).
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux ip-10-149-91-92 6.5.0-1020-aws #20~22.04.1-Ubuntu SMP Wed May  1 16:38:06 UTC 2024 aarch64 aarch64 aarch64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
Custom tooling: https://github.com/zalando-incubator/kubernetes-on-aws
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

è¯¥ Issue å­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

**åŸå› ï¼š**

æ”»å‡»è€…å¦‚æœå…·å¤‡åˆ›å»º Pod çš„æƒé™ï¼Œå¯ä»¥é€šè¿‡æ„é€ ç‰¹æ®Šçš„ Pod å®šä¹‰ï¼ˆåœ¨ `nodeAffinity` ä¸­æŒ‡å®šä¸å­˜åœ¨çš„èŠ‚ç‚¹ï¼‰ï¼Œè§¦å‘ kube-scheduler ä¸­çš„æ•´æ•°é™¤é›¶é”™è¯¯ï¼Œå¯¼è‡´ kube-scheduler å´©æºƒã€‚è¿™ä¼šå¯¼è‡´æ•´ä¸ª Kubernetes é›†ç¾¤çš„æ–° Pod æ— æ³•è¢«è°ƒåº¦ï¼Œé€ æˆæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚

**æ ¹æ® CVSS 3.1 è¯„åˆ†æ ‡å‡†è¿›è¡Œè¯„ä¼°ï¼š**

- **æ”»å‡»å‘é‡ï¼ˆAVï¼‰**ï¼šç½‘ç»œï¼ˆNï¼‰ï¼Œå€¼ä¸º 0.85ï¼ˆæ”»å‡»è€…å¯ä»¥é€šè¿‡ç½‘ç»œæ¥å£è¿œç¨‹å‘èµ·æ”»å‡»ï¼‰
- **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰**ï¼šä½ï¼ˆLï¼‰ï¼Œå€¼ä¸º 0.77ï¼ˆæ”»å‡»ä¸éœ€è¦ç‰¹æ®Šæ¡ä»¶ï¼Œæ˜“äºå®æ–½ï¼‰
- **æƒé™è¦æ±‚ï¼ˆPRï¼‰**ï¼šä½ï¼ˆLï¼‰ï¼Œå€¼ä¸º 0.68ï¼ˆéœ€è¦å…·å¤‡åœ¨é›†ç¾¤ä¸­åˆ›å»º Pod çš„æƒé™ï¼‰
- **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰**ï¼šæ— ï¼ˆNï¼‰ï¼Œå€¼ä¸º 0.85ï¼ˆä¸éœ€è¦å…¶ä»–ç”¨æˆ·çš„äº¤äº’ï¼‰
- **ä½œç”¨èŒƒå›´ï¼ˆSï¼‰**ï¼šå·²æ”¹å˜ï¼ˆCï¼‰ï¼ˆæ”»å‡»è€…çš„æ“ä½œå½±å“åˆ°äº†è¶…è¿‡å…¶æƒé™èŒƒå›´çš„ç»„ä»¶ï¼‰
- **æœºå¯†æ€§å½±å“ï¼ˆCï¼‰**ï¼šæ— ï¼ˆNï¼‰ï¼Œå€¼ä¸º 0.00ï¼ˆä¸æ¶‰åŠæœºå¯†æ€§ä¿¡æ¯æ³„éœ²ï¼‰
- **å®Œæ•´æ€§å½±å“ï¼ˆIï¼‰**ï¼šæ— ï¼ˆNï¼‰ï¼Œå€¼ä¸º 0.00ï¼ˆä¸æ¶‰åŠæ•°æ®ç¯¡æ”¹ï¼‰
- **å¯ç”¨æ€§å½±å“ï¼ˆAï¼‰**ï¼šé«˜ï¼ˆHï¼‰ï¼Œå€¼ä¸º 0.56ï¼ˆå¯¼è‡´ kube-scheduler å´©æºƒï¼Œå½±å“é›†ç¾¤å¯ç”¨æ€§ï¼‰

**CVSS è®¡ç®—ç»“æœï¼š**

- **åŸºæœ¬è¯„åˆ† (Base Score)**ï¼š7.1ï¼ˆé«˜ï¼‰
- **å‘é‡å­—ç¬¦ä¸²**ï¼šCVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:C/C:N/I:N/A:H

**å¯èƒ½çš„å½±å“ï¼š**

- **æ‹’ç»æœåŠ¡æ”»å‡»**ï¼šæ”»å‡»è€…å¯ä»¥å¯¼è‡´ kube-scheduler æŒç»­å´©æºƒï¼Œé˜»æ­¢æ–° Pod çš„è°ƒåº¦ï¼Œå½±å“é›†ç¾¤çš„æ­£å¸¸è¿è¡Œã€‚
- **å½±å“èŒƒå›´å¹¿æ³›**ï¼šç”±äº kube-scheduler æ˜¯é›†ç¾¤å…³é”®ç»„ä»¶ï¼Œå…¶å´©æºƒä¼šå½±å“æ•´ä¸ªé›†ç¾¤ï¼Œè€Œéå•ä¸€å‘½åç©ºé—´æˆ–èŠ‚ç‚¹ã€‚

**Proof of Conceptï¼š**

æ”»å‡»è€…å¯ä»¥åˆ›å»ºä»¥ä¸‹æ¶æ„ Pod æ¥è§¦å‘æ¼æ´ï¼š

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: malicious-pod
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchFields:
              - key: metadata.name
                operator: In
                values:
                  - non-existent-node # æŒ‡å®šä¸å­˜åœ¨çš„èŠ‚ç‚¹
  containers:
    - name: exploit
      image: alpine
      command: ["sleep", "3600"]
```

**è¯´æ˜ï¼š**

- `nodeAffinity` ä¸­å¼•ç”¨äº†ä¸€ä¸ªä¸å­˜åœ¨çš„èŠ‚ç‚¹ `non-existent-node`ã€‚
- å½“ kube-scheduler å¤„ç†è¯¥ Pod æ—¶ï¼Œç”±äºæœªæ‰¾åˆ°åŒ¹é…çš„èŠ‚ç‚¹åˆ—è¡¨ï¼Œå¯¼è‡´ `len(nodes)` ä¸º 0ã€‚
- åœ¨åç»­è®¡ç®—ä¸­å‘ç”Ÿé™¤ä»¥ `len(nodes)` çš„æ“ä½œï¼Œè§¦å‘æ•´æ•°é™¤é›¶é”™è¯¯ï¼Œå¯¼è‡´ kube-scheduler å´©æºƒã€‚

**ç»“è®ºï¼š**

è¯¥é—®é¢˜å…è®¸å…·æœ‰åˆ›å»º Pod æƒé™çš„æ”»å‡»è€…é€šè¿‡å‘é€ç‰¹åˆ¶è¯·æ±‚ï¼Œå¯¼è‡´ kube-scheduler å´©æºƒï¼Œå½±å“æ•´ä¸ªé›†ç¾¤çš„å¯ç”¨æ€§ï¼Œå±äºé«˜å±å®‰å…¨æ¼æ´ï¼Œåº”åŠæ—¶ä¿®å¤å’Œå¤„ç†ã€‚

---

## Issue #125012 kubelet on Windows fails if a pod has SecurityContext with RunAsUser.

- Issue é“¾æ¥ï¼š[#125012](https://github.com/kubernetes/kubernetes/issues/125012)

### Issue å†…å®¹

#### What happened?

Install a cluster with v1.29.4 with linux node as master and worker node as windows
kubeadm upgrade apply v1.30.0 fails with

`[ERROR CreateJob]: Job \"upgrade-health-check-4rxpv\" in the namespace \"kube-system\" did not complete in 15s: no condition of type Complete [preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors`

Describe Pod events:

`mountvolume.setup failed for volume "kube-api-access-rncjh" : chown c:\var\lib\kubelet\pods\c5a3db55-4ee9-4b99-8fd6-17da5799f34a\volumes\kubernetes.io~projected\kube-api-access-rncjh\..2024_05_08_20_28_34.3908400707\token: not supported by windows`

It seems that previously the pod was Pending as well, but this was ignored, because the jobs was successfully deleted in defer and return value was overridden with nil.

```
defer func() {
    lastError = deleteHealthCheckJob(client, ns, jobName)
}()
```

https://github.com/kubernetes/kubernetes/blob/v1.29.1/cmd/kubeadm/app/phases/upgrade/health.go#L151



#### What did you expect to happen?

Upgrade health check job expected to be scheduled only on Linux nodes using node selectors, rather than on Windows nodes.

#### How can we reproduce it (as minimally and precisely as possible)?

See **What happened?**

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

kubeadm version (use kubeadm version): v1.30.0

Environment:

Kubernetes version (use kubectl version): v1.30.0
Cloud provider or hardware configuration:
OS (e.g. from /etc/os-release): Ubuntu 22.04.2 LTS, Windows Server 2022 Datacenter
Kernel (e.g. uname -a): 5.19.0-1025-aws
Container networking plugin (CNI): calico

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124915 Ignore and potentially prevent reporting container status for not-existing containers

- Issue é“¾æ¥ï¼š[#124915](https://github.com/kubernetes/kubernetes/issues/124915)

### Issue å†…å®¹

#### What happened?

Some third party controllers may report the Container Status for containers that are not defined in a pod spec. This may lead to inconsistencies in codebase and ideally needs to be blocked.

We see this with the https://github.com/admiraltyio/admiralty/pull/206, but there may be more examples like this since k8s never checked for consistency of statuses to specs.

#### What did you expect to happen?

As mentioned: https://github.com/kubernetes/kubernetes/pull/124906#pullrequestreview-2061200945 usages of container statuses needs to be reviewed and in most places we should start ignoring statuses for non-existing containers.

#### How can we reproduce it (as minimally and precisely as possible)?

Update the Pod Status with the container status for the container that doesn't exist:

```json
"apiVersion": "v1",
"kind": "Pod",
"metadata": {
            .....
},
"spec": {
  "containers": [],
  "initContainers": [
    {
      ....
      "name": "tester",
      ....
    },
    {
        ....
      "name": "init-proxy",
      ....
    }
  ],
  "nodeName": "foo",
},
"status": {
  "containerStatuses": [
    ....
  ],
  "initContainerStatuses": [
    {
      ....
      "imageID": "",
      "lastState": {},
      "name": "not-existing-container",
      "ready": false,
      "restartCount": 0,
      "state": {
        "waiting": {
          "reason": "PodInitializing"
        }
      }
    },
    {
        ......
      "imageID": "",
      "lastState": {},
      "name": "tester",
      "ready": false,
      "restartCount": 0,
      "state": {
        "waiting": {
          "reason": "PodInitializing"
        }
      }
    },
    {
      ....
      "imageID": "",
      "lastState": {},
      "name": "init-proxy",
      "ready": false,
      "restartCount": 0,
      "state": {
        "waiting": {
          "reason": "PodInitializing"
        }
      }
    }
  ],
  "phase": "Pending",
  "qosClass": "Burstable",
  "startTime": "2024-05-07T23:55:23Z"
}
```

#### Anything else we need to know?

/sig node

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>
N/A
</details>


#### Install tools

<details>
N/A
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Any
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
N/A
</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124903 One Node all pods got crashloopbackoff

- Issue é“¾æ¥ï¼š[#124903](https://github.com/kubernetes/kubernetes/issues/124903)

### Issue å†…å®¹

#### What happened?

Hello 
I  have installed three nodes on my k8s 
one is master node and two nodes are slave node
but one slave node pods works normally but one slave node pods got crashloopback error continously

#### What did you expect to happen?

I want to run all pods norml

#### How can we reproduce it (as minimally and precisely as possible)?

I have installed self kubernetes on my ubuntu servers. if I add new server to it, it doesn't work 

#### Anything else we need to know?

this is just the log from master node 
![image](https://github.com/kubernetes/kubernetes/assets/165806253/2812f5d5-d27c-4462-a522-104d1ab718f9)
this is kubelet log from error node 
![image](https://github.com/kubernetes/kubernetes/assets/165806253/ba1ff7ce-df5f-4219-a526-d8b3da86c717)
this is containerd log from error node
![image](https://github.com/kubernetes/kubernetes/assets/165806253/b4936d4b-e155-405b-9b7c-c009adc1b14b)



#### Kubernetes version

<details>

```console
$ kubectl version
Server Version: v1.28.9



#### Cloud provider

self hosting

#### OS version

ubuntu 22.0.4


#### Install tools

docker 
container
kubectl 
helm 


#### Container runtime (CRI) and version (if applicable)

containerd

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

flannel
![image](https://github.com/kubernetes/kubernetes/assets/165806253/db458c22-8200-4093-8a0d-f94d206d78c2)

```


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124900 1.30 tag also breaks PodIP.IP (which should be marked required)

- Issue é“¾æ¥ï¼š[#124900](https://github.com/kubernetes/kubernetes/issues/124900)

### Issue å†…å®¹

#### What happened?

We have a CRD that contains PodStatus.  After generating CRD with k8s v1.30.1, applying it failed with the message:

> spec.validation.openAPIV3Schema.properties[status].properties[podIPs].items.properties[ip].default: Required value: this property is in x-kubernetes-list-map-keys, so it must have a default or be a required property

Similar issue reported in https://github.com/kubernetes/kubernetes/issues/124540 but different struct field.

#### What did you expect to happen?

CRD is applied successfully.

#### How can we reproduce it (as minimally and precisely as possible)?

Create CRD that has PodStatus and then apply it.

#### Anything else we need to know?

This is a leftover of https://github.com/kubernetes/kubernetes/pull/124553
PodIP.IP should also be marked required because it is +listMapKey=ip in PodStatus.PodIPs.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124879  Linux 6.6 EEVDF scheduler on Kubernetes: openat2 /sys/fs/cgroup/kubepods.slice/cpu.weight: no such file or directory

- Issue é“¾æ¥ï¼š[#124879](https://github.com/kubernetes/kubernetes/issues/124879)

### Issue å†…å®¹

#### What happened?

Linux 6.6 comes with a new job scheduler called EEVDF (Earliest Eligible Virtual Deadline First), replacing the old CFS.

The kubelet/containerd failed to create cgroup

#### What did you expect to happen?

Kubelet/containerd is compatible with linux EEVDF scheduler

#### How can we reproduce it (as minimally and precisely as possible)?

Run kubernetes on Ubuntu 24.04 LTS

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version

Client Version: v1.28.3-aliyun.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.9-aliyun.1
```

</details>


#### Cloud provider

<details>
Aliyun
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124873 Status manager does not normalize ephemeral container statuses

- Issue é“¾æ¥ï¼š[#124873](https://github.com/kubernetes/kubernetes/issues/124873)

### Issue å†…å®¹

#### What happened?

`EphemeralContainerStatuses` is not normalized in 	`normalizeStatus()` in the status manager:
https://github.com/kubernetes/kubernetes/blob/cade1dddd81eba338df85de7b5d17324a87243b5/pkg/kubelet/status/status_manager.go#L1026-L1043

This does not seem to cause any user facing problem because `EphemeralContainerStatuses` is sorted before passed to the status manager and timestamps are normalized at marshaling to create a patch:
https://github.com/kubernetes/kubernetes/blob/cade1dddd81eba338df85de7b5d17324a87243b5/pkg/kubelet/kubelet_pods.go#L2336

https://github.com/kubernetes/kubernetes/blob/cade1dddd81eba338df85de7b5d17324a87243b5/pkg/kubelet/status/status_manager.go#L873

However, an unexpected behavior is caused internally because timestamps are not normalized. If the log verbosity is 3 or larger, the following messages are logged periodically in kubelet when an ephemeral container exists:

```
May 14 20:10:33 kind-control-plane kubelet[720]: I0514 20:10:33.437155 	720 status_manager.go:230] "Syncing all statuses"
May 14 20:10:33 kind-control-plane kubelet[720]: I0514 20:10:33.438512 	720 status_manager.go:984] "Pod status is inconsistent with cached status for pod, a reconciliation should be triggered" pod="default/ephemeral-demo" statusDiff=<
May 14 20:10:33 kind-control-plane kubelet[720]:     	  &v1.PodStatus{
May 14 20:10:33 kind-control-plane kubelet[720]:     	      	... // 8 identical fields
May 14 20:10:33 kind-control-plane kubelet[720]:     	      	PodIPs:            	{{IP: "10.244.0.5"}},
May 14 20:10:33 kind-control-plane kubelet[720]:     	      	StartTime:         	s"2024-05-14 20:09:21 +0000 UTC",
May 14 20:10:33 kind-control-plane kubelet[720]:     	-     	InitContainerStatuses: nil,
May 14 20:10:33 kind-control-plane kubelet[720]:     	+     	InitContainerStatuses: []v1.ContainerStatus{},
May 14 20:10:33 kind-control-plane kubelet[720]:     	      	ContainerStatuses: 	{{Name: "ephemeral-demo", State: {Running: &{StartedAt: {Time: s"2024-05-14 20:09:23 +0000 UTC"}}}, Ready: true, Image: "registry.k8s.io/pause:3.1", ...}},
May 14 20:10:33 kind-control-plane kubelet[720]:     	      	QOSClass:          	"BestEffort",
May 14 20:10:33 kind-control-plane kubelet[720]:     	      	EphemeralContainerStatuses: []v1.ContainerStatus{
May 14 20:10:33 kind-control-plane kubelet[720]:     	              	{
May 14 20:10:33 kind-control-plane kubelet[720]:     	                      	Name: "debugger-qk87g",
May 14 20:10:33 kind-control-plane kubelet[720]:     	                      	State: v1.ContainerState{
May 14 20:10:33 kind-control-plane kubelet[720]:     	                              	Waiting:	nil,
May 14 20:10:33 kind-control-plane kubelet[720]:     	-                             	Running:	&v1.ContainerStateRunning{StartedAt: v1.Time{Time: s"2024-05-14 20:09:35 +0000 UTC"}},
May 14 20:10:33 kind-control-plane kubelet[720]:     	+                             	Running:	&v1.ContainerStateRunning{StartedAt: v1.Time{Time: s"2024-05-14 20:09:35.697513091 +0000 UTC"}},
May 14 20:10:33 kind-control-plane kubelet[720]:     	                              	Terminated: nil,
May 14 20:10:33 kind-control-plane kubelet[720]:     	                      	},
May 14 20:10:33 kind-control-plane kubelet[720]:     	                      	LastTerminationState: {},
May 14 20:10:33 kind-control-plane kubelet[720]:     	                      	Ready:            	false,
May 14 20:10:33 kind-control-plane kubelet[720]:     	                      	... // 7 identical fields
May 14 20:10:33 kind-control-plane kubelet[720]:     	              	},
May 14 20:10:33 kind-control-plane kubelet[720]:     	      	},
May 14 20:10:33 kind-control-plane kubelet[720]:     	      	Resize:            	"",
May 14 20:10:33 kind-control-plane kubelet[720]:     	      	ResourceClaimStatuses: nil,
May 14 20:10:33 kind-control-plane kubelet[720]:     	  }
May 14 20:10:33 kind-control-plane kubelet[720]:  >

```

This message is emitted here:
https://github.com/kubernetes/kubernetes/blob/cade1dddd81eba338df85de7b5d17324a87243b5/pkg/kubelet/status/status_manager.go#L984-L988

After `needsReconcile()` returns `true`, `syncPod()` is called. This does not look so harmful because `unchaged` gets `true` eventually:

https://github.com/kubernetes/kubernetes/blob/cade1dddd81eba338df85de7b5d17324a87243b5/pkg/kubelet/status/status_manager.go#L873-L881

API is called once unnecessarily in `syncPod()`, though:
https://github.com/kubernetes/kubernetes/blob/cade1dddd81eba338df85de7b5d17324a87243b5/pkg/kubelet/status/status_manager.go#L843


#### What did you expect to happen?

It might be better to normalize ephemeral container statuses, at least timestamps, in the status manager to avoid this unexpected behavior.


#### How can we reproduce it (as minimally and precisely as possible)?

Set the log verbosity of kubelet to 3 or larger. Then, create an ephemeral container like [this](https://kubernetes.io/docs/tasks/debug/debug-application/debug-running-pod/#ephemeral-container-example).

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: v1.29.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.2
```
</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124871 kubelet crash loop panic with SIGSEGV

- Issue é“¾æ¥ï¼š[#124871](https://github.com/kubernetes/kubernetes/issues/124871)

### Issue å†…å®¹

#### What happened?

I was running some pods on my node as usual.
At some point (23:41:35.127989 in the log), the kubelet crashed and then went into a crash loop.

Log showing the first crash and several thereafter: [kubelet-crash.txt](https://github.com/kubernetes/kubernetes/files/15312195/kubelet-crash.txt)


The errors I see shortly before the crash are:
```
projected.go:292] Couldn't get configMap default/kube-root-ca.crt: object "default"/"kube-root-ca.crt" not registered
projected.go:198] Error preparing data for projected volume kube-api-access-lwp5v for pod default/run-fluid-wtw5n-99kwf: object "default"/"kube-root-ca.crt" not registered
nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/e6cfc866-a5b2-4e16-9b83-884de8552d45-kube-api-access-lwp5v podName:e6cfc866-a5b2-4e16-9b83-884de8552d45 nodeName:}" failed. No retries permitted until 2024-05-13 23:41:38.8756138 +0000 UTC m=+9176.959811707 (durationBeforeRet
ry 2m2s). Error: MountVolume.SetUp failed for volume "kube-api-access-lwp5v" (UniqueName: "kubernetes.io/projected/e6cfc866-a5b2-4e16-9b83-884de8552d45-kube-api-access-lwp5v") pod "run-fluid-wtw5n-99kwf" (UID: "e6cfc866-a5b2-4e16-9b83-884de8552d45") : object "default"/"kube-root-ca.crt" not registered
```

Stack trace from the first crash (later stack traces are a bit different):
```
goroutine 401 [running]:
k8s.io/apimachinery/pkg/util/runtime.logPanic({0x3d005c0?, 0x6d89410})
        vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:75 +0x99
k8s.io/apimachinery/pkg/util/runtime.HandleCrash({0x0, 0x0, 0x480e338?})
        vendor/k8s.io/apimachinery/pkg/util/runtime/runtime.go:49 +0x75
panic({0x3d005c0, 0x6d89410})
        /usr/local/go/src/runtime/panic.go:884 +0x213
errors.As({0x12, 0x0}, {0x3eac4c0?, 0xc0014e6590?})
        /usr/local/go/src/errors/wrap.go:109 +0x215
k8s.io/apimachinery/pkg/util/net.IsConnectionReset(...)
        vendor/k8s.io/apimachinery/pkg/util/net/util.go:45
k8s.io/client-go/rest.(*Request).request.func2(0x6d2e8a0?, {0x12, 0x0})
        vendor/k8s.io/client-go/rest/request.go:1007 +0x79
k8s.io/client-go/rest.IsRetryableErrorFunc.IsErrorRetryable(...)
        vendor/k8s.io/client-go/rest/with_retry.go:43
k8s.io/client-go/rest.(*withRetry).IsNextRetry(0xc001587f40, {0x0?, 0x0?}, 0x0?, 0xc001ef5f00, 0xc001bb1560, {0x12, 0x0}, 0x480e328)
        vendor/k8s.io/client-go/rest/with_retry.go:169 +0x170
k8s.io/client-go/rest.(*Request).request.func3(0xc001bb1560, 0xc002913af8, {0x4c44840?, 0xc001587f40?}, 0x0?, 0x0?, 0x39efc40?, {0x12?, 0x0?}, 0x480e328)
        vendor/k8s.io/client-go/rest/request.go:1042 +0xba
k8s.io/client-go/rest.(*Request).request(0xc00199f200, {0x4c42e00, 0xc00227b0e0}, 0x2?)
        vendor/k8s.io/client-go/rest/request.go:1048 +0x4e5
k8s.io/client-go/rest.(*Request).Do(0xc00199f200, {0x4c42dc8, 0xc000196010})
        vendor/k8s.io/client-go/rest/request.go:1063 +0xc9
k8s.io/client-go/kubernetes/typed/core/v1.(*nodes).Get(0xc001db6740, {0x4c42dc8, 0xc000196010}, {0x7ffe6a355c42, 0x1d}, {{{0x0, 0x0}, {0x0, 0x0}}, {0x4c03920, ...}})
        vendor/k8s.io/client-go/kubernetes/typed/core/v1/node.go:77 +0x145
k8s.io/kubernetes/pkg/kubelet.(*Kubelet).tryUpdateNodeStatus(0xc000289400, {0x4c42dc8, 0xc000196010}, 0x44ead4?)
        pkg/kubelet/kubelet_node_status.go:561 +0xf6
k8s.io/kubernetes/pkg/kubelet.(*Kubelet).updateNodeStatus(0xc000289400, {0x4c42dc8, 0xc000196010})
        pkg/kubelet/kubelet_node_status.go:536 +0xfc
k8s.io/kubernetes/pkg/kubelet.(*Kubelet).syncNodeStatus(0xc000289400)
        pkg/kubelet/kubelet_node_status.go:526 +0x105
k8s.io/apimachinery/pkg/util/wait.BackoffUntil.func1(0xc002913f28?)
        vendor/k8s.io/apimachinery/pkg/util/wait/backoff.go:226 +0x3e
k8s.io/apimachinery/pkg/util/wait.BackoffUntil(0x0?, {0x4c19ae0, 0xc0008f2000}, 0x1, 0xc000180360)
        vendor/k8s.io/apimachinery/pkg/util/wait/backoff.go:227 +0xb6
k8s.io/apimachinery/pkg/util/wait.JitterUntil(0x0?, 0x2540be400, 0x3fa47ae147ae147b, 0x0?, 0x0?)
        vendor/k8s.io/apimachinery/pkg/util/wait/backoff.go:204 +0x89
created by k8s.io/kubernetes/pkg/kubelet.(*Kubelet).Run
        pkg/kubelet/kubelet.go:1606 +0x58a
panic: runtime error: invalid memory address or nil pointer dereference [recovered]
        panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x1a pc=0x47a935]
```

#### What did you expect to happen?

Ideally no crash, but at least a useful error message.

#### How can we reproduce it (as minimally and precisely as possible)?

Really not sure.

#### Anything else we need to know?

_No response_

#### Kubernetes version

Client Version: v1.28.6
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.6

#### Cloud provider

Bare metal.

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

kubespray

#### Container runtime (CRI) and version (if applicable)

containerd

containerd github.com/containerd/containerd v1.7.13 7c3aca7a610df76212171d200ca3811ff6096eb8

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

CNI = calico v3.26.4


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124868 image-gc-high-threshold should be lower than value causing hard eviction nodefs.available or imagefs.available

- Issue é“¾æ¥ï¼š[#124868](https://github.com/kubernetes/kubernetes/issues/124868)

### Issue å†…å®¹

#### What happened?

when disk fills, it hits the hard eviction threshold, causing node disk pressure in the same moment imagegc spots it should prune something and start acting. this casues node going into disk pressure and evicting pods and not just start imagegc soon enough

#### What did you expect to happen?

i expect imagegc spots the filling disk soon enough, to start garbage collection and empty disk before node hits disk pressure.

from current documentation:

https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/

```
--image-gc-high-threshold int32     Default: 85
--image-gc-low-threshold int32     Default: 80
```

and

https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/
```
    "imageGCHighThresholdPercent": 85,
    "imageGCLowThresholdPercent": 80,
```

https://kubernetes.io/docs/concepts/scheduling-eviction/node-pressure-eviction/#hard-eviction-thresholds

vs.

```
nodefs.available<10%
imagefs.available<15%
```

... this does not make sense having the same percentage for imagefs.available (100-15 = 85 :-)

better approach would be having default values a little bit shifted with eqivalent to setting

```
  - "--image-gc-high-threshold=80"  
  - "--image-gc-low-threshold=75"   
```

... after setting this, i almost never get node disk pressure, because garbage collection and pruning disk happens soon enough


#### How can we reproduce it (as minimally and precisely as possible)?

fill disk, spot node disk pressure state at the same moment it starts garbage collecting.

#### Anything else we need to know?

_No response_

#### Kubernetes version

this is not version specific as checked documentation at the moment, its for years the same.

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

none - kubeadm installation
<details>

</details>


#### OS version

not relevant

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124839 Kubelet crashes on concurrent map iteration and map write

- Issue é“¾æ¥ï¼š[#124839](https://github.com/kubernetes/kubernetes/issues/124839)

### Issue å†…å®¹

#### What happened?

When running e2e, the following fatal error can be encountered:

```
  30715 I0513 08:25:43.792330  285839 reflector.go:289] Starting reflector *v1.ConfigMap (0s) from object-"flexvolume-445 9"/"kube-root-ca.crt"
  30716 I0513 08:25:43.797355  285839 plugins.go:651] "Loaded volume plugin" pluginName="k8s/attachable-with-long-mount-flexvolume-4459"
  30717 I0513 08:25:43.797400  285839 plugins.go:651] "Loaded volume plugin" pluginName="k8s/dummy-attachable"
  30718 I0513 08:25:43.851107  285839 reconciler.go:352] "attacherDetacher.AttachVolume started" volumeName="k8s/attachable-with-long-mount-flexvolume-4459/test-long-detach-flex" nodeName="master" scheduledPods=["        flexvolume-4459/flexvolume-detach-test-client"]
  30719 I0513 08:25:43.854533  285839 operation_generator.go:400] AttachVolume.Attach succeeded for volume "test-long-detach-flex" (UniqueName: "k8s/attachable-with-long-mount-flexvolume-4459/test-long-detach-flex        ") from node "master"
  30720 I0513 08:25:43.854661  285839 event.go:307] "Event occurred" object="flexvolume-4459/flexvolume-detach-test-client" fieldPath="" kind="Pod" apiVersion="v1" type="Normal" reason="SuccessfulAttachVolume" mes        sage="AttachVolume.Attach succeeded for volume \"test-long-detach-flex\" "
  30721 I0513 08:25:43.864876  285839 plugins.go:651] "Loaded volume plugin" pluginName="k8s/attachable-with-long-mount-flexvolume-4459"
  30722 I0513 08:25:43.866879  285839 desired_state_of_world.go:351] "volume add to dsw" volume="test-long-detach-flex" podName="ee90c357-99a0-467c-b829-1525c220da21"
  30723 I0513 08:25:43.866933  285839 desired_state_of_world.go:351] "volume add to dsw" volume="kube-api-access-4525n" podName="ee90c357-99a0-467c-b829-1525c220da21"
  30724 I0513 08:25:43.971490  285839 plugins.go:651] "Loaded volume plugin" pluginName="k8s/dummy-attachable"
  30725 fatal error: concurrent map iteration and map write
  30726 
  30727 goroutine 4078 [running]:
  30728 k8s.io/kubernetes/pkg/volume.(*VolumePluginMgr).FindPluginBySpec(0xc00b0bc008, 0xc00b944168)
  30729         pkg/volume/plugins.go:676 +0x37f
  30730 k8s.io/kubernetes/pkg/volume/util/operationexecutor.(*operationGenerator).GenerateMountVolumeFunc(0xc00b08ef50, 0x8bb2c97000, {{0xc0150614a0, 0x4c}, {0xc0135d7a70, 0x24}, 0xc00b944168, {0xc00c9bfac0, 0xf},         0xc015846488, ...}, ...)
```
The related code is as follows:
Added read lock on line 634:
https://github.com/kubernetes/kubernetes/blob/a07d3c49b301acd3426e985c3ad54a56fdb5b925/pkg/volume/plugins.go#L633-L666
delete map on line 714:
https://github.com/kubernetes/kubernetes/blob/a07d3c49b301acd3426e985c3ad54a56fdb5b925/pkg/volume/plugins.go#L694-L720





#### What did you expect to happen?

What's a good way for the community to solve this problem?

#### How can we reproduce it (as minimally and precisely as possible)?

None at present

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
1.28.1
```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>
false
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124797 app Container can't reuse its init Container cpuset in a specific condition

- Issue é“¾æ¥ï¼š[#124797](https://github.com/kubernetes/kubernetes/issues/124797)

### Issue å†…å®¹

#### What happened?

We can't make sure app Container always reuses init Container cpuset, which may lead to the waste of CPU(init container has already exited but the cpuset can not be reused by other containers) and `not enough cpus available to satis
fy request` error.

<img width="1421" alt="image" src="https://github.com/kubernetes/kubernetes/assets/14137033/1135377d-dde3-48d5-84aa-c940f9bf4992">


#### What did you expect to happen?

app Container always reuses init Container cpuset after init Container exits.

#### How can we reproduce it (as minimally and precisely as possible)?

This is one of the spefic condition that might cause the issue:
- Pod A ready to allocate cpuset with `init container` and `app container` both request 92 cpu.
- Pod B already running on the node and ready to be deleted.

1. Pod A's init container starts to allocate cpusetï¼ˆ4-24,48-60,73-84,100-120,144-156,169-180ï¼‰ï¼š
```
I0510 16:40:21.232949   20266 state_mem.go:80] "Updated desired CPUSet" podUID="2f9922ce-df66-4b58-abd8-01187b813318" containerName="init-container" cpuSet="4-24,48-60,73-84,100-120,144-156,169-180"
```
2. Pod A's init container exits.
3. Before Pod A's app container starts to allocate cpuset, Pod B gets deleted and release it's cpusetï¼ˆcpuSet="0-3,25-47,61-72,85-99,121-143,157-168,181-191ï¼‰ï¼š
```
I0510 16:40:27.759335   20266 state_mem.go:107] "Deleted CPUSet assignment" podUID="74510e24-48ba-4fd7-ab85-80dd99c6df5d" containerName="deleted-container"
I0510 16:40:27.759714   20266 state_mem.go:88] "Updated default CPUSet" cpuSet="0-3,25-47,61-72,85-99,121-143,157-168,181-191"
```
4. Pod A's app container starts to allocate cpuset. 
What we expect is that Pod A's app container reuses its init container's cpuset.
But due to Pod B's deletion, it won't allocate the same cpuset as its init container.ï¼ˆ4-49,100-145ï¼‰ï¼š
```
 I0510 16:40:27.989453   20266 state_mem.go:80] "Updated desired CPUSet" podUID="2f9922ce-df66-4b58-abd8-01187b813318" containerName="app-container" cpuSet="4-49,100-145"
```

Now we have Pod A's init container taking cpuset: 4-24,48-60,73-84,100-120,144-156,169-180
And  Pod A's app container  taking cpuset: 4-49,100-145
The init container cpuset won't be reused as expected.

A new Pod C starts to allocate cpuset, it may get `not enough cpus available to satis
fy request` error

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
1.30
```

</details>


#### Cloud provider

<details>
NONE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124796 bug: kubelet panic & crash if `--config-dir` is used

- Issue é“¾æ¥ï¼š[#124796](https://github.com/kubernetes/kubernetes/issues/124796)

### Issue å†…å®¹

#### What happened?

I created a v1.30.0 k8s cluster with kubeadm and created a  drop-in directory for kubelet configuration under `/etc/kubernetes/kubelet.conf.d`. The normal conf file created by kubeadm is in ` /var/lib/kubelet/config.yaml`

According to the [docs](https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/#kubelet-conf-d) the kubelet should merge all configs in specified order and start. But the kubelet crashes under two setups:

**Setup 1**:

The `/etc/kubernetes/kubelet.conf.d/20-kubelet.conf` is completely empty the kubelet crashes with:
```
>kubelet --config-dir /etc/kubernetes/kubelet.conf.d --config /var/lib/kubelet/config.yaml
E0510 13:47:28.749138    9050 run.go:74] "command failed" err="failed to merge kubelet configs: failed to walk through kubelet dropin directory \"/etc/kubernetes/kubelet.conf.d\": failed to load kubelet dropin file, path: /etc/kubernetes/kubelet.conf.d/20-kubelet.conf, error: kubelet config file \"/etc/kubernetes/kubelet.conf.d/20-kubelet.conf\" was empty"
```
If there is just one space or an newline in that file it crashes with:
```
>kubelet --config-dir /etc/kubernetes/kubelet.conf.d --config /var/lib/kubelet/config.yaml
E0510 13:49:26.243517    9062 run.go:74] "command failed" err=<
        failed to merge kubelet configs: failed to walk through kubelet dropin directory "/etc/kubernetes/kubelet.conf.d": failed to load kubelet dropin file, path: /etc/kubernetes/kubelet.conf.d/20-kubelet.conf, error: Object 'Kind' is missing in '
        '
```
Even when the docs say:

> These files may contain partial configurations and might not be valid config files by themselves. Validation is only performed on the final resulting configuration structure stored internally in the kubelet. 

If there is nothing or only whitspaces in  `/etc/kubernetes/kubelet.conf.d` the merged config should not be invalid and not every file in `/etc/kubernetes/kubelet.conf.d` must contain a `kind` property.

**Setup 2**:
The `/etc/kubernetes/kubelet.conf.d/20-kubelet.conf` is the same as the `/var/lib/kubelet/config.yaml` the kubelet panics and crashes with:
```
> kubelet --config-dir /etc/kubernetes/kubelet.conf.d --config /var/lib/kubelet/config.yaml
panic: non-positive interval for NewTicker

goroutine 1 [running]:
time.NewTicker(0xc00098f7a0?)
        time/tick.go:22 +0xe5
k8s.io/klog/v2/internal/clock.RealClock.NewTicker(...)
        k8s.io/klog/v2@v2.120.1/internal/clock/clock.go:111
k8s.io/klog/v2.(*flushDaemon).run(0xc000229650, 0x0)
        k8s.io/klog/v2@v2.120.1/klog.go:1169 +0x10f
k8s.io/klog/v2.StartFlushDaemon(0x0)
        k8s.io/klog/v2@v2.120.1/klog.go:1220 +0x36
k8s.io/component-base/logs/api/v1.apply(0xc0008c8a38, 0x0, {0x7f1ede53b2f0, 0xc000557130})
        k8s.io/component-base/logs/api/v1/options.go:285 +0x890
k8s.io/component-base/logs/api/v1.validateAndApply(0xc0008c8a38, 0x0, {0x7f1ede53b2f0, 0xc000557130}, 0x4?)
        k8s.io/component-base/logs/api/v1/options.go:132 +0x71
k8s.io/component-base/logs/api/v1.ValidateAndApplyAsField(...)
        k8s.io/component-base/logs/api/v1/options.go:124
k8s.io/kubernetes/cmd/kubelet/app.NewKubeletCommand.func1(0xc0000d8908, {0xc000100060, 0x4, 0x4})
        k8s.io/kubernetes/cmd/kubelet/app/server.go:238 +0x4bc
github.com/spf13/cobra.(*Command).execute(0xc0000d8908, {0xc000100060, 0x4, 0x4})
        github.com/spf13/cobra@v1.7.0/command.go:940 +0x882
github.com/spf13/cobra.(*Command).ExecuteC(0xc0000d8908)
        github.com/spf13/cobra@v1.7.0/command.go:1068 +0x3a5
github.com/spf13/cobra.(*Command).Execute(...)
        github.com/spf13/cobra@v1.7.0/command.go:992
k8s.io/component-base/cli.run(0xc0000d8908)
        k8s.io/component-base/cli/run.go:146 +0x290
k8s.io/component-base/cli.Run(0xc0000061c0?)
        k8s.io/component-base/cli/run.go:46 +0x17
main.main()
        k8s.io/kubernetes/cmd/kubelet/kubelet.go:36 +0x18
```
Using the same config only via `--config` works.

#### What did you expect to happen?

The kubelet should start with empty files in `/etc/kubernetes/kubelet.conf.d` or if the files in `/var/lib/kubelet/config.yaml` and `/etc/kubernetes/kubelet.conf.d` are the same.

If this is not the expected usage, the docs have to be adjusted. 

#### How can we reproduce it (as minimally and precisely as possible)?

Create a cluster with kubeadm v1.30 with kublet config in `/var/lib/kubelet/config.yaml` and `/etc/kubernetes/kubelet.conf.d` or setup kubelet by hand. 

```yaml
# kubeadm config
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration

nodeRegistration:
  kubeletExtraArgs:
    config: /var/lib/kubelet/config.yaml
    config-dir: /etc/kubernetes/kubelet.conf.d
``` 


#### Anything else we need to know?

The kublet conf api version is `apiVersion: kubelet.config.k8s.io/v1beta1` and is was with and without setting `KUBELET_CONFIG_DROPIN_DIR_ALPHA`

Functionality was introduced in #119390

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
$ kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"30", GitVersion:"v1.30.0", GitCommit:"7c48c2bd72b9bf5c44d21d7338cc7bea77d0ad2a", GitTreeState:"clean", BuildDate:"2024-04-17T17:34:08Z", GoVersion:"go1.22.2", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
Hetzner/None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"
$ uname -a
Linux controlplane-node-amd64-vcxkzu 6.1.0-21-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.90-1 (2024-05-03) x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd github.com/containerd/containerd v1.7.16
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
not relevant
</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124880 Kubectl auth can-i doesn't know about the `approve` verb

- Issue é“¾æ¥ï¼š[#124880](https://github.com/kubernetes/kubernetes/issues/124880)

### Issue å†…å®¹

**What happened**:

If you use `kubectl auth can-i` to test whether a user can approve certificate signing requests, you get a response indicating that `approve` is not a valid verb

```bash
kubectl auth can-i approve certificatesigningrequests.certificates.k8s.io
Warning: resource 'certificatesigningrequests' is not namespace scoped in group 'certificates.k8s.io'

Warning: verb 'approve' is not a known verb
```

An example of the `approve` verb being used can be seen [here](https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#approval-rejection-kubectl)

**What you expected to happen**:

`approve` should be recognized as a valid verb on `certificatesigningrequest` objects.


**How to reproduce it (as minimally and precisely as possible)**:
Run

```bash
kubectl auth can-i approve certificatesigningrequests.certificates.k8s.io
```

**Anything else we need to know?**:

I think that `approve` needs to be added to the kubectl auth can-i code [here](https://github.com/kubernetes/kubectl/blob/514f46729f82412dd9cc41f206058bc4ae9b62b0/pkg/cmd/auth/cani.go#L106)

**Environment**:
- Kubernetes client and server versions (use `kubectl version`):
- Cloud provider or hardware configuration:
- OS (e.g: `cat /etc/os-release`):



### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124786 InPlacePodVerticalScaling does not meet the requirement of qosClass being equal to Guaranteed after shrinking the memory

- Issue é“¾æ¥ï¼š[#124786](https://github.com/kubernetes/kubernetes/issues/124786)

### Issue å†…å®¹

#### What happened?

InPlacePodVerticalScaling does not meet the requirement of qosClass being equal to Guaranteed after shrinking the memory
![image](https://github.com/kubernetes/kubernetes/assets/13641341/0c5d19fb-b124-4cbf-a520-b574d3fe0656)


#### What did you expect to happen?

InPlacePodVerticalScaling maintains the same qosclass type of Pod before and after scaling

#### How can we reproduce it (as minimally and precisely as possible)?

After enabling the InPlacePodVerticalScaling feature, the patch modifies the request and limit of the container's resource to a value smaller than usage

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: version.Info{Major:"1", Minor:"24", GitVersion:"v1.24.1", GitCommit:"3ddd0f45aa91e2f30c70734b175631bec5b5825a", GitTreeState:"clean", BuildDate:"2022-05-24T12:17:11Z", GoVersion:"go1.18.3", Compiler:"gc", Platform:"darwin/amd64"}
Kustomize Version: v4.5.4
Server Version: version.Info{Major:"1", Minor:"29", GitVersion:"v1.29.2", GitCommit:"4b8e819355d791d96b7e9d9efe4cbafae2311c88", GitTreeState:"clean", BuildDate:"2024-02-14T22:24:00Z", GoVersion:"go1.21.7", Compiler:"gc", Platform:"linux/amd64"}
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124781 kubectl cp a file from a Windows Pod to Windows 11 localhost failed

- Issue é“¾æ¥ï¼š[#124781](https://github.com/kubernetes/kubernetes/issues/124781)

### Issue å†…å®¹

#### What happened?

I was trying to cp a from a Windows Pod to the Windows localhost. But it failed.


```
kubectl.exe cp --kubeconfig .\kubconfig -n namespace-win pod-20240509-120334-779546rrfsc:C:\\data\\0509-1065.log 0509-1065.log
```

It showed

```
tar: Removing leading drive letter from member names
error: tar contents corrupted
```

#### What did you expect to happen?

I want to copy the file to the localhost.

#### How can we reproduce it (as minimally and precisely as possible)?

Windows 11 localhost
Windows Pod.


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$(powershell) kubectl version
# paste output here

Client Version: v1.29.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Unable to connect to the server: dial tcp 127.0.0.1:6443: connectex: No connection could be made because the target machine actively refused it.
```
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here

BuildNumber  Caption                      OSArchitecture  Version
22621        Microsoft Windows 11 Professional  64-bit          10.0.22621
```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124760 Open API v3 requirement for `PATCH` verb support (introduced PR #115119)

- Issue é“¾æ¥ï¼š[#124760](https://github.com/kubernetes/kubernetes/issues/124760)

### Issue å†…å®¹

#### What happened?

Aggregated API backend:  Call to `CREATE` endpoint consults `PATCH` field validation properties, even if the resource does not support the `PATCH` verb..

#### What did you expect to happen?

Expected `CREATE` to be called with `CREATE` field validation without looking at the `PATCH` endpoint.

#### How can we reproduce it (as minimally and precisely as possible)?

Create Open API v3 specification with a field validator.  Storage for API server (aggregated API server) advertises support only for `CREATE`, `GET`, `DELETE`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.9
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.9-gke.1000000
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

    architecture: amd64
    bootID: f993a6ef-ee9e-457b-856e-bfd51f9cdd22
    containerRuntimeVersion: containerd://1.7.13
    kernelVersion: 5.15.0-1054-gke
    kubeProxyVersion: v1.27.12-gke.1190000
    kubeletVersion: v1.27.12-gke.1190000
    machineID: 4da31cf9d84cf8d82e680abcb09130ae
    operatingSystem: linux
    osImage: Ubuntu 22.04.4 LTS
    systemUUID: 4da31cf9-d84c-f8d8-2e68-0abcb09130ae

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124749 Volumeattachment deletion in a detach operation should carry the resourceVersion

- Issue é“¾æ¥ï¼š[#124749](https://github.com/kubernetes/kubernetes/issues/124749)

### Issue å†…å®¹

#### What happened?

There is a use case in the flow test that creates a pod which uses pvc and then waits about 2 minutes and then delete the pod. Later it was discovered that the pv referenced by the pod had been attached on the node and had not been detached.

Combining the csi plugin and k8s component logs, we found that the csi plugin took a long time to attach, and it was very late before it succeeded, and then it patched finalizers on the volumeattachment resource. At the same time, the deletion of the pod triggered the k8s detach operation, which will delete the The volumeattachment resource.

Due to the multiple instances of apiserver, when the volumeattachment delete operation reaches an apiserver, it does not realize that finalizers have been patched on the va resource, resulting in a successful delete operation that deletes the va.

csi plugin log:
```
opdisk_sts_attacher.log:I0319 16:58:00.344601       1 round_trippers.go:454] PATCH [https://123.123.0.1:443/apis/storage.k8s.io/v1/volumeattachments/csi-a9400fe6be648868b80a0e012f8aa726c23455a934926e22f1e70371fdee2cea](https://123.123.0.1/apis/storage.k8s.io/v1/volumeattachments/csi-a9400fe6be648868b80a0e012f8aa726c23455a934926e22f1e70371fdee2cea) 200 OK in 23 milliseconds

opdisk_sts_attacher.log:I0319 17:00:08.945137       1 csi_handler.go:275] Attached "csi-a9400fe6be648868b80a0e012f8aa726c23455a934926e22f1e70371fdee2cea"

opdisk_sts_attacher.log:I0319 17:00:08.945141       1 util.go:37] Marking as attached "csi-a9400fe6be648868b80a0e012f8aa726c23455a934926e22f1e70371fdee2cea"

opdisk_sts_attacher.log:I0319 17:00:08.949561       1 round_trippers.go:454] PATCH [https://123.123.0.1:443/apis/storage.k8s.io/v1/volumeattachments/csi-a9400fe6be648868b80a0e012f8aa726c23455a934926e22f1e70371fdee2cea/status](https://123.123.0.1/apis/storage.k8s.io/v1/volumeattachments/csi-a9400fe6be648868b80a0e012f8aa726c23455a934926e22f1e70371fdee2cea/status) 404 Not Found in 4 milliseconds

opdisk_sts_attacher.log:I0319 17:00:08.949646       1 csi_handler.go:236] Error processing "csi-a9400fe6be648868b80a0e012f8aa726c23455a934926e22f1e70371fdee2cea": failed to mark as attached: volumeattachments.storage.k8s.io "csi-a9400fe6be648868b80a0e012f8aa726c23455a934926e22f1e70371fdee2cea" not found

opdisk_sts_attacher.log:I0319 17:01:25.093522       1 controller.go:198] Started VA processing "csi-a9400fe6be648868b80a0e012f8aa726c23455a934926e22f1e70371fdee2cea"

opdisk_sts_attacher.log:I0319 17:01:25.093538       1 controller.go:205] VA "csi-a9400fe6be648868b80a0e012f8aa726c23455a934926e22f1e70371fdee2cea" deleted, ignoring
```

kube-controller-manager log:
```
kube-controller-manager.klog:I0319 16:58:00.333212      11 operation_generator.go:1665] Verified volume is safe to detach for volume "pvc-03cb8ebe-d2b5-410c-9db5-0a6b131d8f03" (UniqueName: "kubernetes.io/csi/opdisk.csi.openpalette.org^9df25125-5cb6-4965-9f88-bbceb277224a") on node "minion-0-0"

kube-controller-manager.klog:I0319 16:58:00.885712      11 operation_generator.go:526] DetachVolume.Detach succeeded for volume "pvc-03cb8ebe-d2b5-410c-9db5-0a6b131d8f03" (UniqueName: "kubernetes.io/csi/opdisk.csi.openpalette.org^9df25125-5cb6-4965-9f88-bbceb277224a") on node "minion-0-0"
```

#### What did you expect to happen?

Solve the problem of concurrent operations of finalizers patch and va deletion to ensure the safe deletion of va.

in pkg/volume/csi/csi_attacher.go

we can use c.plugin.volumeAttachmentLister.Get(attachID) to get va resourceVersion and pass to Delete function bellow to solve the problem.
```
c.k8s.StorageV1().VolumeAttachments().Delete(context.TODO(), attachID, metav1.DeleteOptions{Preconditions: &metav1.Preconditions{ResourceVersion: &resourceVersion}})
```

#### How can we reproduce it (as minimally and precisely as possible)?

Low probability of recurrence

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
v1.28.3
```

</details>


#### Cloud provider

<details>
no
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124745 HPAContainerMetrics Version V2Beta2 Cannot Implement on Kubernetes 1.30 as "V2Beta2" was remove since Kubernetes 1.26

- Issue é“¾æ¥ï¼š[#124745](https://github.com/kubernetes/kubernetes/issues/124745)

### Issue å†…å®¹

#### What happened?

Base on HPA/v2beta2 was removed since Kubernetes 1.26 and current HPA/V2 is not support "unknown field "spec.containerMetrics", unknown field "spec.metrics[0].resource.container". So just want suggestion for implement HPAContainerMetrics on cluster.

<img width="1424" alt="328572042-781024d1-7647-43a7-8481-4503f0a9c404" src="https://github.com/kubernetes/kubernetes/assets/16981299/cef4406f-e717-4d74-88f3-69ce6319b5cc">


#### What did you expect to happen?

HPAContainerMetrics should working fine on Kubernetes 1.30 as GA feature gate

#### How can we reproduce it (as minimally and precisely as possible)?

Yes base on issue of syntact api version v2beta2 is not avaliable on Kubernetes 1.30 for create this hpa

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.30.0

#### Cloud provider

AWS Cloud Controller

#### OS version

Ubuntu 22.04

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

Containerd

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124734 CPUStats.UsageCoreNanoSeconds is inaccurate on windows - 1/10 of the real value.

- Issue é“¾æ¥ï¼š[#124734](https://github.com/kubernetes/kubernetes/issues/124734)

### Issue å†…å®¹

#### What happened?

The CPU utilization metrics from Kubelet summary API  [`CPUStats.UsageCoreNanoSeconds`](https://github.com/kubernetes/kubernetes/blob/79470526896f8b6ca744c29109bc3455c1a9d199/staging/src/k8s.io/kubelet/pkg/apis/stats/v1alpha1/types.go#L224C2-L224C22) is not accurate on Windows. The value is only 1/10 of the real value which I have other VM level metrics as a proof.

## summary API CPU utilization metric

![image](https://github.com/kubernetes/kubernetes/assets/14968376/f54a5e13-f6f7-4c6f-afc5-8e9704397600)

## VM CPU utilization metric

![image](https://github.com/kubernetes/kubernetes/assets/14968376/ddc08d96-4ef3-4a98-bda5-328ad864ce2e)



#### What did you expect to happen?

The `CPUStats.UsageCoreNanoSeconds` should be accurate on Windows too.

#### How can we reproduce it (as minimally and precisely as possible)?

```
$ kubectl get --raw "/api/v1/nodes/<NODE>/proxy/stats/summary"
```

T1
```
  "cpu": {
   "time": "2023-12-06T21:23:48Z",
   "usageNanoCores": 8000000,
   "usageCoreNanoSeconds": 20985800000000
  },
```

T2
```
  "cpu": {
   "time": "2023-12-06T21:24:38Z",
   "usageNanoCores": 8000000,
   "usageCoreNanoSeconds": 20986320000000
  },
```

(20986320000000 - 20985800000000)ns / 50s = 0.01 s/s
This matches the magnitude of the metric chart.

#### Anything else we need to know?

This problem doesn't exist on Linux.

Call stack
- metric is set here by kubelet - https://github.com/kubernetes/kubernetes/blob/7fe31be11fbe9b44af262d5f5cffb1e73648aa96/pkg/kubelet/winstats/winstats.go#L118-L129
- the data collected by using windows perf counters - https://github.com/kubernetes/kubernetes/blob/7fe31be11fbe9b44af262d5f5cffb1e73648aa96/pkg/kubelet/winstats/perfcounter_nodestats.go#L206-L212
- query here - https://github.com/kubernetes/kubernetes/blob/7fe31be11fbe9b44af262d5f5cffb1e73648aa96/pkg/kubelet/winstats/perfcounters.go#L32

#### Kubernetes version

<details>

```console
$ kubectl version
1.27
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124733 Confusing doc on name of object of kind defined by CRD

- Issue é“¾æ¥ï¼š[#124733](https://github.com/kubernetes/kubernetes/issues/124733)

### Issue å†…å®¹

#### What happened?

I wondered what are the restrictions (if any) that Kubernetes imposes on the names of objects of a kind that gets defined by a CustomResourceDefinition. I am not talking about the name of the CRD itself. I tried to find this in the documentation, and couldn't. I asked on the `sig-api-machinery` channel on the Kubernetes Slack, and was referred to https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#customresourcedefinitions . The text there says

> The name of a CRD object must be a valid [DNS subdomain name](https://kubernetes.io/docs/concepts/overview/working-with-objects/names#dns-subdomain-names).

Since a CRD is itself an object, I thought this was clearly talking about the name of the CRD itself (I temporarily forgot that I know that the name of the CRD itself is constrained to be `${resouce}.${apiGroup}`). But I was assured that this text is trying to talk about the objects whose kind is defined by the CRD.

#### What did you expect to happen?

The Kubernetes documentation clearly answers my question.

#### How can we reproduce it (as minimally and precisely as possible)?

Explained above.

#### Anything else we need to know?

No.

#### Kubernetes version

<details>

This doc has been this way for a long time.

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

N/A

</details>


#### Install tools

<details>
N/A
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
N/A
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
N/A
</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124716 Kubelet memory leak when a plugin is registered twice

- Issue é“¾æ¥ï¼š[#124716](https://github.com/kubernetes/kubernetes/issues/124716)

### Issue å†…å®¹

#### What happened?

We found that the kubelet memory kept increasingï¼Œand we exported the pprof of the goroutine. The grpc goroutine leaks, causing memory leakage.
![image](https://github.com/kubernetes/kubernetes/assets/54977497/76a1d622-2129-49e0-bd4c-79b6bad786b1)
We found out that the reason was because one of the pluginThe following code causes this situation. One client is lost.s kept registering twice and using the same name for both.
https://github.com/kubernetes/kubernetes/blob/1dc30bf90fd6a729d226b4e942118110b0a73e65/pkg/kubelet/cm/devicemanager/plugin/v1beta1/handler.go#L90-L96
When two requests are registered at the same time, only one client is reserved in s.clients.
https://github.com/kubernetes/kubernetes/blob/1dc30bf90fd6a729d226b4e942118110b0a73e65/pkg/kubelet/cm/devicemanager/plugin/v1beta1/handler.go#L106-L117
Therefore, after the c.Run () method in the runClient method is executed, s.getClient obtains only one registered client. As a result, the c.grpc.Close () method is not invoked, causing memory and coroutine leakage.


#### What did you expect to happen?

Even in this case, kubelet should not leak memory.

#### How can we reproduce it (as minimally and precisely as possible)?

1ã€The plug-in is registered every 5 seconds and two registration requests are sent at the same time.
2ã€The kubelet memory usage keeps increasing.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# 1.28
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124709 Throughput degradation scheduling daemonset pods

- Issue é“¾æ¥ï¼š[#124709](https://github.com/kubernetes/kubernetes/issues/124709)

### Issue å†…å®¹

#### What happened?

https://github.com/kubernetes/kubernetes/pull/119779 added some map queries and creations that add non-negligible latency.

The pprof reveals the following lines inside `findNodesThatFitPod` as too expensive:
- 22.2% https://github.com/kubernetes/kubernetes/blob/44bd04c0cbddde69aaeb7a90d3bd3de4e417f27f/pkg/scheduler/schedule_one.go#L492
- 21.7% https://github.com/kubernetes/kubernetes/blob/44bd04c0cbddde69aaeb7a90d3bd3de4e417f27f/pkg/scheduler/schedule_one.go#L489

![image](https://github.com/kubernetes/kubernetes/assets/1299064/d82a658e-ae01-4bcd-9e0d-6ab892bee6c2)


#### What did you expect to happen?

Scheduler to keep a throughput of 300 pods/s

#### How can we reproduce it (as minimally and precisely as possible)?

Schedule 5k daemonset pods giving 300 qps to the scheduler

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
v1.30
</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124704 Pod phase transition is slower when EventedPLEG is enabled

- Issue é“¾æ¥ï¼š[#124704](https://github.com/kubernetes/kubernetes/issues/124704)

### Issue å†…å®¹

#### What happened?
After #124297, which fixes timestamps set by Evented PLEG, was merged, a pod phase transition is slower especially at deletion when the EventedPLEG is enabled.

EventedPLEG is enabled:
```
$ kubectl create -f simple-pod.yaml; kubectl get pod simple-pod -w & kubectl wait --for=jsonpath='{.status.phase}'=Running pod simple-pod; sleep 1; kubectl delete pod simple-pod; kill $!
pod/simple-pod created
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	3s
pod/simple-pod condition met
simple-pod   1/1 	Terminating     	0      	4s
pod "simple-pod" deleted
simple-pod   0/1 	Terminating     	0      	13s
simple-pod   0/1 	Terminating     	0      	18s
simple-pod   0/1 	Terminating     	0      	18s
simple-pod   0/1 	Terminating     	0      	18s
```

EventedPLEG is disabled:
```
$ kubectl create -f simple-pod.yaml; kubectl get pod simple-pod -w & kubectl wait --for=jsonpath='{.status.phase}'=Running pod simple-pod; sleep 1; kubectl delete pod simple-pod; kill $!
pod/simple-pod created
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	2s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	3s
simple-pod   0/1 	Terminating     	0      	4s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
```

As described here, a pod worker is blocked at `cache.GetNewerThan()` when it is woken up by an update without a cache update in a PLEG:
https://github.com/kubernetes/kubernetes/blob/ade0d2140a68a69f5343c865792393a83c76ca6a/pkg/kubelet/pod_workers.go#L1244-L1253

The worker is unblocked when another event is delivered or the PLEG calls `cache.UpdateTime()`. At the latter case, while the genericPLEG calls `cache.UpdateTime()` every one second along with `Relist()`, the EventPLEG calls `cache.UpdateTime()` every five seconds. Because of this difference, the Evented PLEG spends more time to get pods into another phase.

https://github.com/kubernetes/kubernetes/blob/ade0d2140a68a69f5343c865792393a83c76ca6a/pkg/kubelet/pleg/evented.go#L34-L37

Even if there is a cache update, a worker can be blocked when the cache is updated by an asynchronous event before the worker finishes `SyncPod()`. For instance, when a runtime starts a container, the PLEG gets an event and caches the container status(`running`). If this event is received after a pod worker finishes `SyncPod()` to start the container, the worker gets the new status at `GetNewerThan()` soon and runs `SyncPod()` again to update the pod phase to `running`. However, if the event arrives before the worker finishes `SyncPod()` to start the container, the worker is blocked at `GetNewerThan()` because the cached status is older than `lastSyncTime`.

#### What did you expect to happen?

The pod phase transition should be as fast as GenericPLEG. It would be better to set `globalCacheUpdatePeriod` to 1 second.

#### How can we reproduce it (as minimally and precisely as possible)?
This issue can be observed in the `pull-kubernetes-node-crio-cgrpv1-evented-pleg-e2e` job as described in https://github.com/kubernetes/kubernetes/pull/124297#issuecomment-2340279198:
```
E2eNode Suite: [It] [sig-node] [NodeFeature:SidecarContainers] Containers Lifecycle [It] should not hang in termination if terminated during initialization [sig-node, NodeFeature:SidecarContainers]
{ failed [FAILED] should delete in < 5 seconds, took 10.071795
Expected
    <float64>: 10.071794756
to be <
    <int64>: 5
In [It] at: k8s.io/kubernetes/test/e2e_node/container_lifecycle_test.go:3470 @ 09/10/24 08:54:31.819
}
```

This can be also reproduced manually as follows:

Build kubernetes locally with applying PR #124297 and run with enabling `EventedPLEG` feature gate.

Use this simple-pod.yaml
```
apiVersion: v1
kind: Pod
metadata:
  labels:
	run: simple-pod
  name: simple-pod
spec:
  containers:
  - command:
	- sh
	- -c
	- trap "exit 0" SIGTERM; while true; do sleep 1; done
	image: busybox
	name: simple-container
```

Run the command:
```
$ kubectl create -f simple-pod.yaml; kubectl get pod simple-pod -w & kubectl wait --for=jsonpath='{.status.phase}'=Running pod simple-pod; sleep 1; kubectl delete pod simple-pod; kill $!
```

This is the result I tried the command ten times:
<details>

```
$ for i in `seq 10`; do kubectl create -f simple-pod.yaml; kubectl get pod simple-pod -w & kubectl wait --for=jsonpath='{.status.phase}'=Running pod simple-pod; sleep 1; kubectl delete pod simple-pod; kill $!; done
pod/simple-pod created
[1] 17875
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	2s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	3s
simple-pod   0/1 	Terminating     	0      	12s
simple-pod   0/1 	Terminating     	0      	17s
simple-pod   0/1 	Terminating     	0      	17s
simple-pod   0/1 	Terminating     	0      	17s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 18248
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	5s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	6s
simple-pod   0/1 	Terminating     	0      	15s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 18583
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	5s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	6s
simple-pod   0/1 	Terminating     	0      	15s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 19057
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	5s
pod/simple-pod condition met
simple-pod   1/1 	Terminating     	0      	6s
pod "simple-pod" deleted
simple-pod   0/1 	Terminating     	0      	15s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 19386
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	5s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	6s
simple-pod   0/1 	Terminating     	0      	15s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 19718
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	5s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	6s
simple-pod   0/1 	Terminating     	0      	15s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 20052
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	5s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	6s
simple-pod   0/1 	Terminating     	0      	15s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 20586
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	5s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	6s
simple-pod   0/1 	Terminating     	0      	15s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 20914
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	5s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	6s
simple-pod   0/1 	Terminating     	0      	15s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 21294
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	5s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	6s
simple-pod   0/1 	Terminating     	0      	15s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
simple-pod   0/1 	Terminating     	0      	20s
```

</details>

This is the result when `EventedPLEG` is disabled:

<details>

```
$ for i in `seq 10`; do kubectl create -f simple-pod.yaml; kubectl get pod simple-pod -w & kubectl wait --for=jsonpath='{.status.phase}'=Running pod simple-pod; sleep 1; kubectl delete pod simple-pod; kill $!; done
pod/simple-pod created
[1] 34236
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	2s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	3s
simple-pod   0/1 	Terminating     	0      	4s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 34506
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	2s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	3s
simple-pod   0/1 	Terminating     	0      	4s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 34820
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
pod/simple-pod condition met
simple-pod   1/1 	Running         	0      	2s
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	3s
simple-pod   0/1 	Terminating     	0      	4s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 35084
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	2s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	3s
simple-pod   0/1 	Terminating     	0      	4s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 35373
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	2s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	3s
simple-pod   0/1 	Terminating     	0      	4s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 35640
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	2s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	3s
simple-pod   0/1 	Terminating     	0      	4s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 35900
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	2s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	3s
simple-pod   0/1 	Terminating     	0      	4s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 36173
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	2s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	3s
simple-pod   0/1 	Terminating     	0      	4s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 36430
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	2s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	3s
simple-pod   0/1 	Terminating     	0      	4s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
pod/simple-pod created
[1]+  Exit 1              	kubectl get pod simple-pod -w
[1] 36694
NAME     	READY   STATUS          	RESTARTS   AGE
simple-pod   0/1 	ContainerCreating   0      	0s
simple-pod   1/1 	Running         	0      	2s
pod/simple-pod condition met
pod "simple-pod" deleted
simple-pod   1/1 	Terminating     	0      	3s
simple-pod   0/1 	Terminating     	0      	4s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
simple-pod   0/1 	Terminating     	0      	5s
```

</details>


#### Anything else we need to know?

_No response_

#### Kubernetes version

master + PR #124297

<details>

```console
$ kubectl version
# paste output here
Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.0-alpha.0.582+8ec29a96e91e40
WARNING: version difference between client (1.28) and server (1.31) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124700 "kubectl top nodes" reports "unknown" when executing multiple concurrent â€œkubectl execâ€ requests against a pod running on a Windows node

- Issue é“¾æ¥ï¼š[#124700](https://github.com/kubernetes/kubernetes/issues/124700)

### Issue å†…å®¹

#### What happened?

"kubectl top nodes" reports "unknown" when executing multiple concurrent â€œkubectl execâ€ requests against a pod

#### What did you expect to happen?

"kubectl top nodes" reports the status of nodes.

#### How can we reproduce it (as minimally and precisely as possible)?

1. create an AKS Windows node akswin22000000.
2. deploy a pod to this node with this yaml (kubectl apply -f hpc.yaml):
```
apiVersion: v1
kind: Pod
metadata:
  labels:
    pod: hpc
  name: hpc
spec:
  securityContext:
    windowsOptions:
      hostProcess: true
      runAsUserName: "NT AUTHORITY\\SYSTEM"
  hostNetwork: true
  containers:
    - name: test
      image: mcr.microsoft.com/windows/servercore:ltsc2022
      imagePullPolicy: IfNotPresent
      command:
        - powershell.exe
        - -Command
        - "Start-Sleep 2147483"
  nodeSelector:
    kubernetes.io/hostname: "akswin22000000"
```
3. run the following script (test.py) and wait for several minutes.
```
import subprocess
import threading

def run_command(pod_name, command):
  subprocess.run(["kubectl", "exec", pod_name, "--", "powershell", "-command", command])

threads = []
for i in range(0, 50):
  thread = threading.Thread(target=run_command, args=("hpc", "& { Start-Sleep 0 }"))
  thread.start()
  threads.append(thread)

for thread in threads:
  thread.join()
```
4. run ```kubectl top nodes``` and get the following result:
```
NAME                                CPU(cores)   CPU%        MEMORY(bytes)   MEMORY% 
aks-nodepool1-66451635-vmss000000   170m         8%          1877Mi          41%  
akswin22000000                      <unknown>    <unknown>   <unknown>       <unknown> 
```

#### Anything else we need to know?

Restarting node can recover it. 

But we still have the following questions:
1. Why is â€œkubectl execâ€ causing problems? This looks like a bug: if the API is not designed to handle some load, we expect it to return an error, not hang.
2. Why is the node not recovering its metrics, even after the â€œhangingâ€ pod is restarted?

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: version.Info{Major:"1", Minor:"27+", GitVersion:"v1.27.1-eks-2f008fe", GitCommit:"abfec7d7e55d56346a5259c9379dea9f56ba2926", GitTreeState:"clean", BuildDate:"2023-04-14T20:43:13Z", GoVersion:"go1.20.3", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"28", GitVersion:"v1.28.3", GitCommit:"5214c5ddb5785ed9d8e47e79e67181e205555067", GitTreeState:"clean", BuildDate:"2024-04-12T23:22:53Z", GoVersion:"go1.20.10", Compiler:"gc", Platform:"linux/amd64"}
</details>


#### Cloud provider

<details>
AKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124699 Topology Aware Routing not working

- Issue é“¾æ¥ï¼š[#124699](https://github.com/kubernetes/kubernetes/issues/124699)

### Issue å†…å®¹

#### What happened?

Despite enabling topology-aware routing, I'm still seeing traffic being routed from one AZ to another. This is not the expected behavior, as it should prioritize in-zone traffic.

Current setup: 

3 AZ eu-west
3 pods in each AZ 
3 endpoints per each AZ 



#### What did you expect to happen?

When i enable topology aware routing I expect that traffic will stay in AZ where traffic was generated to maximize cost savings.

#### How can we reproduce it (as minimally and precisely as possible)?

1. create nginx deployment with 9 replicas with :
spec:
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: DoNotSchedule
          labelSelector:
            matchLabels:
              app: nginx
2. create Service 
apiVersion: v1
kind: Service
metadata:
  annotations:
    service.kubernetes.io/topology-aware-hints: Auto
  name: nginx
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
  type: ClusterIP
3. check for endpointslice if hints were generated
4. Label every node with : topology.kubernetes.io/zone

#### Anything else we need to know?

kubectl get events  --field-selector involvedObject.kind=Service,involvedObject.name=nginx
16m         Normal    TopologyAwareHintsEnabled    service/nginx   Topology Aware Hints has been enabled, addressType: IPv4



Kube-Proxy logs:

I0505 16:46:38.759957      10 topology.go:171] "Skipping topology aware endpoint filtering since one or more endpoints is missing a zone hint" 
I0505 16:46:38.105331      10 topology.go:171] "Skipping topology aware endpoint filtering since one or more endpoints is missing a zone hint"    
I0505 16:49:27.896972      10 topology.go:181] "Skipping topology aware endpoint filtering since no hints were provided for zone" zone="eu-west-1c" 
I0505 16:49:28.432488      10 topology.go:181] "Skipping topology aware endpoint filtering since no hints were provided for zone" zone="eu-west-1c" 

Hints were assigned:
addressType: IPv4
apiVersion: discovery.k8s.io/v1
endpoints:
- addresses:
  - 100.96.11.100
  conditions:
    ready: true
    serving: true
    terminating: false
  hints:
    forZones:
    - name: eu-west-1c
  nodeName: i-06c703e12f8392a11
  targetRef:
    kind: Pod
    name: nginx-84bdbbf4bb-jdnd9
    namespace: default
    uid: a142d4d9-e078-44e9-9ce1-3956faa1e5ce
  zone: eu-west-1c
- addresses:
  - 100.96.11.155
  conditions:
    ready: true
    serving: true
    terminating: false
  hints:
    forZones:
    - name: eu-west-1c
  nodeName: i-06c703e12f8392a11
  targetRef:
    kind: Pod
    name: nginx-84bdbbf4bb-cr7f2
    namespace: default
    uid: a2bd185c-86d0-47a8-a5ab-45ff4e32a85d
  zone: eu-west-1c
- addresses:
  - 100.96.9.173
  conditions:
    ready: true
    serving: true
    terminating: false
  hints:
    forZones:
    - name: eu-west-1b
  nodeName: i-088e764bd881db13b
  targetRef:
    kind: Pod
    name: nginx-84bdbbf4bb-znbbm
    namespace: default
    uid: 204eaf01-79bb-4993-a37c-b7bb90fb2094
  zone: eu-west-1b
- addresses:
  - 100.96.9.26
  conditions:
    ready: true
    serving: true
    terminating: false
  hints:
    forZones:
    - name: eu-west-1b
  nodeName: i-088e764bd881db13b
  targetRef:
    kind: Pod
    name: nginx-84bdbbf4bb-lfxqp
    namespace: default
    uid: 47909cec-d536-4bee-ac95-5b021a29041b
  zone: eu-west-1b
- addresses:
  - 100.96.10.164
  conditions:
    ready: true
    serving: true
    terminating: false
  hints:
    forZones:
    - name: eu-west-1a
  nodeName: i-05ab65b1b0ba1ab03
  targetRef:
    kind: Pod
    name: nginx-84bdbbf4bb-mjvd5
    namespace: default
    uid: 777dceea-a87f-483d-ac7f-d30cd7812bb1
  zone: eu-west-1a
- addresses:
  - 100.96.10.225
  conditions:
    ready: true
    serving: true
    terminating: false
  hints:
    forZones:
    - name: eu-west-1a
  nodeName: i-05ab65b1b0ba1ab03
  targetRef:
    kind: Pod
    name: nginx-84bdbbf4bb-ltj2v
    namespace: default
    uid: 2a4d680c-8822-4a2c-b3b8-a56ceaff96e7
  zone: eu-west-1a
- addresses:
  - 100.96.11.62
  conditions:
    ready: true
    serving: true
    terminating: false
  hints:
    forZones:
    - name: eu-west-1c
  nodeName: i-06c703e12f8392a11
  targetRef:
    kind: Pod
    name: nginx-84bdbbf4bb-xwqlm
    namespace: default
    uid: b4fb49ed-630b-4f7c-8ea0-22cc11ab2c9d
  zone: eu-west-1c
- addresses:
  - 100.96.9.138
  conditions:
    ready: true
    serving: true
    terminating: false
  hints:
    forZones:
    - name: eu-west-1b
  nodeName: i-088e764bd881db13b
  targetRef:
    kind: Pod
    name: nginx-84bdbbf4bb-p92c5
    namespace: default
    uid: 48c29d22-98f4-4316-bc96-958d608a8ffc
  zone: eu-west-1b
- addresses:
  - 100.96.12.233
  conditions:
    ready: true
    serving: true
    terminating: false
  hints:
    forZones:
    - name: eu-west-1a
  nodeName: i-0272547448c4bb6b7
  targetRef:
    kind: Pod
    name: nginx-84bdbbf4bb-sqb79
    namespace: default
    uid: 3f206f30-184c-4c83-986f-29b7b4ad5f77
  zone: eu-west-1a
kind: EndpointSlice

#### Kubernetes version

1.26.12

#### Cloud provider

AWS

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124691 When creating two replicas in a deployment with volume mount, one pod become Error or CrashLoopBackOff stage.

- Issue é“¾æ¥ï¼š[#124691](https://github.com/kubernetes/kubernetes/issues/124691)

### Issue å†…å®¹

#### What happened?

When creating a MySQL deployment with two replicas also volume mount added, after some time one pod becomes `CrashLoopBackOff` stage and another is running successfully.

<img width="446" alt="Screenshot 2024-05-04 at 14 14 22" src="https://github.com/kubernetes/kubernetes/assets/51718908/c9095968-ead0-41bb-92cb-dfa172a1caa9">

Also, when we delete the volume mount and try, both replicas are working the same. But the common volume can not be met.

#### What did you expect to happen?

Both pods should be in the same state.

#### How can we reproduce it (as minimally and precisely as possible)?

Create PV and PVC
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: mysql-pv
  labels:
    type: local
spec:
  persistentVolumeReclaimPolicy: Retain
  capacity:
    storage: 1Gi
  accessModes:
  - ReadWriteOnce
  hostPath:
    path: "/mnt/data"
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pvc
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
```
Create the mysql deployment 
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql
spec:
  replicas: 2
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - image: mysql:latest
        name: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: pwd
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: mysql-storage
          mountPath: /var/lib/mysql
      volumes:
      - name: mysql-storage
        persistentVolumeClaim:
          claimName: mysql-pvc
```
After some seconds, you can see both pods running successfully.(`k get pods`)
Then open the bash of from the first pod. `kubectl exec --stdin --tty mysql-pod-name-0 -- /bin/bash`. Then open the mysql in that bash. `mysql -u root -ppwd`. Now you can access the MySQL. Play with it and create a db for reference.

Then exit from the pod-0 and open the bash of the second pod by `kubectl exec --stdin --tty mysql-pod-name-1 -- /bin/bash`. You can access the bash of pod-1. Now open the MySQL there.  `mysql -u root -ppwd`. You will get an error like
```
bash-4.4# mysql -u root -ppwd
mysql: [Warning] Using a password on the command line interface can be insecure.
ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (2
```

In other way, try delete the pod-0(the workable pod).(`kubectl delete pod pod_name_0`). Then you can see another new pod(named pod-2) created. Now do the same procedure to access the mysql from both pods, you can observe that accessing the mysql from pod-1 will works with the already created db. and the pod-2 will expose the same error we observed in the pod-1 early.

#### Anything else we need to know?

-

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
uname -a
Darwin Sivakajans-MacBook-Pro.local 23.3.0 Darwin Kernel Version 23.3.0: Wed Dec 20 21:30:44 PST 2023; root:xnu-10002.81.5~7/RELEASE_ARM64_T6000 arm64
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124686 Kubelet should not add IPv6 entries to /etc/hosts on systems where IPv6 is disabled

- Issue é“¾æ¥ï¼š[#124686](https://github.com/kubernetes/kubernetes/issues/124686)

### Issue å†…å®¹

#### What happened?

Kubelet unconditionally adds IPv6 entries to `/etc/hosts` even when system has IPv6 totally disabled. This results in strange errors due to "happy eyeballs" fallback behavior: when clients see both an IPv4 and an IPv6 address for a host, they first try IPv4 and then fall back to IPv6, but if IPv6 is disabled in the kernel, they then get an "Address family not supported by protocol" exception instead of "Connection refused".

This happens with standard HTTP clients, etc, but here's an example with netcat:
```
# nc -v 127.0.0.1 1111
Ncat: Version 7.92 ( https://nmap.org/ncat )
Ncat: Connection refused.
```
vs
```
Ncat: Version 7.92 ( https://nmap.org/ncat )
Ncat: Connection to 127.0.0.1 failed: Connection refused.
Ncat: Trying next address...
libnsock nsock_make_socket(): Socket trouble: Address family not supported by protocol
Ncat: Address family not supported by protocol.
```

That results in clients needing to handle a different exception. However, even if IPv6 were enabled on the system, it would still be invalid to put loopback addresses in `/etc/hosts` for localhost when they are not actually configured on the loopback interface.

Code here: https://github.com/kubernetes/kubernetes/blob/v1.29.4/pkg/kubelet/kubelet_pods.go#L373-L377

#### What did you expect to happen?

Kubelet should not add localhost entries to `/etc/hosts` for addresses that do not actually exist on the pod's loopback interface.

#### How can we reproduce it (as minimally and precisely as possible)?

I don't think you need a reproducer for this one since the code is just unconditionally doing this. However, if you're looking to test this on a system with ipv6 disabled such that you get the "Address family not supported by protocol" error, or guarantee that CNI does not add an IPv6 loopback address, you can boot your kernel with `ipv6.disable=1` on the kernel command line.

#### Anything else we need to know?

_No response_

#### Kubernetes version

N/A all versions impacted


#### Cloud provider

N/A

#### OS version

N/A

#### Install tools

N/A

#### Container runtime (CRI) and version (if applicable)

N/A

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

N/A

### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124655 The Version Skew Policy does not mention aggregated apiservers

- Issue é“¾æ¥ï¼š[#124655](https://github.com/kubernetes/kubernetes/issues/124655)

### Issue å†…å®¹

#### What happened?

Someone opened #124533 saying

> I expected 1.29 libraries to still be compatible with Kubernetes 1.28 based on the kube-apiserver version skew.

I think that is not what the version skew policy (https://kubernetes.io/releases/version-skew-policy/) is trying to say. I think that the guiding principle is that you must first upgrade the servers that store objects of a given API group and resource/kind, then upgrade the clients of those servers.

In particular, the existing version skew policy document explicitly discusses several kinds of components, but does not mention aggregated apiservers. I think that these should be explicitly mentioned.

#### What did you expect to happen?

Everybody agrees on what to expect.

#### How can we reproduce it (as minimally and precisely as possible)?

Read the cited doc and Issue.

#### Anything else we need to know?

Nope

#### Kubernetes version

<details>

All of them.

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

N/A

</details>


#### Install tools

<details>
N/A
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

