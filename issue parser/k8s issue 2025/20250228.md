# Issue 安全分析报告

# ⚠️ 存在低风险的 Issues (3 个)

## Issue #130442 Service account token refresh failure in pods with pvcs in terminating state

- Issue 链接：[#130442](https://github.com/kubernetes/kubernetes/issues/130442)

### Issue 内容

#### What happened?

When a PersistentVolumeClaim (PVC) enters the Terminating state due to a deletion request, any pod mounting this PVC experiences a failure in ServiceAccount token refresh operations. This prevents the pod from receiving updated tokens once the current token expires.

#### What did you expect to happen?

Token should be refreshed because pod is running.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Set up latest kubernetes cluster (i used [kind](https://kind.sigs.k8s.io/docs/user/quick-start/))
```
$ k get nodes
NAME                 STATUS   ROLES           AGE     VERSION
kind-control-plane   Ready    control-plane   5h29m   v1.32.2
```

2. Create pod mounting pvc with SA token
```
# token-test.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: simple-token-sa
  namespace: default
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: client-data-pvc
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: token-client
spec:
  serviceAccountName: simple-token-sa
  containers:
  - name: client
    image: curlimages/curl:latest
    command:
      - sleep
      - infinite
    volumeMounts:
    - name: token-volume
      mountPath: /var/run/secrets/tokens
    - name: data-volume
      mountPath: /data
  volumes:
  - name: token-volume
    projected:
      sources:
      - serviceAccountToken:
          path: custom-token
          expirationSeconds: 600
          audience: my-custom-audience
  - name: data-volume
    persistentVolumeClaim:
      claimName: client-data-pvc
```
```
$ k apply -f  token-test.yaml
```

3. Request pvc deletion and the pvc enters terminating state because a pod is still mounting it.
```
$ k delete pvc client-data-pvc 
persistentvolumeclaim "client-data-pvc" deleted
```

4. Check if the token refreshes correctly before reaching its expiration time.
```
# monitor-token.sh
#!/bin/bash

# Colors for output formatting
GREEN='\033[0;32m'
YELLOW='\033[0;33m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

echo -e "${CYAN}Token Renewal Monitoring Script${NC}"
echo -e "${CYAN}------------------------------${NC}"
echo -e "Checking for token updates every 30 seconds...\n"

# Initialize variables to track changes
previous_exp=""
first_run=true

while true; do
  # Get the current timestamp for logging
  current_time=$(date "+%Y-%m-%d %H:%M:%S")
  
  # Execute the command to get the token payload
  token_payload=$(kubectl exec -it token-client -- sh -c 'TOKEN=$(cat /var/run/secrets/tokens/custom-token); echo $TOKEN | cut -d. -f2 | base64 -d' 2>/dev/null)
  
  # Check if command was successful
  if [ $? -ne 0 ]; then
    echo -e "${YELLOW}[${current_time}] Error: Could not retrieve token. Check if the pod is running.${NC}"
    sleep 30
    continue
  fi
  
  # Extract expiration time
  exp_value=$(echo $token_payload | grep -o '"exp":[0-9]*' | cut -d':' -f2)
  
  # Format expiration time as human-readable
  exp_human=$(date -d "@$exp_value" "+%Y-%m-%d %H:%M:%S")
  
  # Calculate time until expiration
  now_seconds=$(date +%s)
  seconds_remaining=$((exp_value - now_seconds))
  minutes_remaining=$((seconds_remaining / 60))
  seconds_remainder=$((seconds_remaining % 60))
  
  # If first run, just display the token info
  if $first_run; then
    echo -e "${CYAN}[${current_time}] Initial token detected${NC}"
    echo -e "Token audience: $(echo $token_payload | grep -o '"aud":\[[^]]*\]' | sed 's/"aud":\["\(.*\)"\]/\1/')"
    echo -e "Service account: $(echo $token_payload | grep -o '"sub":"[^"]*"' | sed 's/"sub":"\(.*\)"/\1/')"
    echo -e "Expires at: ${exp_human} (in ${minutes_remaining}m ${seconds_remainder}s)"
    echo -e "JWT ID: $(echo $token_payload | grep -o '"jti":"[^"]*"' | sed 's/"jti":"\(.*\)"/\1/')"
    echo -e "${CYAN}------------------------------${NC}\n"
    previous_exp=$exp_value
    first_run=false
  else
    # Check if token has been renewed (expiration time changed)
    if [ "$previous_exp" != "$exp_value" ]; then
      echo -e "${GREEN}[${current_time}] Token renewed!${NC}"
      echo -e "New expiration: ${exp_human} (in ${minutes_remaining}m ${seconds_remainder}s)"
      echo -e "New JWT ID: $(echo $token_payload | grep -o '"jti":"[^"]*"' | sed 's/"jti":"\(.*\)"/\1/')"
      echo -e "${CYAN}------------------------------${NC}\n"
      previous_exp=$exp_value
    else
      echo -e "[${current_time}] Token unchanged. Expires in ${minutes_remaining}m ${seconds_remainder}s"
    fi
  fi
  
  # Wait for 30 seconds before checking again
  sleep 30
done
```

```
$ ./monitor-token.sh
Token Renewal Monitoring Script
------------------------------
Checking for token updates every 30 seconds...

[2025-02-26 19:12:46] Initial token detected
Token audience: my-custom-audience
Service account: system:serviceaccount:default:simple-token-sa
Expires at: 2025-02-26 19:15:13 (in 2m 27s)
JWT ID: cd509fb4-f1ab-4275-91c9-1f6a72e1860a
------------------------------

[2025-02-26 19:13:16] Token unchanged. Expires in 1m 57s
[2025-02-26 19:13:46] Token unchanged. Expires in 1m 27s
[2025-02-26 19:14:16] Token unchanged. Expires in 0m 57s
[2025-02-26 19:14:46] Token unchanged. Expires in 0m 27s
[2025-02-26 19:15:16] Token unchanged. Expires in 0m -4s
[2025-02-26 19:15:47] Token unchanged. Expires in 0m -34s
[2025-02-26 19:16:17] Token unchanged. Expires in -1m -4s
[2025-02-26 19:16:47] Token unchanged. Expires in -1m -34s
[2025-02-26 19:17:17] Token unchanged. Expires in -2m -4s
[2025-02-26 19:17:47] Token unchanged. Expires in -2m -34s
[2025-02-26 19:18:17] Token unchanged. Expires in -3m -4s
[2025-02-26 19:18:47] Token unchanged. Expires in -3m -35s
[2025-02-26 19:19:18] Token unchanged. Expires in -4m -5s
```


#### Anything else we need to know?

The bug presents a critical failure scenario for environments running service meshes such as Linkerd and Istio.

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>

```
$ k version
Client Version: v1.32.2
Kustomize Version: v5.5.0
Server Version: v1.32.2
```

#### Cloud provider

<details>

</details>

kind

#### OS version

<details>

```console
$ cat /etc/os-release
NAME="Ubuntu"
VERSION="20.04.3 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.3 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal

$ uname -a
Linux k8stest 5.4.0-99-generic #112-Ubuntu SMP Thu Feb 3 13:50:55 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux
```

</details>




#### Install tools

<details>

</details>

kind

#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
根据Issue的描述，PVC处于Terminating状态时导致挂载该PVC的Pod无法刷新ServiceAccount Token，进而可能引发服务不可用或功能受限的情形。若从安全角度评估，这可被视为一种拒绝服务（DoS）问题。不过，需要注意的是要触发该问题，攻击者或操作者需要具备删除PVC等一定的权限（相对于仅具有只读权限的低权限账号来说要求更高）。因此，根据“当漏洞利用需要攻击者具备创建、修改等非只读权限时，则不应判断为高风险”的规则，该问题的风险等级应当被降低。  
综合评估后，此问题可归类为“安全问题-拒绝服务（DoS）”，且攻击者需要较高权限才能利用，CVSS 3.1 可能评分在中低范围，故不达到“high”及以上的标准。

---


## Issue #130331 removeOldestN doesn't make sure container is deleted completly

- Issue 链接：[#130331](https://github.com/kubernetes/kubernetes/issues/130331)

### Issue 内容

#### What happened?

```
func (cgc *containerGC) removeOldestN(ctx context.Context, containers []containerGCInfo, toRemove int) []containerGCInfo {
	// Remove from oldest to newest (last to first).
	numToKeep := len(containers) - toRemove
	if numToKeep > 0 {
		sort.Sort(byCreated(containers))
	}
	for i := len(containers) - 1; i >= numToKeep; i-- {
		if containers[i].unknown {
			// Containers in known state could be running, we should try
			// to stop it before removal.
			id := kubecontainer.ContainerID{
				Type: cgc.manager.runtimeName,
				ID:   containers[i].id,
			}
			message := "Container is in unknown state, try killing it before removal"
			if err := cgc.manager.killContainer(ctx, nil, id, containers[i].name, message, reasonUnknown, nil, nil); err != nil {
				klog.ErrorS(err, "Failed to stop container", "containerID", containers[i].id)
				continue
			}
		}
		if err := cgc.manager.removeContainer(ctx, containers[i].id); err != nil {
			klog.ErrorS(err, "Failed to remove container", "containerID", containers[i].id)
		}
	}

	// Assume we removed the containers so that we're not too aggressive.
	return containers[:numToKeep]
}
```
many logs like this 
```
E0210 19:52:56.897622   38171 remote_runtime.go:347] "StartContainer from runtime service failed" err="rpc error: code = DeadlineExceeded desc = context deadline exceeded" containerID="29d1d59735a2eb389ca29f20bc85d002f90dd509ccd0c1c16c5506e07184cfaa"
I0210 19:52:57.303537   38171 scope.go:143] "RemoveContainer" containerID="29d1d59735a2eb389ca29f20bc85d002f90dd509ccd0c1c16c5506e07184cfaa"
I0210 21:37:42.650459   36915 scope.go:143] "RemoveContainer" containerID="29d1d59735a2eb389ca29f20bc85d002f90dd509ccd0c1c16c5506e07184cfaa"
E0210 21:37:42.650790   36915 remote_runtime.go:439] "ContainerStatus from runtime service failed" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: connection refused\"" containerID="29d1d59735a2eb389ca29f20bc85d002f90dd509ccd0c1c16c5506e07184cfaa"
E0210 21:37:42.650804   36915 kuberuntime_gc.go:151] "Failed to remove container" err="failed to get container status \"29d1d59735a2eb389ca29f20bc85d002f90dd509ccd0c1c16c5506e07184cfaa\": rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: connection refused\"" containerID="29d1d59735a2eb389ca29f20bc85d002f90dd509ccd0c1c16c5506e07184cfaa
```
will cause containerd panic.  program exceeds 10000-thread limit
https://github.com/containerd/containerd/pull/11389
#### What did you expect to happen?

keep delete until cotainerd is deleted.

#### How can we reproduce it (as minimally and precisely as possible)?

difficult to   reproduce.

#### Anything else we need to know?

system is busy.

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
从Issue的描述与代码片段来看，问题主要集中在容器垃圾回收（removeOldestN函数）时，如果容器处于未知状态或无法正常停止与删除，代码会不断尝试删除容器，导致 containerd 可能出现线程暴增，最终可能引发进程崩溃。这属于容器垃圾回收过程中的异常情况，而不仅仅是普通的功能缺陷。

根据给出的风险判断标准，此问题可以被视为一次可能导致拒绝服务（DoS）的安全问题，但需要具备创建、删除（即至少可管理容器或Pod）等非只读权限才能触发。按照第5条规则：“当漏洞利用需要攻击者具备创建、修改等非只读权限时，则不应判断为高风险”，因此可判定为低风险的DoS问题。

**复现过程：**

```python
import os
import time
import signal
import threading

from kubernetes import client, config, watch
from kubernetes.client import V1Pod, V1ObjectMeta, V1PodSpec, V1Container

def timeout_handler(signum, frame):
    raise Exception("执行超时，脚本结束。")

def create_pod_object(name, image="busybox", command=["sleep", "3600"]):
    """
    创建一个简单的 Pod 对象，用于测试。
    """
    return V1Pod(
        metadata=V1ObjectMeta(name=name),
        spec=V1PodSpec(
            containers=[
                V1Container(
                    name="test-container",
                    image=image,
                    command=command
                )
            ],
            restart_policy="Never"
        )
    )

def force_kill_pod(core_v1_api, pod_name, namespace="default"):
    """
    模拟对正在运行/创建中的容器进行强制删除，可能导致容器处于不确定状态。
    """
    try:
        core_v1_api.delete_namespaced_pod(
            name=pod_name,
            namespace=namespace,
            body=client.V1DeleteOptions(grace_period_seconds=0)
        )
        print(f"[INFO] 强制删除请求已发送: {pod_name}")
    except Exception as e:
        print(f"[ERROR] 强制删除请求失败: {pod_name}, 原因: {str(e)}")

def main():
    # 设置超时信号，防止脚本长时间运行
    signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(120)  # 2分钟后触发

    # 1. 加载 kubeconfig，假设在默认位置
    config.load_kube_config()

    # 2. 初始化 Kubernetes API 客户端
    core_v1_api = client.CoreV1Api()

    test_namespace = "default"

    # 3. 尝试创建多个Pod，部分使用不存在或错误镜像，制造“未知”状态
    pod_names = []
    for i in range(5):
        pod_name = f"test-gc-pod-{i}"
        # 让一部分Pod使用不存在的镜像，模拟异常状态
        if i % 2 == 0:
            image_name = "nonexist:latest"  # 不存在的镜像
        else:
            image_name = "busybox"

        pod_obj = create_pod_object(name=pod_name, image=image_name)
        try:
            core_v1_api.create_namespaced_pod(namespace=test_namespace, body=pod_obj)
            print(f"[INFO] 创建Pod: {pod_name}")
            pod_names.append(pod_name)
        except Exception as e:
            print(f"[ERROR] 创建Pod失败: {pod_name}, 原因: {str(e)}")

    # 4. 等待一小段时间，让容器处于Pull镜像或创建状态
    time.sleep(10)

    # 5. 对上述Pod进行强制删除操作，可能导致容器在Runtime中处于不一致或未知状态
    for name in pod_names:
        force_kill_pod(core_v1_api, name)

    # 6. 等待删除操作生效
    time.sleep(10)

    # 7. 此时，如果Kubernetes底层垃圾回收存在问题，可能频繁尝试删除这些无法正常删除的容器，
    #    在极端情况下导致 containerd 出现超量线程、资源耗尽，从而触发DoS风险。
    #
    # 由于具体场景依赖环境繁忙程度、Runtime实现逻辑等，脚本中只展示可能的触发过程。
    # 如要在真实环境中观测，需要配合更多负载或重复操作来放大问题。

    print("[INFO] 测试完成，开始清理环境。")

    # 8. 再次确保Pod已删除
    for name in pod_names:
        try:
            core_v1_api.delete_namespaced_pod(name=name, namespace=test_namespace)
        except:
            pass

    print("[INFO] 清理完成，脚本结束。")
```


**解释说明：**

1. 此脚本通过创建多个可能无法正常启动或处于未知状态的Pod，然后强制删除它们，模拟出容器在Runtime（containerd）中滞留、不一致的场景。  
2. 如果底层垃圾回收逻辑存在问题，重复地删除这些容器可能导致Runtime线程数不断增长，进而出现崩溃（DoS）的风险。  
3. 脚本中使用了超时机制（signal.alarm(120)）来避免脚本长时间或死循环运行。  
4. 脚本仅为本地测试与研究所用，不应在生产环境中滥用。如需进一步验证，可在受控环境下配合更多Pod及负载进行测试。

---


## Issue #130170 Possible memory leak in kube-proxy 1.32: memory usage continuously grows

- Issue 链接：[#130170](https://github.com/kubernetes/kubernetes/issues/130170)

### Issue 内容

#### What happened?

After upgrading from version 1.31 to 1.32
In our Kubernetes cluster we have a continuous growth of memory consumption by kube-proxy process. The logs show multiple repetitions:
```
I0214 09:19:46.453889       1 proxier.go:1547] "Reloading service iptables data" ipFamily="IPv4" numServices=307 numEndpoints=356 numFilterChains=6 numFilterRules=7 numNATChains=70 numNATRules=491
```
and

```
 I0213 17:38:57.097857       1 filter.go:99] "Deleting conntrack entry" flow=<
	udp	17 src=X.X.X.X dst=X.X.X.X sport=55146 dport=8472 packets=0 bytes=0	src=X.X.X.X dst=X.X.X.X sport=8472 dport=55146 packets=0 bytes=0 mark=0x0 start=1970-01-01 00:00:00 +0000 UTC stop=1970-01-01 00:00:00 +0000 UTC timeout=0(sec)
```

#### What did you expect to happen?

We expect that memory consumed by kube-proxy should not grow indefinitely. After iptables synchronization or deleting old conntrack records memory should be freed.

#### How can we reproduce it (as minimally and precisely as possible)?

	1. There is a Kubernetes cluster version 1.32.2 with 3 masters and 5 nodes.
	2. kube-proxy works in iptables mode
	3. Running the cluster with ~ 170 services and ~ 250 pods
	4. When redeploying/scaling pods kube-proxy memory starts growing on nodes

#### Anything else we need to know?

```
net.ipv4.tcp_syncookies=1
net.ipv4.tcp_max_syn_backlog = 262144
net.ipv4.tcp_no_metrics_save=1
net.ipv4.tcp_fin_timeout=30
net.ipv4.tcp_keepalive_time = 1800
net.ipv4.tcp_keepalive_probes = 7
net.ipv4.tcp_keepalive_intvl = 30
net.ipv4.tcp_max_tw_buckets = 720000
net.ipv4.tcp_rmem = 4096 8388608 16777216
net.ipv4.tcp_wmem = 4096 4194394 16777216
net.core.somaxconn=25000
net.core.netdev_max_backlog = 8192
net.nf_conntrack_max = 524288
```

<img width="1073" alt="Image" src="https://github.com/user-attachments/assets/13c9ec6a-6e73-4c63-91de-a61cce6ffd58" />

<img width="1476" alt="Image" src="https://github.com/user-attachments/assets/28c4af3b-21b0-400d-9f29-ae24414fc100" />

#### Kubernetes version

<details>

```console
Client Version: v1.32.2
Kustomize Version: v5.5.0
Server Version: v1.32.2
```

</details>


#### Cloud provider

<details>
Bare metal
</details>


#### OS version

<details>

```console

# On Linux:
NAME="Ubuntu"
VERSION="20.04.6 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.6 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal

Linux  5.4.0-205-generic #225-Ubuntu SMP Fri Jan 10 22:23:35 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
```

</details>

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

```
crictl version v1.32.0

containerd containerd.io 1.7.25 bcc810d6b9066471b0b6fa75f557a15a1cbf31bb
```

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

```
Flannel 

docker.io/flannel/flannel:v0.26.3

docker.io/flannel/flannel-cni-plugin:v1.6.0-flannel1
```

</details>


### 分析结果

**风险定级：**  
低风险

**判断依据：**  
根据Issue内容来看，kube-proxy在执行iptables同步、删除旧conntrack记录等操作后，发生了内存持续增长的问题。该问题本质上属于功能或稳定性Bug，可能在大规模服务/Pod频繁变更时导致内存资源被过度占用，进而影响节点正常运行，属于可导致拒绝服务（DoS）的风险场景。

在CVSS 3.1评分标准下，此类内存泄露如需攻击者具备可创建或修改（如可频繁伸缩Pod、重部署服务）等权限才能触发，对应“Privileges Required”不为None，需要拥有一定的操作权限才能实施大规模重部署；因此综合考量其对可用性的影响，通常处于中等或更低范畴。根据本Issue描述，本质是kube-proxy在正常工作场景中的内存泄露，没有表明恶意攻击者可以在无权限情形下利用该问题直接发起DoS，故判断为低风险。

**解释说明：**

该问题主要是kube-proxy在长时间运行或频繁操作后，因内部进程内存未能得到及时释放导致的资源占用过高。按照本项目的风险判定标准，若DoS漏洞需要较高或非只读权限才能触发，则不视为高风险，因此此处判定为低风险，故无需提供复现脚本。  
如需确认问题，可在测试环境中模拟频繁扩缩容操作监控kube-proxy内存占用情况，但该过程不涉及明显的安全攻击面，更多与kube-proxy的实现缺陷相关。若需彻底修复，应在项目中追踪相关内存释放逻辑或排查特定依赖库问题。

---


# ✅ 不涉及安全风险的 Issues (33 个)

## Issue #130490 GracefulNodeShutdown produces multiple “ContainerStatusUnknown” pods if the node is shut down with pods having not-ready tolerations

- Issue 链接：[#130490](https://github.com/kubernetes/kubernetes/issues/130490)

### Issue 内容

#### What happened?

When having pods that have the following toleration: 
```
      tolerations:
      - key: node.kubernetes.io/not-ready
        operator: Exists
```

Shutting down the pod's node configured with GracefulNodeShutdown will leave a number of replicas of the pods in a `ContainerStatusUnknown` status after restarting the node where the pod originally was.

```
# kubectl get no,po -o wide
NAME                             STATUS   ROLES           AGE   VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE          KERNEL-VERSION                CONTAINER-RUNTIME
node/localhost.localdomain       Ready    control-plane   13h   v1.32.2   192.168.100.110   <none>        CentOS Stream 8   5.4.290-1.el8.elrepo.x86_64   containerd://1.6.32
node/localmachine2.localdomain   Ready    <none>          13h   v1.32.2   192.168.100.111   <none>        CentOS Stream 8   5.4.290-1.el8.elrepo.x86_64   containerd://1.6.32

NAME                           READY   STATUS                   RESTARTS   AGE     IP            NODE                        NOMINATED NODE   READINESS GATES
pod/testpod-558577cfcb-26ztn   0/1     ContainerStatusUnknown   0          6m6s    <none>        localmachine2.localdomain   <none>           <none>
pod/testpod-558577cfcb-7wqjj   0/1     ContainerStatusUnknown   0          6m5s    <none>        localmachine2.localdomain   <none>           <none>
pod/testpod-558577cfcb-lj8j9   0/1     ContainerStatusUnknown   0          6m5s    <none>        localmachine2.localdomain   <none>           <none>
pod/testpod-558577cfcb-mjn6n   0/1     ContainerStatusUnknown   0          6m6s    <none>        localmachine2.localdomain   <none>           <none>
pod/testpod-558577cfcb-mtck6   0/1     ContainerStatusUnknown   0          6m6s    <none>        localmachine2.localdomain   <none>           <none>
pod/testpod-558577cfcb-qnltn   0/1     Error                    0          6m22s   <none>        localmachine2.localdomain   <none>           <none>
pod/testpod-558577cfcb-qpnvb   0/1     ContainerStatusUnknown   0          6m7s    <none>        localmachine2.localdomain   <none>           <none>
pod/testpod-558577cfcb-tflc8   1/1     Running                  0          6m4s    10.244.1.35   localmachine2.localdomain   <none>           <none>

```

According to `kubectl get po -w`:
```
NAME                       READY   STATUS    RESTARTS   AGE
testpod-558577cfcb-qnltn   1/1     Running   0          6s
<-- Node is shutdown -->
testpod-558577cfcb-qnltn   0/1     Error     0          15s
testpod-558577cfcb-qpnvb   0/1     Pending   0          0s
testpod-558577cfcb-qpnvb   0/1     Pending   0          1s
testpod-558577cfcb-qpnvb   0/1     NodeShutdown   0          1s
testpod-558577cfcb-mtck6   0/1     Pending        0          0s
testpod-558577cfcb-mtck6   0/1     Pending        0          0s
testpod-558577cfcb-mtck6   0/1     NodeShutdown   0          0s
testpod-558577cfcb-mjn6n   0/1     Pending        0          0s
testpod-558577cfcb-mjn6n   0/1     Pending        0          0s
testpod-558577cfcb-mjn6n   0/1     NodeShutdown   0          0s
testpod-558577cfcb-26ztn   0/1     Pending        0          0s
testpod-558577cfcb-26ztn   0/1     Pending        0          0s
testpod-558577cfcb-26ztn   0/1     NodeShutdown   0          1s
testpod-558577cfcb-lj8j9   0/1     Pending        0          0s
testpod-558577cfcb-lj8j9   0/1     Pending        0          0s
testpod-558577cfcb-lj8j9   0/1     NodeShutdown   0          0s
testpod-558577cfcb-7wqjj   0/1     Pending        0          0s
testpod-558577cfcb-7wqjj   0/1     Pending        0          0s
testpod-558577cfcb-7wqjj   0/1     NodeShutdown   0          1s
testpod-558577cfcb-tflc8   0/1     Pending        0          0s
testpod-558577cfcb-tflc8   0/1     Pending        0          0s
<-- Node is booted up -->
testpod-558577cfcb-7wqjj   0/1     ContainerCreating   0          112s
testpod-558577cfcb-lj8j9   0/1     ContainerCreating   0          112s
testpod-558577cfcb-tflc8   0/1     ContainerCreating   0          111s
testpod-558577cfcb-qnltn   0/1     Error               0          2m9s
testpod-558577cfcb-mjn6n   0/1     ContainerCreating   0          113s
testpod-558577cfcb-qpnvb   0/1     ContainerCreating   0          115s
testpod-558577cfcb-mtck6   0/1     ContainerCreating   0          114s
testpod-558577cfcb-26ztn   0/1     ContainerCreating   0          114s
testpod-558577cfcb-qpnvb   0/1     ContainerStatusUnknown   0          116s
testpod-558577cfcb-mtck6   0/1     ContainerStatusUnknown   0          115s
testpod-558577cfcb-26ztn   0/1     ContainerStatusUnknown   0          115s
testpod-558577cfcb-mjn6n   0/1     ContainerStatusUnknown   0          115s
testpod-558577cfcb-7wqjj   0/1     ContainerStatusUnknown   0          115s
testpod-558577cfcb-lj8j9   0/1     ContainerStatusUnknown   0          115s
testpod-558577cfcb-tflc8   1/1     Running                  0          2m12s
```

#### What did you expect to happen?

I was expecting that there will be no pods with `ContainerStatusUnknown`.

#### How can we reproduce it (as minimally and precisely as possible)?

Minimal Env:
1 Cluster with 1 control plane and 1 worker node

Reproduction:
1. Enable GracefulNodeShutdown in your node
2. Create the following Deployment:
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: testpod
  namespace: default
  labels:
    app: testpod
spec:
  replicas: 1
  selector:
    matchLabels:
      app: testpod
  template:
    metadata:
      labels:
        app: testpod
    spec:
      containers:
      - name: busybox
        image:  busybox
        command:  ["/bin/sh", "-c"]
        args:
        - |
          trap "trap - TERM; kill 0; exit" TERM; sleep 1000000 & wait
      terminationGracePeriodSeconds: 30
      tolerations:
      - key: node.kubernetes.io/not-ready
        operator: Exists
```
3. Create the deployment
```
# kubectl create -f testpod.yaml
deployment.apps/testpod created

# kubectl get no,po -o wide
NAME                             STATUS   ROLES           AGE   VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE          KERNEL-VERSION                CONTAINER-RUNTIME
node/localhost.localdomain       Ready    control-plane   13h   v1.32.2   192.168.100.110   <none>        CentOS Stream 8   5.4.290-1.el8.elrepo.x86_64   containerd://1.6.32
node/localmachine2.localdomain   Ready    <none>          13h   v1.32.2   192.168.100.111   <none>        CentOS Stream 8   5.4.290-1.el8.elrepo.x86_64   containerd://1.6.32

NAME                          READY   STATUS    RESTARTS   AGE   IP            NODE                        NOMINATED NODE   READINESS GATES
pod/testpod-774dc69db-8ttnq   1/1     Running   0          32s   10.244.1.33   localmachine2.localdomain   <none>           <none>
```
4.  Reboot the worker node (in this example, localmachine2.localdomain)
```
[user@localmachine2 ~] sudo systemctl reboot
```
5.  Wait for a few minutes until the node shuts down and starts up again.
6.  Check the pod list
```
# kubectl get no,po -o wide
NAME                             STATUS   ROLES           AGE   VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE          KERNEL-VERSION                CONTAINER-RUNTIME
node/localhost.localdomain       Ready    control-plane   13h   v1.32.2   192.168.100.110   <none>        CentOS Stream 8   5.4.290-1.el8.elrepo.x86_64   containerd://1.6.32
node/localmachine2.localdomain   Ready    <none>          13h   v1.32.2   192.168.100.111   <none>        CentOS Stream 8   5.4.290-1.el8.elrepo.x86_64   containerd://1.6.32

NAME                           READY   STATUS                   RESTARTS   AGE     IP            NODE                        NOMINATED NODE   READINESS GATES
pod/testpod-558577cfcb-26ztn   0/1     ContainerStatusUnknown   0          6m6s    <none>        localmachine2.localdomain   <none>           <none>
pod/testpod-558577cfcb-7wqjj   0/1     ContainerStatusUnknown   0          6m5s    <none>        localmachine2.localdomain   <none>           <none>
pod/testpod-558577cfcb-lj8j9   0/1     ContainerStatusUnknown   0          6m5s    <none>        localmachine2.localdomain   <none>           <none>
pod/testpod-558577cfcb-mjn6n   0/1     ContainerStatusUnknown   0          6m6s    <none>        localmachine2.localdomain   <none>           <none>
pod/testpod-558577cfcb-mtck6   0/1     ContainerStatusUnknown   0          6m6s    <none>        localmachine2.localdomain   <none>           <none>
pod/testpod-558577cfcb-qnltn   0/1     Error                    0          6m22s   <none>        localmachine2.localdomain   <none>           <none>
pod/testpod-558577cfcb-qpnvb   0/1     ContainerStatusUnknown   0          6m7s    <none>        localmachine2.localdomain   <none>           <none>
pod/testpod-558577cfcb-tflc8   1/1     Running                  0          6m4s    10.244.1.35   localmachine2.localdomain   <none>           <none>

```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
# kubectl version
Client Version: v1.32.2
Kustomize Version: v5.5.0
Server Version: v1.32.2
```

</details>



#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="CentOS Stream"
VERSION="8"
ID="centos"
ID_LIKE="rhel fedora"
VERSION_ID="8"
PLATFORM_ID="platform:el8"
PRETTY_NAME="CentOS Stream 8"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:centos:centos:8"
HOME_URL="https://centos.org/"
BUG_REPORT_URL="https://bugzilla.redhat.com/"
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux 8"
REDHAT_SUPPORT_PRODUCT_VERSION="CentOS Stream"
$ uname -a
Linux localhost.localdomain 5.4.290-1.el8.elrepo.x86_64 #1 SMP Sat Feb 1 18:53:57 EST 2025 x86_64 x86_64 x86_64 GNU/Linux

```

</details>



#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 描述的是在使用 GracefulNodeShutdown 功能时，当节点上运行的 Pod 带有 "node.kubernetes.io/not-ready" 容忍度并且节点被重启后，出现多个处于 ContainerStatusUnknown 状态的 Pod 的问题。该问题本质上是 Kubernetes 在处理节点优雅关机时对存活 Pod 状态管理不完善，导致在节点重启后出现 Pod 状态没有正确更新的现象。

从安全角度来看，该问题并未涉及到敏感信息泄露、越权、任意命令执行、容器逃逸等与安全攻击面相关的情形，也不涉及攻击者如何利用该问题进行进一步的攻击或破坏。该问题更类似一个功能性或可用性问题，未产生可被攻击者利用进行破坏或侵入的风险。

**解释说明：**

这是一个功能性/可用性问题，不属于安全风险范畴，故风险评级为“不涉及”。其主要影响是节点重启后可能产生 Pod 状态异步更新，带来运维或管理上的不便，但并不会带来可利用的安全漏洞。该问题与权限提升、任意代码执行或敏感凭据泄露等攻击场景无关，因此不构成安全风险。故无需提供额外的安全复现脚本。

---


## Issue #130460 CRI and networking behavior

- Issue 链接：[#130460](https://github.com/kubernetes/kubernetes/issues/130460)

### Issue 内容

#### What happened?

This is a follow up to [this Slack thread](https://kubernetes.slack.com/archives/C09QYUH5W/p1740368655309169). I can't say that I understand why, but I'm told this behavior is unexpected: the container runtime sometimes gets a `RunPodSandboxRequest` where the host port is zero.

#### What did you expect to happen?

I believe @aojea wants to "check if we are sending that info on purpose through the CRI API or is just an oversight".

I think more importantly, the CRI is a bit under-documented here. The [documentation of `host_port`](https://github.com/kubernetes/cri-api/blob/v0.32.2/pkg/apis/runtime/v1/api.proto#L204) is just `Port number on the host. Default: 0 (not specified)`, which reveals basically nothing about how an implementation might handle a zero value for that field.

#### How can we reproduce it (as minimally and precisely as possible)?

This can be reproduced with minikube, Istio, and the Gateway API. It can probably be reproduced in other ways, too, but this is the most straightforward I came up with:

```bash
cd minikube  # https://github.com/kubernetes/minikube
git checkout v1.35.0
# Apply this 1-line patch so we can see `PodConfig` in the logs:
git apply patch
make local-kicbase-debug && make

# The problem can be illustrated most easily by booting up Istio and using the Gateway API
# (not sure why), but I was also observing host_port = 0 for my application pods.
./out/minikube start --container-runtime=containerd --memory=16384 --cpus=4
istioctl install --set profile=ambient --skip-confirmation
kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.2.0/standard-install.yaml
```

Where `patch` is this:

```diff
diff --git a/deploy/kicbase/containerd.toml b/deploy/kicbase/containerd.toml
index 873cee549..8bc7bf048 100644
--- a/deploy/kicbase/containerd.toml
+++ b/deploy/kicbase/containerd.toml
@@ -15,7 +15,7 @@ oom_score = 0
   address = ""
   uid = 0
   gid = 0
-  level = ""
+  level = "debug"

 [metrics]
   address = ""
```

Once you run the above, you can check the log by running:

```bash
docker exec -it minikube journalctl -u containerd | grep 'Sandbox config' | grep HostPort
```

You can see that a bunch of pods are run with a port mapping where `HostPort` is zero.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.32.1
Kustomize Version: v5.5.0
Server Version: v1.32.0
```

</details>


#### Cloud provider

None


#### OS version

<details>

```console
NAME="Manjaro Linux"
PRETTY_NAME="Manjaro Linux"
ID=manjaro
ID_LIKE=arch
BUILD_ID=rolling
ANSI_COLOR="32;1;24;144;200"
HOME_URL="https://manjaro.org/"
DOCUMENTATION_URL="https://wiki.manjaro.org/"
SUPPORT_URL="https://forum.manjaro.org/"
BUG_REPORT_URL="https://docs.manjaro.org/reporting-bugs/"
PRIVACY_POLICY_URL="https://manjaro.org/privacy-policy/"
LOGO=manjarolinux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

`containerd containerd.io 1.7.25 bcc810d6b9066471b0b6fa75f557a15a1cbf31bb`

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从 Issue 的描述来看，出现 hostPort=0 的原因可能是容器运行时（container runtime）接收到没有映射主机端口的 Pod 配置。这属于参数或文档层面上的问题，并未提到对此操作能够导致任意代码执行、容器逃逸、提权等安全风险，也没有说明攻击者可以利用该行为提升权限或攻击其他用户，因而并不构成安全漏洞。

根据给定判断标准，Issue 中只体现了 CRI 在主机端口传递时存在“0值”的配置行为，没有提到此配置会带来访问、权限等方面的安全威胁，也无法用于拒绝服务（DoS）或者越权访问。因此，此问题不属于安全风险范畴。

**复现过程：**

```python
# 由于该Issue不涉及安全风险，这里无需提供安全漏洞复现脚本。
# 如果想在本地模拟hostPort=0的场景，可简单演示在Kubernetes中部署一个Pod时，
# 显式地将containerPort设置，比如下面使用Python脚本调度Pod时没有设定hostPort。
# 本脚本非漏洞POC，仅用于演示如何使用kubernetes库部署一个普通Pod。

import time
from kubernetes import client, config

def main():
    # 假设默认环境已经配置好kubeconfig，可直接使用
    config.load_kube_config()

    # 创建一个Pod对象
    pod_manifest = {
        "apiVersion": "v1",
        "kind": "Pod",
        "metadata": {
            "name": "demo-hostport-zero",
        },
        "spec": {
            "containers": [
                {
                    "name": "demo",
                    "image": "nginx:stable-alpine",
                    "ports": [
                        {
                            # 未指定hostPort，仅指定containerPort
                            "containerPort": 80
                        }
                    ]
                }
            ]
        }
    }

    # 初始化CoreV1Api
    v1 = client.CoreV1Api()

    # 创建Pod
    v1.create_namespaced_pod(namespace="default", body=pod_manifest)
    print("Pod demo-hostport-zero created.")

    # 等待片刻查看容器稳定
    time.sleep(10)

    # 获取Pod状态
    pod_status = v1.read_namespaced_pod_status("demo-hostport-zero", "default")
    print(f"Current Pod phase: {pod_status.status.phase}")

    # 清理现场
    v1.delete_namespaced_pod("demo-hostport-zero", "default")
    print("Pod demo-hostport-zero deleted.")
```


**解释说明：**

1. 上述脚本演示了使用 Python 的 kubernetes 库部署一个未指定 hostPort 的容器，使得容器运行时有可能出现 hostPort=0 的配置情况。  
2. 由于此问题与安全无关，因此未提供实际漏洞复现的攻击代码；上述脚本仅用于演示如何在指定 containerPort 的同时不显式配置 hostPort。  
3. 在本地环境中运行该脚本时，请确保已经安装并配置好 Python、kubernetes 库，以及可用的 kubeconfig 配置文件。  
4. 脚本执行完成后，会自动清理创建的 Pod，整个过程不会产生安全隐患。

---


## Issue #130452 Make sets.Set serializable

- Issue 链接：[#130452](https://github.com/kubernetes/kubernetes/issues/130452)

### Issue 内容

#### What happened?

In https://github.com/kubernetes-sigs/kueue/ when running integration tests with verbose logging, there are errors like `"json: unsupported type: sets.Set[sigs.k8s.io/kueue/pkg/resources.FlavorResource]"` — see https://github.com/kubernetes-sigs/kueue/issues/4137

#### What did you expect to happen?

To see json reperesentation of k8s.io/apimachinery/pkg/util/sets.Set objects in logs

#### How can we reproduce it (as minimally and precisely as possible)?

see https://github.com/kubernetes-sigs/kueue/issues/4137

#### Anything else we need to know?

_No response_

#### Kubernetes version


```console
$ kubectl version
v1.31.5
```


#### Cloud provider

<details>
GCP GKE
</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该Issue主要描述了在进行集成测试并开启详细日志时，因“sets.Set”结构无法被JSON序列化而导致日志输出报错的问题。此问题属于正常的功能/兼容性缺陷，仅涉及如何在输出日志时对特殊数据结构进行JSON序列化，未提及敏感信息泄露、权限提升、命令执行或任何其他安全攻击面。根据提供的Issue内容判断，该问题并无安全风险。

**复现过程：**

```python
# 无需复现脚本，此Issue不涉及安全风险
```


---


## Issue #130445 NodeResourcesFit plugin incorrectly computes requested resources

- Issue 链接：[#130445](https://github.com/kubernetes/kubernetes/issues/130445)

### Issue 内容

#### What happened?

NodeResourcesFit plugin gives incorrect score for pods, because somehow it incorrectly computes node requested resources. Even more suspicious is a fact that NodeResourcesBalancedAllocation plugin computes them properly. 

We already got this issue reported on [slack](https://kubernetes.slack.com/archives/C09TP78DV/p1740560748505449?thread_ts=1740500282.620949&cid=C09TP78DV).

#### Example:
Node spec:
```
$ kubectl describe node node1
...
Allocatable:
  cpu:                7910m
  memory:             29077440Ki
  pods:               110
...
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests        Limits
  --------           --------        ------
  cpu                335m (4%)       2 (25%)
  memory             397860096 (1%)  4285048320 (14%)
...
```

Creating a pod with resources:
```
        requests:
          cpu: 200m
          memory: 1Gi
```

kube-scheduler logs:
```
I0226 11:28:40.956301      13 resource_allocation.go:76] "Listed internal info for allocatable resources, requested resources and score" logger="Score.NodeResourcesFit" pod="default/pod-jlqv8" node="node1" resourceAllocationScorer="LeastAllocated" allocatableResource=[7910,29775298560] requestedResource=[630,1855032320] resourceScore=92
I0226 11:28:40.956328      13 resource_allocation.go:76] "Listed internal info for allocatable resources, requested resources and score" logger="Score.NodeResourcesBalancedAllocation" pod="default/pod-jlqv8" node="node1" resourceAllocationScorer="NodeResourcesBalancedAllocation" allocatableResource=[7910,29775298560] requestedResource=[535,1471601920] resourceScore=99
```

As we can see, `requestedResource` in second log (for `NodeResourcesBalancedAllocation` plugin) is correctly computed as allocated + requested, but for `NodeResourcesFit`, the numbers are different.

Score for this node should be:
```
100 * ((7910 - 535) / 7910 + (29775298560 - 1471601920) / 29775298560) / 2 = 94
```
But it is 92.

/sig scheduling

#### What did you expect to happen?

`requestedResource` be computed correctly as well as the score.

#### How can we reproduce it (as minimally and precisely as possible)?

Schedule pods in a cluster and check detailed kube-scheduler logs (on verbosity 10).

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Server Version: v1.32.1-gke.1489001
```

</details>


#### Cloud provider

<details>
Tested on GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该 Issue 描述来看，这是一个在 Kubernetes 调度器中资源计算出现偏差的功能性问题，导致调度评分异常。该问题并未涉及到能够被攻击者利用的安全漏洞（如权限提升、远程命令执行、容器逃逸、凭据泄露等），也不存在明显的利用途径导致拒绝服务或系统被入侵。因此，该问题本质上是一个功能性缺陷，尚未构成可被恶意利用的安全风险。

**复现过程：**

```python
# 该问题与安全风险无关，无需提供复现测试脚本
# 因此此处留空。
pass
```


---


## Issue #130441 CEL,CRD: Combining nested macros and CIDR containing another CIDR checks doesnt seem to work

- Issue 链接：[#130441](https://github.com/kubernetes/kubernetes/issues/130441)

### Issue 内容

#### What happened?

Hi, I trying to use CEL validation on a CRD to verify a given subnet from one field is in range of subnets of another field, but it doesnt seem to work. Please see bellow details: 

The CRD has the following fields
```
subnets:
  description: ...
  items:
    maxLength: 43
    type: string
    x-kubernetes-validations:
    - message: CIDR is invalid
      rule: isCIDR(self)
  maxItems: 2
  minItems: 1
  type: array
  x-kubernetes-validations:
  - message: When 2 CIDRs are set, they must be from different
      IP families
    rule: size(self) != 2 || !isCIDR(self[0]) || !isCIDR(self[1])
      || cidr(self[0]).ip().family() != cidr(self[1]).ip().family()
excludeSubnets:
  description: ...
  items:
    maxLength: 43
    type: string
    x-kubernetes-validations:
    - message: CIDR is invalid
      rule: isCIDR(self)
  maxItems: 25
  minItems: 1
  type: array
```

Using the following rule, to verify each item in `excludeSubnets` is in range of at least one item in `subents`:
```
self.excludeSubnets.all(e, self.subnets.exists(s,cidr(s).containsCIDR(cidr(e))))
```
1. Given the following input the validation should block the request (return false): 
```
subnets: ["192.168.0.0/16"]
excludeSubnets: ["192.200.0.0/30", "192.300.0.1/32"]
```
2. Given the following input the validation should pass (return true): 
```
subnets: ["192.168.0.0/16"]
excludeSubnets: ["192.168.0.100/32", "192.168.0.1/32"]
```
The rule doesnt seem to work and result in blocking inputs (i.e.: return false in both cases).




#### What did you expect to happen?

I expect the above CEL rule to work, it should return false for input (1), and return true for input (2).

#### How can we reproduce it (as minimally and precisely as possible)?

Install the following CRD  [crd.md](https://github.com/user-attachments/files/18981991/crd.md)
Apply [cudn-should-fail.md](https://github.com/user-attachments/files/18985446/cudn-should-fail.md), expect failure.
Apply [cudn-should-pass.md](https://github.com/user-attachments/files/18981992/cudn-should-pass.md), expect success.

#### Anything else we need to know?

It seem that nested macros work with more simple condition, please see this [CEL playground example](https://playcel.undistro.io/?content=H4sIAAAAAAAAA3WS22rCQBCGX2WaXqgQDy30RnojRWiLbaVSpJCbMRnjwmY37OxqpPTdOxsDYkECYTKH%2F%2FtnNz9JTjqZJuMxrEnntiLwFvyO4Gm%2BgKXGY%2BlsMMVNZqTlMgeKAQ0o48lh7tWeRGMzY6Zqo49Q2wM5KoDMXjlrKjI%2BalNTa%2BtIJosYk1Nt5aD87sS1VWUNzJvaEbOScIGmDFgS9IU%2FGEUnrZshrJ3yBEcbXGuNzjM7QXdNX0ytMDpCkFKMWRUEW%2BvEfB08FOgxlRi%2BZ28LkPTr6uM91iv0ncoyKkPvM5heu8UedcArcCxRGfYt6QzohObd%2FnEut1qTnJzM2K0oYFVr4s4Y18phrGXyUJPrUNAqbAx5HqHWfUqBu09qFHvucwoEj8AwGGQmSZNIfYl8ueBbeJYjiVd26erK1qMI7eSnmRnC3WTy30Wbf5icqvF9H3uEW9mCBBn%2FrN8%2FNsGsyWECAAA%3D) 

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.1
Kustomize Version: v5.4.2
Server Version: v1.31.1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该Issue的描述来看，这属于CRD中使用CEL进行网络CIDR校验时的逻辑或使用方式的问题，并未体现出恶意利用或导致实际安全漏洞的场景。Issue的核心是“嵌套CEL宏编写”的规则不符合预期，导致在某些情况下出现了错误的验证结果，并未说明能够被攻击者利用从而带来权限提升、任意代码执行等安全后果。   
因此，该Issue并未说明存在产品本身的安全风险，而是用户在自定义资源定义及验证规则中的配置或实现上出现的逻辑问题。

---


## Issue #130440 Set maxpods to a small value caused existing running pods failed admit when to do inplace update operation

- Issue 链接：[#130440](https://github.com/kubernetes/kubernetes/issues/130440)

### Issue 内容

#### What happened?

Set maxpods to a small value caused existing running pods failed admit when to do inplace update operation

#### What did you expect to happen?

ignore maxpods validate when do admit for inplace update

#### How can we reproduce it (as minimally and precisely as possible)?

1. schedule 110 pods to a node with maxpods set to 110
2. adjust maxpods to 100
3. do inplace update operation with some existing pods
4. pods will stuck in err status due to failed admin

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
master
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue描述来看，问题与节点上已运行的Pod数量及maxpods设置值不匹配导致Pod在进行inplace update操作时验证失败，最终导致Pod无法正常更新。整个过程是集群配置或使用方式不当引发的功能性问题，没有证据表明该问题可以被恶意利用，也不存在凭据泄露、权限提升、命令执行或其他安全攻击路径，因此与安全漏洞无关。

**复现过程：**

```python
# 该Issue与安全无关，无需复现脚本
def main():
    print("该Issue为功能性问题，非安全风险，无需复现POC脚本。")
main()
```


---


## Issue #130439 correct the usage of ephemeral storage volumes in the eviction message

- Issue 链接：[#130439](https://github.com/kubernetes/kubernetes/issues/130439)

### Issue 内容

#### What happened?

In k8s 1.32 cluster, I created a pod with `emptydir` volume and continuously wrote data into the `emptydir` volume to trigger node eviction. However, the event generated during the eviction is as follows:
```
115s        Warning   Evicted                       pod/test-xxxxxsas-b966b9bd4-pzbbc                        The node was low on resource: ephemeral-storage. Threshold quantity: 94576558032, available: 92034400Ki. Container container-1 was using 122Ki, request is 0, has larger consumption of ephemeral-storage.
```
The reported `122Ki` value only includes the usage of `rootfs` and `logs`, without accounting for the usage of volumes such as `emptydir`. 
https://github.com/kubernetes/kubernetes/blob/39c640fe9051b60bacbacf47f64cb697fb807861/pkg/kubelet/eviction/helpers.go#L1262
This may lead to user confusion: why is the pod being evicted when it appears to be using so little ephemeral storage?
Therefore, it is necessary to include the consideration of `emptydir` in the eviction message.

/sig node
friendly ping @pacoxu  @kannon92 

#### What did you expect to happen?

the eviction message contains the correct usage of ephemeral storage volumes

#### How can we reproduce it (as minimally and precisely as possible)?

1. create a pod with `emptydir` volume
2. continuously write data into this volume directory to trigger node eviction

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 仅涉及对 Kubernetes 驱逐（eviction）消息中临时存储（ephemeral storage）用量的统计问题，表现为事件消息中未包含 `emptydir` 卷的实际存储使用量，导致用户在查看日志时出现困惑。此问题不会导致未授权访问、提权、远程命令执行、数据泄露等安全后果，因此不属于安全漏洞。

**解释说明：**

该问题仅在驱逐消息中缺少对 `emptydir` 卷存储用量的统计，属于功能/可观测性层面的改进需求，不涉及安全风险。根据问题描述，不会产生不当信息泄露或被攻击者利用的漏洞，故风险评级为“不涉及”。此处无需提供复现安全问题的脚本。

---


## Issue #130432 pod_startup_latency_tracker returned negative value

- Issue 链接：[#130432](https://github.com/kubernetes/kubernetes/issues/130432)

### Issue 内容

#### What happened?

This can be observed easily when deployed a large number of pods (i.e. 20 pods) at the same time in Kubernetes version 1.31. I found an issue with similar symptom, but it's supposed to be resolved on version 1.27
```
Feb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.323529    3201 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/resource-consume-5fb68775dc-c74hl" podStartSLOduration=1.344002188 podStartE2EDuration="3.323519624s" podCreationTimestamp="2025-02-25 21:38:53 +0000 UTC" firstStartedPulling="2025-02-25 21:38:53.734807377 +0000 UTC m=+565.347657070" lastFinishedPulling="2025-02-25 21:38:55.714324713 +0000 UTC m=+567.327174506" observedRunningTime="2025-02-25 21:38:56.323492624 +0000 UTC m=+567.936342317" watchObservedRunningTime="2025-02-25 21:38:56.323519624 +0000 UTC m=+567.936369517"
Feb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.338810    3201 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/resource-consume-5fb68775dc-bd4br" podStartSLOduration=1.2549581779999999 podStartE2EDuration="3.338800195s" podCreationTimestamp="2025-02-25 21:38:53 +0000 UTC" firstStartedPulling="2025-02-25 21:38:53.661167137 +0000 UTC m=+565.274016830" lastFinishedPulling="2025-02-25 21:38:55.745009154 +0000 UTC m=+567.357858847" observedRunningTime="2025-02-25 21:38:56.338414293 +0000 UTC m=+567.951264086" watchObservedRunningTime="2025-02-25 21:38:56.338800195 +0000 UTC m=+567.951649988"
Feb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.361959    3201 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/resource-consume-5fb68775dc-5ljwx" podStartSLOduration=1.360169063 podStartE2EDuration="3.361949302s" podCreationTimestamp="2025-02-25 21:38:53 +0000 UTC" firstStartedPulling="2025-02-25 21:38:53.709078158 +0000 UTC m=+565.321927851" lastFinishedPulling="2025-02-25 21:38:55.710858397 +0000 UTC m=+567.323708090" observedRunningTime="2025-02-25 21:38:56.353660063 +0000 UTC m=+567.966509756" watchObservedRunningTime="2025-02-25 21:38:56.361949302 +0000 UTC m=+567.974799095"
Feb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.369811    3201 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/resource-consume-5fb68775dc-btpww" podStartSLOduration=1.305661411 podStartE2EDuration="3.369803938s" podCreationTimestamp="2025-02-25 21:38:53 +0000 UTC" firstStartedPulling="2025-02-25 21:38:53.681766232 +0000 UTC m=+565.294615925" lastFinishedPulling="2025-02-25 21:38:55.745908759 +0000 UTC m=+567.358758452" observedRunningTime="2025-02-25 21:38:56.369554537 +0000 UTC m=+567.982404230" watchObservedRunningTime="2025-02-25 21:38:56.369803938 +0000 UTC m=+567.982653731"
Feb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.387444    3201 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/resource-consume-5fb68775dc-46lsl" podStartSLOduration=1.348360709 podStartE2EDuration="3.387435419s" podCreationTimestamp="2025-02-25 21:38:53 +0000 UTC" firstStartedPulling="2025-02-25 21:38:53.674280998 +0000 UTC m=+565.287130691" lastFinishedPulling="2025-02-25 21:38:55.713355708 +0000 UTC m=+567.326205401" observedRunningTime="2025-02-25 21:38:56.387005517 +0000 UTC m=+567.999855210" watchObservedRunningTime="2025-02-25 21:38:56.387435419 +0000 UTC m=+568.000285112"
Feb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.403513    3201 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/resource-consume-5fb68775dc-jnjrp" podStartSLOduration=1.3758508360000001 podStartE2EDuration="3.403501694s" podCreationTimestamp="2025-02-25 21:38:53 +0000 UTC" firstStartedPulling="2025-02-25 21:38:53.719430606 +0000 UTC m=+565.332280399" lastFinishedPulling="2025-02-25 21:38:55.747081464 +0000 UTC m=+567.359931257" observedRunningTime="2025-02-25 21:38:56.403166792 +0000 UTC m=+568.016016485" watchObservedRunningTime="2025-02-25 21:38:56.403501694 +0000 UTC m=+568.016351387"
Feb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.419566    3201 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/resource-consume-5fb68775dc-ccb4x" podStartSLOduration=1.444765554 podStartE2EDuration="3.419556368s" podCreationTimestamp="2025-02-25 21:38:53 +0000 UTC" firstStartedPulling="2025-02-25 21:38:53.728079746 +0000 UTC m=+565.340929439" lastFinishedPulling="2025-02-25 21:38:55.70287056 +0000 UTC m=+567.315720253" observedRunningTime="2025-02-25 21:38:56.419161366 +0000 UTC m=+568.032011159" watchObservedRunningTime="2025-02-25 21:38:56.419556368 +0000 UTC m=+568.032406061"
Feb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.436171    3201 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/resource-consume-5fb68775dc-r87rt" podStartSLOduration=1.4568067089999999 podStartE2EDuration="3.436161144s" podCreationTimestamp="2025-02-25 21:38:53 +0000 UTC" firstStartedPulling="2025-02-25 21:38:53.730592558 +0000 UTC m=+565.343442251" lastFinishedPulling="2025-02-25 21:38:55.709946993 +0000 UTC m=+567.322796686" observedRunningTime="2025-02-25 21:38:56.435724342 +0000 UTC m=+568.048574035" watchObservedRunningTime="2025-02-25 21:38:56.436161144 +0000 UTC m=+568.049010837"
Feb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.451845    3201 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/resource-consume-5fb68775dc-ddtjq" podStartSLOduration=1.468768364 podStartE2EDuration="3.451837417s" podCreationTimestamp="2025-02-25 21:38:53 +0000 UTC" firstStartedPulling="2025-02-25 21:38:53.716675693 +0000 UTC m=+565.329525386" lastFinishedPulling="2025-02-25 21:38:55.699744746 +0000 UTC m=+567.312594439" observedRunningTime="2025-02-25 21:38:56.451532415 +0000 UTC m=+568.064382108" watchObservedRunningTime="2025-02-25 21:38:56.451837417 +0000 UTC m=+568.064687210"
Feb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.466178    3201 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/resource-consume-5fb68775dc-dvz9k" podStartSLOduration=1.349237313 podStartE2EDuration="3.466167683s" podCreationTimestamp="2025-02-25 21:38:53 +0000 UTC" firstStartedPulling="2025-02-25 21:38:53.600616658 +0000 UTC m=+565.213466351" lastFinishedPulling="2025-02-25 21:38:55.717546928 +0000 UTC m=+567.330396721" observedRunningTime="2025-02-25 21:38:56.466025982 +0000 UTC m=+568.078875675" watchObservedRunningTime="2025-02-25 21:38:56.466167683 +0000 UTC m=+568.079017476"
Feb 25 21:39:05 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:39:05.352953    3201 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/resource-consume-5fb68775dc-8fdzh" podStartSLOduration=-9223372024.501835 podStartE2EDuration="12.352941551s" podCreationTimestamp="2025-02-25 21:38:53 +0000 UTC" firstStartedPulling="2025-02-25 21:38:53.759004789 +0000 UTC m=+565.371854482" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-25 21:39:05.35260405 +0000 UTC m=+576.965453743" watchObservedRunningTime="2025-02-25 21:39:05.352941551 +0000 UTC m=+576.965791244"
Feb 25 21:39:07 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:39:07.357166    3201 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/resource-consume-5fb68775dc-xjqwh" podStartSLOduration=-9223372022.497623 podStartE2EDuration="14.357152842s" podCreationTimestamp="2025-02-25 21:38:53 +0000 UTC" firstStartedPulling="2025-02-25 21:38:53.747887837 +0000 UTC m=+565.360737530" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-25 21:39:07.356718546 +0000 UTC m=+578.969568239" watchObservedRunningTime="2025-02-25 21:39:07.357152842 +0000 UTC m=+578.970002535"
Feb 25 21:39:07 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:39:07.373079    3201 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/resource-consume-5fb68775dc-txh69" podStartSLOduration=-9223372022.481707 podStartE2EDuration="14.373068399s" podCreationTimestamp="2025-02-25 21:38:53 +0000 UTC" firstStartedPulling="2025-02-25 21:38:53.77438226 +0000 UTC m=+565.387231953" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-25 21:39:07.372808301 +0000 UTC m=+578.985657994" watchObservedRunningTime="2025-02-25 21:39:07.373068399 +0000 UTC m=+578.985918092"
Feb 25 21:39:07 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:39:07.392708    3201 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/resource-consume-5fb68775dc-7nhl7" podStartSLOduration=-9223372022.462074 podStartE2EDuration="14.392701821s" podCreationTimestamp="2025-02-25 21:38:53 +0000 UTC" firstStartedPulling="2025-02-25 21:38:53.758577787 +0000 UTC m=+565.371427480" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-25 21:39:07.392238626 +0000 UTC m=+579.005088319" watchObservedRunningTime="2025-02-25 21:39:07.392701821 +0000 UTC m=+579.005551614"
Feb 25 21:39:07 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:39:07.410015    3201 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/resource-consume-5fb68775dc-t877k" podStartSLOduration=-9223372022.44477 podStartE2EDuration="14.410004765s" podCreationTimestamp="2025-02-25 21:38:53 +0000 UTC" firstStartedPulling="2025-02-25 21:38:53.758583587 +0000 UTC m=+565.371433280" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-25 21:39:07.409663668 +0000 UTC m=+579.022513361" watchObservedRunningTime="2025-02-25 21:39:07.410004765 +0000 UTC m=+579.022854458"
Feb 25 21:39:08 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:39:08.365845    3201 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/resource-consume-5fb68775dc-gb6tb" podStartSLOduration=-9223372021.488941 podStartE2EDuration="15.365835135s" podCreationTimestamp="2025-02-25 21:38:53 +0000 UTC" firstStartedPulling="2025-02-25 21:38:53.748261839 +0000 UTC m=+565.361111632" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-25 21:39:08.36525234 +0000 UTC m=+579.978102133" watchObservedRunningTime="2025-02-25 21:39:08.365835135 +0000 UTC m=+579.978684828"
Feb 25 21:39:08 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:39:08.414713    3201 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/resource-consume-5fb68775dc-6h58c" podStartSLOduration=-9223372021.440079 podStartE2EDuration="15.414697194s" podCreationTimestamp="2025-02-25 21:38:53 +0000 UTC" firstStartedPulling="2025-02-25 21:38:53.768832034 +0000 UTC m=+565.381681827" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-25 21:39:08.414602395 +0000 UTC m=+580.027452188" watchObservedRunningTime="2025-02-25 21:39:08.414697194 +0000 UTC m=+580.027546987"
Feb 25 21:39:08 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:39:08.479999    3201 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/resource-consume-5fb68775dc-8kf54" podStartSLOduration=-9223372021.374792 podStartE2EDuration="15.479984263s" podCreationTimestamp="2025-02-25 21:38:53 +0000 UTC" firstStartedPulling="2025-02-25 21:38:53.773336955 +0000 UTC m=+565.386186648" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-25 21:39:08.432632432 +0000 UTC m=+580.045482125" watchObservedRunningTime="2025-02-25 21:39:08.479984263 +0000 UTC m=+580.092834156"
Feb 25 21:39:09 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:39:09.368159    3201 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/resource-consume-5fb68775dc-2vfzh" podStartSLOduration=-9223372020.48663 podStartE2EDuration="16.368146878s" podCreationTimestamp="2025-02-25 21:38:53 +0000 UTC" firstStartedPulling="2025-02-25 21:38:53.740005101 +0000 UTC m=+565.352854894" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-02-25 21:39:09.368031478 +0000 UTC m=+580.980881171" watchObservedRunningTime="2025-02-25 21:39:09.368146878 +0000 UTC m=+580.980996571"
Feb 25 21:39:09 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:39:09.383023    3201 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/resource-consume-5fb68775dc-hgd78" **podStartSLOduration=-9223372020.471764** podStartE2EDuration="16.383012397s" podCreationTimestamp="2025-02-25 21:38:53 +0000 UTC" firstStartedPulling="2025-02-25 21:38:53.753171862 +0000 UTC m=+565.366021555" **lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC"** observedRunningTime="2025-02-25 21:39:09.382750199 +0000 UTC m=+580.995599892" watchObservedRunningTime="2025-02-25 21:39:09.383012397 +0000 UTC m=+580.995862090"
```

For the last 10 pods, the log is like this: pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/resource-consume-5fb68775dc-hgd78" **podStartSLOduration=-9223372020.471764** podStartE2EDuration="16.383012397s" podCreationTimestamp="2025-02-25 21:38:53 +0000 UTC" firstStartedPulling="2025-02-25 21:38:53.753171862 +0000 UTC m=+565.366021555" **lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC"** observedRunningTime="2025-02-25 21:39:09.382750199 +0000 UTC m=+580.995599892" watchObservedRunningTime="2025-02-25 21:39:09.383012397 +0000 UTC m=+580.995862090"

I saw similar issue [here](https://github.com/kubernetes/kubernetes/issues/114903) and it's supposed to be fixed in k8s version 1.27



#### What did you expect to happen?

The result should be like the first 10 pods, but somehow it returned negative value for the last 10 pods:
Feb 25 21:38:56 aks-userpool0-20459031-vmss000001 kubelet[3201]: I0225 21:38:56.323529    3201 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/resource-consume-5fb68775dc-c74hl" **podStartSLOduration=1.344002188** podStartE2EDuration="3.323519624s" podCreationTimestamp="2025-02-25 21:38:53 +0000 UTC" firstStartedPulling="2025-02-25 21:38:53.734807377 +0000 UTC m=+565.347657070" **lastFinishedPulling="2025-02-25 21:38:55.714324713 +0000 UTC** m=+567.327174506" observedRunningTime="2025-02-25 21:38:56.323492624 +0000 UTC m=+567.936342317" watchObservedRunningTime="2025-02-25 21:38:56.323519624 +0000 UTC m=+567.936369517"

#### How can we reproduce it (as minimally and precisely as possible)?

Create a cluster with kubernetes version 1.31. Use the following deployment to create pods 
Deployment spec
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: resource-consume
  labels:
    group: resource-consume
spec:
  replicas: 20
  selector:
    matchLabels:
      name: resource-consume
  template:
    metadata:
      labels:
        name: resource-consume
        group: resource-consume
    spec:
      hostNetwork: true
      nodeSelector:
        kubernetes.io/hostname: "aks-userpool0-20459031-vmss000001"
      containers:
      - name: resource-consumer-memory
        image: registry.k8s.io/e2e-test-images/resource-consumer:1.9
        imagePullPolicy: IfNotPresent
        command:
          - stress
        args:
          - --vm
          - "1"
          - --vm-bytes
          - 5000K
          - --vm-hang
          - "0"
          - --timeout
          - "3600"
        resources:
          requests:
            memory: 5000Ki
      tolerations:
      - key: "cri-resource-consume"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
      - key: "cri-resource-consume"
        operator: "Equal"
        value: "true"
        effect: "NoExecute"
```
Run `kubectl node-shell` into the node to check kubelet log and grep the podStartE2EDuration
```
kubectl node-shell <node-name>
cd var/log
cat messages | grep pod_startup_latency_tracker
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.31.5
```

</details>


#### Cloud provider

<details>
AKS

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
PRETTY_NAME="Ubuntu 22.04.5 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
# paste output here
Linux aks-userpool0-20459031-vmss000001 5.15.0-1079-azure #88-Ubuntu SMP Thu Jan 16 19:18:54 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd://1.7.25-1
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>

/sig node
/kind bug
/help

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该 Issue 的描述和日志信息来看，问题集中在 Kubernetes kubelet 组件对于容器启动时间的计算出现了负值。这类负值通常是由于时间戳计算或内部计数溢出等问题所导致，属于功能或逻辑层面的 Bug，并未说明可以被攻击者利用从而导致命令执行、权限提升、容器逃逸、敏感信息泄露或其他严重后果，也没有涉及到拒绝服务（DoS）的描述或可被利用的场景。因此，本 Issue 并没有体现出可被利用的安全风险。

**复现过程：**

```python
# 该问题非安全风险，无需提供安全漏洞复现脚本
# 如果需要重现场景，仅需按照 Issue 所描述的方式部署和查看日志即可
```


---


## Issue #130429 Preemption loop when using multiple schedulers

- Issue 链接：[#130429](https://github.com/kubernetes/kubernetes/issues/130429)

### Issue 内容

#### What happened?

Our system has two schedulers running (they are both running default kube-scheduler image but using different configurations). 

Recently we observed a large number of unexpected preemptions. 

After investigation, here's what happened.

A pod controlled by scheduler-1 needs to be scheduled to a node. The node is at max pod limit, so eviction of lower priority pod needs to happen.

1. Scheduler-1 preempted a lower priority pod that is controlled by scheduler-2. 
2. During the termination phase, the controller manager created a new replica of the victim pod.
3. Scheduler-2 put the new pod back to the same node.

Preemptor pod doesn't get scheduled, and the above loop continues.

We observed that the preemptor was stuck in pending for almost 2 days. And victim pods were evicted and re-scheduled repeatedly 8K+ times during this period.



#### What did you expect to happen?

1. High priority pod should be scheduled after preempting one low priority pod

2. https://kubernetes.io/docs/tasks/extend-kubernetes/configure-multiple-schedulers/    this doc shouldn't recommend multiple schedulers set up without mentioning this problem?

3. Can we make schedulers talk with each other to avoid such a situation?

#### How can we reproduce it (as minimally and precisely as possible)?

Scheduler-1 is using default scheduler image, default configuration
Scheduler-2 is using default scheduler image, most-allocated strategy, disabled the default podTopologySpread constraint

Use scheduler-2 to scheduler low priority pods until node at max pod limit

Use scheduler-1 to schedule a high priority pod

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
v1.29.7
```

</details>


#### Cloud provider

<details>
AKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从描述来看，该 Issue 主要涉及在 Kubernetes 集群中使用多个调度器（scheduler）所导致的调度环路问题（高优先级 Pod 与低优先级 Pod 之间反复抢占、重建）。这一问题并不涉及到任何敏感信息泄露、权限提升、命令执行或容器逃逸等安全风险，也不涉及到由于配置疏忽而可被恶意攻击者利用的高风险情形。严格来说，这更像是多调度器协作逻辑上的缺陷或是不完善的调度策略，导致系统出现无限重复的抢占与重建循环，并没有明确的攻击向量，也不属于凭证泄露、越权访问或其他安全漏洞。

根据给出的风险判断标准，这种由于配置或设计导致的资源使用问题，并非传统意义上的安全漏洞。该问题本质上会造成资源浪费、集群不稳定乃至影响高优先级 Pod 正常运行，但并不直接给出攻击者利用的场景，无法构成針对安全的攻击手段，因此判定为“非安全问题”。

**复现过程：**

```python
# 该Issue并非安全问题，故无需提供安全复现脚本。
# 若需要单纯重现多调度器导致的抢占环路，可参照Issue中描述的部署及调度步骤，
# 但这里不属于安全风险POC范围，无需在此给出具体的脚本。
```


---


## Issue #130418 pods stuck in containerstatusunknown not GCed

- Issue 链接：[#130418](https://github.com/kubernetes/kubernetes/issues/130418)

### Issue 内容

#### What happened?

I have a cluster running on 1.30. During cluster bring up, there are a bunch of pods that end up in containerstatusunknown status as the node is short of cpu requested by the pod. eventually the pod goes into running state. however the pods spawned earlier that ended up in containerstatusunknown are not cleaned up.

#### What did you expect to happen?

Expected pods in cotainerstatusunknown to be garbage collected once a sunsequent pod moves into running state

#### How can we reproduce it (as minimally and precisely as possible)?

bring up more number of pods that max out the cpu on the node.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
v1.30.5
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# NAME="VMware Photon OS"
VERSION="5.0"
ID=photon
VERSION_ID=5.0
$ uname -a
# Linux 421451ae19017c0215cc9480570adc3a 6.1.126-5.ph5

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该 Issue 描述来看，主要是 Kubernetes 节点 CPU 资源不足，导致一些 Pod 状态卡在 containerstatusunknown，后续即使新的 Pod 成功运行，先前的处于状态 unknown 的 Pod 也没有被清理掉。该问题反映了调度或垃圾回收流程上的异常，并未体现出可被恶意利用的安防漏洞，也没有提及敏感数据泄露、权限提升、命令执行或其他潜在的安全问题。  
因此，该问题属于功能性或稳定性问题，而非安全风险。

**解释说明：**

由于此 Issue 不涉及安全风险，故无需提供安全漏洞的复现脚本。该问题更多是调度、资源回收等方面的功能性缺陷，可通过 Kubernetes 的问题排查与配置优化来解决。若需重现该问题，可在 Kubernetes 集群中创建大量 Pod 占满 CPU 资源并观察容器状态，但这与安全漏洞复现无关，因此不附带安全问题复现脚本。

---


## Issue #130415 If node reports falsely Ready before it creates server CSR, certificate templates never populate and server CSR is not created

- Issue 链接：[#130415](https://github.com/kubernetes/kubernetes/issues/130415)

### Issue 内容

#### What happened?

When debugging https://github.com/kubernetes/kubernetes/issues/130001 with @lentzi90, we tried so many ways to figure out why the CSR was not created. One was #130409 and another one was that if node is considering itself `Ready` while not actually being Ready, before it creates server CSR, those will be forever stuck as it does not get templates. Client CSR and cert are provisioned properly.

Detecting this loop required additional logging to be added, such as @lentzi90's branch [here for 1.32](https://github.com/Nordix/kubernetes/tree/lentzi90/kubelet-logging-release-1.32) or [here for main](https://github.com/Nordix/kubernetes/tree/lentzi90/kubelet-logging).

Logs:

```console
Feb 24 14:12:56 tuomo-ubuntu kubelet[1799127]: I0212 14:12:56.080199 1799127 server.go:607] "[===DEBUG===] KubeletServer validated"
Feb 24 14:12:56 tuomo-ubuntu kubelet[1799127]: I0212 14:12:56.080335 1799127 server.go:647] "[===DEBUG===] Standalone Mode" standaloneMode=false
Feb 24 14:12:56 tuomo-ubuntu kubelet[1799127]: I0212 14:12:56.080351 1799127 server.go:673] "[===DEBUG===] Host and node names" hostName="tuomo-ubuntu" nodeName="tuomo-ubuntu"
Feb 24 14:12:56 tuomo-ubuntu kubelet[1799127]: I0212 14:12:56.080365 1799127 server.go:684] "[===DEBUG===] Setting up API clients"
Feb 24 14:12:56 tuomo-ubuntu kubelet[1799127]: I0212 14:12:56.082073 1799127 server.go:725] "[===DEBUG===] Setting up Auth"
Feb 24 14:12:56 tuomo-ubuntu kubelet[1799127]: I0212 14:12:56.088155 1799127 server.go:737] "[===DEBUG===] PreInitRuntimeService completed"
Feb 24 14:12:56 tuomo-ubuntu kubelet[1799127]: I0212 14:12:56.106391 1799127 certificate_manager.go:356] kubernetes.io/kubelet-serving: [===DEBUG===] Dynamic template exists
Feb 24 14:12:56 tuomo-ubuntu kubelet[1799127]: I0212 14:12:56.106478 1799127 certificate_manager.go:356] kubernetes.io/kubelet-serving: [===DEBUG===] No certificate template 
available
Feb 24 14:12:56 tuomo-ubuntu kubelet[1799127]: I0212 14:12:56.106597 1799127 kubelet.go:1730] "[===DEBUG===] Modules initialized"
Feb 24 14:12:56 tuomo-ubuntu kubelet[1799127]: I0212 14:12:56.106650 1799127 kubelet.go:1769] "[===DEBUG===] Container runtime up"
Feb 24 14:12:57 tuomo-ubuntu kubelet[1799127]: I0212 14:12:57.106513 1799127 certificate_manager.go:356] kubernetes.io/kubelet-serving: [===DEBUG===] No certificate template 
available
Feb 24 14:12:58 tuomo-ubuntu kubelet[1799127]: I0212 14:12:58.107473 1799127 certificate_manager.go:356] kubernetes.io/kubelet-serving: [===DEBUG===] No certificate template 
available
Feb 24 14:12:59 tuomo-ubuntu kubelet[1799127]: I0212 14:12:59.107548 1799127 certificate_manager.go:356] kubernetes.io/kubelet-serving: [===DEBUG===] No certificate template 
available
Feb 24 14:13:00 tuomo-ubuntu kubelet[1799127]: I0212 14:13:00.107777 1799127 certificate_manager.go:356] kubernetes.io/kubelet-serving: [===DEBUG===] No certificate template 
available
Feb 24 14:13:01 tuomo-ubuntu kubelet[1799127]: I0212 14:13:01.108748 1799127 certificate_manager.go:356] kubernetes.io/kubelet-serving: [===DEBUG===] No certificate template 
available
Feb 24 14:13:02 tuomo-ubuntu kubelet[1799127]: I0212 14:13:02.109771 1799127 certificate_manager.go:356] kubernetes.io/kubelet-serving: [===DEBUG===] No certificate template 
available
...
```
and it sits there for all eternity, printing it does not have certificate template.

#### What did you expect to happen?

Kubelet to initialize itself, request client and server CSRs as configured with bootstrapping and rotation in kubeadm config.


#### How can we reproduce it (as minimally and precisely as possible)?

Now this is tricky part. Kubelet thinks its ready when it has CNI config files in `/etc/cni/net.d/...` and it has _some_ routes (not exactly sure what counts). This means this issue can be occuring only when resetting an existing node, or maybe when upgrading, or some other lifecycle management operations are run, or maybe if someone optimizes the node installation somehow. A clean VM and and first time kubeadm init will not run into this issue AFAIK.

Using very simple kubeadm config:

```yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
serverTLSBootstrap: true
rotateCertificates: true
``` 

we init the cluster, and then install Calico on it (to get the config, and the routes):

```console
kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
...
``` 
and we can see node turns `Ready` way before Calico manages to run more than one or two calico-node containers (which create the configs/binaries/routes). We then do some LCM operation, simulated here with a reset, without manually nuking more than kubeadm is cleaning up:

```console
sudo kubeadm reset -f
```

If we now run `kubeadm init` again, it will report node Ready (as it has everything in place right away, without installing Calico again) and CSR will be created just fine.  But there are certain conditions where node thinks it is still Ready, but it fails, especially related to networking/routes, which sometimes happen during maintenance and are later really hard to catch and debug, as Kubelet does not print a single thing related to server CSRs, unless instrumented as documented above.

Would a PR that adds some logging to server CSR creation, say at level 4, be acceptable?

#### Anything else we need to know?

I apologize for the lack of exact reproduction steps. I managed to achieve this condition several times during CSR creation debugging, but due nature of debugging failed to capture/understand the exact condition that satisfies the node checks, yet still causes this issue. I'm hoping this helps someone more knowledgeable realize which condition that might be.

I will keep debugging this for a while more, pointers are welcome.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.2
Kustomize Version: v5.5.0
Server Version: v1.32.2```
</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 24.04.2 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04.2 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo
$ uname -a
Linux tuomo-ubuntu 6.8.0-40-generic #40-Ubuntu SMP PREEMPT_DYNAMIC Fri Jul  5 10:34:03 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>
kubeadm, kubectl, kubelet from k8s's apt repos.
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
$ kubectl get node -owide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME tuomo-ubuntu NotReady control-plane 17m v1.32.2 Ubuntu 24.04.2 LTS 6.8.0-40-generic containerd://1.7.24
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
Calico latest used here
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据该Issue描述，Kubelet错误地将自身状态标为“Ready”从而导致Server CSR无法及时创建，这一问题会使节点的证书轮转过程无法正常进行，但从描述内容来看，它并不会引发敏感信息泄露、权限提升或远程代码执行等安全风险，更像是一个节点初始化或网络配置的逻辑缺陷。  
整个流程主要问题在于：Kubelet错误地判定自身为“Ready”，从而跳过或错过了后续Server CSR模板生成及证书请求的步骤。这会导致集群节点无法正确产生并使用服务端证书，但并不使攻击者能利用某种缺陷进行入侵或越权操作。因此，此问题更多是可用性或可操作性方面的Bug，而非安全漏洞。

综合评估，此Issue不涉及敏感信息泄露、未授权访问、命令执行或容器逃逸等安全场景，也未出现攻击者可趁机利用的高危手段。故在本次判断标准下，认定为“不涉及”安全风险。

**复现过程：**

```python
# 该问题并非安全问题，无需提供安全性复现脚本
# 如需要在本地测试Kubelet错误就绪状态，可在部署/重置集群时跟踪kubelet日志，
# 并调用Kubernetes API动态查看CSR生成情况，但这属于功能性测试而非安全复现。
```


---


## Issue #130410 API server: spammy error logs for proxied watch requests

- Issue 链接：[#130410](https://github.com/kubernetes/kubernetes/issues/130410)

### Issue 内容

#### What happened?

Our project (Calico) includes an aggregated API server.  (I think) when somethin watches resources exposed by the aggregated server via the kube API server, the kube API server emits error logs every 10 minutes for each resource:
```
wrap.go:53] timeout or abort while handling: method=GET URI="/apis/projectcalico.org/v3/profiles?allowWatchBookmarks=true&resourceVersion=73417144%2F73417144&watch=true" audit-ID="c17b2ff6-c3a8-4698-a9b5-58b9dc6bf56a"
wrap.go:53] timeout or abort while handling: method=GET URI="/apis/projectcalico.org/v3/globalnetworksets?allowWatchBookmarks=true&resourceVersion=73417144&watch=true" audit-ID="e853832c-3508-48a7-b5a8-f3e787df9c94
wrap.go:53] timeout or abort while handling: method=GET URI="/apis/projectcalico.org/v3/hostendpoints?allowWatchBookmarks=true&resourceVersion=73417144&watch=true" audit-ID="31ef1042-a457-4510-997a-26cd306e0551"
...
```
Since these are watches, the client retries and the errors are benign AFAICT.

Since these are errors they tend to get picked up by log monitoring tools and need to be filtered out (or cost money in log storage).

#### What did you expect to happen?

No timeout errors should for watch requests.  Either there's some detail that Calico's API server is getting wrong(?) or these should be handled/suppressed in the k8s API server.

#### How can we reproduce it (as minimally and precisely as possible)?

Install a cluster with Calico's aggregated API server and watch the kube API server logs.

#### Anything else we need to know?

Quite possible it's a bug on our side or mismatched expectation but I'm having a very hard time tracking down the Kubernetes code that handles these proxied requests (and it's all very abstract so I'm finding it very hard to guess what we might need to do on our side to avoid this error).

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

Users have reported the issue on k8s 1.25 and 1.29.6

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
Calico with its API server installed.
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该Issue的描述来看，主要是Kubernetes在使用Calico聚合API服务器时产生了超时日志，Issue内容关注的是“定时产生的错误日志会消耗存储空间或导致日志监控误报”的问题。该问题并未涉及任何越权访问、命令执行、提权、容器逃逸或机密信息泄露等安全风险场景，也不属于拒绝服务（DoS）攻击范畴（仅仅是日志层面被动写入，并无证据表明会造成系统资源被耗尽）。  
因此，从安全风险角度上看，此问题属于正常的功能或兼容性故障，不会带来实际的安全威胁或攻击面，不构成安全漏洞。

**解释说明：**

该Issue描述的问题并非安全问题，不存在潜在的攻击向量或敏感信息泄露，不需要复现脚本。根据Issue提供的信息，在多用户或高权限环境中也没有构成任意攻击或破坏的可能，因此风险评级判断为“不涉及”。  
如果需要解决该日志问题，可从配置或兼容性的角度进行排查或修复，而非从安全加固或漏洞修补的角度展开工作。

---


## Issue #130409 Adding link-local ipv6 address to --node-ip makes node have no IP address

- Issue 链接：[#130409](https://github.com/kubernetes/kubernetes/issues/130409)

### Issue 内容

#### What happened?

When debugging https://github.com/kubernetes/kubernetes/issues/130001 with @lentzi90, we tried so many ways to figure out why the CSR was not created. One of the early reproduction cases was adding a link-local ipv6 address to kubelet's `--node-ip` argument in kubeadm `InitConfiguration`.

```yaml
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
nodeRegistration:
  kubeletExtraArgs:
  - name: node-ip
    value: "10.1.0.201,fe80::f816:3eff:fe05:60bd"
``` 

leads to node not having the `addresses` at all:

```yaml
apiVersion: v1
items:
- apiVersion: v1
  kind: Node
  metadata:
    annotations:
      kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
      node.alpha.kubernetes.io/ttl: "0"
      volumes.kubernetes.io/controller-managed-attach-detach: "true"
    creationTimestamp: "2025-02-25T09:46:40Z"
    labels:
      beta.kubernetes.io/arch: amd64
      beta.kubernetes.io/os: linux
      kubernetes.io/arch: amd64
      kubernetes.io/hostname: tuomo-ubuntu
      kubernetes.io/os: linux
      node-role.kubernetes.io/control-plane: ""
      node.kubernetes.io/exclude-from-external-load-balancers: ""
    name: tuomo-ubuntu
    resourceVersion: "355"
    uid: 590a5d1b-ae95-4ccb-819d-f679c09dbada
  spec:
    podCIDR: 192.168.0.0/24
    podCIDRs:
    - 192.168.0.0/24
    taints:
    - effect: NoSchedule
      key: node-role.kubernetes.io/control-plane
    - effect: NoSchedule
      key: node.kubernetes.io/not-ready
  status:
    allocatable:
      cpu: "8"
      ephemeral-storage: "279820272382"
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 31988700Ki
      pods: "110"
    capacity:
      cpu: "8"
      ephemeral-storage: 303624428Ki
      hugepages-1Gi: "0"
      hugepages-2Mi: "0"
      memory: 32091100Ki
      pods: "110"
    conditions:
    - lastHeartbeatTime: "2025-02-25T09:46:43Z"
      lastTransitionTime: "2025-02-25T09:46:39Z"
      message: kubelet has sufficient memory available
      reason: KubeletHasSufficientMemory
      status: "False"
      type: MemoryPressure
    - lastHeartbeatTime: "2025-02-25T09:46:43Z"
      lastTransitionTime: "2025-02-25T09:46:39Z"
      message: kubelet has no disk pressure
      reason: KubeletHasNoDiskPressure
      status: "False"
      type: DiskPressure
    - lastHeartbeatTime: "2025-02-25T09:46:43Z"
      lastTransitionTime: "2025-02-25T09:46:39Z"
      message: kubelet has sufficient PID available
      reason: KubeletHasSufficientPID
      status: "False"
      type: PIDPressure
    - lastHeartbeatTime: "2025-02-25T09:46:43Z"
      lastTransitionTime: "2025-02-25T09:46:39Z"
      message: 'container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady
        message:Network plugin returns error: cni plugin not initialized'
      reason: KubeletNotReady
      status: "False"
      type: Ready
    daemonEndpoints:
      kubeletEndpoint:
        Port: 10250
    images:
    - names:
      - registry.k8s.io/etcd@sha256:c6a9d11cc5c04b114ccdef39a9265eeef818e3d02f5359be035ae784097fdec5
      - registry.k8s.io/etcd:3.5.16-0
      sizeBytes: 57680541
    - names:
      - registry.k8s.io/kube-proxy@sha256:83c025f0faa6799fab6645102a98138e39a9a7db2be3bc792c79d72659b1805d
      - registry.k8s.io/kube-proxy:v1.32.2
      sizeBytes: 30907858
    - names:
      - registry.k8s.io/kube-apiserver@sha256:c47449f3e751588ea0cb74e325e0f83db335a415f4f4c7fb147375dd6c84757f
      - registry.k8s.io/kube-apiserver:v1.32.2
      sizeBytes: 28670731
    - names:
      - registry.k8s.io/kube-controller-manager@sha256:399aa50f4d1361c59dc458e634506d02de32613d03a9a614a21058741162ef90
      - registry.k8s.io/kube-controller-manager:v1.32.2
      sizeBytes: 26259392
    - names:
      - registry.k8s.io/kube-scheduler@sha256:45710d74cfd5aa10a001d0cf81747b77c28617444ffee0503d12f1dcd7450f76
      - registry.k8s.io/kube-scheduler:v1.32.2
      sizeBytes: 20657902
    - names:
      - registry.k8s.io/coredns/coredns@sha256:9caabbf6238b189a65d0d6e6ac138de60d6a1c419e5a341fbbb7c78382559c6e
      - registry.k8s.io/coredns/coredns:v1.11.3
      sizeBytes: 18562039
    - names:
      - registry.k8s.io/pause@sha256:ee6521f290b2168b6e0935a181d4cff9be1ac3f505666ef0e3c98fae8199917a
      - registry.k8s.io/pause:3.10
      sizeBytes: 320368
    - names:
      - registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d
      - registry.k8s.io/pause:3.8
      sizeBytes: 311286
    nodeInfo:
      architecture: amd64
      bootID: 5b61602f-1461-4512-b3a1-dbbd22b64ae9
      containerRuntimeVersion: containerd://1.7.24
      kernelVersion: 6.8.0-40-generic
      kubeProxyVersion: v1.32.2
      kubeletVersion: v1.32.2
      machineID: 6d38c45e1b4f467bacb46196e2d4fdb5
      operatingSystem: linux
      osImage: Ubuntu 24.04.2 LTS
      systemUUID: 6d38c45e-1b4f-467b-acb4-6196e2d4fdb5
kind: List
metadata:
  resourceVersion: ""
``` 

which then leads to Kubelet being not able to request server CSRs, as it is lacking the IPv4 address too.


#### What did you expect to happen?

Kubelet either errors out when it detects link-local address is configured, or just drops it, but it should not drop the valid IPv4 address, and the addresses config completely. 

Here is sample of `addresses` when non-link-local IPv6 is used (which is completely missing in problem case):

```yaml
  status:
    addresses:
    - address: 10.1.0.120
      type: InternalIP
    - address: fdec:c5cc:d73b:37d::76
      type: InternalIP
    - address: tuomo-ubuntu2
      type: Hostname
```

#### How can we reproduce it (as minimally and precisely as possible)?

Using k8s 1.32, and following kubeadm InitConfiguration:

```yaml
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
nodeRegistration:
  kubeletExtraArgs:
  - name: node-ip
    value: "10.1.0.201,fe80::f816:3eff:fe05:60bd"
``` 
where the `fe80::....` is the link-local address of the machine:

```console
$ ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host noprefixroute 
       valid_lft forever preferred_lft forever
2: ens3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether fa:16:3e:05:60:bd brd ff:ff:ff:ff:ff:ff
    altname enp0s3
    inet 10.1.0.201/24 metric 100 brd 10.1.0.255 scope global dynamic ens3
       valid_lft 85280sec preferred_lft 85280sec
    inet6 fdec:c5cc:d73b:37d::189/128 scope global dynamic noprefixroute 
       valid_lft 85281sec preferred_lft 85281sec
    inet6 fe80::f816:3eff:fe05:60bd/64 scope link 
       valid_lft forever preferred_lft forever
``` 

and if CSR part is relevant (it should not, as it is obvious that it cannot work without addresses), we can add KubeletConfiguration that enables TLS bootstrapping and rotation:

```yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
serverTLSBootstrap: true
rotateCertificates: true
```


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.2
Kustomize Version: v5.5.0
Server Version: v1.32.2
```

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 24.04.2 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04.2 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo
$ uname -a
Linux tuomo-ubuntu 6.8.0-40-generic #40-Ubuntu SMP PREEMPT_DYNAMIC Fri Jul  5 10:34:03 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>
kubeadm, kubectl, kubelet from k8s's apt repos.
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
$ kubectl get node -owide
NAME           STATUS     ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
tuomo-ubuntu   NotReady   control-plane   17m   v1.32.2   <none>        <none>        Ubuntu 24.04.2 LTS   6.8.0-40-generic   containerd://1.7.24
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
Not relevant
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该 Issue 的描述来看，问题是在 kubeadm 安装或初始化时，给 kubelet 传入了含有 link-local IPv6 地址 (形如 fe80:: 开头) 的参数，导致节点状态中缺少所有 IP 地址，从而 Kubelet 无法正常创建证书签名请求 (CSR)。这一问题本质上是一个配置或兼容性 Bug，并不涉及到越权访问、信息泄露、远程命令执行、提权等常见安全风险，也没有证据表明攻击者可以利用该问题来进行针对性的恶意行为。因此，根据“Issue风险判断标准”，该问题并不属于安全漏洞，而是一个项目配置兼容性问题。

**复现过程：**

```python
import os
import time
import subprocess
import tempfile
import yaml
from kubernetes import client, config

def main():
    """
    本脚本仅用于在本地环境中模拟该Issue场景，验证使用带有link-local IPv6地址的node-ip参数
    是否会导致Node缺少地址信息。该脚本需要在已安装kubeadm、kubernetes客户端工具并且
    可以以root（或sudo）权限运行的Linux环境里执行。请在测试环境中运行，避免影响生产环境。
    脚本执行预期在2分钟内完成。
    """
    
    # 超时时间设置为120秒，避免脚本长时间阻塞
    timeout_seconds = 120

    # 第一步：准备自定义的 kubeadm 配置文件，包含 link-local IPv6 地址
    init_config = {
        "apiVersion": "kubeadm.k8s.io/v1beta4",
        "kind": "InitConfiguration",
        "nodeRegistration": {
            "kubeletExtraArgs": [
                {
                    "name": "node-ip",
                    # 这里的 fe80::f816:3eff:fe05:60bd 仅作示例，需要替换为真实环境的 link-local IPv6
                    "value": "10.1.0.201,fe80::f816:3eff:fe05:60bd"
                }
            ]
        }
    }

    tmp_dir = tempfile.mkdtemp(prefix="kubeadm-linklocal-test-")
    config_path = os.path.join(tmp_dir, "init-config.yaml")
    with open(config_path, "w") as f:
        yaml.dump(init_config, f)

    # 第二步：先 reset 以清理旧集群（谨慎使用，可能会丢失已有集群数据）
    try:
        print("Running kubeadm reset...")
        subprocess.run(["kubeadm", "reset", "-f"], check=True, timeout=timeout_seconds)
    except subprocess.CalledProcessError as e:
        print(f"kubeadm reset 失败: {e}")
        return
    except subprocess.TimeoutExpired:
        print("kubeadm reset 超时，脚本退出。")
        return

    # 第三步：使用上述配置文件初始化集群
    try:
        print("Running kubeadm init with link-local IPv6 in node-ip...")
        subprocess.run(["kubeadm", "init", "--config", config_path],
                       check=True, timeout=timeout_seconds)
    except subprocess.CalledProcessError as e:
        print(f"kubeadm init 失败: {e}")
        return
    except subprocess.TimeoutExpired:
        print("kubeadm init 超时，脚本退出。")
        return

    # 第四步：配置本地 kubeconfig，使客户端能访问新集群
    kube_config_dir = os.path.expanduser("~/.kube")
    admin_conf = "/etc/kubernetes/admin.conf"
    if not os.path.exists(kube_config_dir):
        os.makedirs(kube_config_dir, exist_ok=True)
    if os.path.exists(admin_conf):
        subprocess.run(["cp", admin_conf, os.path.join(kube_config_dir, "config")], check=False)
        subprocess.run(["chown", f"{os.getuid()}:{os.getgid()}", os.path.join(kube_config_dir, "config")], check=False)

    # 第五步：等待Kubernetes组件启动
    # 由于是演示环境，简单等待几秒；真实环境可能需要更长时间
    time.sleep(20)

    # 第六步：使用 python kubernetes 客户端检查 Node 列表，查看 addresses 是否为空
    try:
        config.load_kube_config()  # 尝试加载 ~/.kube/config
        v1 = client.CoreV1Api()
        nodes = v1.list_node()
        for node in nodes.items:
            print(f"Node Name: {node.metadata.name}")
            if node.status.addresses:
                for addr in node.status.addresses:
                    print(f" - {addr.type}: {addr.address}")
            else:
                print(" - 该节点的 addresses 列表为空，复现了 Issue 描述中的情况。")
    except Exception as e:
        print(f"获取 Node 信息失败: {e}")

    # 第七步：清理，用户可根据需要决定是否保留集群
    print("脚本执行完毕，如需清理可手动执行: kubeadm reset -f")

main()
```


---


## Issue #130393 probes do not run immediately

- Issue 链接：[#130393](https://github.com/kubernetes/kubernetes/issues/130393)

### Issue 内容

#### What happened?

When configuring probes for pods I expected that the probe runs immediately after waiting for the initialDelaySeconds.
but It only ran after initialDelaySeconds + periodSeconds


#### What did you expect to happen?

That the probe runs immediately after waiting for initialDelaySeconds.
for example: a probe after 60 seconds and an another one at 120 seconds.

#### How can we reproduce it (as minimally and precisely as possible)?

            startupProbe:
              initialDelaySeconds: 60
              periodSeconds: 60
              exec:
                command:
                  - /bin/sh

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.3", GitCommit:"9e644106593f3f4aa98f8a84b23db5fa378900bd", GitTreeState:"clean", BuildDate:"2023-03-15T13:40:17Z", GoVersion:"go1.19.7", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"30", GitVersion:"v1.30.7", GitCommit:"0c76c645d5a665cfeb736719b1cc47354193dc9a", GitTreeState:"clean", BuildDate:"2024-12-02T23:17:46Z", GoVersion:"go1.22.8", Compiler:"gc", Platform:"linux/amd64"}
WARNING: version difference between client (1.26) and server (1.30) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该 Issue 的描述来看，用户所遇到的问题是 “startupProbe 在等待 initialDelaySeconds 后，并没有立刻运行，而是隔了 periodSeconds 才执行”，属于预期与实际行为不一致的使用和配置问题。该问题和安全方面无直接关联，也不涉及凭据泄露、权限提升、代码执行、容器逃逸等安全风险场景。因此，根据给出的风险判断标准，此 Issue 不属于安全问题。

**解释说明：**

此问题是功能或使用方式上的疑问，并未涉及到任何安全机制或敏感信息披露，无需进行安全复现脚本编写。根据风险判断标准，该问题风险评级为“不涉及”，故不提供相应的 POC 脚本。若后续有安全相关的内容或漏洞出现，可再针对具体风险给出复现脚本。

---


## Issue #130377 PostStartHook "start-service-ip-repair-controllers" failed: unable to perform initial IP and Port allocation check

- Issue 链接：[#130377](https://github.com/kubernetes/kubernetes/issues/130377)

### Issue 内容

#### What happened?

Hi everyone,
I encountered an issue when restarting one of our API servers (v1.29.10). When I restart it, it never comes back up and remains unready. Its post-start hook fails with the following error:
`F0223 14:49:57.253137       1 hooks.go:203] PostStartHook "start-service-ip-repair-controllers" failed: unable to perform initial IP and Port allocation check`
Below is the output of its liveness endpoint (`https://127.0.0.1:6443/livez`):
```
curl -k https://127.0.0.1:6443/livez
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[-]poststarthook/start-service-ip-repair-controllers failed: reason withheld
[+]poststarthook/rbac/bootstrap-roles ok
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok

livez check failed
```

I also encounter the following errors on my currently running API server instances:

```
E0223 14:48:58.900100       1 repair.go:85] Operation cannot be fulfilled on servicenodeportallocations: the provided resource version does not match
E0223 14:48:59.153223       1 repair.go:127] Operation cannot be fulfilled on serviceipallocations: the provided resource version does not match
```
I also checked my etcd cluster, and everything is OK—there is no latency or I/O wait and read/write time is under 1 millisecond.

![Image](https://github.com/user-attachments/assets/a9bf48a9-2e7e-46f0-8b43-8f4d802dffbf)


#### What did you expect to happen?

I expect the API server to work correctly after a restart.

#### How can we reproduce it (as minimally and precisely as possible)?

I don't know how to reproduce this situation. I tried it in my staging environment, and everything was fine.

#### Anything else we need to know?

I also checked kube-apiserver code and realized that this error may relates to this part of code:
https://github.com/kubernetes/kubernetes/blob/v1.29.10/pkg/registry/core/rest/storage_core.go#L466
or this part:
https://github.com/kubernetes/kubernetes/blob/v1.29.10/pkg/registry/core/service/allocator/storage/storage.go#L203


#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.10
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.10
```

</details>


#### Cloud provider

<details>
self-hosted bare-metal using kubespray.
</details>


#### OS version

<details>

```console
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.3 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux controlplane1 5.15.0-92-generic #102-Ubuntu SMP Wed Jan 10 09:33:48 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>
kubespray
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd v1.7.22
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从描述来看，问题主要是 Kubernetes API Server 在启动时未能正常完成与 ServiceIP 和 NodePort 分配相关的检查，导致 PostStartHook “start-service-ip-repair-controllers” 失败。该问题更可能是某种资源版本不匹配等配置或逻辑导致的启动故障，暂未发现任何可利用的安全漏洞，也未见敏感信息泄露、命令执行、提权等风险场景，也没有任何外部攻击者可滥用的场景，因此该 Issue 不属于安全问题。

**解释说明：**

这是一个功能性/配置性问题，而非安全漏洞，不存在可导致非法访问或攻击场景的风险，因此无需提供复现脚本。若需进一步定位问题，可结合 Kubernetes 日志、etcd 状态等进行排查。该脚本栏保留为空，用于符合回答格式要求。

---


## Issue #130373 incorrect HTTP Verb  in apiserver log if a watch request uses the deprecated path pattern

- Issue 链接：[#130373](https://github.com/kubernetes/kubernetes/issues/130373)

### Issue 内容

#### What happened?

For a watch request using the depractead pattern, specifically, "/api/v1/watch/*" the log shows the HTTP verb as 'LIST', where it should be 'WATCH'.


```
HTTP verb="LIST" URI=“api/v1/watch/namespaces?resourceVersion=4550966175" Iatency="53m24.474837559s" userAgent="<>" audit-ID="<>"
srcIP="<>" apf_pl="workload-low" apf_fs="service-accounts" apf_iseats=1 apf_fseats=O apf_additionalLatency="Os" apf_init_latency="400.617ps" apf_execution_time="402.837ps" resp=200
```



#### What did you expect to happen?

For a watch request using the depractead pattern, specifically, "/api/v1/watch/*". the log should show the HTTP verb as 'WATCH'. just as those normal watch requets. For example:
```
HTTP verb="WATCH" URI="/api/v1/pods?watch=true" latency="8m18.00060226s" userAgent="<>" audit-ID="<>" srcIP="<>" apf_pl="workload-high" apf_fs="kube-scheduler" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_init_latency="280.462µs" apf_execution_time="281.612µs" resp=200
```



#### How can we reproduce it (as minimally and precisely as possible)?


1. kubectl get --raw "/api/v1/watch/namespaces"
2. check the http logs of apiserver for the reqeust.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Server Version: version.Info{Major:"1", Minor:"30", GitVersion:"v1.30.8-gke.1261000", GitCommit:"5061d8cd449c6a56827885cf24c5b6ee5492e0c9", GitTreeState:"clean", BuildDate:"2025-01-09T02:49:04Z", GoVersion:"go1.22.9 X:boringcrypto", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
v1.30.8-gke.1261000
</details>


#### OS version

<details>

not related

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该 Issue 的标题与内容来看，问题是 Kubernetes apiserver 在处理使用了已废弃的 "/api/v1/watch/*" 路径格式的 watch 请求时，日志中将其记录为 LIST 而不是 WATCH。该问题仅限于日志标记不准确，并不会造成越权访问、信息泄露、拒绝服务等安全风险，也并未导致敏感信息的错误暴露。因此，这属于功能层面的缺陷或日志格式问题，而非安全漏洞。

基于以上判断，本 Issue 并无实际安全威胁，也不会导致额外的潜在攻击面或敏感信息泄露，故风险评级可判断为“不涉及”。

**复现过程：**

```python
import os
import time
import ssl
import urllib3
import requests
from kubernetes import config

def main():
    """
    该脚本尝试访问 /api/v1/watch/namespaces 路径，复现 apiserver 对已废弃的 watch 请求路径进行处理的场景。
    脚本执行成功后，可以查看 apiserver 日志，验证其是否将请求错误记录为 LIST 而不是 WATCH。
    注意：
    1. 需要本地已安装并配置好 python-kubernetes 库和 kubeconfig，
       且默认位置 (一般 ~/.kube/config) 中可以正常访问 K8s 集群。
    2. 该脚本仅用于演示如何触发该日志问题，不涉及任何安全风险或漏洞利用。
    """

    # 忽略 InsecureRequestWarning
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

    # 加载默认 kubeconfig 配置
    config.load_kube_config()
    cfg = config.load_kube_config()
    # 获取当前默认配置中的主机、token 和 CA 证书信息
    current_configuration = config.kube_config.Configuration()
    host = current_configuration.host
    token = current_configuration.api_key.get('authorization')
    ca_cert = current_configuration.ssl_ca_cert

    # 如果 CA 或 token 为空，需要检查 kubeconfig 配置是否加载成功
    if not host or not token:
        print("无法从默认 kubeconfig 中获取 host/token 信息，请检查配置。")
        return

    # 构造请求会话
    session = requests.Session()
    # 设置认证头
    session.headers.update({"Authorization": token})

    # 如果 CA 证书存在，则进行验证，否则跳过证书验证
    verify_ssl = True
    if not ca_cert or not os.path.isfile(ca_cert):
        verify_ssl = False

    # watch 请求的 API 路径（已废弃），用来复现日志问题
    url = host.rstrip("/") + "/api/v1/watch/namespaces"

    try:
        # 发起 GET 请求，流式读取部分数据后退出，避免长时间阻塞
        print(f"向 {url} 发送 watch 请求...")
        response = session.get(url, stream=True, verify=verify_ssl, timeout=10)
        print(f"HTTP 响应状态码：{response.status_code}")
        if response.status_code == 200:
            line_count = 0
            # 仅示例性读取少量行以触发日志写入
            for line in response.iter_lines():
                if line:
                    line_count += 1
                    print("返回行：", line.decode('utf-8', errors='replace'))
                if line_count >= 5:
                    print("演示完成，主动退出。")
                    break
        else:
            print("请求未成功，请检查网络及账户权限。")

    except requests.exceptions.RequestException as e:
        print(f"请求出现异常：{e}")
```


**解释说明：**

• 上述脚本会加载本地默认的 kubeconfig，并尝试对 “/api/v1/watch/namespaces” 发起请求，以复现 Issue 中所述的日志问题(将应为 WATCH 的请求记录成 LIST)。  
• 脚本会用流式方式获取响应内容，并在读取若干行后主动终止，以防 watch 请求一直阻塞。  
• 需要注意的是，本脚本并不涉及任何实际安全风险，也没有造成越权、 数据泄露或权限提升等情况，仅用来演示如何触发该已废弃路径下的 watch 行为，以便在 apiserver 日志中观察其错误的 HTTP 动词标记。  
• 由于该问题仅为日志记录不准确，并不会影响 Kubernetes 集群的安全性，因此风险评级为“不涉及”。

---


## Issue #130369 e2e: remote.go fails to copy pod logs

- Issue 链接：[#130369](https://github.com/kubernetes/kubernetes/issues/130369)

### Issue 内容

#### What happened?

/var/log/pods permissons don't allow e2e runner to copy logs from **fedora-coreos** instances. It fails with this error:
```
I0213 15:47:50.870841  901711 ssh.go:146] Running the command scp, with args: [-o UserKnownHostsFile=/dev/null -o IdentitiesOnly=yes -o CheckHostIP=no -o StrictHostKeyChecking=no -o ServerAliveInterval=30 -o LogLevel=ERROR -i /home/ed/.ssh/google_compute_engine -r ed@34.169.181.247:/var/log/pods/ _artifacts/tmp-node-e2e-43c8c248-cos-beta-117-18613-164-28]
E0213 15:47:54.250634  901711 ssh.go:149] failed to run SSH command: out: scp: /var/log/pods/pods-4991_pod-logs-websocket-604bd213-08c1-43d3-a957-83f639a496bc_56c9e6de-c882-4880-87ce-1fb6ef8f19d0: Permission denied
, err: exit status 1
```

#### What did you expect to happen?

pod logs copied successfully


#### How can we reproduce it (as minimally and precisely as possible)?

Run e2e tests from linux machine this way:
```
kubetest2-gce --test=node --down=false -- --parallelism=1 --gcp-zone=us-west1-a --gcp-project=<put your project name here> --repo-root=. --image-config-file=/home/prow/go/src/k8s.io/test-infra/jobs/e2e_node/crio/latest/image-config-cgroupv1.yaml --delete-instances=false --test-args='--container-runtime-endpoint=unix:///var/run/crio/crio.sock --container-runtime-process-name=/usr/local/bin/crio --container-runtime-pid-file= --kubelet-flags="--cgroup-driver=systemd --cgroups-per-qos=true --cgroup-root=/ --runtime-cgroups=/system.slice/crio.service --kubelet-cgroups=/system.slice/kubelet.service" --extra-log="{\"name\": \"crio.log\", \"journalctl\": [\"-u\", \"crio\"]}"' --skip-regex='' --focus-regex='ImageGCNoEviction' --v 10 --timeout=300m 2>&1 | tee /tmp/log
```

I believe it's easily reproducible with kubetest as well.

#### Anything else we need to know?

The fix is underway.

#### Kubernetes version

<details>

```console
k/k@master
```

</details>


#### Cloud provider

<details>
google
</details>


#### OS version

<details>

not relevant

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该 Issue 的描述来看，出现的问题是 e2e 测试 runner 在尝试拷贝 /var/log/pods 目录下的日志文件时，由于权限不足而导致“Permission denied”错误。这属于测试环境或权限配置层面的问题，不存在可被恶意利用的安全漏洞，也并未暴露敏感信息或导致越权访问。该问题仅影响日志收集工作流程，不涉及安全漏洞利用场景。因此，本 Issue 不属于安全风险。

**复现过程：**

```python
# 本Issue不涉及安全风险，无需提供复现脚本
```


**解释说明：**

该问题是由于远程执行或复制日志时缺少相应权限导致，与安全漏洞无关。没有敏感信息泄露、没有权限提升、没有容器逃逸或命令执行风险，因此此类问题不构成安全风险。拷贝日志失败仅影响测试流程的完整性，不会导致系统被攻击或敏感数据被非法访问。由此判断该Issue为“不涉及”安全风险，无需提供复现脚本。

---


## Issue #130361 Running update-codegen FAILED

- Issue 链接：[#130361](https://github.com/kubernetes/kubernetes/issues/130361)

### Issue 内容

#### What happened?

```
[root@EulerOS kubernetes]# make update
Running in silent mode, run with SILENT=false if you want to see script logs.
Running in short-circuit mode; run with FORCE_ALL=true to force all scripts to run.
Running update-go-workspace
Running update-codegen
F0222 17:03:19.318481  152900 main.go:107] Error: failed making a parser: error(s) in "./staging/src/k8s.io/code-generator/examples/HyphenGroup/apis/example/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/HyphenGroup/apis/example/v1
error(s) in "./staging/src/k8s.io/code-generator/examples/MixedCase/apis/example/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/MixedCase/apis/example/v1
error(s) in "./staging/src/k8s.io/code-generator/examples/apiserver/apis/core":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/apiserver/apis/core
error(s) in "./staging/src/k8s.io/code-generator/examples/apiserver/apis/core/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/apiserver/apis/core/v1
error(s) in "./staging/src/k8s.io/code-generator/examples/apiserver/apis/example":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/apiserver/apis/example
error(s) in "./staging/src/k8s.io/code-generator/examples/apiserver/apis/example/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/apiserver/apis/example/v1
error(s) in "./staging/src/k8s.io/code-generator/examples/apiserver/apis/example2":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/apiserver/apis/example2
error(s) in "./staging/src/k8s.io/code-generator/examples/apiserver/apis/example2/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/apiserver/apis/example2/v1
error(s) in "./staging/src/k8s.io/code-generator/examples/apiserver/apis/example3.io":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/apiserver/apis/example3.io
error(s) in "./staging/src/k8s.io/code-generator/examples/apiserver/apis/example3.io/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/apiserver/apis/example3.io/v1
error(s) in "./staging/src/k8s.io/code-generator/examples/crd/apis/example/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/crd/apis/example/v1
error(s) in "./staging/src/k8s.io/code-generator/examples/crd/apis/example2/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/crd/apis/example2/v1
error(s) in "./staging/src/k8s.io/code-generator/examples/single/api/v1":
-: main module (k8s.io/code-generator) does not contain package k8s.io/code-generator/examples/single/api/v1
!!! [0222 17:03:19] Call tree:
!!! [0222 17:03:19]  1: /opt/kubernetes/hack/update-codegen.sh:885 codegen::deepcopy(...)
Running update-codegen FAILED
make: *** [Makefile:166: update] Error 1
```
When I run the make update command, an error is reported. How do we solve it?

#### What did you expect to happen?

The make update operation is successful.

#### How can we reproduce it (as minimally and precisely as possible)?

N/A

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.31.1
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该 Issue 的描述来看，主要是代码生成（update-codegen）过程中无法找到对应包而导致编译失败或执行错误。这属于常见的环境配置或依赖问题，未涉及任何潜在的敏感数据泄露、权限提升、远程代码执行等安全场景。Issue 中没有暴露敏感信息，也未体现可以被攻击者利用的漏洞，因此可以判断该问题不属于安全问题。

**复现过程：**

```python
# 该Issue不涉及安全漏洞，无需复现攻击或利用场景，故此处不提供复现脚本
```


---


## Issue #130353 Build the kube-addon-manager image with the fix of typo in function name being called in kube-addons.sh script

- Issue 链接：[#130353](https://github.com/kubernetes/kubernetes/issues/130353)

### Issue 内容

#### What happened?

When using kube-addon-manager image with version 9.1.7, it is calling the function which has a typo, and it was fixed in the kube-addons.sh script but an image was never built with the fix. So, the 9.1.7 still trying to call the function which has a typo in the name.

#### What did you expect to happen?

It should be calling the function with correct name.

#### How can we reproduce it (as minimally and precisely as possible)?

Bring up a container with kube-addon-manager version of 9.1.7 and check the logs of that container

Feb 18 17:30:21 nerdctl[8144]: /opt/kube-addons.sh: line 136: generate_prune_allowallowlist_flags: command not found
Feb 18 17:30:21  nerdctl[8144]: INFO: == Generated kubectl prune allowlist flags:  ==

#### Anything else we need to know?

Tried to build an new image with the fix in the kube-addons.sh script, but getting 404 not found errors for the debian packages which is the base image being used for this.

Fix for this:
https://github.com/kubernetes/kubernetes/commit/0f46033dca2d3596e3029a9fc9cf6755c106b37d

But getting below errors when trying to build an image with the fix.
#5 1.404 Err:6 http://security.debian.org/debian-security stretch/updates/main amd64 Packages
#5 1.404   404  Not Found [IP: 151.101.194.132 80]
#5 2.040 Err:9 http://deb.debian.org/debian stretch/main amd64 Packages
#5 2.040   404  Not Found
#5 2.090 Ign:10 http://deb.debian.org/debian stretch/main all Packages
#5 2.135 Err:11 http://deb.debian.org/debian stretch-updates/main amd64 Packages
#5 2.135   404  Not Found
#5 2.178 Ign:12 http://deb.debian.org/debian stretch-updates/main all Packages
#5 2.278 Reading package lists...

#### Kubernetes version

<details>

```console
$ kubectl version
# 1.31.6
```

</details>


#### Cloud provider

<details>
bare metal servers
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# NAME="Flatcar Container Linux by Kinvolk"
ID=flatcar
ID_LIKE=coreos
VERSION=4152.2.1
VERSION_ID=4152.2.1
BUILD_ID=2025-02-13-1820
SYSEXT_LEVEL=1.0
PRETTY_NAME="Flatcar Container Linux by Kinvolk 4152.2.1 (Oklo)"
ANSI_COLOR="38;5;75"
HOME_URL="https://flatcar.org/"
BUG_REPORT_URL="https://issues.flatcar.org"
FLATCAR_BOARD="amd64-usr"
$ uname -a
# Linux  6.6.74-flatcar 

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Containerd: 1.7.25
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
Calico: 3.28.0
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue内容来看，问题仅涉及kube-addon-manager脚本中函数名拼写错误导致调用不到正确的函数，这属于功能性错误或打包缺陷，并未提及任何可导致权限提升、命令执行、容器逃逸或凭据泄露等安全问题，因此没有体现出可被攻击者利用的风险点。

**复现过程：**

```python
# 该Issue描述的问题为功能性缺陷而非安全漏洞，无需复现脚本
# 在此空置占位，符合回答格式要求
pass
```


---


## Issue #130310 Secrets are outputted in random order

- Issue 链接：[#130310](https://github.com/kubernetes/kubernetes/issues/130310)

### Issue 内容

#### What happened?

`kubectl describe secret {your-secret}` should output keys in a stable order.

#### What did you expect to happen?

Keys are ordered in a stable order.

#### How can we reproduce it (as minimally and precisely as possible)?

Run this command a few times and notices that the order in not persistent:

`kubectl describe secret {your-secret}`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"26+", GitVersion:"v1.26.14-dispatcher", GitCommit:"929bbaf1bd77b0524b42b8d1ffb1d8410532b904", GitTreeState:"clean", BuildDate:"2024-03-11T22:18:01Z", GoVersion:"go1.21.7", Compiler:"gc", Platform:"darwin/arm64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"30", GitVersion:"v1.30.9-gke.1046000", GitCommit:"bd22bb26d389ceda2b24067faf3a6346c722a21d", GitTreeState:"clean", BuildDate:"2025-01-15T22:50:02Z", GoVersion:"go1.22.10 X:boringcrypto", Compiler:"gc", Platform:"linux/amd64"}
WARNING: version difference between client (1.26) and server (1.30) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
Google Cloud (GKE)
</details>


#### OS version

<details>

```console
$ uname -a
Darwin Mac.localdomain 24.3.0 Darwin Kernel Version 24.3.0: Thu Jan  2 20:23:36 PST 2025; root:xnu-11215.81.4~3/RELEASE_ARM64_T8112 arm64
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据Issue内容来看，主要问题是“kubectl describe secret”命令在输出时，Secret对象的键值顺序不稳定。此问题与安全漏洞或风险无关，仅仅是无序输出导致的使用体验问题，也并未涉及敏感信息泄露或其他潜在漏洞。因此，不存在实际安全风险。

**复现过程：**

```python
# 该Issue不涉及安全风险，无需编写复现脚本
```


**解释说明：**

该Issue描述的只是Secret键值输出顺序随机的问题，不会导致敏感信息泄露、访问控制绕过或其他安全风险。故风险评级为“不涉及”，无需提供安全相关的复现脚本。

---


## Issue #130300 CSI Volume fails to remount after kubelet exits abnormally on Windows

- Issue 链接：[#130300](https://github.com/kubernetes/kubernetes/issues/130300)

### Issue 内容

#### What happened?

The CSI driver mount may show this error which prevents the volume to be mounted to the pod on Windows.

```
E0215 06:30:42.244047    4176 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/csi.vsphere.vmware.com^3a8ec4cb-cd82-4e6a-826a-3ff0b998d347-4f75432b-3c45-42a4-b205-176a45bf7237 podName: nodeName:}" failed. No retries permitted until 2025-02-15 06:32:44.2440476 -0800 PST m=+373.590890801 (durationBeforeRetry 2m2s). Error: MountVolume.MountDevice failed for volume "pvc-4f75432b-3c45-42a4-b205-176a45bf7237" (UniqueName: "kubernetes.io/csi/csi.vsphere.vmware.com^3a8ec4cb-cd82-4e6a-826a-3ff0b998d347-4f75432b-3c45-42a4-b205-176a45bf7237") pod "win-webserver-new-57df95657d-58p96" (UID: "bc613216-d761-4d4f-8677-b00249bff5fd") : kubernetes.io/csi: attacher.MountDevice failed to create dir "\\var\\lib\\kubelet\\plugins\\kubernetes.io\\csi\\csi.vsphere.vmware.com\\a2d2504ecac0aea11a8ddbfe72ebd253579aa79a98c6133b71a9a26200d740b8\\globalmount":  path \var\lib\kubelet\plugins\kubernetes.io\csi\csi.vsphere.vmware.com\a2d2504ecac0aea11a8ddbfe72ebd253579aa79a98c6133b71a9a26200d740b8\globalmount exists but is not a directory
```

The current logic to detect whether the volume is a mount point on Windows is not reliable any longer.


#### What did you expect to happen?

kubelet should ensure the CSI volume mounted correctly on Windows after crashing.

#### How can we reproduce it (as minimally and precisely as possible)?

Relaunch kubelet after CSI volume is mounted.

#### Anything else we need to know?

This is related to the volume mount point check on Windows.

https://github.com/kubernetes/kubernetes/blob/c2529e844395f8895ae809fa1a5775ea8181fd20/pkg/util/filesystem/defaultfs.go#L93-L94

This didn't work on Windows with NTFS.
A more reliable way to do the check is recommended by Windows documentation
https://learn.microsoft.com/en-us/windows/win32/fileio/determining-whether-a-directory-is-a-volume-mount-point
as demonstrated in this csi-proxy PR.
https://github.com/kubernetes-csi/csi-proxy/blob/c55b906bb336e30e8074a41a2063543647fb6c91/pkg/os/volume/api.go#L408-L447

Another approach could be checking `ModeIrregular` from `Lstat`.

#### Kubernetes version

<details>

```console
$ kubectl version
```

</details>

1.31.4 and 1.32.x


#### Cloud provider

<details>
vSphere
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
Microsoft Windows Server 2022 Standard
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该 Issue 的描述来看，这是一个针对 Windows 平台上 CSI 卷在 kubelet 异常退出后无法正常重新挂载的功能性/兼容性问题，反映了当前检测挂载点逻辑在 Windows 下不可靠，导致在某些场景中无法正确完成卷的挂载。该问题本质上是功能性缺陷或兼容性问题，并未涉及到敏感信息泄露、命令执行、权限提升、容器逃逸等可直接被攻击者利用的安全漏洞，也无证据表明会造成无法预期的拒绝服务攻击或其他加以利用的安全风险。因此，本 Issue 不涉及安全层面的风险。

**复现过程：**

```python
# 本Issue不涉及安全风险，无需提供复现脚本
# 该问题属于功能/兼容性问题，不影响安全
```


---


## Issue #130299 Different IP same port problem

- Issue 链接：[#130299](https://github.com/kubernetes/kubernetes/issues/130299)

### Issue 内容

#### What happened?

There has been a recurring issue with Clearml sessions where, even if the IP addresses are different, having the same port causes SSH login sessions to be mismatched (resulting in login failures due to incorrect passwords).

Additionally, we observed that different LLM inference services running on different IPs but using the same port could receive requests intended for another service.

This might be related to how Kubernetes handles NodePort, but further investigation is needed. According to the principles of NodePort operation in Kubernetes, this issue should not occur. However, changing the configuration to work around this problem would increase management complexity.

what can I fix this problem?

#### What did you expect to happen?

Different IP same port can not receive requests intended for another service.
and SSH login sessions to be success.

#### How can we reproduce it (as minimally and precisely as possible)?

Sometimes it happens accidentally, we have to reopen other sessions to solve this problem.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue描述来看，实际问题是不同IP却使用相同端口时，出现了服务请求或SSH登录会话被错误路由到其他服务的现象。根据提示，这更可能是Kubernetes NodePort配置或网络环境中出现的路由/负载均衡异常，属于环境或配置层面的冲突或错误，而非项目本身存在的安全漏洞。从描述并不能得出存在可被恶意攻击者利用的高风险安全问题，也没有提到攻击者如何能利用这一情况进行提权、命令执行或越权访问等典型高危场景。

根据“Issue风险判断标准”中的第4条和整体描述，本质上这是一个配置或环境问题，而并非项目自身代码导致的安全漏洞；同时也未见显式的敏感信息泄露或其他可导致高风险安全后果的情形。因此，本Issue不属于安全风险或仅与用户的实际配置有关，不涉及项目本身的安全漏洞。

**解释说明：**

根据Issue描述，此问题主要与Kubernetes NodePort或网络配置相关，是环境或配置场景所导致的端口冲突或路由异常，并非项目本身的安全漏洞，因此风险评级为“不涉及”，无需编写PoC脚本。若要修复或避免此类问题，可重点排查Kubernetes中的Service配置、NodePort管理方式以及底层网络插件（CNI）是否工作正常，或考虑使用不同端口以及正确的路由规则来隔离各自服务实例。若怀疑是Kubernetes层面Bug，可进一步定位日志和网络流转过程，但并不涉及项目功能的安全漏洞。

---


## Issue #130296 Bad hugepage request validation for hugepage volume mount

- Issue 链接：[#130296](https://github.com/kubernetes/kubernetes/issues/130296)

### Issue 内容

#### What happened?

When setting a hugepage volume mount in a container, the validation is not checking if the 

When setting a hugepage volume mount in a container, the validation is incorrectly checking if the requested `volumeMount` has a corresponding hugepage resource request. It is not searching within the container that declares the `volumeMount`, but against all the containers in the pod spec when searching for the medium's page size, allowing to bypass the validation and deploy a pod with a hugepage configuration that does not work.

https://github.com/kubernetes/kubernetes/blob/1b2262961563cf590597f5c4c210099e53f80530/pkg/volume/emptydir/empty_dir.go#L407-L432

This can cause confusion to the user, because of not having a visible error that explains why the containers are not able to use hugepages.

#### What did you expect to happen?

The validation logic should check if the hugepage `volumeMount` has a corresponding resource request only inside the container to which it belongs.

#### How can we reproduce it (as minimally and precisely as possible)?

It is possible to bypass the volume mount check by requesting hugepages in a different container from where the `volumeMount` was declared:
```
apiVersion: v1
kind: Pod
metadata:
  name: bad-volume-mount-validation
spec:
  containers:
  - name: hugepage-1
    image: busybox
    command: ["sh", "-c", "sleep infinity"]
    resources:
      limits:
        memory: 100Mi
        hugepages-2Mi: 128Mi
  - name: hugepage-2
    image: busybox
    command: ["sh", "-c", "sleep infinity"]
    volumeMounts:
    - mountPath: /hugepages
      name: hugepage
  volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
1.32
```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
N/A
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 主要描述了在 Kubernetes 中对 hugepage volume mount 进行校验时，逻辑会错误地在 Pod 的所有容器而非该容器本身内搜索对应的 hugepage 资源请求，从而导致用户在部署时产生困惑或误配置。根据目前的信息，这个问题并未被描述为可以导致越权访问、提权、敏感信息泄露、远程命令执行或任何可被攻击者利用的严重后果，实际影响仅表现为逻辑和配置层面的混淆或错误，不会使攻击者获得额外权限或能力。

综上，此问题并不涉及具体的安全攻击面，也没有说明可被攻击者利用来发起 DoS 或其他高风险攻击，因此并不属于安全漏洞。

**解释说明：**

该 Issue 涉及的错误校验逻辑仅会导致用户配置时出现混淆，并不会带来可被攻击者利用的安全后果，故不视为安全风险。通常只需要修正相关校验函数，将 volumeMount 的有效性限定在声明了相应资源的容器范围内即可解决此类问题。该问题仅属于功能或可用性层面的缺陷。

---


## Issue #130292 SSA with force conflicts should update Service

- Issue 链接：[#130292](https://github.com/kubernetes/kubernetes/issues/130292)

### Issue 内容

#### What happened?

We have a controller in our clusters with a Service as an owned resource. A user (multi-tenant cluster) modified one of these services to simulate an application failure (chaos testing). The change triggered our controller, but the controller's attempt to correct the drift in the actual state was rejected by the API server. This error persisted until we manually corrected the Service.

Our controller is migrated to use SSA for all API operations and is configured to force conflicts (as controllers should do).

#### What did you expect to happen?

The expected behavior is that the controller's SSA request (with force conflicts) should immediately revert the drift of the Service's actual state without any error.

#### How can we reproduce it (as minimally and precisely as possible)?

Given the controller and user desired state of a Service:

````
$ cat controller-service.yaml 
apiVersion: v1
kind: Service
metadata:
  name: app
spec:
  ports:
    - name: app
      protocol: TCP
      port: 8080
      targetPort: 8080

$ cat user-service.yaml 
apiVersion: v1
kind: Service
metadata:
  name: app
spec:
  ports:
    - name: app
      protocol: UDP
      port: 8080
      targetPort: 8080

$ dyff between controller-service.yaml user-service.yaml --omit-header
spec.ports.app.protocol
  ± value change
    - TCP
    + UDP
````

First apply the Service as the controller:

````
$ k apply -f controller-service.yaml --field-manager controller --server-side --force-conflicts 
service/app serverside-applied
````

Then update the Service as the user:
````
$ k apply -f user-service.yaml 
Warning: resource services/app is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
service/app configured
````

Then try to apply the Service as the controller again:
````
$ k apply -f controller-service.yaml --field-manager controller --server-side --force-conflicts 
The Service "app" is invalid: spec.ports[1].name: Duplicate value: "app"
````

#### Anything else we need to know?

[I asked about this on Slack](https://kubernetes.slack.com/archives/C0EG7JC6T/p1739537899673079) before creating this issue.

#### Kubernetes version

OpenShift 4.16.24 is our current target cluster, where this incident occurred. Reproduced using `kubectl` in a local k3s ephemeral cluster.

<details>

```console
$ kubectl version
$ k version
Client Version: v1.29.7
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.32.1+k3s1
WARNING: version difference between client (1.29) and server (1.32) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

On-prem

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据该Issue的描述，问题主要在于Kubernetes控制器在使用SSA（Server-Side Apply）并启用了“force conflicts”后，无法正常覆盖用户对Service资源的手动修改，从而导致了“spec.ports[1].name: Duplicate value: 'app'”的报错。  
从安全角度分析，这个问题并不涉及到敏感信息泄露、权限提升、远程代码执行、容器逃逸等常见的高危漏洞场景，仅属于业务逻辑或资源配置冲突的问题，不会对系统造成安全隐患。  
因此，该Issue不属于安全风险。

**解释说明：**

此问题为在多租户环境下对Service资源进行SSA操作时产生的冲突，与安全风险无关，不涉及任何可能导致系统被攻击或被利用的安全漏洞，因此不需要提供复现脚本。  
该问题的严重性更多是对于业务可用性或管理员操作体验的影响，而非对服务的保密性、完整性和可用性的严重影响。故风险评级为“不涉及”。

---


## Issue #130288 RestartPolicy doesn't make sense for static pods

- Issue 链接：[#130288](https://github.com/kubernetes/kubernetes/issues/130288)

### Issue 内容

/kind bug

Static pods should only ever have a restart policy of always. Anything else doesn't make sense, since the Kubelet doesn't track the pod status in a persistent way.

I don't think we can fail validation for backwards-compatibility, but maybe we can just unconditionally overwrite the restart policy when static pods are parsed.

/sig node

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据Issue的描述，“RestartPolicy doesn't make sense for static pods”仅涉及到如何在静态Pod中配置重启策略的逻辑，并未提出任何与安全相关的攻击场景、敏感信息泄露或可导致高危漏洞（如远程命令执行、容器逃逸等）的内容，亦未涉及低权限用户可滥用等问题。因此，该Issue仅是对Kubernetes功能行为的讨论，并不构成项目层面的安全问题。

**复现过程：**

```python
# 该 Issue 不涉及安全风险，无需安全复现脚本
# 如仅需验证重启策略逻辑，可在测试环境中编写简单脚本查看行为，但与安全无关
print("This issue does not involve security risks. No POC needed.")
```


---


## Issue #130275 Error messages make debugging unnecessarily complex

- Issue 链接：[#130275](https://github.com/kubernetes/kubernetes/issues/130275)

### Issue 内容

#### What happened?

An error message from the `OwnerReferencesPermissionEnforcement` plugin:

> cannot set blockOwnerDeletion if an ownerReference refers to a resource you can't set finalizers on

The word "you" makes debugging unnecessarily complex, especially when the error concerns a PVC generated for a Pod in a StatefulSet generated by an operator. There is normally a service account behind the "you", so its name could just be added to the logs instead. 

#### What did you expect to happen?

I'd expect the error message to be 

> cannot set blockOwnerDeletion if an ownerReference refers to a resource `system:serviceaccount:namespace:service-account` can't set finalizers on


#### How can we reproduce it (as minimally and precisely as possible)?

No specific reproduction, but that specific error message is at https://github.com/kubernetes/kubernetes/blob/f0077a3689669018557de723a0416fda267d132f/plugin/pkg/admission/gc/gc_admission.go#L149.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.32.1
Kustomize Version: v5.5.0
Server Version: v1.31.4-dirty
```

</details>


#### Cloud provider

None


#### OS version




#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)




#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue描述来看，这个问题仅涉及错误日志输出信息不够直观、对调试带来额外困难，与常见的安全问题（如越权访问、命令执行、提权、敏感信息泄露等）无关，也并未涉及不当配置或潜在的安全利用途径。因此该Issue不涉及任何潜在安全风险。

**复现过程：**

```python
# 本Issue并非安全问题，无需复现脚本。
# 如需在本地测试正常的Kubernetes日志输出，可参照以下简单示例，但此示例与安全风险无关。

import time
import logging

def main():
    logging.basicConfig(level=logging.INFO)
    logging.info("此处仅演示一般日志输出，不涉及安全风险。")
    time.sleep(2)
    logging.info("本脚本执行结束。")

main()
```


---


## Issue #130271 Kubelet's e2e tests need a new mechanism for private image pull tests

- Issue 链接：[#130271](https://github.com/kubernetes/kubernetes/issues/130271)

### Issue 内容

#### What happened?

Current private image pull e2e tests for kubelet rely on hardcoded credentials to a public repo that is getting decommissioned as a part of https://github.com/kubernetes/k8s.io/issues/1469. These tests will permafail once that happens.

#### What did you expect to happen?

The tests should be written in such a way that they don't depend on external infrastructure. A possible solution might be to stand up an in-cluster registry and populate it with the necessary images.

#### How can we reproduce it (as minimally and precisely as possible)?

Wait for https://github.com/kubernetes/k8s.io/issues/1469 to finish and try running some of the tests that rely on the secret from https://github.com/kubernetes/kubernetes/blob/bdc15f9e19c9331fff685ea4ea264f07605a1fcb/test/e2e/common/node/runtime.go#L284-L291 like the `ImageCredentialProvider` tests.

**Node conformance** test `[sig-node] Container Runtime blackbox test when running a container with a new image should be able to pull from private registry with secret [NodeConformance]` is among those that will be affected.

#### Anything else we need to know?

Raised by @BenTheElder in a couple places, like https://github.com/kubernetes/kubernetes/pull/128372#discussion_r1918965807

#### Kubernetes version

<details>
all supported
</details>


#### Cloud provider

<details>
irrelevant
</details>


#### OS version

<details>

```console
--
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue描述来看，该问题主要是Kubernetes的e2e测试用例依赖了一个即将被废弃的外部镜像仓库以及对应的测试凭证，一旦该公共仓库被移除，测试会因为无法拉取镜像而失败。此问题本质上是测试环境依赖公共仓库且凭证被硬编码的问题，并不涉及可被恶意利用的安全漏洞，也未提及可能导致权限提升、命令执行、容器逃逸等严重风险场景，故不属于项目自身的安全缺陷。

---


## Issue #130263 Statefulset: cannot update spec.revisionHistoryLimit

- Issue 链接：[#130263](https://github.com/kubernetes/kubernetes/issues/130263)

### Issue 内容

#### What happened?

We cannot update/change the revisionHistoryLimit of a STS. #56341 looks solved(?) but might have regressed since then.

#### What did you expect to happen?

Modifications to the `spec.revisionHistoryLimit` field are allowed for STS.

#### How can we reproduce it (as minimally and precisely as possible)?

- Create STS
- Modify its `spec.revisionHistoryLimit` field
- See error message

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.2
Kustomize Version: v5.5.0
Server Version: v1.31.4
```

</details>


#### Cloud provider

<details>
None/Bare metal
</details>


#### OS version

<details>

not relevant

</details>


#### Install tools

<details>
not relevant
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
not relevant
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
not relevant
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
根据Issue的描述，该问题仅涉及在StatefulSet中更新spec.revisionHistoryLimit字段失败，属于Kubernetes在配置或功能上的限制或潜在Bug，与安全漏洞无关。Issue中并未涉及任何敏感信息泄露、权限提升、远程命令执行、容器逃逸等安全问题，因此不属于安全风险范畴。

**复现过程：**

```python
# 该Issue不涉及安全风险，因此无需提供安全漏洞复现脚本
# 本段内容仅为空示例，标明本Issue并非安全问题
def main():
    print("该Issue并非安全风险，无需安全漏洞复现脚本。")
    
main()
```


---


## Issue #130261 The pod created by the job is in the Complate state, and the GC is not reclaimed.

- Issue 链接：[#130261](https://github.com/kubernetes/kubernetes/issues/130261)

### Issue 内容

#### What happened?

```shell
[root@master1-LMr87cx7 ~]# kubectl get pod managerrestoretool-l4qcj -nmanager -oyaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    network.alpha.kubernetes.io/network: '[{"name":"custom-default","interface":"eth0"}]'
  creationTimestamp: "2025-02-15T06:14:06Z"
  generateName: managerrestoretool-
  labels:
    app: managerrestoretool
    batch.kubernetes.io/controller-uid: a8b28b98-6359-4efc-9660-66b20159e98e
    batch.kubernetes.io/job-name: managerrestoretool
    controller-uid: a8b28b98-6359-4efc-9660-66b20159e98e
    job-name: managerrestoretool
  name: managerrestoretool-l4qcj
  namespace: manager
  ownerReferences:
  - apiVersion: batch/v1
    blockOwnerDeletion: true
    controller: true
    kind: Job
    name: managerrestoretool
    uid: a8b28b98-6359-4efc-9660-66b20159e98e
  resourceVersion: "155221"
  uid: d888d8a1-30ee-4d4b-af3a-f245dd17a2f1
[root@master1-LMr87cx7 ~]# kubectl get pod managerrestoretool-l4qcj -nmanager
NAME                       READY   STATUS      RESTARTS   AGE
managerrestoretool-l4qcj   0/1     Completed   0          4d2h
[root@master1-LMr87cx7 ~]# kubectl get job -A
No resources found
[root@master1-LMr87cx7 ~]# 
```
The pod created by the job is in the Completed state and the corresponding job has been deleted. However, the pod is not reclaimed by GC controller. In addition, the GC controller does not print any related logs. This is a bug.

#### What did you expect to happen?

After a job is deleted, pods in the Completed state should be reclaimed by the GC controller.

#### How can we reproduce it (as minimally and precisely as possible)?

N/A

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.31
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从已有信息来看，该 Issue 仅描述了 Kubernetes GC 控制器未正常回收已删除 Job 所对应的已完成状态 Pod，属于资源回收和状态管理上的问题，并未体现任何可导致权限提升、信息泄露、命令执行或其他安全层面的漏洞。根据提供的内容和风险判断标准，这个问题并不涉及安全风险。

**解释说明：**

该问题属于 Kubernetes 集群资源清理的功能性缺陷，无法导致攻击者获取更高权限、泄露敏感信息或发起安全攻击，因此无需提供安全漏洞复现脚本。此类 Issue 的修复可通过改进或修正 Kubernetes GC 控制器在删除 Job 后自动回收其已完成状态的 Pod 来实现。若需要针对功能性问题编写测试脚本，可使用现有 CI/CD 流程和单元测试进行验证，但并不涉及安全风险。

---


## Issue #130239 Error deleting subpath-volumes/container-name dir, casing related Pod stuck in deleting

- Issue 链接：[#130239](https://github.com/kubernetes/kubernetes/issues/130239)

### Issue 内容

#### What happened?

Cannot remove a Pod with subPath volumes, and get related logs from kubelet like:
```
Feb 17 15:03:05 iZ6wecx4y9bqgkpeqekcivZ kubelet[3138]: W0217 14:03:05.239516    3138 mount_helper_common.go:34] Warning: mount cleanup skipped because path does not exist: /var/lib/kubelet/pods/9393787f-a767-4a7e-a96a-62a326fa122f/volume-subpaths/<VOLUME_NAME>/<CONTAINER_NAME>/<INDEX>
...
Feb 17 15:03:05 iZ6wecx4y9bqgkpeqekcivZ kubelet[3138]: E0217 14:03:05.239631    3138 nestedpendingoperations.go:348] Operation for "{xxxxxxxx. Error: error cleaning subPath mounts for volume "<VOLUME_NAME>" (UniqueName: "kubernetes.io/csi/ossplugin.csi.alibabacloud.com^<VOLUME_NAME>") pod "9393787f-a767-4a7e-a96a-62a326fa122f" (UID: "9393787f-a767-4a7e-a96a-62a326fa122f") : error deleting /var/lib/kubelet/pods/9393787f-a767-4a7e-a96a-62a326fa122f/v<VOLUME_NAME>/<CONTAINER_NAME>: remove /var/lib/kubelet/pods/9393787f-a767-4a7e-a96a-62a326fa122f/<VOLUME_NAME>/<CONTAINER_NAME>: directory not empty
```
And  `kubernetes.io/csi/ossplugin.csi.alibabacloud.com` is a driver for object storage on Alibaba Cloud, that is , it's a CSI driver for remote storage. I think this happens because:
1. When mounting with subPath, kubelet will create a dir `<KUBELET_DIR>/pods/<POD_UID>/<VOLUME_NAME>/mount`, and mount the remote storage on it through plugin like CSI.
2.  Then, create a subPath remotely if not exists. 
3.  Kubelet will create a dir `<KUBELET_DIR>/pods/<POD_UID>/<VOLUME_NAME>/<CONTAINER_NAME>/<INDEX>`,  and mount `<KUBELET_DIR>/pods/<POD_UID>/<VOLUME_NAME>/mount/subpath` on it.
4. However, different with the local filesystem, users can always remove "dir" (for object storage, just some objects with the same prefix) remotely. 
5. If users remove subPath remotely, when try to clean the subpath during deleting Pod as https://github.com/kubernetes/kubernetes/blob/931ad2a9fdedaf1e47126f5b3e5880eb3708bfb2/pkg/volume/util/subpath/subpath_linux.go#L236 kubelet will try Stat the `...<CONTAINER_NAME>/<INDEX>` first (a mountpoint, actually access remote storage), and get NOT EXIST error, then skip an unmount  or removal, just keep removing `.../<CONTAINER_NAME>` (not a mountpoint) and find sthe <INDEX> dir created before, and throws NOT EMPTY error. Just like the error logs I put above.

#### What did you expect to happen?

Successfully remove the Pod with no unexpected paths under kubelet-dir left.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a remote storage volume and mounts it with subpath mode, then remove the related subpath remotely, then delete the pod. It will stuck in Completed or Terminating.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
1.31.1-aliyun.1
```

</details>


#### Cloud provider

<details>
Alibaba Cloud, ACK
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
Code changes may also effect on CSI
</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从该 Issue 的描述来看，这是一个由于在使用远程存储（如阿里云 OSS CSI Driver）并启用了 subPath 功能时，如果用户在远程端手动删除了子目录（subPath 所对应的“目录”对象），Kubelet 清理本地挂载点时会出现无法正常卸载并删除的情况，导致 Pod 卡在 Completed 或者 Terminating 状态。  
这属于子路径挂载在使用远程存储时的功能性或兼容性问题，并不涉及凭据泄露、越权访问或命令执行等安全风险，也并未出现攻击者可利用的高危情形。因此，该问题更像是一个功能缺陷或流程 BUG，而非安全漏洞。

**复现过程：**

```python
# 该问题不涉及安全漏洞，无需提供复现脚本
```


---


## Issue #130235 CRD: discovery inconsistent with storage due to a race

- Issue 链接：[#130235](https://github.com/kubernetes/kubernetes/issues/130235)

### Issue 内容

#### What happened?

While troubleshooting https://github.com/kubernetes/kubernetes/issues/130148, I discovered that API discovery for CRDs might be serving data inconsistent with the underlying storage for a short period. This is likely caused by the fact that the `crdHandler` and `DiscoveryController` don't appear to sync on storage/discovery for the same CRD.

#### What did you expect to happen?

I expected that storage version would always match the "storageVersionHash" at the CRD's API group discovery endpoint (e.g. h/apis/stable.example.com/v2)

#### How can we reproduce it (as minimally and precisely as possible)?

1. create a CRD with 2 different versions - v1 (served, storage) and v2 (served)
2. update the CRD versions so that: v1 (served), v2 (served, **storage**)
3. wait until the `/apis/<yourapigroup>/v2` starts broadcasting your CRD-defined resource's "storageVersionHash" matching `<yourapigroup>/v2` as calculated by `staging/src/k8s.io/apiserver/pkg/endpoints/discovery.StorageVersionHash`
4. immediatelly create a resource that you defined in your CRD
5. retrieve that resource directly from etcd and check its API version - should be v2 but it's actually v1 in about 30% of the cases

#### Anything else we need to know?

I wrote an integration test to prove this race exists. I'll attach the patch in the comments.

#### Kubernetes version

<details>

```console
master
```

</details>


#### Cloud provider

<details>
irrelevant
</details>


#### OS version

<details>

```console
Linux 6.12.7-arch1
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue内容来看，描述的主要问题是当CRD资源的版本变更后，短时间内API的discovery信息与实际底层存储不一致，导致在更新后的短时间窗口里，存储可能仍然使用旧版本。这属于典型的读写一致性问题或数据竞争（race condition）导致的功能性缺陷，并不涉及凭据泄露、越权访问或命令执行等安全问题，也不属于拒绝服务或其他常见范畴的安全风险。该问题不具备被攻击者利用从而影响系统安全或造成信息泄露、提权的条件，因此可判断为不涉及安全风险。

**解释说明：**

该Issue并未反映出可被恶意利用的安全漏洞，属于数据一致性问题，因此无需编写安全漏洞复现脚本。若有需要测试该问题，可针对CRD在Kubernetes集群上的创建、更新和读取过程编写测试脚本来验证数据一致性的竞争窗口，但这不属于安全复现POC范畴。该问题更多是功能逻辑上的稳定性与一致性改进需求，并非安全漏洞。

---


## Issue #130229 Check whether etcd needs to be checked for the livez interface of APIServer.

- Issue 链接：[#130229](https://github.com/kubernetes/kubernetes/issues/130229)

### Issue 内容

#### What happened?

Apiserver is used as a static pod, and the live probe is configured as the livez interface. When other processes occupy too much disk I/O for a short time, the etcd processing efficiency decreases. As a result, the live probe of apiserver fails and the pod restarts. In the scenario where the disk I/O deteriorates, the apiserver process should not exit, and its interface may respond slowly. Therefore, should the livez interface not check whether etcd is normal?

#### What did you expect to happen?

When the disk I/O deteriorates, the livez interface returns a normal response.

#### How can we reproduce it (as minimally and precisely as possible)?

N/A

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.31
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
从Issue描述来看，主要问题是当磁盘I/O负载过高时，etcd处理变慢导致APIServer的livez检查失败，进而出现Pod重启现象，这属于对可用性和稳定性的讨论。Issue中并未提及任何可能导致攻击者利用的安全问题，例如代码注入、越权访问、凭据泄露等，也没有对外暴露未授权访问或提供可利用的接口。从现有信息判断，这更像是探针配置或架构设计导致的可靠性问题，不属于项目本身存在的安全漏洞或错误配置，被攻击者利用的可能性较低。

根据给定的风险判断标准，未发现可导致命令执行、容器逃逸、敏感数据泄露等高等级安全风险点，也不符合拒绝服务（DoS）攻击的典型场景（Issue并未说明该问题可被恶意直接触发，更多是因磁盘I/O竞用引起的访问延迟）。因此可以认定该Issue并不存在安全问题。

**解释说明：**

Issue中的问题是可用性与运维配置的问题，不属于安全漏洞，因此无需提供POC脚本。该问题并未体现出可被攻击者利用的安全风险。若后续出现对该问题的恶意利用细节，再考虑其是否构成安全漏洞并进行相应升级判定。

---


## Issue #130201 Commit b31e779 added a blocker to dead code elimination by the Go compiler

- Issue 链接：[#130201](https://github.com/kubernetes/kubernetes/issues/130201)

### Issue 内容

#### What happened?

Commit b31e779 by @p0lyn0mial uses `cmp.Diff()` which depends on `reflect.Type.Method(n int)`. Upon encountering the use of this method, the go compiler disables the dead code elimination.

The commit message for b31e779 indicates that 
> The consistency check is meant to be enforced only in the CI, not in production.

Please fix the consistency check to disable the DCE only in CI, not in the production.

#### What did you expect to happen?

Debug code must not pessimise the code generation for prod environments.

#### How can we reproduce it (as minimally and precisely as possible)?

You can use `go build -ldflags '-dumpdep'` to have the compiler dump the dependency graph of linked symbols. Any symbol marked with `<ReflectMethod>` disables the DCE.

#### Anything else we need to know?

_No response_

#### Kubernetes version

b31e7793d0d873a71c90caf8455556aa905cf88d

#### Cloud provider

any

#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### 分析结果

**风险定级：**  
不涉及

**判断依据：**  
该 Issue 讨论的是一个涉及 Go 编译器死代码消除（DCE）失效的问题。具体原因是由于使用了 reflect.Type.Method(n int) 导致编译器无法进行 DCE，从而在生产环境中也带来冗余代码。通篇内容并未涉及任何敏感信息泄漏、权限提升、命令执行、容器逃逸或其他高风险操作，也不属于不当操作或不当配置导致的安全风险。综合判断，此问题仅与编译器优化有关，不会直接导致安全漏洞。

---


