# Issue å®‰å…¨åˆ†ææŠ¥å‘Š

# ğŸš¨ å­˜åœ¨å®‰å…¨é£é™©çš„ Issues (7 ä¸ª)

## Issue #125619  endpoints cannot be changed from notReadyAddresses to addresses

- Issue é“¾æ¥ï¼š[#125619](https://github.com/kubernetes/kubernetes/issues/125619)

### Issue å†…å®¹

#### What happened?

I have a k8s clusters that have encountered the following situation: 
1ã€All master nodes are full of memory burst
2ã€After the memory of all master nodes is restored, the Ready type status of pod is true,  but some endpoints in the subsets still remain in notReadyAddresses and do not recover. 
service:
```yaml
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: "2024-05-29T07:42:43Z"
  name: nginx-service
  namespace: default
  resourceVersion: "412201"
  uid: 17f43ff9-16fe-4507-a593-ede1a29a1255
spec:
  clusterIP: 123.123.95.164
  clusterIPs:
  - 123.123.95.164
  externalTrafficPolicy: Cluster
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    app: nginx
```
enpoints:
```yaml
apiVersion: v1
kind: Endpoints
metadata:
  annotations:
    endpoints.kubernetes.io/last-change-trigger-time: "2024-06-12T02:41:41Z"
 creationTimestamp: "2024-05-29T07:42:43Z"
  name: nginx-service
  namespace: default
  resourceVersion: "468584"
  uid: 85b1134f-7f5d-4087-b76f-e7aa5371c016
subsets:
- notReadyAddresses:
  - ip: 172.22.161.17
    nodeName: 192.168.3.89
    targetRef:
      kind: Pod
      name: nginx
      namespace: default
      uid: 6022f2d6-ead8-4882-9a76-8e51901731ee
  ports:
  - port: 80
    protocol: TCP
```

pod
```yaml
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2024-05-29T07:42:43Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2024-05-29T07:42:49Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2024-05-29T07:42:49Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2024-05-29T07:42:43Z"
    status: "True"
    type: PodScheduled
```

3ã€The issue is resolved only by restarting kube-controller-manager.


/area controller-manager 

#### What did you expect to happen?

When all master node memory is restored, endpoints change from `notReadyAddresses` to `addresses`

#### How can we reproduce it (as minimally and precisely as possible)?

Refer to the description of "What happened?"

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
1.28.3
```

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

æ¶‰åŠå®‰å…¨é£é™©ã€‚

**åˆ†æï¼š**

è¯¥Issueæè¿°äº†ä¸€ä¸ªé—®é¢˜ï¼Œå½“æ‰€æœ‰masterèŠ‚ç‚¹å†…å­˜è€—å°½ï¼ˆâ€œAll master nodes are full of memory burstâ€ï¼‰å¹¶æ¢å¤åï¼Œè™½ç„¶Podçš„ReadyçŠ¶æ€ä¸ºTrueï¼Œä½†Endpointsä¸­çš„åœ°å€ä»ç„¶åœç•™åœ¨`notReadyAddresses`ï¼Œæ— æ³•è½¬æ¢ä¸º`addresses`ï¼Œå¯¼è‡´æœåŠ¡ä¸å¯ç”¨ã€‚åªæœ‰åœ¨é‡å¯`kube-controller-manager`åï¼Œé—®é¢˜æ‰å¾—ä»¥è§£å†³ã€‚

**æ½œåœ¨å®‰å…¨é£é™©ï¼š**

æ”»å‡»è€…å¯èƒ½åˆ©ç”¨è¿™ä¸€è¡Œä¸ºï¼Œè¯±ä½¿`kube-controller-manager`è¿›å…¥å¼‚å¸¸çŠ¶æ€ï¼Œå¯¼è‡´æœåŠ¡æŒç»­ä¸å¯ç”¨ã€‚è¿™å¯èƒ½é€šè¿‡ä»¥ä¸‹æ–¹å¼å®ç°ï¼š

1. **å†…å­˜è€—å°½æ”»å‡»**ï¼šæ”»å‡»è€…åœ¨é›†ç¾¤ä¸­åˆ›å»ºå¤§é‡æ¶ˆè€—å†…å­˜çš„èµ„æºï¼Œè¯±å‘masterèŠ‚ç‚¹å†…å­˜è€—å°½ã€‚
2. **æŒä¹…åŒ–çš„æœåŠ¡ä¸­æ–­**ï¼šå³ä½¿å†…å­˜æ¢å¤ï¼Œ`kube-controller-manager`ä»æœªæ­£å¸¸æ›´æ–°Endpointsï¼Œæ”»å‡»è€…æ— éœ€æŒç»­è€—å°½å†…å­˜ï¼Œå³å¯ä¿æŒæœåŠ¡ä¸å¯ç”¨çŠ¶æ€ã€‚
3. **éœ€è¦é‡å¯æœåŠ¡**ï¼šç®¡ç†å‘˜éœ€è¦æ‰‹åŠ¨é‡å¯`kube-controller-manager`æ‰èƒ½æ¢å¤æœåŠ¡ï¼Œå¢åŠ äº†æ¢å¤éš¾åº¦å’Œæ—¶é—´ã€‚

**æ»¡è¶³é£é™©åˆ¤æ–­æ ‡å‡†ï¼š**

1. **å¯è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ”»å‡»è€…å¯é€šè¿‡åœ¨é›†ç¾¤ä¸­éƒ¨ç½²æ¶æ„å·¥ä½œè´Ÿè½½å¯¼è‡´masterèŠ‚ç‚¹å†…å­˜è€—å°½ï¼Œè§¦å‘è¯¥é—®é¢˜ã€‚
2. **å¯èƒ½æˆä¸ºæ¼æ´å¹¶è¢«åˆ†é…CVEç¼–å·**ï¼šæŒ‰ç…§CVSS 3.1è¯„åˆ†æ ‡å‡†ï¼Œæ­¤æ¼æ´çš„åˆ©ç”¨æ— éœ€é«˜æƒé™ï¼ˆå‡è®¾æ”»å‡»è€…æœ‰éƒ¨ç½²æƒé™ï¼‰ï¼Œå½±å“èŒƒå›´å¹¿ï¼Œæ”»å‡»å¤æ‚åº¦ä½ï¼Œå¯èƒ½è·å¾—é«˜äºHighçš„è¯„åˆ†ã€‚
3. **ä¸å±äºIssueæäº¤è€…çš„é—®é¢˜**ï¼šè¯¥é—®é¢˜æ˜¯`kube-controller-manager`åœ¨å†…å­˜æ¢å¤åæœªèƒ½æ­£ç¡®æ›´æ–°çŠ¶æ€ï¼Œå¼•èµ·çš„æœåŠ¡ä¸å¯ç”¨ã€‚
4. **æ— éœ€é«˜æƒé™å³å¯å®æ–½æ”»å‡»**ï¼šå¦‚æœæ”»å‡»è€…æœ‰æƒåœ¨é›†ç¾¤ä¸­åˆ›å»ºèµ„æºï¼Œå°±å¯èƒ½å®æ–½æ­¤æ”»å‡»ã€‚

**å¯èƒ½çš„å½±å“ï¼š**

- **æœåŠ¡ä¸å¯ç”¨**ï¼šé›†ç¾¤å†…çš„æœåŠ¡å› Endpointsæœªæ›´æ–°ï¼Œæ— æ³•è¢«è®¿é—®ã€‚
- **å¢åŠ è¿ç»´éš¾åº¦**ï¼šéœ€è¦äººå·¥å¹²é¢„é‡å¯`kube-controller-manager`ï¼Œè€—è´¹æ—¶é—´å’ŒäººåŠ›ã€‚
- **å½±å“èŒƒå›´å¹¿æ³›**ï¼šåœ¨å¤šç§Ÿæˆ·ç¯å¢ƒä¸‹ï¼Œå¯èƒ½å½±å“å…¶ä»–ç§Ÿæˆ·çš„æœåŠ¡ã€‚

**Proof of Conceptï¼š**

1. **æ­¥éª¤ä¸€**ï¼šæ”»å‡»è€…åœ¨é›†ç¾¤ä¸­éƒ¨ç½²å¤§é‡é«˜å†…å­˜æ¶ˆè€—çš„Podï¼Œç›®æ ‡æ˜¯è€—å°½masterèŠ‚ç‚¹çš„å†…å­˜ã€‚
   ```yaml
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: memory-hog
     namespace: default
   spec:
     replicas: 50
     selector:
       matchLabels:
         app: memory-hog
     template:
       metadata:
         labels:
           app: memory-hog
       spec:
         containers:
         - name: memory-hog
           image: alpine
           command: ["dd"]
           args: ["if=/dev/zero", "of=/dev/null"]
           resources:
             requests:
               memory: "1Gi"
             limits:
               memory: "1Gi"
   ```
2. **æ­¥éª¤äºŒ**ï¼šè§‚å¯ŸmasterèŠ‚ç‚¹å†…å­˜è¢«è€—å°½ï¼Œ`kube-controller-manager`å¼€å§‹å¼‚å¸¸å·¥ä½œã€‚
3. **æ­¥éª¤ä¸‰**ï¼šåœæ­¢å†…å­˜æ¶ˆè€—ï¼ŒmasterèŠ‚ç‚¹å†…å­˜æ¢å¤ã€‚
4. **æ­¥éª¤å››**ï¼šæ£€æµ‹å‘ç°Endpointsçš„`notReadyAddresses`æœªè½¬æ¢ä¸º`addresses`ï¼ŒæœåŠ¡ä»ä¸å¯ç”¨ã€‚
5. **æ­¥éª¤äº”**ï¼šé‡å¯`kube-controller-manager`ï¼ŒæœåŠ¡æ¢å¤æ­£å¸¸ã€‚

**æ€»ç»“ï¼š**

è¯¥é—®é¢˜å­˜åœ¨è¢«åˆ©ç”¨çš„é£é™©ï¼Œæ”»å‡»è€…å¯ä»¥é€šè¿‡è€—å°½masterèŠ‚ç‚¹å†…å­˜å¹¶åˆ©ç”¨`kube-controller-manager`åœ¨å†…å­˜æ¢å¤åæœªèƒ½æ­£ç¡®æ›´æ–°Endpointsçš„æ¼æ´ï¼Œå¯¼è‡´æœåŠ¡æŒç»­æ€§ä¸å¯ç”¨ï¼Œé€ æˆä¸¥é‡çš„æ‹’ç»æœåŠ¡æ”»å‡»ã€‚

---

## Issue #125580 kube-apiserver oom, list resource consume too much memory cause json decode

- Issue é“¾æ¥ï¼š[#125580](https://github.com/kubernetes/kubernetes/issues/125580)

### Issue å†…å®¹

#### What happened?

The APIServer concurrency capability is too weak. In the test, the memory usage of 20 concurrent requests increases to 12 GB. The data size of "kubectl get crd -A -o yaml" is 20 MB.

1. Why does serialization consume so much memory? Is there any optimization mechanism?
2. Another point to note: when I call 100 watches concurrentlyï¼ˆand we know that watch will initially treat all items as add events, which are equivalent to lists.ï¼‰, kube-apiserver only goes up to 3GB, while 10 LIST concurrent operations go up to 8GB. Why is the memory usage difference between watches and lists so huge?

![image](https://github.com/kubernetes/kubernetes/assets/17514799/5c86cad6-0963-4745-bcc2-f969db55ba7f)

![image](https://github.com/kubernetes/kubernetes/assets/17514799/5f8e8143-059a-4a0a-892c-e0407ad2818a)

![image](https://github.com/kubernetes/kubernetes/assets/17514799/4b7858cd-2d47-4876-8ac1-1144f22ed207)

![image](https://github.com/kubernetes/kubernetes/assets/17514799/fc70cd4c-3dec-466a-a04d-2b3c4d52b035)


after i specfic  resourceVersion=0, memory usage does not improve: 


![image](https://github.com/kubernetes/kubernetes/assets/17514799/4b7adf44-9be3-4595-8276-8578d113b429)

![image](https://github.com/kubernetes/kubernetes/assets/17514799/d5ee683d-09b5-4243-9422-179158b0eca9)
#### What did you expect to happen?

Why does serialization consume so much memory? Is there any optimization mechanism?

#### How can we reproduce it (as minimally and precisely as possible)?

just kubectl get crd -A -o yaml & concurrently 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ç»è¿‡åˆ†æï¼ŒIssueå†…å®¹æ¶‰åŠåˆ°åœ¨kube-apiserverä¸­ï¼Œé€šè¿‡å¹¶å‘çš„LISTè¯·æ±‚ä¼šå¯¼è‡´å†…å­˜æ¶ˆè€—è¿‡é«˜ï¼Œç”šè‡³å¯¼è‡´OOMï¼ˆå†…å­˜æº¢å‡ºï¼‰ã€‚è¿™å¯èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œæ„æˆæ½œåœ¨çš„å®‰å…¨é£é™©ï¼Œç¬¦åˆä»¥ä¸‹é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ”»å‡»è€…å¯ä»¥é€šè¿‡å‘é€å¤§é‡å¹¶å‘çš„LISTè¯·æ±‚ï¼ˆå¦‚`kubectl get crd -A -o yaml`ï¼‰ï¼Œå¯¼è‡´kube-apiserveræ¶ˆè€—å¤§é‡å†…å­˜ï¼Œæœ€ç»ˆå¯èƒ½å¯¼è‡´æœåŠ¡å´©æºƒã€‚

2. **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä½¿ç”¨CVSS 3.1è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœåœ¨highä»¥ä¸Š**ï¼š

   - **æ”»å‡»å‘é‡ï¼ˆAVï¼‰**ï¼šç½‘ç»œï¼ˆNetworkï¼ŒNï¼‰â€”â€”æ”»å‡»è€…å¯ä»¥é€šè¿‡ç½‘ç»œè¿œç¨‹å‘é€è¯·æ±‚ã€‚
   - **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰**ï¼šä½ï¼ˆLowï¼ŒLï¼‰â€”â€”æ‰§è¡Œæ”»å‡»ä¸éœ€è¦å¤æ‚çš„æ¡ä»¶ã€‚
   - **æ‰€éœ€æƒé™ï¼ˆPRï¼‰**ï¼šä½ï¼ˆLowï¼ŒLï¼‰â€”â€”æ”»å‡»è€…éœ€è¦å…·å¤‡ä½æƒé™ï¼ˆè¯»å–æƒé™ï¼‰å³å¯ã€‚
   - **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰**ï¼šæ— ï¼ˆNoneï¼ŒNï¼‰â€”â€”ä¸éœ€è¦é¢å¤–çš„ç”¨æˆ·äº¤äº’ã€‚
   - **ä½œç”¨èŒƒå›´ï¼ˆSï¼‰**ï¼šæœªæ”¹å˜ï¼ˆUnchangedï¼ŒUï¼‰â€”â€”æ”»å‡»å½±å“çš„ç»„ä»¶ä¸è¢«æ”»å‡»çš„ç»„ä»¶ç›¸åŒã€‚
   - **æœºå¯†æ€§ï¼ˆCï¼‰**ï¼šæ— ï¼ˆNoneï¼ŒNï¼‰
   - **å®Œæ•´æ€§ï¼ˆIï¼‰**ï¼šæ— ï¼ˆNoneï¼ŒNï¼‰
   - **å¯ç”¨æ€§ï¼ˆAï¼‰**ï¼šé«˜ï¼ˆHighï¼ŒHï¼‰â€”â€”æœåŠ¡è¢«æ‹’ç»ï¼Œä¸¥é‡å½±å“å¯ç”¨æ€§ã€‚

   æ ¹æ®ä»¥ä¸ŠæŒ‡æ ‡ï¼Œä½¿ç”¨CVSS 3.1è®¡ç®—å¾—åˆ†ä¸º7.5ï¼Œè¯„çº§ä¸º**é«˜é£é™©ï¼ˆHighï¼‰**ã€‚

5. **å¯¹äºæ—¥å¿—ä¸­æ³„éœ²å‡­æ®çš„é£é™©â€¦â€¦ï¼ˆæ­¤é¡¹ä¸é€‚ç”¨æœ¬æƒ…å†µï¼‰**

**å¯èƒ½çš„å½±å“**ï¼š

- æ”»å‡»è€…å¯ä»¥åˆ©ç”¨ä½æƒé™ç”¨æˆ·ï¼Œé€šè¿‡å‘é€å¤§é‡å¹¶å‘çš„LISTè¯·æ±‚ï¼Œå¯¼è‡´kube-apiserverå†…å­˜æ¶ˆè€—æ®†å°½ï¼Œæœ€ç»ˆå¯¼è‡´æœåŠ¡æ‹’ç»å“åº”ï¼ˆDoSæ”»å‡»ï¼‰ã€‚
- è¿™ä¼šå½±å“æ•´ä¸ªKubernetesé›†ç¾¤çš„ç¨³å®šæ€§å’Œå¯ç”¨æ€§ï¼Œé˜»ç¢æ­£å¸¸çš„æœåŠ¡è¿è¡Œã€‚

**Proof of Conceptï¼ˆæ¦‚å¿µéªŒè¯ï¼‰**ï¼š

æ”»å‡»è€…å¯ä»¥æ‰§è¡Œä»¥ä¸‹æ“ä½œæ¥é‡ç°é—®é¢˜ï¼š

1. **ç¼–å†™å¹¶å‘è¯·æ±‚è„šæœ¬**ï¼š

   ä½¿ç”¨å¦‚ä¸‹è„šæœ¬ï¼Œæ¨¡æ‹Ÿå¤šä¸ªå¹¶å‘çš„LISTè¯·æ±‚ï¼š

   ```bash
   for i in {1..20}; do
     kubectl get crd -A -o yaml &
   done
   wait
   ```

   æˆ–è€…ä½¿ç”¨å‹åŠ›æµ‹è¯•å·¥å…·ï¼Œå¦‚`ab`æˆ–`siege`ï¼Œå¯¹kube-apiserverçš„ç›¸åº”APIç«¯ç‚¹å‘é€å¹¶å‘è¯·æ±‚ã€‚

2. **ç›‘æ§kube-apiserverå†…å­˜ä½¿ç”¨æƒ…å†µ**ï¼š

   åœ¨æ”»å‡»æœŸé—´ï¼Œè§‚å¯Ÿkube-apiserverçš„å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œå¯ä»¥ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤ï¼š

   ```bash
   kubectl top pod kube-apiserver -n kube-system
   ```

   æˆ–è€…ä½¿ç”¨ç›‘æ§å·¥å…·ï¼Œå¦‚Prometheuså’ŒGrafanaï¼Œç»˜åˆ¶å†…å­˜ä½¿ç”¨æ›²çº¿ã€‚

3. **è§‚å¯ŸæœåŠ¡çŠ¶æ€**ï¼š

   åœ¨å†…å­˜æ¶ˆè€—è¿‡é«˜æ—¶ï¼Œkube-apiserverå¯èƒ½ä¼šå´©æºƒæˆ–é‡å¯ï¼Œå¯¼è‡´é›†ç¾¤ä¸å¯ç”¨ã€‚

**å»ºè®®**ï¼š

- **ä¼˜åŒ–kube-apiserverçš„å†…å­˜ç®¡ç†**ï¼šæ£€æŸ¥åºåˆ—åŒ–å’Œååºåˆ—åŒ–è¿‡ç¨‹ï¼Œä¼˜åŒ–å†…å­˜å ç”¨ï¼Œé¿å…å•ä¸ªè¯·æ±‚æ¶ˆè€—è¿‡å¤šèµ„æºã€‚
- **é™åˆ¶å¹¶å‘è¯·æ±‚æ•°**ï¼šåœ¨APIç½‘å…³æˆ–Ingresså±‚é¢ï¼Œé™åˆ¶å•ä¸ªç”¨æˆ·æˆ–IPçš„æœ€å¤§å¹¶å‘è¯·æ±‚æ•°ï¼Œé˜²æ­¢æ¶æ„æ»¥ç”¨ã€‚
- **å¢åŠ è®¤è¯å’Œæˆæƒæ§åˆ¶**ï¼šç¡®ä¿åªæœ‰ç»è¿‡æˆæƒçš„ç”¨æˆ·æ‰èƒ½è®¿é—®é«˜æ¶ˆè€—èµ„æºçš„APIæ¥å£ã€‚
- **ç›‘æ§å’ŒæŠ¥è­¦**ï¼šè®¾ç½®å¯¹kube-apiserverå†…å­˜ä½¿ç”¨çš„ç›‘æ§ï¼Œä¸€æ—¦è¶…è¿‡é˜ˆå€¼ï¼Œç«‹å³æŠ¥è­¦å¹¶é‡‡å–æªæ–½ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueæ¶‰åŠçš„å†…å®¹å­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ï¼Œéœ€è¦å¼•èµ·é‡è§†å¹¶åŠæ—¶ä¿®å¤ã€‚

---

## Issue #125484 Missed k8s.io/kube-openapi/cmd/openapi-gen dependency on code-generator go.mod

- Issue é“¾æ¥ï¼š[#125484](https://github.com/kubernetes/kubernetes/issues/125484)

### Issue å†…å®¹

#### What happened?

We're trying to upgrade `k8s.io/code-generator` to `v1.30.1`  (https://github.com/kubernetes-sigs/kueue/pull/2402) and found the issue on executing `kube::codegen::gen_openapi` (https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/kubernetes-sigs_kueue/2402/pull-kueue-verify-main/1800788997593632768). 

```
../kube-openapi@v0.0.0-20240228011516-70dd3763d340/pkg/common/common.go:23:2: missing go.sum entry for module providing package github.com/emicklei/go-restful/v3 (imported by k8s.io/kube-openapi/pkg/common); to add:
	go get k8s.io/kube-openapi/pkg/common@v0.0.0-20240228011516-70dd3763d340
```

The problem is that on go.mod (https://github.com/kubernetes/code-generator/blob/master/go.mod) missed `github.com/emicklei/go-restful/v3` dependency that not allow to install `k8s.io/kube-openapi` (https://github.com/kubernetes/code-generator/blob/master/kube_codegen.sh#L330C45-L330C64)

#### What did you expect to happen?

No error

#### How can we reproduce it (as minimally and precisely as possible)?

To reproduce the same what happening on https://github.com/kubernetes/code-generator/blob/master/kube_codegen.sh#L325-L330, just execute this command on code-generator folder:
```bash
GO111MODULE=on go install k8s.io/kube-openapi/cmd/openapi-gen
```

#### Anything else we need to know?

I think it's happens after this commit https://github.com/kubernetes/kubernetes/commit/1a4450f6ffdad44bfc12fae5911c93e3bd238f0f.

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ç»è¿‡åˆ†æï¼Œè¯¥ Issue æ¶‰åŠæ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

**åŸå› åˆ†æï¼š**

åœ¨å‡çº§ `k8s.io/code-generator` è‡³ `v1.30.1` æ—¶ï¼Œæ‰§è¡Œ `kube::codegen::gen_openapi` é‡åˆ°é”™è¯¯ï¼Œæç¤ºç¼ºå°‘ `go.sum` æ¡ç›®ï¼Œéœ€è¦æ‰‹åŠ¨è¿è¡Œ `go get` å‘½ä»¤æ¥è·å–ç¼ºå¤±çš„ä¾èµ–åŒ… `github.com/emicklei/go-restful/v3`ã€‚

ç¼ºå°‘ `go.sum` ä¸­çš„æ ¡éªŒå’Œæ¡ç›®ï¼Œæ„å‘³ç€ Go åœ¨æ‹‰å–ä¾èµ–æ—¶æ— æ³•éªŒè¯æ¨¡å—çš„å®Œæ•´æ€§å’ŒçœŸå®æ€§ã€‚æ”»å‡»è€…å¯èƒ½åˆ©ç”¨è¿™ä¸€ç‚¹ï¼Œé€šè¿‡ä¸­é—´äººæ”»å‡»ï¼ˆMITMï¼‰ã€DNS åŠ«æŒæˆ–ä¾›åº”é“¾æ”»å‡»ï¼Œå‘å—å®³è€…æä¾›ä¸€ä¸ªä¼ªé€ çš„ã€åŒ…å«æ¶æ„ä»£ç çš„æ¨¡å—ã€‚

ç”±äºå—å®³è€…åœ¨æ„å»ºè¿‡ç¨‹ä¸­éœ€è¦æ‹‰å–è¯¥ä¾èµ–ï¼Œå¦‚æœæœªå¼€å¯ Go çš„æ ¡éªŒå’ŒéªŒè¯ï¼ˆä¾‹å¦‚è®¾ç½®äº†ç¯å¢ƒå˜é‡ `GOSUMDB=off`ï¼‰ï¼Œæˆ–è€…ä½¿ç”¨äº†ä¸å®‰å…¨çš„ä»£ç†ï¼Œæ”»å‡»è€…çš„æ¶æ„ä»£ç å¯èƒ½è¢«ç¼–è¯‘è¿›æœ€ç»ˆçš„äºŒè¿›åˆ¶æ–‡ä»¶ä¸­ã€‚

**å¯èƒ½çš„å½±å“ï¼š**

- **è¿œç¨‹ä»£ç æ‰§è¡Œï¼ˆRCEï¼‰**ï¼šæ¶æ„ä»£ç è¢«ç¼–è¯‘å¹¶æ‰§è¡Œï¼Œæ”»å‡»è€…èƒ½å¤Ÿåœ¨å—å®³è€…çš„ç³»ç»Ÿæˆ–æœ€ç»ˆç”¨æˆ·çš„ç¯å¢ƒä¸­æ‰§è¡Œä»»æ„ä»£ç ã€‚
- **ä¾›åº”é“¾æ”»å‡»**ï¼šå½±å“ä½¿ç”¨è¯¥ä»£ç ç”Ÿæˆå™¨çš„ä¸‹æ¸¸é¡¹ç›®ï¼Œå¯¼è‡´å¤§èŒƒå›´çš„å®‰å…¨äº‹ä»¶ã€‚
- **æ•°æ®æ³„éœ²æˆ–ç ´å**ï¼šæ”»å‡»è€…å¯èƒ½è®¿é—®ã€ç¯¡æ”¹æˆ–ç ´åæ•æ„Ÿæ•°æ®ã€‚

æ ¹æ® CVSS 3.1 è¯„åˆ†æ ‡å‡†ï¼Œæ­¤æ¼æ´çš„ä¸¥é‡æ€§å¯èƒ½ä¸ºé«˜å±ï¼ˆHighï¼‰æˆ–ä¸¥é‡ï¼ˆCriticalï¼‰ï¼Œå› ä¸ºæ”»å‡»è€…å¯ä»¥æœªç»æˆæƒæ‰§è¡Œä»»æ„ä»£ç ï¼Œå…¨é¢å±å®³ç³»ç»Ÿçš„æœºå¯†æ€§ã€å®Œæ•´æ€§å’Œå¯ç”¨æ€§ã€‚

**Proof of Conceptï¼ˆæ¦‚å¿µéªŒè¯ï¼‰ï¼š**

1. **æ”»å‡»è€…å‡†å¤‡æ¶æ„æ¨¡å—ï¼š**

   æ”»å‡»è€…åœ¨è‡ªå·±çš„ GitHub è´¦æˆ·ä¸‹åˆ›å»ºä¸€ä¸ªåä¸º `github.com/emicklei/go-restful/v3` çš„ä»“åº“ï¼ŒåŠ å…¥æ¶æ„ä»£ç ã€‚

2. **è¯±ä½¿å—å®³è€…æ‹‰å–æ¶æ„ä¾èµ–ï¼š**

   - å¦‚æœå—å®³è€…åœ¨æ‰§è¡Œ `go get` æ—¶ï¼Œæœªä½¿ç”¨å®˜æ–¹çš„ Go æ¨¡å—ä»£ç†å’Œæ ¡éªŒå’Œæ•°æ®åº“ï¼ˆä¾‹å¦‚è®¾ç½®äº† `GOPROXY=direct` å’Œ `GOSUMDB=off`ï¼‰ï¼ŒGo å°†ç›´æ¥ä»æ”»å‡»è€…çš„ä»“åº“æ‹‰å–æ¨¡å—ã€‚
   - æˆ–è€…ï¼Œæ”»å‡»è€…é€šè¿‡åŠ«æŒ DNSï¼Œå°† `github.com` æŒ‡å‘æ¶æ„æœåŠ¡å™¨ã€‚

3. **å—å®³è€…æ„å»ºå¹¶è¿è¡Œå«æœ‰æ¶æ„ä»£ç çš„äºŒè¿›åˆ¶æ–‡ä»¶ï¼š**

   å—å®³è€…æŒ‰ç…§æç¤ºæ‰§è¡Œ `go get`ï¼Œæ‹‰å–å¹¶ç¼–è¯‘äº†åŒ…å«æ¶æ„ä»£ç çš„æ¨¡å—ï¼Œæœ€ç»ˆç”Ÿæˆçš„å·¥å…·æˆ–åº”ç”¨ç¨‹åºè¢«æ¤å…¥äº†åé—¨ã€‚

**é˜²èŒƒæªæ–½ï¼š**

- **å®Œå–„ä¾èµ–ç®¡ç†ï¼š** ç¡®ä¿åœ¨ `go.mod` å’Œ `go.sum` ä¸­æ˜ç¡®å£°æ˜æ‰€æœ‰ä¾èµ–ï¼Œå¹¶åŒ…å«æ­£ç¡®çš„æ ¡éªŒå’Œã€‚
- **ä½¿ç”¨å¯ä¿¡çš„æ¨¡å—ä»£ç†å’Œæ ¡éªŒå’Œæ•°æ®åº“ï¼š** ä¸è¦ç¦ç”¨ Go çš„æ¨¡å—æ ¡éªŒå’ŒéªŒè¯æœºåˆ¶ï¼Œé¿å…ä½¿ç”¨ä¸å®‰å…¨çš„ä»£ç†ã€‚
- **å®šæœŸå®¡æ ¸ä¾èµ–ï¼š** å®šæœŸæ£€æŸ¥å’Œæ›´æ–°ä¾èµ–é¡¹ï¼Œå…³æ³¨å®˜æ–¹å‘å¸ƒçš„å®‰å…¨å…¬å‘Šã€‚

**ç»“è®ºï¼š**

è¯¥ Issue æš´éœ²äº†ä¸€ä¸ªå¯èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨é£é™©ï¼Œç¬¦åˆé£é™©åˆ¤æ–­æ ‡å‡† 1 å’Œ 2ã€‚å»ºè®®å°½å¿«ä¿®å¤ç¼ºå¤±çš„ä¾èµ–å£°æ˜ï¼Œç¡®ä¿æ„å»ºè¿‡ç¨‹çš„å®‰å…¨æ€§ã€‚

---

## Issue #125426 ExtendedResourceToleration adds tolerations even when the quantity of requested resources is "0"

- Issue é“¾æ¥ï¼š[#125426](https://github.com/kubernetes/kubernetes/issues/125426)

### Issue å†…å®¹

#### What happened?

ExtendedResourceToleration is useful for setting tolerations automatically to pods requesting extended resources like GPUs, but the tolerations are still given even if the quantity of extended resources is set to â€œ0â€.

#### What did you expect to happen?

ExtendedResourceToleration admission should skip adding toleration if the quantity of extended resources is set to â€œ0â€.

#### How can we reproduce it (as minimally and precisely as possible)?

You can reproduce it by creating the following pod:

```
â¯ cat pod-with-extended-resource.yaml    
apiVersion: v1
kind: Pod
metadata:
  name: debug
spec:
  containers:
  - command:
    - /bin/sh
    - -c
    - sleep infinity
    image: busybox
    name: debug
    resources:
      limits:
        nvidia.com/gpu: "0"

â¯ kubectl apply -f pod-with-extended-resource.yaml 
pod/debug created

â¯ kubectl get po debug -o jsonpath='{.spec.tolerations[?(@.key == "nvidia.com/gpu")]}' | jq
{
  "effect": "NoSchedule",
  "key": "nvidia.com/gpu",
  "operator": "Exists"
}
```


#### Anything else we need to know?

It seems that this issue can be resolved by adding an additional conditional check whether the quantity of the extended resources is over 0 here: 
https://github.com/kubernetes/kubernetes/blob/a7ca13ea29ba5b3c91fd293cdbaec8fb5b30cee2/plugin/pkg/admission/extendedresourcetoleration/admission.go#L72-L85

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.5
```

</details>


#### Cloud provider

N/A

#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

è¯¥é—®é¢˜æ¶‰åŠæ½œåœ¨çš„å®‰å…¨é£é™©ï¼ŒåŸå› å¦‚ä¸‹ï¼š

**é—®é¢˜æè¿°ï¼š**

åœ¨ä½¿ç”¨ `ExtendedResourceToleration` å‡†å…¥æ’ä»¶æ—¶ï¼Œå³ä½¿ Pod è¯·æ±‚çš„æ‰©å±•èµ„æºæ•°é‡ä¸º "0"ï¼Œæ’ä»¶ä»ä¼šä¸ºè¯¥ Pod æ·»åŠ ç›¸åº”çš„å®¹å¿ï¼ˆtolerationï¼‰ã€‚è¿™æ„å‘³ç€ Pod ä¼šè¢«è‡ªåŠ¨æ·»åŠ å¯¹ç‰¹å®šæ‰©å±•èµ„æºï¼ˆå¦‚ GPUï¼‰çš„å®¹å¿ï¼Œå³ä½¿å®é™…ä¸Šå¹¶ä¸éœ€è¦è¯¥èµ„æºã€‚

**æ½œåœ¨é£é™©åˆ†æï¼š**

1. **ç»•è¿‡èŠ‚ç‚¹éš”ç¦»æœºåˆ¶ï¼š** Kubernetes ä¸­ï¼ŒèŠ‚ç‚¹å¯ä»¥é€šè¿‡æ±¡ç‚¹ï¼ˆtaintsï¼‰æ¥é˜»æ­¢éé¢„æœŸçš„ Pods è¢«è°ƒåº¦åˆ°ç‰¹å®šèŠ‚ç‚¹ä¸Šï¼Œè€Œ Pod éœ€è¦ç›¸åº”çš„å®¹å¿ï¼ˆtolerationï¼‰æ‰èƒ½è¢«è°ƒåº¦åˆ°è¿™äº›èŠ‚ç‚¹ã€‚æ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¿™ä¸€é—®é¢˜ï¼Œé€šè¿‡åˆ›å»ºè¯·æ±‚ 0 ä¸ªæ‰©å±•èµ„æºçš„ Podï¼Œè·å¾—å¯¹åº”çš„å®¹å¿ï¼Œä»è€Œå°† Pod è°ƒåº¦åˆ°å—æ±¡ç‚¹ä¿æŠ¤çš„èŠ‚ç‚¹ä¸Šã€‚

2. **æœªæˆæƒè®¿é—®ï¼š** å¦‚æœèŠ‚ç‚¹ä¸Šå­˜åœ¨æ•æ„Ÿèµ„æºã€æ•°æ®æˆ–æœåŠ¡ï¼Œæ”»å‡»è€…çš„ Pod è¢«è°ƒåº¦åˆ°è¿™äº›èŠ‚ç‚¹ä¸Šå¯èƒ½ä¼šå¯¼è‡´æœªæˆæƒçš„è®¿é—®ï¼Œé€ æˆæœºå¯†æ€§å’Œå®Œæ•´æ€§çš„å¨èƒã€‚

3. **èµ„æºæ»¥ç”¨å’Œæ‹’ç»æœåŠ¡ï¼š** æ”»å‡»è€…å¯ä»¥åœ¨å—ä¿æŠ¤çš„èŠ‚ç‚¹ä¸Šè¿è¡Œé«˜è´Ÿè½½çš„ Podï¼Œå ç”¨èŠ‚ç‚¹èµ„æºï¼Œå½±å“å…¶ä»–å…³é”®å·¥ä½œè´Ÿè½½çš„æ­£å¸¸è¿è¡Œï¼Œé€ æˆå¯ç”¨æ€§å½±å“ã€‚

**ç¬¦åˆé£é™©åˆ¤æ–­æ ‡å‡†ï¼š**

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨ï¼š** æ˜¯çš„ï¼Œæ”»å‡»è€…å¯ä»¥é€šè¿‡è¯·æ±‚ 0 ä¸ªæ‰©å±•èµ„æºï¼Œè·å–ç›¸åº”çš„å®¹å¿ï¼Œè¿›è€Œå°† Pod è°ƒåº¦åˆ°ç‰¹å®šèŠ‚ç‚¹ã€‚

2. **å¯èƒ½æˆä¸ºæ¼æ´å¹¶è¢«åˆ†é… CVE ç¼–å·ï¼ŒCVSS è¯„åˆ†åœ¨ High ä»¥ä¸Šï¼š**

   - **æ”»å‡»å‘é‡ï¼ˆAVï¼‰ï¼š** ç½‘ç»œï¼ˆNï¼‰â€” æ”»å‡»è€…é€šè¿‡ Kubernetes API æäº¤æ¶æ„ Podã€‚
   - **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰ï¼š** ä½ï¼ˆLï¼‰â€” æ”»å‡»ä¸éœ€è¦ç‰¹æ®Šæ¡ä»¶ã€‚
   - **æ‰€éœ€æƒé™ï¼ˆPRï¼‰ï¼š** ä½ï¼ˆLï¼‰â€” éœ€è¦å…·å¤‡åˆ›å»º Pod çš„æƒé™ï¼Œè¿™æ˜¯è®¸å¤šç”¨æˆ·æ­£å¸¸å…·æœ‰çš„æƒé™ã€‚
   - **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰ï¼š** æ— ï¼ˆNï¼‰â€” æ”»å‡»ä¸éœ€è¦é¢å¤–çš„ç”¨æˆ·äº¤äº’ã€‚
   - **ä½œç”¨èŒƒå›´ï¼ˆSï¼‰ï¼š** æœªæ”¹å˜ï¼ˆUï¼‰â€” æ”»å‡»å½±å“åœ¨ Kubernetes é›†ç¾¤èŒƒå›´å†…ã€‚
   - **æœºå¯†æ€§å½±å“ï¼ˆCï¼‰ï¼š** é«˜ï¼ˆHï¼‰â€” å¯èƒ½è®¿é—®åˆ°æ•æ„Ÿæ•°æ®ã€‚
   - **å®Œæ•´æ€§å½±å“ï¼ˆIï¼‰ï¼š** é«˜ï¼ˆHï¼‰â€” å¯èƒ½ç¯¡æ”¹æ•°æ®æˆ–é…ç½®ã€‚
   - **å¯ç”¨æ€§å½±å“ï¼ˆAï¼‰ï¼š** é«˜ï¼ˆHï¼‰â€” å¯èƒ½å¯¼è‡´æœåŠ¡ä¸­æ–­æˆ–æ‹’ç»æœåŠ¡ã€‚

   æ ¹æ® CVSS 3.1 è®¡ç®—ï¼Œæ€»åˆ†ä¸º 8.8ï¼Œé«˜å±ï¼ˆHighï¼‰ã€‚

3. **Issue æäº¤è€…çš„è¡Œä¸ºä¸å½±å“é£é™©è¯„ä¼°ï¼š** ä¸é€‚ç”¨ã€‚

4. **æ”»å‡»è€…åˆ©ç”¨éœ€è¦çš„æƒé™ï¼š** è™½ç„¶éœ€è¦åˆ›å»º Pod çš„æƒé™ï¼Œä½†åœ¨è®¸å¤šç¯å¢ƒä¸­è¿™æ˜¯å¸¸è§æƒé™ï¼Œä¸åº”é™ä½é£é™©è¯„çº§ã€‚

5. **æ—¥å¿—æ³„éœ²å‡­æ®çš„é£é™©ï¼š** ä¸é€‚ç”¨ã€‚

**å¯èƒ½çš„å½±å“ï¼š**

- **å®‰å…¨éš”ç¦»è¢«ç ´åï¼š** æ”»å‡»è€…å¯ä»¥è®¿é—®åˆ°åŸæœ¬é€šè¿‡æ±¡ç‚¹ä¿æŠ¤çš„èŠ‚ç‚¹ï¼Œç»•è¿‡äº†é›†ç¾¤çš„å®‰å…¨ç­–ç•¥ã€‚
- **æ•°æ®æ³„éœ²ï¼š** å¦‚æœå—å½±å“èŠ‚ç‚¹ä¸Šæœ‰æ•æ„Ÿæ•°æ®ï¼Œå¯èƒ½è¢«æ”»å‡»è€…çš„ Pod è¯»å–ã€‚
- **æœåŠ¡ä¸­æ–­ï¼š** æ”»å‡»è€…çš„ Pod å¯èƒ½å½±å“èŠ‚ç‚¹ä¸Šå…¶ä»–å…³é”®æœåŠ¡çš„ç¨³å®šæ€§ã€‚

**Proof of Conceptï¼š**

1. **åˆ›å»ºè¯·æ±‚ 0 ä¸ªæ‰©å±•èµ„æºçš„ Podï¼š**

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: malicious-pod
   spec:
     containers:
     - name: malicious-container
       image: busybox
       command: ["sleep", "infinity"]
       resources:
         limits:
           nvidia.com/gpu: "0"
   ```

2. **åº”ç”¨ Podï¼š**

   ```bash
   kubectl apply -f malicious-pod.yaml
   ```

3. **æŸ¥çœ‹ Pod çš„å®¹å¿ï¼š**

   ```bash
   kubectl get pod malicious-pod -o jsonpath='{.spec.tolerations}'
   ```

   è¾“å‡ºæ˜¾ç¤º Pod å…·æœ‰å¯¹ `nvidia.com/gpu` çš„å®¹å¿ï¼š

   ```json
   [
     {
       "key": "nvidia.com/gpu",
       "operator": "Exists",
       "effect": "NoSchedule"
     }
   ]
   ```

4. **éªŒè¯ Pod è¢«è°ƒåº¦åˆ°å—ä¿æŠ¤çš„èŠ‚ç‚¹ï¼š**

   å¦‚æœé›†ç¾¤ä¸­å­˜åœ¨å¸¦æœ‰ `nvidia.com/gpu` æ±¡ç‚¹çš„èŠ‚ç‚¹ï¼ŒPod ä¼šè¢«è°ƒåº¦åˆ°è¿™äº›èŠ‚ç‚¹ä¸Šã€‚

**æ€»ç»“ï¼š**

è¯¥é—®é¢˜å…è®¸æ”»å‡»è€…é€šè¿‡è¯·æ±‚æ•°é‡ä¸º 0 çš„æ‰©å±•èµ„æºï¼Œè·å–å¯¹åº”çš„å®¹å¿ï¼Œè¿›è€Œå°† Pod è°ƒåº¦åˆ°å—æ±¡ç‚¹ä¿æŠ¤çš„èŠ‚ç‚¹ä¸Šï¼Œå¯èƒ½å¯¼è‡´æœªæˆæƒçš„è®¿é—®å’Œå®‰å…¨ç­–ç•¥çš„ç»•è¿‡ï¼Œç¬¦åˆé«˜é£é™©æ¼æ´çš„è¯„ä¼°æ ‡å‡†ã€‚

---

## Issue #125403 kubectl --server-side apply replaces the live manifest instead of merging when migrating from clinet side apply to server side apply 

- Issue é“¾æ¥ï¼š[#125403](https://github.com/kubernetes/kubernetes/issues/125403)

### Issue å†…å®¹

#### What happened?

If a resource is created initially with kubectl client-side apply and tries to update it with server-side apply, the live manifest got replaced by the applied manifest and the missing fields got removed,  instead of merging with kubectl version  v1.29.2. This was working with a lower version(v1.22.0) of kubectl 

#### What did you expect to happen?

I expected the live manifest and the applied manifest to be merged instead of replacing.

#### How can we reproduce it (as minimally and precisely as possible)?

1. ```kubectl create namespace kiali-test``` to create a testing namespace.
2. ```kubectl apply -f https://raw.githubusercontent.com/SebastianJ91/argocd-server-side-apply/main/1-manual/1-kiali.io_kialis.yaml``` to install the CRD.
3. ```kubectl apply -f https://raw.githubusercontent.com/SebastianJ91/argocd-server-side-apply/main/1-manual/2-kiali.io_v1alpha1_kiali.yaml``` to install a specific Kiali instance manifest.
4. ```kubectl apply --server-side=true -f https://raw.githubusercontent.com/SebastianJ91/argocd-server-side-apply/main/2-argocd-application/kiali.yaml```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.2
```

</details>


#### Cloud provider

<details>
kind cluster on local machines
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

è¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨`kubectl`ä»å®¢æˆ·ç«¯åº”ç”¨ï¼ˆclient-side applyï¼‰è¿ç§»åˆ°æœåŠ¡ç«¯åº”ç”¨ï¼ˆserver-side applyï¼‰æ—¶ï¼Œèµ„æºçš„ç°åœºæ¸…å•ï¼ˆlive manifestï¼‰è¢«åº”ç”¨çš„æ¸…å•æ‰€æ›¿æ¢ï¼Œç¼ºå¤±çš„å­—æ®µè¢«ç§»é™¤ï¼Œè€Œä¸æ˜¯è¿›è¡Œåˆå¹¶ã€‚

**æ½œåœ¨çš„å®‰å…¨é£é™©åˆ†æï¼š**

1. **é£é™©æè¿°ï¼š**
   - åœ¨ä»å®¢æˆ·ç«¯åº”ç”¨è¿ç§»åˆ°æœåŠ¡ç«¯åº”ç”¨çš„è¿‡ç¨‹ä¸­ï¼Œå¦‚æœèµ„æºçš„æŸäº›å…³é”®é…ç½®ï¼ˆä¾‹å¦‚å®‰å…¨ç­–ç•¥ã€è®¿é—®æ§åˆ¶ã€ç½‘ç»œé…ç½®ç­‰ï¼‰è¢«æ„å¤–åˆ é™¤ï¼Œå¯èƒ½å¯¼è‡´èµ„æºçš„å®‰å…¨é…ç½®è¢«ç ´åã€‚
   - æ”»å‡»è€…å¯èƒ½åˆ©ç”¨è¿™ä¸€è¡Œä¸ºï¼Œè¯±å¯¼ç®¡ç†å‘˜æˆ–è‡ªåŠ¨åŒ–æµç¨‹è¿›è¡ŒæœåŠ¡ç«¯åº”ç”¨ï¼Œä»è€Œç§»é™¤å…³é”®çš„å®‰å…¨é…ç½®ï¼Œå¯¼è‡´æƒé™æå‡æˆ–æœªæˆæƒçš„è®¿é—®ã€‚

2. **é£é™©åˆ©ç”¨å¯èƒ½æ€§ï¼š**
   - **å¯è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šå¦‚æœæ”»å‡»è€…èƒ½å¤Ÿæ§åˆ¶æˆ–å½±å“åº”ç”¨çš„æ¸…å•ï¼Œä½¿å…¶ç¼ºå°‘å…³é”®çš„å®‰å…¨é…ç½®ï¼Œè€Œç”±äº`kubectl`çš„è¡Œä¸ºä¼šç§»é™¤ç°åœºèµ„æºä¸­çš„è¿™äº›é…ç½®ï¼Œé‚£ä¹ˆæ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¿™ä¸€ç‚¹ç ´åèµ„æºçš„å®‰å…¨æ€§ã€‚
   - **å¯èƒ½æˆä¸ºæ¼æ´å¹¶è¢«åˆ†é…CVEç¼–å·**ï¼šç”±äºè¿™ä¸€è¡Œä¸ºå¯èƒ½å¯¼è‡´å®‰å…¨é…ç½®è¢«æ„å¤–åˆ é™¤ï¼Œç¬¦åˆæ¼æ´çš„å®šä¹‰ï¼Œå¯ä»¥è¢«åˆ†é…CVEç¼–å·ã€‚

3. **å½±å“åˆ†æï¼š**
   - **å®‰å…¨é…ç½®è¢«ç§»é™¤**ï¼šå…³é”®çš„å®‰å…¨è®¾ç½®ï¼ˆå¦‚RBACè§„åˆ™ã€ç½‘ç»œç­–ç•¥ã€Podå®‰å…¨ç­–ç•¥ç­‰ï¼‰è¢«æ„å¤–åˆ é™¤ï¼Œå¯èƒ½å¯¼è‡´æœªæˆæƒçš„è®¿é—®ã€æ•°æ®æ³„éœ²æˆ–æƒé™æå‡ã€‚
   - **æœåŠ¡ä¸­æ–­**ï¼šé‡è¦çš„é…ç½®è¢«åˆ é™¤ï¼Œå¯èƒ½å¯¼è‡´æœåŠ¡ä¸å¯ç”¨ï¼Œå½±å“ä¸šåŠ¡è¿ç»­æ€§ã€‚

4. **CVSSè¯„åˆ†ï¼ˆç¤ºä¾‹ï¼‰ï¼š**
   - **æ”»å‡»å‘é‡ï¼ˆAVï¼‰**ï¼šç½‘ç»œï¼ˆNï¼‰
   - **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰**ï¼šä½ï¼ˆLï¼‰
   - **ç‰¹æƒè¦æ±‚ï¼ˆPRï¼‰**ï¼šä½ï¼ˆLï¼‰
   - **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰**ï¼šéœ€è¦ï¼ˆRï¼‰
   - **å½±å“èŒƒå›´ï¼ˆSï¼‰**ï¼šæ”¹å˜ï¼ˆCï¼‰
   - **ä¿å¯†æ€§ï¼ˆCï¼‰**ï¼šé«˜ï¼ˆHï¼‰
   - **å®Œæ•´æ€§ï¼ˆIï¼‰**ï¼šé«˜ï¼ˆHï¼‰
   - **å¯ç”¨æ€§ï¼ˆAï¼‰**ï¼šé«˜ï¼ˆHï¼‰
   - **ç»¼åˆè¯„åˆ†**ï¼š9.0ï¼ˆCriticalï¼‰

**Proof of Conceptï¼ˆæ¦‚å¿µéªŒè¯ï¼‰ï¼š**

1. **åˆ›å»ºå…·å¤‡å®‰å…¨é…ç½®çš„èµ„æºï¼š**

   ```bash
   kubectl apply -f secure-resource.yaml
   ```

   `secure-resource.yaml`åŒ…å«äº†å…³é”®çš„å®‰å…¨é…ç½®ï¼Œä¾‹å¦‚RBACè§„åˆ™æˆ–ç½‘ç»œç­–ç•¥ã€‚

2. **ä½¿ç”¨å®¢æˆ·ç«¯åº”ç”¨è¿›è¡Œæ›´æ–°ï¼š**

   ```bash
   kubectl apply -f updated-resource.yaml
   ```

   `updated-resource.yaml`æ›´æ–°äº†ä¸€äº›éå®‰å…¨ç›¸å…³çš„å­—æ®µã€‚

3. **è¿ç§»åˆ°æœåŠ¡ç«¯åº”ç”¨ï¼š**

   ```bash
   kubectl apply --server-side=true -f updated-resource.yaml
   ```

   ç”±äºè¿ç§»åˆ°æœåŠ¡ç«¯åº”ç”¨ï¼Œä¸”`updated-resource.yaml`ç¼ºå°‘ä¹‹å‰çš„å®‰å…¨é…ç½®ï¼Œ`kubectl`ä¼šå°†ç°åœºèµ„æºçš„ç¼ºå¤±å­—æ®µç§»é™¤ã€‚

4. **ç»“æœéªŒè¯ï¼š**

   ```bash
   kubectl get resource -o yaml
   ```

   å‘ç°å…³é”®çš„å®‰å…¨é…ç½®å·²è¢«åˆ é™¤ï¼Œèµ„æºå¤„äºä¸å®‰å…¨çš„çŠ¶æ€ï¼Œå¯èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨ã€‚

**æ€»ç»“ï¼š**

è¯¥Issueæ­ç¤ºäº†ä»å®¢æˆ·ç«¯åº”ç”¨è¿ç§»åˆ°æœåŠ¡ç«¯åº”ç”¨æ—¶ï¼Œ`kubectl`çš„è¡Œä¸ºå¯èƒ½å¯¼è‡´èµ„æºçš„å…³é”®é…ç½®è¢«æ„å¤–åˆ é™¤ï¼Œå­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚å»ºè®®åœ¨è¿ç§»è¿‡ç¨‹ä¸­ï¼Œä»”ç»†æ£€æŸ¥èµ„æºæ¸…å•ï¼Œç¡®ä¿æ‰€æœ‰å…³é”®é…ç½®å‡è¢«åŒ…å«ï¼Œæˆ–è€…ç­‰å¾…å®˜æ–¹ä¿®å¤è¯¥è¡Œä¸ºã€‚

---

## Issue #125357 kubeadm leaves backup files after a successful upgrade

- Issue é“¾æ¥ï¼š[#125357](https://github.com/kubernetes/kubernetes/issues/125357)

### Issue å†…å®¹

#### What happened?

When I successfully upgraded the cluster using kubeadm, I found the /etc/kubernetes/tmp, which contains the backup files, was left.
```
~]# du -h /etc/kubernetes/tmp/
20K	/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-06-06-18-02-44
32M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-02-44/etcd/member/snap
123M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-02-44/etcd/member/wal
155M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-02-44/etcd/member
155M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-02-44/etcd
155M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-02-44
16K	/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-06-06-18-13-10
32M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-13-10/etcd/member/snap
123M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-13-10/etcd/member/wal
155M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-13-10/etcd/member
155M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-13-10/etcd
155M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-13-10
4.0K	/etc/kubernetes/tmp/kubeadm-kubelet-config3316271885
16K	/etc/kubernetes/tmp/kubeadm-backup-manifests-2024-06-06-18-22-34
32M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-22-34/etcd/member/snap
123M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-22-34/etcd/member/wal
155M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-22-34/etcd/member
155M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-22-34/etcd
155M	/etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-22-34
4.0K	/etc/kubernetes/tmp/kubeadm-kubelet-config3278283034
463M	/etc/kubernetes/tmp/
```

#### What did you expect to happen?

I would like to know if this is intended or if it's a bug.
Maybe the document https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/ should make it clear that the user needs to remove the backup files manually.


#### How can we reproduce it (as minimally and precisely as possible)?

Upgrade the cluster with kubeadm.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.8
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.8
```
upgraded from v1.25.16 to v1.26.15 to 1.27.12 to 1.27.8

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

å­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

**åŸå› åŠå¯èƒ½çš„å½±å“ï¼š**

åœ¨ä½¿ç”¨kubeadmæˆåŠŸå‡çº§é›†ç¾¤åï¼Œ/etc/kubernetes/tmp/ç›®å½•ä¸­é—ç•™äº†å¤‡ä»½æ–‡ä»¶ï¼Œè¿™äº›å¤‡ä»½æ–‡ä»¶åŒ…å«äº†etcdçš„æ•°æ®å¤‡ä»½ã€‚etcdæ˜¯Kubernetesé›†ç¾¤çš„å…³é”®ç»„ä»¶ï¼Œè´Ÿè´£å­˜å‚¨é›†ç¾¤çš„çŠ¶æ€å’Œé…ç½®ä¿¡æ¯ï¼Œå…¶ä¸­å¯èƒ½åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼Œä¾‹å¦‚ï¼š

- é›†ç¾¤çš„é…ç½®ä¿¡æ¯
- è®¤è¯å‡­æ®
- å¯†é’¥å’Œè¯ä¹¦
- ServiceAccountä»¤ç‰Œ
- å…¶ä»–æ•æ„Ÿæ•°æ®

å¦‚æœè¿™äº›å¤‡ä»½æ–‡ä»¶çš„æƒé™è®¾ç½®ä¸å½“ï¼Œéæˆæƒç”¨æˆ·ï¼ˆå¦‚å…·æœ‰ä½æƒé™çš„æœ¬åœ°ç”¨æˆ·ï¼‰å¯èƒ½è®¿é—®åˆ°è¿™äº›å¤‡ä»½æ–‡ä»¶ï¼Œä»è€Œæå–å…¶ä¸­çš„æ•æ„Ÿä¿¡æ¯ã€‚ä¸€æ—¦æ”»å‡»è€…è·å–äº†è¿™äº›ä¿¡æ¯ï¼Œå¯èƒ½å¯¼è‡´ä»¥ä¸‹ä¸¥é‡åæœï¼š

- **æœªæˆæƒè®¿é—®ï¼š** æ”»å‡»è€…å¯ä»¥ä½¿ç”¨è·å–çš„å‡­æ®è®¿é—®é›†ç¾¤èµ„æºã€‚
- **æƒé™æå‡ï¼š** åˆ©ç”¨æ•æ„Ÿä¿¡æ¯ï¼Œæ”»å‡»è€…å¯èƒ½æå‡è‡ªèº«æƒé™ï¼Œæ§åˆ¶æ•´ä¸ªé›†ç¾¤ã€‚
- **æ•°æ®ç¯¡æ”¹ï¼š** æ”»å‡»è€…å¯èƒ½ä¿®æ”¹é›†ç¾¤çš„é…ç½®æˆ–æ•°æ®ï¼Œå¯¼è‡´æœåŠ¡ä¸­æ–­æˆ–æ•°æ®æ³„éœ²ã€‚
- **æ¨ªå‘ç§»åŠ¨ï¼š** åˆ©ç”¨é›†ç¾¤ä¸­çš„ä¿¡æ¯ï¼Œæ”»å‡»è€…å¯èƒ½è¿›ä¸€æ­¥æ”»å‡»å…¶ä»–ç³»ç»Ÿæˆ–æœåŠ¡ã€‚

æ ¹æ®**CVSS 3.1è¯„åˆ†æ ‡å‡†**ï¼Œç”±äºè¯¥æ¼æ´å¯èƒ½å¯¼è‡´é«˜æœºå¯†æ€§å½±å“ï¼ˆConfidentiality Impactï¼‰ï¼Œä¸”æ”»å‡»å¤æ‚åº¦ä½ï¼ˆä½æƒé™ç”¨æˆ·å³å¯è®¿é—®ï¼‰ï¼Œç»¼åˆè¯„åˆ†å¯èƒ½è¾¾åˆ°**High**æˆ–æ›´é«˜ã€‚

**æ¦‚å¿µéªŒè¯ï¼ˆProof of Conceptï¼‰ï¼š**

1. **æŸ¥çœ‹å¤‡ä»½æ–‡ä»¶çš„æƒé™è®¾ç½®ï¼š**

   ```bash
   ls -l /etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-02-44/etcd/member/wal
   ```

   å¦‚æœè¾“å‡ºæ˜¾ç¤ºæ–‡ä»¶å¯¹å…¶ä»–ç”¨æˆ·å¯è¯»ï¼Œä¾‹å¦‚ï¼š

   ```
   -rw-r--r-- 1 root root 123456 Jun 6 18:02 /etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-02-44/etcd/member/wal
   ```

   è¡¨æ˜å…¶ä»–ç”¨æˆ·å…·æœ‰è¯»å–æƒé™ã€‚

2. **ä½æƒé™ç”¨æˆ·è¯»å–æ•æ„Ÿæ–‡ä»¶ï¼š**

   åˆ‡æ¢åˆ°éç‰¹æƒç”¨æˆ·ï¼š

   ```bash
   sudo -u someuser bash
   ```

   å°è¯•è¯»å–etcdå¤‡ä»½æ–‡ä»¶ï¼š

   ```bash
   cat /etc/kubernetes/tmp/kubeadm-backup-etcd-2024-06-06-18-02-44/etcd/member/wal
   ```

   å¦‚æœæˆåŠŸè¯»å–ï¼Œåˆ™è¯æ˜å­˜åœ¨æƒé™é—®é¢˜ã€‚

3. **æå–æ•æ„Ÿä¿¡æ¯ï¼š**

   æ”»å‡»è€…å¯ä»¥è§£æetcdçš„WALï¼ˆWrite-Ahead Logï¼‰æ–‡ä»¶ï¼Œä½¿ç”¨å·¥å…·æˆ–è„šæœ¬æå–å…¶ä¸­çš„é”®å€¼æ•°æ®ï¼Œè·å–æ•æ„Ÿä¿¡æ¯ã€‚

**å»ºè®®ï¼š**

- **æƒé™æ§åˆ¶ï¼š** ç¡®ä¿å¤‡ä»½æ–‡ä»¶çš„æƒé™è®¾ç½®æ­£ç¡®ï¼Œä»…é™rootç”¨æˆ·è®¿é—®ã€‚
- **æ¸…ç†å¤‡ä»½ï¼š** åœ¨å‡çº§å®Œæˆåï¼Œè‡ªåŠ¨åˆ é™¤ä¸´æ—¶å¤‡ä»½æ–‡ä»¶ï¼Œé˜²æ­¢é•¿æœŸå­˜åœ¨å¯¼è‡´é£é™©ã€‚
- **æ–‡æ¡£è¯´æ˜ï¼š** åœ¨å®˜æ–¹æ–‡æ¡£ä¸­æ˜ç¡®æŒ‡å‡ºå¤‡ä»½æ–‡ä»¶çš„å®‰å…¨æ³¨æ„äº‹é¡¹ï¼Œæé†’ç”¨æˆ·æ‰‹åŠ¨æ¸…ç†æˆ–æ£€æŸ¥æƒé™ã€‚
- **æ”¹è¿›å·¥å…·ï¼š** ä¿®æ”¹kubeadmçš„è¡Œä¸ºï¼Œåœ¨å‡çº§æˆåŠŸåè‡ªåŠ¨æ¸…ç†ä¸´æ—¶å¤‡ä»½ï¼Œæˆ–æç¤ºç”¨æˆ·è¿›è¡Œå¤„ç†ã€‚

---

## Issue #125343 ephermal containers stdout written to container logs

- Issue é“¾æ¥ï¼š[#125343](https://github.com/kubernetes/kubernetes/issues/125343)

### Issue å†…å®¹

#### What happened?

The stdout of commands running in an ephemeral container started with `kubectl debug --interactive --tty`, including the echo of commands entered by users, is written to the container's log file, where it can be displayed with `kubectl logs` and picked up by log file ingestion.

#### What did you expect to happen?

It is expected that by passing `--tty` the standard file descriptors would be attached to kubectl, and not the log files. This difference in behavior to `kubectl exec --interactive --tty` surprises users who are debugging containers and may inspect the values of files or the environment.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Deploy a standard pod. I used `mccutchen/go-httpbin` as my container image.
2. Create an ephemeral container for the deployed pod. `kubectl debug httpbin --interactive --tty --image=busybox`
3. Inside the tty the terminal is attached to, do something to mimic exposing a password. I chose `echo "topsecretpassword2"`
4. Observe that the shell echo and output of the command is visible in `kubectl logs httpbin -c debugger` and in the kubelet pod logs at `/var/logs/containers`.

#### Anything else we need to know?

<details>

From **/var/log/containers/httpbin-56d7db8d6d-55ql7_default_debugger-f5lwr-4ad6566be3590520b0135c3e621ebd3cd8a2088149174fc5c72553d9861f6234.log**:
```log
2024-06-05T11:23:20.051079932+00:00 stdout F M/ # [[JM
2024-06-05T11:23:20.051079932+00:00 stdout P / #
2024-06-05T11:23:39.975294284+00:00 stdout P e
2024-06-05T11:23:40.057105746+00:00 stdout P c
2024-06-05T11:23:40.139436734+00:00 stdout P h
2024-06-05T11:23:40.319770664+00:00 stdout P o
2024-06-05T11:23:40.523725228+00:00 stdout P
2024-06-05T11:23:40.973457013+00:00 stdout P "
2024-06-05T11:23:41.257984775+00:00 stdout P t
2024-06-05T11:23:41.349361250+00:00 stdout P o
2024-06-05T11:23:41.411602281+00:00 stdout P p
2024-06-05T11:23:41.623549256+00:00 stdout P s
2024-06-05T11:23:41.752892792+00:00 stdout P e
2024-06-05T11:23:41.867923955+00:00 stdout P c
2024-06-05T11:23:42.098814184+00:00 stdout P r
2024-06-05T11:23:42.204348673+00:00 stdout P e
2024-06-05T11:23:42.345322167+00:00 stdout P t
2024-06-05T11:23:42.770278261+00:00 stdout P p
2024-06-05T11:23:42.871145319+00:00 stdout P a
2024-06-05T11:23:43.016143517+00:00 stdout P s
2024-06-05T11:23:43.172425724+00:00 stdout P s
2024-06-05T11:23:43.336211796+00:00 stdout P w
2024-06-05T11:23:43.467535508+00:00 stdout P o
2024-06-05T11:23:43.602068604+00:00 stdout P r
2024-06-05T11:23:43.771406396+00:00 stdout P d
2024-06-05T11:23:43.953834957+00:00 stdout P 2
2024-06-05T11:23:44.269025862+00:00 stdout P "
2024-06-05T11:23:44.470971214+00:00 stdout F M
2024-06-05T11:23:44.471392405+00:00 stdout P topsecretpassword2
2024-06-05T11:23:44.471506289+00:00 stdout F M
2024-06-05T11:23:44.471766497+00:00 stdout P / #
```

</details>

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.3
```

</details>


#### Cloud provider

<details>

- minikube with cri-o and containerd for versions 1.28-1.30
- GKE autocluster 1.28
- EKS 1.29 with Amazon Linux 2 AMI

</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

è¯¥é—®é¢˜æ¶‰åŠæ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

**åŸå› ï¼š**

å½“ä½¿ç”¨ `kubectl debug --interactive --tty` å¯åŠ¨ä¸´æ—¶å®¹å™¨ï¼ˆephemeral containerï¼‰è¿›è¡Œè°ƒè¯•æ—¶ï¼Œç”¨æˆ·åœ¨ç»ˆç«¯ä¸­è¾“å…¥çš„å‘½ä»¤å’Œå‘½ä»¤çš„è¾“å‡ºéƒ½ä¼šè¢«å†™å…¥å®¹å™¨çš„æ—¥å¿—æ–‡ä»¶ã€‚è¿™äº›æ—¥å¿—å¯ä»¥é€šè¿‡ `kubectl logs` å‘½ä»¤æŸ¥çœ‹ï¼Œæˆ–è€…è¢«æ—¥å¿—æ”¶é›†ç³»ç»Ÿæ”¶é›†åˆ°é›†ä¸­å¼æ—¥å¿—å¹³å°ã€‚ç”±äºè¿™äº›æ—¥å¿—ä¸­å¯èƒ½åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼ˆä¾‹å¦‚å¯†ç ã€ç§˜é’¥ã€æœºå¯†é…ç½®ç­‰ï¼‰ï¼Œå¦‚æœæ”»å‡»è€…èƒ½å¤Ÿè®¿é—®è¿™äº›æ—¥å¿—ï¼Œå°±å¯èƒ½è·å–åˆ°è¿™äº›æ•æ„Ÿä¿¡æ¯ã€‚

**å¯èƒ½çš„å½±å“ï¼š**

1. **æ•æ„Ÿä¿¡æ¯æ³„éœ²ï¼š** æ”»å‡»è€…å¯ä»¥ä»æ—¥å¿—ä¸­è·å–ç”¨æˆ·åœ¨è°ƒè¯•è¿‡ç¨‹ä¸­è¾“å…¥çš„å‘½ä»¤å’ŒæŸ¥çœ‹çš„æ•æ„Ÿæ•°æ®ï¼Œä¾‹å¦‚æ•°æ®åº“å¯†ç ã€APIå¯†é’¥ç­‰ã€‚

2. **æ‰©å¤§æ”»å‡»é¢ï¼š** å¦‚æœæ—¥å¿—è¢«é›†ä¸­æ”¶é›†åˆ°æ—¥å¿—ç®¡ç†ç³»ç»Ÿï¼Œè€Œè¯¥ç³»ç»Ÿçš„è®¿é—®æƒé™æ§åˆ¶ä¸ä¸¥æ ¼ï¼Œæ›´å¤šçš„äººå‘˜å¯èƒ½æœ‰æœºä¼šæ¥è§¦åˆ°è¿™äº›æ•æ„Ÿä¿¡æ¯ã€‚

3. **æƒé™æå‡ï¼š** æ”»å‡»è€…åˆ©ç”¨è·å–çš„æ•æ„Ÿä¿¡æ¯ï¼Œå¯èƒ½å¯¹ç³»ç»Ÿè¿›è¡Œè¿›ä¸€æ­¥çš„æ”»å‡»ï¼Œè·å–æ›´é«˜çš„æƒé™æˆ–è®¿é—®æ›´å¤šçš„èµ„æºã€‚

**ç¬¦åˆé£é™©åˆ¤æ–­æ ‡å‡†ï¼š**

1. **å¯è¢«æ”»å‡»è€…åˆ©ç”¨ï¼š** æ”»å‡»è€…åªéœ€è¦å…·å¤‡è®¿é—®å®¹å™¨æ—¥å¿—çš„æƒé™ï¼Œå³å¯è·å–æ•æ„Ÿä¿¡æ¯ï¼Œè€Œè®¿é—®æ—¥å¿—çš„æƒé™é€šå¸¸æ¯”ç›´æ¥è®¿é—®å®¹å™¨çš„æƒé™è¦æ±‚æ›´ä½ã€‚

2. **å¯èƒ½æˆä¸ºæ¼æ´å¹¶è¢«åˆ†é… CVE ç¼–å·ï¼š** æ ¹æ® CVSS 3.1 è¯„çº§ï¼Œè¿™ä¸ªæ¼æ´çš„å¾—åˆ†å¯èƒ½åœ¨é«˜å±ï¼ˆHighï¼‰èŒƒå›´å†…ã€‚

   **CVSS 3.1 è¯„åˆ†è®¡ç®—ï¼š**

   - **æ”»å‡»å‘é‡ (AV)ï¼š** ç½‘ç»œ (N)
   - **æ”»å‡»å¤æ‚åº¦ (AC)ï¼š** ä½ (L)
   - **æƒé™è¦æ±‚ (PR)ï¼š** ä½ (L)
   - **ç”¨æˆ·äº¤äº’ (UI)ï¼š** æ—  (N)
   - **æœºå¯†æ€§å½±å“ (C)ï¼š** é«˜ (H)
   - **å®Œæ•´æ€§å½±å“ (I)ï¼š** ä½ (L) / æ—  (N) ï¼ˆè§†å…·ä½“æƒ…å†µï¼‰
   - **å¯ç”¨æ€§å½±å“ (A)ï¼š** æ—  (N)

   **ç»¼åˆå¾—åˆ†ï¼š** 7.5ï¼ˆé«˜å±ï¼‰

3. **æ—¥å¿—ä¸­æ³„éœ²å‡­æ®çš„é£é™©ï¼š** æ”»å‡»è€…å¯èƒ½ä»¥æ¯”æ³„éœ²å‡­æ®æ›´ä½çš„æƒé™è®¿é—®æ—¥å¿—ï¼Œä»è€Œè·å–æ›´é«˜çº§åˆ«çš„å‡­æ®ï¼Œç¬¦åˆæé«˜é£é™©è¯„çº§çš„æ¡ä»¶ã€‚

**PoCï¼ˆæ¦‚å¿µéªŒè¯ï¼‰ç¤ºä¾‹ï¼š**

1. **éƒ¨ç½²ä¸€ä¸ªæ ‡å‡†çš„ Podï¼š**

   ```bash
   kubectl run sensitive-app --image=yourappimage
   ```

2. **ä½¿ç”¨ kubectl debug å¯åŠ¨ä¸€ä¸ªä¸´æ—¶å®¹å™¨è¿›è¡Œè°ƒè¯•ï¼š**

   ```bash
   kubectl debug sensitive-app --interactive --tty --image=busybox
   ```

3. **åœ¨è°ƒè¯•å®¹å™¨ä¸­ï¼Œæ‰§è¡ŒæŸ¥çœ‹æ•æ„Ÿä¿¡æ¯çš„å‘½ä»¤ï¼š**

   ```bash
   # å‡è®¾å­˜åœ¨åŒ…å«æ•æ„Ÿä¿¡æ¯çš„æ–‡ä»¶ /etc/secret-config
   cat /etc/secret-config
   ```

   æˆ–è€…è¾“å…¥æ•æ„Ÿä¿¡æ¯ï¼š

   ```bash
   echo "MySecretPassword123"
   ```

4. **é€€å‡ºè°ƒè¯•ä¼šè¯ã€‚**

5. **æŸ¥çœ‹è°ƒè¯•å®¹å™¨çš„æ—¥å¿—ï¼š**

   ```bash
   kubectl logs sensitive-app -c debugger
   ```

6. **åœ¨æ—¥å¿—ä¸­æ‰¾åˆ°åˆšæ‰è¾“å…¥çš„å‘½ä»¤å’Œè¾“å‡ºï¼ŒåŒ…æ‹¬æ•æ„Ÿä¿¡æ¯ï¼š**

   ```
   / # cat /etc/secret-config
   sensitive_key=ABC123XYZ
   / # echo "MySecretPassword123"
   MySecretPassword123
   / #
   ```

**æ€»ç»“ï¼š**

ç”±äºè°ƒè¯•è¿‡ç¨‹ä¸­ç”¨æˆ·è¾“å…¥çš„å‘½ä»¤å’Œè¾“å‡ºè¢«è®°å½•åœ¨æ—¥å¿—ä¸­ï¼Œä¸”è¿™äº›æ—¥å¿—å¯èƒ½è¢«å…·æœ‰è¾ƒä½æƒé™çš„ç”¨æˆ·æˆ–ç³»ç»Ÿè®¿é—®ï¼Œå¯¼è‡´æ•æ„Ÿä¿¡æ¯æ³„éœ²ï¼Œå­˜åœ¨é«˜é£é™©çš„å®‰å…¨æ¼æ´ï¼Œå»ºè®®å°½å¿«ä¿®å¤ã€‚

---

# ğŸ“Œ ä¸æ¶‰åŠå®‰å…¨é£é™©çš„ Issues (66 ä¸ª)

## Issue #125810 NodePort service with endpoints has "has no local endpoints" in iptables

- Issue é“¾æ¥ï¼š[#125810](https://github.com/kubernetes/kubernetes/issues/125810)

### Issue å†…å®¹

#### What happened?

I set up a NodePort service pointing to an envoy gateway. Initially this works, and the port is connected to the envoy gateway correctly. However when something changes about the gateway (does not matter what), the service does not connect anymore. In iptables-save, I found the following rule which blocks traffic:
```
-A KUBE-EXTERNAL-SERVICES -p tcp -m comment --comment "envoy-gateway-system/envoy-default-eg-e41e7b31:http-80 has no local endpoints" -m addrtype --dst-type LOCAL -m tcp --dport 30100 -j DROP
```

Even though this rule also still exists:
```
-A KUBE-NODEPORTS -p tcp -m comment --comment "envoy-gateway-system/envoy-default-eg-e41e7b31:http-80" -m tcp --dport 30100 -j KUBE-EXT-GGWL76EB3A5WOB4Q
-A KUBE-EXT-GGWL76EB3A5WOB4Q -s 10.1.0.0/16 -m comment --comment "pod traffic for envoy-gateway-system/envoy-default-eg-e41e7b31:http-80 external destinations" -j KUBE-SVC-GGWL76EB3A5WOB4Q
-A KUBE-EXT-GGWL76EB3A5WOB4Q -m comment --comment "masquerade LOCAL traffic for envoy-gateway-system/envoy-default-eg-e41e7b31:http-80 external destinations" -m addrtype --src-type LOCAL -j KUBE-MARK-MASQ
-A KUBE-EXT-GGWL76EB3A5WOB4Q -m comment --comment "route LOCAL traffic for envoy-gateway-system/envoy-default-eg-e41e7b31:http-80 external destinations" -m addrtype --src-type LOCAL -j KUBE-SVC-GGWL76EB3A5WOB4Q
-A KUBE-SVC-GGWL76EB3A5WOB4Q ! -s 10.1.0.0/16 -d 10.0.0.251/32 -p tcp -m comment --comment "envoy-gateway-system/envoy-default-eg-e41e7b31:http-80 cluster IP" -m tcp --dport 80 -j KUBE-MARK-MASQ
-A KUBE-SVC-GGWL76EB3A5WOB4Q -m comment --comment "envoy-gateway-system/envoy-default-eg-e41e7b31:http-80 -> 10.1.1.19:10080" -j KUBE-SEP-TW3CXZOYNBZZ6B2U
```

#### What did you expect to happen?

When the envoy gateway changes, the nodeport should still work.

#### How can we reproduce it (as minimally and precisely as possible)?

The nodeport service:

```yaml
apiVersion: v1
kind: Service
metadata:
  name: envoy-default-eg-e41e7b31
  namespace: envoy-gateway-system
spec:
  ports:
    - name: http-80
      protocol: TCP
      port: 80
      targetPort: 10080
      nodePort: 30100
    - name: https-443
      protocol: TCP
      port: 443
      targetPort: 10443
      nodePort: 30101
  selector:
    app.kubernetes.io/component: proxy
    app.kubernetes.io/managed-by: envoy-gateway
    app.kubernetes.io/name: envoy
    gateway.envoyproxy.io/owning-gateway-name: eg
    gateway.envoyproxy.io/owning-gateway-namespace: default
  type: NodePort
```

The gateway:

```yaml
apiVersion: gateway.networking.k8s.io/v1beta1
kind: GatewayClass
metadata:
  name: eg
spec:
  controllerName: gateway.envoyproxy.io/gatewayclass-controller
---
apiVersion: gateway.networking.k8s.io/v1beta1
kind: Gateway
metadata:
  name: eg
spec:
  gatewayClassName: eg
  listeners:
  - name: http
    protocol: HTTP
    port: 80
  - name: https
    protocol: HTTPS
    hostname: "bot.yele.dev"
    port: 443
    tls:
      mode: Terminate
      certificateRefs:
      - kind: Secret
        name: eg-https
```

#### Anything else we need to know?

```
; kubectl -n envoy-gateway-system get endpointslices
NAME                              ADDRESSTYPE   PORTS                           ENDPOINTS   AGE
envoy-default-eg-e41e7b31-cx6mm   IPv4          10443,10080                     10.1.1.19   66m
envoy-gateway-mxd5w               IPv4          19001,18000,18002 + 1 more...   10.1.2.80   4h4m
; kubectl -n envoy-gateway-system get endpoints envoy-default-eg-e41e7b31
NAME                        ENDPOINTS                         AGE
envoy-default-eg-e41e7b31   10.1.1.19:10443,10.1.1.19:10080   57m
```

When I replace the service, it starts working again.

#### Kubernetes version

<details>

```console
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.2", GitCommit:"5835544ca568b757a8ecae5c153f317e5736700e", GitTreeState:"clean", BuildDate:"2022-09-21T14:33:49Z", GoVersion:"go1.19.1", Compiler:"gc", Platform:"darwin/arm64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"28", GitVersion:"v1.28.4", GitCommit:"bae2c62678db2b5053817bc97181fcc2e8388103", GitTreeState:"archive", BuildDate:"1980-01-01T00:00:00Z", GoVersion:"go1.21.5", Compiler:"gc", Platform:"linux/amd64"}
WARNING: version difference between client (1.25) and server (1.28) exceeds the supported minor version skew of +/-1```

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
BUG_REPORT_URL="https://github.com/NixOS/nixpkgs/issues"
BUILD_ID="23.11.20240213.01885a0"
DOCUMENTATION_URL="https://nixos.org/learn.html"
HOME_URL="https://nixos.org/"
ID=nixos
LOGO="nix-snowflake"
NAME=NixOS
PRETTY_NAME="NixOS 23.11 (Tapir)"
SUPPORT_END="2024-06-30"
SUPPORT_URL="https://nixos.org/community.html"
VERSION="23.11 (Tapir)"
VERSION_CODENAME=tapir
VERSION_ID="23.11"
$ uname -a
Linux vmbox-1 6.1.77 #1-NixOS SMP PREEMPT_DYNAMIC Mon Feb  5 20:13:03 UTC 2024 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125807 pull-kubernetes-typecheck doesn't notice compile errors in staging test files

- Issue é“¾æ¥ï¼š[#125807](https://github.com/kubernetes/kubernetes/issues/125807)

### Issue å†…å®¹

_Originally posted by @liggitt in https://github.com/kubernetes/kubernetes/issues/125571#issuecomment-2198419708_

/sig testing
/area test
/kind bug

`pull-kubernetes-typecheck â€” Job succeeded.`
yet:
```
ERROR: staging/src/k8s.io/dynamic-resource-allocation/structured/namedresources/cel/compile.go:1: : # k8s.io/dynamic-resource-allocation/structured/namedresources/cel [k8s.io/dynamic-resource-allocation/structured/namedresources/cel.test]
ERROR: staging/src/k8s.io/dynamic-resource-allocation/structured/namedresources/cel/compile_test.go:127:14: undefined: Compiler (typecheck)
```

(I fixed this in that PR, but it should be reproducible by reintroducing a compile error in a test file like that)

### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125792 Invalid kube-reserved configuration in kubelet causes frequent node status patch updates ignoring node-status-report-frequency

- Issue é“¾æ¥ï¼š[#125792](https://github.com/kubernetes/kubernetes/issues/125792)

### Issue å†…å®¹

#### What happened?

#### Summary

If the CPU values set in kube-reserved/system-reserved configurations are in terms of decimal values of millicores, Kubelet should throw an error and not start, Else, it should throw a warning to aide the triaging. 

Withiout this check, the issue manifests in unexpected places, like frequent status updates, ignoring the node-status-report-frequency value.

#### Detailed description

1. We have set the Kubelet's node-status-report-frequency property to 5 minutes to reduce the frequency of the status updates to kube-apiserver
2. But we noticed that this is not honoured and the updates were sent every few seconds
3. On further triaging, we found that, since we  set the kube-reserved values as a function of number of cores, for some worker node compute shapes, we are setting the kube-reserved cpu values as decimal values (in millicores).

E.g. 
"kubeReserved": {
    "cpu": "237.5m",
    "memory": "10Gi"
  }

Kubelet accepted the values provided and kubelet was running without any errors or warnings.

The kubelet module that posts the statuses to kube-apiserver  calculates the current status (to compare  with the status posted already to API server) so as to post the status even if the node-status-report-frequency value is not exhausted.

This module calculates the current capacity in terms of millicores and this results in mismatch with the status stored in kube-apiserver.  

#### What did you expect to happen?

Kubelet should fail to start or throw warnings when the kube-reserved/system-reserved values are incorrect

#### How can we reproduce it (as minimally and precisely as possible)?

Set the kubeReserved value to a decimal value as below and increase the node-status-report-frequency to 5 minutes

` "kubeReserved": {
    "cpu": "237.5m",
    "memory": "10Gi"
  },``

You would notice that the status is posted every few seconds 


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.2
Server Version: v1.27.2
```

</details>


#### Cloud provider

<details>
Oracle Container Engine for Kubernetes
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125775 CEL: CallCost the function args mix receiver with argument in way that is prone to misuse

- Issue é“¾æ¥ï¼š[#125775](https://github.com/kubernetes/kubernetes/issues/125775)

### Issue å†…å®¹

#### What happened?

Reported here: https://github.com/kubernetes/kubernetes/pull/125571/files/b0bcc0b20d5d97efdd30215ea410c3bc56d8916b..e2cee4d48f596d6ac4032e22bb08cc98252cb3f5#r1657864062

#### What did you expect to happen?

CallCost is improved to provide receiver argument separate from positional arguments (nil if absent).

#### How can we reproduce it (as minimally and precisely as possible)?

See report

#### Anything else we need to know?

_No response_

#### Kubernetes version

N/A

#### Cloud provider

N/A

#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125770 unexpected grpc error (use of closed network connection) during apiserver lifecycle

- Issue é“¾æ¥ï¼š[#125770](https://github.com/kubernetes/kubernetes/issues/125770)

### Issue å†…å®¹

#### What happened?

We have recently noticed an increase in gRPC errors (specifically, `use of closed network connection`) in apiserver logs.

log sample
`
I0619 00:22:[31.927056 11](tel:3192705611) http2_client.go:959] "[transport] [client-transport 0xc004144900] Closing: connection error: desc = \"error reading from server: read tcp 10.0.33.210:43716â†’10.0.32.16:2379: use of closed network connection\"\n"
`

These errors occurs intermittently every few hours during the lifecycle of apiserver.

Although there have been no reported performance regression yet, the recurring error message is concerning.



#### What did you expect to happen?

Assume the error may occur when the apiserver is restarted, rather than during its lifecycle.

#### How can we reproduce it (as minimally and precisely as possible)?

launch apiserver against k8s version `v1.28.9`







#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

Kubernetes v1.28.9


</details>


#### Cloud provider

<details>

AWS EKS
</details>


#### OS version

<details>

N/A

</details>


#### Install tools

<details>

N/A

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #125763 Unable to set enforceNodeAllocatable for system-reserved on cgroup v1 systems

- Issue é“¾æ¥ï¼š[#125763](https://github.com/kubernetes/kubernetes/issues/125763)

### Issue å†…å®¹

#### What happened?

While trying to set enforceNodeAllocatable (kubelet setting) for system-reserved on cgroup v1 systems using, 

```yaml
    enforceNodeAllocatable:
      - "pods"
      - "system-reserved"
```
ends up kubelet not being able to start with following error, 

```
Jun 27 14:03:55 ip-10-0-11-74 kubenswrapper[2669]: I0627 14:03:55.845147    2669 node_container_manager_linux.go:116] "Enforcing system reserved on cgroup" cgroupName="/system.slice" limits={"cpu":"1","ephemeral-storage":"1Gi","memory":"1Gi"}
Jun 27 14:03:55 ip-10-0-11-74 kubenswrapper[2669]: E0627 14:03:55.845233    2669 kubelet.go:1559] "Failed to start ContainerManager" err="Failed to enforce System Reserved Cgroup Limits on \"/system.slice\": cgroup [\"system\"] has some missing paths: /sys/fs/cgroup/hugetlb/system.slice, /sys/fs/cgroup/cpuset/system.slice"
Jun 27 14:03:55 ip-10-0-11-74 systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Jun 27 14:03:55 ip-10-0-11-74 systemd[1]: kubelet.service: Failed with result 'exit-code'.
```

It works fine on cgroup v2 systems though. 

#### What did you expect to happen?

Users should be able to set enforceNodeAllocatable for system-reserved on cgroup v1 systems

#### How can we reproduce it (as minimally and precisely as possible)?

Make sure the node is using cgroup v1 and apply the kubelet's enforceNodeAllocatable for system-reserved. 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125762 pods with PVs stuck Pending even though PVCs bound to PVs correctly

- Issue é“¾æ¥ï¼š[#125762](https://github.com/kubernetes/kubernetes/issues/125762)

### Issue å†…å®¹

#### What happened?

I'm running a kubeadm cluster on RHEL9 VMs. I was encountering the panic "integer divide by zero" [issue ](https://github.com/kubernetes/kubernetes/issues/124930) so I just upgraded all of my VMs to the latest kubernetes 1.30.2 when it released.
I used helm to install my application. Most of the pods are running fine, but all pods that have a Persistent Volume (PV) are stuck in Pending. This usually means that the PV is not binding to the PVC correctly, but in this case, all are bound as they should be.

I described the nodes, and checked for resource shortage issues, but there are none. When I describe the Pending pods, the only event is
 
```  
Warning  FailedScheduling  4m26s (x663 over 114m)  default-scheduler  0/4 nodes are available: preemption: 0/4 nodes are available: 4 Preemption is not helpful for scheduling.
```

I checked the kube-scheduler-mgr pod logs and it was full of these errors:

```
W0626 11:26:14.020041       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found, clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:discovery" not found]
E0626 11:26:14.020095       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope: RBAC: [clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found, clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:discovery" not found]
W0626 11:26:14.020195       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0626 11:26:14.020232       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0626 11:26:14.020282       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0626 11:26:14.020351       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0626 11:26:14.020448       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0626 11:26:14.020514       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0626 11:26:14.020604       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0626 11:26:14.020667       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0626 11:26:14.020753       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0626 11:26:14.020825       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0626 11:26:14.020915       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0626 11:26:14.020984       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W0626 11:26:14.021065       1 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0626 11:26:14.021157       1 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0626 11:26:14.030767       1 reflector.go:547] runtime/asm_amd64.s:1695: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system": RBAC: [clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found, clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:discovery" not found]
E0626 11:26:14.030890       1 reflector.go:150] runtime/asm_amd64.s:1695: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system": RBAC: [clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found, clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:discovery" not found]
I0626 11:26:15.109531       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0626 11:26:15.609850       1 leaderelection.go:250] attempting to acquire leader lease kube-system/kube-scheduler...
I0626 11:26:33.327740       1 leaderelection.go:260] successfully acquired lease kube-system/kube-scheduler
```

I checked and all of the clusterroles that it said are missing were indeed present, so I just updated them as follows:

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:basic-user
rules:
- apiGroups:
  - '*'
  resources:
  - '*'
  verbs:
  - '*'
- nonResourceURLs:
  - '*'
  verbs:
  - '*'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:discovery
rules:
- apiGroups:
  - '*'
  resources:
  - '*'
  verbs:
  - '*'
- nonResourceURLs:
  - '*'
  verbs:
  - '*'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:public-info-viewer
rules:
- apiGroups:
  - '*'
  resources:
  - '*'
  verbs:
  - '*'
- nonResourceURLs:
  - '*'
  verbs:
  - '*'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-scheduler
rules:
- apiGroups:
  - '*'
  resources:
  - '*'
  verbs:
  - '*'
- nonResourceURLs:
  - '*'
  verbs:
  - '*'
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:volume-scheduler
rules:
- apiGroups:
  - '*'
  resources:
  - '*'
  verbs:
  - '*'
- nonResourceURLs:
  - '*'
  verbs:
  - '*'
---
```

After applying these, the kube-scheduler-mgr pod no longer generates any new logs, even when I delete the pending pods, or even when i delete the kube-scheduler-mgr pod, then follow it's logs while I delete a Pending pod. The kube-scheduler-mgr pod is not crashing or reporting any errors.

I am at a loss to figure out why all of the pods that have PVs are stuck pending, while all of their PVs are properly bound to the PVCs. It is also weird that the kube-scheduler-mgr is reporting the clusterroles as missing, although when I list clusterroles, I see them there. If the kube-scheduler-mgr pod were still generating logs, that would be helpful, but ever since I applied the file to grant the full permissions to the clusterroles it said were "missing" it seems that no action prompts it to generate logs. I'm wondering if this is a bug that was introduced in this release. I've often had pods stuck Pending when the PVCs are not binding correctly to the PVs, but I've never seen them stuck Pending when the binds are successful and there is no resource contention like this.

Oh, and all nodes are in a Ready state, of course.
```

#### What did you expect to happen?

pods to run

#### How can we reproduce it (as minimally and precisely as possible)?

Deploy a kubeadm cluster on kubernetes v1.30.2 and try to run a pod that has a PV.

#### Anything else we need to know?

@mikkeloscar  @AxeZhan tagging you here in case you can think of anything in the latest fix that may be causing this.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.2

```

</details>


#### Cloud provider

AWS


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Red Hat Enterprise Linux"
VERSION="9.0 (Plow)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="9.0"
PLATFORM_ID="platform:el9"
PRETTY_NAME="Red Hat Enterprise Linux 9.0 (Plow)"
ANSI_COLOR="0;31"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:redhat:enterprise_linux:9::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/red_hat_enterprise_linux/9/"
BUG_REPORT_URL="https://bugzilla.redhat.com/"

REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 9"
REDHAT_BUGZILLA_PRODUCT_VERSION=9.0
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.0"
Red Hat Enterprise Linux release 9.0 (Plow)
Red Hat Enterprise Linux release 9.0 (Plow)

$ uname -a
Linux mgr 5.14.0-70.75.1.el9_0.x86_64 #1 SMP PREEMPT Thu Sep 14 13:42:21 EDT 2023 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>
helm version
version.BuildInfo{Version:"v3.14.0", GitCommit:"3fc9f4b2638e76f26739cd77c7017139be81d0ea", GitTreeState:"clean", GoVersion:"go1.21.5"}

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
ctr -v
ctr containerd.io 1.7.18

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125760 flaky test: TestCustomResourceDefaultingWithoutWatchCache

- Issue é“¾æ¥ï¼š[#125760](https://github.com/kubernetes/kubernetes/issues/125760)

### Issue å†…å®¹

#### Which jobs are flaking?

https://prow.k8s.io/view/gs/kubernetes-jenkins/pr-logs/pull/125758/pull-kubernetes-integration/1806280169099366400

#### Which tests are flaking?

--- FAIL: TestCustomResourceDefaultingWithoutWatchCache (4.58s)

#### Since when has it been flaking?

unknown

#### Testgrid link

https://testgrid.k8s.io/presubmits-kubernetes-blocking#pull-kubernetes-integration

#### Reason for failure (if possible)

```
{Failed;  === RUN   TestCustomResourceDefaultingWithoutWatchCache
    testserver.go:280: Resolved testserver package path to: "/home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiextensions-apiserver/pkg/cmd/server/testing"
    testserver.go:164: runtime-config=map[api/all:true]
    testserver.go:165: Starting apiextensions-apiserver on port 33019...
I0627 11:11:35.766913  103190 serving.go:380] Generated self-signed cert (/tmp/apiextensions-apiserver4170115585/apiserver.crt, /tmp/apiextensions-apiserver4170115585/apiserver.key)
W0627 11:11:36.322777  103190 mutation_detector.go:53] Mutation detector is enabled, this will result in memory leakage.
I0627 11:11:36.328509  103190 handler.go:286] Adding GroupVersion apiextensions.k8s.io v1 to ResourceManager
W0627 11:11:36.328547  103190 genericapiserver.go:761] Skipping API apiextensions.k8s.io/v1beta1 because it has no resources.
W0627 11:11:36.328727  103190 mutation_detector.go:53] Mutation detector is enabled, this will result in memory leakage.
    testserver.go:191: Waiting for /healthz to be ok...
I0627 11:11:36.341029  103190 dynamic_serving_content.go:135] "Starting controller" name="serving-cert::/tmp/apiextensions-apiserver4170115585/apiserver.crt::/tmp/apiextensions-apiserver4170115585/apiserver.key"
I0627 11:11:36.341623  103190 secure_serving.go:213] Serving securely on 127.0.0.1:33019
I0627 11:11:36.341819  103190 customresource_discovery_controller.go:292] Starting DiscoveryController
I0627 11:11:36.341859  103190 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0627 11:11:36.341959  103190 naming_controller.go:294] Starting NamingConditionController
I0627 11:11:36.342008  103190 controller.go:142] Starting OpenAPI controller
I0627 11:11:36.342030  103190 controller.go:90] Starting OpenAPI V3 controller
I0627 11:11:36.342053  103190 crd_finalizer.go:269] Starting CRDFinalizer
I0627 11:11:36.342086  103190 establishing_controller.go:79] Starting EstablishingController
I0627 11:11:36.342104  103190 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0627 11:11:36.342117  103190 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
W0627 11:11:36.342225  103190 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "http://127.1.2.3:12345/api/v1/services?limit=500&resourceVersion=0": dial tcp 127.1.2.3:12345: connect: connection refused
E0627 11:11:36.342333  103190 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"http://127.1.2.3:12345/api/v1/services?limit=500&resourceVersion=0\": dial tcp 127.1.2.3:12345: connect: connection refused" logger="UnhandledError"
I0627 11:11:36.546629  103190 handler.go:286] Adding GroupVersion tests.example.com v1beta1 to ResourceManager
W0627 11:11:37.822574  103190 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: Get "http://127.1.2.3:12345/api/v1/services?limit=500&resourceVersion=0": dial tcp 127.1.2.3:12345: connect: connection refused
E0627 11:11:37.822650  103190 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: Get \"http://127.1.2.3:12345/api/v1/services?limit=500&resourceVersion=0\": dial tcp 127.1.2.3:12345: connect: connection refused" logger="UnhandledError"
W0627 11:11:39.048433  103190 warnings.go:70] unknown field "alpha"
W0627 11:11:39.048463  103190 warnings.go:70] unknown field "beta"
W0627 11:11:39.048468  103190 warnings.go:70] unknown field "delta"
W0627 11:11:39.048472  103190 warnings.go:70] unknown field "epsilon"
W0627 11:11:39.048476  103190 warnings.go:70] unknown field "gamma"
    defaulting_test.go:288: Creating CR and expecting defaulted fields in spec, but status does not exist at all
    defaulting_test.go:301: CR created: map[string]interface {}{"apiVersion":"tests.example.com/v1beta1", "kind":"Foo", "metadata":map[string]interface {}{"creationTimestamp":"2024-06-27T11:11:39Z", "generation":1, "managedFields":[]interface {}{map[string]interface {}{"apiVersion":"tests.example.com/v1beta1", "fieldsType":"FieldsV1", "fieldsV1":map[string]interface {}{"f:spec":map[string]interface {}{".":map[string]interface {}{}, "f:a":map[string]interface {}{}, "f:b":map[string]interface {}{}, "f:replicas":map[string]interface {}{}, "f:v1beta1":map[string]interface {}{}}}, "manager":"integration.test", "operation":"Update", "time":"2024-06-27T11:11:39Z"}}, "name":"foo", "resourceVersion":"3714", "uid":"1fd9d30a-5657-4f26-bff8-e7b2f99cd18c"}, "spec":map[string]interface {}{"a":"a", "b":"B", "replicas":1, "v1beta1":"v1beta1", "v1beta2":"v1beta2"}}
    defaulting_test.go:308: Updating status and expecting 'a' and 'b' to show up.
    defaulting_test.go:315: Add 'c' default to the storage version and wait until GET sees it in both status and spec
    defaulting_test.go:318: wait until GET sees 'c' in both status and spec
    defaulting_test.go:325: will retry, did not find spec.c in the object
    defaulting_test.go:335: wait until GET sees 'c' in both status and spec of cached get
    defaulting_test.go:352: verify LIST sees 'c' in both status and spec
    defaulting_test.go:361: verify LIST from cache sees 'c' in both status and spec
    defaulting_test.go:374: verify WATCH sees 'c' in both status and spec
W0627 11:11:39.282080  103190 watcher.go:338] watch chan error: etcdserver: mvcc: required revision has been compacted
    defaulting_test.go:382: unexpected watch event: ERROR, &v1.Status{TypeMeta:v1.TypeMeta{Kind:"Status", APIVersion:"v1"}, ListMeta:v1.ListMeta{SelfLink:"", ResourceVersion:"", Continue:"", RemainingItemCount:(*int64)(nil)}, Status:"Failure", Message:"The resourceVersion for the provided watch is too old.", Reason:"Expired", Details:(*v1.StatusDetails)(nil), Code:410}
I0627 11:11:39.282619  103190 customresource_discovery_controller.go:328] Shutting down DiscoveryController
I0627 11:11:39.282624  103190 object_count_tracker.go:151] "StorageObjectCountTracker pruner is exiting"
I0627 11:11:39.282651  103190 controller.go:170] Shutting down OpenAPI controller
I0627 11:11:39.282657  103190 naming_controller.go:305] Shutting down NamingConditionController
I0627 11:11:39.282675  103190 apiapproval_controller.go:201] Shutting down KubernetesAPIApprovalPolicyConformantConditionController
I0627 11:11:39.282674  103190 dynamic_serving_content.go:149] "Shutting down controller" name="serving-cert::/tmp/apiextensions-apiserver4170115585/apiserver.crt::/tmp/apiextensions-apiserver4170115585/apiserver.key"
I0627 11:11:39.282681  103190 crd_finalizer.go:281] Shutting down CRDFinalizer
I0627 11:11:39.282721  103190 nonstructuralschema_controller.go:207] Shutting down NonStructuralSchemaConditionController
I0627 11:11:39.282742  103190 secure_serving.go:258] Stopped listening on 127.0.0.1:33019
I0627 11:11:39.282761  103190 establishing_controller.go:90] Shutting down EstablishingController
I0627 11:11:39.282778  103190 controller.go:120] Shutting down OpenAPI V3 controller
I0627 11:11:39.282762  103190 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
--- FAIL: TestCustomResourceDefaultingWithoutWatchCache (4.58s)
;}
```

#### Anything else we need to know?

_No response_

#### Relevant SIG(s)

/sig api-machinery


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125734 Volume with storage medium "memory" must be title case with unhelpful error message

- Issue é“¾æ¥ï¼š[#125734](https://github.com/kubernetes/kubernetes/issues/125734)

### Issue å†…å®¹

#### What happened?

When you create a deployment with volume that has a medium of `memory` (note here that it is not title case), it fails with an error message of `unknown storage medium "memory"` and the pod gets stuck in the `ContainerCreating` state. As this is a `beta` feature (enabled with the `SizeMemoryBackedVolumes` feature gate), we originally thought that this feature gate was disabled and was the cause of this error message. It turns out that is was simply because it was `memory` and not `Memory`.

#### What did you expect to happen?

Either accept `memory` as a value or there would be a more meaningful error message with something along the lines of `unknown storage medium 'memory' - only the following are accepted: 'Memory', [...]`.

In addition, it was misleading that this value was accepted when applying. I would have expected the apply to fail when attempting to apply an invalid value for this field. 

#### How can we reproduce it (as minimally and precisely as possible)?

Create a deployment with the following volume definition:
```yaml
volumes:
- emptyDir:
     medium: memory # note the lower case.
     sizeLimit: 32Mi
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
```

</details>


#### Cloud provider

<details>
This originally occurred on EKS, but we were also able to replicate this on a `kind` cluster.
</details>

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125720 Pod Memory Request=Limit Triggers Pagecache Reclamation and Service Jitter on Node

- Issue é“¾æ¥ï¼š[#125720](https://github.com/kubernetes/kubernetes/issues/125720)

### Issue å†…å®¹

#### What happened?

##### Description:
When deploying pods on a Kubernetes node with memory request equal to memory limit, despite no memory overcommitment on the host, scheduling a new pod triggers memory reclamation, leading to service jitter for some pods.
<img width="1285" alt="image" src="https://github.com/kubernetes/kubernetes/assets/58540165/dfdfa8ed-3d09-4060-b929-250688c8d786">


##### Observations:
1. Monitoring indicates that pagecache reclamation is being triggered.
2. Before the issue occurs, the node has sufficient memory requests available for scheduling the pod, and the host memory is not overcommitted.
3. Comparison of RSS (via `ps/top` on the host or `docker stats`) and `container_memory_usage_bytes` (from cAdvisor metrics, which are derived from `/sys/fs/cgroup/<pod_uid>/<container_uid>/memory/memory.usage_in_bytes`) shows a significant difference:
   - RSS: 14.2GB
   - `container_memory_usage_bytes`: 32GB (equal to the limit)

   This discrepancy can be attributed to RSS not including pagecache, while `memory.usage_in_bytes` does.

##### Questions and Analysis:
- If the host memory is not overcommitted and there are sufficient memory requests available for scheduling, why does scheduling a new pod trigger pagecache reclamation?
- Given that the limit already constrains the total memory usage (including pagecache), why does this issue arise?

##### Detailed Scenario:
- Our pods (Kafka, Elasticsearch) make extensive use of pagecache.
- Based on documentation, RSS metrics are from the `/proc` filesystem, while `memory.usage_in_bytes` is from the cgroup filesystem. The former excludes pagecache, and the latter includes it.
- This makes sense because the pods are designed to use significant pagecache. However, in a non-overcommitted memory scenario with sufficient remaining requests, new pod scheduling shouldn't cause pagecache reclamation issues.

##### Additional Information:
- Logs and monitoring metrics can be provided upon request.
- Detailed setup and configuration files can be shared if needed.

Thank you for investigating this issue.

#### What did you expect to happen?

The node should handle new pod scheduling without triggering pagecache reclamation if the total memory usage is within limits and memory requests are available.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Deploy pods on a Kubernetes node with memory requests equal to memory limits.
2. Ensure the node has available memory requests and is not overcommitted.
3. Schedule a new pod on the node.
4. Observe the triggered pagecache reclamation and service jitter in existing pods.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.18.8-aliyun.1
WARNING: version difference between client (1.29) and server (1.18) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Alibaba Cloud Linux"
VERSION="3 (Soaring Falcon)"
ID="alinux"
ID_LIKE="rhel fedora centos anolis"
VERSION_ID="3"
PLATFORM_ID="platform:al8"
PRETTY_NAME="Alibaba Cloud Linux 3 (Soaring Falcon)"
ANSI_COLOR="0;31"
HOME_URL="https://www.aliyun.com/"
$ uname -a
Linux xxx 5.10.134-12.2.al8.x86_64 #1 SMP Thu Oct 27 10:07:15 CST 2022 x86_64 x86_64 x86_64 GNU/Linux

```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125715 K8S not running POD only in k8s but working in K3S and Minikube

- Issue é“¾æ¥ï¼š[#125715](https://github.com/kubernetes/kubernetes/issues/125715)

### Issue å†…å®¹

#### What happened?

Been getting this error from many days initialization - cancelling refresh attempt: org.springframework.beans.factory.BeanCreationException: Error creating bean with name â€˜processorMetricsâ€™ defined in class path resource [org/springframework/boot/actuate/autoconfigure/metrics/SystemMetricsAutoConfiguration.class]: Failed to instantiate [io.micrometer.core.instrument.binder.system.ProcessorMetrics]: Factory method â€˜processorMetricsâ€™ threw exception with message: java.lang.reflect.InvocationTargetException
this only happens in k8s and not in k3s and minikube why??

#### What did you expect to happen?

The pod should have ran successfully as it did in minikube and k3s

#### How can we reproduce it (as minimally and precisely as possible)?

Taking a good spring boot application image and running it 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
The connection to the server 10.251.0.188:6443 was refused - did you specify the right host or port?


</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
NAME="Red Hat Enterprise Linux"
VERSION="8.10 (Ootpa)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="8.10"
PLATFORM_ID="platform:el8"
PRETTY_NAME="Red Hat Enterprise Linux 8.10 (Ootpa)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:redhat:enterprise_linux:8::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8"
BUG_REPORT_URL="https://bugzilla.redhat.com/"

REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 8"
REDHAT_BUGZILLA_PRODUCT_VERSION=8.10
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="8.10"

$ uname -a
# paste output here
Linux demo 4.898.0-553.5.1.el8_10.x86_64 #1 SMP Tue May 21 03:13:04 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux


# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125702 CEL stored messageExpressions of CRDs are not validated with the correct CEL environment

- Issue é“¾æ¥ï¼š[#125702](https://github.com/kubernetes/kubernetes/issues/125702)

### Issue å†…å®¹

#### What happened?

Noticed a copy & paste issue when reading through the CEL code.

Basically here: https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiextensions-apiserver/pkg/apis/apiextensions/validation/validation.go#L182

This should be 
> expressions.messageExpressions.Insert(v.MessageExpression)
instead of:
> expressions.messageExpressions.Insert(v.Rule)

The impact of this issue should be that messageExpressions are always evaluated with the CEL environment of the compatibility version. But stored (i.e. pre-existing) messageExpressions should be evaluated with a CEL environment with the "max" version instead.

For users of Kubernetes this means that messageExpressions deployed with one minor version of Kubernetes might not work anymore if they rollback e.g. by one minor version.




#### What did you expect to happen?

messageExpressions should be treated the same way as rules in that regard.



#### How can we reproduce it (as minimally and precisely as possible)?

* Deploy Kubernetes 1.30
* Deploy CRD that uses messageExpression that is using Sets (which became available with Kubernetes 1.30 that is using compatibilty version 1.29)
* Rollback to Kubernetes 1.29
* Update something in the CRD
  * => CRD update will fail because messageExpression is evaluated with the wrong CEL environment (it should still work)



#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

At least on Kubernetes main, but I would assume this isuse is there since CEL was implemented

</details>


#### Cloud provider

-

#### OS version

-

#### Install tools

-

#### Container runtime (CRI) and version (if applicable)

-

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125695 Kubernetes Job failed status after a graceful termination

- Issue é“¾æ¥ï¼š[#125695](https://github.com/kubernetes/kubernetes/issues/125695)

### Issue å†…å®¹

#### What happened?

Let's take the following Kubernetes Cronjob

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: sleep-cronjob
spec:
  schedule: "*/5 * * * *"
  jobTemplate:
    spec:
      backoffLimit: 0 # Avoid two executions
      template:
        spec:
          containers:
          - name: sleep
            image: busybox
            command: ["sh", "-c", "sleep 120"] # 2 minutes
          restartPolicy: Never
          terminationGracePeriodSeconds: 300 # 5 minutes
```

It runs a job every 5 minutes, and the pod sleeps for 2 minutes.

If during the sleep I execute the kubectl delete pod <POD_NAME> for a graceful termination, it will work, it will wait for the specified terminationGracePeriodSeconds and in this case, it will allow the pod to finish its job successfully.

But, the Kubernetes Job will be marked as 1 Failed with the reason BackoffLimitExceeded.

I have tried the Job configuration [activeDeadlineSeconds](https://kubernetes.io/docs/concepts/workloads/controllers/job/#job-termination-and-cleanup) and I'm still getting the error.

Could this be a bug or am I missing some configuration?

#### What did you expect to happen?

The job is mark as completed.

#### How can we reproduce it (as minimally and precisely as possible)?

Using the following, run it, and delete the pod before it finish its operation and then check for associated job status.

```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: sleep-cronjob
spec:
  schedule: "*/5 * * * *"
  jobTemplate:
    spec:
      backoffLimit: 0 # Avoid two executions
      template:
        spec:
          containers:
          - name: sleep
            image: busybox
            command: ["sh", "-c", "sleep 120"] # 2 minutes
          restartPolicy: Never
          terminationGracePeriodSeconds: 300 # 5 minutes
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.29.5
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.5-gke.1091000
```

</details>


#### Cloud provider

<details>
GCP with GKE v1.29.5-gke.1091000
</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #125684 Performance Optimization for UpdateSnapshot when there is only one zone

- Issue é“¾æ¥ï¼š[#125684](https://github.com/kubernetes/kubernetes/issues/125684)

### Issue å†…å®¹

#### What happened?

When there is a node add or delete event, the `UpdateSnapshot` method triggers the updateAll logic, which reallocates memory for the entire nodeInfoList.

#### What did you expect to happen?

When there is only a single zone, we can operate on the nodeInfoList directly to optimize it and avoid reallocating memory. This approach is both CPU and memory-friendly for large clusters where nodes are frequently added or deleted. We don't need additional memory in this scenario, as there is only one zone by default.

#### How can we reproduce it (as minimally and precisely as possible)?

1. add or delete node
2. call `UpdateSnapshot` function

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version

master

```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125680 invalid memory address or nil pointer dereference" in wait.JitterUntil

- Issue é“¾æ¥ï¼š[#125680](https://github.com/kubernetes/kubernetes/issues/125680)

### Issue å†…å®¹

#### What happened?

![event-panic](https://github.com/kubernetes/kubernetes/assets/76192004/3ed97110-e8ad-4b8b-9a64-0ce1cd0ff4fa)


#### What did you expect to happen?

no panic 

#### How can we reproduce it (as minimally and precisely as possible)?

func (e *EventWatcher) OnAdd(obj interface{}) {
	defer recoverPanic("addEvent")

	event := obj.(*corev1.Event)
	e.watchEvent(event)
}

func (e *EventWatcher) OnUpdate(oldObj, newObj interface{}) {
	defer recoverPanic("updateEvent")
	event := newObj.(*corev1.Event)
	e.watchEvent(event)
}

func (e *EventWatcher) OnDelete(obj interface{}) {
	// nil
}

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
k8s version 1.19.5
k8s.io/client-go v0.22.6
```

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125671 TypeMeta is empty in Type client Apply and Patch responses

- Issue é“¾æ¥ï¼š[#125671](https://github.com/kubernetes/kubernetes/issues/125671)

### Issue å†…å®¹

#### What happened?

Called:

```go
applied, err := client.AppsV1().Deployments("default").Apply(context.TODO(), deployment, metav1.ApplyOptions{FieldManager: "test-fieldmanager"})
```
I found:

```go
applied.TypeMeta.Kind == ""
applied.TypeMeta.APIVersion == ""
```



#### What did you expect to happen?

```go
applied.TypeMeta.Kind == "Deployment"
applied.TypeMeta.APIVersion == "apps/v1"
```

#### How can we reproduce it (as minimally and precisely as possible)?

Make the above call.

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.30, but this probably goes back to introduction of patch support in typed clients.

#### Cloud provider

N/A

#### OS version

Linux

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125639 Pod with exitCode 137ï¼Œ The reason has nothing to do with resourcesã€‚

- Issue é“¾æ¥ï¼š[#125639](https://github.com/kubernetes/kubernetes/issues/125639)

### Issue å†…å®¹

#### What happened?

After the pod runs for a period of time, it will be killed. Through auditctl tracking, it is found that the container is killed by runc.

Through monitoring, it is found that all resource utilization is very low, requests and limits are within the range, and there is no OOM situation.

#### What did you expect to happen?

Reason for being killed

#### How can we reproduce it (as minimally and precisely as possible)?

Because it is sporadic, it is difficult to reproduce accurately

#### Anything else we need to know?

The probe used is execProbe

#### Kubernetes version

> kubectl version
Client Version: version.Info{Major:"1", Minor:"19", GitVersion:"v1.19.7", GitCommit:"1dd5338295409edcfff11505e7bb246f0d325d15", GitTreeState:"clean", BuildDate:"2021-01-13T13:23:52Z", GoVersion:"go1.15.5", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"18", GitVersion:"v1.18.20", GitCommit:"1f3e19b7beb1cc0110255668c4238ed63dadb7ad", GitTreeState:"clean", BuildDate:"2021-06-16T12:51:17Z", GoVersion:"go1.13.15", Compiler:"gc", Platform:"linux/amd64"}

> docker info
.....
 Server Version: 19.03.15
 Storage Driver: overlay2
......
 Default Runtime: runc
 Init Binary: docker-init
 containerd version: 9754871865f7fe2f4e74d43e2fc7ccd237edcbce
 runc version: v1.1.6-0-g0f48801a
 init version: fec3683
 ......
 Kernel Version: 5.4.242-1.el7.elrepo.x86_64
 Operating System: CentOS Linux 7 (Core)
 ......

#### Cloud provider

Private cloud, self-built VM

#### OS version

# cat /etc/os-release
NAME="CentOS Linux"
VERSION="7 (Core)"
ID="centos"
# uname -a
......5.4.242-1.el7.elrepo.x86_64 #1 SMP Tue Apr 25 09:46:07 EDT 2023 x86_64 x86_64 x86_64 GNU/Linux



#### Install tools

Nothing


#### Container runtime (CRI) and version (if applicable)

# docker version
Client: Docker Engine - Community
 Version:           19.03.15
 API version:       1.40
 Go version:        go1.13.15
 Git commit:        99e3ed8919
 Built:             Sat Jan 30 03:17:57 2021
 OS/Arch:           linux/amd64
 Experimental:      false

Server: Docker Engine - Community
 Engine:
  Version:          19.03.15
  API version:      1.40 (minimum version 1.12)
  Go version:       go1.13.15
  Git commit:       99e3ed8919
  Built:            Sat Jan 30 03:16:33 2021
  OS/Arch:          linux/amd64
 containerd:
  Version:          1.2.2
  GitCommit:        9754871865f7fe2f4e74d43e2fc7ccd237edcbce
 runc:
  Version:          1.1.6
  GitCommit:        v1.1.6-0-g0f48801a
 docker-init:
  Version:          0.18.0
  GitCommit:        fec3683


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

Nothing

### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125638 The endpoint status does not update when the pod state changes rapidly.

- Issue é“¾æ¥ï¼š[#125638](https://github.com/kubernetes/kubernetes/issues/125638)

### Issue å†…å®¹

#### What happened?


![image](https://github.com/kubernetes/kubernetes/assets/19217340/7686ae10-e598-4d8f-8c8c-b28b16338912)

----
t2
1. Pod update event: ready to notReady
2. syncService
3. currentEndpoints state(from lister: Ready) do not equal to  target state(notReady)
4. clientSet.Endpoints.Update: update etcd to notReady

----
t3
1. Pod update event : notReady to ready(<xxMS)
2. syncService
3. **currentEndpoints state(from lister: Ready) equal to  target state(ready)ï¼Œdo not update Endpoints**

----
----
t4
1. Endpoint update event:  update endpoint lister cache to notReady
----


#### What did you expect to happen?

The status of the endpoint should match the status of the pod.

#### How can we reproduce it (as minimally and precisely as possible)?

Make the Pod status change rapidly. 
Lowering the etcd push interval should make it easier to reproduce.









#### Anything else we need to know?

_No response_

#### Kubernetes version

v1.25.3


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125624 [FG:InPlacePodVerticalScaling] resources in pod status are never updated if EventedPLEG is enabled

- Issue é“¾æ¥ï¼š[#125624](https://github.com/kubernetes/kubernetes/issues/125624)

### Issue å†…å®¹

#### What happened?

If eventedPLEG is enabled, resources in a pod status are never updated after the pod is resized.


#### What did you expect to happen?

Resources in a pod status are updated after the pod is resized.


#### How can we reproduce it (as minimally and precisely as possible)?

1. Enable `EventedPLEG` and `InPlacePodVerticalScaling`.
2. Create a pod and update resources in the pod following the [documentation](https://kubernetes.io/docs/tasks/configure-pod-container/resize-container-resources/#updating-the-pod-s-resources).

```
$ kubectl -n qos-example patch pod qos-demo-5 --patch '{"spec":{"containers":[{"name":"qos-demo-ctr-5", "resources":{"requests":{"cpu":"800m"}, "limits":{"cpu":"800m"}}}]}}'
$ # Wait for more than 10 minutes...
$ kubectl -n qos-example get pod qos-demo-5 -o json | jq '.status.containerStatuses[0].resources'
{
  "limits": {
    "cpu": "700m",
    "memory": "200Mi"
  },
  "requests": {
    "cpu": "700m",
    "memory": "200Mi"
  }
}
```


#### Anything else we need to know?

The root cause of this issue is the same problem mentioned in #124297. Evented PLEG passes a timestamp wrongly to a cache:
https://github.com/kubernetes/kubernetes/blob/5ec31e84d6c525c173906b1497ee6f075c1926e9/pkg/kubelet/pleg/evented.go#L269

EventedPLEG does not emit any event or update a cache because a runtime raises no event at resizing a container without restart. Though GenericPLEG is expected to back up in such a case, this does not work. Generic PLEG relists pods every 5 minutes. It also updates the cache when pod resizing is in progress:
https://github.com/kubernetes/kubernetes/blob/5ec31e84d6c525c173906b1497ee6f075c1926e9/pkg/kubelet/kubelet.go#L2008-L2011

However, an update from GenericPLEG is rejected because the new timestamp looks older than the existing timestamp that EventedPLEG sets at starting the pod because of the bug mentioned above:
https://github.com/kubernetes/kubernetes/blob/5ec31e84d6c525c173906b1497ee6f075c1926e9/pkg/kubelet/container/cache.go#L106-L112


#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: v1.29.6
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
</details>


#### Cloud provider

N/A


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125623 'kubectl delete istag/$ISTAG --dry-run=server' is unexpectedly deleting the object from the server

- Issue é“¾æ¥ï¼š[#125623](https://github.com/kubernetes/kubernetes/issues/125623)

### Issue å†…å®¹

#### What happened?

'kubectl delete istag/$ISTAG --dry-run=server' is unexpectedly deleting the object from the server this is not expected behavior.

- Attempted to delete the `istag` resource object by using the `dry-run=server` option. However, the object(image) was actually deleted from the server.
- Conversely, when applied the same `dry-run=server` parameter to other resources, they remained intact and were not deleted.

#### What did you expect to happen?

After deleting the `istag` using the `--dry-run=server` option with the server strategy, the `istag` object should still remain on the server.


#### How can we reproduce it (as minimally and precisely as possible)?

Try deleting the `istag` resource using the `--dry-run=server` option with the server strategy.

**Steps to Reproduce:**

1. Example with resource ImageStreamTag istag

- Try to delete the `istage` object using the `--dry-run` option with `client`  strategy 
~~~ 
[quickcluster@upi-0 ~]$ kubectl delete -n openshift istag/ubi8-openjdk-8:1.11 --dry-run=client
imagestreamtag.image.openshift.io "ubi8-openjdk-8:1.11" deleted (dry run)
~~~
- Verify whether the deleted object with `client` strategy is actually removed or not 
~~~
[quickcluster@upi-0 ~]$ kubectl get istag -n openshift | grep -i ubi8-openjdk-8:1.11
ubi8-openjdk-8:1.11                                         image-registry.openshift-image-registry.svc:5000/openshift/ubi8-openjdk-8@sha256:022488b1bf697b7dd8c393171a3247bef4ea545a9ab828501e72168f2aac9415                                       4 weeks ago
~~~
- Now, delete the `istage`  object using the `--dry-run` option with `server` strategy 
~~~
[quickcluster@upi-0 ~]$ kubectl delete -n openshift istag/ubi8-openjdk-8:1.11 --dry-run=server
imagestreamtag.image.openshift.io "ubi8-openjdk-8:1.11" deleted (server dry run)
~~~
- Verify whether the deleted object with `server` strategy  is actually removed or not 
~~~
[quickcluster@upi-0 ~]$ kubectl get -n openshift istag | grep -i ubi8-openjdk-8:1.11 
~~~

2. Example with resource secret
- Try to delete the `secret` using the `--dry-run` option with `client`  strategy
~~~ 
[quickcluster@upi-0 ~]$ kubectl delete secrets/deployer-token-jz46t --dry-run=client
secret "deployer-token-jz46t" deleted (dry run)
~~~
- Verify whether the deleted object with `client` strategy is actually removed or not 
~~~
[quickcluster@upi-0 ~]$ kubectl get secret | grep -i deployer-token-jz46t
deployer-token-jz46t       kubernetes.io/service-account-token   4      28d
~~~
- Now, delete the `secret` using the `--dry-run` option with `server` strategy 
~~~
[quickcluster@upi-0 ~]$ kubectl delete secrets/deployer-token-jz46t --dry-run=server
secret "deployer-token-jz46t" deleted (server dry run)
~~~
-  Verify whether the deleted object with `server` strategy  is actually removed or not 
~~~
[quickcluster@upi-0 ~]$ kubectl get secret | grep -i deployer-token-jz46t
deployer-token-jz46t       kubernetes.io/service-account-token   4      28d
~~~ 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.7+c1f5b34
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Red Hat Enterprise Linux Server"
VERSION="7.9 (Maipo)"
ID="rhel"
ID_LIKE="fedora"
VARIANT="Server"
VARIANT_ID="server"
VERSION_ID="7.9"
PRETTY_NAME="Red Hat Enterprise Linux Server 7.9 (Maipo)"
ANSI_COLOR="0;31"
CPE_NAME="cpe:/o:redhat:enterprise_linux:7.9:GA:server"
HOME_URL="https://www.redhat.com/"
BUG_REPORT_URL="https://bugzilla.redhat.com/"
REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 7"
REDHAT_BUGZILLA_PRODUCT_VERSION=7.9
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="7.9"

$ uname -a
Linux upi-0.harshaltest24.lab.psi.pnq2.redhat.com 3.10.0-1160.el7.x86_64 #1 SMP Tue Aug 18 14:50:17 EDT 2020 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>
```
- oc 4.15
- kubectl  v1.28.2
```
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ç»åˆ†æï¼Œæ‚¨æä¾›çš„Issueæè¿°äº†åœ¨ä½¿ç”¨`kubectl delete istag/$ISTAG --dry-run=server`å‘½ä»¤æ—¶ï¼ŒæœŸæœ›åªè¿›è¡ŒæœåŠ¡å™¨ç«¯çš„æ¨¡æ‹Ÿåˆ é™¤æ“ä½œï¼ˆå³ä¸å®é™…åˆ é™¤èµ„æºï¼‰ï¼Œä½†å®é™…ä¸Š`istag`èµ„æºè¢«çœŸæ­£åˆ é™¤äº†ã€‚è¿™ç§è¡Œä¸ºä¸é¢„æœŸä¸ç¬¦ï¼Œå±äºåŠŸèƒ½æ€§é”™è¯¯ã€‚

æ ¹æ®æ‚¨æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œå…·ä½“åˆ†æå¦‚ä¸‹ï¼š

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šè¦æ‰§è¡Œ`kubectl delete`å‘½ä»¤ï¼Œæ— è®ºæ˜¯å¦ä½¿ç”¨`--dry-run`å‚æ•°ï¼Œç”¨æˆ·éƒ½éœ€è¦å¯¹ç›®æ ‡èµ„æºå…·æœ‰åˆ é™¤æƒé™ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œåªæœ‰å…·å¤‡åˆ é™¤æƒé™çš„ç”¨æˆ·æ‰èƒ½æ‰§è¡Œè¯¥å‘½ä»¤ã€‚æ”»å‡»è€…å¦‚æœæ²¡æœ‰é€‚å½“çš„æƒé™ï¼Œæ— æ³•åˆ©ç”¨æ­¤é—®é¢˜åˆ é™¤èµ„æºã€‚å› æ­¤ï¼Œè¯¥é—®é¢˜ä¸èƒ½è¢«æœªæˆæƒçš„æ”»å‡»è€…åˆ©ç”¨ã€‚

2. **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä½¿ç”¨CVSS 3.1è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœè¦åœ¨highä»¥ä¸Š**ï¼šç”±äºè¯¥é—®é¢˜ä¸æ¶‰åŠæƒé™æå‡ã€æœªæˆæƒè®¿é—®æˆ–å…¶ä»–å®‰å…¨æ¼æ´ï¼Œä¸”ä¸æ»¡è¶³CVSSé«˜å±æ¼æ´çš„è¯„åˆ†æ ‡å‡†ï¼Œæ‰€ä»¥ä¸å¤ªå¯èƒ½è¢«åˆ†é…CVEç¼–å·ã€‚

3. **Issueæäº¤è€…åœ¨æäº¤å†…å®¹ä¸­æš´éœ²çš„æ•æ„Ÿä¿¡æ¯ã€ä¸å½“æ“ä½œã€ä¸å½“é…ç½®ç­‰é—®é¢˜ï¼Œä¸å±äºå®‰å…¨é£é™©ï¼Œå› ä¸ºå®ƒæ˜¯Issueæäº¤è€…çš„é—®é¢˜ï¼Œè€Œä¸æ˜¯é¡¹ç›®çš„é—®é¢˜**ï¼šè¯¥é—®é¢˜æ˜¯ç”±äºå‘½ä»¤çš„æ„å¤–è¡Œä¸ºå¼•èµ·çš„åŠŸèƒ½æ€§é”™è¯¯ï¼Œä¸æ¶‰åŠæäº¤è€…çš„ä¸å½“æ“ä½œæˆ–é…ç½®ã€‚

4. **åœ¨é£é™©ä¸ºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»æ—¶ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å¤Ÿå®æ–½è¯¥æ”»å‡»ï¼Œåˆ™è§†æƒ…å†µéœ€è¦é™çº§å¤„ç†**ï¼šæœ¬é—®é¢˜ä¸­ï¼Œåˆ é™¤èµ„æºçš„æ“ä½œéœ€è¦å…·å¤‡åˆ é™¤æƒé™ï¼Œå› æ­¤å³ä½¿å­˜åœ¨èµ„æºè¢«æ„å¤–åˆ é™¤çš„æƒ…å†µï¼Œä¹Ÿä¸å±äºé«˜é£é™©çš„DoSæ”»å‡»ã€‚

5. **å¯¹äºæ—¥å¿—ä¸­æ³„éœ²å‡­æ®çš„é£é™©**ï¼šæœ¬é—®é¢˜ä¸å‡­æ®æ³„éœ²æ— å…³ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueåæ˜ çš„æ˜¯åœ¨ç‰¹å®šå‘½ä»¤ä¸‹åŠŸèƒ½æœªæŒ‰é¢„æœŸæ‰§è¡Œçš„é—®é¢˜ï¼Œå¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #125618 Node Lifecycle Controller does not mark pods not ready when node becomes Ready=False

- Issue é“¾æ¥ï¼š[#125618](https://github.com/kubernetes/kubernetes/issues/125618)

### Issue å†…å®¹

#### What happened?

The node lifecycle controller is responsible for marking the ready condition on pods with Ready=False when the node becomes unhealthy. See https://github.com/kubernetes/kubernetes/blob/6d0ac8c561a7ac66c21e4ee7bd1976c2ecedbf32/pkg/controller/nodelifecycle/node_lifecycle_controller.go#L757

It's not document clearly today, but the behavior appears to be that the only time when the node lifecycle controller will mark pods not ready is if the **kubelet failed to renewed it's lease for more than `nodeMonitorGracePeriod`**.  The node lifecycle controller will **not** mark pods as Ready=False, if the kubelet continues to updates it's lease but reports Ready=False.

This is because for `MarkPodsNotReady` to be called [this expression](https://github.com/kubernetes/kubernetes/blob/6d0ac8c561a7ac66c21e4ee7bd1976c2ecedbf32/pkg/controller/nodelifecycle/node_lifecycle_controller.go#L752) must be evaluated where `currentReadyCondition != observedReadyCondition`. The only situation where this will be true is if the lease was [not renewed](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/nodelifecycle/node_lifecycle_controller.go#L936-L987). 

This has also been raised in this discussion thread -- https://github.com/kubernetes/kubernetes/pull/112928#discussion_r992874302

#### What did you expect to happen?

I think it's worth to consider if this behavior should be changed -- if nodes report `Ready=False`, should the node-lifecycle-controller mark the pods as not ready?

One of the motivations of the change is for the Graceful Node Shutdown feature. As was discussed in https://github.com/kubernetes/kubernetes/issues/116965, in some cases, Graceful Shutdown is not able to fully complete and shutdown all the pods on the node (for example on a short emergency shutdown or if kubelet was unable to make the status update). Graceful node shutdown does set the node ready condition as false prior to the shutdown. If the node lifecycle controller would mark pods as not ready in this case, it would help https://github.com/kubernetes/kubernetes/issues/116965 and remove pods from endpoints (by marking them as non ready).

#### How can we reproduce it (as minimally and precisely as possible)?

n/a

#### Anything else we need to know?

_No response_

#### Kubernetes version

n/a

#### Cloud provider

n/a

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125614 kube-apiserver logs watch requests before they end in 1.30

- Issue é“¾æ¥ï¼š[#125614](https://github.com/kubernetes/kubernetes/issues/125614)

### Issue å†…å®¹

#### What happened?


In 1.30, WATCH request is logged immediately when a watch is opened, with very low latency. There appears to be no trace of when the watch ends. This significantly impacts debugging capability for issues with establishing watches. 

Example for consecutive watches on pods opened by kube-scheduler - each appears to be lasting 8-9 minutes, everything works in the cluster, but the latency doesn't match watch length, the request is logged immediately, and with `resp=0`:

```
I0620 12:33:17.389016      11 httplog.go:132] "HTTP" verb="WATCH" URI="/api/v1/pods?allowWatchBookmarks=true&fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&resourceVersion=97186&timeout=9m12s&timeoutSeconds=552&watch=true" latency="470.891Âµs" userAgent="kube-scheduler/v1.30.2 (linux/amd64) kubernetes/85fad5b/scheduler" audit-ID="2cf3b6d8-8c04-4065-8bb8-ccd209f24063" srcIP="[::1]:60566" apf_pl="workload-high" apf_fs="kube-scheduler" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_init_latency="252.569Âµs" apf_execution_time="253.277Âµs" resp=0
...
I0620 12:42:29.390797      11 get.go:261] "Starting watch" path="/api/v1/pods" resourceVersion="135790" labels="" fields="status.phase!=Failed,status.phase!=Succeeded" timeout="6m10s"
I0620 12:42:29.391006      11 httplog.go:132] "HTTP" verb="WATCH" URI="/api/v1/pods?allowWatchBookmarks=true&fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&resourceVersion=135790&timeout=6m10s&timeoutSeconds=370&watch=true" latency="629.842Âµs" userAgent="kube-scheduler/v1.30.2 (linux/amd64) kubernetes/85fad5b/scheduler" audit-ID="619843d2-3f2a-4c2e-960a-17eecc71695b" srcIP="[::1]:60566" apf_pl="workload-high" apf_fs="kube-scheduler" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_init_latency="299.938Âµs" apf_execution_time="300.55Âµs" resp=0
```

#### What did you expect to happen?

Watches are usually long (~minutes). In 1.29 and earlier log looks like this:

```
I0620 12:30:26.940451      11 get.go:260] "Starting watch" path="/api/v1/pods" resourceVersion="93431" labels="" fields="status.phase!=Failed,status.phase!=Succeeded" timeout="8m18s"
...
I0620 12:38:44.940817      11 httplog.go:132] "HTTP" verb="WATCH" URI="/api/v1/pods?allowWatchBookmarks=true&fieldSelector=status.phase%21%3DSucceeded%2Cstatus.phase%21%3DFailed&resourceVersion=93431&timeout=8m18s&timeoutSeconds=498&watch=true" latency="8m18.00060226s" userAgent="kube-scheduler/v1.29.6 (linux/amd64) kubernetes/43a444e/scheduler" audit-ID="c39f0303-10ad-4b51-baf4-87c592fcb85e" srcIP="[::1]:39438" apf_pl="workload-high" apf_fs="kube-scheduler" apf_iseats=1 apf_fseats=0 apf_additionalLatency="0s" apf_init_latency="280.462Âµs" apf_execution_time="281.612Âµs" resp=200
```

#### How can we reproduce it (as minimally and precisely as possible)?

Compare kube-apiserver log in 1.29 and 1.30.

#### Anything else we need to know?

Log line examples are taken from passing runs of gce-scalabiltiy-100 tests: 
- https://testgrid.k8s.io/sig-scalability-gce#gce-cos-1.29-scalability-100 : https://prow.k8s.io/view/gs/kubernetes-jenkins/logs/ci-kubernetes-e2e-gci-gce-scalability-1-29/1803759918289784832
- https://testgrid.k8s.io/sig-scalability-gce#gce-cos-1.30-scalability-100 : https://prow.k8s.io/view/gs/kubernetes-jenkins/logs/ci-kubernetes-e2e-gci-gce-scalability-1-30/1803759918403031040

#### Kubernetes version

Observed in 1.30.2 and 1.30.1. To be verified if present in 1.30.0..

#### Cloud provider

N/A


#### OS version

N/A

#### Install tools

N/A


#### Container runtime (CRI) and version (if applicable)

N/A

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

N/A

### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125611 NetPol block self pod trafic using an svc and not direct call

- Issue é“¾æ¥ï¼š[#125611](https://github.com/kubernetes/kubernetes/issues/125611)

### Issue å†…å®¹

#### What happened?

Hello,

In case of a pod exposed by an internal service (ClusterIP) and a presence of a network-policy allowing ingress traffic from some namespaces and th pod namespace it self (my-namespace) like :

```yaml
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: my-namespace
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: nginx
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: prometheus
  podSelector: {}
  policyTypes:
  - Ingress
```


When the pod call it self using the service IP or service name (DNS), got timeout when the NetPol is presente.



#### What did you expect to happen?

From the pod, i can : 

1. Pod can call it self by his IP
2. Pod can call it self by the service IP
3. Pod can call it self by the service FQDN (DNS)

#### How can we reproduce it (as minimally and precisely as possible)?


Deploy pod with svc and netpol with ingress filtering. Run from inside the pod, a curl call to the service like :  
`wget http://hello-world.my-namespace.svc.cluster.local`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# v1.25.7
```

</details>


#### Cloud provider

<details>
On promise
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
CNS : Antre v1.9.0
</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125610 cronjob schedule with multiple conditions not working - conflict between day (week) and day (month)

- Issue é“¾æ¥ï¼š[#125610](https://github.com/kubernetes/kubernetes/issues/125610)

### Issue å†…å®¹

#### What happened?

Hello,
I have a kubernetes cronjob (apiVersion: batch/v1 type: CronJob) that I would like to run only on Tuesdays. Then I'd like to run it this first week of the month and the third week of the month. Here is my definition of cronjob: `0 18 1-7,15-21 * TUE`
The problem is that the day (week) condition does not work and my cron runs every day between 1 - 7 and 15 - 21 days (month).

#### What did you expect to happen?

I would like the two conditions to work together: AND on Tuesday AND between the 1st and 7th of the month and the 15th and 21st of the month.

#### How can we reproduce it (as minimally and precisely as possible)?

It is possible to reproduce it on each cronjob, here is the model from the Kubernetes documentation
```
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: "0 18 1-7,15-21 * TUE"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox:1.28
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - date; echo Hello from the Kubernetes cluster
          restartPolicy: OnFailure
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.1+rke2r1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125609 [FG:InPlacePodVerticalScaling] e2e test does not verify resource update in pod status

- Issue é“¾æ¥ï¼š[#125609](https://github.com/kubernetes/kubernetes/issues/125609)

### Issue å†…å®¹

#### What happened?

e2e tests for pod resizing does not verify if resources in a pod status is updated to the same values in a pod spec after resizing is actuated. This is caused because the result of the runtime support check is reversed:

https://github.com/kubernetes/kubernetes/blob/1519f802816f6a6b9bd4cfb259c93644fe950e0e/test/e2e/node/pod_resize.go#L501-L513


#### What did you expect to happen?

Resources in pod status are verified.


#### How can we reproduce it (as minimally and precisely as possible)?

Run e2e test with enabling InPlacePodVerticalScaling.


#### Anything else we need to know?

I'm not sure if we should fix it immediately. If this issue is fixed, it will take much longer to finish the tests because there are some issues that it takes much time (60-90s) till resources in a pod status is updated after resizing is actuated such as #123940 and #125559. In addition, pod resize e2e tests are being migrated to e2e node tests in PR #124296.


#### Kubernetes version

master branch

#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125608 Node Labeling node.kubernetes.io/out-of-service Taint Label Delay

- Issue é“¾æ¥ï¼š[#125608](https://github.com/kubernetes/kubernetes/issues/125608)

### Issue å†…å®¹

#### What happened?

After the node is powered off, the node.kubernetes.io/out-of-service label is not added in time. Therefore, the pod created by the statefulset is not deleted in time.
 ```
 taints:
  - effect: NoSchedule
    key: node.kubernetes.io/out-of-service
    value: nodeshutdown
  - effect: NoExecute
    key: node.kubernetes.io/out-of-service
    value: nodeshutdown
  - effect: NoSchedule
    key: node.kubernetes.io/unreachable
    timeAdded: "2024-06-20T08:56:43Z"
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    timeAdded: "2024-06-20T08:56:55Z"
```
A node is powered off. At least one pod created by statefulset exists on the node. The pod tolerance is set to 30s. After the node is powered off, the node status changes to notready. The pod is in the terminating state 30s later but is deleted after a long time. The reason is that the node.kubernetes.io/out-of-service taint label is updated late.
So I want to know when the node.kubernetes.io/out-of-service taint tag was added.

#### What did you expect to happen?

The pod should be deleted in a timely manner.

#### How can we reproduce it (as minimally and precisely as possible)?

Powering Off a Node

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.28
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125607 kubectl --server-side --dry-run=server - wrong output for converting client side applied manifest

- Issue é“¾æ¥ï¼š[#125607](https://github.com/kubernetes/kubernetes/issues/125607)

### Issue å†…å®¹

#### What happened?

When converting a client-side-applied manifest to a server side applied manifest `--dry-run=server` doesn't show the correct output.
It still shows client-side-applied fields, which will be removed, when running without `--dry-run=server`.

#### What did you expect to happen?

running `--server-side` with `--dry-run=server` will generate the same manifest as running without `--dry-run=server`

#### How can we reproduce it (as minimally and precisely as possible)?

1. Apply an example configmap with 3 keys with client-side-apply:

`kubectl apply -o yaml --show-managed-fields -f https://raw.githubusercontent.com/SebastianJ91/argocd-server-side-apply/main/3-configmap/configmap-original.yaml`

Output:

<details>

<pre>
apiVersion: v1
data:
  key1: key-original
  key2: key-original
  key3: key-original
kind: ConfigMap
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","data":{"key1":"key-original","key2":"key-original","key3":"key-original"},"kind":"ConfigMap","metadata":{"annotations":{},"name":"configmap","namespace":"default"}}
  creationTimestamp: "2024-06-20T10:55:40Z"
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:data:
        .: {}
        f:key1: {}
        f:key2: {}
        f:key3: {}
      f:metadata:
        f:annotations:
          .: {}
          f:kubectl.kubernetes.io/last-applied-configuration: {}
    manager: kubectl-client-side-apply
    operation: Update
    time: "2024-06-20T10:55:40Z"
  name: configmap
  namespace: default
  resourceVersion: "27130"
  uid: ca98d345-18e8-4fb4-a9a7-526d79756244
</pre>

</details>

2. Do a dry run for a server-side-apply for a diff manifest changing a single key of the configmap:
`kubectl apply -o yaml --server-side --dry-run=server --show-managed-fields --force-conflicts --field-manager=manager -f https://raw.githubusercontent.com/SebastianJ91/argocd-server-side-apply/main/3-configmap/configmap-patch.yaml`

Look at the resulting manifest, which is merged by both the manifests:
<details>

<pre>
apiVersion: v1
data:
  key1: key-original
  key2: key-patch
  key3: key-original
kind: ConfigMap
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","data":{"key1":"key-original","key2":"key-original","key3":"key-original"},"kind":"ConfigMap","metadata":{"annotations":{},"name":"configmap","namespace":"default"}}
  creationTimestamp: "2024-06-20T10:55:40Z"
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:data:
        f:key2: {}
    manager: manager
    operation: Apply
    time: "2024-06-20T10:59:45Z"
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:data:
        .: {}
        f:key1: {}
        f:key3: {}
      f:metadata:
        f:annotations:
          .: {}
          f:kubectl.kubernetes.io/last-applied-configuration: {}
    manager: kubectl-client-side-apply
    operation: Update
    time: "2024-06-20T10:55:40Z"
  name: configmap
  namespace: default
  resourceVersion: "27130"
  uid: ca98d345-18e8-4fb4-a9a7-526d79756244

</pre>

</details>

3. Run the server-side-apply:
`kubectl apply -o yaml --server-side --show-managed-fields --force-conflicts --field-manager=manager -f https://raw.githubusercontent.com/SebastianJ91/argocd-server-side-apply/main/3-configmap/configmap-patch.yaml`

Look at the applied manifest, where the fields from client-side-apply are removed:
<details>

<pre>
apiVersion: v1
data:
  key2: key-patch
kind: ConfigMap
metadata:
  creationTimestamp: "2024-06-20T10:55:40Z"
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:data:
        f:key2: {}
    manager: manager
    operation: Apply
    time: "2024-06-20T11:03:56Z"
  name: configmap
  namespace: default
  resourceVersion: "28025"
  uid: ca98d345-18e8-4fb4-a9a7-526d79756244
</pre>

</details>


#### Anything else we need to know?

Regarding #125403 kubectl is removing existing client-side-fields which are not part of a manifest for the same resource applied by server-side-apply.
This behavior was introduced since #112905 is fixed.

This should also be reflected, if -dry-run=server is used 

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.2
```

</details>


#### Cloud provider

<details>
local Podman Desktop v1.10.3 - kind-cluster
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
BuildNumber  Caption                   OSArchitecture  Version
22631        Microsoft Windows 11 Pro  64-bit          10.0.22631
```

</details>


#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125604 Can't get secrets when adding imagePullSecrets 

- Issue é“¾æ¥ï¼š[#125604](https://github.com/kubernetes/kubernetes/issues/125604)

### Issue å†…å®¹

#### What happened?

Image can't be pulled after edit container image and imagePullSecrets, and I get this error in kubelets log:

```
 User "system:node:xxxx cannot get resource "secrets" in API group "" in the namespace "yyyy": no relationship found between node "xxxx" and this objectã€‚
```

#### What did you expect to happen?

The pod should be ready with the new image.

#### How can we reproduce it (as minimally and precisely as possible)?

1. create a normal pod and it get ready on node **without any workload**.
2. edit the image with a **different registry** and **add** the correct imagePullSecret has been created under the namespace

#### Anything else we need to know?

From the code

https://github.com/kubernetes/kubernetes/blob/1519f802816f6a6b9bd4cfb259c93644fe950e0e/plugin/pkg/auth/authorizer/node/graph_populator.go#L95-L102

I think the graph for this pod in the node authorizer will only populated once when it is scheduled to a node, so when I add a new imagePullSecrets to the pod, the secret will not be added to the graph.


#### Kubernetes version

<details>

```console
$ kubectl version
v1.18
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #125602 The startup time of the init container is later than that of the application container.

- Issue é“¾æ¥ï¼š[#125602](https://github.com/kubernetes/kubernetes/issues/125602)

### Issue å†…å®¹

#### What happened?

When the argo task is executed, The startup time of the init container is later than that of the application container.

In the output of the kubectl describe pod xxx command, the value of `startedAt` for the init container is later than that for the application container.

```yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubectl.kubernetes.io/default-container: main
    kubernetes.io/psp: psp-global
    workflows.argoproj.io/node-id: vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99-333335174
    workflows.argoproj.io/node-name: vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99.coverage(1:cov-type:coverage_report,version:0.2.5)
  creationTimestamp: "2024-06-19T19:44:39Z"
  labels:
    fuzz-worker: "true"
    workflows.argoproj.io/completed: "true"
    workflows.argoproj.io/workflow: vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99
  name: vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99-333335174
  namespace: argo
  ownerReferences:
  - apiVersion: argoproj.io/v1alpha1
    blockOwnerDeletion: true
    controller: true
    kind: Workflow
    name: vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99
    uid: 2e182e9c-f249-44cf-911a-b3c10310e451
  resourceVersion: "544998173"
  selfLink: /api/v1/namespaces/argo/pods/vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99-333335174
  uid: 9b7ba2d3-65f7-4e3f-8621-538d1cfb256f
spec:
  activeDeadlineSeconds: 259200
  containers:
  - command:
    - argoexec
    - wait
    - --loglevel
    - info
    env:
    - name: ARGO_POD_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.name
    - name: ARGO_POD_UID
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.uid
    - name: ARGO_CONTAINER_RUNTIME_EXECUTOR
      value: emissary
    - name: GODEBUG
      value: x509ignoreCN=0
    - name: ARGO_WORKFLOW_NAME
      value: vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99
    - name: ARGO_CONTAINER_NAME
      value: wait
    - name: ARGO_TEMPLATE
      value: '....'
    - name: ARGO_NODE_ID
      value: vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99-333335174
    - name: ARGO_INCLUDE_SCRIPT_OUTPUT
      value: "false"
    - name: ARGO_DEADLINE
      value: "0001-01-01T00:00:00Z"
    - name: ARGO_PROGRESS_FILE
      value: /var/run/argo/progress
    - name: ARGO_PROGRESS_PATCH_TICK_DURATION
      value: 1m0s
    - name: ARGO_PROGRESS_FILE_TICK_DURATION
      value: 3s
    image: ../argoexec:v3.3.8
    imagePullPolicy: IfNotPresent
    name: wait
    resources:
      requests:
        cpu: 10m
        memory: 64Mi
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /argo/secret/my-minio-cred
      name: my-minio-cred
      readOnly: true
    - mountPath: /var/run/argo
      name: var-run-argo
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-6htk6
      readOnly: true
  - args:
    - 'python3 entry_point.py; sh run.sh '
    command:
    - /var/run/argo/argoexec
    - emissary
    - --
    - sh
    - -c
    env:
    - name: TENANT_ID
      value: "1009"
    - name: DEBUG_COV
      value: "false"
    - name: GIT_SSL_NO_VERIFY
      value: "1"
    - name: GIT_BRANCH
      value: master
    - name: FUZZ_CONFIG
      value: '...'
    - name: COVERAGE_ARGS
      value: -C 10000000 -T 3600 -r reportPath -c corpusPath
    - name: PROJECT_NAME
      value: VOS_CortexM
    - name: JOB_NAME
      value: VOS_CortexM
    - name: S3_DIR
      value: SECTRACY/261035810/VOS_CortexM/vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99/
    - name: S3_REPORT_DIR
      value: SECTRACY-REPORT/261035810/VOS_CortexM/0.2.5/vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99/
    - name: VERSION
      value: 0.2.5
    - name: COV_TYPE
      value: coverage_report
    - name: FUZZ_DB_HOST
      value: 7.191.2.211
    - name: FUZZ_DB
      value: fuzz-admin
    - name: FUZZ_DB_PASS
      value: VGRhdGFfb3NzQDIwMjI=
    - name: WORKFLOW_ID
      value: 3763805e-581a-4070-96e9-8c8ff7b74058
    - name: BRANCH
      value: master
    - name: COVERAGE_URL
      value: ..
    - name: FUZZTYPE
      value: DTFUZZ
    - name: SERVICE_AREA
      value: yellow
    - name: SERVICE_ENVIR
      value: pro
    - name: PROJECT_PBI
      value: "261035810"
    - name: ARGO_CONTAINER_NAME
      value: main
    - name: ARGO_TEMPLATE
      value: '...'
    - name: ARGO_NODE_ID
      value: vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99-333335174
    - name: ARGO_INCLUDE_SCRIPT_OUTPUT
      value: "false"
    - name: ARGO_DEADLINE
      value: "0001-01-01T00:00:00Z"
    - name: ARGO_PROGRESS_FILE
      value: /var/run/argo/progress
    - name: ARGO_PROGRESS_PATCH_TICK_DURATION
      value: 1m0s
    - name: ARGO_PROGRESS_FILE_TICK_DURATION
      value: 3s
    image: ..
    imagePullPolicy: IfNotPresent
    name: main
    resources: {}
    securityContext:
      privileged: true
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/argo
      name: var-run-argo
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-6htk6
      readOnly: true
    workingDir: /Sectracy
  dnsConfig:
    options:
    - name: single-request-reopen
      value: ""
    - name: timeout
      value: "2"
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  imagePullSecrets:
  - name: regcred
  - name: default-secret
  initContainers:
  - command:
    - argoexec
    - init
    - --loglevel
    - info
    env:
    - name: ARGO_POD_NAME
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.name
    - name: ARGO_POD_UID
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.uid
    - name: ARGO_CONTAINER_RUNTIME_EXECUTOR
      value: emissary
    - name: GODEBUG
      value: x509ignoreCN=0
    - name: ARGO_WORKFLOW_NAME
      value: vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99
    - name: ARGO_CONTAINER_NAME
      value: init
    - name: ARGO_TEMPLATE
      value: '...'
    - name: ARGO_NODE_ID
      value: vos-ee-erika3-dc5d779f1c7c42beba46e7b7909fbb99-333335174
    - name: ARGO_INCLUDE_SCRIPT_OUTPUT
      value: "false"
    - name: ARGO_DEADLINE
      value: "0001-01-01T00:00:00Z"
    - name: ARGO_PROGRESS_FILE
      value: /var/run/argo/progress
    - name: ARGO_PROGRESS_PATCH_TICK_DURATION
      value: 1m0s
    - name: ARGO_PROGRESS_FILE_TICK_DURATION
      value: 3s
    image: ../argoexec:v3.3.8
    imagePullPolicy: IfNotPresent
    name: init
    resources:
      requests:
        cpu: 10m
        memory: 64Mi
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /argo/secret/my-minio-cred
      name: my-minio-cred
      readOnly: true
    - mountPath: /var/run/argo
      name: var-run-argo
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-6htk6
      readOnly: true
  nodeName: 7.218.69.27
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Never
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - emptyDir: {}
    name: var-run-argo
  - name: my-minio-cred
    secret:
      defaultMode: 420
      items:
      - key: accesskey
        path: accesskey
      - key: secretkey
        path: secretkey
      secretName: my-minio-cred
  - name: default-token-6htk6
    secret:
      defaultMode: 420
      secretName: default-token-6htk6
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2024-06-19T21:43:38Z"
    message: 'containers with incomplete status: [init]'
    reason: ContainersNotInitialized
    status: "False"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2024-06-19T21:43:39Z"
    message: 'containers with unready status: [wait main]'
    reason: ContainersNotReady
    status: "False"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2024-06-19T21:43:39Z"
    message: 'containers with unready status: [wait main]'
    reason: ContainersNotReady
    status: "False"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2024-06-19T19:44:39Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: docker://9208031ef8307e4d1561020ae84cfb867ea435bac4fffde09aa2efd33c05531c
    image: ..
    imageID: ..
    lastState: {}
    name: main
    ready: false
    restartCount: 0
    started: false
    state:
      terminated:
        containerID: docker://9208031ef8307e4d1561020ae84cfb867ea435bac4fffde09aa2efd33c05531c
        exitCode: 137
        finishedAt: "2024-06-19T21:43:38Z"
        reason: Error
        startedAt: "2024-06-19T21:43:36Z"
  - containerID: docker://5d1d650c0225e09a0077768cd0bba2b24458de5495e88e4db5793aa488fbd20e
    image: ../argoexec:v3.3.8
    imageID: ..
    lastState: {}
    name: wait
    ready: false
    restartCount: 0
    started: false
    state:
      terminated:
        containerID: docker://5d1d650c0225e09a0077768cd0bba2b24458de5495e88e4db5793aa488fbd20e
        exitCode: 0
        finishedAt: "2024-06-19T21:43:38Z"
        message: Step terminated
        reason: Completed
        startedAt: "2024-06-19T19:44:41Z"
  hostIP: 7.218.69.27
  initContainerStatuses:
  - containerID: docker://d9ab7d5ec93b06c04f2970ce2b30f672298069ed3098176683a654f487f28072
    image: ../argoexec:v3.3.8
    imageID: ..
    lastState: {}
    name: init
    ready: false
    restartCount: 0
    state:
      terminated:
        containerID: docker://d9ab7d5ec93b06c04f2970ce2b30f672298069ed3098176683a654f487f28072
        exitCode: 1
        finishedAt: "2024-06-19T21:43:37Z"
        message: 'open /var/run/argo/argoexec: text file busy'
        reason: Error
        startedAt: "2024-06-19T21:43:37Z"
  phase: Failed
  podIP: 12.11.1.191
  podIPs:
  - ip: 12.11.1.191
  qosClass: Burstable
  startTime: "2024-06-19T19:44:39Z"

```

#### What did you expect to happen?

The init container starts to run the application container after the running of the init container is complete. However, the application container runs first. The init container runs during the running of the application container.

When the two containers run at the same time, the argoexec file is operated. As a result, the open` /var/run/argo/argoexec: text file busy` exception occurs.

#### How can we reproduce it (as minimally and precisely as possible)?

I don't know how to reproduce it, Same here. Sometimes it shows up, sometimes it doesn't.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: version.Info{Major:"1", Minor:"23", GitVersion:"v1.23.0", GitCommit:"ab69524f795c42094a6630298ff53f3c3ebab7f4", GitTreeState:"clean", BuildDate:"2021-12-07T18:16:20Z", GoVersion:"go1.17.3", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"19+", GitVersion:"v1.19.10-r1.0.0-source-124-g05dc99ca321c86", GitCommit:"05dc99ca321c86aa539928891fcd3622eba7e965", GitTreeState:"clean", BuildDate:"2021-09-16T17:07:56Z", GoVersion:"go1.15.15", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
EKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
NAME="Ubuntu"
VERSION="20.04.1 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.1 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=foca

$ uname -a
# paste output here
Linux pekphis94681 5.4.0-162-generic #179-Ubuntu SMP Mon Aug 14 08:51:31 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Docker 

Client:
 Version:           20.10.25
 API version:       1.41
 Go version:        go1.20.3
 Git commit:        20.10.25-0ubuntu1~20.04.2
 Built:             Thu Aug 10 20:14:50 2023
 OS/Arch:           linux/amd64
 Context:           default
 Experimental:      true

Server:
 Engine:
  Version:          20.10.25
  API version:      1.41 (minimum version 1.12)
  Go version:       go1.20.3
  Git commit:       20.10.25-0ubuntu1~20.04.2
  Built:            Thu Aug  3 18:03:37 2023
  OS/Arch:          linux/amd64
  Experimental:     false
 containerd:
  Version:          1.7.2
  GitCommit:        
 runc:
  Version:          1.1.7-0ubuntu1~20.04.2
  GitCommit:        
 docker-init:
  Version:          0.19.0
  GitCommit:
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #125579 Node reboot leaving existing pod using resources stuck with error UnexpectedAdmissionError 

- Issue é“¾æ¥ï¼š[#125579](https://github.com/kubernetes/kubernetes/issues/125579)

### Issue å†…å®¹

#### What happened?

When a node is rebooted, pod using resources allocated by device plugin will encounter UnexpectedAdmissionError error as below:

```
  Warning  UnexpectedAdmissionError  84s                kubelet            Allocate failed due to no healthy devices present; cannot allocate unhealthy devices xxx, which is unexpected
```

What makes it really bad is if it's a raw pod, it stucks in such state and never recover.

#### What did you expect to happen?

The pod should be retried until device plugin is ready

#### How can we reproduce it (as minimally and precisely as possible)?

1. create a pod requesting any resources allocated via a device plugin
2. reboot the node
3. observe pod failure


#### Anything else we need to know?

The behavior is introduced with #116376

And there are various issues opened around kubelet restart #118559 #124345

But this issue is about node restart. When node is restarted, kubelet started to rerun existing pods in random order. So a pod can run into this issue before device plugin pod is healthy on the node

#### Kubernetes version

<details>

```console
kubectl version
Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.1
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #125568 When Deployment is editing replicas and strategy simultaneously, it may get stuck and not continue to execute

- Issue é“¾æ¥ï¼š[#125568](https://github.com/kubernetes/kubernetes/issues/125568)

### Issue å†…å®¹

#### What happened?

When Deployment is editing replicas and strategy simultaneously, it may get stuck and not continue to execute

#### What did you expect to happen?

deployment continue upgrade or scaled

#### How can we reproduce it (as minimally and precisely as possible)?

1. created a deployment with a rolling upgrade and set a large number of replicas, such as 100. 
2. After creating 100 pods, use a slow rolling upgrade like RollingUpdateStrategy:  1 max unavailable, 1 max surge
 to make two active replicasets appear at the same time. 
3. While in the process of rolling upgrade (2 replicaSet), simultaneously edit the upgrade strategy of the deployment to recreate, and modify the number of replicas to 101. (kubectl apply -f xxx.yaml)
4. At this time, the deployment will be in a stuck state, and it will not be upgraded or scaled no matter how it is modified.

#### Anything else we need to know?

Deployment will get stuck because of these few lines of code.

https://github.com/kubernetes/kubernetes/blob/64355780d9d945a1135a004829ab9dcc38c3911d/pkg/controller/deployment/deployment_controller.go#L665C2-L671C3

#### Kubernetes version

<details>
v1.23.3
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125561 RuntimeHandlerResolver: interface invalid nil checking

- Issue é“¾æ¥ï¼š[#125561](https://github.com/kubernetes/kubernetes/issues/125561)

### Issue å†…å®¹

#### What happened?

the nil checking for interface RuntimeHandlerResolver in #L109 will never check whether the underlying struct is nil. 
A reflect nil checking is needed to avoid panic in running 'rcManager.LookupRuntimeHandler'(#L111)
https://github.com/kubernetes/kubernetes/blob/a3a49887ee73fa1108adac97a797dec02ccb00d4/pkg/kubelet/kuberuntime/util/util.go#L107-L115

#### What did you expect to happen?

Add reflect nil checking for rcManager


#### How can we reproduce it (as minimally and precisely as possible)?

1. kubelet enable standaloneMode
2. pod.Spec.SecurityContext not nil

#### Anything else we need to know?

_No response_

#### Kubernetes version

master

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #125559 [FG:InPlacePodVerticalScaling] Pod Resize - resize stuck "InProgress" when only resizing memory requests

- Issue é“¾æ¥ï¼š[#125559](https://github.com/kubernetes/kubernetes/issues/125559)

### Issue å†…å®¹

#### What happened?

InPlacePodVerticalScaling causes pods to get stuck in resizing "InProgress" when resizing solely memory requests for a QoS Burstable pod. The feature works for memory limits etc.

Example is variation from [here](https://kubernetes.io/docs/tasks/configure-pod-container/resize-container-resources/) (see below)

#### What did you expect to happen?

InPlacePodVerticalScaling resizes pod inplace when only memory request is patched.

#### How can we reproduce it (as minimally and precisely as possible)?

Pod Spec: 
```
apiVersion: v1
kind: Pod
metadata:
  name: qos-demo-5
  namespace: qos-example
spec:
  containers:
  - name: qos-demo-ctr-5
    image: nginx
    resizePolicy:
    - resourceName: cpu
      restartPolicy: RestartContainer
    - resourceName: memory
      restartPolicy: RestartContainer
    resources:
      limits:
        memory: "800Mi"
        cpu: "700m"
      requests:
        memory: "200Mi"
        cpu: "700m"
```

`kubectl apply -f podspec.yaml`
`kubectl -n qos-example patch pod qos-demo-5 --patch '{"spec":{"containers":[{"name":"qos-demo-ctr-5", "resources":{"requests":{"memory":"400Mi"}}}]}}'`

Output:

Initial PodInfo:

```
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"name":"qos-demo-5","namespace":"qos-example"},"spec":{"containers":[{"image":"nginx","name":"qos-demo-ctr-5","resizePolicy":[{"resourceName":"cpu","restartPolicy":"NotRequired"},{"resourceName":"memory","restartPolicy":"RestartContainer"}],"resources":{"limits":{"cpu":"700m","memory":"800Mi"},"requests":{"cpu":"700m","memory":"200Mi"}}}]}}
  creationTimestamp: "2024-06-17T21:11:33Z"
  name: qos-demo-5
  namespace: qos-example
  resourceVersion: "818"
  uid: f19d015b-6e38-42ae-b245-6936383e4102
spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: qos-demo-ctr-5
    resizePolicy:
    - resourceName: cpu
      restartPolicy: NotRequired
    - resourceName: memory
      restartPolicy: RestartContainer
    resources:
      limits:
        cpu: 700m
        memory: 800Mi
      requests:
        cpu: 700m
        memory: 200Mi
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-xpksb
      readOnly: true
  ...
status:
  conditions:
    ...
  containerStatuses:
  - allocatedResources:
      cpu: 700m
      memory: 200Mi
    containerID: containerd://c4193e1640c9dc7b32c910cecb46b622d472c122a454f2fedbba15ef3932c1d5
    image: docker.io/library/nginx:latest
    imageID: docker.io/library/nginx@sha256:56b388b0d79c738f4cf51bbaf184a14fab19337f4819ceb2cae7d94100262de8
    lastState: {}
    name: qos-demo-ctr-5
    ready: true
    resources:
      limits:
        cpu: 700m
        memory: 800Mi
      requests:
        cpu: 700m
        memory: 200Mi
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2024-06-17T21:11:34Z"
  ...
  qosClass: Burstable
  startTime: "2024-06-17T21:11:33Z"
```

After resize memory request (no restart):

```
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"name":"qos-demo-5","namespace":"qos-example"},"spec":{"containers":[{"image":"nginx","name":"qos-demo-ctr-5","resizePolicy":[{"resourceName":"cpu","restartPolicy":"NotRequired"},{"resourceName":"memory","restartPolicy":"RestartContainer"}],"resources":{"limits":{"cpu":"700m","memory":"800Mi"},"requests":{"cpu":"700m","memory":"200Mi"}}}]}}
  creationTimestamp: "2024-06-17T21:11:33Z"
  name: qos-demo-5
  namespace: qos-example
  resourceVersion: "941"
  uid: f19d015b-6e38-42ae-b245-6936383e4102
spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: qos-demo-ctr-5
    resizePolicy:
    - resourceName: cpu
      restartPolicy: NotRequired
    - resourceName: memory
      restartPolicy: RestartContainer
    resources:
      limits:
        cpu: 700m
        memory: 800Mi
      requests:
        cpu: 700m
        memory: 400Mi
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-xpksb
      readOnly: true
  ...
status:
  conditions:
    ...
  containerStatuses:
  - allocatedResources:
      cpu: 700m
      memory: 400Mi
    containerID: containerd://c4193e1640c9dc7b32c910cecb46b622d472c122a454f2fedbba15ef3932c1d5
    image: docker.io/library/nginx:latest
    imageID: docker.io/library/nginx@sha256:56b388b0d79c738f4cf51bbaf184a14fab19337f4819ceb2cae7d94100262de8
    lastState: {}
    name: qos-demo-ctr-5
    ready: true
    resources:
      limits:
        cpu: 700m
        memory: 800Mi
      requests:
        cpu: 700m
        memory: 200Mi
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2024-06-17T21:11:34Z"
  ...
  qosClass: Burstable
  resize: InProgress
  startTime: "2024-06-17T21:11:33Z"
```

After resize memory request (with restart):

```
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"Pod","metadata":{"annotations":{},"name":"qos-demo-5","namespace":"qos-example"},"spec":{"containers":[{"image":"nginx","name":"qos-demo-ctr-5","resizePolicy":[{"resourceName":"cpu","restartPolicy":"RestartContainer"},{"resourceName":"memory","restartPolicy":"RestartContainer"}],"resources":{"limits":{"cpu":"700m","memory":"800Mi"},"requests":{"cpu":"700m","memory":"200Mi"}}}]}}
  creationTimestamp: "2024-06-17T21:16:14Z"
  name: qos-demo-5
  namespace: qos-example
  resourceVersion: "1230"
  uid: 80927d45-019e-4fe6-b7d4-7334a653e4b7
spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: qos-demo-ctr-5
    resizePolicy:
    - resourceName: cpu
      restartPolicy: RestartContainer
    - resourceName: memory
      restartPolicy: RestartContainer
    resources:
      limits:
        cpu: 700m
        memory: 800Mi
      requests:
        cpu: 700m
        memory: 400Mi
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-pqrld
      readOnly: true
  ...
status:
  conditions:
  ...
  containerStatuses:
  - allocatedResources:
      cpu: 700m
      memory: 400Mi
    containerID: containerd://98d8f353ff23df61c9a31a96f18f70d0566406125304908e92c256931a3f9707
    image: docker.io/library/nginx:latest
    imageID: docker.io/library/nginx@sha256:56b388b0d79c738f4cf51bbaf184a14fab19337f4819ceb2cae7d94100262de8
    lastState: {}
    name: qos-demo-ctr-5
    ready: true
    resources:
      limits:
        cpu: 700m
        memory: 800Mi
      requests:
        cpu: 700m
        memory: 200Mi
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2024-06-17T21:16:15Z"
  ...
  qosClass: Burstable
  resize: InProgress
  startTime: "2024-06-17T21:16:14Z"
```

Strangely the Pod allocations are updated, but the containerStatus.resources are not, and the pod is not restarted.

#### Anything else we need to know?

Reported in #124712 

This differs from #124786 in that the memory request after resizing is large. Differs from the samples previously run by @esotsal as only the memory requests are changed and not the limits.

#### Kubernetes version

<details>
Tested on Kubernetes v1.30.1
</details>


#### Cloud provider

Using KinD to run on local WSL2 (ew) machine. Definitely need somebody to check this on proper Linux or otherwise.

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125541 The PV may be in a Terminating state and cannot be deleted when the pod are created and then the pod and pvc are quickly force deleted .

- Issue é“¾æ¥ï¼š[#125541](https://github.com/kubernetes/kubernetes/issues/125541)

### Issue å†…å®¹

#### What happened?

In my testing environment, I discovered a strange phenomenon: when I create a pod that uses a PVC and then forcefully delete the pod and PVC before the pod creation is complete, the PV bound to the PVC **may** enter a Terminating state and cannot be deleted.

The log is as follows
```
I0614 10:04:56.153700   77153 kubelet.go:2389] "SyncLoop ADD" source="api" pods=[default/hp-volume-dbwfm]

I0614 10:04:58.233778   77153 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820\" (UniqueName: \"kubernetes.io/csi/XXXX^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820\") pod \"hp-volume-dbwfm\" (UID: \"42077f53-c1df-4b6f-8647-f06100647095\") " pod="default/hp-volume-dbwfm"
I0614 10:04:58.234344   77153 operation_generator.go:1101] "MapVolume.WaitForAttach entering for volume \"pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820\" (UniqueName: \"kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820\") pod \"hp-volume-dbwfm\" (UID: \"42077f53-c1df-4b6f-8647-f06100647095\") DevicePath \"/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820/eac1a6c0-bdc4-4b75-804f-7f89df07694b\"" pod="default/hp-volume-dbwfm"
I0614 10:04:58.308685   77153 operation_generator.go:1111] "MapVolume.WaitForAttach succeeded for volume \"pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820\" (UniqueName: \"kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820\") pod \"hp-volume-dbwfm\" (UID: \"42077f53-c1df-4b6f-8647-f06100647095\") DevicePath \"csi-75dac338e0aff53508bd6995ba2aa58d65d6eb42310c3d9d7111f2720a1d2823\"" pod="default/hp-volume-dbwfm"

I0614 10:05:02.444088   77153 kubelet.go:2405] "SyncLoop DELETE" source="api" pods=[default/hp-volume-dbwfm]
I0614 10:05:02.520860   77153 kubelet.go:2399] "SyncLoop REMOVE" source="api" pods=[default/hp-volume-dbwfm]

I0614 10:05:17.729528   77153 operation_generator.go:1225] "MapVolume.MapPodDevice succeeded for volume \"pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820\" (UniqueName: \"kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820\") pod \"hp-volume-dbwfm\" (UID: \"42077f53-c1df-4b6f-8647-f06100647095\") volumeMapPath \"/var/lib/kubelet/pods/42077f53-c1df-4b6f-8647-f06100647095/volumeDevices/kubernetes.io~csi\"" pod="default/hp-volume-dbwfm"
I0614 10:05:17.734612   77153 operation_generator.go:1233] MapVolume.NodeExpandVolume failed for volume "pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820" (UniqueName: "kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820") pod "hp-volume-dbwfm" (UID: "42077f53-c1df-4b6f-8647-f06100647095") : mountVolume.NodeExpandVolume get PVC failed : persistentvolumeclaims "pvc-volume-251e86b0-cfe7-475b-88f4-cced2416d3ff" is forbidden: User "system:node:worker3" cannot get resource "persistentvolumeclaims" in API group "" in the namespace "default": no relationship found between node 'worker3' and this object
I0614 10:05:17.734835   77153 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820 podName: nodeName:}" failed. No retries permitted until 2024-06-14 10:05:18.734806235 +0800 CST m=+62756.584763139 (durationBeforeRetry 1s). Error: MapVolume.MarkVolumeAsMounted failed while expanding volume for volume "pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820" (UniqueName: "kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820") pod "hp-volume-dbwfm" (UID: "42077f53-c1df-4b6f-8647-f06100647095") : mountVolume.NodeExpandVolume get PVC failed : persistentvolumeclaims "pvc-volume-251e86b0-cfe7-475b-88f4-cced2416d3ff" is forbidden: User "system:node:worker3" cannot get resource "persistentvolumeclaims" in API group "" in the namespace "default": no relationship found between node 'worker3' and this object
I0614 10:05:18.795992   77153 operation_generator.go:1101] "MapVolume.WaitForAttach entering for volume \"pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820\" (UniqueName: \"kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820\") pod \"hp-volume-r5sp7\" (UID: \"eac1a6c0-bdc4-4b75-804f-7f89df07694b\") DevicePath \"/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820/42077f53-c1df-4b6f-8647-f06100647095\"" pod="default/hp-volume-r5sp7"
I0614 10:06:03.681110   77153 reconciler_common.go:231] "operationExecutor.MountVolume started for volume \"pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820\" (UniqueName: \"kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820\") pod \"hp-volume-dbwfm\" (UID: \"42077f53-c1df-4b6f-8647-f06100647095\") " pod="default/hp-volume-dbwfm"
I0614 10:06:03.681366   77153 operation_generator.go:1101] "MapVolume.WaitForAttach entering for volume \"pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820\" (UniqueName: \"kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820\") pod \"hp-volume-dbwfm\" (UID: \"42077f53-c1df-4b6f-8647-f06100647095\") DevicePath \"/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/publish/pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820/7fb78f1b-06f4-4b95-8514-a80de1f0294d\"" pod="default/hp-volume-dbwfm"
I0614 10:06:03.726906   77153 operation_generator.go:1111] "MapVolume.WaitForAttach succeeded for volume \"pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820\" (UniqueName: \"kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820\") pod \"hp-volume-dbwfm\" (UID: \"42077f53-c1df-4b6f-8647-f06100647095\") DevicePath \"csi-75dac338e0aff53508bd6995ba2aa58d65d6eb42310c3d9d7111f2720a1d2823\"" pod="default/hp-volume-dbwfm"
I0614 10:06:15.315317   77153 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820 podName: nodeName:}" failed. No retries permitted until 2024-06-14 10:06:15.815290561 +0800 CST m=+62813.665247389 (durationBeforeRetry 500ms). Error: MapVolume.MapPodDevice failed for volume "pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820" (UniqueName: "kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820") pod "hp-volume-dbwfm" (UID: "42077f53-c1df-4b6f-8647-f06100647095") : rpc error: code = Internal desc = failed to check client lun mapping status for volume pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820
I0614 10:06:16.176459   77153 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820 podName: nodeName:}" failed. No retries permitted until 2024-06-14 10:06:16.676425647 +0800 CST m=+62814.526382436 (durationBeforeRetry 500ms). Error: UnmapDevice failed for volume "pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820" (UniqueName: "kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820") on node "worker3" : the device "/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820/dev" is still referenced from other Pods [/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820/dev/42077f53-c1df-4b6f-8647-f06100647095]
I0614 10:06:16.696898   77153 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820 podName: nodeName:}" failed. No retries permitted until 2024-06-14 10:06:17.696865394 +0800 CST m=+62815.546822186 (durationBeforeRetry 1s). Error: UnmapDevice failed for volume "pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820" (UniqueName: "kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820") on node "worker3" : the device "/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820/dev" is still referenced from other Pods [/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820/dev/42077f53-c1df-4b6f-8647-f06100647095]
I0614 10:06:17.742271   77153 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820 podName: nodeName:}" failed. No retries permitted until 2024-06-14 10:06:19.742240671 +0800 CST m=+62817.592197458 (durationBeforeRetry 2s). Error: UnmapDevice failed for volume "pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820" (UniqueName: "kubernetes.io/csi/iscsi.csi.xsky.com^csi-iscsi-pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820") on node "worker3" : the device "/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820/dev" is still referenced from other Pods [/var/lib/kubelet/plugins/kubernetes.io/csi/volumeDevices/pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820/dev/42077f53-c1df-4b6f-8647-f06100647095]

```

kubectl get pv pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820
```
pv-dc3407bc-6c3f-4123-bcf8-8f28b7575820   2Gi       RWX            Delete           Terminating   default/pvc-dc3407bc-6c3f-4123-bcf8-8f28b7575820  csi-XXX            1h
```

 I drawn up a time-series chart for the log contents .

![image](https://github.com/kubernetes/kubernetes/assets/8870947/7f97ebcb-b963-4c02-9cce-bcc5b040a558)


#### What did you expect to happen?

the PV can be delete successfully

#### How can we reproduce it (as minimally and precisely as possible)?

1. create a pvc , the bound pv's persistentVolumeReclaimPolicy is **Delete**;
2. create a pod use the pvc;
3. force delete the pod and the pvcï¼ˆkubectl delete po xx --force && kubectl delete pvc yy --force ï¼‰
4. It is possible that the PV may be in a Terminating state and cannot be deleted

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>
1.28

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125530 kube-controller-manager Master Election Time Exceeds the Lease Time

- Issue é“¾æ¥ï¼š[#125530](https://github.com/kubernetes/kubernetes/issues/125530)

### Issue å†…å®¹

#### What happened?

First, the configuration of controller-manager election is as follows:
`--leader-elect-lease-duration=25s
 --leader-elect-renew-deadline=20s`
The active controller node is master2. After the master2 node is powered off:
The controller-manager master election request of the master1 node fails because the persistent connection of http2 is not disconnected for about 30s.
After the connection is restored, it takes about 30 seconds to select the active node.
![image](https://github.com/kubernetes/kubernetes/assets/54977497/dfa6b745-6b4b-4ffb-8d43-de57c52f68ac)
![image](https://github.com/kubernetes/kubernetes/assets/54977497/649db3ef-aa87-475b-8e32-236eb864ba6a)
![image](https://github.com/kubernetes/kubernetes/assets/54977497/fefd0dd0-f8d6-449e-808c-8b475ab80782)


#### What did you expect to happen?

The owner can complete the selection according to the lease time.

#### How can we reproduce it (as minimally and precisely as possible)?

Power on and off the active kube-controller-manager node. There is a low probability that this problem recurs.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.28
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #125526 AppArmor Profile not activated. #2310

- Issue é“¾æ¥ï¼š[#125526](https://github.com/kubernetes/kubernetes/issues/125526)

### Issue å†…å®¹

#### What happened?

I installed SPO and followed the documentation regarding an example installation of an AppArmor Profile. I am running Kubernetes 1.30.0. If I use the securityContext clause, it has no effect. Even more, after Pod creation, its content is deleted. If I use the deprecated annotation, I get an error telling me The Pod "testpod2" is invalid: metadata.annotations[container.apparmor.security.beta.kubernetes.io/testpod2]: Invalid value: test-profile: invalid AppArmor profile name: test-profile

#### What did you expect to happen?

I'd expect, after preparing everything by the book to have a Pod running with an AppArmor Profile.



#### How can we reproduce it (as minimally and precisely as possible)?

I've installed SPO via OLM:
```
---
apiVersion: v1
kind: Namespace
metadata:
  name: security-profiles-operator
  labels:
    openshift.io/cluster-monitoring: "true"
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: security-profiles-operator
  namespace: security-profiles-operator
spec:
  targetNamespaces:
  - security-profiles-operator
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: security-profiles-operator-sub
  namespace: security-profiles-operator
spec:
  channel: stable
  name: security-profiles-operator
  source: operatorhubio-catalog
  sourceNamespace: olm
```

I then applied the patch and created an example Profile, as documented in: https://github.com/kubernetes-sigs/security-profiles-operator/blob/main/installation-usage.md#create-an-apparmor-profile

I can verify that up to this point, all is fine:

```
$ k -n security-profiles-operator get spod spod -o yaml

apiVersion: security-profiles-operator.x-k8s.io/v1alpha1
kind: SecurityProfilesOperatorDaemon
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"security-profiles-operator.x-k8s.io/v1alpha1","kind":"SecurityProfilesOperatorDaemon","metadata":{"annotations":{},"name":"spod","namespace":"security-profiles-operator"},"spec":{"enableAppArmor":true,"enableLogEnricher":false,"enableSelinux":false}}
  creationTimestamp: "2024-06-13T18:34:41Z"
  generation: 4
  labels:
    app: security-profiles-operator
  name: spod
  namespace: security-profiles-operator
  resourceVersion: "5738208"
  uid: b0427364-278a-43a9-bfeb-1b5cfa1ead63
spec:
  disableOciArtifactSignatureVerification: false
  enableAppArmor: true
  enableLogEnricher: false
  enableSelinux: false
  hostProcVolumePath: /proc
  priorityClassName: system-node-critical
  selinuxOptions:
    allowedSystemProfiles:
    - container
  selinuxTypeTag: spc_t
  staticWebhookConfig: false
  tolerations:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
    operator: Exists
  - effect: NoSchedule
    key: node-role.kubernetes.io/control-plane
    operator: Exists
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
status:
  conditions:
  - lastTransitionTime: "2024-06-13T23:08:28Z"
    reason: Available
    status: "True"
    type: Ready
  state: RUNNING


$ k get securityprofilenodestatuses.security-profiles-operator.x-k8s.io

NAME                           STATUS      AGE
test-profile-master0-k8s.lan   Installed   45h
test-profile-master1-k8s.lan   Installed   45h
test-profile-master2-k8s.lan   Installed   45h
test-profile-worker1-k8s.lan   Installed   45h
test-profile-worker2-k8s.lan   Installed   45h
test-profile-worker3-k8s.lan   Installed   45h
test-profile-worker4-k8s.lan   Installed   45h


$ k get apparmorprofiles.security-profiles-operator.x-k8s.io test-profile -o yaml

apiVersion: security-profiles-operator.x-k8s.io/v1alpha1
kind: AppArmorProfile
metadata:
  annotations:
    description: Block writing to any files in the disk.
  creationTimestamp: "2024-06-13T20:35:30Z"
  finalizers:
  - worker4-k8s.lan-deleted
  - master1-k8s.lan-deleted
  - master0-k8s.lan-deleted
  - worker2-k8s.lan-deleted
  - worker3-k8s.lan-deleted
  - master2-k8s.lan-deleted
  - worker1-k8s.lan-deleted
  generation: 1
  labels:
    spo.x-k8s.io/profile-id: AppArmorProfile-test-profile
  name: test-profile
  namespace: default
  resourceVersion: "5720939"
  uid: ea8c3705-1ba2-4b14-afc5-d0a05aa958fc
spec:
  policy: |
    #include <tunables/global>

    profile test-profile flags=(attach_disconnected) {
      #include <abstractions/base>

      file,

      # Deny all file writes.
      deny /** w,
    }
```

Here is my simple Pod yaml used in a first test:

```
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: testpod2
  name: testpod2
  annotations:
    container.apparmor.security.beta.kubernetes.io/testpod2: test-profile
spec:
# securityContext:
#   appArmorProfile:
#     type: Localhost
#     localhostProfile: test-profile
  containers:
  - image: nginx
    name: testpod2
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Never
status: {}
```

When I would like to create this Pod, it looks like this:

```
Warning: metadata.annotations[container.apparmor.security.beta.kubernetes.io/testpod2]: deprecated since v1.30; use the "appArmorProfile" field instead
The Pod "testpod2" is invalid: metadata.annotations[container.apparmor.security.beta.kubernetes.io/testpod2]: Invalid value: "test-profile": invalid AppArmor profile name: "test-profile"
```

If I remove the annotation and uncomment the securityContext, the Pod will be created, but no AppArmor Profile is active. And if I check the deployed Pod, it looks like this then:

```
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2024-06-15T19:41:11Z"
  labels:
    run: testpod2
  name: testpod2
  namespace: default
  resourceVersion: "5759727"
  uid: 67ec29a1-c224-4832-9c1a-0c79f71d8aa4
spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: testpod2
    resources: {}
    securityContext: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-xqpz2
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: worker4-k8s.lan
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Never
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-xqpz2
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2024-06-15T19:41:13Z"
    status: "True"
    type: PodReadyToStartContainers
  - lastProbeTime: null
    lastTransitionTime: "2024-06-15T19:41:11Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2024-06-15T19:41:13Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2024-06-15T19:41:13Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2024-06-15T19:41:11Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: cri-o://c48853001023f76e93ece437368807343209a3711706e3a16a52c352cbac2f73
    image: docker.io/library/nginx:latest
    imageID: docker.io/library/nginx@sha256:0f04e4f646a3f14bf31d8bc8d885b6c951fdcf42589d06845f64d18aec6a3c4d
    lastState: {}
    name: testpod2
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2024-06-15T19:41:13Z"
  hostIP: 192.168.121.153
  hostIPs:
  - ip: 192.168.121.153
  phase: Running
  podIP: 10.0.2.208
  podIPs:
  - ip: 10.0.2.208
  qosClass: BestEffort
  startTime: "2024-06-15T19:41:11Z"
```


#### Anything else we need to know?



Here's what I get, when I check for AppArmor from withing the Pod:

```
$ kubectl exec testpod2 -- cat /proc/1/attr/current

crio-default (enforce)
```

And here's what I see regarding loaded Profiles on each Node:

```
$ sudo cat /sys/kernel/security/apparmor/profiles

test-profile (enforce)
crio-default (enforce)
/{,usr/}sbin/dhclient (enforce)
/usr/lib/connman/scripts/dhclient-script (enforce)
/usr/lib/NetworkManager/nm-dhcp-helper (enforce)
/usr/lib/NetworkManager/nm-dhcp-client.action (enforce)
/usr/sbin/chronyd (enforce)
nvidia_modprobe (enforce)
nvidia_modprobe//kmod (enforce)
man_groff (enforce)
man_filter (enforce)
/usr/bin/man (enforce)
lsb_release (enforce)
```

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0

```

</details>


#### Cloud provider

<details>
Bare Metal
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ uname -a
Linux worker3-k8s.lan 6.1.0-15-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.66-1 (2023-12-09) x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
```
Version:  0.1.0
RuntimeName:  cri-o
RuntimeVersion:  1.30.0
RuntimeApiVersion:  v1
```
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125522 ValidatingAdmissionPolicy: auditAnnotations are included in the audit event always

- Issue é“¾æ¥ï¼š[#125522](https://github.com/kubernetes/kubernetes/issues/125522)

### Issue å†…å®¹

#### What happened?

When an audit annotation is defined in a `Validating Admission Policy` (VAP), this annotation is added to the api-server audit event always.

#### What did you expect to happen?

The audit annotation is only included to the audit event in case any of the VAP validations expressions evaluates to false, in oder words, in case the VAP is violated.

> The documentation is kind of confusing in that regard, because [here](https://kubernetes.io/docs/reference/access-authn-authz/validating-admission-policy/#audit-annotations) it says: _In this example the annotation will only be included if the `spec.replicas` of the Deployment is more than 50, otherwise the CEL expression evaluates to null and the annotation will not be included_. But if in fact `spec.replicas` is more than 50, it means that the VAP is met, because the CEL expression evaluates to true.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a VAP and binding that prevents creating pods in `default` namespace, like this:

> **Edited:** I had a typo in the  `spec.auditAnnotations[0].key`, correct value is validation_failed, and not validation_failure as I initially posted.

```yaml
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicy
metadata:
  name: "prevent-default-namespace"
spec:
  failurePolicy: Fail
  matchConstraints:
    resourceRules:
    - apiGroups:   ["*"]
      apiVersions: ["*"]
      operations:  ["CREATE", "UPDATE"]
      resources:   ["pods"]
  validations:
    - expression: "object.metadata.namespace != 'default'"
      message: "Pods should not be created in the default namespace"
  auditAnnotations:
    - key: "validation_failed"
      valueExpression: "'true'"
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: "prevent-default-namespace"
spec:
  policyName: "prevent-default-namespace"
  validationActions: [Warn, Audit]
```

This policy expression will validate to `false` for all pods created in `default` namespace and to `true` for all pods created in any other pod.

Now create two pods, one of them in `default` namespace:
```bash
â¯ kubectl run my-pod-10 --image=curlimages/curl:8.6.0 --command -- sleep 1000
Warning: Validation failed for ValidatingAdmissionPolicy 'prevent-default-namespace' with binding 'prevent-default-namespace': Pods should not be created in the default namespace. 
pod/my-pod-10 created

â¯ kubectl run my-pod-20 -n testing --image=curlimages/curl:8.6.0 --command -- sleep 1000
pod/my-pod-20 created
```

If we check the api-server audit event corresponding to the creation of each pod, we can see that the audit annotation `"prevent-default-namespace/validation_failed":"true"` has been set in both cases:

```bash
â¯ talosctl read /var/log/audit/kube/kube-apiserver.log | grep "my-pod-10" | grep "validation_failed"
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"Metadata","auditID":"7703d71a-00e8-4b81-9fb2-d08fbd136fc5","stage":"ResponseComplete","requestURI":"/api/v1/namespaces/default/pods?fieldManager=kubectl-run","verb":"create","user":{"username":"admin","groups":["system:masters","system:authenticated"]},"sourceIPs":["192.168.65.1"],"userAgent":"kubectl/v1.28.0 (darwin/arm64) kubernetes/855e7c4","objectRef":{"resource":"pods","namespace":"default","name":"my-pod-10","apiVersion":"v1"},"responseStatus":{"metadata":{},"code":201},"requestReceivedTimestamp":"2024-06-15T15:14:40.823489Z","stageTimestamp":"2024-06-15T15:14:40.830217Z","annotations":{"authorization.k8s.io/decision":"allow","authorization.k8s.io/reason":"","mutation.webhook.admission.k8s.io/round_0_index_1":"{\"configuration\":\"gatekeeper-mutating-webhook-configuration\",\"webhook\":\"mutation.gatekeeper.sh\",\"mutated\":false}","mutation.webhook.admission.k8s.io/round_0_index_4":"{\"configuration\":\"opentelemetry-operator-mutation\",\"webhook\":\"mpod.kb.io\",\"mutated\":false}","pod-security.kubernetes.io/enforce-policy":"privileged:latest","prevent-default-namespace/validation_failed":"true","validation.policy.admission.k8s.io/validation_failure":"[{\"message\":\"Pods should not be created in the default namespace\",\"policy\":\"prevent-default-namespace\",\"binding\":\"prevent-default-namespace\",\"expressionIndex\":0,\"validationActions\":[\"Warn\",\"Audit\"]}]"}}

â¯ talosctl read /var/log/audit/kube/kube-apiserver.log | grep "my-pod-20" | grep "validation_failed"
{"kind":"Event","apiVersion":"audit.k8s.io/v1","level":"Metadata","auditID":"7ba57b1d-6cbd-489d-bef1-709f337b9eda","stage":"ResponseComplete","requestURI":"/api/v1/namespaces/testing/pods?fieldManager=kubectl-run","verb":"create","user":{"username":"admin","groups":["system:masters","system:authenticated"]},"sourceIPs":["192.168.65.1"],"userAgent":"kubectl/v1.28.0 (darwin/arm64) kubernetes/855e7c4","objectRef":{"resource":"pods","namespace":"testing","name":"my-pod-20","apiVersion":"v1"},"responseStatus":{"metadata":{},"code":201},"requestReceivedTimestamp":"2024-06-15T15:15:49.607569Z","stageTimestamp":"2024-06-15T15:15:49.614769Z","annotations":{"authorization.k8s.io/decision":"allow","authorization.k8s.io/reason":"","mutation.webhook.admission.k8s.io/round_0_index_1":"{\"configuration\":\"gatekeeper-mutating-webhook-configuration\",\"webhook\":\"mutation.gatekeeper.sh\",\"mutated\":false}","mutation.webhook.admission.k8s.io/round_0_index_4":"{\"configuration\":\"opentelemetry-operator-mutation\",\"webhook\":\"mpod.kb.io\",\"mutated\":false}","pod-security.kubernetes.io/enforce-policy":"privileged:latest","prevent-default-namespace/validation_failed":"true"}}
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

I'm using in this case Talos in Docker, but it should not be relevant in this case:

```console
â¯ talosctl read /etc/os-release
NAME="Talos"
ID=talos
VERSION_ID=v1.7.2
PRETTY_NAME="Talos (v1.7.2)"
HOME_URL="https://www.talos.dev/"
BUG_REPORT_URL="https://github.com/siderolabs/talos/issues"
VENDOR_NAME="Sidero Labs"
VENDOR_URL="https://www.siderolabs.com/"

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125506 /var/lib/kubelet/device-plugins/kubelet.sock Connection refused

- Issue é“¾æ¥ï¼š[#125506](https://github.com/kubernetes/kubernetes/issues/125506)

### Issue å†…å®¹

#### What happened?

The service pod needs to register the device plug-in. Invoking the kubelet registration interface times out. The kubelet log does not contain error information. The OS environment is normal.When we look at the code, we find that the s.grpc.Serve (ln) method does not handle the returned err information. Is it possible that this is the cause?
https://github.com/kubernetes/kubernetes/blob/eb6840928df59bf8203b1eda839ccd3da68fb37d/pkg/kubelet/cm/devicemanager/plugin/v1beta1/server.go#L110-L113

#### What did you expect to happen?

The connection to kubelet through kubelet.sock is normal.

#### How can we reproduce it (as minimally and precisely as possible)?

The problem occurs when the OS is restarted.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.28
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125505 Suppress all logs and only see errors. Verbosity=0 does not help

- Issue é“¾æ¥ï¼š[#125505](https://github.com/kubernetes/kubernetes/issues/125505)

### Issue å†…å®¹

#### What happened?

Setting verbosity to 0 does not disable all logs. 

#### What did you expect to happen?

Setting verbosity to the lowest level should only show fatal error messages or there should be some other setting that the user can set to suppress all logs and see only error logs.

#### How can we reproduce it (as minimally and precisely as possible)?

Create kind cluster with verbosity of k8s components set to 0.
example config:
```
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        v: "0"
    extraArgs:
      kube-apiserver:
        v: "0"
      kube-controller-manager:
        v: "0"
      kube-scheduler:
        v: "0"
- role: worker
  kubeadmConfigPatches:
  - |
    kind: JoinConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        v: "0"
```
Get logs from any of k8s components and you will see info logs.

#### Anything else we need to know?

this is needed for optimizing the CPU usage in resource intensive environments.

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.2", GitCommit:"7f6f68fdabc4df88cfea2dcf9a19b2b830f1e647", GitTreeState:"clean", BuildDate:"2023-05-17T14:20:07Z", GoVersion:"go1.20.4", Compiler:"gc", Platform:"darwin/arm64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.3", GitCommit:"25b4e43193bcda6c7328a6d147b1fb73a33f1598", GitTreeState:"clean", BuildDate:"2023-06-15T00:38:14Z", GoVersion:"go1.20.5", Compiler:"gc", Platform:"linux/arm64"}
```

</details>


#### Cloud provider

<details>
Kind cluster
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125503 The cluster kube-scheduler scheduling is unbalanced, causing the pod to hang and fail to run, even though there are currently idle nodes

- Issue é“¾æ¥ï¼š[#125503](https://github.com/kubernetes/kubernetes/issues/125503)

### Issue å†…å®¹

#### What happened?

I have 10 nodes in my cluster, and I keep the default cluster scheduling mechanism without configuring the manual scheduling mechanism for tasks (including node taints, label scheduling affinity, etc., and the configuration of pod limit request), but I found that one of my pods will use up 50% of the node's memory after running for a period of time, and the total memory occupied by other nodes of this node is more than 90%. But when I restarted this pod, I found the first problem, it was still scheduled to this node, although there were other nodes with only 30% memory usage, and the second problem was that I found that the pod seemed to like this node very much, and other newly started pods would also be assigned to this 90% load machine, resulting in these pod tasks can only be suspended, can not run, resulting in business stagnation. Why is this?

Off topic: I understand that you can use artificial scheduling mechanism to solve this problem, but the default scheduling I found from the official website's documentation explanation is to score nodes, and there is no additional mechanism to affect other I am very curious about this problem, and I found that many people in the k8s community have encountered this problem.

#### What did you expect to happen?

I want to know two questions about the above:
1. Regarding the node score, even though the load of this node is 90%, why is its score still higher than other nodes or is it scheduled to this machine for other reasons?
2. Why do subsequent pods like to run on this node so much, when there are other nodes and their models and configurations are exactly the same? Is there any data affinity (pod will look for the node where the data was stored last time to reduce the cluster's response time and performance consumption)?

#### How can we reproduce it (as minimally and precisely as possible)?

This is a long-term practical problem

#### Anything else we need to know?

![187d5c530b74963a020687139c0cc24](https://github.com/kubernetes/kubernetes/assets/66284983/7d3996d3-3523-47d2-b58f-a04529838206)
![8612d5413d9fd36f5052b6b1ea96936](https://github.com/kubernetes/kubernetes/assets/66284983/cbd2348e-aaac-41ab-8f34-f0dfec06f59f)


#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>
v1.26.1

#### Cloud provider

<details>

</details>
aws and tencent k8s cluster

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>
centos7

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>
kubeadm

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125491 Scheduler pre-binding can cause race conditions with automated empty node removal

- Issue é“¾æ¥ï¼š[#125491](https://github.com/kubernetes/kubernetes/issues/125491)

### Issue å†…å®¹

#### What happened?

In a Google Kubernetes Engine (GKE) environment, a pod was requesting a large Persistent Volume Claim (PVC). After the appropriate node was identified for the pod, the pod became stuck in the prebinding stage for several minutes while the volume provisioning process completed.  Since the node name was not assigned to the pod during this time, the Cluster Autoscaler perceived the node as unoccupied. Consequently, the Cluster Autoscaler initiated a scale-down of the node, unaware that the pending pod was scheduled to run there.

#### What did you expect to happen?

I would expect that the Scheduler would communicate the intended binding of the pod to the identified node. This would enable the Cluster Autoscaler to recognize that the node is not actually empty and prevent it from being scaled down prematurely.

#### How can we reproduce it (as minimally and precisely as possible)?

The issue arose in a large GKE cluster with pods requesting substantial PVCs, making replication potentially challenging. However, the race condition within the Scheduler is evident.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
1.27
</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #125476 Can k8s restrict kubelet from using kmem through configuration

- Issue é“¾æ¥ï¼š[#125476](https://github.com/kubernetes/kubernetes/issues/125476)

### Issue å†…å®¹

#### What happened?

Can k8s restrict kubelet from using kmem through configuration

#### What did you expect to happen?

Which specific version is supported

#### How can we reproduce it (as minimally and precisely as possible)?

Which specific version is supported

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
1.19
```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125471 Race between seeing a CRD added event and being able to select the kind

- Issue é“¾æ¥ï¼š[#125471](https://github.com/kubernetes/kubernetes/issues/125471)

### Issue å†…å®¹

#### What happened?

We have observed a situation that looks to be a race condition whereby we have a watch on `CustomResourceDefinitions` and when we see an `added` event for a new one, we perform a list on the `apiVersion` + `kind`. When this runs on a pod however we get a 404 when performing the list operation unless we add an arbitrary delay. 

_The context behind this is that we use this as a check to make sure a kind exists before performing other actions, such as creating a watch. We do not expect any resources to be present immediately after adding a CRD._





#### What did you expect to happen?

We would expect that we would not see an added event for a custom resource before the API server is able to serve the resource.

#### How can we reproduce it (as minimally and precisely as possible)?

It's hard as it's a bit racey, this is only reproducible from a pod and even then, not always.

1. Create a watch for `CustomResourceDefinitions`
2. Add a handler so when an `ADDED` event is seen it will perform a get for the kind e.g. `GET: /apis/GROUP/VERSION/KIND`
3. Add a new custom resource

If the event is received fast enough, the handler will see a 404 for it's GET call.

#### Anything else we need to know?

We are running a single master, so there is only one `etcd`

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.8
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.8-gke.1095000
```

</details>


#### Cloud provider

<details>
GKE: v1.28.8-gke.1095000
</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125467 Conntrack tables having stale entries for UDP connection

- Issue é“¾æ¥ï¼š[#125467](https://github.com/kubernetes/kubernetes/issues/125467)

### Issue å†…å®¹

#### What happened?

We experienced an EC2 node failure within our EKS cluster. This affected node was running two CoreDNS pods, which are responsible for DNS resolution in our Kubernetes cluster. Envoy connects to CoreDNS through the UDP protocol. After these CoreDNS pods were terminated, Envoy continued to attempt connections to the terminated IP for DNS resolution.
The kube-proxy failed to update the entry in the conntrack tables, causing some Envoy pods to still connect to the terminated CoreDNS pod IP. Once we restarted the Envoy pods, the entry was refreshed, and the DNS timeout issue was resolved.

Mapping in Conntrack table for src pod ip-10.103.83.53 for UDP protocol.

Query : â€œconntrack -p udp -L --src 10.103.83.53â€

Response : â€œudp  17 27 src=10.103.83.53 dst=<clusterIp> sport=21667 dport=53 [UNREPLIED] src=10.103.78.37 dst=10.103.83.53 sport=53 dport=21667 mark=0 use=1 contrack v1.4.4 (conntrack-tools): 1 flow entries have been shownâ€

#### What did you expect to happen?

Kubeproxy should update or refresh this conntrack table. 
conntrack shouldn't have stale UDP connection routes.

#### How can we reproduce it (as minimally and precisely as possible)?

KubeProxy version we tested with - kube-proxy:v1.29.4-minimal-eksbuild.1

which include this fix as well -  https://github.com/kubernetes/kubernetes/issues/119249

Steps we followed in our EKS cluster to stimulate this issue

- Remove podAntiAffinity on the coreDNS deployment
- Identify the node where we would want to concentrate the coreDNS to cordon it
- Evict workloads from that node
- Make that for not getting scaled down using node annotations
- Uncordon that node, and cordon the rest of the node
- Delete two coreDNS pods
- Ensure they get scheduled to that targeted node
- Login to that node, install tmux, and down all the network interfaces to simulate the 2/2 failure
- Notice the node going into NotReady state, and check for the DNS c-ares errors

#### Anything else we need to know?



#### Kubernetes version

<details>

```console
$ kubectl version
Server Version: v1.29.4-eks-036c24b
```

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
      NAME="Amazon Linux"
      VERSION="2"
      ID="amzn"
      ID_LIKE="centos rhel fedora"
      VERSION_ID="2"
      PRETTY_NAME="Amazon Linux 2"
      ANSI_COLOR="0;33"
      CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2"
      HOME_URL="https://amazonlinux.com/"
      SUPPORT_END="2025-06-30"
$ uname -a
Linux ip-10-185-97-105.ec2.internal 5.10.215-203.850.amzn2.aarch64 #1 SMP Tue Apr 23 20:32:21 UTC 2024 aarch64 aarch64 aarch64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125453 kubelet unbalanced affinity pod in different numa node

- Issue é“¾æ¥ï¼š[#125453](https://github.com/kubernetes/kubernetes/issues/125453)

### Issue å†…å®¹

#### What happened?

in current kubelet below algorithm, the pods/containers shall be affinity to different numa node unbalanced. 

for example: if there is 6 pods with integer cpu container configured, most of pods will be scheduled into numa0 if numa0 has enough available cpus (4 pods in numa0 and 2 pod in numa1). 

then if turboboost is enabled with OS control or static max performance, the cpu frequency will be different in callload running. for example cpus in numa0 maybe 2500 but cpus in numa1 maybe 2900 since workload is different in two numa nodes. 

in this environment, even the incoming traffic to those pods which are scheduled in numa0 and numa1 is same (round robin), the cpu usage in those pods are different. that will cause issue which traffic load can not be increased to expected values since 4 pods in numa0 is already with high cpu usage (call maybe starting drop)  but 2 pods in numa1 with low cpu usage.

       // Algorithm: topology-aware best-fit
        // 1. Acquire whole NUMA nodes and sockets, if available and the container
        //    requires at least a NUMA node or socket's-worth of CPUs. If NUMA
        //    Nodes map to 1 or more sockets, pull from NUMA nodes first.
        //    Otherwise pull from sockets first.
        acc.numaOrSocketsFirst.takeFullFirstLevel()
        if acc.isSatisfied() {
                return acc.result, nil
        }
        acc.numaOrSocketsFirst.takeFullSecondLevel()
        if acc.isSatisfied() {
                return acc.result, nil
        }

        // 2. Acquire whole cores, if available and the container requires at least
        //    a core's-worth of CPUs.
        acc.takeFullCores()
        if acc.isSatisfied() {
                return acc.result, nil
        }

        // 3. Acquire single threads, preferring to fill partially-allocated cores
        //    on the same sockets as the whole cores we have already taken in this
        //    allocation.
        acc.takeRemainingCPUs()
        if acc.isSatisfied() {
                return acc.result, nil
        }


#### What did you expect to happen?

affinity pod/container balanced to different numa instead of filling numa0 first and then numa1

#### How can we reproduce it (as minimally and precisely as possible)?

1. turboboost enabled
2. cpuManagePolicy as static 
3. Pod with GQoS and Container with integer cpu request/limit

#### Anything else we need to know?

_No response_

#### Kubernetes version

test version is 1.27


#### Cloud provider

any cloud provider


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125448 Incorrect error reporting in case of missing cgroup controllers

- Issue é“¾æ¥ï¼š[#125448](https://github.com/kubernetes/kubernetes/issues/125448)

### Issue å†…å®¹

#### What happened?

If the cgroup validation fails due to missing required controllers (e.g. https://github.com/kubernetes/kubernetes/issues/122955) the error that surfaces is, 
```bash
Jun 11 20:22:16 ip-10-0-2-54 kubenswrapper[2176]: E0611 20:22:16.903259    2176 kubelet.go:1559] "Failed to start ContainerManager" err="failed to initialize top level QOS containers: root container [kubepods] doesn't exist"
```
This is incorrect. The code at, https://github.com/kubernetes/kubernetes/blob/d593c886b1fd4119204b8e5a5fc012e36f42fd7a/pkg/kubelet/cm/qos_container_manager_linux.go#L85 

calls https://github.com/kubernetes/kubernetes/blob/d593c886b1fd4119204b8e5a5fc012e36f42fd7a/pkg/kubelet/cm/cgroup_manager_linux.go#L239 but ignores the error. 

This could be quite misleading. In case of https://github.com/kubernetes/kubernetes/issues/122955, the kubelet didn't fail to start because of missing `kubepods` cgroup, but rather due to missing `cpuset`. 

The correct reported error should have been, 

```bash
 Jun 11 20:30:33 ip-10-0-2-54 kubenswrapper[2178]: E0611 20:30:33.085077    2178 kubelet.go:1559] "Failed to start ContainerManager" err="failed to initialize top level QOS containers: error validating root container [kubepods] : cgroup [\"kubepods\"] has some missing controllers: cpuset"
 ```

#### What did you expect to happen?

Correct error to propagate when cgroup validation fails during kubelet startup. 

#### How can we reproduce it (as minimally and precisely as possible)?

Everytime kubelet fails to start due to cgroup validation. e.g https://github.com/kubernetes/kubernetes/issues/122955

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125427 When a deployment selects a node with the kubelet service not running as the nodeName, the Pods will remain in the pending state, then move to Terminating, and new Pods will be continuously created in a loop, resulting in a large number of Terminating Pods that cannot be terminated.

- Issue é“¾æ¥ï¼š[#125427](https://github.com/kubernetes/kubernetes/issues/125427)

### Issue å†…å®¹

#### What happened?

When a deployment selects a node with the kubelet service not running as the nodeName, the Pods will remain in the pending state, then move to Terminating, and new Pods will be continuously created in a loop, resulting in a large number of Terminating Pods that cannot be terminated.

#### What did you expect to happen?

Waiting for the node to be ready, the Pods remain in the pending state.







#### How can we reproduce it (as minimally and precisely as possible)?

1ï¼šcreate app
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: nginx
  name: nginx
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
      nodeName: host3.example.com

2ï¼š disable kubelet
systemctl disable kubelet

3ï¼šhost3.example.com 
reboot

#### Anything else we need to know?

_No response_

#### Kubernetes version

[root@host1 ~]#  kubectl version --output=yaml
clientVersion:
  buildDate: "2023-10-18T11:44:31Z"
  compiler: gc
  gitCommit: b8609d4dd75c5d6fba4a5eaa63a5507cb39a6e99
  gitTreeState: clean
  gitVersion: v1.26.10
  goVersion: go1.20.10
  major: "1"
  minor: "26"
  platform: linux/amd64
kustomizeVersion: v4.5.7
serverVersion:
  buildDate: "2023-10-18T11:33:36Z"
  compiler: gc
  gitCommit: b8609d4dd75c5d6fba4a5eaa63a5507cb39a6e99
  gitTreeState: clean
  gitVersion: v1.26.10
  goVersion: go1.20.10
  major: "1"
  minor: "26"
  platform: linux/amd64

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125412 kubectl port-forward failing for named ports in native sidecar

- Issue é“¾æ¥ï¼š[#125412](https://github.com/kubernetes/kubernetes/issues/125412)

### Issue å†…å®¹

#### What happened?

```
kubectl port-forward svc/adguard-metrics metrics (named port on service)
error: Pod 'adguard-primary-7cc5d498f4-67zs4' does not have a named port 'metrics'

kubectl port-forward adguard-primary-7cc5d498f4-67zs4 metrics (named port on native sidecar)
error: Pod 'adguard-primary-7cc5d498f4-67zs4' does not have a named port 'metrics'
```

#### What did you expect to happen?

Since native sidecards support named ports, named ports on native sidecar should be supported in kubectl port-forward.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a pod/deployment with a named port. Example:

<details>

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: app
  labels:
    app.kubernetes.io/name: app
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: app
  template:
    metadata:
      labels:
        app.kubernetes.io/name: app
    spec:
      initContainers:
        - name: nginx
          image: nginx
          restartPolicy: Always
          ports:
            - name: metrics
              protocol: TCP
              containerPort: 80
          livenessProbe:
            httpGet:
              port: metrics
              path: /metrics
            failureThreshold: 5
            periodSeconds: 10
          readinessProbe:
            httpGet:
              port: metrics
              path: /metrics
            failureThreshold: 5
            periodSeconds: 10
      containers:
        - name: busybox
          image: busybox
          command: ['sh', '-c', 'echo Hello Kubernetes! && sleep infinity']
```

</details>

Port forward with named port does not work:
```
kubectl port-forward app-<pod-id> 8080:metrics
error: Pod 'app-7c74c5d9db-bzmqb' does not have a named port 'metrics'
```

Port forward with port number works:
`kubectl port-forward app-<pod-id> 8080:80`
```
Forwarding from 127.0.0.1:8080 -> 80
Forwarding from [::1]:8080 -> 80
Handling connection for 8080
```

Same deal when port-forwarding through deployment or service:

```
kubectl port-forward deploy/app 8080:metrics
error: Pod 'app-645b84f678-tjh8j' does not have a named port 'metrics'
```

```
kubectl port-forward svc/app 8080:metrics
error: Pod 'app-645b84f678-tjh8j' does not have a named port 'metrics'
```

#### Anything else we need to know?

@aojea ping as requested

#### Kubernetes version

<details>

```console
kubectl version
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.0
```

</details>


#### Cloud provider

<details>
Talos Linux in VMs on ProxMox
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Talos"
ID=talos
VERSION_ID=v1.7.1
PRETTY_NAME="Talos (v1.7.1)"
HOME_URL="https://www.talos.dev/"
BUG_REPORT_URL="https://github.com/siderolabs/talos/issues"
VENDOR_NAME="Sidero Labs"
VENDOR_URL="https://www.siderolabs.com/"
$ uname -a
Linux version 6.6.29-talos (@buildkitsandbox) (gcc (GCC) 13.2.0, GNU ld (GNU Binutils) 2.42) #1 SMP Tue Apr 30 14:19:14 UTC 2024
```

</details>


#### Install tools

N/A

#### Container runtime (CRI) and version (if applicable)

N/A


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
CNI: Cilium v1.15.5
</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125410 Job may get stuck repeatedly failing with Duplicate value message for uncountedTerminatedPods.failed

- Issue é“¾æ¥ï¼š[#125410](https://github.com/kubernetes/kubernetes/issues/125410)

### Issue å†…å®¹

#### What happened?

We have encountered a situation when a Job controller is stuck constantly failing on the `syncJob`. It fails with the validation message 
like this `job_controller.go:600] "Unhandled Error" err="syncing job: tracking status: adding uncounted pods to status: Job.batch \"pi\" is invalid: status.uncountedTerminatedPods.failed[0]: Duplicate value: \"ca8e72a0-735a-45c9-925d-d82c91256b86\"" logger="UnhandledError"`.

The Job:
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  generation: 1
  namespace: default
spec:
  backoffLimit: 6
  completionMode: NonIndexed
  completions: 1
  parallelism: 1
  suspend: false
  ...
status:
  failed: 1
  ready: 0
  ...
  uncountedTerminatedPods:
    succeeded:
    - xyz
  ```
  Pod:
```yaml
apiVersion: v1
kind: Pod
metadata:
  ...
  finalizers:
  - batch.kubernetes.io/job-tracking
  ...
  uid: xyz
spec:
  affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - preference:
          matchExpressions:
          - ...
        weight: 100
...
status:
  conditions:
  - message: 'Taint manager: deleting due to NoExecute taint'
    reason: DeletionByTaintManager
    status: "True"
    type: DisruptionTarget
  - status: "True"
    type: Initialized
  - reason: PodFailed
    status: "False"
    type: Ready
  - reason: PodFailed
    status: "False"
    type: ContainersReady
  - status: "True"
    type: PodScheduled
  containerStatuses:
  - state:
      terminated:
        exitCode: 137
        finishedAt: null
        message: The container could not be located when the pod was terminated
        reason: ContainerStatusUnknown
        startedAt: null
  message: 'Pod was rejected: Predicate NodeAffinity failed'
  phase: Failed
  qosClass: Guaranteed
  reason: NodeAffinity

```

#### What did you expect to happen?

Job controller does not get stuck. It might be that the issue originated from user actions, or kubelet bug which transitioned the Pod first to `Succeeded`, then to `Failed`. However, Job controller should be resilient to a situation like this.

#### How can we reproduce it (as minimally and precisely as possible)?

It remains unclear what happened exactly - if the pod was transitioned to `Succeeded`, and after to `Failed`.
See below an artificial scenario which reproduces the Job controller getting stuck.



#### Anything else we need to know?

This scenario renders the Job controller stuck in v1.30.0. It involves a pod transitioning first to `Succeeded`. then to `Failed`. We had a similar issue in the past with pods being categorized first as Failed, then Succeeded: https://github.com/kubernetes/kubernetes/pull/111646/

1. Create the Job as follows:
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
  labels:
    environment: test
spec:
  completions: 2
  parallelism: 2
  template:
    metadata:
      labels:
        environment: test
    spec:
      terminationGracePeriodSeconds: 1
      containers:
        - name: busybox
          image: busybox
          command: ["sleep", "5m"]
      restartPolicy: Never
  backoffLimit: 1
```
2. Create CEL validation `kubectl create -f cel.yaml` to make sure the Pod finalizer removal request fails:
```yaml
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicy
metadata:
  name: "demo-policy.example.com"
spec:
  failurePolicy: Fail
  matchConstraints:
    resourceRules:
    - apiGroups:   [""]
      apiVersions: ["v1"]
      operations:  ["CREATE", "UPDATE"]
      resources:   ["pods"]
  validations:
    - expression: "size(object.metadata.finalizers) > 0"
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: "demo-binding-test.example.com"
spec:
  policyName: "demo-policy.example.com"
  validationActions: [Deny]
```
3. Set pod status as `Succeeded` with `kubectl edit pods/pi-766q7 --subresource=status`

You can observe that the Job status looks like this:
```yaml
  status:
    active: 2
    ready: 2
    uncountedTerminatedPods:
      succeeded:
      - 56f6d408-7949-4c66-8256-9a9c57595dab
```
4. Kill the pod that was mark succeeded: `kubectl delete pods/pi-766q7` - it makes the Pod to transition to `Failed`
5. Delete the CEL rule `kubectl delete -f cel.yaml`

Issue: Job controller does not remove the Job finalizer from the `pi-766q7` pod. It perdiodically fails with errors like this: `E0610 16:05:18.089533       1 job_controller.go:600] "Unhandled Error" err="syncing job: tracking status: adding uncounted pods to status: Job.batch \"pi\" is invalid: status.uncountedTerminatedPods.failed[0]: Duplicate value: \"56f6d408-7949-4c66-8256-9a9c57595dab\"" logger="UnhandledError"`

#### Kubernetes version

<details>
1.27
```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #125409 High kubepods cgroup cpu.weight/shares starves kernel threads on many core systems

- Issue é“¾æ¥ï¼š[#125409](https://github.com/kubernetes/kubernetes/issues/125409)

### Issue å†…å®¹

#### What happened?

Hi team,

originally this issue manifested as poor performance being observed in Redpanda but for the ease of reproducing I will only talk about [fio](https://github.com/axboe/fio) below.

Running fio on large servers "inside" of kubernetes results in less IOPS (2x in the example below) than running it  "outside" of kubernetes natively on the host.

This is caused by the cgroup settings as created by k8s effectively starving important kernel threads handling interrupts/direct-io handling.

k8s creates a cgroup hierarchy in which the pods are going to run. At the root it creates the `kubepods` cgroup. The cgroup cpu.shares/weight (v1/v2) parameter will be set to N*DEFAULT (1024 for v1/100 for v2) where N is the number of cores on the system (I think potentially `systemReserved` is being substracted from this?).

As a consequence anything running in k8s pod will get a lot higher priority than kernel threads which by default run in the `/` cgroup with a default weight (1024/100). On large many-core servers this could potentially be a very big difference - originally we discovered this on 72 core server.

Note that this problem doesn't necessarily require all cores being busy or having "allocated" all quota. Processes are being weighted against each other at their hierarchy so even if a process has been assigned just one core (1000 millicpus) and that's all it uses it will still compete with the full kubepods cgroup weight (N*DEFAULT) against kernel threads. 

Further the problem with some kernel threads is that they run on the same core as the application thread related to the work (IO etc.).



#### What did you expect to happen?

We shouldn't lose IOPS.

It's unclear to me whether the root kubepods group needs to have a non-default cpu.weight/shares. As mentioned above this shouldn't be needed for scheduling in regards to k8s pods against each other.



#### How can we reproduce it (as minimally and precisely as possible)?

The issue can be easily reproduced outside of kubernetes by just creating the needed cgroups manually:

Create a cgroup (note this is v2 so default is 100):

```
cgcreate -g cpu:/kubepods
cgset -r cpu.weight=1000 kubepods
```

Run outside of the cgroup:

```
taskset -c 11 fio --name=write_iops --directory=/mnt/xfs --size=10G
--time_based --runtime=1m --ramp_time=10s \
  --ioengine=libaio --direct=1 --verify=0 --bs=4K --iodepth=128
--rw=randwrite --group_reporting=1  --iodepth_batch_submit=128
--iodepth_batch_complete_max=128
...
   iops        : min=200338, max=200944, avg=200570.37, stdev=60.93, samples=120
...
```

Now run inside the cgroup:

```
cgexec -g cpu:kubepods -- taskset -c 11 fio --name=write_iops
--directory=/mnt/xfs --size=10G --time_based --runtime=1m \
  --ramp_time=10s --ioengine=libaio --direct=1 --verify=0 --bs=4K
--iodepth=128 --rw=randwrite --group_reporting=1
--iodepth_batch_submit=128  --iodepth_batch_complete_max=128
...
   iops        : min=113589, max=120554, avg=116073.72, stdev=1334.46,
samples=120
...
```

We see that we almost lost 50% of our IOPS (potentially worse if the disks could actually go faster).

This is on:

 - Amazon Linux 2023 / 6.1 Linux
 - i3en.3xlarge instance / 200k IOPS@4K write
 - XFS filesystem

#### Anything else we need to know?

We can also further debug of why this is happening at the kernel level (also posted [here](https://lore.kernel.org/linux-fsdevel/CAM9ScsHJ1zQ4j+0J+jQ1fUyRvxTMCF9OKC9kcvD5uyQZKxN1Pg@mail.gmail.com/T/#u)):

Comparing cpu time and context switches we see that in the bad case we are context switching a lot more. Overall the core is running at 100% in the bad case while only at something like 50% in the good case.

no-cgroup: task clock of fio:

```
perf stat -e task-clock -p 27393 -- sleep 1

 Performance counter stats for process id '27393':

            442.62 msec task-clock                       #    0.442
CPUs utilized

       1.002110208 seconds time elapsed
```

no cgroup: context switches on that core:

```
perf stat -e context-switches -C 11  -- sleep 1

 Performance counter stats for 'CPU(s) 11':

            103001      context-switches

       1.001048841 seconds time elapsed
```

Using the cgroup: task clock of fio:

```
perf stat -e task-clock -p 27456 -- sleep 1

 Performance counter stats for process id '27456':

            695.30 msec task-clock                       #    0.695
CPUs utilized

       1.001112431 seconds time elapsed
```

Using the cgroup: context switches on that core:

```
perf stat -e context-switches -C 11  -- sleep 1

 Performance counter stats for 'CPU(s) 11':

            243755      context-switches

       1.001096517 seconds time elapsed
```

So we are doing about 2.5x more context switches in the bad case. Doing the math at about ~120k IOPS we see that for every IOP we are doing two interrupts (in and out).

Finally we can also look at some perf sched traces to get an idea for what is happening (sched_stat_runtime calls omitted):

The general pattern in the good case seems to be:

```
fio 28143 [011]  2038.648954:       sched:sched_waking:
comm=kworker/11:68 pid=27489 prio=120 target_cpu=011
        ffffffff9f0d7ba3 try_to_wake_up+0x2b3 ([kernel.kallsyms])
        ffffffff9f0d7ba3 try_to_wake_up+0x2b3 ([kernel.kallsyms])
        ffffffff9f0b91d5 __queue_work+0x1d5 ([kernel.kallsyms])
        ffffffff9f0b93a4 queue_work_on+0x24 ([kernel.kallsyms])
        ffffffff9f3bb04c iomap_dio_bio_end_io+0x8c ([kernel.kallsyms])
        ffffffff9f53749d blk_mq_end_request_batch+0xfd ([kernel.kallsyms])
        ffffffff9f7198df nvme_irq+0x7f ([kernel.kallsyms])
        ffffffff9f113956 __handle_irq_event_percpu+0x46 ([kernel.kallsyms])
        ffffffff9f113b14 handle_irq_event+0x34 ([kernel.kallsyms])
        ffffffff9f118257 handle_edge_irq+0x87 ([kernel.kallsyms])
        ffffffff9f033eee __common_interrupt+0x3e ([kernel.kallsyms])
        ffffffff9fa023ab common_interrupt+0x7b ([kernel.kallsyms])
        ffffffff9fc00da2 asm_common_interrupt+0x22 ([kernel.kallsyms])
        ffffffff9f297a4b internal_get_user_pages_fast+0x10b ([kernel.kallsyms])
        ffffffff9f591bdb __iov_iter_get_pages_alloc+0xdb ([kernel.kallsyms])
        ffffffff9f591ef9 iov_iter_get_pages2+0x19 ([kernel.kallsyms])
        ffffffff9f5269af __bio_iov_iter_get_pages+0x5f ([kernel.kallsyms])
        ffffffff9f526d6d bio_iov_iter_get_pages+0x1d ([kernel.kallsyms])
        ffffffff9f3ba578 iomap_dio_bio_iter+0x288 ([kernel.kallsyms])
        ffffffff9f3bab72 __iomap_dio_rw+0x3e2 ([kernel.kallsyms])
        ffffffff9f3baf8e iomap_dio_rw+0xe ([kernel.kallsyms])
        ffffffff9f45ff58 xfs_file_dio_write_aligned+0x98 ([kernel.kallsyms])
        ffffffff9f460644 xfs_file_write_iter+0xc4 ([kernel.kallsyms])
        ffffffff9f39c876 aio_write+0x116 ([kernel.kallsyms])
        ffffffff9f3a034e io_submit_one+0xde ([kernel.kallsyms])
        ffffffff9f3a0960 __x64_sys_io_submit+0x80 ([kernel.kallsyms])
        ffffffff9fa01135 do_syscall_64+0x35 ([kernel.kallsyms])
        ffffffff9fc00126 entry_SYSCALL_64_after_hwframe+0x6e ([kernel.kallsyms])
                   3ee5d syscall+0x1d (/usr/lib64/libc.so.6)
              2500000025 [unknown] ([unknown])

fio 28143 [011]  2038.648974:       sched:sched_switch: prev_comm=fio
prev_pid=28143 prev_prio=120 prev_state=R ==> next_comm=kworker/11:68
next_pid=27489 next_prio=120
        ffffffff9fa0d002 __schedule+0x282 ([kernel.kallsyms])
        ffffffff9fa0d002 __schedule+0x282 ([kernel.kallsyms])
        ffffffff9fa0d3aa schedule+0x5a ([kernel.kallsyms])
        ffffffff9f135a36 exit_to_user_mode_prepare+0xa6 ([kernel.kallsyms])
        ffffffff9fa050fd syscall_exit_to_user_mode+0x1d ([kernel.kallsyms])
        ffffffff9fa01142 do_syscall_64+0x42 ([kernel.kallsyms])
        ffffffff9fc00126 entry_SYSCALL_64_after_hwframe+0x6e ([kernel.kallsyms])
                   3ee5d syscall+0x1d (/usr/lib64/libc.so.6)
              2500000025 [unknown] ([unknown])

kworker/11:68-d 27489 [011]  2038.648984:       sched:sched_switch:
prev_comm=kworker/11:68 prev_pid=27489 prev_prio=120 prev_state=I ==>
next_comm=fio next_pid=28143 next_prio=120
        ffffffff9fa0d002 __schedule+0x282 ([kernel.kallsyms])
        ffffffff9fa0d002 __schedule+0x282 ([kernel.kallsyms])
        ffffffff9fa0d3aa schedule+0x5a ([kernel.kallsyms])
        ffffffff9f0ba249 worker_thread+0xb9 ([kernel.kallsyms])
        ffffffff9f0c1559 kthread+0xd9 ([kernel.kallsyms])
        ffffffff9f001e02 ret_from_fork+0x22 ([kernel.kallsyms])
```

fio is busy submitting aio events and gets interrupted from the nvme interrupts at which point control is yielded to the dio thread which handles the completion and yields back to fio.

Looking at the bad case there now seems to be some form of ping pong:

```
fio 28517 [011]  2702.018634:       sched:sched_switch: prev_comm=fio
prev_pid=28517 prev_prio=120 prev_state=S ==> next_comm=kworker/11:68
next_pid=27489 next_prio=120
        ffffffff9fa0d002 __schedule+0x282 ([kernel.kallsyms])
        ffffffff9fa0d002 __schedule+0x282 ([kernel.kallsyms])
        ffffffff9fa0d3aa schedule+0x5a ([kernel.kallsyms])
        ffffffff9f39de89 read_events+0x119 ([kernel.kallsyms])
        ffffffff9f39e042 do_io_getevents+0x72 ([kernel.kallsyms])
        ffffffff9f39e689 __x64_sys_io_getevents+0x59 ([kernel.kallsyms])
        ffffffff9fa01135 do_syscall_64+0x35 ([kernel.kallsyms])
        ffffffff9fc00126 entry_SYSCALL_64_after_hwframe+0x6e ([kernel.kallsyms])
                   3ee5d syscall+0x1d (/usr/lib64/libc.so.6)
             11300000113 [unknown] ([unknown])

kworker/11:68+d 27489 [011]  2702.018639:       sched:sched_waking:
comm=fio pid=28517 prio=120 target_cpu=011
        ffffffff9f0d7ba3 try_to_wake_up+0x2b3 ([kernel.kallsyms])
        ffffffff9f0d7ba3 try_to_wake_up+0x2b3 ([kernel.kallsyms])
        ffffffff9f0fa9d1 autoremove_wake_function+0x11 ([kernel.kallsyms])
        ffffffff9f0fbb90 __wake_up_common+0x80 ([kernel.kallsyms])
        ffffffff9f0fbd23 __wake_up_common_lock+0x83 ([kernel.kallsyms])
        ffffffff9f39f9df aio_complete_rw+0xef ([kernel.kallsyms])
        ffffffff9f0b9c35 process_one_work+0x1e5 ([kernel.kallsyms])
        ffffffff9f0ba1e0 worker_thread+0x50 ([kernel.kallsyms])
        ffffffff9f0c1559 kthread+0xd9 ([kernel.kallsyms])
        ffffffff9f001e02 ret_from_fork+0x22 ([kernel.kallsyms])

kworker/11:68+d 27489 [011]  2702.018642:       sched:sched_switch:
prev_comm=kworker/11:68 prev_pid=27489 prev_prio=120 prev_state=R+ ==>
next_comm=fio next_pid=28517 next_prio=120
        ffffffff9fa0d002 __schedule+0x282 ([kernel.kallsyms])
        ffffffff9fa0d002 __schedule+0x282 ([kernel.kallsyms])
        ffffffff9fa0d4cb preempt_schedule_common+0x1b ([kernel.kallsyms])
        ffffffff9fa0d51c __cond_resched+0x1c ([kernel.kallsyms])
        ffffffff9f0b9c56 process_one_work+0x206 ([kernel.kallsyms])
        ffffffff9f0ba1e0 worker_thread+0x50 ([kernel.kallsyms])
        ffffffff9f0c1559 kthread+0xd9 ([kernel.kallsyms])
        ffffffff9f001e02 ret_from_fork+0x22 ([kernel.kallsyms])
```

fio is sleeping in io_getevents waiting for all events to complete. The dio worker thread gets scheduled in handling aio completions one by one. This allows fio to wake as there are some amount of completions ready for it to process. Now because of the high weight of the fio process the kernel worker only gets a short amount of runtime and gets preempted by the scheduler yielding back to fio (notice the stack and R+ in the above trace). However because fio is waiting for all aios to complete it wakes up and goes straight back to sleep again. This ping pong continues.


#### Kubernetes version

1.25

#### Cloud provider

All / AWS EKS

#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

containerd

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #125397 After the kubelet restarted, the ready state of the pod should not change.

- Issue é“¾æ¥ï¼š[#125397](https://github.com/kubernetes/kubernetes/issues/125397)

### Issue å†…å®¹

#### What happened?

A pod with startuoProbe/readinessProbe has been ready, it will be used as backends for Services. After the kubelet restarted, this pod will be marked `not ready`.  An additional startuoProbe needs to be executed.



#### What did you expect to happen?

After the kubelet restarted, the ready state of the pod should not change. The ready status should only be false if a probe fails.

#### How can we reproduce it (as minimally and precisely as possible)?

1. create a pod with startupProbe 
Pod demo yaml:
```
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: startup
  name: startup-exec
spec:
  containers:
  - name: startup
    image: busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy;  sleep infinity
    startupProbe:
      exec:
        command:
        - /bin/sh
        - -c
        - sleep 10; cat /tmp/healthy
      initialDelaySeconds: 6
      timeoutSeconds: 15
      periodSeconds: 60
```

2. wait this pod to be ready
```
    - lastProbeTime: null
      lastTransitionTime: "2024-06-08T07:53:27Z"
      status: "True"
      type: ContainersReady
```
3. restart the kubelet 
```
kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; preset: enabled)
    Drop-In: /etc/systemd/system/kubelet.service.d
             â””â”€10-kubeadm.conf, 11-kind.conf
     Active: active (running) since Sat 2024-06-08 08:17:09 UTC; 1min 16s ago
```
4. this pod will be not ready. 
```
Jun 08 08:17:55 kind-control-plane kubelet[4644]: I0608 08:17:55.463964    4644 kubelet.go:2527] "SyncLoop (probe)" probe="startup" 
status="unhealthy" pod="default/startup-exec"


    - lastProbeTime: null
      lastTransitionTime: "2024-06-08T08:17:55Z"
      message: 'containers with unready status: [liveness]'
      reason: ContainersNotReady
      status: "False"
      type: ContainersReady
```

5.After an additional startuoProbe, it will be ready again.
```
        - lastProbeTime: null
      lastTransitionTime: "2024-06-08T08:19:05Z"
      status: "True"
      type: ContainersReady
```

#### Anything else we need to know?

It also affects readiness probe

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Server Version: version.Info{Major:"1", Minor:"30", GitVersion:"v1.30.0", GitCommit:"7c48c2bd72b9bf5c44d21d7338cc7bea77d0ad2a", GitTreeState:"clean", BuildDate:"2024-05-13T22:00:36Z", GoVersion:"go1.22.2", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
kind
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125394 [FG:InPlacePodVerticalScaling] Race condition setting pod resize status

- Issue é“¾æ¥ï¼š[#125394](https://github.com/kubernetes/kubernetes/issues/125394)

### Issue å†…å®¹

The kubelet updates the pod status by first generating the pod status based on its internal representation, and then [merging](https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/status/status_manager.go#L1049) that with the latest status from the API, based on whether the Kubelet should be the source of truth for those fields.

Currently, the PodResizeStatus is not synced with the API server status, which means that whatever the Kubelet thinks the resize status should be will overwrite whatever PodResizeStatus is currently stored in the API server. This can lead to a race condition where the Kubelet has not yet seen a resize update, and clobbers the `Proposed` resize status.

I'm not sure exactly what the right behavior is here, but it probably needs to be an intelligent state transition, or may require comparing resource versions.

/sig node
/kind bug
/cc @vinaykul @thockin 

### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #125393 Remove Kubelet soft-admission

- Issue é“¾æ¥ï¼š[#125393](https://github.com/kubernetes/kubernetes/issues/125393)

### Issue å†…å®¹

Kubelet soft admission handlers were created as a way for the Kubelet to block pods that cannot be run for reasons that can be resolved. In practice, this mechanism was only ever used by AppArmor ([here](https://github.com/kubernetes/kubernetes/blob/eef6c6082d4e34fc4a0675a36ec5cc575cd13696/pkg/kubelet/kubelet.go#L912)).

Problems with soft-admission rejection:
1. This is only used by AppArmor, and only for situations that cannot realistically be resolved (host or build does not support apparmor)
2. Pods held in the blocked state must be manually cleaned up. While there they hold reserved capacity from the scheduler and continually retry syncpod.
3. No other features work this way - usually Kubelet hard rejects pods that it knows it cannot run, or the runtime fails to start the pod/container.

This was originally added to prevent controllers from continuously recreating pods that cannot run, but that is a separate problem that should be approached globally, rather than with this 1-off AppArmor solution.

I propose we remove Kubelet soft admission entirely, and move the AppArmor admission handler to the hard-admission list.

/sig node
/milestone v1.31
/kind bug

### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125380 Kubelet stop watching Pods from API-Server

- Issue é“¾æ¥ï¼š[#125380](https://github.com/kubernetes/kubernetes/issues/125380)

### Issue å†…å®¹

#### What happened?

We have encountered several instances where certain AKS nodes fail to respond to pod updates. This issue includes:

- Terminating pods not receiving SIGTERM notifications.
- New pods not being started as scheduled.

After reviewing the API requests to the API server during one of these incidents, we noticed that the kubelet on the affected node stopped watching pods (`/api/v1/pods?allowWatchBookmarks=true&watch=true`) before the issue happened. Despite this, the kubelet continued to report themselves as healthy via heartbeat.

Kubelet cannot recover from this non-responding state by itself.
The issue can be mitigated by restarting the kubelet on the affected node.

#### What did you expect to happen?

Kubelet should not stop responding to pods while reporting healthy.

#### How can we reproduce it (as minimally and precisely as possible)?

We have not found a stable way to reproduce this issue.

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.27.13

#### Cloud provider

Azure


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Common Base Linux Mariner"
VERSION="2.0.20240425"
ID=mariner
VERSION_ID="2.0"
PRETTY_NAME="CBL-Mariner/Linux"
ANSI_COLOR="1;34"
HOME_URL="https://aka.ms/cbl-mariner"
BUG_REPORT_URL="https://aka.ms/cbl-mariner"
SUPPORT_URL="https://aka.ms/cbl-mariner"

$ uname -a
# not same node, only for reference
Linux aks-agentpoolm-26294222-vmss000067 5.15.153.1-2.cm2 #1 SMP Thu Apr 25 16:39:35 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125376 Unstructured converter should produce int64 given uint input

- Issue é“¾æ¥ï¼š[#125376](https://github.com/kubernetes/kubernetes/issues/125376)

### Issue å†…å®¹

#### What happened?

DeepCopy on DefaultUnstructuredConverter.ToUnstructured output from `struct { Field uint32 }` panics.

#### What did you expect to happen?

runtime/unstructured inconsistently generates int64 for uint in maps but uses uint64 in structs. They should all be using int64.

#### How can we reproduce it (as minimally and precisely as possible)?

```go
package main

import "k8s.io/apimachinery/pkg/runtime"
import "k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"

func main() {
	type Good struct { Hash map[string]uint32 }
	type Panics struct { Hash uint32 }

	obj, _ := runtime.DefaultUnstructuredConverter.ToUnstructured(&Good{Hash: map[string]uint32{"a":1}})
	uns := &unstructured.Unstructured{Object: obj}
	uns.DeepCopy()

	obj, _ = runtime.DefaultUnstructuredConverter.ToUnstructured(&Panics{Hash: 1})
	uns = &unstructured.Unstructured{Object: obj}
	uns.DeepCopy()
}
```

```text
panic: cannot deep copy uint64

goroutine 1 [running]:
k8s.io/apimachinery/pkg/runtime.DeepCopyJSONValue({0x6e3020?, 0x9c09a8?})
        /home/chankyin/go/pkg/mod/k8s.io/apimachinery@v0.30.1/pkg/runtime/converter.go:639 +0x273
k8s.io/apimachinery/pkg/runtime.DeepCopyJSONValue({0x6f6fc0?, 0xc000193a10})
        /home/chankyin/go/pkg/mod/k8s.io/apimachinery@v0.30.1/pkg/runtime/converter.go:623 +0x2b7
k8s.io/apimachinery/pkg/runtime.DeepCopyJSON(...)
        /home/chankyin/go/pkg/mod/k8s.io/apimachinery@v0.30.1/pkg/runtime/converter.go:608
k8s.io/apimachinery/pkg/apis/meta/v1/unstructured.(*Unstructured).DeepCopy(0xc00015ff30)
        /home/chankyin/go/pkg/mod/k8s.io/apimachinery@v0.30.1/pkg/apis/meta/v1/unstructured/unstructured.go:151 +0x58
main.main()
        /data00/home/chankyin/go/src/code.byted.org/tce/federation/tmp/main.go:16 +0xed
exit status 2

```

The relevant code is found at https://github.com/kubernetes/kubernetes/blob/eef6c6082d4e34fc4a0675a36ec5cc575cd13696/staging/src/k8s.io/apimachinery/pkg/runtime/converter.go#L838

#### Anything else we need to know?

_No response_

#### Kubernetes version

eef6c6082d4e34fc4a0675a36ec5cc575cd13696

#### Cloud provider

N/A

#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125370 Pod IP temporarily removed from status when pod transitions to a terminal state

- Issue é“¾æ¥ï¼š[#125370](https://github.com/kubernetes/kubernetes/issues/125370)

### Issue å†…å®¹

#### What happened?

My pod completes (success or failure). The IP temporarily is removed on one of the status updates.

#### What did you expect to happen?

The IP is persistent .

#### How can we reproduce it (as minimally and precisely as possible)?

Apply:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  template:
    spec:
      containers:
      - name: pi
        image: howardjohn/shell
        command:
        - bash
        - -c
        - |
          sleep 2
          exit 0
      restartPolicy: Never
  backoffLimit: 1
```

Watch:
```shell
$ kubectl get pod -ojson -w > res
$ cat res |jq -c '.status.phase + " " +.status.podIP' -r
Pending
Pending
Pending
Running 10.244.0.6
Running 10.244.0.6
Succeeded
Succeeded 10.244.0.6
Succeeded 10.244.0.6
```

The initial 'succeeded' should have the IP still.

On older versions:
```
Pending 
Pending 
Pending 
Running 10.244.0.6
Running 10.244.0.6
Succeeded 10.244.0.6
```

#### Anything else we need to know?

|Version|Status|
|-|-|
|1.22|IP retained|
|1.26|IP retained|
|1.27|IP Lost|
|1.28|IP lost|
|1.30|IP lost|

Full pod state, from 1.27:

<details>

```json
{
    "apiVersion": "v1",
    "kind": "Pod",
    "metadata": {
        "creationTimestamp": "2024-06-06T19:34:19Z",
        "finalizers": [
            "batch.kubernetes.io/job-tracking"
        ],
        "generateName": "pi-",
        "labels": {
            "batch.kubernetes.io/controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "batch.kubernetes.io/job-name": "pi",
            "controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "job-name": "pi"
        },
        "name": "pi-72qn6",
        "namespace": "default",
        "ownerReferences": [
            {
                "apiVersion": "batch/v1",
                "blockOwnerDeletion": true,
                "controller": true,
                "kind": "Job",
                "name": "pi",
                "uid": "824325a8-6d11-4160-bcd7-206fe31620c3"
            }
        ],
        "resourceVersion": "357",
        "uid": "6a2bd1d2-63b8-4580-8e30-adadacf5ae6d"
    },
    "spec": {
        "containers": [
            {
                "command": [
                    "bash",
                    "-c",
                    "sleep 2\nexit 0\n"
                ],
                "image": "howardjohn/shell",
                "imagePullPolicy": "Always",
                "name": "pi",
                "resources": {},
                "terminationMessagePath": "/dev/termination-log",
                "terminationMessagePolicy": "File",
                "volumeMounts": [
                    {
                        "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
                        "name": "kube-api-access-qlm5h",
                        "readOnly": true
                    }
                ]
            }
        ],
        "dnsPolicy": "ClusterFirst",
        "enableServiceLinks": true,
        "preemptionPolicy": "PreemptLowerPriority",
        "priority": 0,
        "restartPolicy": "Never",
        "schedulerName": "default-scheduler",
        "securityContext": {},
        "serviceAccount": "default",
        "serviceAccountName": "default",
        "terminationGracePeriodSeconds": 30,
        "tolerations": [
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/not-ready",
                "operator": "Exists",
                "tolerationSeconds": 300
            },
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/unreachable",
                "operator": "Exists",
                "tolerationSeconds": 300
            }
        ],
        "volumes": [
            {
                "name": "kube-api-access-qlm5h",
                "projected": {
                    "defaultMode": 420,
                    "sources": [
                        {
                            "serviceAccountToken": {
                                "expirationSeconds": 3607,
                                "path": "token"
                            }
                        },
                        {
                            "configMap": {
                                "items": [
                                    {
                                        "key": "ca.crt",
                                        "path": "ca.crt"
                                    }
                                ],
                                "name": "kube-root-ca.crt"
                            }
                        },
                        {
                            "downwardAPI": {
                                "items": [
                                    {
                                        "fieldRef": {
                                            "apiVersion": "v1",
                                            "fieldPath": "metadata.namespace"
                                        },
                                        "path": "namespace"
                                    }
                                ]
                            }
                        }
                    ]
                }
            }
        ]
    },
    "status": {
        "phase": "Pending",
        "qosClass": "BestEffort"
    }
}
{
    "apiVersion": "v1",
    "kind": "Pod",
    "metadata": {
        "creationTimestamp": "2024-06-06T19:34:19Z",
        "finalizers": [
            "batch.kubernetes.io/job-tracking"
        ],
        "generateName": "pi-",
        "labels": {
            "batch.kubernetes.io/controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "batch.kubernetes.io/job-name": "pi",
            "controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "job-name": "pi"
        },
        "name": "pi-72qn6",
        "namespace": "default",
        "ownerReferences": [
            {
                "apiVersion": "batch/v1",
                "blockOwnerDeletion": true,
                "controller": true,
                "kind": "Job",
                "name": "pi",
                "uid": "824325a8-6d11-4160-bcd7-206fe31620c3"
            }
        ],
        "resourceVersion": "362",
        "uid": "6a2bd1d2-63b8-4580-8e30-adadacf5ae6d"
    },
    "spec": {
        "containers": [
            {
                "command": [
                    "bash",
                    "-c",
                    "sleep 2\nexit 0\n"
                ],
                "image": "howardjohn/shell",
                "imagePullPolicy": "Always",
                "name": "pi",
                "resources": {},
                "terminationMessagePath": "/dev/termination-log",
                "terminationMessagePolicy": "File",
                "volumeMounts": [
                    {
                        "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
                        "name": "kube-api-access-qlm5h",
                        "readOnly": true
                    }
                ]
            }
        ],
        "dnsPolicy": "ClusterFirst",
        "enableServiceLinks": true,
        "preemptionPolicy": "PreemptLowerPriority",
        "priority": 0,
        "restartPolicy": "Never",
        "schedulerName": "default-scheduler",
        "securityContext": {},
        "serviceAccount": "default",
        "serviceAccountName": "default",
        "terminationGracePeriodSeconds": 30,
        "tolerations": [
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/not-ready",
                "operator": "Exists",
                "tolerationSeconds": 300
            },
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/unreachable",
                "operator": "Exists",
                "tolerationSeconds": 300
            }
        ],
        "volumes": [
            {
                "name": "kube-api-access-qlm5h",
                "projected": {
                    "defaultMode": 420,
                    "sources": [
                        {
                            "serviceAccountToken": {
                                "expirationSeconds": 3607,
                                "path": "token"
                            }
                        },
                        {
                            "configMap": {
                                "items": [
                                    {
                                        "key": "ca.crt",
                                        "path": "ca.crt"
                                    }
                                ],
                                "name": "kube-root-ca.crt"
                            }
                        },
                        {
                            "downwardAPI": {
                                "items": [
                                    {
                                        "fieldRef": {
                                            "apiVersion": "v1",
                                            "fieldPath": "metadata.namespace"
                                        },
                                        "path": "namespace"
                                    }
                                ]
                            }
                        }
                    ]
                }
            }
        ]
    },
    "status": {
        "conditions": [
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:19Z",
                "message": "0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling..",
                "reason": "Unschedulable",
                "status": "False",
                "type": "PodScheduled"
            }
        ],
        "phase": "Pending",
        "qosClass": "BestEffort"
    }
}
{
    "apiVersion": "v1",
    "kind": "Pod",
    "metadata": {
        "creationTimestamp": "2024-06-06T19:34:19Z",
        "finalizers": [
            "batch.kubernetes.io/job-tracking"
        ],
        "generateName": "pi-",
        "labels": {
            "batch.kubernetes.io/controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "batch.kubernetes.io/job-name": "pi",
            "controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "job-name": "pi"
        },
        "name": "pi-72qn6",
        "namespace": "default",
        "ownerReferences": [
            {
                "apiVersion": "batch/v1",
                "blockOwnerDeletion": true,
                "controller": true,
                "kind": "Job",
                "name": "pi",
                "uid": "824325a8-6d11-4160-bcd7-206fe31620c3"
            }
        ],
        "resourceVersion": "420",
        "uid": "6a2bd1d2-63b8-4580-8e30-adadacf5ae6d"
    },
    "spec": {
        "containers": [
            {
                "command": [
                    "bash",
                    "-c",
                    "sleep 2\nexit 0\n"
                ],
                "image": "howardjohn/shell",
                "imagePullPolicy": "Always",
                "name": "pi",
                "resources": {},
                "terminationMessagePath": "/dev/termination-log",
                "terminationMessagePolicy": "File",
                "volumeMounts": [
                    {
                        "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
                        "name": "kube-api-access-qlm5h",
                        "readOnly": true
                    }
                ]
            }
        ],
        "dnsPolicy": "ClusterFirst",
        "enableServiceLinks": true,
        "nodeName": "old-control-plane",
        "preemptionPolicy": "PreemptLowerPriority",
        "priority": 0,
        "restartPolicy": "Never",
        "schedulerName": "default-scheduler",
        "securityContext": {},
        "serviceAccount": "default",
        "serviceAccountName": "default",
        "terminationGracePeriodSeconds": 30,
        "tolerations": [
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/not-ready",
                "operator": "Exists",
                "tolerationSeconds": 300
            },
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/unreachable",
                "operator": "Exists",
                "tolerationSeconds": 300
            }
        ],
        "volumes": [
            {
                "name": "kube-api-access-qlm5h",
                "projected": {
                    "defaultMode": 420,
                    "sources": [
                        {
                            "serviceAccountToken": {
                                "expirationSeconds": 3607,
                                "path": "token"
                            }
                        },
                        {
                            "configMap": {
                                "items": [
                                    {
                                        "key": "ca.crt",
                                        "path": "ca.crt"
                                    }
                                ],
                                "name": "kube-root-ca.crt"
                            }
                        },
                        {
                            "downwardAPI": {
                                "items": [
                                    {
                                        "fieldRef": {
                                            "apiVersion": "v1",
                                            "fieldPath": "metadata.namespace"
                                        },
                                        "path": "namespace"
                                    }
                                ]
                            }
                        }
                    ]
                }
            }
        ]
    },
    "status": {
        "conditions": [
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "status": "True",
                "type": "PodScheduled"
            }
        ],
        "phase": "Pending",
        "qosClass": "BestEffort"
    }
}
{
    "apiVersion": "v1",
    "kind": "Pod",
    "metadata": {
        "creationTimestamp": "2024-06-06T19:34:19Z",
        "finalizers": [
            "batch.kubernetes.io/job-tracking"
        ],
        "generateName": "pi-",
        "labels": {
            "batch.kubernetes.io/controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "batch.kubernetes.io/job-name": "pi",
            "controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "job-name": "pi"
        },
        "name": "pi-72qn6",
        "namespace": "default",
        "ownerReferences": [
            {
                "apiVersion": "batch/v1",
                "blockOwnerDeletion": true,
                "controller": true,
                "kind": "Job",
                "name": "pi",
                "uid": "824325a8-6d11-4160-bcd7-206fe31620c3"
            }
        ],
        "resourceVersion": "431",
        "uid": "6a2bd1d2-63b8-4580-8e30-adadacf5ae6d"
    },
    "spec": {
        "containers": [
            {
                "command": [
                    "bash",
                    "-c",
                    "sleep 2\nexit 0\n"
                ],
                "image": "howardjohn/shell",
                "imagePullPolicy": "Always",
                "name": "pi",
                "resources": {},
                "terminationMessagePath": "/dev/termination-log",
                "terminationMessagePolicy": "File",
                "volumeMounts": [
                    {
                        "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
                        "name": "kube-api-access-qlm5h",
                        "readOnly": true
                    }
                ]
            }
        ],
        "dnsPolicy": "ClusterFirst",
        "enableServiceLinks": true,
        "nodeName": "old-control-plane",
        "preemptionPolicy": "PreemptLowerPriority",
        "priority": 0,
        "restartPolicy": "Never",
        "schedulerName": "default-scheduler",
        "securityContext": {},
        "serviceAccount": "default",
        "serviceAccountName": "default",
        "terminationGracePeriodSeconds": 30,
        "tolerations": [
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/not-ready",
                "operator": "Exists",
                "tolerationSeconds": 300
            },
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/unreachable",
                "operator": "Exists",
                "tolerationSeconds": 300
            }
        ],
        "volumes": [
            {
                "name": "kube-api-access-qlm5h",
                "projected": {
                    "defaultMode": 420,
                    "sources": [
                        {
                            "serviceAccountToken": {
                                "expirationSeconds": 3607,
                                "path": "token"
                            }
                        },
                        {
                            "configMap": {
                                "items": [
                                    {
                                        "key": "ca.crt",
                                        "path": "ca.crt"
                                    }
                                ],
                                "name": "kube-root-ca.crt"
                            }
                        },
                        {
                            "downwardAPI": {
                                "items": [
                                    {
                                        "fieldRef": {
                                            "apiVersion": "v1",
                                            "fieldPath": "metadata.namespace"
                                        },
                                        "path": "namespace"
                                    }
                                ]
                            }
                        }
                    ]
                }
            }
        ]
    },
    "status": {
        "conditions": [
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "status": "True",
                "type": "Initialized"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "message": "containers with unready status: [pi]",
                "reason": "ContainersNotReady",
                "status": "False",
                "type": "Ready"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "message": "containers with unready status: [pi]",
                "reason": "ContainersNotReady",
                "status": "False",
                "type": "ContainersReady"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "status": "True",
                "type": "PodScheduled"
            }
        ],
        "containerStatuses": [
            {
                "image": "howardjohn/shell",
                "imageID": "",
                "lastState": {},
                "name": "pi",
                "ready": false,
                "restartCount": 0,
                "started": false,
                "state": {
                    "waiting": {
                        "reason": "ContainerCreating"
                    }
                }
            }
        ],
        "hostIP": "172.18.0.3",
        "phase": "Pending",
        "qosClass": "BestEffort",
        "startTime": "2024-06-06T19:34:20Z"
    }
}
{
    "apiVersion": "v1",
    "kind": "Pod",
    "metadata": {
        "creationTimestamp": "2024-06-06T19:34:19Z",
        "finalizers": [
            "batch.kubernetes.io/job-tracking"
        ],
        "generateName": "pi-",
        "labels": {
            "batch.kubernetes.io/controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "batch.kubernetes.io/job-name": "pi",
            "controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "job-name": "pi"
        },
        "name": "pi-72qn6",
        "namespace": "default",
        "ownerReferences": [
            {
                "apiVersion": "batch/v1",
                "blockOwnerDeletion": true,
                "controller": true,
                "kind": "Job",
                "name": "pi",
                "uid": "824325a8-6d11-4160-bcd7-206fe31620c3"
            }
        ],
        "resourceVersion": "483",
        "uid": "6a2bd1d2-63b8-4580-8e30-adadacf5ae6d"
    },
    "spec": {
        "containers": [
            {
                "command": [
                    "bash",
                    "-c",
                    "sleep 2\nexit 0\n"
                ],
                "image": "howardjohn/shell",
                "imagePullPolicy": "Always",
                "name": "pi",
                "resources": {},
                "terminationMessagePath": "/dev/termination-log",
                "terminationMessagePolicy": "File",
                "volumeMounts": [
                    {
                        "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
                        "name": "kube-api-access-qlm5h",
                        "readOnly": true
                    }
                ]
            }
        ],
        "dnsPolicy": "ClusterFirst",
        "enableServiceLinks": true,
        "nodeName": "old-control-plane",
        "preemptionPolicy": "PreemptLowerPriority",
        "priority": 0,
        "restartPolicy": "Never",
        "schedulerName": "default-scheduler",
        "securityContext": {},
        "serviceAccount": "default",
        "serviceAccountName": "default",
        "terminationGracePeriodSeconds": 30,
        "tolerations": [
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/not-ready",
                "operator": "Exists",
                "tolerationSeconds": 300
            },
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/unreachable",
                "operator": "Exists",
                "tolerationSeconds": 300
            }
        ],
        "volumes": [
            {
                "name": "kube-api-access-qlm5h",
                "projected": {
                    "defaultMode": 420,
                    "sources": [
                        {
                            "serviceAccountToken": {
                                "expirationSeconds": 3607,
                                "path": "token"
                            }
                        },
                        {
                            "configMap": {
                                "items": [
                                    {
                                        "key": "ca.crt",
                                        "path": "ca.crt"
                                    }
                                ],
                                "name": "kube-root-ca.crt"
                            }
                        },
                        {
                            "downwardAPI": {
                                "items": [
                                    {
                                        "fieldRef": {
                                            "apiVersion": "v1",
                                            "fieldPath": "metadata.namespace"
                                        },
                                        "path": "namespace"
                                    }
                                ]
                            }
                        }
                    ]
                }
            }
        ]
    },
    "status": {
        "conditions": [
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "status": "True",
                "type": "Initialized"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:23Z",
                "status": "True",
                "type": "Ready"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:23Z",
                "status": "True",
                "type": "ContainersReady"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "status": "True",
                "type": "PodScheduled"
            }
        ],
        "containerStatuses": [
            {
                "containerID": "containerd://5fb7d18767aa7f1057590743426cd7596ccf1c623534475885c6a630c1ddd78d",
                "image": "docker.io/howardjohn/shell:latest",
                "imageID": "docker.io/howardjohn/shell@sha256:23546d7e285397394c46f44ecb503309096a24724cd1b20b841dfb49d8d6fb6f",
                "lastState": {},
                "name": "pi",
                "ready": true,
                "restartCount": 0,
                "started": true,
                "state": {
                    "running": {
                        "startedAt": "2024-06-06T19:34:22Z"
                    }
                }
            }
        ],
        "hostIP": "172.18.0.3",
        "phase": "Running",
        "podIP": "10.244.0.2",
        "podIPs": [
            {
                "ip": "10.244.0.2"
            }
        ],
        "qosClass": "BestEffort",
        "startTime": "2024-06-06T19:34:20Z"
    }
}
{
    "apiVersion": "v1",
    "kind": "Pod",
    "metadata": {
        "creationTimestamp": "2024-06-06T19:34:19Z",
        "finalizers": [
            "batch.kubernetes.io/job-tracking"
        ],
        "generateName": "pi-",
        "labels": {
            "batch.kubernetes.io/controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "batch.kubernetes.io/job-name": "pi",
            "controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "job-name": "pi"
        },
        "name": "pi-72qn6",
        "namespace": "default",
        "ownerReferences": [
            {
                "apiVersion": "batch/v1",
                "blockOwnerDeletion": true,
                "controller": true,
                "kind": "Job",
                "name": "pi",
                "uid": "824325a8-6d11-4160-bcd7-206fe31620c3"
            }
        ],
        "resourceVersion": "495",
        "uid": "6a2bd1d2-63b8-4580-8e30-adadacf5ae6d"
    },
    "spec": {
        "containers": [
            {
                "command": [
                    "bash",
                    "-c",
                    "sleep 2\nexit 0\n"
                ],
                "image": "howardjohn/shell",
                "imagePullPolicy": "Always",
                "name": "pi",
                "resources": {},
                "terminationMessagePath": "/dev/termination-log",
                "terminationMessagePolicy": "File",
                "volumeMounts": [
                    {
                        "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
                        "name": "kube-api-access-qlm5h",
                        "readOnly": true
                    }
                ]
            }
        ],
        "dnsPolicy": "ClusterFirst",
        "enableServiceLinks": true,
        "nodeName": "old-control-plane",
        "preemptionPolicy": "PreemptLowerPriority",
        "priority": 0,
        "restartPolicy": "Never",
        "schedulerName": "default-scheduler",
        "securityContext": {},
        "serviceAccount": "default",
        "serviceAccountName": "default",
        "terminationGracePeriodSeconds": 30,
        "tolerations": [
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/not-ready",
                "operator": "Exists",
                "tolerationSeconds": 300
            },
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/unreachable",
                "operator": "Exists",
                "tolerationSeconds": 300
            }
        ],
        "volumes": [
            {
                "name": "kube-api-access-qlm5h",
                "projected": {
                    "defaultMode": 420,
                    "sources": [
                        {
                            "serviceAccountToken": {
                                "expirationSeconds": 3607,
                                "path": "token"
                            }
                        },
                        {
                            "configMap": {
                                "items": [
                                    {
                                        "key": "ca.crt",
                                        "path": "ca.crt"
                                    }
                                ],
                                "name": "kube-root-ca.crt"
                            }
                        },
                        {
                            "downwardAPI": {
                                "items": [
                                    {
                                        "fieldRef": {
                                            "apiVersion": "v1",
                                            "fieldPath": "metadata.namespace"
                                        },
                                        "path": "namespace"
                                    }
                                ]
                            }
                        }
                    ]
                }
            }
        ]
    },
    "status": {
        "conditions": [
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "reason": "PodCompleted",
                "status": "True",
                "type": "Initialized"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:25Z",
                "reason": "PodCompleted",
                "status": "False",
                "type": "Ready"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:25Z",
                "reason": "PodCompleted",
                "status": "False",
                "type": "ContainersReady"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "status": "True",
                "type": "PodScheduled"
            }
        ],
        "containerStatuses": [
            {
                "containerID": "containerd://5fb7d18767aa7f1057590743426cd7596ccf1c623534475885c6a630c1ddd78d",
                "image": "docker.io/howardjohn/shell:latest",
                "imageID": "docker.io/howardjohn/shell@sha256:23546d7e285397394c46f44ecb503309096a24724cd1b20b841dfb49d8d6fb6f",
                "lastState": {},
                "name": "pi",
                "ready": false,
                "restartCount": 0,
                "started": false,
                "state": {
                    "terminated": {
                        "containerID": "containerd://5fb7d18767aa7f1057590743426cd7596ccf1c623534475885c6a630c1ddd78d",
                        "exitCode": 0,
                        "finishedAt": "2024-06-06T19:34:24Z",
                        "reason": "Completed",
                        "startedAt": "2024-06-06T19:34:22Z"
                    }
                }
            }
        ],
        "hostIP": "172.18.0.3",
        "phase": "Running",
        "podIP": "10.244.0.2",
        "podIPs": [
            {
                "ip": "10.244.0.2"
            }
        ],
        "qosClass": "BestEffort",
        "startTime": "2024-06-06T19:34:20Z"
    }
}
{
    "apiVersion": "v1",
    "kind": "Pod",
    "metadata": {
        "creationTimestamp": "2024-06-06T19:34:19Z",
        "finalizers": [
            "batch.kubernetes.io/job-tracking"
        ],
        "generateName": "pi-",
        "labels": {
            "batch.kubernetes.io/controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "batch.kubernetes.io/job-name": "pi",
            "controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "job-name": "pi"
        },
        "name": "pi-72qn6",
        "namespace": "default",
        "ownerReferences": [
            {
                "apiVersion": "batch/v1",
                "blockOwnerDeletion": true,
                "controller": true,
                "kind": "Job",
                "name": "pi",
                "uid": "824325a8-6d11-4160-bcd7-206fe31620c3"
            }
        ],
        "resourceVersion": "498",
        "uid": "6a2bd1d2-63b8-4580-8e30-adadacf5ae6d"
    },
    "spec": {
        "containers": [
            {
                "command": [
                    "bash",
                    "-c",
                    "sleep 2\nexit 0\n"
                ],
                "image": "howardjohn/shell",
                "imagePullPolicy": "Always",
                "name": "pi",
                "resources": {},
                "terminationMessagePath": "/dev/termination-log",
                "terminationMessagePolicy": "File",
                "volumeMounts": [
                    {
                        "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
                        "name": "kube-api-access-qlm5h",
                        "readOnly": true
                    }
                ]
            }
        ],
        "dnsPolicy": "ClusterFirst",
        "enableServiceLinks": true,
        "nodeName": "old-control-plane",
        "preemptionPolicy": "PreemptLowerPriority",
        "priority": 0,
        "restartPolicy": "Never",
        "schedulerName": "default-scheduler",
        "securityContext": {},
        "serviceAccount": "default",
        "serviceAccountName": "default",
        "terminationGracePeriodSeconds": 30,
        "tolerations": [
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/not-ready",
                "operator": "Exists",
                "tolerationSeconds": 300
            },
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/unreachable",
                "operator": "Exists",
                "tolerationSeconds": 300
            }
        ],
        "volumes": [
            {
                "name": "kube-api-access-qlm5h",
                "projected": {
                    "defaultMode": 420,
                    "sources": [
                        {
                            "serviceAccountToken": {
                                "expirationSeconds": 3607,
                                "path": "token"
                            }
                        },
                        {
                            "configMap": {
                                "items": [
                                    {
                                        "key": "ca.crt",
                                        "path": "ca.crt"
                                    }
                                ],
                                "name": "kube-root-ca.crt"
                            }
                        },
                        {
                            "downwardAPI": {
                                "items": [
                                    {
                                        "fieldRef": {
                                            "apiVersion": "v1",
                                            "fieldPath": "metadata.namespace"
                                        },
                                        "path": "namespace"
                                    }
                                ]
                            }
                        }
                    ]
                }
            }
        ]
    },
    "status": {
        "conditions": [
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "reason": "PodCompleted",
                "status": "True",
                "type": "Initialized"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:25Z",
                "reason": "PodCompleted",
                "status": "False",
                "type": "Ready"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:25Z",
                "reason": "PodCompleted",
                "status": "False",
                "type": "ContainersReady"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "status": "True",
                "type": "PodScheduled"
            }
        ],
        "containerStatuses": [
            {
                "containerID": "containerd://5fb7d18767aa7f1057590743426cd7596ccf1c623534475885c6a630c1ddd78d",
                "image": "docker.io/howardjohn/shell:latest",
                "imageID": "docker.io/howardjohn/shell@sha256:23546d7e285397394c46f44ecb503309096a24724cd1b20b841dfb49d8d6fb6f",
                "lastState": {},
                "name": "pi",
                "ready": false,
                "restartCount": 0,
                "started": false,
                "state": {
                    "terminated": {
                        "containerID": "containerd://5fb7d18767aa7f1057590743426cd7596ccf1c623534475885c6a630c1ddd78d",
                        "exitCode": 0,
                        "finishedAt": "2024-06-06T19:34:24Z",
                        "reason": "Completed",
                        "startedAt": "2024-06-06T19:34:22Z"
                    }
                }
            }
        ],
        "hostIP": "172.18.0.3",
        "phase": "Succeeded",
        "qosClass": "BestEffort",
        "startTime": "2024-06-06T19:34:20Z"
    }
}
{
    "apiVersion": "v1",
    "kind": "Pod",
    "metadata": {
        "creationTimestamp": "2024-06-06T19:34:19Z",
        "finalizers": [
            "batch.kubernetes.io/job-tracking"
        ],
        "generateName": "pi-",
        "labels": {
            "batch.kubernetes.io/controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "batch.kubernetes.io/job-name": "pi",
            "controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "job-name": "pi"
        },
        "name": "pi-72qn6",
        "namespace": "default",
        "ownerReferences": [
            {
                "apiVersion": "batch/v1",
                "blockOwnerDeletion": true,
                "controller": true,
                "kind": "Job",
                "name": "pi",
                "uid": "824325a8-6d11-4160-bcd7-206fe31620c3"
            }
        ],
        "resourceVersion": "501",
        "uid": "6a2bd1d2-63b8-4580-8e30-adadacf5ae6d"
    },
    "spec": {
        "containers": [
            {
                "command": [
                    "bash",
                    "-c",
                    "sleep 2\nexit 0\n"
                ],
                "image": "howardjohn/shell",
                "imagePullPolicy": "Always",
                "name": "pi",
                "resources": {},
                "terminationMessagePath": "/dev/termination-log",
                "terminationMessagePolicy": "File",
                "volumeMounts": [
                    {
                        "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
                        "name": "kube-api-access-qlm5h",
                        "readOnly": true
                    }
                ]
            }
        ],
        "dnsPolicy": "ClusterFirst",
        "enableServiceLinks": true,
        "nodeName": "old-control-plane",
        "preemptionPolicy": "PreemptLowerPriority",
        "priority": 0,
        "restartPolicy": "Never",
        "schedulerName": "default-scheduler",
        "securityContext": {},
        "serviceAccount": "default",
        "serviceAccountName": "default",
        "terminationGracePeriodSeconds": 30,
        "tolerations": [
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/not-ready",
                "operator": "Exists",
                "tolerationSeconds": 300
            },
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/unreachable",
                "operator": "Exists",
                "tolerationSeconds": 300
            }
        ],
        "volumes": [
            {
                "name": "kube-api-access-qlm5h",
                "projected": {
                    "defaultMode": 420,
                    "sources": [
                        {
                            "serviceAccountToken": {
                                "expirationSeconds": 3607,
                                "path": "token"
                            }
                        },
                        {
                            "configMap": {
                                "items": [
                                    {
                                        "key": "ca.crt",
                                        "path": "ca.crt"
                                    }
                                ],
                                "name": "kube-root-ca.crt"
                            }
                        },
                        {
                            "downwardAPI": {
                                "items": [
                                    {
                                        "fieldRef": {
                                            "apiVersion": "v1",
                                            "fieldPath": "metadata.namespace"
                                        },
                                        "path": "namespace"
                                    }
                                ]
                            }
                        }
                    ]
                }
            }
        ]
    },
    "status": {
        "conditions": [
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "reason": "PodCompleted",
                "status": "True",
                "type": "Initialized"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:25Z",
                "reason": "PodCompleted",
                "status": "False",
                "type": "Ready"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:25Z",
                "reason": "PodCompleted",
                "status": "False",
                "type": "ContainersReady"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "status": "True",
                "type": "PodScheduled"
            }
        ],
        "containerStatuses": [
            {
                "containerID": "containerd://5fb7d18767aa7f1057590743426cd7596ccf1c623534475885c6a630c1ddd78d",
                "image": "docker.io/howardjohn/shell:latest",
                "imageID": "docker.io/howardjohn/shell@sha256:23546d7e285397394c46f44ecb503309096a24724cd1b20b841dfb49d8d6fb6f",
                "lastState": {},
                "name": "pi",
                "ready": false,
                "restartCount": 0,
                "started": false,
                "state": {
                    "terminated": {
                        "containerID": "containerd://5fb7d18767aa7f1057590743426cd7596ccf1c623534475885c6a630c1ddd78d",
                        "exitCode": 0,
                        "finishedAt": "2024-06-06T19:34:24Z",
                        "reason": "Completed",
                        "startedAt": "2024-06-06T19:34:22Z"
                    }
                }
            }
        ],
        "hostIP": "172.18.0.3",
        "phase": "Succeeded",
        "podIP": "10.244.0.2",
        "podIPs": [
            {
                "ip": "10.244.0.2"
            }
        ],
        "qosClass": "BestEffort",
        "startTime": "2024-06-06T19:34:20Z"
    }
}
{
    "apiVersion": "v1",
    "kind": "Pod",
    "metadata": {
        "creationTimestamp": "2024-06-06T19:34:19Z",
        "generateName": "pi-",
        "labels": {
            "batch.kubernetes.io/controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "batch.kubernetes.io/job-name": "pi",
            "controller-uid": "824325a8-6d11-4160-bcd7-206fe31620c3",
            "job-name": "pi"
        },
        "name": "pi-72qn6",
        "namespace": "default",
        "ownerReferences": [
            {
                "apiVersion": "batch/v1",
                "blockOwnerDeletion": true,
                "controller": true,
                "kind": "Job",
                "name": "pi",
                "uid": "824325a8-6d11-4160-bcd7-206fe31620c3"
            }
        ],
        "resourceVersion": "503",
        "uid": "6a2bd1d2-63b8-4580-8e30-adadacf5ae6d"
    },
    "spec": {
        "containers": [
            {
                "command": [
                    "bash",
                    "-c",
                    "sleep 2\nexit 0\n"
                ],
                "image": "howardjohn/shell",
                "imagePullPolicy": "Always",
                "name": "pi",
                "resources": {},
                "terminationMessagePath": "/dev/termination-log",
                "terminationMessagePolicy": "File",
                "volumeMounts": [
                    {
                        "mountPath": "/var/run/secrets/kubernetes.io/serviceaccount",
                        "name": "kube-api-access-qlm5h",
                        "readOnly": true
                    }
                ]
            }
        ],
        "dnsPolicy": "ClusterFirst",
        "enableServiceLinks": true,
        "nodeName": "old-control-plane",
        "preemptionPolicy": "PreemptLowerPriority",
        "priority": 0,
        "restartPolicy": "Never",
        "schedulerName": "default-scheduler",
        "securityContext": {},
        "serviceAccount": "default",
        "serviceAccountName": "default",
        "terminationGracePeriodSeconds": 30,
        "tolerations": [
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/not-ready",
                "operator": "Exists",
                "tolerationSeconds": 300
            },
            {
                "effect": "NoExecute",
                "key": "node.kubernetes.io/unreachable",
                "operator": "Exists",
                "tolerationSeconds": 300
            }
        ],
        "volumes": [
            {
                "name": "kube-api-access-qlm5h",
                "projected": {
                    "defaultMode": 420,
                    "sources": [
                        {
                            "serviceAccountToken": {
                                "expirationSeconds": 3607,
                                "path": "token"
                            }
                        },
                        {
                            "configMap": {
                                "items": [
                                    {
                                        "key": "ca.crt",
                                        "path": "ca.crt"
                                    }
                                ],
                                "name": "kube-root-ca.crt"
                            }
                        },
                        {
                            "downwardAPI": {
                                "items": [
                                    {
                                        "fieldRef": {
                                            "apiVersion": "v1",
                                            "fieldPath": "metadata.namespace"
                                        },
                                        "path": "namespace"
                                    }
                                ]
                            }
                        }
                    ]
                }
            }
        ]
    },
    "status": {
        "conditions": [
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "reason": "PodCompleted",
                "status": "True",
                "type": "Initialized"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:25Z",
                "reason": "PodCompleted",
                "status": "False",
                "type": "Ready"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:25Z",
                "reason": "PodCompleted",
                "status": "False",
                "type": "ContainersReady"
            },
            {
                "lastProbeTime": null,
                "lastTransitionTime": "2024-06-06T19:34:20Z",
                "status": "True",
                "type": "PodScheduled"
            }
        ],
        "containerStatuses": [
            {
                "containerID": "containerd://5fb7d18767aa7f1057590743426cd7596ccf1c623534475885c6a630c1ddd78d",
                "image": "docker.io/howardjohn/shell:latest",
                "imageID": "docker.io/howardjohn/shell@sha256:23546d7e285397394c46f44ecb503309096a24724cd1b20b841dfb49d8d6fb6f",
                "lastState": {},
                "name": "pi",
                "ready": false,
                "restartCount": 0,
                "started": false,
                "state": {
                    "terminated": {
                        "containerID": "containerd://5fb7d18767aa7f1057590743426cd7596ccf1c623534475885c6a630c1ddd78d",
                        "exitCode": 0,
                        "finishedAt": "2024-06-06T19:34:24Z",
                        "reason": "Completed",
                        "startedAt": "2024-06-06T19:34:22Z"
                    }
                }
            }
        ],
        "hostIP": "172.18.0.3",
        "phase": "Succeeded",
        "podIP": "10.244.0.2",
        "podIPs": [
            {
                "ip": "10.244.0.2"
            }
        ],
        "qosClass": "BestEffort",
        "startTime": "2024-06-06T19:34:20Z"
    }
}
```

</details>

Prior discussion https://github.com/kubernetes/kubernetes/issues/82268

#### Kubernetes version

many, see above

#### Cloud provider

Tested on kind

#### OS version

Linux

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125354 endpointslicemirroring controller not create endpointslice when the endpoints are recreate

- Issue é“¾æ¥ï¼š[#125354](https://github.com/kubernetes/kubernetes/issues/125354)

### Issue å†…å®¹

#### What happened?

* create a service without selector, and manual create endpoints for this service
* endpointslicemirroring controller will create an endpointslice for this service.
* after endpoint slice created, and kube-controller-manager restart.
* during the kube-controller-manager restart, the endpoint was deleted and recreated
* after kube-controller-manager restart,  garbagecollector controller delete this endpointslice and endpointslice not recreate until endpoint/service update event or kube-controller-manager restart

#### What did you expect to happen?

endpointslice will be recreated

#### How can we reproduce it (as minimally and precisely as possible)?

* create a service without selector, and manual create endpoints for this service
* endpointslicemirroring controller will create an endpointslice for this service.
* after endpoint slice created, and kube-controller-manager restart.
* during the kube-controller-manager restart, the endpoint was deleted and recreated


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
v 1.30.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
kind 1.30.0
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125339 readinessProbe not failing if command is not installed

- Issue é“¾æ¥ï¼š[#125339](https://github.com/kubernetes/kubernetes/issues/125339)

### Issue å†…å®¹

#### What happened?

I have the following readinessProbe in my .yaml file:
```
readinessProbe:
  exec:
    command:
      - /bin/sh
      - -c
      - >
        curl -s http://127.0.0.1:8008/metrics |
        awk '!/^#/ && /^libp2p_gossipsub_healthy_peers_topics /{
          print "Found gossipsub:", $0;
          if ($2 == 1.0) {
            exit 0;  # success, healthy state
          } else {
            exit 1;  # failure, unhealthy state
          }
        }'
  successThreshold: 5
  initialDelaySeconds: 5
  periodSeconds: 1
  failureThreshold: 2
  timeoutSeconds: 5
```

The pod only has one container, and this container doesn't have `curl` installed. Still, the readinessProbe is making the pod as Ready.

I then manually entered in the container, and installed `curl` by hand. 1 second after that, the POD was marked as not ready.

If I don't manually install `curl`, and I run the same command, I got an error, as curl is not installed.

#### What did you expect to happen?

The POD should not be marked as ready.

#### How can we reproduce it (as minimally and precisely as possible)?

I edited my yaml to be reproducible and reach to the same issue.
```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: nodes
  namespace: zerotesting
spec:
  replicas: 1
  podManagementPolicy: "Parallel"
  selector:
    matchLabels:
      app: zerotenkay
  template:
    metadata:
      labels:
        app: zerotenkay
    spec:
      containers:
        - name: test
          image: alpine:3.18
          imagePullPolicy: IfNotPresent
          readinessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - >
                  curl -s http://127.0.0.1:8008/metrics |
                  awk 'asd'
            successThreshold: 5
            initialDelaySeconds: 5
            periodSeconds: 1
            failureThreshold: 2
            timeoutSeconds: 5
          resources:
            requests:
              memory: "64Mi"
              cpu: "150m"
            limits:
              memory: "600Mi"
              cpu: "400m"
          command:
            - sh
            - -c
            - sleep 99999
```

If I run this, I still can see that the pod is marked as Ready, even curl is not installed.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
 v1.28.9+k3s1
```

</details>


#### Cloud provider

<details>
Own deployment
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125336 volume-binding scheduler prefilter assumes that a node's metadata.name == metadata.labels["kubernetes.io/hostname"]

- Issue é“¾æ¥ï¼š[#125336](https://github.com/kubernetes/kubernetes/issues/125336)

### Issue å†…å®¹

#### What happened?

Running on a system which has node names that look like FQDNs, but hostname labels which are unqualified.

The local path PV provisioner has (correctly) added nodeAffinity constraints to the PV that reference a node's `hostname` label.

A replacement pod for a statefulset that has a bound PVC cannot be re-scheduled, because the scheduler interprets  `PreFilterResult.NodeNames` as node *names*, but the code in volume_binding.go that runs the prefilter collects a set of *kubeternetes.io/hostname label values*.

#### What did you expect to happen?

Pod rescheduling should not wedge. The volume-binding scheduler plugin should resolve match constraints to a set of nodes and return their node names in its PreFilterResult.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a node with distinct name and hostname label [k8s documentation reiterates that this situation is possible]. Schedule a pod onto it with a local path PV bound. Observe the PV has a nodeAffinity constraint that contains the node's hostname label. Attempt to reschedule a pod to use this PV.

Precise behaviour may vary from 1.27 (which introduced this prefilter notion) forwards. On 1.27, the scheduler failes with a "nodeinfo not found". A workaround was backported into the prefilter loop of `schedulePod` but AFAICT the root cause was never identified. Later versions look to end up filtering out all nodes in `schedulePod` - but the root cause is the same in both cases.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.11", GitCommit:"b9e2ad67ad146db566be5a6db140d47e52c8adb2", GitTreeState:"clean", BuildDate:"2024-02-14T10:40:40Z", GoVersion:"go1.21.7", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"27+", GitVersion:"v1.27.13-eks-3af4770", GitCommit:"4873544ec1ec7d3713084677caa6cf51f3b1ca6f", GitTreeState:"clean", BuildDate:"2024-04-30T03:31:44Z", GoVersion:"go1.21.9", Compiler:"gc", Platform:"linux/amd64"}
```

</details>

The nodes in question were older ubuntu EKS images, but that's largely irrelevant; the critical point is that nodes are registered by kubelet with a FQDN name but a short hostname. (AFAICT newer ubuntu EKS images will mask this behaviour by setting both of these to the same value, but the same erroneous assumption is baked into volume-binding still.)

#### Cloud provider

<details>
EKS, 1.27 (at present)
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
public.ecr.aws/ebs-csi-driver/aws-ebs-csi-driver:v1.8.0
</details>

but the PV record that it creates is (IMO) correct; the matchExpression attempts to identify a node by its hostname label.

### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125332 Pod stuck at Running state due to unexpected skip of taint manager work

- Issue é“¾æ¥ï¼š[#125332](https://github.com/kubernetes/kubernetes/issues/125332)

### Issue å†…å®¹

#### What happened?

1ã€We shutdown the worker node that our pods is running on
2ã€A pod stays in Running state and never change again



#### What did you expect to happen?

We expected the pod to be update to Terminating.

#### How can we reproduce it (as minimally and precisely as possible)?

It is unlikely to be reproduced since we found the 'only possible' cause to be of ridiculously low probability.

#### Anything else we need to know?

The pod is defined with tolerations as below, but doesn't seem to be a problem:
```
tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 2
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 2 
```
Strange log sequence was found in controller-manager's log
```
I0604 15:28:49.239218       1 timed_workers.go:113] Adding TimedWorkerQueue item dpa/infrastructure-0 at 2024-06-04 15:28:49.239118259 +0800 CST m=+252387.993120493 to be fired at 2024-06-04 15:28:51.239118259 +0800 CST m=+252389.993120493
......
I0604 15:28:51.240319       1 taint_manager.go:106] "NoExecuteTaintManager is deleting pod" pod="dpa/infrastructure-0"
I0604 15:28:51.259631       1 taint_manager.go:416] "Noticed pod deletion" pod="dpa/infrastructure-0"
I0604 15:28:51.259642       1 timed_workers.go:132] Cancelling TimedWorkerQueue item dpa/infrastructure-0 at 2024-06-04 15:28:51.259639403 +0800 CST m=+252390.013641637
......
I0604 17:38:00.725378       1 timed_workers.go:113] Adding TimedWorkerQueue item dpa/infrastructure-0 at 2024-06-04 17:38:00.725348125 +0800 CST m=+260139.479350359 to be fired at 2024-06-04 17:38:02.725348125 +0800 CST m=+260141.479350359
W0604 17:38:00.725393       1 timed_workers.go:118] Trying to add already existing work for &{NamespacedName:dpa/infrastructure-0}. Skipping.
......
```
We noticed that we had restarted the node once at arround 15:28, and that trigger the eviction of pod dpa/infrastructure-0 as expected. But when we shutdown the node again at arround 17:38, the taint manager skipped the work due to the same key found in workers map.
The controller manager was never restarted or changed leader.
The log is ridiculous, but after look at the code, we did find a way that a key stays in workers map after Cancelling:
```
func (q *TimedWorkerQueue) getWrappedWorkerFunc(key string) func(ctx context.Context, args *WorkArgs) error {
	return func(ctx context.Context, args *WorkArgs) error {
		err := q.workFunc(ctx, args)
		q.Lock()
		defer q.Unlock()
		if err == nil {
			// To avoid duplicated calls we keep the key in the queue, to prevent
			// subsequent additions.
			q.workers[key] = nil
		} else {
			delete(q.workers, key)
		}
		return err
	}
}
```
In this part of code generates a WorkerFunc that would be called in a work, taint manager will run q.workFunc to delete pod from apiserver, which would trigger a podUpdate and lead to CancelWork:
```
func (q *TimedWorkerQueue) CancelWork(key string) bool {
	q.Lock()
	defer q.Unlock()
	worker, found := q.workers[key]
	result := false
	if found {
		klog.V(4).Infof("Cancelling TimedWorkerQueue item %v at %v", key, time.Now())
		if worker != nil {
			result = true
			worker.Cancel()
		}
		delete(q.workers, key)
	}
	return result
}
```
If and only if CancelWork acquired the lock before WorkerFunc, the `q.workers[key]` won't be deleted as expected, but `q.workers[key] = nil` instead, and lead to later skip of work.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"25+", GitVersion:"v1.25.3+77d8f59014e1f3-dirty", GitCommit:"77d8f59014e1f3ca907c1b4a4f57900539b88dc8", GitTreeState:"dirty", BuildDate:"2024-04-29T13:26:15Z", GoVersion:"go1.19.6", Compiler:"gc", Platform:"linux/arm64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"25+", GitVersion:"v1.25.3+77d8f59014e1f3-dirty", GitCommit:"77d8f59014e1f3ca907c1b4a4f57900539b88dc8", GitTreeState:"dirty", BuildDate:"2024-04-29T13:23:44Z", GoVersion:"go1.19.6", Compiler:"gc", Platform:"linux/arm64"}
```

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125331 the node is in the notready state while the pod is still running 1/1. Node dynamic.7.220.110.21 is healthy again, removing all taints, but not update ready status

- Issue é“¾æ¥ï¼š[#125331](https://github.com/kubernetes/kubernetes/issues/125331)

### Issue å†…å®¹

#### What happened?

node dynamic.7.220.110.21 not go into `case currentReadyCondition.Status != v1.ConditionTrue && observedReadyCondition.Status == v1.ConditionTrue:` and fallthrough -> MarkPodsNotReady.

**As a result, the node is in the notready state while the pod is still running 1/1.**

![image](https://github.com/kubernetes/kubernetes/assets/17514799/c4265bce-a983-4f42-9362-d11036b432f0)

---------
**log is below**:  **I notice `NodeCondition{Type:Ready,Status:Unknown`  is to `NodeCondition{Type:Ready,Status:False`. 

I0528 07:44:10.430657 9 node_lifecycle_controller.go:1176] node dynamic.7.220.110.21 hasn't been updated for 40.125415326s. Last Ready is: &NodeCondition{Type:Ready,Status:False,LastHeartbeatTime:2024-05-28 07:43:20 +0000 UTC,LastTransitionTime:2024-05-28 07:43:20 +0000 UTC,Reason:KubeletNotReady,Message:container runtime not ready: RuntimeReady=false reason:DockerDaemonNotReady message:docker: failed to get docker version: failed to get docker version: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?,}

i guess the step is: 
1ã€kubelet post False state because docker is kill  
2ã€kill kubelet before the pod status is reported.
3ã€now pod status stuck in running 1/1 and never become 0/1
----------

I0527 14:24:51.746645       9 node_lifecycle_controller.go:1176] node dynamic.7.220.110.21 hasn't been updated for 2m47.996844161s. Last Ready is: &**NodeCondition{Type:Ready,Status:Unknown**,LastHeartbeatTime:2024-05-27 14:22:00 +0000 UTC,LastTransitionTime:2024-05-27 14:23:05 +0000 UTC,Reason:NodeStatusNeverUpdated,Message:Kubelet never posted node status.,}
I0527 14:24:51.746667       9 node_lifecycle_controller.go:1176] node dynamic.7.220.110.21 hasn't been updated for 2m47.99686763s. Last MemoryPressure is: &NodeCondition{Type:MemoryPressure,Status:Unknown,LastHeartbeatTime:2024-05-27 14:22:00 +0000 UTC,LastTransitionTime:2024-05-27 14:23:05 +0000 UTC,Reason:NodeStatusNeverUpdated,Message:Kubelet never posted node status.,}
I0527 14:24:51.746686       9 node_lifecycle_controller.go:1176] node dynamic.7.220.110.21 hasn't been updated for 2m47.996886936s. Last DiskPressure is: &NodeCondition{Type:DiskPressure,Status:Unknown,LastHeartbeatTime:2024-05-27 14:22:00 +0000 UTC,LastTransitionTime:2024-05-27 14:23:05 +0000 UTC,Reason:NodeStatusNeverUpdated,Message:Kubelet never posted node status.,}
I0527 14:24:51.746704       9 node_lifecycle_controller.go:1176] node dynamic.7.220.110.21 hasn't been updated for 2m47.996904333s. Last PIDPressure is: &NodeCondition{Type:PIDPressure,Status:Unknown,LastHeartbeatTime:2024-05-27 14:22:00 +0000 UTC,LastTransitionTime:2024-05-27 14:23:05 +0000 UTC,Reason:NodeStatusNeverUpdated,Message:Kubelet never posted node status.,}
I0527 14:24:51.746744       9 node_lifecycle_controller.go:960] Node dynamic.7.220.110.21 is unresponsive as of 2024-05-27 14:24:51.746735259 +0000 UTC m=+187413.002563875. Adding it to the Taint queue.
I0527 14:24:53.451556       9 controller_utils.go:120] "Update ready status of pods on node" node="dynamic.7.220.110.21"
I0527 14:24:56.798746       9 node_lifecycle_controller.go:971] **Node dynamic.7.220.110.21 is healthy again, removing all taints**
I0528 07:43:25.301189       9 node_lifecycle_controller.go:947] **Node dynamic.7.220.110.21 is NotReady as of 2024-05-28** 07:43:25.301181507 +0000 UTC m=+249725.057044658. Adding it to the Taint queue.
I0528 07:44:10.430657       9 node_lifecycle_controller.go:1176] node dynamic.7.220.110.21 hasn't been updated for 40.125415326s. Last Ready is: &NodeCondition{**Type:Ready,Status:False**,LastHeartbeatTime:2024-05-28 07:43:20 +0000 UTC,LastTransitionTime:2024-05-28 07:43:20 +0000 UTC,Reason:KubeletNotReady,Message:container runtime not ready: RuntimeReady=false reason:DockerDaemonNotReady message:docker: failed to get docker version: failed to get docker version: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?,}
I0528 07:44:10.430692       9 node_lifecycle_controller.go:1176] node dynamic.7.220.110.21 hasn't been updated for 40.125451926s. Last MemoryPressure is: &NodeCondition{Type:MemoryPressure,Status:False,LastHeartbeatTime:2024-05-28 07:43:20 +0000 UTC,LastTransitionTime:2024-05-27 14:24:53 +0000 UTC,Reason:KubeletHasSufficientMemory,Message:kubelet has sufficient memory available,}
I0528 07:44:10.430723       9 node_lifecycle_controller.go:1176] node dynamic.7.220.110.21 hasn't been updated for 40.125483489s. Last DiskPressure is: &NodeCondition{Type:DiskPressure,Status:False,LastHeartbeatTime:2024-05-28 07:43:20 +0000 UTC,LastTransitionTime:2024-05-27 14:24:53 +0000 UTC,Reason:KubeletHasNoDiskPressure,Message:kubelet has no disk pressure,}
I0528 07:44:10.430743       9 node_lifecycle_controller.go:1176] node dynamic.7.220.110.21 hasn't been updated for 40.125503499s. Last PIDPressure is: &NodeCondition{Type:PIDPressure,Status:False,LastHeartbeatTime:2024-05-28 07:43:20 +0000 UTC,LastTransitionTime:2024-05-27 14:24:53 +0000 UTC,Reason:KubeletHasSufficientPID,Message:kubelet has sufficient PID available,}


#### What did you expect to happen?

the node is in the notready state while the pod is should be running 0/1.

#### How can we reproduce it (as minimally and precisely as possible)?

see below

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125315 Kubernetes 1.30 and containerd 1.7.16 onpremise setup, pods networking is not working containerd status is failing

- Issue é“¾æ¥ï¼š[#125315](https://github.com/kubernetes/kubernetes/issues/125315)

### Issue å†…å®¹

#### What happened?

I installed containerd 1.7.16 and kubernetes 1.30.0 on RHEL centos machines.The pods are unable to run.Multiple issues are noticed.
1. apiserver, coredns, controller-manager, scheduler restarted 22 times
2. Pod networking is failing(redis nodes unable to join)
3. containerd and kubelet status is showing error while fetching containerID
4. Kubectl get events display "invalid 0 image filesystem capacity"
5. Disk utiliation on one worker node goes to 84% always(with no pods)

#### What did you expect to happen?

Perfectly working cluster with
1. image and node filesystem to automatically garbage collect with default configurations
2. Able to use runc container runtime and schedule pods
3. pod networking
4. use private registry and local images

#### How can we reproduce it (as minimally and precisely as possible)?

1. Downloaded RPM'S
![image](https://github.com/kubernetes/kubernetes/assets/43172017/3bead595-7d66-4a82-8357-9b31331923a2)
Skipped rpms(glibc, iptable,iproute,systemd,system-libs,utils-linux ) as Faced conflicts while installing rpms as slightly lower versions are already present in system.
2. 1.7.16 containerd,1.4.1 CNI plugin without package manager, Containerd , stage directory (/data/containers and /data/containerd) Consumption is going to 84 percent with no pods running(state directory not GC automatically)
3. installed cluster using kubeadm 
4. CNI flannel Deployed

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
![image](https://github.com/kubernetes/kubernetes/assets/43172017/a262508f-41d5-401c-894f-f873ea959fcf)

</details>


#### Cloud provider

<details>
ON-PREMISE CENTOS RHEL 8.6
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
![image](https://github.com/kubernetes/kubernetes/assets/43172017/ab675ca7-991f-42f8-a6d4-0cfda38a6805)

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125312 False "v1 Binding is deprecated in v1.6+" warning for pods/bindings sub-resource

- Issue é“¾æ¥ï¼š[#125312](https://github.com/kubernetes/kubernetes/issues/125312)

### Issue å†…å®¹

#### What happened?

https://github.com/kubernetes/kubernetes/commit/8d45bbea2b464e856ddcfe3f6ee410ddea0cee32 (new for 1.31.0 alpha 1) added this:
https://github.com/kubernetes/kubernetes/blob/ae5543e4c8f99cb1555102a8ebc310aed3c82596/staging/src/k8s.io/api/core/v1/zz_generated.prerelease-lifecycle.go#L30-L34

For some reason, this code ends up checking the `Binding` type when registering the action for the `"namespaces/{namespace}/pods/{name}/binding"` sub-resource:
https://github.com/kubernetes/kubernetes/blob/ae5543e4c8f99cb1555102a8ebc310aed3c82596/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go#L767-L776

cc @liggitt as the author of that code

As a result, the Kubernetes scheduler gets a warning from the apiserver when it calls `err := b.handle.ClientSet().CoreV1().Pods(binding.Namespace).Bind(ctx, binding, metav1.CreateOptions{})`:

```
W0604 11:50:04.854792  276440 warnings.go:70] v1 Binding is deprecated in v1.6+
```

#### What did you expect to happen?

No warning because the sub-resource is not deprecated.

#### How can we reproduce it (as minimally and precisely as possible)?

```console
$ dlv test ./test/integration/scheduler_perf/ -- -test.run=TestScheduling/SchedulingWithResourceClaimTemplateStructured/fast
...

b installer.go:745
cond 1 action.Path == "namespaces/{namespace}/pods/{name}/binding"
```

#### Anything else we need to know?

Did not happen in 1.30.0.


#### Kubernetes version

master, shortly before v1.31.0 alpha 1.


#### Cloud provider

n/a

#### OS version

_No response_

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125285 Named ports in initContainer sidecars do not work with NetworkPolicies

- Issue é“¾æ¥ï¼š[#125285](https://github.com/kubernetes/kubernetes/issues/125285)

### Issue å†…å®¹

#### What happened?

We have recently migrated our traditional sidecar definitions to the new recommended initContainer sidecars.

We are running a CockroachDB StatefulSet that has a Vault sidecar, such that members 0 and 1 have a "legacy" sidecar container:

```console
$ kubectl --context prod-aws -n partner-registration get -o json pod cockroachdb-0 | jq -r '.spec.containers[] | select(.name == "vault-credentials-agent") | .ports'
[
  {
    "containerPort": 8099,
    "name": "metrics",
    "protocol": "TCP"
  }
]

$ kubectl --context prod-aws -n partner-registration get -o json pod cockroachdb-1 | jq -r '.spec.containers[] | select(.name == "vault-credentials-agent") | .ports'
[
  {
    "containerPort": 8099,
    "name": "metrics",
    "protocol": "TCP"
  }
]
```

And member 2 has a new initContainer style sidecar:

```console
$ kubectl --context prod-aws -n partner-registration get -o json pod cockroachdb-2 | jq -r '.spec.initContainers[] | select(.name == "vault-credentials-agent") | .ports[]'
{
  "containerPort": 8099,
  "name": "metrics",
  "protocol": "TCP"
}
```

They all expose a named port `metrics`.

We also have a NetworkPolicy in this namespace such that:

```console
$ kubectl --context prod-aws -n partner-registration get netpol private-ingress-sys-prom -o yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"networking.k8s.io/v1","kind":"NetworkPolicy","metadata":{"annotations":{},"name":"private-ingress-sys-prom","namespace":"partner-registration"},"spec":{"ingress":[{"from":[{"namespaceSelector":{"matchLabels":{"name":"sys-prom"}}}],"ports":[{"port":"metrics"}]}],"podSelector":{},"policyTypes":["Ingress"]}}
  creationTimestamp: "2021-03-26T10:52:05Z"
  generation: 3
  name: private-ingress-sys-prom
  namespace: partner-registration
  resourceVersion: "3454111968"
  uid: 26a53b37-53e3-4a8d-92dd-8e17836b087d
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: sys-prom
    ports:
    - port: metrics
      protocol: TCP
  podSelector: {}
  policyTypes:
  - Ingress
```

Testing the connection from Prometheus I observe the following results for member 0 and 1:

```console
$ kubectl --context prod-aws -n sys-prom exec -ti prometheus-system-0 -c prometheus -- wget -T2 -O - http://10.2.142.58:8099/__/metrics | head -4
Connecting to 10.2.142.58:8099 (10.2.142.58:8099)
writing to stdout
# HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles.
# TYPE go_gc_duration_seconds summary

$ kubectl --context prod-aws -n sys-prom exec -ti prometheus-system-0 -c prometheus -- wget -T2 -O - http://10.2.13.56:8099/__/metrics | head -4
Connecting to 10.2.13.56:8099 (10.2.13.56:8099)
writing to stdout
# HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles.
# TYPE go_gc_duration_seconds summary
```

And a timeout for member 2 (running initContainer sidecar):

```console
$ kubectl --context prod-aws -n sys-prom exec -ti prometheus-system-0 -c prometheus -- wget -T2 -O - http://10.2.2.53:8099/__/metrics | head -4
Connecting to 10.2.2.53:8099 (10.2.2.53:8099)
wget: download timed out
command terminated with exit code 1
```

If I apply the following patch:

```patch
diff --git a/prod-aws/partner-registration/02-network-policies.yaml b/prod-aws/partner-registration/02-network-policies.yaml
index 9d3ea45d86..c685cc8667 100644
--- a/prod-aws/partner-registration/02-network-policies.yaml
+++ b/prod-aws/partner-registration/02-network-policies.yaml
@@ -43,7 +43,7 @@ spec:
             matchLabels:
               name: sys-prom
       ports:
-        - port: metrics
+        - port: 8099
 ---
 #  Allow private Ingress
 ---
```

Then I'm able to establish a connection with cockroachdb-2:

```console
$ kubectl --context prod-aws -n sys-prom exec -ti prometheus-system-0 -c prometheus -- wget -T2 -O - http://10.2.2.53:8099/__/metrics | head -4
Connecting to 10.2.2.53:8099 (10.2.2.53:8099)
writing to stdout
# HELP go_gc_duration_seconds A summary of the pause duration of garbage collection cycles.
# TYPE go_gc_duration_seconds summary
```

This is the Kyverno config that injects our sidecars the describes the whole config of the injected initContainer sidecar: https://github.com/utilitywarehouse/system-manifests/blob/master/kyverno/policies/pods/injectSidecar.yaml#L40-L88


#### What did you expect to happen?

named port in the initContainer spec allows the traffic if it matches config of the NetworkPolicy.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a StatefulSet with an initContainer sidecar that specifies a named port. Create a NetworkPolicy allowing access to the named port. You should observe timeouts trying to reach that port.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl --context prod-aws version
Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.0
```

</details>


#### Cloud provider

<details>
AWS EC2 running our own control plane.
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Flatcar Container Linux by Kinvolk"
ID=flatcar
ID_LIKE=coreos
VERSION=3815.2.0
VERSION_ID=3815.2.0
BUILD_ID=2024-02-12-2039
SYSEXT_LEVEL=1.0
PRETTY_NAME="Flatcar Container Linux by Kinvolk 3815.2.0 (Oklo)"
ANSI_COLOR="38;5;75"
HOME_URL="https://flatcar.org/"
BUG_REPORT_URL="https://issues.flatcar.org"
FLATCAR_BOARD="amd64-usr"
CPE_NAME="cpe:2.3:o:flatcar-linux:flatcar_linux:3815.2.0:*:*:*:*:*:*:*"

$ uname -a
Linux ip-10-44-18-96 6.1.77-flatcar #1 SMP PREEMPT_DYNAMIC Mon Feb 12 19:37:08 -00 2024 x86_64 AMD EPYC 7571 AuthenticAMD GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
Terraform / Ignition.
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
CNI: Calico v3.27.3
</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125283 The connection to the server Master_IP:6443 was refused - did you specify the right host or port?

- Issue é“¾æ¥ï¼š[#125283](https://github.com/kubernetes/kubernetes/issues/125283)

### Issue å†…å®¹

#### What happened?

Hello All, 

I am setting up Kubernetes in my VMs for some testing, here is the info - 


Below CLI is used to initialize --> kubeadm init --pod-network-cidr=10.10.0.0/16 --apiserver-advertise-address=Master_IP --cri-socket /run/containerd/containerd.sock

kubectl version
Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3



ot-master1 $ cat  /etc/containerd/config.toml
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
  [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
    SystemdCgroup = true
ot-master1 $


Whenever I try to execute kubectl CLI, it throws - The connection to the server Master_IP:6443 was refused - did you specify the right host or port?
there is No firewall/ufw enabled in the system and not sure why there' are lots of rules updated once the kubeadm is initialized

and to make it work I have set to reset the iptables, and trying to clear them out, Not sure what to do!

If I performed the steps below for all the workers and master then I can connect to API\ then after a few secs it blocked the API 

iptables -L
iptables -F
iptables -X
iptables -P INPUT ACCEPT
iptables -P OUTPUT ACCEPT
iptables -P FORWARD ACCEPT
iptables -L


#### What did you expect to happen?

kubectl get nodes -v=10
I0601 11:38:36.649526  795967 loader.go:395] Config loaded from file:  /root/.kube/config
I0601 11:38:36.650037  795967 round_trippers.go:466] curl -v -XGET  -H "Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json" -H "User-Agent: kubectl/v1.30.1 (linux/amd64) kubernetes/6911225" 'https://Master_IP:6443/api?timeout=32s'
I0601 11:38:36.650302  795967 round_trippers.go:508] HTTP Trace: Dial to tcp:Master_IP:6443 failed: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.650330  795967 round_trippers.go:553] GET https://Master_IP:6443/api?timeout=32s  in 0 milliseconds
I0601 11:38:36.650342  795967 round_trippers.go:570] HTTP Statistics: DNSLookup 0 ms Dial 0 ms TLSHandshake 0 ms Duration 0 ms
I0601 11:38:36.650352  795967 round_trippers.go:577] Response Headers:
E0601 11:38:36.650401  795967 memcache.go:265] couldn't get current server API group list: Get https://Master_IP:6443/api?timeout=32s: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.650414  795967 cached_discovery.go:120] skipped caching discovery info due to Get https://Master_IP:6443/api?timeout=32s: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.650497  795967 round_trippers.go:466] curl -v -XGET  -H "Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json" -H "User-Agent: kubectl/v1.30.1 (linux/amd64) kubernetes/6911225" 'https://Master_IP:6443/api?timeout=32s'
I0601 11:38:36.650676  795967 round_trippers.go:508] HTTP Trace: Dial to tcp:Master_IP:6443 failed: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.650714  795967 round_trippers.go:553] GET https://Master_IP:6443/api?timeout=32s  in 0 milliseconds
I0601 11:38:36.650727  795967 round_trippers.go:570] HTTP Statistics: DNSLookup 0 ms Dial 0 ms TLSHandshake 0 ms Duration 0 ms
I0601 11:38:36.650738  795967 round_trippers.go:577] Response Headers:
E0601 11:38:36.650773  795967 memcache.go:265] couldn't get current server API group list: Get https://Master_IP:6443/api?timeout=32s: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.651887  795967 cached_discovery.go:120] skipped caching discovery info due to Get https://Master_IP:6443/api?timeout=32s: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.651930  795967 shortcut.go:103] Error loading discovery information: Get https://Master_IP:6443/api?timeout=32s: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.652026  795967 round_trippers.go:466] curl -v -XGET  -H "User-Agent: kubectl/v1.30.1 (linux/amd64) kubernetes/6911225" -H "Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json" 'https://Master_IP:6443/api?timeout=32s'
I0601 11:38:36.652159  795967 round_trippers.go:508] HTTP Trace: Dial to tcp:Master_IP:6443 failed: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.652178  795967 round_trippers.go:553] GET https://Master_IP:6443/api?timeout=32s  in 0 milliseconds
I0601 11:38:36.652188  795967 round_trippers.go:570] HTTP Statistics: DNSLookup 0 ms Dial 0 ms TLSHandshake 0 ms Duration 0 ms
I0601 11:38:36.652198  795967 round_trippers.go:577] Response Headers:
E0601 11:38:36.652226  795967 memcache.go:265] couldn't get current server API group list: Get https://Master_IP:6443/api?timeout=32s: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.652233  795967 cached_discovery.go:120] skipped caching discovery info due to Get https://Master_IP:6443/api?timeout=32s: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.652289  795967 round_trippers.go:466] curl -v -XGET  -H "Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json" -H "User-Agent: kubectl/v1.30.1 (linux/amd64) kubernetes/6911225" 'https://Master_IP:6443/api?timeout=32s'
I0601 11:38:36.652412  795967 round_trippers.go:508] HTTP Trace: Dial to tcp:Master_IP:6443 failed: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.652447  795967 round_trippers.go:553] GET https://Master_IP:6443/api?timeout=32s  in 0 milliseconds
I0601 11:38:36.652459  795967 round_trippers.go:570] HTTP Statistics: DNSLookup 0 ms Dial 0 ms TLSHandshake 0 ms Duration 0 ms
I0601 11:38:36.652468  795967 round_trippers.go:577] Response Headers:
E0601 11:38:36.652502  795967 memcache.go:265] couldn't get current server API group list: Get https://Master_IP:6443/api?timeout=32s: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.653592  795967 cached_discovery.go:120] skipped caching discovery info due to Get https://Master_IP:6443/api?timeout=32s: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.653691  795967 round_trippers.go:466] curl -v -XGET  -H "User-Agent: kubectl/v1.30.1 (linux/amd64) kubernetes/6911225" -H "Accept: application/json;g=apidiscovery.k8s.io;v=v2;as=APIGroupDiscoveryList,application/json;g=apidiscovery.k8s.io;v=v2beta1;as=APIGroupDiscoveryList,application/json" 'https://Master_IP:6443/api?timeout=32s'
I0601 11:38:36.653841  795967 round_trippers.go:508] HTTP Trace: Dial to tcp:Master_IP:6443 failed: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.653872  795967 round_trippers.go:553] GET https://Master_IP:6443/api?timeout=32s  in 0 milliseconds
I0601 11:38:36.653894  795967 round_trippers.go:570] HTTP Statistics: DNSLookup 0 ms Dial 0 ms TLSHandshake 0 ms Duration 0 ms
I0601 11:38:36.653903  795967 round_trippers.go:577] Response Headers:
E0601 11:38:36.653939  795967 memcache.go:265] couldn't get current server API group list: Get https://Master_IP:6443/api?timeout=32s: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.653955  795967 cached_discovery.go:120] skipped caching discovery info due to Get https://Master_IP:6443/api?timeout=32s: dial tcp Master_IP:6443: connect: connection refused
I0601 11:38:36.653998  795967 helpers.go:264] Connection error: Get https://Master_IP:6443/api?timeout=32s: dial tcp Master_IP:6443: connect: connection refused
The connection to the server Master_IP:6443 was refused - did you specify the right host or port?
-->while checking the kubelet / even restarted many times, it didnt help 
ot-master1 $ systemctl status kubelet
â— kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             â””â”€10-kubeadm.conf
     Active: active (running) since Wed 2024-05-29 19:51:15 CDT; 4 days ago
       Docs: https://kubernetes.io/docs/
   Main PID: 261048 (kubelet)
      Tasks: 17 (limit: 38415)
     Memory: 51.5M
        CPU: 1h 6min 1.020s
     CGroup: /system.slice/kubelet.service
             â””â”€261048 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///run/containerd/containerd.sock --pod-infra-container-image=registry.k8s.io/pause:3.9

Jun 02 22:06:52 ot-master1.internal.local kubelet[261048]: E0602 22:06:52.847762  261048 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-proxy\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kube-proxy pod=kube-proxy-vtthh_kube-system(c53eac75-2bcd-480d-a6b6-cbf635217113)\"" pod="kube-system/kube-proxy-vtthh" po>
Jun 02 22:06:53 ot-master1.internal.local kubelet[261048]: E0602 22:06:53.915602  261048 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"ot-master1.internal.local\": Get \"https://Master_IP:6443/api/v1/nodes/ot-master1.internal.local?resourceVersion=0&timeout=10s\": dial tcp Master_IP:6443: connect: connection refused"
Jun 02 22:06:53 ot-master1.internal.local kubelet[261048]: E0602 22:06:53.915811  261048 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"ot-master1.internal.local\": Get \"https://Master_IP:6443/api/v1/nodes/ot-master1.internal.local?timeout=10s\": dial tcp Master_IP:6443: connect: connection refused"
Jun 02 22:06:53 ot-master1.internal.local kubelet[261048]: E0602 22:06:53.915923  261048 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"ot-master1.internal.local\": Get \"https://Master_IP:6443/api/v1/nodes/ot-master1.internal.local?timeout=10s\": dial tcp Master_IP:6443: connect: connection refused"
Jun 02 22:06:53 ot-master1.internal.local kubelet[261048]: E0602 22:06:53.916140  261048 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"ot-master1.internal.local\": Get \"https://Master_IP:6443/api/v1/nodes/ot-master1.internal.local?timeout=10s\": dial tcp Master_IP:6443: connect: connection refused"
Jun 02 22:06:53 ot-master1.internal.local kubelet[261048]: E0602 22:06:53.916329  261048 kubelet_node_status.go:544] "Error updating node status, will retry" err="error getting node \"ot-master1.internal.local\": Get \"https://Master_IP:6443/api/v1/nodes/ot-master1.internal.local?timeout=10s\": dial tcp Master_IP:6443: connect: connection refused"
Jun 02 22:06:53 ot-master1.internal.local kubelet[261048]: E0602 22:06:53.916348  261048 kubelet_node_status.go:531] "Unable to update node status" err="update node status exceeds retry count"
Jun 02 22:06:53 ot-master1.internal.local kubelet[261048]: E0602 22:06:53.951079  261048 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://Master_IP:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/ot-master1.internal.local?timeout=10s\": dial tcp Master_IP:6443: connect: connection refused" interval="7s"
Jun 02 22:06:54 ot-master1.internal.local kubelet[261048]: I0602 22:06:54.846684  261048 scope.go:117] "RemoveContainer" containerID="5907d016cfb243f11019b26d636b13d3f1f1d4caa3fcfe464021f743d4087383"
Jun 02 22:06:54 ot-master1.internal.local kubelet[261048]: E0602 22:06:54.846915  261048 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kube-scheduler\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kube-scheduler pod=kube-scheduler-ot-master1.internal.local_kube-system(7e4870baad9d42588bd86d4db89bbc3a)\"" pod="kube>
lines 1-23/23 (END)

#### How can we reproduce it (as minimally and precisely as possible)?

it is newly setup, so not sure how to reproduce it 

#### Anything else we need to know?

ot-master1 $ crictl ps -a
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
WARN[0000] image connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
a983facbb5bc1       3861cfcd7c04c       1 second ago        Running             etcd                      3903                8d5e41e38f052       etcd-ot-master1.internal.local
4e9b8fe986eb9       25a1387cdab82       21 seconds ago      Exited              kube-controller-manager   1144                28d528679ff60       kube-controller-manager-ot-master1.internal.local
e06e9abb29332       747097150317f       2 minutes ago       Exited              kube-proxy                947                 fc5c6f557c735       kube-proxy-vtthh
a19c34b9cb64d       3861cfcd7c04c       3 minutes ago       Exited              etcd                      3902                497039ab0fd10       etcd-ot-master1.internal.local
5907d016cfb24       a52dc94f0a912       4 minutes ago       Exited              kube-scheduler            3815                17c84d5c165cf       kube-scheduler-ot-master1.internal.local
e32430f6860f9       91be940803172       5 minutes ago       Exited              kube-apiserver            3598                9145c833b443c       kube-apiserver-ot-master1.internal.local
ot-master1 $

ot-master1 $ crictl logs e32430f6860f9
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
I0603 03:02:32.967767       1 options.go:221] external host was not specified, using Master_IP
I0603 03:02:32.968407       1 server.go:148] Version: v1.30.1
I0603 03:02:32.968488       1 server.go:150] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0603 03:02:33.629837       1 shared_informer.go:313] Waiting for caches to sync for node_authorizer
I0603 03:02:33.632482       1 shared_informer.go:313] Waiting for caches to sync for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0603 03:02:33.634263       1 plugins.go:157] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0603 03:02:33.634277       1 plugins.go:160] Loaded 13 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,ClusterTrustBundleAttest,CertificateSubjectRestriction,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook,ResourceQuota.
I0603 03:02:33.634470       1 instance.go:299] Using reconciler: lease
I0603 03:02:33.657645       1 handler.go:286] Adding GroupVersion apiextensions.k8s.io v1 to ResourceManager
W0603 03:02:33.657663       1 genericapiserver.go:733] Skipping API apiextensions.k8s.io/v1beta1 because it has no resources.
I0603 03:02:33.792328       1 handler.go:286] Adding GroupVersion  v1 to ResourceManager
I0603 03:02:33.792720       1 instance.go:696] API group "internal.apiserver.k8s.io" is not enabled, skipping.
I0603 03:02:33.933622       1 instance.go:696] API group "storagemigration.k8s.io" is not enabled, skipping.
I0603 03:02:34.057807       1 instance.go:696] API group "resource.k8s.io" is not enabled, skipping.
I0603 03:02:34.070687       1 handler.go:286] Adding GroupVersion authentication.k8s.io v1 to ResourceManager
W0603 03:02:34.070714       1 genericapiserver.go:733] Skipping API authentication.k8s.io/v1beta1 because it has no resources.
W0603 03:02:34.070721       1 genericapiserver.go:733] Skipping API authentication.k8s.io/v1alpha1 because it has no resources.
I0603 03:02:34.071041       1 handler.go:286] Adding GroupVersion authorization.k8s.io v1 to ResourceManager
W0603 03:02:34.071053       1 genericapiserver.go:733] Skipping API authorization.k8s.io/v1beta1 because it has no resources.
I0603 03:02:34.071685       1 handler.go:286] Adding GroupVersion autoscaling v2 to ResourceManager
I0603 03:02:34.072164       1 handler.go:286] Adding GroupVersion autoscaling v1 to ResourceManager
W0603 03:02:34.072178       1 genericapiserver.go:733] Skipping API autoscaling/v2beta1 because it has no resources.
W0603 03:02:34.072183       1 genericapiserver.go:733] Skipping API autoscaling/v2beta2 because it has no resources.
I0603 03:02:34.073187       1 handler.go:286] Adding GroupVersion batch v1 to ResourceManager
W0603 03:02:34.073205       1 genericapiserver.go:733] Skipping API batch/v1beta1 because it has no resources.
I0603 03:02:34.073803       1 handler.go:286] Adding GroupVersion certificates.k8s.io v1 to ResourceManager
W0603 03:02:34.073819       1 genericapiserver.go:733] Skipping API certificates.k8s.io/v1beta1 because it has no resources.
W0603 03:02:34.073824       1 genericapiserver.go:733] Skipping API certificates.k8s.io/v1alpha1 because it has no resources.
I0603 03:02:34.074233       1 handler.go:286] Adding GroupVersion coordination.k8s.io v1 to ResourceManager
W0603 03:02:34.074245       1 genericapiserver.go:733] Skipping API coordination.k8s.io/v1beta1 because it has no resources.
W0603 03:02:34.074280       1 genericapiserver.go:733] Skipping API discovery.k8s.io/v1beta1 because it has no resources.
I0603 03:02:34.074674       1 handler.go:286] Adding GroupVersion discovery.k8s.io v1 to ResourceManager
I0603 03:02:34.075775       1 handler.go:286] Adding GroupVersion networking.k8s.io v1 to ResourceManager
W0603 03:02:34.075789       1 genericapiserver.go:733] Skipping API networking.k8s.io/v1beta1 because it has no resources.
W0603 03:02:34.075794       1 genericapiserver.go:733] Skipping API networking.k8s.io/v1alpha1 because it has no resources.
I0603 03:02:34.076090       1 handler.go:286] Adding GroupVersion node.k8s.io v1 to ResourceManager
W0603 03:02:34.076101       1 genericapiserver.go:733] Skipping API node.k8s.io/v1beta1 because it has no resources.
W0603 03:02:34.076105       1 genericapiserver.go:733] Skipping API node.k8s.io/v1alpha1 because it has no resources.
I0603 03:02:34.076670       1 handler.go:286] Adding GroupVersion policy v1 to ResourceManager
W0603 03:02:34.076683       1 genericapiserver.go:733] Skipping API policy/v1beta1 because it has no resources.
I0603 03:02:34.077878       1 handler.go:286] Adding GroupVersion rbac.authorization.k8s.io v1 to ResourceManager
W0603 03:02:34.077892       1 genericapiserver.go:733] Skipping API rbac.authorization.k8s.io/v1beta1 because it has no resources.
W0603 03:02:34.077897       1 genericapiserver.go:733] Skipping API rbac.authorization.k8s.io/v1alpha1 because it has no resources.
I0603 03:02:34.078217       1 handler.go:286] Adding GroupVersion scheduling.k8s.io v1 to ResourceManager
W0603 03:02:34.078230       1 genericapiserver.go:733] Skipping API scheduling.k8s.io/v1beta1 because it has no resources.
W0603 03:02:34.078234       1 genericapiserver.go:733] Skipping API scheduling.k8s.io/v1alpha1 because it has no resources.
I0603 03:02:34.079820       1 handler.go:286] Adding GroupVersion storage.k8s.io v1 to ResourceManager
W0603 03:02:34.079838       1 genericapiserver.go:733] Skipping API storage.k8s.io/v1beta1 because it has no resources.
W0603 03:02:34.079842       1 genericapiserver.go:733] Skipping API storage.k8s.io/v1alpha1 because it has no resources.
I0603 03:02:34.080958       1 handler.go:286] Adding GroupVersion flowcontrol.apiserver.k8s.io v1 to ResourceManager
I0603 03:02:34.082005       1 handler.go:286] Adding GroupVersion flowcontrol.apiserver.k8s.io v1beta3 to ResourceManager
W0603 03:02:34.082021       1 genericapiserver.go:733] Skipping API flowcontrol.apiserver.k8s.io/v1beta2 because it has no resources.
W0603 03:02:34.082042       1 genericapiserver.go:733] Skipping API flowcontrol.apiserver.k8s.io/v1beta1 because it has no resources.
I0603 03:02:34.085267       1 handler.go:286] Adding GroupVersion apps v1 to ResourceManager
W0603 03:02:34.085284       1 genericapiserver.go:733] Skipping API apps/v1beta2 because it has no resources.
W0603 03:02:34.085307       1 genericapiserver.go:733] Skipping API apps/v1beta1 because it has no resources.
I0603 03:02:34.087311       1 handler.go:286] Adding GroupVersion admissionregistration.k8s.io v1 to ResourceManager
W0603 03:02:34.087330       1 genericapiserver.go:733] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W0603 03:02:34.087352       1 genericapiserver.go:733] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I0603 03:02:34.087879       1 handler.go:286] Adding GroupVersion events.k8s.io v1 to ResourceManager
W0603 03:02:34.087902       1 genericapiserver.go:733] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0603 03:02:34.097441       1 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0603 03:02:34.097455       1 genericapiserver.go:733] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0603 03:02:34.374456       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/etc/kubernetes/pki/front-proxy-ca.crt"
I0603 03:02:34.374501       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
I0603 03:02:34.374769       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/etc/kubernetes/pki/apiserver.crt::/etc/kubernetes/pki/apiserver.key"
I0603 03:02:34.375033       1 secure_serving.go:213] Serving securely on [::]:6443
I0603 03:02:34.375088       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0603 03:02:34.375151       1 controller.go:78] Starting OpenAPI AggregationController
I0603 03:02:34.375181       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/etc/kubernetes/pki/front-proxy-client.crt::/etc/kubernetes/pki/front-proxy-client.key"
I0603 03:02:34.375195       1 available_controller.go:423] Starting AvailableConditionController
I0603 03:02:34.375205       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0603 03:02:34.375267       1 apf_controller.go:374] Starting API Priority and Fairness config controller
I0603 03:02:34.375273       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0603 03:02:34.375355       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0603 03:02:34.375411       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0603 03:02:34.375610       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0603 03:02:34.375674       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0603 03:02:34.375995       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0603 03:02:34.376151       1 controller.go:116] Starting legacy_token_tracking_controller
I0603 03:02:34.376216       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0603 03:02:34.376287       1 aggregator.go:163] waiting for initial CRD sync...
I0603 03:02:34.376392       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0603 03:02:34.376474       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0603 03:02:34.376555       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/etc/kubernetes/pki/ca.crt"
I0603 03:02:34.376678       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/etc/kubernetes/pki/front-proxy-ca.crt"
I0603 03:02:34.376984       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0603 03:02:34.377017       1 controller.go:139] Starting OpenAPI controller
I0603 03:02:34.377123       1 crd_finalizer.go:266] Starting CRDFinalizer
I0603 03:02:34.377636       1 controller.go:87] Starting OpenAPI V3 controller
I0603 03:02:34.377665       1 naming_controller.go:291] Starting NamingConditionController
I0603 03:02:34.377694       1 establishing_controller.go:76] Starting EstablishingController
I0603 03:02:34.377713       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0603 03:02:34.390422       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0603 03:02:34.390440       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0603 03:02:34.430041       1 shared_informer.go:320] Caches are synced for node_authorizer
I0603 03:02:34.433278       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0603 03:02:34.433295       1 policy_source.go:224] refreshing policies
I0603 03:02:34.475605       1 apf_controller.go:379] Running API Priority and Fairness config worker
I0603 03:02:34.475631       1 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0603 03:02:34.475642       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0603 03:02:34.475729       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0603 03:02:34.475861       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0603 03:02:34.476382       1 shared_informer.go:320] Caches are synced for configmaps
I0603 03:02:34.479739       1 handler_discovery.go:447] Starting ResourceDiscoveryManager
I0603 03:02:34.490818       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0603 03:02:34.490846       1 aggregator.go:165] initial CRD sync complete...
I0603 03:02:34.490852       1 autoregister_controller.go:141] Starting autoregister controller
I0603 03:02:34.490858       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0603 03:02:34.490863       1 cache.go:39] Caches are synced for autoregister controller
I0603 03:02:34.503950       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0603 03:02:35.378607       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
W0603 03:02:35.585354       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [Master_IP]
I0603 03:02:35.586406       1 controller.go:615] quota admission added evaluator for: endpoints
I0603 03:02:35.589649       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
E0603 03:02:57.793100       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"context canceled"}: context canceled
E0603 03:02:57.793158       1 writers.go:122] apiserver was unable to write a JSON response: http: Handler timeout
E0603 03:02:57.794763       1 status.go:71] apiserver received an error that is not an metav1.Status: &errors.errorString{s:"http: Handler timeout"}: http: Handler timeout
E0603 03:02:57.794797       1 writers.go:135] apiserver was unable to write a fallback JSON response: http: Handler timeout
E0603 03:02:57.796007       1 timeout.go:142] post-timeout activity - time-elapsed: 3.158512ms, GET "/api/v1/namespaces/kube-system/configmaps/extension-apiserver-authentication" result: <nil>
E0603 03:02:57.838464       1 watcher.go:342] watch chan error: rpc error: code = Unknown desc = malformed header: missing HTTP content-type
E0603 03:02:57.838541       1 watcher.go:342] watch chan error: rpc error: code = Unknown desc = malformed header: missing HTTP content-type
E0603 03:02:57.838581       1 watcher.go:342] watch chan error: rpc error: code = Unknown desc = malformed header: missing HTTP content-type
W0603 03:02:57.839111       1 logging.go:59] [core] [Channel #175 SubChannel #176] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 03:02:57.839130       1 logging.go:59] [core] [Channel #19 SubChannel #20] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0603 03:02:57.839170       1 logging.go:59] [core] [Channel #94 SubChannel #95] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection 

#### Kubernetes version

kubectl version
Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
The connection to the server 16.32.12.201:6443 was refused - did you specify the right host or port?

#### Cloud provider

NA


#### OS version


ot-master1 $ cat /etc/*release*
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=22.04
DISTRIB_CODENAME=jammy
DISTRIB_DESCRIPTION="Ubuntu 22.04.4 LTS"
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
ot-master1 $

#### Install tools

NA


#### Container runtime (CRI) and version (if applicable)

crictl version
WARN[0000] runtime connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead.
Version:  0.1.0
RuntimeName:  containerd
RuntimeVersion:  1.6.31
RuntimeApiVersion:  v1



#### Related plugins (CNI, CSI, ...) and versions (if applicable)

NA


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #125267 Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized

- Issue é“¾æ¥ï¼š[#125267](https://github.com/kubernetes/kubernetes/issues/125267)

### Issue å†…å®¹

#### What happened?

Right after installing kubernetes control plane  using `kubeadm` and `weave net` as network plugin the API server goes down with the below error msg.
```
[root@kubemaster ~]# kubectl get pods -A --watch
error: Get "https://10.74.250.78:6443/api/v1/pods?limit=500": dial tcp 10.74.250.78:6443: connect: connection refused - error from a previous attempt: read tcp 10.74.250.78:44740->10.74.250.78:6443: read: connection reset by peer
[root@kubemaster ~]#  
```

#### What did you expect to happen?

The cluster should be working fine with `weave net` 

#### How can we reproduce it (as minimally and precisely as possible)?

```
kubeadm init --pod-network-cidr=10.10.0.0/16 --apiserver-advertise-address=10.74.250.78 --v=5

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 10.74.250.78:6443 --token 7l1k0v.gnzdt2i4a0hg6uaq \
	--discovery-token-ca-cert-hash sha256:9f307309f8fd825a99a30e2643a59159499d4fd78843ac14d0c78f85849da411 
[root@kubemaster ~]# 
[root@kubemaster ~]# export KUBECONFIG=/etc/kubernetes/admin.conf
[root@kubemaster ~]# 
```



```
[root@kubemaster ~]# kubectl apply -f weave-daemonset-k8s.yaml
serviceaccount/weave-net created
clusterrole.rbac.authorization.k8s.io/weave-net created
clusterrolebinding.rbac.authorization.k8s.io/weave-net created
role.rbac.authorization.k8s.io/weave-net created
rolebinding.rbac.authorization.k8s.io/weave-net created
daemonset.apps/weave-net created
[root@kubemaster ~]#
```
```
[root@kubemaster ~]# kubectl get pods -A  --watch
NAMESPACE     NAME                                 READY   STATUS             RESTARTS        AGE
kube-system   coredns-7db6d8ff4d-6t5cb             0/1     Pending            0               3m25s
kube-system   coredns-7db6d8ff4d-z9lvq             0/1     Pending            0               3m25s
kube-system   etcd-kubemaster                      1/1     Running            173 (90s ago)   58s
kube-system   kube-apiserver-kubemaster            1/1     Running            174 (72s ago)   3m59s
kube-system   kube-controller-manager-kubemaster   1/1     Running            12 (112s ago)   4m33s
kube-system   kube-proxy-jgnb6                     0/1     CrashLoopBackOff   2 (7s ago)      3m25s
kube-system   kube-scheduler-kubemaster            0/1     CrashLoopBackOff   181 (30s ago)   58s
kube-system   weave-net-fdvrk                      0/2     Pending            0               8s
kube-system   kube-scheduler-kubemaster            0/1     Running            182 (53s ago)   81s
kube-system   kube-proxy-jgnb6                     1/1     Running            3 (32s ago)     3m50s
kube-system   kube-scheduler-kubemaster            0/1     Completed          182 (61s ago)   89s
kube-system   kube-scheduler-kubemaster            0/1     CrashLoopBackOff   182 (2s ago)    90s
^C[root@kubemaster ~]# 

```




 Error:
====================
```
Jun  1 09:35:45 kubemaster kubelet[15562]: E0601 09:35:45.262058   15562 kubelet.go:2900] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
Jun  1 09:35:46 kubemaster kubelet[15562]: E0601 09:35:46.177829   15562 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://10.74.250.78:6443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/kubemaster?timeout=10s\": dial tcp 10.74.250.78:6443: connect: connection refused" interval="7s"
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
[root@kubemaster ~]# kubectl version
Client Version: v1.30.1
```

</details>


#### Cloud provider

<details>
VM 
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
NAME="Red Hat Enterprise Linux"
VERSION="9.4 (Plow)"
ID="rhel"
ID_LIKE="fedora"
VERSION_ID="9.4"
PLATFORM_ID="platform:el9"
PRETTY_NAME="Red Hat Enterprise Linux 9.4 (Plow)"
ANSI_COLOR="0;31"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:redhat:enterprise_linux:9::baseos"
HOME_URL="https://www.redhat.com/"
DOCUMENTATION_URL="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9"
BUG_REPORT_URL="https://bugzilla.redhat.com/"

REDHAT_BUGZILLA_PRODUCT="Red Hat Enterprise Linux 9"
REDHAT_BUGZILLA_PRODUCT_VERSION=9.4
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.4"
[root@kubemaster ~]# 
[root@kubemaster ~]# 
[root@kubemaster ~]# uname -r
5.14.0-427.18.1.el9_4.x86_64
[root@kubemaster ~]# 

```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
Weave Net 
</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

