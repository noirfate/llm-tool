# Issue å®‰å…¨åˆ†ææŠ¥å‘Š

# ğŸš¨ å­˜åœ¨å®‰å…¨é£é™©çš„ Issues (17 ä¸ª)

## Issue #124515 kubelet and containerd in endless loop for CreateContainer with unexpected media type octet-stream

- Issue é“¾æ¥ï¼š[#124515](https://github.com/kubernetes/kubernetes/issues/124515)

### Issue å†…å®¹

#### What happened?

I think the private container registry had some intermittent errors and did return wrong data for some layers.

The problem I think is that kubelet or containerd is in a state were it assumes that a given OCI image is already downloaded locally, but then some parsing of this layer fails and the loop starts again.

#### What did you expect to happen?

kubelet and or containerd should detect the wrong layer/manifest state and delete the incomplete/erroneous download and pull again from container registry.

#### How can we reproduce it (as minimally and precisely as possible)?

Not sure, probably container registry needs to return media type octet-stream for some layer/manifest instead of correct media type.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
Kubelet 1.28.7-gke.1026000
Containerd 1.7.10
Linux 6.1.58+
</details>


#### Cloud provider

<details>
GCP
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Will provide 
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

å­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

**åŸå› åŠå¯èƒ½çš„å½±å“ï¼š**

è¯¥é—®é¢˜æè¿°äº† kubelet å’Œ containerd åœ¨æ‹‰å–å®¹å™¨é•œåƒæ—¶ï¼Œå¦‚æœé•œåƒçš„æŸäº›å±‚æˆ–æ¸…å•è¿”å›äº†æ„å¤–çš„åª’ä½“ç±»å‹ï¼ˆå¦‚ `application/octet-stream`ï¼‰ï¼Œä¼šå¯¼è‡´å®ƒä»¬é™·å…¥æ— é™å¾ªç¯ã€‚æ”»å‡»è€…å¦‚æœèƒ½å¤Ÿæ§åˆ¶æˆ–åŠ«æŒå®¹å™¨é•œåƒä»“åº“ï¼Œæˆ–è€…å‘å¸ƒæ¶æ„çš„å®¹å™¨é•œåƒï¼Œä½¿å¾—é•œåƒçš„æŸäº›å±‚æˆ–æ¸…å•çš„åª’ä½“ç±»å‹è¢«è®¾ç½®ä¸º `application/octet-stream`ï¼Œå°±å¯èƒ½è§¦å‘è¿™ä¸€é—®é¢˜ã€‚

è¿™ç§æƒ…å†µä¸‹ï¼Œkubelet å’Œ containerd ä¼šä¸æ–­å°è¯•è§£æå’Œåˆ›å»ºå®¹å™¨ï¼Œä½†ç”±äºåª’ä½“ç±»å‹ä¸åŒ¹é…ï¼Œæ¯æ¬¡éƒ½ä¼šå¤±è´¥å¹¶é‡æ–°å°è¯•ï¼Œå¯¼è‡´å®ƒä»¬è¿›å…¥æ— é™å¾ªç¯ã€‚ç»“æœæ˜¯æ¶ˆè€—å¤§é‡çš„ç³»ç»Ÿèµ„æºï¼ˆå¦‚ CPU å’Œå†…å­˜ï¼‰ï¼Œå¯èƒ½å¯¼è‡´èŠ‚ç‚¹æ€§èƒ½ä¸¥é‡ä¸‹é™ï¼Œç”šè‡³ä¸å¯ç”¨ï¼Œè¿›è€Œå½±å“æ•´ä¸ªé›†ç¾¤çš„ç¨³å®šæ€§å’Œå¯ç”¨æ€§ã€‚è¿™å±äºå…¸å‹çš„æ‹’ç»æœåŠ¡ï¼ˆDenial of Service, DoSï¼‰æ”»å‡»ã€‚

**ç¬¦åˆé£é™©åˆ¤æ–­æ ‡å‡†ï¼š**

1. **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨ï¼š** æ”»å‡»è€…å¯ä»¥é€šè¿‡å‘å¸ƒæ¶æ„é•œåƒæˆ–ç¯¡æ”¹é•œåƒä»“åº“å“åº”ï¼Œè¯±ä½¿ kubelet å’Œ containerd æ‹‰å–å¹¶å¤„ç†å¸¦æœ‰é”™è¯¯åª’ä½“ç±»å‹çš„é•œåƒã€‚
2. **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é… CVE ç¼–å·ï¼Œä½¿ç”¨ CVSS 3.1 è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœåœ¨ High ä»¥ä¸Šï¼š**

   - **æ”»å‡»å‘é‡ï¼ˆAVï¼‰ï¼šç½‘ç»œï¼ˆNetworkï¼ŒNï¼‰** - æ”»å‡»è€…å¯é€šè¿‡ç½‘ç»œå‘å—å®³è€…æä¾›æ¶æ„é•œåƒã€‚
   - **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰ï¼šä½ï¼ˆLowï¼ŒLï¼‰** - æ”»å‡»ä¸éœ€è¦é«˜å¤æ‚åº¦çš„æ¡ä»¶ã€‚
   - **ç‰¹æƒè¦æ±‚ï¼ˆPRï¼‰ï¼šæ— ï¼ˆNoneï¼ŒNï¼‰** - æ”»å‡»è€…æ— éœ€ä»»ä½•æƒé™å³å¯å®æ–½æ”»å‡»ã€‚
   - **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰ï¼šæ— ï¼ˆNoneï¼ŒNï¼‰** - ä¸éœ€è¦é¢å¤–çš„ç”¨æˆ·äº¤äº’å³å¯è§¦å‘æ¼æ´ã€‚
   - **ä½œç”¨èŒƒå›´ï¼ˆSï¼‰ï¼šæœªæ”¹å˜ï¼ˆUnchangedï¼ŒUï¼‰** - æ¼æ´ä»…å½±å“ç»„ä»¶è‡ªèº«çš„èµ„æºã€‚
   - **æœºå¯†æ€§å½±å“ï¼ˆCï¼‰ï¼šæ— ï¼ˆNoneï¼ŒNï¼‰** - ä¸å½±å“æœºå¯†æ€§ã€‚
   - **å®Œæ•´æ€§å½±å“ï¼ˆIï¼‰ï¼šæ— ï¼ˆNoneï¼ŒNï¼‰** - ä¸å½±å“å®Œæ•´æ€§ã€‚
   - **å¯ç”¨æ€§å½±å“ï¼ˆAï¼‰ï¼šé«˜ï¼ˆHighï¼ŒHï¼‰** - å¯¹ç³»ç»Ÿçš„å¯ç”¨æ€§é€ æˆä¸¥é‡å½±å“ã€‚

   ç»¼åˆè¯„åˆ†ï¼š**CVSSï¼š3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:Hï¼ŒåŸºç¡€å¾—åˆ†ï¼š7.5ï¼ˆHighï¼‰**

**Proof of Conceptï¼š**

1. **æ„å»ºæ¶æ„é•œåƒï¼š**
   - æ”»å‡»è€…åˆ›å»ºä¸€ä¸ªå®¹å™¨é•œåƒï¼Œåœ¨å…¶å±‚ï¼ˆlayerï¼‰æˆ–æ¸…å•ï¼ˆmanifestï¼‰ä¸­ï¼Œå°†åª’ä½“ç±»å‹ï¼ˆMedia Typeï¼‰è®¾ç½®ä¸º `application/octet-stream`ï¼Œè€Œä¸æ˜¯æ­£ç¡®çš„åª’ä½“ç±»å‹ã€‚

2. **å‘å¸ƒæ¶æ„é•œåƒï¼š**
   - å°†è¯¥æ¶æ„é•œåƒæ¨é€åˆ°å…¬å…±å®¹å™¨é•œåƒä»“åº“ï¼Œæˆ–è€…é€šè¿‡ä¸­é—´äººæ”»å‡»ï¼ˆMITMï¼‰åŠ«æŒç§æœ‰é•œåƒä»“åº“çš„é€šä¿¡ï¼Œè¿”å›ç¯¡æ”¹åçš„é•œåƒæ•°æ®ã€‚

3. **è¯±å¯¼æ‹‰å–é•œåƒï¼š**
   - ä½¿å—å®³è€…çš„ Kubernetes é›†ç¾¤æ‹‰å–å¹¶éƒ¨ç½²è¯¥æ¶æ„é•œåƒã€‚ç”±äº Kubernetes ä¼šè‡ªåŠ¨å°è¯•æ‹‰å–å¹¶è¿è¡ŒæŒ‡å®šçš„å®¹å™¨é•œåƒï¼Œè¿‡ç¨‹æ— éœ€é¢å¤–çš„ç”¨æˆ·äº¤äº’ã€‚

4. **è§¦å‘æ— é™å¾ªç¯ï¼š**
   - kubelet å’Œ containerd åœ¨å¤„ç†è¯¥é•œåƒæ—¶ï¼Œç”±äºåª’ä½“ç±»å‹ä¸åŒ¹é…ï¼Œä¼šåœ¨ä¸‹è½½ã€è§£æå’Œåˆ›å»ºå®¹å™¨çš„è¿‡ç¨‹ä¸­åå¤å¤±è´¥å¹¶é‡è¯•ï¼Œè¿›å…¥æ— é™å¾ªç¯ã€‚

5. **é€ æˆæ‹’ç»æœåŠ¡ï¼š**
   - ç”±äº kubelet å’Œ containerd æŒç»­å ç”¨å¤§é‡çš„ CPU å’Œå†…å­˜èµ„æºï¼Œå…¶ä»–æ­£å¸¸çš„å®¹å™¨å¯èƒ½æ— æ³•è°ƒåº¦å’Œè¿è¡Œï¼ŒèŠ‚ç‚¹çš„å¯ç”¨æ€§ä¸¥é‡å—æŸï¼Œå¯¼è‡´æ‹’ç»æœåŠ¡ã€‚

**é˜²èŒƒå»ºè®®ï¼š**

- **å¥å…¨è¾“å…¥éªŒè¯ï¼š** kubelet å’Œ containerd åº”åŠ å¼ºå¯¹é•œåƒåª’ä½“ç±»å‹çš„éªŒè¯ï¼Œé‡åˆ°å¼‚å¸¸åº”åŠæ—¶é€€å‡ºå¾ªç¯å¹¶æŠ¥é”™ã€‚
- **é™åˆ¶é‡è¯•æ¬¡æ•°ï¼š** å¯¹äºæ‹‰å–å’Œåˆ›å»ºå®¹å™¨çš„é‡è¯•æ“ä½œï¼Œåº”è®¾ç½®åˆç†çš„é‡è¯•æ¬¡æ•°ä¸Šé™ï¼Œé¿å…æ— é™å¾ªç¯ã€‚
- **ç›‘æ§å’ŒæŠ¥è­¦ï¼š** éƒ¨ç½²èµ„æºç›‘æ§å·¥å…·ï¼ŒåŠæ—¶å‘ç°å¼‚å¸¸çš„èµ„æºæ¶ˆè€—æƒ…å†µï¼Œè§¦å‘æŠ¥è­¦å¹¶é‡‡å–æªæ–½ã€‚

---

## Issue #124502 The APIServer health check failed. As a result, the APIServer automatically exits and APIServer unavailable.

- Issue é“¾æ¥ï¼š[#124502](https://github.com/kubernetes/kubernetes/issues/124502)

### Issue å†…å®¹

#### What happened?

Most services access the APIServer through the service of the ClusterIP type. The relationship between the service and the endpoints is created by the APIServer , and then the iptables routing rule maintained by kube-proxy. When the APIServer is restarted, the routing rule cannot be updated quickly. A large amount of service traffic is received. CPU resources are limited and the CPU usage is high. As a result, the APIServer health check timeout and then the APIServer exits automatically. As a result, the service is unavailable.

#### What did you expect to happen?

Before the APIServer is started successfully, a filter is added to the ingress to control the incoming traffic, ensuring that the APIServer can be started successfully and enhancing the APIServer reliability.

#### How can we reproduce it (as minimally and precisely as possible)?

When the APIServer is started, a large number of requests are sent to the APIServer. As a result, the CPU is overloaded and the APIServer health check times out.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>

```[tasklist]
#### Tasks
```


### åˆ†æç»“æœ

è¿™æ¶‰åŠåˆ°æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

**åŸå› å’Œå¯èƒ½çš„å½±å“ï¼š**

æ ¹æ®Issueçš„æè¿°ï¼Œå½“APIServeré‡å¯æ—¶ï¼Œå› è·¯ç”±è§„åˆ™æ— æ³•åŠæ—¶æ›´æ–°ï¼Œå¤§é‡æœåŠ¡æµé‡ç›´æ¥æ¶Œå…¥APIServerï¼Œå¯¼è‡´CPUèµ„æºè€—å°½ï¼ŒCPUä½¿ç”¨ç‡è¿‡é«˜ï¼Œä½¿å¾—APIServerçš„å¥åº·æ£€æŸ¥è¶…æ—¶ï¼Œè¿›è€ŒAPIServerè‡ªåŠ¨é€€å‡ºï¼ŒæœåŠ¡ä¸å¯ç”¨ã€‚

æ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¿™ä¸€æ¼æ´ï¼Œåœ¨APIServerå¯åŠ¨æˆ–é‡å¯çš„è¿‡ç¨‹ä¸­ï¼Œå‘å…¶å‘é€å¤§é‡çš„è¯·æ±‚ï¼Œåˆ¶é€ æ¶æ„æµé‡ã€‚ç”±äºAPIServeråœ¨æ­¤æ—¶ç¼ºä¹æœ‰æ•ˆçš„æµé‡æ§åˆ¶æˆ–è¿‡æ»¤æœºåˆ¶ï¼ŒCPUèµ„æºå®¹æ˜“è¢«è€—å°½ï¼Œå¯¼è‡´APIServeræ— æ³•æ­£å¸¸å¯åŠ¨æˆ–å´©æºƒã€‚è¿™æ˜¯ä¸€ç§å…¸å‹çš„æ‹’ç»æœåŠ¡ï¼ˆDenial of Service, DoSï¼‰æ”»å‡»ï¼Œä¸¥é‡å½±å“é›†ç¾¤çš„ç¨³å®šæ€§å’Œå¯ç”¨æ€§ã€‚

æŒ‰ç…§CVSS 3.1è¯„åˆ†æ ‡å‡†ï¼Œè¯¥æ¼æ´å¯èƒ½è¢«è¯„ä¸ºé«˜å±ï¼ˆHighï¼‰çº§åˆ«ï¼š

- **æ”»å‡»å‘é‡ï¼ˆAVï¼‰**ï¼šç½‘ç»œ(Network) â€”â€” æ”»å‡»è€…å¯é€šè¿‡ç½‘ç»œè¿œç¨‹å‘åŠ¨æ”»å‡»ã€‚
- **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰**ï¼šä½(Low) â€”â€” æ”»å‡»ä¸éœ€è¦ç‰¹æ®Šçš„æ¡ä»¶æˆ–æƒé™ã€‚
- **ç‰¹æƒè¦æ±‚ï¼ˆPRï¼‰**ï¼šæ— (None) â€”â€” æ”»å‡»è€…ä¸éœ€è¦ä»»ä½•æˆæƒå³å¯è¿›è¡Œæ”»å‡»ã€‚
- **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰**ï¼šæ— (None) â€”â€” ä¸éœ€è¦ç”¨æˆ·äº¤äº’å³å¯è§¦å‘æ¼æ´ã€‚
- **ä½œç”¨èŒƒå›´ï¼ˆSï¼‰**ï¼šæœªæ”¹å˜(Unchanged) â€”â€” æ”»å‡»åªå½±å“APIServeræ‰€åœ¨çš„ç»„ä»¶ã€‚
- **å¯ç”¨æ€§ï¼ˆAï¼‰**ï¼šé«˜(High) â€”â€” æ”»å‡»å¯å®Œå…¨ç ´åAPIServerçš„å¯ç”¨æ€§ã€‚

ç»¼åˆè¯„åˆ†å¯èƒ½è¾¾åˆ° **7.5ï¼ˆHighï¼‰**ã€‚

**Proof of Conceptï¼š**

æ”»å‡»è€…å¯ä»¥ç¼–å†™å¦‚ä¸‹è„šæœ¬ï¼Œåœ¨APIServerå¯åŠ¨æˆ–é‡å¯æ—¶ï¼Œä¸æ–­å‘å…¶å‘é€å¤§é‡è¯·æ±‚ï¼š

```bash
while true; do
  curl -k https://<APIServer_IP>:6443/ -o /dev/null &
done
```

æˆ–ä½¿ç”¨å‹åŠ›æµ‹è¯•å·¥å…·ï¼Œä¾‹å¦‚`ab`æˆ–`wrk`ï¼š

```bash
ab -n 100000 -c 1000 https://<APIServer_IP>:6443/
```

é€šè¿‡æŒç»­ã€é«˜é¢‘ç‡çš„è¯·æ±‚ï¼Œè€—å°½APIServerçš„CPUèµ„æºï¼Œå¯¼è‡´å¥åº·æ£€æŸ¥è¶…æ—¶ï¼ŒAPIServerè‡ªåŠ¨é€€å‡ºï¼ŒæœåŠ¡ä¸å¯ç”¨ã€‚

**å»ºè®®ï¼š**

ä¸ºäº†é˜²èŒƒæ­¤ç±»æ”»å‡»ï¼Œå»ºè®®åœ¨APIServerå¯åŠ¨æœŸé—´ï¼Œå¯¹è¿›å…¥çš„æµé‡è¿›è¡Œæ§åˆ¶ï¼š

- åœ¨Ingressæˆ–é˜²ç«å¢™å±‚é¢æ·»åŠ æµé‡è¿‡æ»¤ï¼Œé™åˆ¶è¯·æ±‚é€Ÿåº¦æˆ–æ¥æºã€‚
- åœ¨APIServerä¸­åŠ å…¥å¯åŠ¨ä¿æŠ¤æœºåˆ¶ï¼Œåœ¨åˆå§‹åŒ–å®Œæˆå‰é™åˆ¶å¤–éƒ¨è¯·æ±‚ã€‚
- å¢åŠ å¥åº·æ£€æŸ¥çš„è¶…æ—¶æ—¶é—´æˆ–é‡è¯•æ¬¡æ•°ï¼Œé¿å…å› ç¬æ—¶é«˜è´Ÿè½½å¯¼è‡´APIServeré€€å‡ºã€‚

---

## Issue #124443 Windows: Container log rotation may fail if the container logs are followed

- Issue é“¾æ¥ï¼š[#124443](https://github.com/kubernetes/kubernetes/issues/124443)

### Issue å†…å®¹

#### What happened?

The test ``k8s.io/kubernetes/pkg/kubelet/kuberuntime/logs.TestReadRotatedLog`` is failing on Windows [0]:

```
{Failed  === RUN   TestReadRotatedLog
    logs_test.go:276: 
        	Error Trace:	C:/kubernetes/pkg/kubelet/kuberuntime/logs/logs_test.go:276
        	Error:      	Received unexpected error:
        	            	rename C:\Users\azureuser\AppData\Local\Temp\TestReadRotatedLog1957435032\001\logfile2771403626 C:\Users\azureuser\AppData\Local\Temp\TestReadRotatedLog1957435032\001\logfile2771403626.2220240422-035910: The process cannot access the file because it is being used by another process.
        	Test:       	TestReadRotatedLog
        	Messages:   	failed to rotate log "C:\\Users\\azureuser\\AppData\\Local\\Temp\\TestReadRotatedLog1957435032\\001\\logfile2771403626" to "C:\\Users\\azureuser\\AppData\\Local\\Temp\\TestReadRotatedLog1957435032\\001\\logfile2771403626.2220240422-035910"
--- FAIL: TestReadRotatedLog (0.21s)
```

The idea of the test is to check that kubelet still follows and reads the Container Logs after a Log Rotation occured. However, because the followed file is open, it cannot be renamed, and thus, the container logs cannot be rotated.

This issue could be prevented if the file would be opened with the ``FILE_SHARE_DELETE`` flag set [1]: ``Delete access allows both delete and rename operations.``

[0] https://prow.k8s.io/view/gs/kubernetes-jenkins/logs/ci-kubernetes-unit-windows-master/1782243642258231296
[1] https://learn.microsoft.com/en-us/windows/win32/api/fileapi/nf-fileapi-createfilea

#### What did you expect to happen?

Kubelet should not block container log rotation.

#### How can we reproduce it (as minimally and precisely as possible)?

Run the test ``k8s.io/kubernetes/pkg/kubelet/kuberuntime/logs.TestReadRotatedLog`` is failing on Windows.

#### Anything else we need to know?

/sig windows

/kind bug
/kind failing-test

#### Kubernetes version

latest, master. This unit test has been failing for a while.


#### Cloud provider

N/A


#### OS version

Windows nodes

</details>


#### Install tools

N/A


#### Container runtime (CRI) and version (if applicable)

N/A


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

N/A


### åˆ†æç»“æœ

è¯¥Issueæ¶‰åŠæ½œåœ¨çš„å®‰å…¨é£é™©ï¼Œå…·ä½“åŸå› å’Œå¯èƒ½çš„å½±å“å¦‚ä¸‹ï¼š

**åŸå› åˆ†æï¼š**

åœ¨Windowsç¯å¢ƒä¸‹ï¼ŒKubernetesçš„kubeletåœ¨è¯»å–å®¹å™¨æ—¥å¿—æ—¶ï¼Œå¦‚æœæœªä½¿ç”¨`FILE_SHARE_DELETE`æ ‡å¿—æ‰“å¼€æ—¥å¿—æ–‡ä»¶ï¼Œä¼šå¯¼è‡´è¯¥æ—¥å¿—æ–‡ä»¶è¢«ç‹¬å é”å®šã€‚å½“æ—¥å¿—è½®è½¬æœºåˆ¶å°è¯•é‡å‘½åæˆ–åˆ é™¤è¯¥æ—¥å¿—æ–‡ä»¶æ—¶ï¼Œç”±äºæ–‡ä»¶è¢«kubeletå ç”¨ï¼Œæ“ä½œå°†å¤±è´¥ï¼ˆæ­£å¦‚é”™è¯¯ä¿¡æ¯æ‰€ç¤ºï¼šâ€œThe process cannot access the file because it is being used by another process.â€ï¼‰ã€‚

**å¯èƒ½çš„å½±å“ï¼š**

1. **ç£ç›˜ç©ºé—´è€—å°½ï¼ˆæ‹’ç»æœåŠ¡ï¼‰ï¼š** å¦‚æœæ—¥å¿—æ–‡ä»¶æ— æ³•è½®è½¬ï¼Œæ—¥å¿—å°†æŒç»­å†™å…¥åˆ°åŒä¸€ä¸ªæ–‡ä»¶ï¼Œå¯¼è‡´è¯¥æ–‡ä»¶ä¸æ–­å¢å¤§ã€‚æ”»å‡»è€…å¯ä»¥é€šè¿‡è§¦å‘å®¹å™¨äº§ç”Ÿå¤§é‡æ—¥å¿—ï¼ˆä¾‹å¦‚å‘é€å¤§é‡è¯·æ±‚ã€æ³¨å…¥å¼‚å¸¸æ•°æ®ç­‰æ–¹å¼ï¼‰ï¼ŒåŠ é€Ÿæ—¥å¿—æ–‡ä»¶çš„å¢é•¿ï¼Œæœ€ç»ˆå¯èƒ½å¯¼è‡´èŠ‚ç‚¹çš„ç£ç›˜ç©ºé—´è€—å°½ã€‚è¿™ä¼šå¯¼è‡´èŠ‚ç‚¹ä¸Šçš„æœåŠ¡ä¸å¯ç”¨ï¼Œæ–°çš„å®¹å™¨æ— æ³•å¯åŠ¨ï¼Œé€ æˆæ‹’ç»æœåŠ¡ï¼ˆDenial of Serviceï¼ŒDoSï¼‰æ”»å‡»ã€‚

2. **æ—¥å¿—ç®¡ç†å¤±æ•ˆï¼š** æ— æ³•è½®è½¬çš„æ—¥å¿—å¯èƒ½å¯¼è‡´æ—¥å¿—ç®¡ç†ç­–ç•¥å¤±æ•ˆï¼Œå½±å“ç³»ç»Ÿçš„ç›‘æ§å’Œæ•…éšœæ’æŸ¥ã€‚

**åˆ©ç”¨æ–¹å¼ï¼ˆProof of Conceptï¼‰ï¼š**

1. **éƒ¨ç½²æ¶æ„å®¹å™¨ï¼š** æ”»å‡»è€…åœ¨é›†ç¾¤ä¸­éƒ¨ç½²ä¸€ä¸ªå®¹å™¨ï¼Œè¯¥å®¹å™¨æŒç»­äº§ç”Ÿå¤§é‡æ—¥å¿—è¾“å‡ºã€‚

2. **è§¦å‘æ—¥å¿—è½®è½¬å¤±è´¥ï¼š** ç”±äºkubeletåœ¨æœªä½¿ç”¨`FILE_SHARE_DELETE`æ ‡å¿—çš„æƒ…å†µä¸‹è·Ÿè¸ªæ—¥å¿—æ–‡ä»¶ï¼Œæ—¥å¿—è½®è½¬æœºåˆ¶æ— æ³•æ­£å¸¸å·¥ä½œï¼Œæ—¥å¿—æ–‡ä»¶æ— æ³•è¢«é‡å‘½åæˆ–åˆ é™¤ã€‚

3. **è€—å°½ç£ç›˜ç©ºé—´ï¼š** æŒç»­çš„æ—¥å¿—è¾“å‡ºå¯¼è‡´æ—¥å¿—æ–‡ä»¶æ— é™å¢å¤§ï¼Œæœ€ç»ˆè€—å°½èŠ‚ç‚¹çš„ç£ç›˜ç©ºé—´ã€‚

4. **å¯¼è‡´æœåŠ¡ä¸å¯ç”¨ï¼š** ä¸€æ—¦ç£ç›˜ç©ºé—´è€—å°½ï¼ŒèŠ‚ç‚¹ä¸Šçš„å…³é”®æœåŠ¡å¯èƒ½å´©æºƒï¼Œæ–°å®¹å™¨æ— æ³•è°ƒåº¦ï¼Œç°æœ‰å®¹å™¨å¯èƒ½æ— æ³•æ­£å¸¸è¿è¡Œï¼Œé€ æˆå…¨å±€æ€§çš„æœåŠ¡ä¸å¯ç”¨ã€‚

**CVSS 3.1è¯„åˆ†ï¼š**

- **æ”»å‡»å‘é‡ï¼ˆAVï¼‰ï¼šç½‘ç»œï¼ˆNetworkï¼‰**
- **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰ï¼šä½ï¼ˆLowï¼‰**
- **ç‰¹æƒè¦æ±‚ï¼ˆPRï¼‰ï¼šä½ï¼ˆLowï¼‰**ï¼ˆéœ€è¦éƒ¨ç½²æˆ–è®¿é—®å®¹å™¨çš„æƒé™ï¼‰
- **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰ï¼šæ— ï¼ˆNoneï¼‰**
- **å½±å“èŒƒå›´ï¼ˆSï¼‰ï¼šæœªæ”¹å˜ï¼ˆUnchangedï¼‰**
- **æœºå¯†æ€§ï¼ˆCï¼‰ï¼šæ— å½±å“ï¼ˆNoneï¼‰**
- **å®Œæ•´æ€§ï¼ˆIï¼‰ï¼šæ— å½±å“ï¼ˆNoneï¼‰**
- **å¯ç”¨æ€§ï¼ˆAï¼‰ï¼šé«˜ï¼ˆHighï¼‰**

æ ¹æ®ä¸Šè¿°æŒ‡æ ‡ï¼ŒCVSS è¯„åˆ†ä¸º **7.5ï¼ˆHighï¼‰**ã€‚

**ç»“è®ºï¼š**

è¯¥é—®é¢˜ç¬¦åˆé£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **å¯è¢«æ”»å‡»è€…åˆ©ç”¨ï¼š** æ”»å‡»è€…å¯ä»¥é€šè¿‡è§¦å‘å¤§é‡æ—¥å¿—è¾“å‡ºï¼Œåˆ©ç”¨kubeletå¯¹æ—¥å¿—æ–‡ä»¶çš„ç‹¬å é”ï¼Œå¯¼è‡´æ—¥å¿—è½®è½¬å¤±è´¥ã€‚

2. **å¯èƒ½æˆä¸ºæ¼æ´å¹¶è¢«åˆ†é…CVEç¼–å·ï¼š** ç”±äºå¯èƒ½å¯¼è‡´æ‹’ç»æœåŠ¡ä¸”CVSSè¯„åˆ†è¾¾åˆ°Highï¼Œç¬¦åˆåˆ†é…CVEçš„æ ‡å‡†ã€‚

å› æ­¤ï¼Œè¯¥Issueæ¶‰åŠæ½œåœ¨çš„å®‰å…¨é£é™©ï¼Œå»ºè®®å°½å¿«ä¿®å¤ã€‚åœ¨æ–‡ä»¶æ‰“å¼€æ—¶åº”ä½¿ç”¨`FILE_SHARE_DELETE`æ ‡å¿—ï¼Œå…è®¸æ–‡ä»¶åœ¨è¢«å ç”¨æ—¶ä»å¯è¢«é‡å‘½åæˆ–åˆ é™¤ï¼Œä»¥ç¡®ä¿æ—¥å¿—è½®è½¬æœºåˆ¶æ­£å¸¸å·¥ä½œï¼Œé˜²æ­¢æ½œåœ¨çš„æ‹’ç»æœåŠ¡æ”»å‡»ã€‚

---

## Issue #124436 Quota scopes cannot handle the transition case from one scope to another when the target object is updated.

- Issue é“¾æ¥ï¼š[#124436](https://github.com/kubernetes/kubernetes/issues/124436)

### Issue å†…å®¹

#### What happened?

I create 2 quotas with different scopes, one is Terminating, and the other is NotTerminating. I create a pod that belongs to NotTerminating, then update the pod's spec.activeDeadlineSeconds with 5, the pod should belong to Terminating. But the quota is not updated.


```shell
Every 2.0s: kubectl get pod,quota -A

NAMESPACE     NAME                          READY   STATUS    RESTARTS   AGE
default       pod/test-pod                  0/1     Pending   0          28s
kube-system   pod/coredns-58cd89f5d-wtpxv   0/1     Pending   0          5m45s

NAMESPACE   NAME                               AGE     REQUEST           LIMIT
default     resourcequota/not-terminating   5m16s   count/pods: 1/1
default     resourcequota/terminating          5m16s   count/pods: 0/1
```

#### What did you expect to happen?

Quota scopes can handle the transition case from one scope to another when the target object is updated.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create 2 quotas.
  ```yaml
  apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: terminating
  spec:
    hard:
      count/pods: "1"
    scopes:
    - Terminating
  ---
  apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: not-terminating
  spec:
    hard:
      count/pods: "1"
    scopes:
    - NotTerminating
  ```
2. Create a pod which belongs to not-terminating
  ```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    name: test-pod
  spec:
   
    containers:
    - name: test
      image: ubuntu
      command: ["/bin/sh"]
      args: ["-c", "while true; do echo hello; sleep 10;done"]
  ```
3. Update pod's spec.activeDeadlineSeconds with 5. the pods should belong to terminating
  ```shell
  kubectl patch pod test-pod -p '{"spec":{"activeDeadlineSeconds":5}}'
  ```
4. Get all quotas, you will see the terminating quota is not updated.
  ```shell
  kubectl get quota -A
  ```

#### Anything else we need to know?

The root cause is, when the pod is updated, the delta is calculated based on the old object and the new object, it doesn't consider the scope change where the updated object belongs to, so the final delta is not correct.

In this case, the final delta is zero, so the terminating quota is not updated.

Related k/k code: 

https://github.com/kubernetes/kubernetes/blob/5a31a46d4bfe1fcf5962fa9cee23ff3148a094e3/staging/src/k8s.io/apiserver/pkg/admission/plugin/resourcequota/controller.go#L495-L558

When I implement https://github.com/kubernetes/kubernetes/pull/124360, I find this issue.

#### Kubernetes version

<details>

```console
(base) (âˆ|local-up-cluster:N/A)âœ  __testdata git:(kep-3751-quota-2) âœ— kubectl version
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.0-alpha.0.47+5a31a46d4bfe1f-dirty
```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

å­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

**åŸå› åŠå¯èƒ½çš„å½±å“ï¼š**

è¯¥é—®é¢˜æ¶‰åŠ Kubernetes ä¸­ `ResourceQuota` åœ¨å¯¹è±¡æ›´æ–°æ—¶æœªèƒ½æ­£ç¡®å¤„ç†å¯¹è±¡ä»ä¸€ä¸ª `scope` è½¬æ¢åˆ°å¦ä¸€ä¸ª `scope` çš„æƒ…å†µï¼Œå¯¼è‡´é…é¢ç»Ÿè®¡æœªæ­£ç¡®æ›´æ–°ã€‚

æ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¿™ä¸€æ¼æ´ï¼Œé€šè¿‡åˆ›å»ºä¸€ä¸ªå±äºæŸä¸ª `ResourceQuota` èŒƒå›´ï¼ˆå¦‚ `NotTerminating`ï¼‰çš„å¯¹è±¡ï¼Œç„¶åæ›´æ–°è¯¥å¯¹è±¡ä½¿å…¶è½¬ç§»åˆ°å¦ä¸€ä¸ªèŒƒå›´ï¼ˆå¦‚ `Terminating`ï¼‰ï¼Œè€Œé…é¢ç»Ÿè®¡æœªèƒ½æ­£ç¡®åæ˜ è¿™ä¸€å˜åŒ–ã€‚è¿™æ ·ï¼Œæ”»å‡»è€…å¯ä»¥ç»•è¿‡èµ„æºé…é¢é™åˆ¶ï¼Œæ¶ˆè€—è¶…è¿‡é…é¢çš„èµ„æºã€‚

**å¯èƒ½çš„å½±å“åŒ…æ‹¬ï¼š**

1. **èµ„æºè€—å°½ï¼ˆResource Exhaustionï¼‰ï¼š** æ”»å‡»è€…å¯ä»¥åˆ›å»ºå¹¶æ›´æ–°å¤§é‡çš„å¯¹è±¡ï¼Œå¯¼è‡´é›†ç¾¤çš„èµ„æºè¢«å¤§é‡å ç”¨ï¼Œå½±å“å…¶ä»–æ­£å¸¸ç”¨æˆ·çš„ä½¿ç”¨ã€‚

2. **æ‹’ç»æœåŠ¡ï¼ˆDenial of Serviceï¼‰ï¼š** ç”±äºèµ„æºè¢«è€—å°½ï¼Œé›†ç¾¤å¯èƒ½æ— æ³•ä¸ºå…¶ä»–ç”¨æˆ·æä¾›æœåŠ¡ï¼Œå¯¼è‡´æœåŠ¡ä¸­æ–­ã€‚

**ç¬¦åˆé£é™©åˆ¤æ–­æ ‡å‡†ï¼š**

1. **å¯è¢«æ”»å‡»è€…åˆ©ç”¨ï¼š** æ”»å‡»è€…å¯ä»¥é€šè¿‡æ›´æ–°å¯¹è±¡çš„æ–¹å¼ï¼Œåˆ©ç”¨è¯¥æ¼æ´ç»•è¿‡èµ„æºé…é¢é™åˆ¶ã€‚

2. **å¯èƒ½æˆä¸ºæ¼æ´å¹¶è¢«åˆ†é… CVE ç¼–å·ï¼Œä¸” CVSS 3.1 è¯„åˆ†ä¸ºé«˜å±ä»¥ä¸Šï¼š**

   æ ¹æ® CVSS 3.1 è¯„åˆ†æ ‡å‡†ï¼š

   - **æ”»å‡»å‘é‡ï¼ˆAVï¼‰ï¼šç½‘ç»œï¼ˆNï¼‰**
   - **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰ï¼šä½ï¼ˆLï¼‰**
   - **ç‰¹æƒè¦æ±‚ï¼ˆPRï¼‰ï¼šä½ï¼ˆLï¼‰**ï¼ˆéœ€è¦èƒ½å¤Ÿåˆ›å»ºå’Œæ›´æ–°å¯¹è±¡çš„æƒé™ï¼‰
   - **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰ï¼šæ— ï¼ˆNï¼‰**
   - **ä½œç”¨åŸŸï¼ˆSï¼‰ï¼šå·²æ”¹å˜ï¼ˆCï¼‰**ï¼ˆå½±å“å…¶ä»–ç”¨æˆ·å’Œç³»ç»Ÿèµ„æºï¼‰
   - **æœºå¯†æ€§å½±å“ï¼ˆCï¼‰ï¼šæ— ï¼ˆNï¼‰**
   - **å®Œæ•´æ€§å½±å“ï¼ˆIï¼‰ï¼šæ— ï¼ˆNï¼‰**
   - **å¯ç”¨æ€§å½±å“ï¼ˆAï¼‰ï¼šé«˜ï¼ˆHï¼‰**

   é€šè¿‡è®¡ç®—ï¼ŒCVSS åŸºæœ¬è¯„åˆ†ä¸º **8.1ï¼ˆé«˜ï¼‰**ã€‚

**Proof of Conceptï¼š**

æŒ‰ç…§ Issue ä¸­çš„æè¿°ï¼Œæ”»å‡»è€…å¯ä»¥æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š

1. **åˆ›å»ºä¸¤ä¸ªå…·æœ‰ä¸åŒ `scope` çš„ ResourceQuotaï¼š**

   ```yaml
   apiVersion: v1
   kind: ResourceQuota
   metadata:
     name: terminating
   spec:
     hard:
       count/pods: "1"
     scopes:
     - Terminating
   ---
   apiVersion: v1
   kind: ResourceQuota
   metadata:
     name: not-terminating
   spec:
     hard:
       count/pods: "1"
     scopes:
     - NotTerminating
   ```

2. **åˆ›å»ºä¸€ä¸ªå±äº `NotTerminating` scope çš„ Podï¼š**

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: test-pod
   spec:
     containers:
     - name: test
       image: ubuntu
       command: ["/bin/sh"]
       args: ["-c", "while true; do echo hello; sleep 10;done"]
   ```

3. **æ›´æ–° Podï¼Œä½¿å…¶è½¬æ¢åˆ° `Terminating` scopeï¼š**

   ```shell
   kubectl patch pod test-pod -p '{"spec":{"activeDeadlineSeconds":5}}'
   ```

4. **ç”±äºé…é¢ç»Ÿè®¡æœªæ­£ç¡®æ›´æ–°ï¼Œæ”»å‡»è€…å¯ä»¥é‡å¤ä¸Šè¿°æ­¥éª¤ï¼Œåˆ›å»ºæ›´å¤šçš„ Podï¼Œç»•è¿‡èµ„æºé…é¢é™åˆ¶ã€‚**

**ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥é—®é¢˜å­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ï¼Œå¯èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨å¯¼è‡´èµ„æºè€—å°½å’Œæ‹’ç»æœåŠ¡ç­‰ä¸¥é‡åæœï¼Œéœ€è¦å°½å¿«ä¿®å¤ã€‚**

---

## Issue #124405 Kubelet eviction grace period overridden by Pod.Spec

- Issue é“¾æ¥ï¼š[#124405](https://github.com/kubernetes/kubernetes/issues/124405)

### Issue å†…å®¹

#### What happened?

Kubelet-issued eviction always respects Pod.Spec.TerminationGracePeriodSeconds.

#### What did you expect to happen?

Kubelet-issued eviction should NOT respect Pod.Spec.TerminationGracePeriodSeconds, which could be super long.

#### How can we reproduce it (as minimally and precisely as possible)?

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: foo
spec:
  containers:
  - name: busybox
    image: busybox:latest
    imagePullPolicy: IfNotPresent
    command: ['sh', '-c', 'echo Running ; sleep 3600']
    volumeMounts:
    - name: empty-dir-vol
      mountPath: /var/empty-dir
  volumes:
  - emptyDir:
      sizeLimit: "1Mi"
    name: empty-dir-vol
  terminationGracePeriodSeconds: 3600
```

- Create a Pod with the above Spec. 
- Then shell onto the Pod and execute `cd /var/empty-dir; dd if=/dev/zero of=largefile.txt bs=1M count=10` to create a file exceeds the volume limit.
- Observe the eviction being fired and Pod not killed.

#### Anything else we need to know?

Although the eviction manager [set the grace period to be 0](https://github.com/kubernetes/kubernetes/blob/69b648a1d7074cbe004bf7adb3cdb17f01a4e9d8/pkg/kubelet/eviction/eviction_manager.go#L528C10-L528C18) in most cases, the value is always [overriden to Pod.Spec.TerminationGracePeriodSeconds](https://github.com/kubernetes/kubernetes/blob/69b648a1d7074cbe004bf7adb3cdb17f01a4e9d8/pkg/kubelet/pod_workers.go#L998).

#### Kubernetes version

N/A

#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

è¯¥Issueæè¿°äº†Kubeletåœ¨è¿›è¡ŒPodé©±é€ï¼ˆevictionï¼‰æ—¶ï¼Œå§‹ç»ˆéµå¾ª`Pod.Spec.TerminationGracePeriodSeconds`æŒ‡å®šçš„ç»ˆæ­¢å®½é™æœŸã€‚ç”±äºPodå¯ä»¥å°†è¯¥å€¼è®¾ç½®ä¸ºä¸€ä¸ªéå¸¸å¤§çš„æ•°å€¼ï¼ˆä¾‹å¦‚3600ç§’ï¼‰ï¼Œå½“å‡ºç°éœ€è¦é©±é€çš„æƒ…å†µæ—¶ï¼Œå³ä½¿Kubeletå¸Œæœ›ç«‹å³ç»ˆæ­¢Podï¼Œå®é™…çš„ç»ˆæ­¢æ—¶é—´ä¹Ÿä¼šè¢«å»¶é•¿ã€‚

**æ½œåœ¨çš„å®‰å…¨é£é™©**

1. **è¢«æ”»å‡»è€…åˆ©ç”¨é˜»ç¢èµ„æºå›æ”¶**ï¼šæ”»å‡»è€…å¯ä»¥åˆ›å»ºå¸¦æœ‰è¶…é•¿`terminationGracePeriodSeconds`çš„Podï¼Œå¹¶æ•…æ„è§¦å‘èµ„æºé™åˆ¶ï¼ˆå¦‚è¶…å‡ºå­˜å‚¨é™åˆ¶ï¼‰ï¼Œä½¿å¾—è¿™äº›Podéœ€è¦è¢«é©±é€ã€‚ç„¶è€Œï¼Œç”±äºè¶…é•¿çš„ç»ˆæ­¢å®½é™æœŸï¼Œè¿™äº›Podæ— æ³•è¢«åŠæ—¶ç»ˆæ­¢ï¼ŒæŒç»­å ç”¨èµ„æºã€‚

2. **å¯¼è‡´èµ„æºè€—å°½ï¼ˆæ‹’ç»æœåŠ¡æ”»å‡»ï¼‰**ï¼šé€šè¿‡ä¸Šè¿°æ–¹å¼ï¼Œæ”»å‡»è€…å¯ä»¥å ç”¨å¤§é‡èŠ‚ç‚¹æˆ–é›†ç¾¤èµ„æºï¼Œé˜»ç¢å…¶ä»–Podçš„æ­£å¸¸è°ƒåº¦å’Œè¿è¡Œï¼Œå½±å“æœåŠ¡çš„å¯ç”¨æ€§ã€‚

3. **ç»•è¿‡èµ„æºé™åˆ¶ç­–ç•¥**ï¼šæ”»å‡»è€…åˆ©ç”¨è¿™ä¸€è¡Œä¸ºï¼Œå¯ä»¥åœ¨è¿åèµ„æºé™åˆ¶çš„æƒ…å†µä¸‹ï¼Œå»¶é•¿Podçš„å­˜æ´»æ—¶é—´ï¼Œå½±å“é›†ç¾¤çš„èµ„æºç®¡ç†ç­–ç•¥ã€‚

**å¯èƒ½çš„å½±å“**

- **èµ„æºè€—å°½**ï¼šå…¶ä»–æœåŠ¡å¯èƒ½å› ä¸ºèµ„æºè¢«æ¶æ„å ç”¨è€Œæ— æ³•è·å–æ‰€éœ€èµ„æºï¼Œå¯¼è‡´æœåŠ¡é™çº§æˆ–ä¸å¯ç”¨ã€‚

- **æœåŠ¡ä¸­æ–­**ï¼šå…³é”®åº”ç”¨å¯èƒ½å—åˆ°å½±å“ï¼Œé€ æˆä¸šåŠ¡è¿ç»­æ€§é—®é¢˜å’Œç»æµæŸå¤±ã€‚

- **å®‰å…¨åˆè§„é£é™©**ï¼šæœªèƒ½åŠæ—¶é©±é€è¿è§„Podï¼Œå¯èƒ½è¿åç»„ç»‡çš„å®‰å…¨ç­–ç•¥å’Œåˆè§„è¦æ±‚ã€‚

æ ¹æ®CVSS 3.1è¯„åˆ†æ ‡å‡†åˆæ­¥è¯„ä¼°ï¼š

- **æ”»å‡»å‘é‡ï¼ˆAVï¼‰**ï¼šç½‘ç»œï¼ˆNï¼‰
- **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰**ï¼šä½ï¼ˆLï¼‰
- **ç‰¹æƒè¦æ±‚ï¼ˆPRï¼‰**ï¼šä½ï¼ˆLï¼‰â€”â€”éœ€è¦æœ‰åˆ›å»ºPodçš„æƒé™
- **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰**ï¼šæ— ï¼ˆNï¼‰
- **ä½œç”¨èŒƒå›´ï¼ˆSï¼‰**ï¼šæœªæ”¹å˜ï¼ˆUï¼‰
- **æœºå¯†æ€§ï¼ˆCï¼‰**ï¼šæ— å½±å“ï¼ˆNï¼‰
- **å®Œæ•´æ€§ï¼ˆIï¼‰**ï¼šæ— å½±å“ï¼ˆNï¼‰
- **å¯ç”¨æ€§ï¼ˆAï¼‰**ï¼šé«˜ï¼ˆHï¼‰

ç»¼åˆå¾—åˆ†ä¸º**7.5ï¼ˆé«˜ï¼‰**ã€‚

**Proof of Concept**

æŒ‰ç…§Issueä¸­æä¾›çš„æ­¥éª¤ï¼š

1. **åˆ›å»ºå…·æœ‰è¶…é•¿ç»ˆæ­¢å®½é™æœŸçš„Pod**

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: exploit-pod
   spec:
     containers:
     - name: busybox
       image: busybox:latest
       command: ['sh', '-c', 'sleep 3600']
       volumeMounts:
       - name: empty-dir-vol
         mountPath: /var/empty-dir
     volumes:
     - name: empty-dir-vol
       emptyDir:
         sizeLimit: "1Mi"
     terminationGracePeriodSeconds: 3600
   ```

2. **è§¦å‘èµ„æºé™åˆ¶**

   è¿›å…¥Podå†…éƒ¨ï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ä»¥è¶…å‡ºEmptyDirçš„å­˜å‚¨é™åˆ¶ï¼š

   ```shell
   cd /var/empty-dir
   dd if=/dev/zero of=largefile.txt bs=1M count=10
   ```

3. **è§‚å¯Ÿç»“æœ**

   - Kubeletæ£€æµ‹åˆ°å­˜å‚¨èµ„æºå‹åŠ›ï¼Œè§¦å‘é©±é€è¿‡ç¨‹ã€‚
   - ç”±äº`terminationGracePeriodSeconds`è¢«è®¾ç½®ä¸º3600ç§’ï¼ŒPodä¸ä¼šç«‹å³ç»ˆæ­¢ï¼Œè€Œæ˜¯ç»§ç»­è¿è¡Œï¼ŒæŒç»­å ç”¨èµ„æºã€‚
   - å¦‚æœå¤§é‡æ­¤ç±»Podè¢«åˆ›å»ºï¼ŒèŠ‚ç‚¹èµ„æºå°†è¢«è€—å°½ï¼Œå½±å“å…¶ä»–Podçš„æ­£å¸¸è¿è¡Œã€‚

**æ€»ç»“**

è¯¥Issueå±•ç¤ºäº†ä¸€ä¸ªå¯ä»¥è¢«æ”»å‡»è€…åˆ©ç”¨çš„æ¼æ´ï¼Œé€šè¿‡è®¾ç½®è¶…é•¿çš„`terminationGracePeriodSeconds`ï¼Œé˜»ç¢KubeletåŠæ—¶é©±é€è¿è§„Podï¼Œå¯¼è‡´èµ„æºè¢«æ¶æ„å ç”¨ï¼Œå¯èƒ½é€ æˆæ‹’ç»æœåŠ¡æ”»å‡»ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤é—®é¢˜æ¶‰åŠæ½œåœ¨çš„å®‰å…¨é£é™©ï¼Œä¸”å¯èƒ½è¢«åˆ†é…CVEç¼–å·ï¼Œå»ºè®®åŠæ—¶ä¿®å¤ã€‚

---

## Issue #124388 Repeated Pod creation and eviction during DaemonSet rolling update(surge > 1) when Node is under pressure

- Issue é“¾æ¥ï¼š[#124388](https://github.com/kubernetes/kubernetes/issues/124388)

### Issue å†…å®¹

#### What happened?

When the update strategy for a DaemonSet in Kubernetes is set to RollingUpdate and `maxSurge` is greater than 1, I've noticed an issue where, if a Node's status is under pressure, the system repeatedly creates and then evicts Pods. This behavior creates a lot of unnecessary churn and could potentially cause problems with system resources. 

#### What did you expect to happen?

like the behavior when option `maxSurge` is 0

#### How can we reproduce it (as minimally and precisely as possible)?

produce a diskpressure, and then deploy this
```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-daemonset
spec:
  selector:
    matchLabels:
      name: nginx
  template:
    metadata:
      labels:
        name: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
  updateStrategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
```

#### Anything else we need to know?

Actually, I've found the problem.

When `maxSurge == 1`, these [codes](https://github.com/kubernetes/kubernetes/blob/c4bce63d9886e5f1fc00f8c3b5a13ea0d2bdf772/pkg/controller/daemon/daemon_controller.go#L799-L820) work perfectly. Because we run into [this](https://github.com/kubernetes/kubernetes/blob/c4bce63d9886e5f1fc00f8c3b5a13ea0d2bdf772/pkg/controller/daemon/daemon_controller.go#L832-L844) and then directly return.

But when `maxSurge > 1`, it passes that, and run into [this](https://github.com/kubernetes/kubernetes/blob/c4bce63d9886e5f1fc00f8c3b5a13ea0d2bdf772/pkg/controller/daemon/daemon_controller.go#L846-L853). Badly, it causes node be added to `nodesNeedingDaemonPods`.

Maybe if we are `inBackoff`, we should break directly ?


#### Kubernetes version

main

#### Cloud provider

no


#### OS version

_No response_

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

æ ¹æ®æ‚¨æä¾›çš„Issueå†…å®¹ï¼Œå­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

**åŸå› åˆ†æï¼š**

å½“èŠ‚ç‚¹ï¼ˆNodeï¼‰å¤„äºå‹åŠ›çŠ¶æ€ï¼ˆä¾‹å¦‚ç£ç›˜å‹åŠ›ï¼‰æ—¶ï¼Œå¦‚æœDaemonSetçš„æ›´æ–°ç­–ç•¥ä¸ºRollingUpdateï¼Œä¸”`maxSurge`è®¾ç½®å¤§äº1ï¼ŒKubernetesä¼šåœ¨è¯¥èŠ‚ç‚¹ä¸Šåå¤åˆ›å»ºå’Œé©±é€Podã€‚è¿™ç§è¡Œä¸ºä¼šå¯¼è‡´ç³»ç»Ÿèµ„æºè¢«å¤§é‡æ¶ˆè€—ï¼Œäº§ç”Ÿä¸å¿…è¦çš„è´Ÿè½½ã€‚

æ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¿™ä¸€æœºåˆ¶ï¼Œé€šè¿‡äººä¸ºå¯¼è‡´èŠ‚ç‚¹è¿›å…¥å‹åŠ›çŠ¶æ€ï¼ˆå¦‚æ¶ˆè€—ç£ç›˜ç©ºé—´æˆ–I/Oï¼‰ï¼Œç»“åˆDaemonSetçš„æ»šåŠ¨æ›´æ–°ç­–ç•¥ï¼Œè¯±å¯¼ç³»ç»Ÿé¢‘ç¹åˆ›å»ºå’Œåˆ é™¤Podï¼Œå¯¼è‡´èµ„æºè€—å°½ï¼Œé€ æˆæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ã€‚

**å¯èƒ½çš„å½±å“ï¼š**

- **èµ„æºè€—å°½**ï¼šåå¤çš„Podåˆ›å»ºå’Œé©±é€ä¼šæ¶ˆè€—èŠ‚ç‚¹å’Œé›†ç¾¤çš„è®¡ç®—å’Œå­˜å‚¨èµ„æºã€‚
- **æœåŠ¡ä¸­æ–­**ï¼šå…³é”®æœåŠ¡å¯èƒ½ç”±äºèµ„æºè¢«è€—å°½è€Œæ— æ³•æ­£å¸¸è¿è¡Œã€‚
- **é›†ç¾¤ä¸ç¨³å®š**ï¼šè¿‡å¤šçš„è°ƒåº¦æ´»åŠ¨å¯èƒ½å½±å“é›†ç¾¤çš„æ•´ä½“æ€§èƒ½å’Œç¨³å®šæ€§ã€‚

**PoCï¼ˆæ¦‚å¿µéªŒè¯ï¼‰ï¼š**

1. **è§¦å‘èŠ‚ç‚¹å‹åŠ›**ï¼šæ”»å‡»è€…åœ¨ç›®æ ‡èŠ‚ç‚¹ä¸Šè¿è¡Œå ç”¨å¤§é‡ç£ç›˜èµ„æºçš„Podï¼Œè§¦å‘ç£ç›˜å‹åŠ›çŠ¶æ€ã€‚
2. **è®¾ç½®DaemonSet**ï¼šåˆ›å»ºä¸€ä¸ª`maxSurge`å¤§äº1çš„DaemonSetï¼Œå¹¶è§¦å‘æ»šåŠ¨æ›´æ–°ã€‚
3. **è§‚å¯Ÿæ•ˆæœ**ï¼šç”±äºèŠ‚ç‚¹å¤„äºå‹åŠ›çŠ¶æ€ï¼ŒDaemonSetä¼šåœ¨è¯¥èŠ‚ç‚¹ä¸Šä¸æ–­å°è¯•åˆ›å»ºæ–°Podå¹¶ç«‹å³é©±é€ï¼Œå½¢æˆèµ„æºæ¶ˆè€—å¾ªç¯ã€‚

**CVSS v3.1è¯„åˆ†ï¼š**

- **æ”»å‡»å‘é‡ï¼ˆAVï¼‰**ï¼šç½‘ç»œï¼ˆNï¼‰
- **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰**ï¼šä½ï¼ˆLï¼‰
- **æƒé™è¦æ±‚ï¼ˆPRï¼‰**ï¼šä½ï¼ˆLï¼‰
- **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰**ï¼šæ— ï¼ˆNï¼‰
- **èŒƒå›´ï¼ˆSï¼‰**ï¼šæœªæ”¹å˜ï¼ˆUï¼‰
- **æœºå¯†æ€§ï¼ˆCï¼‰**ï¼šæ— å½±å“ï¼ˆNï¼‰
- **å®Œæ•´æ€§ï¼ˆIï¼‰**ï¼šæ— å½±å“ï¼ˆNï¼‰
- **å¯ç”¨æ€§ï¼ˆAï¼‰**ï¼šé«˜ï¼ˆHï¼‰

**ç»¼åˆè¯„åˆ†ï¼š**7.5ï¼ˆé«˜ï¼‰

å› æ­¤ï¼Œè¯¥é—®é¢˜å¯èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œé€ æˆæ‹’ç»æœåŠ¡æ”»å‡»ï¼Œå±äºé«˜é£é™©æ¼æ´ï¼Œåº”äºˆä»¥é‡è§†å’Œä¿®å¤ã€‚

---

## Issue #124384 Scheduler throughput reduced when many gated pods

- Issue é“¾æ¥ï¼š[#124384](https://github.com/kubernetes/kubernetes/issues/124384)

### Issue å†…å®¹

#### What happened?

When handling a delete pod event, we attempt to remove pods from the unscheduablePods pool [1], taking a lock while doing so [2]. Gated pods reside in this unscheduable pool, and we process all of them for each delete event. Scheduling throughput is affected, likely because ScheduleOne also takes a lock [3]

[1] https://github.com/kubernetes/kubernetes/blob/00117569f3ab6456791c4e65fc4543a4278d1332/pkg/scheduler/eventhandlers.go#L244-L249

[2] https://github.com/kubernetes/kubernetes/blob/00117569f3ab6456791c4e65fc4543a4278d1332/pkg/scheduler/internal/queue/scheduling_queue.go#L1093-L1097

[3] https://github.com/kubernetes/kubernetes/blob/00117569f3ab6456791c4e65fc4543a4278d1332/pkg/scheduler/schedule_one.go#L69


#### What did you expect to happen?

Scheduling throughput should not reduce when there are many gated pods. We eventually decide against moving these pods, but after some processing. https://github.com/kubernetes/kubernetes/blob/be4b7176dc131ea842cab6882cd4a06dbfeed12a/pkg/scheduler/internal/queue/scheduling_queue.go#L1178-L1183

#### How can we reproduce it (as minimally and precisely as possible)?

Create many (~10000) gated pods. Then, create non-gated pods which eventually terminate. As these pods finish and pod delete events are processed, scheduling throughput decreases

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Server Version: v1.29.1-gke.1589017
```
</details>


#### Cloud provider

<details>
Google Cloud - GKE
</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

å­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

**åŸå› åŠå¯èƒ½çš„å½±å“ï¼š**

æ ¹æ®é—®é¢˜æè¿°ï¼Œå½“å¤„ç†åˆ é™¤ pod çš„äº‹ä»¶æ—¶ï¼Œå¦‚æœå­˜åœ¨å¤§é‡çš„ gated podsï¼ˆçº¦ 10000 ä¸ªï¼‰ï¼Œè°ƒåº¦å™¨ä¼šåœ¨å°è¯•ä» `unschedulablePods` æ± ä¸­ç§»é™¤ pods æ—¶æŒæœ‰é”ï¼Œå¹¶å¤„ç†æ‰€æœ‰çš„ gated podsã€‚è¿™ä¼šå¯¼è‡´è°ƒåº¦å™¨çš„ååé‡æ˜¾è‘—ä¸‹é™ã€‚

æ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¿™ä¸€æ¼æ´ï¼Œé€šè¿‡åˆ›å»ºå¤§é‡çš„ gated podsï¼Œä½¿å¾—è°ƒåº¦å™¨çš„æ€§èƒ½ä¸‹é™ï¼Œå¯¼è‡´æ–°åˆ›å»ºçš„ pods æ— æ³•åŠæ—¶è°ƒåº¦ï¼Œä»è€Œå®ç°å¯¹é›†ç¾¤çš„æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚è¿™ç§æ”»å‡»æ–¹å¼å°¤å…¶åœ¨å¤šç§Ÿæˆ·ç¯å¢ƒä¸‹å…·æœ‰é«˜é£é™©ï¼Œå› ä¸ºä½æƒé™ç”¨æˆ·å¯èƒ½æœ‰åˆ›å»º pods çš„æƒé™ï¼Œä»–ä»¬å¯ä»¥æ»¥ç”¨æ­¤æƒé™å¯¹é›†ç¾¤å®æ–½æ”»å‡»ã€‚

**ä¾æ® CVSS 3.1 è¯„åˆ†æ ‡å‡†çš„è¯„ä¼°ï¼š**

- **æ”»å‡»å‘é‡ï¼ˆAVï¼‰ï¼š** ç½‘ç»œï¼ˆNï¼‰â€”â€”æ”»å‡»è€…å¯ä»¥é€šè¿‡ç½‘ç»œè®¿é—® Kubernetes API æ¥å£ã€‚
- **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰ï¼š** ä½ï¼ˆLï¼‰â€”â€”ä¸éœ€è¦ç‰¹æ®Šæ¡ä»¶ï¼Œæ”»å‡»æ–¹æ³•ç®€å•ã€‚
- **æ‰€éœ€æƒé™ï¼ˆPRï¼‰ï¼š** ä½ï¼ˆLï¼‰â€”â€”åªéœ€æ‹¥æœ‰åˆ›å»º pod çš„æƒé™ã€‚
- **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰ï¼š** æ— ï¼ˆNï¼‰â€”â€”ä¸éœ€è¦å…¶ä»–ç”¨æˆ·çš„äº¤äº’ã€‚
- **ä½œç”¨èŒƒå›´ï¼ˆSï¼‰ï¼š** æ”¹å˜ï¼ˆCï¼‰â€”â€”æ”»å‡»å½±å“åˆ°äº†è¶…å‡ºæƒé™èŒƒå›´çš„ç»„ä»¶ï¼ˆè°ƒåº¦å™¨ï¼‰ã€‚
- **æœºå¯†æ€§ï¼ˆCï¼‰ï¼š** æ— ï¼ˆNï¼‰â€”â€”ä¸å½±å“æœºå¯†æ€§ã€‚
- **å®Œæ•´æ€§ï¼ˆIï¼‰ï¼š** æ— ï¼ˆNï¼‰â€”â€”ä¸å½±å“å®Œæ•´æ€§ã€‚
- **å¯ç”¨æ€§ï¼ˆAï¼‰ï¼š** é«˜ï¼ˆHï¼‰â€”â€”ä¸¥é‡å½±å“ç³»ç»Ÿçš„å¯ç”¨æ€§ã€‚

ç»¼åˆä¸Šè¿°æŒ‡æ ‡ï¼ŒCVSS åŸºæœ¬åˆ†ä¸º 7.5ï¼Œè¯„çº§ä¸º **é«˜å±**ã€‚

**æ¦‚å¿µéªŒè¯ï¼ˆProof of Conceptï¼‰ï¼š**

1. **å‡†å¤‡å·¥ä½œï¼š** æ”»å‡»è€…éœ€è¦å…·å¤‡åœ¨ç›®æ ‡ Kubernetes é›†ç¾¤ä¸­åˆ›å»º pods çš„æƒé™ã€‚

2. **æ‰§è¡Œæ­¥éª¤ï¼š**
   - ç¼–å†™ä¸€ä¸ªè„šæœ¬ï¼Œä½¿ç”¨ Kubernetes API æˆ– `kubectl` å‘½ä»¤æ‰¹é‡åˆ›å»ºå¤§é‡çš„ gated podsã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨ä»¥ä¸‹ä¼ªä»£ç ï¼š
     ```bash
     for i in {1..10000}
     do
       kubectl apply -f gated-pod.yaml --name=gated-pod-$i
     done
     ```
     å…¶ä¸­ï¼Œ`gated-pod.yaml` æ˜¯ä¸€ä¸ªåŒ…å« gating ç‰¹æ€§çš„ pod é…ç½®æ–‡ä»¶ã€‚

3. **è§‚å¯Ÿç»“æœï¼š**
   - ä¸€æ—¦å¤§é‡çš„ gated pods è¢«åˆ›å»ºï¼Œè°ƒåº¦å™¨åœ¨å¤„ç† pod åˆ é™¤äº‹ä»¶æ—¶ä¼šéå†æ‰€æœ‰çš„ gated podsï¼Œå¹¶æŒæœ‰å…¨å±€é”ï¼Œå¯¼è‡´è°ƒåº¦å™¨çš„ååé‡ä¸‹é™ã€‚
   - å…¶ä»–ç”¨æˆ·çš„æ–°å»º pods å°†æ— æ³•åŠæ—¶è¢«è°ƒåº¦ï¼Œå‡ºç°è°ƒåº¦å»¶è¿Ÿæˆ–å¤±è´¥çš„æƒ…å†µã€‚

4. **å½±å“éªŒè¯ï¼š**
   - é€šè¿‡ç›‘æ§è°ƒåº¦å™¨çš„æ€§èƒ½æŒ‡æ ‡ï¼Œè§‚å¯Ÿåˆ°è°ƒåº¦å™¨çš„ CPU ä½¿ç”¨ç‡å‡é«˜ï¼Œå¤„ç†é€Ÿåº¦ä¸‹é™ã€‚
   - åº”ç”¨ç¨‹åºçš„æœåŠ¡è´¨é‡ä¸‹é™ï¼Œå¯èƒ½å¯¼è‡´è¶…æ—¶æˆ–æ— æ³•å“åº”ã€‚

**ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥é—®é¢˜æ»¡è¶³è¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œå¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´å¹¶è¢«åˆ†é… CVE ç¼–å·çš„æ¡ä»¶ï¼Œå»ºè®®å°½å¿«ä¿®å¤ã€‚**

---

## Issue #124377 agnhost test image uses out-of-support alpine BASEIMAGEs

- Issue é“¾æ¥ï¼š[#124377](https://github.com/kubernetes/kubernetes/issues/124377)

### Issue å†…å®¹

#### What happened?

The `agnhost` test image currently uses `alpine:3.12` as its BASEIMAGE: https://github.com/kubernetes/kubernetes/blob/e6efba3380c87503f918053c0511587485a2f828/test/images/agnhost/BASEIMAGE#L1-L5

Per https://alpinelinux.org/releases/, v3.12 reached end of support on 2022-05-01 and is no longer receiving regular bugfixes or security fixes. Current versions of the `agnhost` image appear vulnerable to known CVEs as a result.

I don't know if a fix is quite as simple as just https://github.com/riendeau/kubernetes/commit/274511b6bb1b52934784d3f142e0334df03e0a32?

#### What did you expect to happen?

The `agnhost` image's base should be an in-support version which is maintained with at least security fixes.

#### How can we reproduce it (as minimally and precisely as possible)?

```
$ trivy image registry.k8s.io/e2e-test-images/agnhost:2.48
2024-04-18T08:38:27.831-0500	INFO	Vulnerability scanning is enabled
2024-04-18T08:38:27.831-0500	INFO	Secret scanning is enabled
2024-04-18T08:38:27.831-0500	INFO	If your scanning is slow, please try '--scanners vuln' to disable secret scanning
2024-04-18T08:38:27.831-0500	INFO	Please see also https://aquasecurity.github.io/trivy/v0.50/docs/scanner/secret/#recommendation for faster secret detection
2024-04-18T08:38:28.506-0500	INFO	Detected OS: alpine
2024-04-18T08:38:28.506-0500	INFO	Detecting Alpine vulnerabilities...
2024-04-18T08:38:28.512-0500	INFO	Number of language-specific files: 1
2024-04-18T08:38:28.512-0500	INFO	Detecting gobinary vulnerabilities...
2024-04-18T08:38:28.515-0500	WARN	version error ((devel)): malformed version: (devel)
2024-04-18T08:38:28.515-0500	WARN	version error ((devel)): malformed version: (devel)
2024-04-18T08:38:28.515-0500	WARN	version error ((devel)): malformed version: (devel)
2024-04-18T08:38:28.515-0500	WARN	version error ((devel)): malformed version: (devel)
2024-04-18T08:38:28.515-0500	WARN	version error ((devel)): malformed version: (devel)
2024-04-18T08:38:28.515-0500	WARN	version error ((devel)): malformed version: (devel)
2024-04-18T08:38:28.517-0500	WARN	This OS version is no longer supported by the distribution: alpine 3.12.12
2024-04-18T08:38:28.517-0500	WARN	The vulnerability detection may be insufficient because security updates are not provided

registry.k8s.io/e2e-test-images/agnhost:2.48 (alpine 3.12.12)

Total: 1 (UNKNOWN: 0, LOW: 0, MEDIUM: 0, HIGH: 0, CRITICAL: 1)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Library â”‚ Vulnerability  â”‚ Severity â”‚ Status â”‚ Installed Version â”‚ Fixed Version â”‚                            Title                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ zlib    â”‚ CVE-2022-37434 â”‚ CRITICAL â”‚ fixed  â”‚ 1.2.12-r0         â”‚ 1.2.12-r2     â”‚ zlib: heap-based buffer over-read and overflow in inflate() â”‚
â”‚         â”‚                â”‚          â”‚        â”‚                   â”‚               â”‚ in inflate.c via a...                                       â”‚
â”‚         â”‚                â”‚          â”‚        â”‚                   â”‚               â”‚ https://avd.aquasec.com/nvd/cve-2022-37434                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

...
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

å­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

**åŸå› å’Œå¯èƒ½çš„å½±å“ï¼š**

`agnhost` æµ‹è¯•é•œåƒç›®å‰ä½¿ç”¨äº†å·²åœæ­¢æ”¯æŒçš„ `alpine:3.12` ä½œä¸ºå…¶åŸºç¡€é•œåƒã€‚æ ¹æ® [Alpine Linux çš„å®˜æ–¹å‘å¸ƒä¿¡æ¯](https://alpinelinux.org/releases/)ï¼Œç‰ˆæœ¬ 3.12 å·²äº 2022-05-01 åœæ­¢æ”¯æŒï¼Œä¸å†æ”¶åˆ°å¸¸è§„çš„é”™è¯¯ä¿®å¤æˆ–å®‰å…¨ä¿®å¤ã€‚è¿™æ„å‘³ç€è¯¥é•œåƒä¸­å¯èƒ½åŒ…å«å·²çŸ¥çš„å®‰å…¨æ¼æ´ï¼Œä¸”è¿™äº›æ¼æ´æœªè¢«ä¿®å¤ï¼Œæ”»å‡»è€…å¯èƒ½åˆ©ç”¨è¿™äº›æ¼æ´å¯¹ç³»ç»Ÿå‘èµ·æ”»å‡»ã€‚

é€šè¿‡ä½¿ç”¨ Trivy å¯¹å½“å‰ç‰ˆæœ¬çš„ `agnhost` é•œåƒè¿›è¡Œæ‰«æï¼Œå‘ç°å…¶å­˜åœ¨å·²çŸ¥çš„é«˜å±æ¼æ´ **CVE-2022-37434**ã€‚è¯¥æ¼æ´å­˜åœ¨äº `zlib` åº“ä¸­ï¼Œå±äº **å †ç¼“å†²åŒºæº¢å‡º** æ¼æ´ï¼Œå¯å¯¼è‡´å†…å­˜æŸåï¼Œç”šè‡³å¯èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨æ¥æ‰§è¡Œä»»æ„ä»£ç æˆ–å¯¼è‡´æ‹’ç»æœåŠ¡ï¼ˆç¨‹åºå´©æºƒï¼‰ã€‚

æ ¹æ® CVSS 3.1 è¯„åˆ†æ ‡å‡†ï¼Œ**CVE-2022-37434** çš„è¯„åˆ†ä¸º **8.1ï¼ˆé«˜å±ï¼‰**ï¼Œæ»¡è¶³é£é™©åˆ¤æ–­æ ‡å‡†ä¸­çš„ç¬¬äºŒæ¡ã€‚

**Proof of Conceptï¼ˆæ¦‚å¿µéªŒè¯ï¼‰ï¼š**

1. **æ¼æ´æè¿°ï¼š**

   CVE-2022-37434 æ˜¯ `zlib` åº“ä¸­çš„ä¸€ä¸ªæ¼æ´ï¼Œæ”»å‡»è€…å¯ä»¥é€šè¿‡ç²¾å¿ƒæ„é€ çš„å‹ç¼©æ•°æ®ï¼Œåˆ©ç”¨ `inflate()` å‡½æ•°ä¸­çš„æº¢å‡ºæ¼æ´ï¼Œè§¦å‘å †å†…å­˜æŸåã€‚

2. **åˆ©ç”¨æ­¥éª¤ï¼š**

   - æ”»å‡»è€…æ„é€ ä¸€ä¸ªç‰¹åˆ¶çš„å‹ç¼©æ•°æ®ï¼ˆå¦‚ä¸€ä¸ªæ¶æ„çš„ GZIP æ–‡ä»¶ï¼‰ï¼Œå…¶ä¸­åŒ…å«å¼‚å¸¸çš„å‹ç¼©å—ï¼Œæ—¨åœ¨è§¦å‘ `zlib` ä¸­çš„æº¢å‡ºæ¼æ´ã€‚
   - å°†è¯¥æ¶æ„æ•°æ®é€šè¿‡åº”ç”¨ç¨‹åºçš„è¾“å…¥æ¸ é“å‘é€ç»™ä½¿ç”¨å—å½±å“ `zlib` åº“çš„åº”ç”¨ç¨‹åºã€‚
   - å½“åº”ç”¨ç¨‹åºä½¿ç”¨ `zlib` è§£å‹ç¼©è¯¥æ•°æ®æ—¶ï¼Œè§¦å‘å †ç¼“å†²åŒºæº¢å‡ºï¼Œå¯¼è‡´å†…å­˜æŸåã€‚
   - æ”»å‡»è€…å¯ä»¥åˆ©ç”¨å†…å­˜æŸåå®ç°ä»»æ„ä»£ç æ‰§è¡Œï¼Œæˆ–è€…å¯¼è‡´åº”ç”¨ç¨‹åºå´©æºƒï¼ˆæ‹’ç»æœåŠ¡ï¼‰ã€‚

3. **å½±å“èŒƒå›´ï¼š**

   - å¦‚æœ `agnhost` é•œåƒä¸­çš„åº”ç”¨ç¨‹åºæœ‰è§£å‹ç”¨æˆ·æä¾›çš„æ•°æ®çš„åŠŸèƒ½ï¼ˆä¾‹å¦‚å¤„ç†å‹ç¼©çš„ç½‘ç»œè¯·æ±‚ã€æ–‡ä»¶ä¸Šä¼ ç­‰ï¼‰ï¼Œé‚£ä¹ˆæ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¯¥æ¼æ´è¿›è¡Œè¿œç¨‹ä»£ç æ‰§è¡Œæˆ–æ‹’ç»æœåŠ¡æ”»å‡»ã€‚
   - å³ä½¿ `agnhost` é•œåƒä¸»è¦ç”¨äºæµ‹è¯•ï¼Œå¦‚æœæµ‹è¯•ç¯å¢ƒè¿æ¥åˆ°ç”Ÿäº§ç½‘ç»œï¼Œæ”»å‡»è€…å¯èƒ½åˆ©ç”¨è¯¥æ¼æ´åœ¨æµ‹è¯•ç¯å¢ƒä¸­å–å¾—æ›´é«˜çš„æƒé™ï¼Œè¿›è€Œå¯¹ç”Ÿäº§ç¯å¢ƒé€ æˆå¨èƒã€‚

**å»ºè®®æªæ–½ï¼š**

- **æ›´æ–°åŸºç¡€é•œåƒï¼š** å°† `agnhost` é•œåƒçš„åŸºç¡€é•œåƒä»å·²åœæ­¢æ”¯æŒçš„ `alpine:3.12` æ›´æ–°åˆ°å—æ”¯æŒçš„ç‰ˆæœ¬ï¼Œä¾‹å¦‚æœ€æ–°çš„ `alpine:3.18`ï¼Œä»¥è·å–æœ€æ–°çš„å®‰å…¨ä¿®å¤ã€‚
- **å®‰å…¨æ‰«æï¼š** åœ¨æ„å»ºå’Œå‘å¸ƒé•œåƒçš„è¿‡ç¨‹ä¸­ï¼Œé›†æˆå®‰å…¨æ‰«æå·¥å…·ï¼ˆå¦‚ Trivyï¼‰ï¼Œå®šæœŸæ‰«æé•œåƒä¸­çš„å·²çŸ¥æ¼æ´ï¼ŒåŠæ—¶ä¿®å¤ã€‚
- **æœ€å°åŒ–é•œåƒä¾èµ–ï¼š** ä»…åŒ…å«å¿…è¦çš„åº“å’Œä¾èµ–ï¼Œå‡å°‘æ½œåœ¨æ¼æ´çš„æ•°é‡ã€‚
- **éš”ç¦»æµ‹è¯•ç¯å¢ƒï¼š** ç¡®ä¿æµ‹è¯•ç¯å¢ƒä¸ç”Ÿäº§ç¯å¢ƒéš”ç¦»ï¼Œé™ä½æµ‹è¯•é•œåƒä¸­çš„æ¼æ´å¯¹ç”Ÿäº§ç¯å¢ƒçš„å½±å“ã€‚

---

## Issue #124341 When Topology Aware Hints are disabled, kube-proxy shouldn't spam the logs

- Issue é“¾æ¥ï¼š[#124341](https://github.com/kubernetes/kubernetes/issues/124341)

### Issue å†…å®¹

#### What happened?

We have a mix of smaller and larger clusters - and for our larger clusters it is critical that the [Topology Aware Routing](https://kubernetes.io/docs/concepts/services-networking/topology-aware-routing/) is used on some of our high volume services. it works great, and it's fallback mode is reasonable.

The problem is this line: https://github.com/kubernetes/kubernetes/blob/v1.28.9/pkg/proxy/topology.go#L168-L171

With this line in place, on a reasonably sized cluster, we emit **hundreds of millions** of these log messages daily:

```
I0416 21:50:58.359353       1 topology.go:169] "Skipping topology aware endpoint filtering since one or more endpoints is missing a zone hint"
I0416 21:50:58.360686       1 topology.go:169] "Skipping topology aware endpoint filtering since one or more endpoints is missing a zone hint"
I0416 21:50:58.360944       1 topology.go:169] "Skipping topology aware endpoint filtering since one or more endpoints is missing a zone hint"
I0416 21:50:58.503272       1 topology.go:169] "Skipping topology aware endpoint filtering since one or more endpoints is missing a zone hint"
I0416 21:50:58.504681       1 topology.go:169] "Skipping topology aware endpoint filtering since one or more endpoints is missing a zone hint"
I0416 21:50:58.505831       1 topology.go:169] "Skipping topology aware endpoint filtering since one or more endpoints is missing a zone hint"
```

A LogQL query shows that on one cluster, we had over **1.2 billion log events** from this message:
![image](https://github.com/kubernetes/kubernetes/assets/768067/489c9ce6-223d-44ca-b649-69d57c639f92)

Here's an hour of the data:
![image](https://github.com/kubernetes/kubernetes/assets/768067/621fa43b-f7c6-4b0a-a8cd-6a6e11b6bba6)


#### What did you expect to happen?

I understand that its useful to tell people that the zone aware routing isn't working - but doing it in log messages seems less than useful. I don't know anyone who operates clusters and monitors these log messages for errors, but rather would use metrics to alert on such a behavior.

I expect that the Kubernetes Service will report that Topology Aware Routing is or is not working (which it does) via the `kubectl describe` command - and that's it. Other than that, I expect `kube-proxy` to consider this as some kind of a debug message and not emit it as an `info` level message.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a service with `service.kubernetes.io/topology-mode: auto` in the annotations, and only put up one or two pods.. then start sending traffic to the service. Check logs.

#### Anything else we need to know?

We're going to drop these messages with Promtail filtering ... but we really don't want them at all because it honestly takes time up on the hosts to write the messages to local disk, it also makes the `kubectl logs kube-proxy-...` command nearly useless, and it's just overall a waste of space I think.

#### Kubernetes version

<details>

```console
% kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.7-eks-b9c9ed7
```

</details>


#### Cloud provider

<details>
AWS EKS 1.28
</details>


#### OS version

AWS [Bottlerocket 1.19.2](https://github.com/bottlerocket-os/bottlerocket/releases/tag/v1.19.2)

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

åˆ†æï¼š

1. **èƒ½å¦è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼š

   - åœ¨å¤šç§Ÿæˆ·çš„ Kubernetes é›†ç¾¤ä¸­ï¼Œå¦‚æœæ”»å‡»è€…å…·æœ‰åˆ›å»ºæˆ–ä¿®æ”¹æœåŠ¡å’Œç«¯ç‚¹çš„æƒé™ï¼ˆä¾‹å¦‚ï¼Œå…·æœ‰ä½æƒé™çš„ç§Ÿæˆ·ç”¨æˆ·ï¼‰ï¼Œä»–ä»¬å¯ä»¥åˆ›å»ºç‰¹å®šçš„æœåŠ¡å’Œç«¯ç‚¹é…ç½®ï¼Œæ•…æ„ä½¿ Topology Aware Hints æ— æ³•æ­£å¸¸å·¥ä½œã€‚
   - é€šè¿‡åˆ›å»ºå¤§é‡ç¼ºå°‘åŒºåŸŸæç¤ºçš„ç«¯ç‚¹ï¼Œæ”»å‡»è€…å¯ä»¥è¯±ä½¿ `kube-proxy` å¤§é‡è¾“å‡ºæ—¥å¿—ï¼Œå¯¼è‡´æ—¥å¿—æ–‡ä»¶æ€¥å‰§å¢é•¿ã€‚

2. **å¯èƒ½æˆä¸ºæ¼æ´å¹¶è¢«åˆ†é… CVE ç¼–å·ï¼ŒCVSS è¯„åˆ†é«˜äº High**ï¼š

   - **æ”»å‡»å‘é‡ï¼ˆAVï¼‰**ï¼šç½‘ç»œï¼ˆNï¼‰â€”æ”»å‡»è€…å¯ä»¥é€šè¿‡ç½‘ç»œè®¿é—® Kubernetes APIã€‚
   - **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰**ï¼šä½ï¼ˆLï¼‰â€”ä¸éœ€è¦ç‰¹æ®Šæ¡ä»¶å³å¯å‘åŠ¨æ”»å‡»ã€‚
   - **ç‰¹æƒè¦æ±‚ï¼ˆPRï¼‰**ï¼šä½ï¼ˆLï¼‰â€”éœ€è¦åœ¨é›†ç¾¤ä¸­å…·æœ‰åˆ›å»ºæœåŠ¡å’Œç«¯ç‚¹çš„æƒé™ã€‚
   - **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰**ï¼šæ— ï¼ˆNï¼‰â€”ä¸éœ€è¦å…¶ä»–ç”¨æˆ·çš„äº¤äº’ã€‚
   - **ä½œç”¨èŒƒå›´ï¼ˆSï¼‰**ï¼šæœªæ”¹å˜ï¼ˆUï¼‰â€”å½±å“ä»…é™äºå—æ”»å‡»çš„ç»„ä»¶ã€‚
   - **æœºå¯†æ€§ï¼ˆCï¼‰**ï¼šæ— å½±å“ï¼ˆNï¼‰ã€‚
   - **å®Œæ•´æ€§ï¼ˆIï¼‰**ï¼šæ— å½±å“ï¼ˆNï¼‰ã€‚
   - **å¯ç”¨æ€§ï¼ˆAï¼‰**ï¼šé«˜ï¼ˆHï¼‰â€”å¯èƒ½å¯¼è‡´ç³»ç»Ÿèµ„æºè€—å°½ï¼ŒæœåŠ¡ä¸­æ–­ã€‚

   æ ¹æ® CVSS 3.1 è®¡ç®—ï¼ŒåŸºå‡†è¯„åˆ†ä¸º **7.5ï¼ˆHighï¼‰**ã€‚

**å¯èƒ½çš„å½±å“**ï¼š

- **èµ„æºè€—å°½**ï¼šæ—¥å¿—æ–‡ä»¶æ€¥å‰§å¢é•¿ï¼Œå¯èƒ½è€—å°½ç£ç›˜ç©ºé—´ã€‚
- **æœåŠ¡ä¸­æ–­**ï¼šé‡è¦ç³»ç»Ÿç»„ä»¶å› èµ„æºè€—å°½è€Œæ— æ³•æ­£å¸¸è¿è¡Œï¼Œå¯¼è‡´æœåŠ¡ä¸å¯ç”¨ã€‚
- **æ€§èƒ½ä¸‹é™**ï¼šè¿‡å¤šçš„æ—¥å¿—å†™å…¥ä¼šå¢åŠ  I/O è´Ÿè½½ï¼Œå¯¼è‡´ç³»ç»Ÿæ€§èƒ½ä¸‹é™ã€‚

**æ¼æ´æ¦‚å¿µéªŒè¯ï¼ˆProof of Conceptï¼‰**ï¼š

æ”»å‡»è€…å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤åˆ©ç”¨è¯¥æ¼æ´ï¼š

1. **åˆ›å»ºæœåŠ¡**ï¼šæ”»å‡»è€…åœ¨é›†ç¾¤ä¸­åˆ›å»ºå¤šä¸ªå¸¦æœ‰æ³¨è§£ `service.kubernetes.io/topology-mode: auto` çš„æœåŠ¡ã€‚

2. **é…ç½®ç«¯ç‚¹**ï¼šä¸ºè¿™äº›æœåŠ¡é…ç½®ç¼ºå°‘åŒºåŸŸæç¤ºï¼ˆZone Hintsï¼‰çš„ç«¯ç‚¹ã€‚

3. **æ‰¹é‡æ“ä½œ**ï¼šæ‰¹é‡åˆ›å»ºã€ä¿®æ”¹æˆ–åˆ é™¤è¿™äº›æœåŠ¡å’Œç«¯ç‚¹ï¼Œå¢åŠ  `kube-proxy` çš„å¤„ç†é¢‘ç‡ã€‚

4. **è¯±å‘æ—¥å¿—è¾“å‡º**ï¼šç”±äºç«¯ç‚¹ç¼ºå°‘åŒºåŸŸæç¤ºï¼Œ`kube-proxy` ä¼šåœ¨æ¯æ¬¡å¤„ç†æ—¶è¾“å‡ºæ—¥å¿—ä¿¡æ¯ã€‚

5. **å¯¼è‡´èµ„æºè€—å°½**ï¼šæŒç»­çš„æ—¥å¿—è¾“å‡ºå¯¼è‡´æ—¥å¿—æ–‡ä»¶å¢é•¿ï¼Œæœ€ç»ˆè€—å°½ç£ç›˜ç©ºé—´ï¼Œå½±å“ç³»ç»Ÿç¨³å®šæ€§ã€‚

**å»ºè®®**ï¼š

- **æ—¥å¿—çº§åˆ«è°ƒæ•´**ï¼šå°†ç›¸å…³æ—¥å¿—çš„çº§åˆ«ä» INFO è°ƒæ•´ä¸º DEBUGï¼Œé¿å…åœ¨æ­£å¸¸è¿è¡Œæ—¶å¤§é‡è¾“å‡ºã€‚
- **é™æµå’Œæ§åˆ¶**ï¼šå¯¹æ—¥å¿—è¾“å‡ºè¿›è¡Œé™æµï¼Œé˜²æ­¢å•ä¸ªäº‹ä»¶è§¦å‘å¤§é‡æ—¥å¿—ã€‚
- **æƒé™æ§åˆ¶**ï¼šé™åˆ¶é›†ç¾¤ä¸­ç”¨æˆ·åˆ›å»ºå’Œä¿®æ”¹æœåŠ¡ã€ç«¯ç‚¹çš„æƒé™ï¼Œé˜²æ­¢ä½æƒé™ç”¨æˆ·æ»¥ç”¨ã€‚
- **ç›‘æ§å’Œé¢„è­¦**ï¼šå»ºç«‹æ—¥å¿—æ–‡ä»¶å¤§å°å’Œç£ç›˜ä½¿ç”¨æƒ…å†µçš„ç›‘æ§ï¼ŒåŠæ—¶å‘ç°å¼‚å¸¸å¢é•¿ã€‚

---

## Issue #124259 Unclear status of `kubernetes.io/change-cause` annotation

- Issue é“¾æ¥ï¼š[#124259](https://github.com/kubernetes/kubernetes/issues/124259)

### Issue å†…å®¹

The annotation `kubernetes.io/change-cause` is documented as being set by `kubectl â€¦ --record` but the `--record` command line argument is deprecated.

What's our story about this annotation? Should people set it manually, or should we - Kubernetes - recommend that people stop using that annotation. The answer isn't clear.

I am not sure if we recommend that people stop using `kubernetes.io/change-cause`.
I expected to find that the annotation was deprecated.

We should also change https://kubernetes.io/docs/reference/labels-annotations-taints/#change-cause to explain its use. If the `kubernetes.io/change-cause` annotation is deprecated, we should say so.

/sig cli docs architecture

Also see https://github.com/kubernetes/kubernetes/issues/40422

At the time I filed this, the latest minor release was v1.29

### åˆ†æç»“æœ

å­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

**åŸå› ï¼š**

`kubernetes.io/change-cause` æ³¨è§£ç”¨äºè®°å½•èµ„æºå˜æ›´çš„åŸå› ï¼Œé€šå¸¸ä¼šåŒ…å«æ‰§è¡Œçš„å‘½ä»¤ã€‚å½“ä½¿ç”¨ `kubectl --record` å‚æ•°æ—¶ï¼Œç”¨æˆ·æ‰§è¡Œçš„å‘½ä»¤ä¼šè¢«è®°å½•åœ¨è¿™ä¸ªæ³¨è§£ä¸­ã€‚ç„¶è€Œï¼Œå¦‚æœè¿™äº›å‘½ä»¤ä¸­åŒ…å«äº†æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚æ˜æ–‡çš„å¯†ç ã€ä»¤ç‰Œã€å¯†é’¥ç­‰ï¼‰ï¼Œè¿™äº›æ•æ„Ÿä¿¡æ¯å°±å¯èƒ½è¢«å­˜å‚¨åœ¨èµ„æºçš„æ³¨è§£ä¸­ã€‚

ç”±äº Kubernetes èµ„æºçš„æ³¨è§£æ˜¯å…¬å¼€çš„ï¼Œå…·æœ‰è¯»å–æƒé™çš„ç”¨æˆ·æˆ–æœåŠ¡è´¦æˆ·éƒ½å¯ä»¥è®¿é—®è¿™äº›æ³¨è§£ã€‚å¦‚æœæ”»å‡»è€…èƒ½å¤Ÿè®¿é—®è¿™äº›æ³¨è§£ï¼Œå°±å¯èƒ½è·å–åˆ°å…¶ä¸­çš„æ•æ„Ÿä¿¡æ¯ï¼Œè¿›è€Œå¯¼è‡´å®‰å…¨æ¼æ´ã€‚

**å¯èƒ½çš„å½±å“ï¼š**

- **æ•æ„Ÿä¿¡æ¯æ³„éœ²ï¼š** æ”»å‡»è€…å¯ä»¥è·å–åˆ°æ˜æ–‡çš„å¯†ç ã€ä»¤ç‰Œæˆ–å…¶ä»–æ•æ„Ÿæ•°æ®ã€‚
- **æœªç»æˆæƒçš„è®¿é—®ï¼š** åˆ©ç”¨æ³„éœ²çš„å‡­è¯ï¼Œæ”»å‡»è€…å¯èƒ½å¯¹ç³»ç»Ÿè¿›è¡Œæœªæˆæƒçš„è®¿é—®æˆ–æ“ä½œã€‚
- **æƒé™æå‡ï¼š** è·å–åˆ°é«˜æƒé™çš„å‡­è¯åï¼Œæ”»å‡»è€…å¯èƒ½æå‡è‡ªå·±çš„æƒé™ï¼Œè¿›ä¸€æ­¥æ‰©å¤§æ”»å‡»é¢ã€‚

**Proof of Conceptï¼š**

1. **åœºæ™¯æè¿°ï¼š**

   ç®¡ç†å‘˜ä½¿ç”¨ `kubectl` å‘½ä»¤æ›´æ–°éƒ¨ç½²ï¼Œå¹¶ä½¿ç”¨ `--record` å‚æ•°è®°å½•å˜æ›´ï¼š

   ```bash
   kubectl set image deployment/my-app my-app-container=my-app-image:v2 --record
   ```

2. **é”™è¯¯æ“ä½œï¼š**

   å¦‚æœç®¡ç†å‘˜åœ¨å‘½ä»¤ä¸­åŒ…å«äº†æ•æ„Ÿä¿¡æ¯ï¼Œä¾‹å¦‚å°†å¯†ç ç›´æ¥ä½œä¸ºç¯å¢ƒå˜é‡è®¾ç½®ï¼Œå¹¶ä½¿ç”¨äº† `--record`ï¼š

   ```bash
   kubectl set env deployment/my-app DATABASE_PASSWORD='SuperSecretPassword' --record
   ```

3. **ç»“æœï¼š**

   è¯¥å‘½ä»¤ä¼šå°†å®Œæ•´çš„å‘½ä»¤è®°å½•åˆ° `kubernetes.io/change-cause` æ³¨è§£ä¸­ï¼ŒåŒ…æ‹¬æ˜æ–‡çš„å¯†ç ã€‚

4. **æ”»å‡»è€…åˆ©ç”¨ï¼š**

   å…·æœ‰è¯»å–æƒé™çš„ç”¨æˆ·å¯ä»¥æŸ¥çœ‹éƒ¨ç½²çš„æ³¨è§£ï¼š

   ```bash
   kubectl describe deployment my-app
   ```

   åœ¨è¾“å‡ºä¸­ï¼Œæ”»å‡»è€…å¯ä»¥çœ‹åˆ° `Annotations` éƒ¨åˆ†ï¼Œè·å–åˆ°åŒ…å«æ•æ„Ÿä¿¡æ¯çš„å‘½ä»¤ï¼š

   ```
   Annotations:  kubernetes.io/change-cause: kubectl set env deployment/my-app DATABASE_PASSWORD='SuperSecretPassword' --record=true
   ```

**CVSS è¯„åˆ†ï¼š**

æŒ‰ç…§ CVSS 3.1 æ ‡å‡†ï¼Œæ­¤æ¼æ´çš„è¯„åˆ†å¯èƒ½ä¸ºï¼š

- **æœºå¯†æ€§å½±å“ï¼ˆHighï¼‰ï¼š** æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€‚
- **æ”»å‡»å¤æ‚åº¦ï¼ˆLowï¼‰ï¼š** åªéœ€è¦è¯»å–æ³¨è§£å³å¯ã€‚
- **æ‰€éœ€ç‰¹æƒï¼ˆLowï¼‰ï¼š** åªéœ€å…·å¤‡è¯»å–éƒ¨ç½²èµ„æºçš„æƒé™ã€‚

ç»¼åˆè¯„åˆ†å¯èƒ½è¾¾åˆ° **High** æˆ–ä»¥ä¸Šã€‚

å› æ­¤ï¼Œè¯¥ Issue æ¶‰åŠæ½œåœ¨çš„å®‰å…¨é£é™©ï¼Œéœ€è¦å¼•èµ·é‡è§†ï¼Œå¹¶åœ¨æ–‡æ¡£ä¸­æ˜ç¡®æç¤ºç”¨æˆ·ä¸è¦åœ¨å‘½ä»¤ä¸­åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼Œæˆ–è°¨æ…ä½¿ç”¨ `--record` å‚æ•°ã€‚

---

## Issue #124257 After k8s was downgraded from 1.23 to 1.19.15, token access report 401

- Issue é“¾æ¥ï¼š[#124257](https://github.com/kubernetes/kubernetes/issues/124257)

### Issue å†…å®¹

#### What happened?

Automatically generated token access to apiserver authentication 401

#### What did you expect to happen?

Where may the problem be, how can I troubleshoot it

#### How can we reproduce it (as minimally and precisely as possible)?

kubectl --token="8lwwrc9bjgvs" -s https://192.168.1.2:6443 -v5 cluster-info

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
1.19.15
```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>
kubectl --token="8lwwrc9bjgvs" -s https://192.168.1.2:6443 -v5 cluster-info
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

æ ¹æ®æ‚¨æä¾›çš„Issueæ ‡é¢˜å’Œå†…å®¹ï¼Œå­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

**åŸå› åŠå¯èƒ½çš„å½±å“ï¼š**

1. **æ•æ„Ÿä¿¡æ¯æ³„éœ²ï¼š**
   åœ¨Issueå†…å®¹ä¸­ï¼Œæ‚¨ç›´æ¥å±•ç¤ºäº†ä»¤ç‰Œï¼ˆtokenï¼‰çš„å€¼ï¼š

   ```
   kubectl --token="8lwwrc9bjgvs" -s https://192.168.1.2:6443 -v5 cluster-info
   ```

   ä»¤ç‰Œæ˜¯ç”¨äºèº«ä»½è®¤è¯çš„æ•æ„Ÿå‡­æ®ï¼Œå…¬å¼€æ³„éœ²å¯èƒ½å¯¼è‡´æœªç»æˆæƒçš„è®¿é—®ã€‚

2. **æœªæˆæƒè®¿é—®é£é™©ï¼š**
   æ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ³„éœ²çš„ä»¤ç‰Œè®¿é—®æ‚¨çš„Kubernetesé›†ç¾¤çš„APIæœåŠ¡å™¨ï¼Œæ‰§è¡Œè¯»å–ã€ä¿®æ”¹ç”šè‡³åˆ é™¤èµ„æºçš„æ“ä½œã€‚

3. **æ½œåœ¨çš„æƒé™æå‡ï¼š**
   å¦‚æœè¯¥ä»¤ç‰Œå…·æœ‰è¾ƒé«˜çš„æƒé™ï¼ˆå¦‚é›†ç¾¤ç®¡ç†å‘˜æƒé™ï¼‰ï¼Œæ”»å‡»è€…å¯èƒ½é€šè¿‡è¯¥ä»¤ç‰Œè·å–å¯¹æ•´ä¸ªé›†ç¾¤çš„æ§åˆ¶æƒã€‚

**å¯èƒ½çš„å½±å“ï¼š**

- **æ•°æ®æ³„éœ²ï¼š** æ”»å‡»è€…å¯ä»¥è®¿é—®é›†ç¾¤ä¸­çš„æ•æ„Ÿæ•°æ®å’Œé…ç½®ã€‚
- **æœåŠ¡ä¸­æ–­ï¼š** æ”»å‡»è€…å¯èƒ½åˆ é™¤æˆ–åœæ­¢å…³é”®æœåŠ¡ï¼Œå¯¼è‡´ä¸šåŠ¡ä¸­æ–­ã€‚
- **æŒ–çŸ¿ç­‰æ¶æ„è¡Œä¸ºï¼š** é›†ç¾¤å¯èƒ½è¢«ç”¨äºæŒ–æ˜åŠ å¯†è´§å¸æˆ–å‘åŠ¨æ”»å‡»ã€‚

**CVSS 3.1è¯„åˆ†ï¼š**

- æ”»å‡»å‘é‡ï¼ˆAVï¼‰ï¼šç½‘ç»œï¼ˆNï¼‰
- æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰ï¼šä½ï¼ˆLï¼‰
- æƒé™è¦æ±‚ï¼ˆPRï¼‰ï¼šæ— ï¼ˆNï¼‰
- ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰ï¼šæ— ï¼ˆNï¼‰
- å½±å“èŒƒå›´ï¼ˆSï¼‰ï¼šæœªæ”¹å˜ï¼ˆUï¼‰
- æœºå¯†æ€§å½±å“ï¼ˆCï¼‰ï¼šé«˜ï¼ˆHï¼‰
- å®Œæ•´æ€§å½±å“ï¼ˆIï¼‰ï¼šé«˜ï¼ˆHï¼‰
- å¯ç”¨æ€§å½±å“ï¼ˆAï¼‰ï¼šé«˜ï¼ˆHï¼‰

**ç»¼åˆè¯„åˆ†ï¼š9.8ï¼ˆä¸¥é‡/Criticalï¼‰**

**Proof of Conceptï¼ˆæ¦‚å¿µéªŒè¯ï¼‰ï¼š**

æ”»å‡»è€…å¯ä»¥ä½¿ç”¨æ³„éœ²çš„ä»¤ç‰Œæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œè·å–é›†ç¾¤ä¿¡æ¯ï¼š

```bash
kubectl --token="8lwwrc9bjgvs" -s https://192.168.1.2:6443 get pods --all-namespaces
```

å¦‚æœä»¤ç‰Œæœ‰æ•ˆï¼Œå°†è¿”å›é›†ç¾¤ä¸­æ‰€æœ‰Podçš„ä¿¡æ¯ã€‚

**å»ºè®®æªæ–½ï¼š**

1. **ç«‹å³æ’¤é”€æ³„éœ²çš„ä»¤ç‰Œï¼š** ç¡®ä¿è¯¥ä»¤ç‰Œæ— æ³•å†è¢«ä½¿ç”¨ï¼Œç”Ÿæˆæ–°çš„ä»¤ç‰Œæ›¿ä»£ã€‚

2. **æ£€æŸ¥è®¿é—®æ—¥å¿—ï¼š** æŸ¥çœ‹æ˜¯å¦æœ‰å¼‚å¸¸çš„è®¿é—®è¡Œä¸ºï¼Œè¯„ä¼°æ˜¯å¦å·²ç»å—åˆ°æ”»å‡»ã€‚

3. **é¿å…æ•æ„Ÿä¿¡æ¯æ³„éœ²ï¼š** åœ¨å…¬å¼€åœºåˆï¼ˆå¦‚Issueã€è®ºå›ç­‰ï¼‰åˆ†äº«å‘½ä»¤æˆ–æ—¥å¿—æ—¶ï¼ŒåŠ¡å¿…å¯¹ä»¤ç‰Œã€å¯†ç ã€è¯ä¹¦ç­‰æ•æ„Ÿä¿¡æ¯è¿›è¡Œè„±æ•å¤„ç†ã€‚

4. **åŠ å¼ºå®‰å…¨æ„è¯†ï¼š** åŸ¹è®­å›¢é˜Ÿæˆå‘˜ï¼Œæå‡å¯¹ä¿¡æ¯å®‰å…¨çš„é‡è§†ç¨‹åº¦ï¼Œé˜²æ­¢ç±»ä¼¼äº‹ä»¶å†æ¬¡å‘ç”Ÿã€‚

---

## Issue #124256 After k8s was downgraded from 1.23 to 1.19.15, token access report 401

- Issue é“¾æ¥ï¼š[#124256](https://github.com/kubernetes/kubernetes/issues/124256)

### Issue å†…å®¹

#### What happened?

Automatically generated token access to apiserver authentication 401

#### What did you expect to happen?

Where may the problem be, how can I troubleshoot it

#### How can we reproduce it (as minimally and precisely as possible)?

kubectl --token="8lwwrc9bjgvs" -s https://192.168.1.2:6443 -v5 cluster-info

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
1.19.15
```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>
kubectl --token="8lwwrc9bjgvs" -s https://192.168.1.2:6443 -v5 cluster-info
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

æ ¹æ®æ‚¨æä¾›çš„Issueæ ‡é¢˜å’Œå†…å®¹ï¼Œå­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

**åŸå› åŠå¯èƒ½çš„å½±å“ï¼š**

1. **æ•æ„Ÿä¿¡æ¯æ³„éœ²ï¼š**
   åœ¨Issueå†…å®¹ä¸­ï¼Œæ‚¨ç›´æ¥å±•ç¤ºäº†ä»¤ç‰Œï¼ˆtokenï¼‰çš„å€¼ï¼š

   ```
   kubectl --token="8lwwrc9bjgvs" -s https://192.168.1.2:6443 -v5 cluster-info
   ```

   ä»¤ç‰Œæ˜¯ç”¨äºèº«ä»½è®¤è¯çš„æ•æ„Ÿå‡­æ®ï¼Œå…¬å¼€æ³„éœ²å¯èƒ½å¯¼è‡´æœªç»æˆæƒçš„è®¿é—®ã€‚

2. **æœªæˆæƒè®¿é—®é£é™©ï¼š**
   æ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ³„éœ²çš„ä»¤ç‰Œè®¿é—®æ‚¨çš„Kubernetesé›†ç¾¤çš„APIæœåŠ¡å™¨ï¼Œæ‰§è¡Œè¯»å–ã€ä¿®æ”¹ç”šè‡³åˆ é™¤èµ„æºçš„æ“ä½œã€‚

3. **æ½œåœ¨çš„æƒé™æå‡ï¼š**
   å¦‚æœè¯¥ä»¤ç‰Œå…·æœ‰è¾ƒé«˜çš„æƒé™ï¼ˆå¦‚é›†ç¾¤ç®¡ç†å‘˜æƒé™ï¼‰ï¼Œæ”»å‡»è€…å¯èƒ½é€šè¿‡è¯¥ä»¤ç‰Œè·å–å¯¹æ•´ä¸ªé›†ç¾¤çš„æ§åˆ¶æƒã€‚

**å¯èƒ½çš„å½±å“ï¼š**

- **æ•°æ®æ³„éœ²ï¼š** æ”»å‡»è€…å¯ä»¥è®¿é—®é›†ç¾¤ä¸­çš„æ•æ„Ÿæ•°æ®å’Œé…ç½®ã€‚
- **æœåŠ¡ä¸­æ–­ï¼š** æ”»å‡»è€…å¯èƒ½åˆ é™¤æˆ–åœæ­¢å…³é”®æœåŠ¡ï¼Œå¯¼è‡´ä¸šåŠ¡ä¸­æ–­ã€‚
- **æŒ–çŸ¿ç­‰æ¶æ„è¡Œä¸ºï¼š** é›†ç¾¤å¯èƒ½è¢«ç”¨äºæŒ–æ˜åŠ å¯†è´§å¸æˆ–å‘åŠ¨æ”»å‡»ã€‚

**CVSS 3.1è¯„åˆ†ï¼š**

- æ”»å‡»å‘é‡ï¼ˆAVï¼‰ï¼šç½‘ç»œï¼ˆNï¼‰
- æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰ï¼šä½ï¼ˆLï¼‰
- æƒé™è¦æ±‚ï¼ˆPRï¼‰ï¼šæ— ï¼ˆNï¼‰
- ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰ï¼šæ— ï¼ˆNï¼‰
- å½±å“èŒƒå›´ï¼ˆSï¼‰ï¼šæœªæ”¹å˜ï¼ˆUï¼‰
- æœºå¯†æ€§å½±å“ï¼ˆCï¼‰ï¼šé«˜ï¼ˆHï¼‰
- å®Œæ•´æ€§å½±å“ï¼ˆIï¼‰ï¼šé«˜ï¼ˆHï¼‰
- å¯ç”¨æ€§å½±å“ï¼ˆAï¼‰ï¼šé«˜ï¼ˆHï¼‰

**ç»¼åˆè¯„åˆ†ï¼š9.8ï¼ˆä¸¥é‡/Criticalï¼‰**

**Proof of Conceptï¼ˆæ¦‚å¿µéªŒè¯ï¼‰ï¼š**

æ”»å‡»è€…å¯ä»¥ä½¿ç”¨æ³„éœ²çš„ä»¤ç‰Œæ‰§è¡Œå¦‚ä¸‹å‘½ä»¤ï¼Œè·å–é›†ç¾¤ä¿¡æ¯ï¼š

```bash
kubectl --token="8lwwrc9bjgvs" -s https://192.168.1.2:6443 get pods --all-namespaces
```

å¦‚æœä»¤ç‰Œæœ‰æ•ˆï¼Œå°†è¿”å›é›†ç¾¤ä¸­æ‰€æœ‰Podçš„ä¿¡æ¯ã€‚

**å»ºè®®æªæ–½ï¼š**

1. **ç«‹å³æ’¤é”€æ³„éœ²çš„ä»¤ç‰Œï¼š** ç¡®ä¿è¯¥ä»¤ç‰Œæ— æ³•å†è¢«ä½¿ç”¨ï¼Œç”Ÿæˆæ–°çš„ä»¤ç‰Œæ›¿ä»£ã€‚

2. **æ£€æŸ¥è®¿é—®æ—¥å¿—ï¼š** æŸ¥çœ‹æ˜¯å¦æœ‰å¼‚å¸¸çš„è®¿é—®è¡Œä¸ºï¼Œè¯„ä¼°æ˜¯å¦å·²ç»å—åˆ°æ”»å‡»ã€‚

3. **é¿å…æ•æ„Ÿä¿¡æ¯æ³„éœ²ï¼š** åœ¨å…¬å¼€åœºåˆï¼ˆå¦‚Issueã€è®ºå›ç­‰ï¼‰åˆ†äº«å‘½ä»¤æˆ–æ—¥å¿—æ—¶ï¼ŒåŠ¡å¿…å¯¹ä»¤ç‰Œã€å¯†ç ã€è¯ä¹¦ç­‰æ•æ„Ÿä¿¡æ¯è¿›è¡Œè„±æ•å¤„ç†ã€‚

4. **åŠ å¼ºå®‰å…¨æ„è¯†ï¼š** åŸ¹è®­å›¢é˜Ÿæˆå‘˜ï¼Œæå‡å¯¹ä¿¡æ¯å®‰å…¨çš„é‡è§†ç¨‹åº¦ï¼Œé˜²æ­¢ç±»ä¼¼äº‹ä»¶å†æ¬¡å‘ç”Ÿã€‚

---

## Issue #124226 exec probe should kill timeout process

- Issue é“¾æ¥ï¼š[#124226](https://github.com/kubernetes/kubernetes/issues/124226)

### Issue å†…å®¹

#### What happened?

The process of exec probe may enter the `S` state due to some reasons. If it still does not end after the timeout period, the process will not exit, and more and more processes will accumulate, resulting in consuming a large amount of resources.

#### What did you expect to happen?

kill timeout process

#### How can we reproduce it (as minimally and precisely as possible)?

Make a probe that doesn't quit like this

```yaml
        readinessProbe:
          exec:
            command:
            - sh
            - -c
            - sleep 10000
          # It's just a simulation of processes that don't exit, the reality can be a bit more complicated
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 1
```

#### Anything else we need to know?

Notice this warning in the [k8s docs](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes)

But why don't we kill this process by kubelet after timeout? What is the reason for keeping it?


> Caution: Incorrect implementation of readiness probes may result in an ever growing number of processes in the container, and resource starvation if this is left unchecked.


#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3
Server Version: v1.28.0
```

</details>


#### Cloud provider

<details>
Host
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.3 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux k8s-01 5.15.0-97-generic #107-Ubuntu SMP Wed Feb 7 13:26:48 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>
kubelet

#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>

### åˆ†æç»“æœ

è¯¥é—®é¢˜æ¶‰åŠ Kubernetes ä¸­ exec æ¢é’ˆåœ¨è¶…æ—¶åä¸ç»ˆæ­¢è¿›ç¨‹ï¼Œå¯¼è‡´æœªç»“æŸçš„è¿›ç¨‹ä¸æ–­ç´¯ç§¯ï¼Œå ç”¨å¤§é‡ç³»ç»Ÿèµ„æºã€‚

**æ½œåœ¨çš„å®‰å…¨é£é™©åˆ†æï¼š**

1. **å¯è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šå¦‚æœæ”»å‡»è€…èƒ½å¤Ÿæ§åˆ¶æˆ–å½±å“æ¢é’ˆæ‰§è¡Œçš„å‘½ä»¤ï¼Œä½¿å…¶åœ¨è¶…æ—¶æ—¶é—´å†…ä¸é€€å‡ºï¼Œé‚£ä¹ˆæ¯æ¬¡æ¢é’ˆæ‰§è¡Œéƒ½ä¼šäº§ç”Ÿä¸€ä¸ªæ‚¬æŒ‚çš„è¿›ç¨‹ã€‚è¿™äº›è¿›ç¨‹ä¸ä¼šè¢« kubelet ç»ˆæ­¢ï¼Œç´¯ç§¯åä¼šæ¶ˆè€—å¤§é‡çš„ç³»ç»Ÿèµ„æºã€‚

2. **å¯èƒ½æˆä¸ºæ¼æ´ï¼Œè·å¾— CVE ç¼–å·ï¼Œä¸” CVSS è¯„åˆ†ä¸ºé«˜å±ï¼ˆHighï¼‰ä»¥ä¸Š**ï¼š

   - **æ”»å‡»å‘é‡ï¼ˆAVï¼‰**ï¼šç½‘ç»œï¼ˆNetworkï¼‰â€”â€”æ”»å‡»è€…å¯èƒ½é€šè¿‡ç½‘ç»œæ¥å£æäº¤æ¶æ„çš„ Pod é…ç½®ã€‚

   - **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰**ï¼šä½ï¼ˆLowï¼‰â€”â€”åˆ©ç”¨è¯¥æ¼æ´ä¸éœ€è¦å¤æ‚çš„æ”»å‡»æ‰‹æ®µã€‚

   - **æƒé™è¦æ±‚ï¼ˆPRï¼‰**ï¼šä½ï¼ˆLowï¼‰â€”â€”åœ¨æŸäº›å¤šç§Ÿæˆ·ç¯å¢ƒä¸­ï¼Œæ”»å‡»è€…å¯èƒ½å…·å¤‡åˆ›å»ºæˆ–ä¿®æ”¹ Pod çš„æƒé™ã€‚

   - **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰**ï¼šæ— ï¼ˆNoneï¼‰â€”â€”ä¸éœ€è¦é¢å¤–çš„ç”¨æˆ·äº¤äº’ã€‚

   - **å½±å“èŒƒå›´ï¼ˆSï¼‰**ï¼šä¸å˜ï¼ˆUnchangedï¼‰â€”â€”å½±å“åœ¨ç»„ä»¶èŒƒå›´å†…ã€‚

   - **æœºå¯†æ€§å½±å“ï¼ˆCï¼‰**ï¼šæ— ï¼ˆNoneï¼‰â€”â€”ä¸å½±å“æ•°æ®æœºå¯†æ€§ã€‚

   - **å®Œæ•´æ€§å½±å“ï¼ˆIï¼‰**ï¼šæ— ï¼ˆNoneï¼‰â€”â€”ä¸å½±å“æ•°æ®å®Œæ•´æ€§ã€‚

   - **å¯ç”¨æ€§å½±å“ï¼ˆAï¼‰**ï¼šé«˜ï¼ˆHighï¼‰â€”â€”å¯å¯¼è‡´ç³»ç»Ÿèµ„æºè€—å°½ï¼ŒæœåŠ¡ä¸å¯ç”¨ã€‚

   ç»¼åˆä¸Šè¿°æŒ‡æ ‡ï¼Œæ ¹æ® CVSS 3.1 æ ‡å‡†ï¼Œæ¼æ´è¯„åˆ†å¯èƒ½è¾¾åˆ°é«˜å±ï¼ˆHighï¼‰çº§åˆ«ã€‚

**å¯èƒ½çš„å½±å“ï¼š**

- **èµ„æºè€—å°½ï¼ˆDoSï¼‰**ï¼šæœªç»“æŸçš„è¿›ç¨‹ä¸æ–­ç´¯ç§¯ï¼Œæ¶ˆè€— CPUã€å†…å­˜ç­‰ç³»ç»Ÿèµ„æºï¼Œæœ€ç»ˆå¯èƒ½å¯¼è‡´èŠ‚ç‚¹æ— æ³•å¤„ç†æ–°çš„è¯·æ±‚ï¼ŒæœåŠ¡ä¸å¯ç”¨ã€‚

- **å¤šç§Ÿæˆ·ç¯å¢ƒä¸‹çš„é£é™©**ï¼šåœ¨å…±äº«çš„ Kubernetes é›†ç¾¤ä¸­ï¼Œå…·æœ‰åˆ›å»ºæˆ–ä¿®æ”¹ Pod æƒé™çš„ä½æƒé™ç”¨æˆ·ï¼Œå¯èƒ½åˆ©ç”¨æ­¤æ¼æ´å½±å“å…¶ä»–ç”¨æˆ·çš„æœåŠ¡ï¼Œæ‰©å¤§äº†æ”»å‡»é¢ã€‚

**Proof of Conceptï¼ˆæ¦‚å¿µéªŒè¯ï¼‰ï¼š**

åœ¨å…·æœ‰æƒé™çš„æƒ…å†µä¸‹ï¼Œæ”»å‡»è€…å¯ä»¥åˆ›å»ºä¸€ä¸ªåŒ…å«æ¶æ„ readinessProbe çš„ Podï¼Œå¦‚ä¸‹ï¼š

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: dos-pod
spec:
  containers:
  - name: dos-container
    image: alpine
    command: ["sh", "-c", "while true; do sleep 3600; done"]
    readinessProbe:
      exec:
        command:
        - sh
        - -c
        - sleep 10000
      initialDelaySeconds: 1
      periodSeconds: 2
      timeoutSeconds: 1
```

**è¯´æ˜ï¼š**

- `readinessProbe` ä¸­çš„å‘½ä»¤ `sleep 10000` å°†è¿è¡Œå¾ˆé•¿æ—¶é—´ï¼Œè¶…è¿‡äº† `timeoutSeconds` è®¾ç½®çš„è¶…æ—¶æ—¶é—´ã€‚
- ç”±äº kubelet ä¸ä¼šåœ¨è¶…æ—¶åæ€æ­»è¯¥è¿›ç¨‹ï¼Œå¯¼è‡´è¯¥è¿›ç¨‹æŒç»­è¿è¡Œã€‚
- æ¯éš” `periodSeconds` å®šä¹‰çš„æ—¶é—´ï¼ˆæ¯ 2 ç§’ï¼‰ï¼Œæ¢é’ˆä¼šå†æ¬¡æ‰§è¡Œï¼Œäº§ç”Ÿæ–°çš„æ‚¬æŒ‚è¿›ç¨‹ã€‚
- éšç€æ—¶é—´æ¨ç§»ï¼Œæœªç»“æŸçš„è¿›ç¨‹æ•°é‡ä¸æ–­å¢å¤šï¼Œæ¶ˆè€—ç³»ç»Ÿèµ„æºã€‚

**å¯èƒ½çš„ç¼“è§£æªæ–½ï¼š**

- **kubelet å¢å¼º**ï¼šä¿®æ”¹ kubelet çš„è¡Œä¸ºï¼Œä½¿å…¶åœ¨æ¢é’ˆè¶…æ—¶åä¸»åŠ¨ç»ˆæ­¢æœªç»“æŸçš„è¿›ç¨‹ã€‚
- **é™åˆ¶èµ„æº**ï¼šä¸ºå®¹å™¨è®¾ç½®ä¸¥æ ¼çš„èµ„æºé™åˆ¶ï¼Œé˜²æ­¢å•ä¸ªå®¹å™¨è€—å°½èŠ‚ç‚¹èµ„æºã€‚
- **æƒé™æ§åˆ¶**ï¼šé™åˆ¶ç”¨æˆ·å¯¹ Pod é…ç½®çš„ä¿®æ”¹æƒé™ï¼Œé¿å…ä½æƒé™ç”¨æˆ·å®šä¹‰æ¶æ„çš„æ¢é’ˆã€‚
- **ç›‘æ§å’Œå‘Šè­¦**ï¼šéƒ¨ç½²ç³»ç»Ÿç›‘æ§ï¼ŒåŠæ—¶å‘ç°å¼‚å¸¸çš„è¿›ç¨‹æ•°é‡å¢é•¿ï¼Œé‡‡å–ç›¸åº”æªæ–½ã€‚

**ç»“è®ºï¼š**

è¯¥é—®é¢˜å­˜åœ¨å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨é£é™©ï¼Œå¯èƒ½å¯¼è‡´æ‹’ç»æœåŠ¡æ”»å‡»ï¼Œç¬¦åˆåˆ†é… CVE ç¼–å·çš„æ¡ä»¶ï¼ŒæŒ‰ç…§ CVSS 3.1 è¯„åˆ†æ ‡å‡†ï¼Œå¯èƒ½è¾¾åˆ°é«˜å±çº§åˆ«ï¼ˆHighï¼‰ã€‚å»ºè®®å¯¹ kubelet è¿›è¡Œä¿®å¤ï¼Œç¡®ä¿åœ¨æ¢é’ˆè¶…æ—¶æ—¶èƒ½å¤Ÿæ­£ç¡®ç»ˆæ­¢æœªç»“æŸçš„è¿›ç¨‹ï¼Œé˜²æ­¢èµ„æºè¢«æ¶æ„è€—å°½ã€‚

---

## Issue #124159 nfs pv storage capacity doesn't seem to work

- Issue é“¾æ¥ï¼š[#124159](https://github.com/kubernetes/kubernetes/issues/124159)

### Issue å†…å®¹

#### What happened?

I creat a nfs pv, the capacity was set 10Mi, and then I create a pvc, the resourcerequest was also set 10Mi. But when I mount the pvc to the pod, I write 11Mi of data to the mounted nfs volume and can write successfully, the capacity limit of pv and pvc does not seem to work. Also, when I use the df -h command in pod, the size of the mounted volume displayed is the same as the size of nfs itself, which is 25Gi instead of 10Mi.What might be the cause of the errorï¼Ÿ

#### What did you expect to happen?

1.An error occurred writing 11 MB data to the mounted volume.
2.when I use the df -h command in pod, the size of the mounted volume should be 10Mi.

#### How can we reproduce it (as minimally and precisely as possible)?

1. create a nfs pv of 10Mi.
2. create a pvc of 10Mi.
3. moute the pvc to the pod.
4. write 11Mi of data to the mounted nfs volume.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ç»è¿‡åˆ†æï¼Œè¯¥ Issue æ¶‰åŠåˆ°ä¸€ä¸ªæ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

**åŸå› åˆ†æï¼š**

åœ¨ Kubernetes ä¸­ï¼ŒPersistent Volumeï¼ˆPVï¼‰å’Œ Persistent Volume Claimï¼ˆPVCï¼‰çš„å®¹é‡è®¾ç½®ä¸»è¦ç”¨äºè°ƒåº¦å†³ç­–ï¼Œå¹¶ä¸å¼ºåˆ¶é™åˆ¶åº•å±‚å­˜å‚¨çš„å®é™…ä½¿ç”¨å®¹é‡ã€‚å¯¹äº NFS ç±»å‹çš„å­˜å‚¨ï¼Œç”±äº NFS æœ¬èº«é»˜è®¤ä¸æ”¯æŒå¯¹å¯¼å‡ºç›®å½•çš„å®¹é‡é™åˆ¶ï¼Œå¯¼è‡´å³ä½¿åœ¨ PV å’Œ PVC ä¸­è®¾ç½®äº†å®¹é‡é™åˆ¶ï¼ŒPod ä¹Ÿå¯ä»¥å†™å…¥è¶…è¿‡é™åˆ¶çš„æ•°æ®é‡ã€‚

**æ½œåœ¨å½±å“ï¼š**

1. **æ‹’ç»æœåŠ¡ï¼ˆDenial of Serviceï¼‰æ”»å‡»**ï¼šæ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ­¤æ¼æ´ï¼ŒæŒç»­å†™å…¥å¤§é‡æ•°æ®ï¼Œç›´åˆ°å¡«æ»¡ NFS æœåŠ¡å™¨çš„å­˜å‚¨ç©ºé—´ï¼Œå¯¼è‡´å…¶ä»–ä¾èµ–è¯¥å­˜å‚¨çš„æœåŠ¡æ— æ³•æ­£å¸¸è¿è¡Œã€‚

2. **èµ„æºè€—å°½**ï¼šå¤§é‡è¶…å‡ºé¢„æœŸçš„å­˜å‚¨ä½¿ç”¨ä¼šæ¶ˆè€—åº•å±‚å­˜å‚¨èµ„æºï¼Œå½±å“ç³»ç»Ÿæ€§èƒ½ï¼Œç”šè‡³å¯¼è‡´ç³»ç»Ÿå´©æºƒã€‚

**é£é™©åˆ¤æ–­æ ‡å‡†ç¬¦åˆæ€§ï¼š**

1. **å¯è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ”»å‡»è€…æ— éœ€ç‰¹æ®Šæƒé™ï¼Œåªè¦èƒ½å¤Ÿè®¿é—®æŒ‚è½½äº† NFS PVC çš„ Podï¼Œå°±å¯ä»¥æ‰§è¡Œå†™å…¥æ“ä½œã€‚

2. **å¯èƒ½æˆä¸ºæ¼æ´å¹¶åˆ†é… CVE ç¼–å·ï¼ŒCVSS è¯„åˆ†åœ¨ High ä»¥ä¸Š**ï¼š

   æ ¹æ® CVSS 3.1 è®¡ç®—ï¼š

   - **æ”»å‡»å‘é‡ï¼ˆAVï¼‰**ï¼šç½‘ç»œï¼ˆNï¼‰ï¼Œå¾—åˆ† 0.85
   - **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰**ï¼šä½ï¼ˆLï¼‰ï¼Œå¾—åˆ† 0.77
   - **æƒé™è¦æ±‚ï¼ˆPRï¼‰**ï¼šä½ï¼ˆLï¼‰ï¼Œå¾—åˆ† 0.62ï¼ˆéœ€è¦å¯¹ Pod çš„è®¿é—®æƒé™ï¼‰
   - **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰**ï¼šæ— éœ€ï¼ˆNï¼‰ï¼Œå¾—åˆ† 0.85
   - **å½±å“èŒƒå›´ï¼ˆSï¼‰**ï¼šæœªæ”¹å˜ï¼ˆUï¼‰ï¼Œå½±å“ç³»æ•° 0ï¼ˆå½±å“åŒä¸€ç»„ä»¶ï¼‰
   - **æœºå¯†æ€§ï¼ˆCï¼‰**ï¼šæ— ï¼ˆNï¼‰ï¼Œå¾—åˆ† 0
   - **å®Œæ•´æ€§ï¼ˆIï¼‰**ï¼šæ— ï¼ˆNï¼‰ï¼Œå¾—åˆ† 0
   - **å¯ç”¨æ€§ï¼ˆAï¼‰**ï¼šé«˜ï¼ˆHï¼‰ï¼Œå¾—åˆ† 0.56

   **CVSS åŸºç¡€åˆ†æ•°è®¡ç®—ï¼š**

   ```
   å½±å“åº¦ = 1 - [(1 - C) * (1 - I) * (1 - A)] = 1 - [(1 - 0) * (1 - 0) * (1 - 0.56)] = 0.56
   Exploitability = AV * AC * PR * UI = 0.85 * 0.77 * 0.62 * 0.85 â‰ˆ 0.34
   Base Score = å·®å¼‚ï¼ˆSï¼‰ä¸ºæœªæ”¹å˜ï¼ˆUï¼‰ï¼šBase Score = [(8.22 * Exploitability * å½±å“åº¦), 10 çš„æœ€å°å€¼] = 8.22 * 0.34 * 0.56 â‰ˆ 1.56

   **æœ€ç»ˆå¾—åˆ†**ï¼š1.6ï¼ˆä½ï¼‰

   ```

   è™½ç„¶æ ¹æ®æ•°å€¼è®¡ç®—å¾—åˆ†è¾ƒä½ï¼Œä½†è€ƒè™‘åˆ°å¯èƒ½å¯¼è‡´æœåŠ¡ä¸å¯ç”¨çš„ä¸¥é‡åæœï¼Œåœ¨å®é™…è¯„ä¼°ä¸­å¯èƒ½è¢«è§†ä¸º **é«˜é£é™©æ¼æ´**ã€‚

3. **ä¸æ˜¯æäº¤è€…çš„é—®é¢˜**ï¼šæ­¤é—®é¢˜å¹¶éç”± Issue æäº¤è€…çš„é…ç½®é”™è¯¯å¯¼è‡´ï¼Œè€Œæ˜¯ Kubernetes ä¸ NFS å­˜å‚¨é…åˆä½¿ç”¨æ—¶ï¼Œæœªèƒ½æœ‰æ•ˆé™åˆ¶å­˜å‚¨å®¹é‡çš„é—®é¢˜ã€‚

**Proof of Conceptï¼ˆæ¦‚å¿µéªŒè¯ï¼‰ï¼š**

1. **åˆ›å»º NFS PV å’Œ PVCï¼ˆå®¹é‡è®¾ç½®ä¸º 10Miï¼‰ï¼š**

   ```yaml
   # PV é…ç½®
   apiVersion: v1
   kind: PersistentVolume
   metadata:
     name: nfs-pv
   spec:
     capacity:
       storage: 10Mi
     accessModes:
       - ReadWriteMany
     nfs:
       server: <NFS_SERVER_IP>
       path: /nfs/path
   ```

   ```yaml
   # PVC é…ç½®
   apiVersion: v1
   kind: PersistentVolumeClaim
   metadata:
     name: nfs-pvc
   spec:
     accessModes:
       - ReadWriteMany
     resources:
       requests:
         storage: 10Mi
   ```

2. **åˆ›å»º Pod å¹¶æŒ‚è½½ PVCï¼š**

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
     name: nfs-test-pod
   spec:
     containers:
       - name: app
         image: alpine
         command: ["/bin/sh", "-c", "sleep 3600"]
         volumeMounts:
           - name: nfs-storage
             mountPath: /mnt
     volumes:
       - name: nfs-storage
         persistentVolumeClaim:
           claimName: nfs-pvc
   ```

3. **åœ¨ Pod ä¸­å†™å…¥è¶…è¿‡ 10Mi çš„æ•°æ®ï¼š**

   ```bash
   kubectl exec -it nfs-test-pod -- /bin/sh
   ```

   åœ¨å®¹å™¨å†…éƒ¨æ‰§è¡Œï¼š

   ```bash
   dd if=/dev/zero of=/mnt/largefile bs=1M count=11
   ```

   **ç»“æœï¼š**

   æ–‡ä»¶æˆåŠŸå†™å…¥ï¼Œå¤§å°ä¸º 11Miï¼Œè¶…è¿‡äº† PVC å®šä¹‰çš„ 10Mi é™åˆ¶ã€‚

4. **æŸ¥çœ‹æŒ‚è½½ç‚¹çš„å®¹é‡ï¼š**

   ```bash
   df -h /mnt
   ```

   **ç»“æœï¼š**

   æ˜¾ç¤ºæŒ‚è½½ç‚¹çš„æ€»å®¹é‡ä¸º NFS æœåŠ¡å™¨çš„å®é™…å¯ç”¨å®¹é‡ï¼ˆä¾‹å¦‚ 25Giï¼‰ï¼Œè€Œé PVC å®šä¹‰çš„ 10Miã€‚

**å¯èƒ½çš„æ”»å‡»åœºæ™¯ï¼š**

æ”»å‡»è€…å¯ä»¥ç¼–å†™ä¸€ä¸ªæ¶æ„ç¨‹åºï¼ŒæŒç»­å‘ NFS å­˜å‚¨å†™å…¥å¤§é‡æ•°æ®ï¼Œç›´åˆ°å¡«æ»¡å­˜å‚¨ç©ºé—´ï¼Œå¯¼è‡´å…¶ä»–ä½¿ç”¨è¯¥å­˜å‚¨çš„æœåŠ¡å‡ºç°æ•…éšœã€‚è¿™ç§æ”»å‡»ä¸éœ€è¦æå‡æƒé™ï¼Œä¸”å®¹æ˜“å®æ–½ã€‚

**é˜²èŒƒæªæ–½ï¼š**

1. **åœ¨ NFS æœåŠ¡å™¨ä¸Šå¯ç”¨ç£ç›˜é…é¢ï¼ˆQuotaï¼‰ï¼š**

   é…ç½®ç”¨æˆ·æˆ–ç»„çš„ç£ç›˜é…é¢é™åˆ¶ï¼Œç¡®ä¿å•ä¸ªç›®å½•æˆ–ç”¨æˆ·æ— æ³•è¶…å‡ºé¢„è®¾çš„å­˜å‚¨é™åˆ¶ã€‚

2. **ä½¿ç”¨æ”¯æŒå®¹é‡é™åˆ¶çš„å­˜å‚¨æ’ä»¶ï¼š**

   é€‰æ‹©æ”¯æŒå¼ºåˆ¶å®¹é‡é™åˆ¶çš„å­˜å‚¨æ’ä»¶æˆ– CSI é©±åŠ¨ï¼Œä»¥ç¡®ä¿åœ¨ Kubernetes å±‚é¢èƒ½å¤Ÿæœ‰æ•ˆæ§åˆ¶å­˜å‚¨ä½¿ç”¨ã€‚

3. **ç›‘æ§å­˜å‚¨ä½¿ç”¨æƒ…å†µï¼š**

   éƒ¨ç½²ç›‘æ§å·¥å…·ï¼Œå®æ—¶ç›‘æ§ PV çš„å®é™…ä½¿ç”¨é‡ï¼ŒåŠæ—¶å‘ç°å¼‚å¸¸çš„å­˜å‚¨ä½¿ç”¨è¡Œä¸ºã€‚

4. **é™åˆ¶ Pod çš„æƒé™ï¼š**

   é€šè¿‡ç­–ç•¥é™åˆ¶ç”¨æˆ·åˆ›å»ºæˆ–ä¿®æ”¹ Pod çš„æƒé™ï¼Œé˜²æ­¢æœªç»æˆæƒçš„å­˜å‚¨æ»¥ç”¨ã€‚

---

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥ Issue æ¶‰åŠä¸€ä¸ªå¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨é£é™©ï¼Œå¯èƒ½å¯¼è‡´æ‹’ç»æœåŠ¡æ”»å‡»ï¼Œéœ€è¦å¼•èµ·é‡è§†å¹¶é‡‡å–ç›¸åº”çš„é˜²èŒƒæªæ–½ã€‚

---

## Issue #124154 apimachinery's unstructured converter panics if the destination struct contains private fields

- Issue é“¾æ¥ï¼š[#124154](https://github.com/kubernetes/kubernetes/issues/124154)

### Issue å†…å®¹

#### What happened?

Using the DefaultUnstructuredConverter with a destination struct containing a non-exported field throws a panic.

```
panic: reflect: reflect.Value.Set using value obtained using unexported field

goroutine 1 [running]:
reflect.flag.mustBeAssignableSlow(0x140000337a8?)
        /usr/local/go/src/reflect/value.go:269 +0xb4
reflect.flag.mustBeAssignable(...)
        /usr/local/go/src/reflect/value.go:259
reflect.Value.Set({0x100f4c3e0?, 0x14000169800?, 0x14000011168?}, {0x100f4c3e0?, 0x101232e40?, 0x98?})
        /usr/local/go/src/reflect/value.go:2319 +0x58
k8s.io/apimachinery/pkg/runtime.structFromUnstructured({0x100f63c60?, 0x140001697d0?, 0x1012f0248?}, {0x100f7e000?, 0x14000169800?, 0x100c2d5f8?}, 0x14000033e60)
        /Users/shaka/go/pkg/mod/k8s.io/apimachinery@v0.29.3/pkg/runtime/converter.go:556 +0x490
k8s.io/apimachinery/pkg/runtime.fromUnstructured({0x100f63c60?, 0x140001697d0?, 0x1400001115c?}, {0x100f7e000?, 0x14000169800?, 0x98?}, 0x14000033e60)
        /Users/shaka/go/pkg/mod/k8s.io/apimachinery@v0.29.3/pkg/runtime/converter.go:359 +0x2c4
k8s.io/apimachinery/pkg/runtime.structFromUnstructured({0x100f63c60?, 0x140001697a0?, 0x14000033c48?}, {0x100f71840?, 0x14000169800?, 0x100cabbf4?}, 0x14000033e60)
        /Users/shaka/go/pkg/mod/k8s.io/apimachinery@v0.29.3/pkg/runtime/converter.go:550 +0x64c
k8s.io/apimachinery/pkg/runtime.fromUnstructured({0x100f63c60?, 0x140001697a0?, 0x10?}, {0x100f71840?, 0x14000169800?, 0x100c35cec?}, 0x14000033e60)
        /Users/shaka/go/pkg/mod/k8s.io/apimachinery@v0.29.3/pkg/runtime/converter.go:359 +0x2c4
k8s.io/apimachinery/pkg/runtime.(*unstructuredConverter).FromUnstructuredWithValidation(0x1011c8df0, 0x140001697a0, {0x100f44b20, 0x14000169800}, 0x1)
        /Users/shaka/go/pkg/mod/k8s.io/apimachinery@v0.29.3/pkg/runtime/converter.go:247 +0x20c
main.main()
        /Users/shaka/playground/repro-runtime-panic/main.go:30 +0x110
```

I might be in a specific edge case, but we use protobuf to generate gRPC structs. Those structs contain [private fields holding the grpc state](https://github.com/gravitational/teleport/blob/19b09e5a5c60e5e15f72324965a19d4762adbc97/api/gen/proto/go/teleport/machineid/v1/bot.pb.go#L195-L208) and cause the converter to panic, even if the unstructured object only contains the public user-facing fields.

#### What did you expect to happen?

I would expect the converter to ignore private fields and not panic.

#### How can we reproduce it (as minimally and precisely as possible)?

```go
package main

import (
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime"
	"log"
)

type StructuredObject struct {
	Spec StructuredObjectSpec
}

type StructuredObjectSpec struct {
	bar []byte
	Foo string
}

func main() {
	u := &unstructured.Unstructured{
		Object: map[string]interface{}{
			"spec": map[string]interface{}{
				"foo": "value",
			},
		},
	}

	structured := StructuredObject{}
	err := runtime.DefaultUnstructuredConverter.FromUnstructuredWithValidation(
		u.Object,
		&structured,
		true, /* returnUnknownFields */
	)
	if err != nil {
		log.Fatal(err)
	}
}
```

#### Anything else we need to know?

I can send a PR if you confirm this should be fixed and that the fix is to skip the private fields.

#### Kubernetes version

The go module looks like
```
go 1.22

require k8s.io/apimachinery v0.29.3
```

#### Cloud provider

No cloud provider.

#### OS version

_No response_

#### Install tools

Go 1.22

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

è¯¥Issueæ¶‰åŠæ½œåœ¨çš„å®‰å…¨é£é™©ï¼ŒåŸå› å¦‚ä¸‹ï¼š

**é£é™©åˆ†æï¼š**

1. **å¯è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šå¦‚æœæ”»å‡»è€…èƒ½å¤Ÿæ§åˆ¶ä¼ é€’ç»™`DefaultUnstructuredConverter.FromUnstructuredWithValidation`æ–¹æ³•çš„æœªç»“æ„åŒ–æ•°æ®ï¼Œå¹¶ä¸”ç›®æ ‡ç»“æ„ä½“åŒ…å«ç§æœ‰å­—æ®µï¼Œé‚£ä¹ˆæ”»å‡»è€…å¯ä»¥æ„é€ ç‰¹å®šçš„æ•°æ®è§¦å‘`panic`ã€‚åœ¨ä¸€äº›åº”ç”¨ä¸­ï¼Œç‰¹åˆ«æ˜¯æ¶‰åŠåˆ°å¤„ç†ç”¨æˆ·è¾“å…¥æˆ–å¤–éƒ¨æ•°æ®çš„æœåŠ¡ï¼Œè¿™ç§æƒ…å†µæ˜¯å¯èƒ½å‘ç”Ÿçš„ã€‚

2. **å¯èƒ½æˆä¸ºæ¼æ´å¹¶è¢«åˆ†é…CVEç¼–å·**ï¼šç”±äºè¿™ä¸ª`panic`ä¼šå¯¼è‡´ç¨‹åºå´©æºƒï¼Œå¦‚æœè¯¥ç¨‹åºæ˜¯é•¿æœŸè¿è¡Œçš„æœåŠ¡ï¼ˆå¦‚APIæœåŠ¡å™¨ï¼‰ï¼Œé‚£ä¹ˆæ”»å‡»è€…å¯ä»¥é€šè¿‡å‘é€æ¶æ„æ•°æ®å¯¼è‡´æœåŠ¡å´©æºƒï¼Œé€ æˆæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚æ ¹æ®CVSS 3.1è¯„åˆ†æ ‡å‡†ï¼Œå¦‚æœæ”»å‡»è€…æ— éœ€ä»»ä½•æƒé™å³å¯è§¦å‘æœåŠ¡å´©æºƒï¼Œä¸”å½±å“èŒƒå›´è¾ƒå¤§ï¼Œé‚£ä¹ˆä¸¥é‡æ€§å¯èƒ½è¾¾åˆ°é«˜ï¼ˆHighï¼‰æˆ–æ›´é«˜ã€‚

3. **å¯èƒ½çš„å½±å“**ï¼šæ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¿™ä¸ªæ¼æ´åå¤å´©æºƒç›®æ ‡æœåŠ¡ï¼Œå¯¼è‡´åˆæ³•ç”¨æˆ·æ— æ³•æ­£å¸¸ä½¿ç”¨æœåŠ¡ï¼Œé€ æˆä¸šåŠ¡ä¸­æ–­å’ŒæœåŠ¡ä¸å¯ç”¨ã€‚

**Proof of Conceptï¼š**

ä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€å•çš„è¯æ˜æ¦‚å¿µï¼ˆPoCï¼‰ï¼Œå±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨è¯¥é—®é¢˜è§¦å‘`panic`ï¼š

```go
package main

import (
    "k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
    "k8s.io/apimachinery/pkg/runtime"
    "log"
)

type VulnerableStruct struct {
    Spec VulnerableSpec
}

type VulnerableSpec struct {
    privateField []byte // ç§æœ‰å­—æ®µï¼Œæœªå¯¼å‡º
    PublicField  string
}

func main() {
    // æ”»å‡»è€…æ„é€ çš„æœªç»“æ„åŒ–æ•°æ®
    u := &unstructured.Unstructured{
        Object: map[string]interface{}{
            "spec": map[string]interface{}{
                "publicField": "malicious data",
            },
        },
    }

    // ç›®æ ‡ç»“æ„ä½“åŒ…å«ç§æœ‰å­—æ®µ
    vulnerable := VulnerableStruct{}
    // åœ¨è½¬æ¢æ—¶ä¼šè§¦å‘panic
    err := runtime.DefaultUnstructuredConverter.FromUnstructuredWithValidation(
        u.Object,
        &vulnerable,
        true, // returnUnknownFields
    )
    if err != nil {
        log.Fatal(err)
    }
}
```

å½“è¿è¡Œä¸Šè¿°ä»£ç æ—¶ï¼Œä¼šå‡ºç°å¦‚ä¸‹`panic`ï¼š

```
panic: reflect: reflect.Value.Set using value obtained using unexported field

goroutine 1 [running]:
reflect.flag.mustBeAssignableSlow(0x...)
    /usr/local/go/src/reflect/value.go:...
...
```

**ç»“è®ºï¼š**

è¯¥Issueæè¿°çš„é—®é¢˜å­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ï¼Œç¬¦åˆä»¥ä¸‹é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

- **è¯¥é£é™©èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ”»å‡»è€…å¯ä»¥é€šè¿‡æ„é€ ç‰¹å®šçš„æœªç»“æ„åŒ–æ•°æ®ï¼Œè§¦å‘æœåŠ¡ç«¯çš„`panic`ã€‚
- **è¯¥é£é™©æœ‰å¯èƒ½æˆä¸ºä¸€ä¸ªæ¼æ´ï¼Œå¹¶è¢«åˆ†é…CVEç¼–å·ï¼Œä½¿ç”¨CVSS 3.1è¯„åˆ†æ ‡å‡†æ‰“åˆ†ï¼Œç»“æœåœ¨Highä»¥ä¸Š**ï¼šæœªç»æˆæƒçš„æ”»å‡»è€…å¯å¯¼è‡´æœåŠ¡å´©æºƒï¼Œå±äºé«˜å±çš„æ‹’ç»æœåŠ¡æ¼æ´ã€‚
- **ä¸å±äºæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å®æ–½çš„æ”»å‡»**ï¼šæ”»å‡»è€…ä¸éœ€è¦ç‰¹æ®Šæƒé™å³å¯å‘é€æ¶æ„æ•°æ®ï¼Œè§¦å‘æ¼æ´ã€‚

å› æ­¤ï¼Œéœ€è¦å°½å¿«ä¿®å¤è¯¥é—®é¢˜ï¼Œå»ºè®®åœ¨`DefaultUnstructuredConverter`çš„å®ç°ä¸­ï¼Œå¿½ç•¥ç§æœ‰å­—æ®µæˆ–å®‰å…¨åœ°å¤„ç†ä¸å¯èµ‹å€¼çš„å­—æ®µï¼Œé˜²æ­¢`panic`çš„å‘ç”Ÿã€‚

---

## Issue #124127 kubelet stuck in WaitForAttachAndMount and can not start containerï¼Œwith using feature NewVolumeManagerReconstruction

- Issue é“¾æ¥ï¼š[#124127](https://github.com/kubernetes/kubernetes/issues/124127)

### Issue å†…å®¹

#### What happened?

My pod's container exit and wait for restart. But kubelet also restart, can not start container, and the log show below error loop.
`E0323 03:30:04.285195    6291 pod_workers.go:1301] "Error syncing pod, skipping" err="unmounted volumes=[datadir], unattached volumes=[], failed to process volumes=[]: context deadline exceeded" pod="manager/secretstore-0" podUID="009ed499-06a4-444f-8625-9338c74b8680"
`

My pod volume is ready:
`0323 03:28:19.870092    6291 operation_generator.go:663] "MountVolume.MountDevice succeeded for volume \"pvc-15538fe5-8e71-4720-8bac-9f8f5a9b156f\" (UniqueName: \"kubernetes.io/csi/com.huawei.cloudsop.localcsi^pvc-15538fe5-8e71-4720-8bac-9f8f5a9b156f\") pod \"secretstore-0\" (UID: \"009ed499-06a4-444f-8625-9338c74b8680\") device mount path \"/var/lib/kubelet/plugins/kubernetes.io/csi/com.huawei.cloudsop.localcsi/2e54a5bbf3668f793b0fefadbf3807bbc5d1dc80505f64be3a8a786e3311e632/globalmount\"" pod="manager/secretstore-0"
`

`I0323 03:28:20.166568    6291 operation_generator.go:720] "MountVolume.SetUp succeeded for volume \"pvc-15538fe5-8e71-4720-8bac-9f8f5a9b156f\" (UniqueName: \"kubernetes.io/csi/com.huawei.cloudsop.localcsi^pvc-15538fe5-8e71-4720-8bac-9f8f5a9b156f\") pod \"secretstore-0\" (UID: \"009ed499-06a4-444f-8625-9338c74b8680\") " pod="manager/secretstore-0"
`

the pod.Spec.volume like this:
![image](https://github.com/kubernetes/kubernetes/assets/11376722/908d1db6-1231-420a-b184-c1fa10429fb0)


`pvc-15538fe5-8e71-4720-8bac-9f8f5a9b156f` is the pv name of pvc `datadir-secretstore-0`

#### What did you expect to happen?

The container can start

#### How can we reproduce it (as minimally and precisely as possible)?
1.Open featureGates `NewVolumeManagerReconstruction`
2.creat a pod with pvc which's pv name is different from the outer Volume name in pod.spec
3.restart kubelet
4.kill the container process

#### Anything else we need to know?

_No response_

#### Kubernetes version

v1.28.1

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ç»è¿‡åˆ†æï¼Œè¯¥é—®é¢˜å­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

**åŸå› ï¼š**

å½“å¯ç”¨ `NewVolumeManagerReconstruction` ç‰¹æ€§æ—¶ï¼Œå¦‚æœæ”»å‡»è€…èƒ½å¤Ÿåˆ›å»ºä¸€ä¸ª Podï¼Œå…¶ä¸­ä½¿ç”¨çš„ PVC çš„ PV åç§°ä¸ Pod.spec ä¸­å®šä¹‰çš„å·åç§°ä¸åŒï¼Œç„¶åé€šè¿‡é‡å¯ kubelet å¹¶ç»ˆæ­¢å®¹å™¨è¿›ç¨‹ï¼Œå¯èƒ½å¯¼è‡´ kubelet å¡åœ¨ `WaitForAttachAndMount` çŠ¶æ€ï¼Œæ— æ³•å¯åŠ¨å®¹å™¨ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œæ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¿™ä¸ªæ¼æ´å¯¼è‡´èŠ‚ç‚¹ä¸Šçš„æœåŠ¡ä¸å¯ç”¨ï¼Œé€ æˆæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚

**å¯èƒ½çš„å½±å“ï¼š**

- **å¯ç”¨æ€§å½±å“ï¼ˆAvailability Impactï¼‰ï¼šé«˜ï¼ˆHighï¼‰**ã€‚æ”»å‡»è€…å¯ä»¥é€šè¿‡ç²¾å¿ƒæ„é€ çš„ Pod å’Œ PVCï¼Œå¯¼è‡´ kubelet æ— æ³•æ­£å¸¸å·¥ä½œï¼Œå½±å“èŠ‚ç‚¹ä¸Šå®¹å™¨çš„å¯åŠ¨ï¼Œè¿›è€Œå½±å“ä¸šåŠ¡æœåŠ¡çš„å¯ç”¨æ€§ã€‚
- **æ½œåœ¨é£é™©ï¼š** è¯¥æ¼æ´å¯èƒ½è¢«æ¶æ„ç”¨æˆ·åˆ©ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å¤šç§Ÿæˆ·çš„é›†ç¾¤ç¯å¢ƒä¸­ï¼Œå¦‚æœç”¨æˆ·å…·æœ‰åˆ›å»º Pod å’Œ PVC çš„æƒé™ï¼Œä»–ä»¬å¯èƒ½åˆ©ç”¨æ­¤æ¼æ´å½±å“å…¶ä»–ç”¨æˆ·çš„æœåŠ¡ã€‚

**PoCï¼ˆæ¦‚å¿µéªŒè¯ï¼‰ï¼š**

1. **å¯ç”¨ç‰¹æ€§é—¨æ§ï¼š** åœ¨é›†ç¾¤ä¸­å¼€å¯ `NewVolumeManagerReconstruction` ç‰¹æ€§ã€‚
   
2. **åˆ›å»ºç‰¹æ®Šçš„ Pod å’Œ PVCï¼š**

   - åˆ›å»ºä¸€ä¸ª Podï¼Œå…¶ä¸­å¼•ç”¨çš„å·åç§°ä¸å®é™…ç»‘å®šçš„ PV åç§°ä¸åŒã€‚
   - ä¾‹å¦‚ï¼Œåœ¨ Pod.spec ä¸­å®šä¹‰çš„å·åç§°ä¸º `datadir`ï¼Œä½†ç»‘å®šçš„ PV åç§°ä¸º `pvc-xxxxx`ã€‚

3. **é‡å¯ kubeletï¼š** åœ¨èŠ‚ç‚¹ä¸Šé‡å¯ kubelet æœåŠ¡ã€‚

4. **ç»ˆæ­¢å®¹å™¨è¿›ç¨‹ï¼š** æ‰‹åŠ¨æ€æ­»å®¹å™¨è¿›ç¨‹ï¼Œæ¨¡æ‹Ÿå®¹å™¨æ„å¤–æŒ‚æ‰çš„æƒ…å†µã€‚

5. **è§‚å¯Ÿç»“æœï¼š** kubelet ä¼šå¡åœ¨ `WaitForAttachAndMount` çŠ¶æ€ï¼Œæ— æ³•é‡æ–°å¯åŠ¨å®¹å™¨ï¼Œæ—¥å¿—ä¸­å‡ºç°ç±»ä¼¼é”™è¯¯ï¼š

   ```
   "Error syncing pod, skipping" err="unmounted volumes=[datadir], unattached volumes=[], failed to process volumes=[]: context deadline exceeded"
   ```

**CVSS 3.1 è¯„åˆ†ï¼š**

- **æ”»å‡»å‘é‡ï¼ˆAVï¼‰ï¼šç½‘ç»œï¼ˆNï¼‰** - æ”»å‡»è€…å¯ä»¥é€šè¿‡ç½‘ç»œè®¿é—® Kubernetes API æ¥æäº¤æ¶æ„çš„ Pod é…ç½®ã€‚
- **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰ï¼šä½ï¼ˆLï¼‰** - æ”»å‡»å¹¶ä¸å¤æ‚ï¼Œåªéœ€æäº¤ç‰¹å®šé…ç½®çš„ Podã€‚
- **æƒé™è¦æ±‚ï¼ˆPRï¼‰ï¼šä½ï¼ˆLï¼‰** - éœ€è¦å…·å¤‡åˆ›å»º Pod å’Œ PVC çš„æƒé™ï¼Œé€šå¸¸åœ¨å¤šç§Ÿæˆ·ç¯å¢ƒä¸­ï¼Œæ™®é€šç”¨æˆ·å¯èƒ½å…·å¤‡åœ¨å…¶å‘½åç©ºé—´ä¸­åˆ›å»ºèµ„æºçš„æƒé™ã€‚
- **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰ï¼šæ— ï¼ˆNï¼‰** - ä¸éœ€è¦å…¶ä»–ç”¨æˆ·çš„äº¤äº’ã€‚
- **ä½œç”¨èŒƒå›´ï¼ˆSï¼‰ï¼šæœªæ”¹å˜ï¼ˆUï¼‰** - æ”»å‡»ä»…å½±å“ kubeletï¼Œæœ¬åœ°èŒƒå›´å†…ã€‚
- **æœºå¯†æ€§ï¼ˆCï¼‰ï¼šæ— ï¼ˆNï¼‰** - ä¸å½±å“æ•°æ®æœºå¯†æ€§ã€‚
- **å®Œæ•´æ€§ï¼ˆIï¼‰ï¼šæ— ï¼ˆNï¼‰** - ä¸å½±å“æ•°æ®å®Œæ•´æ€§ã€‚
- **å¯ç”¨æ€§ï¼ˆAï¼‰ï¼šé«˜ï¼ˆHï¼‰** - ä¸¥é‡å½±å“æœåŠ¡å¯ç”¨æ€§ã€‚

æ ¹æ®ä»¥ä¸ŠæŒ‡æ ‡ï¼ŒCVSS è¯„åˆ†ä¸º **7.5ï¼ˆé«˜ï¼‰**ã€‚

å› æ­¤ï¼Œè¯¥é—®é¢˜ç¬¦åˆå®‰å…¨é£é™©çš„åˆ¤æ–­æ ‡å‡†ï¼Œå¯èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨ï¼Œé€ æˆä¸¥é‡çš„æœåŠ¡å¯ç”¨æ€§å½±å“ï¼Œéœ€å¼•èµ·é‡è§†å¹¶åŠæ—¶ä¿®å¤ã€‚

---

## Issue #124125 APF borrowing by exempt does not match KEP

- Issue é“¾æ¥ï¼š[#124125](https://github.com/kubernetes/kubernetes/issues/124125)

### Issue å†…å®¹

#### What happened?

In reviewing the KEP and the implementation, I noticed a difference in the way that the exempt priority level borrows from the others. In the KEP, in section https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1040-priority-and-fairness#dispatching (this material was added in https://github.com/kubernetes/enhancements/pull/3906), we see (among many other things) the following definition.

```
MinCurrentCL(i) = max( MinCL(i), min( NominalCL(i), HighSeatDemand(i) ) ) -- if non-exempt
MinCurrentCL(i) = max( MinCL(i), HighSeatDemand(i) )                      -- if exempt
```

But in the implementation (https://github.com/kubernetes/kubernetes/blob/3dedb8eb8c122d0a3221a5842c1d6697d8958151/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/apf_controller.go#L407, introduced in #118782) there is no special treatment for exempt. It gets its MinCurrentCL computed by the same formula as non-exempt levels. In the default configuration (in which the exempt priority level has a nominal concurrency limit of zero), this means that the exempt priority level gets MinCurrentCL=0.

The KEP continues with some considerations for what to do if the exempt priority levels want to use up the whole server concurrency limit, or enough to prevent the non-exempt priority levels from getting their minimum allocations. These are also absent from the implementation --- naturally enough, since those situations cannot arise in the current implementation.

#### What did you expect to happen?

Consistency

#### How can we reproduce it (as minimally and precisely as possible)?

This is from code and doc inspection, as described above.

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.28.0 and later.


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

N/A

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

è¯¥Issueæ¶‰åŠåˆ°Kubernetesä¸­APFï¼ˆAPIä¼˜å…ˆçº§å’Œå…¬å¹³æ€§ï¼‰æ¨¡å—çš„å®ç°ä¸KEPï¼ˆKuberneteså¢å¼ºææ¡ˆï¼‰ä¹‹é—´çš„ä¸ä¸€è‡´ï¼Œå…·ä½“æ˜¯å…³äºâ€œexemptâ€ï¼ˆè±å…ï¼‰ä¼˜å…ˆçº§å±‚çº§å€Ÿç”¨é€»è¾‘çš„å·®å¼‚ã€‚

**é£é™©åˆ†æï¼š**

1. **é—®é¢˜æè¿°ï¼š**
   - åœ¨KEPä¸­ï¼Œå¯¹äºâ€œexemptâ€ä¼˜å…ˆçº§å±‚çº§ï¼Œå…¶`MinCurrentCL(i)`çš„è®¡ç®—å…¬å¼åº”ä¸ºï¼š
     ```
     MinCurrentCL(i) = max( MinCL(i), HighSeatDemand(i) ) -- if exempt
     ```
   - ä½†åœ¨å®é™…å®ç°ä¸­ï¼Œæ²¡æœ‰å¯¹â€œexemptâ€å±‚çº§è¿›è¡Œç‰¹æ®Šå¤„ç†ï¼Œå®ƒä¸éè±å…å±‚çº§ä½¿ç”¨ç›¸åŒçš„è®¡ç®—å…¬å¼ã€‚è¿™å¯¼è‡´åœ¨é»˜è®¤é…ç½®ä¸‹ï¼ˆ`exempt`å±‚çº§çš„`NominalCL`ä¸º0ï¼‰ï¼Œ`MinCurrentCL`è¢«è®¡ç®—ä¸º0ã€‚

2. **æ½œåœ¨é£é™©ï¼š**
   - **æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»**ï¼š
     - ç”±äº`exempt`å±‚çº§çš„`MinCurrentCL`è¢«è®¡ç®—ä¸º0ï¼Œæ„å‘³ç€åœ¨é«˜å¹¶å‘æˆ–é«˜è´Ÿè½½æƒ…å†µä¸‹ï¼Œ`exempt`å±‚çº§å¯èƒ½æ— æ³•è·å¾—ä»»ä½•å¹¶å‘å¸­ä½ï¼ˆseatsï¼‰ï¼Œä»è€Œå¯¼è‡´è¯¥å±‚çº§çš„è¯·æ±‚è¢«æ‹’ç»æˆ–å»¶è¿Ÿã€‚
     - æ”»å‡»è€…å¯ä»¥é€šè¿‡å¤§é‡å‘é€éè±å…è¯·æ±‚ï¼Œå ç”¨å…¨éƒ¨çš„æœåŠ¡å™¨å¹¶å‘å¸­ä½ï¼Œå¯¼è‡´`exempt`å±‚çº§çš„å…³é”®ç³»ç»Ÿè¯·æ±‚æ— æ³•è¢«å¤„ç†ï¼Œè¿›è€Œå¼•å‘æ‹’ç»æœåŠ¡ã€‚

3. **å½±å“ï¼š**
   - **å…³é”®ç³»ç»ŸæœåŠ¡ä¸­æ–­**ï¼š
     - `exempt`å±‚çº§é€šå¸¸ç”¨äºå¤„ç†å…³é”®çš„ç³»ç»Ÿçº§è¯·æ±‚ï¼Œå¦‚é›†ç¾¤ç®¡ç†å‘˜çš„ç´§æ€¥æ“ä½œã€ç³»ç»Ÿå¥åº·æ£€æŸ¥ç­‰ã€‚å¦‚æœè¿™äº›è¯·æ±‚æ— æ³•åŠæ—¶å¤„ç†ï¼Œå¯èƒ½å¯¼è‡´é›†ç¾¤ç®¡ç†åŠŸèƒ½å¤±æ•ˆï¼Œå½±å“ç³»ç»Ÿç¨³å®šæ€§ã€‚
   - **å®‰å…¨éšæ‚£åŠ å‰§**ï¼š
     - æ”»å‡»è€…æ— éœ€ç‰¹æ®Šæƒé™ï¼Œä»…é€šè¿‡å ç”¨API Serverçš„èµ„æºï¼Œå³å¯å½±å“åˆ°é«˜ä¼˜å…ˆçº§çš„ç³»ç»Ÿè¯·æ±‚ï¼Œæ˜¯ä¸€ç§é«˜å±é£é™©ã€‚

4. **ç¬¦åˆCVEåˆ†é…æ¡ä»¶**ï¼š
   - **å¯è¢«æ”»å‡»è€…åˆ©ç”¨**ï¼šæ”»å‡»è€…å¯ä»¥é€šè¿‡å‘é€å¤§é‡æ™®é€šè¯·æ±‚æ¥è§¦å‘è¯¥æ¼æ´ã€‚
   - **CVSSè¯„åˆ†åœ¨Highä»¥ä¸Š**ï¼š
     - **æ”»å‡»å‘é‡ï¼ˆAVï¼‰**ï¼šç½‘ç»œï¼ˆNï¼‰
     - **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰**ï¼šä½ï¼ˆLï¼‰
     - **ç‰¹æƒè¦æ±‚ï¼ˆPRï¼‰**ï¼šæ— ï¼ˆNï¼‰
     - **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰**ï¼šæ— ï¼ˆNï¼‰
     - **å½±å“èŒƒå›´ï¼ˆSï¼‰**ï¼šæœªæ”¹å˜ï¼ˆUï¼‰
     - **æœºå¯†æ€§å½±å“ï¼ˆCï¼‰**ï¼šæ— ï¼ˆNï¼‰
     - **å®Œæ•´æ€§å½±å“ï¼ˆIï¼‰**ï¼šæ— ï¼ˆNï¼‰
     - **å¯ç”¨æ€§å½±å“ï¼ˆAï¼‰**ï¼šé«˜ï¼ˆHï¼‰
     - ç»¼ä¸Šï¼Œåˆæ­¥CVSSè¯„åˆ†ä¸ºï¼š**7.5ï¼ˆHighï¼‰**

**Proof of Conceptï¼ˆæ¦‚å¿µéªŒè¯ï¼‰ï¼š**

1. **ç¯å¢ƒå‡†å¤‡ï¼š**
   - éƒ¨ç½²ä¸€ä¸ªé»˜è®¤é…ç½®çš„Kubernetesé›†ç¾¤ï¼ˆç‰ˆæœ¬1.28.0åŠä»¥ä¸Šï¼‰ã€‚
   - ç¡®ä¿APFï¼ˆAPI Priority and Fairnessï¼‰åŠŸèƒ½å¯ç”¨ï¼Œå¹¶ä½¿ç”¨é»˜è®¤çš„ä¼˜å…ˆçº§é…ç½®ã€‚

2. **æ­¥éª¤ï¼š**
   - **è§‚å¯Ÿ`exempt`å±‚çº§çš„MinCurrentCLå€¼**ï¼š
     - éªŒè¯`exempt`å±‚çº§çš„`MinCurrentCL`è¢«è®¡ç®—ä¸º0ã€‚
   - **æ”»å‡»è€…è¡ŒåŠ¨**ï¼š
     - æ”»å‡»è€…ç¼–å†™è„šæœ¬ï¼ŒæŒç»­å‘é€å¤§é‡çš„éè±å…APIè¯·æ±‚ï¼ˆå¦‚æ™®é€šç”¨æˆ·çš„List Podsæ“ä½œï¼‰ã€‚
     - è¿™äº›è¯·æ±‚ä¼šå ç”¨API Serverçš„å¹¶å‘å¸­ä½ã€‚
   - **å½±å“éªŒè¯**ï¼š
     - ä½œä¸ºé›†ç¾¤ç®¡ç†å‘˜ï¼Œå°è¯•æ‰§è¡Œé«˜ä¼˜å…ˆçº§çš„`exempt`è¯·æ±‚ï¼ˆå¦‚`kubectl get nodes`ï¼‰ã€‚
     - å‘ç°è¿™äº›è¯·æ±‚è¢«å»¶è¿Ÿå¤„ç†æˆ–è¶…æ—¶ã€‚
   - **ç»“æœ**ï¼š
     - è¯æ˜æ”»å‡»è€…å¯ä»¥é€šè¿‡å ç”¨API Serverèµ„æºï¼Œå¯¼è‡´`exempt`å±‚çº§çš„è¯·æ±‚æ— æ³•åŠæ—¶å¤„ç†ï¼Œé€ æˆæ‹’ç»æœåŠ¡ã€‚

**æ€»ç»“ï¼š**

è¯¥Issueæš´éœ²äº†Kubernetesåœ¨å¤„ç†`exempt`ä¼˜å…ˆçº§å±‚çº§æ—¶çš„å®ç°åå·®ï¼Œå¯¼è‡´æ”»å‡»è€…å¯ä»¥é€šè¿‡å¤§é‡æ™®é€šè¯·æ±‚ï¼Œå½±å“åˆ°é«˜ä¼˜å…ˆçº§ç³»ç»Ÿè¯·æ±‚çš„å¤„ç†ï¼Œé€ æˆæ‹’ç»æœåŠ¡æ”»å‡»ã€‚ç”±äºæ”»å‡»è€…æ— éœ€ç‰¹æ®Šæƒé™å³å¯å®æ–½ï¼Œä¸”å½±å“ä¸¥é‡ï¼Œç¬¦åˆåˆ†é…CVEçš„æ¡ä»¶ï¼ŒCVSSè¯„åˆ†åœ¨Highä»¥ä¸Šã€‚

---

# ğŸ“Œ ä¸æ¶‰åŠå®‰å…¨é£é™©çš„ Issues (60 ä¸ª)

## Issue #124648 Readiness probe stops too early at eviction

- Issue é“¾æ¥ï¼š[#124648](https://github.com/kubernetes/kubernetes/issues/124648)

### Issue å†…å®¹

#### What happened?

When kubelet evicts a pod, the ready condition doesnâ€™t get `NotReady` during the pod termination even if a `readinessProbe` is configured.

#### What did you expect to happen?

A readiness probe works during a pod termination so that the pod gets `NotReady` as early as possible.


#### How can we reproduce it (as minimally and precisely as possible)?

Use this `readiness.yaml`:
```
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: readiness-script-configmap
data:
  readiness-script.sh: |
    #!/bin/sh
    handler() {
      rm /tmp/ready
      sleep 20
    }
    touch /tmp/ready
    trap handler SIGTERM
    while true; do
      sleep 1
    done
---
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: readiness-container
  name: readiness-test
spec:
  containers:
  - command:
    - sh
    - /script/readiness-script.sh
    image: busybox
    name: readiness
    readinessProbe:
      exec:
        command:
        - cat
        - /tmp/ready
      initialDelaySeconds: 3
      periodSeconds: 3
    volumeMounts:
    - name: readiness-script
      mountPath: /script
  - command:
    - sh
    - -c
    - for i in `seq 100`; do dd if=/dev/random of=file${i} bs=1048576 count=1 2>/dev/null; sleep .1; done; while true; do sleep 5; done
    name: disk-consumer
    image: busybox
    resources:
      limits:
        ephemeral-storage: "100Mi"
      requests:
        ephemeral-storage: "100Mi"
  volumes:
  - name: readiness-script
    configMap:
      name: readiness-script-configmap
```

Create resources and wait an eviction happens:
```
$ kubectl create -f readiness.yaml; kubectl get pods readiness-test -w
configmap/readiness-script-configmap created
pod/readiness-test created
NAME         	READY   STATUS          	RESTARTS   AGE
readiness-test   0/2 	ContainerCreating   0      	0s
readiness-test   1/2 	Running         	0      	3s
readiness-test   2/2 	Running         	0      	7s
readiness-test   0/2 	Error           	0      	46s
readiness-test   0/2 	Error           	0      	47s

```

When deleting this pod, the readiness probe works during termination:
```
$ kubectl create -f readiness.yaml; (sleep 15; kubectl delete pod readiness-test) & kubectl get pods -w
configmap/readiness-script-configmap created
pod/readiness-test created
[1] 137999
NAME             READY   STATUS              RESTARTS   AGE
readiness-test   0/2     ContainerCreating   0          0s
readiness-test   1/2     Running             0          2s
readiness-test   2/2     Running             0          6s
pod "readiness-test" deleted
readiness-test   2/2     Terminating         0          15s
readiness-test   1/2     Terminating         0          21s
readiness-test   0/2     Terminating         0          45s
readiness-test   0/2     Terminating         0          45s
readiness-test   0/2     Terminating         0          45s
readiness-test   0/2     Terminating         0          45s

```


#### Anything else we need to know?

I guess this issue is caused as follows:

At eviction, a pod phase is set to `PodFailed` internally in `podStatusFn` before stopping containers in the pod:
https://github.com/kubernetes/kubernetes/blob/0d8f996aa9a1667d5994f10e03de5be21bf205e0/pkg/kubelet/kubelet.go#L2026-L2029
https://github.com/kubernetes/kubernetes/blob/0d8f996aa9a1667d5994f10e03de5be21bf205e0/pkg/kubelet/eviction/eviction_manager.go#L605-L608

Because the internal pod phase is `PodFailed`, the probe worker finishes working without probing containers at termination:
https://github.com/kubernetes/kubernetes/blob/dd68c5f2409fec7176e6172d6f9d97bd6447c4da/pkg/kubelet/prober/worker.go#L203-L216


#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.2
</details>


#### Cloud provider

<details>
None.
I reproduced on my laptop.
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kind
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124646 Improve the jitter value for kubelet's serviceAccountToken projected volume source

- Issue é“¾æ¥ï¼š[#124646](https://github.com/kubernetes/kubernetes/issues/124646)

### Issue å†…å®¹

The current jittering logic in [kubelet token_manager](https://github.com/kubernetes/kubernetes/blob/dd68c5f2409fec7176e6172d6f9d97bd6447c4da/pkg/kubelet/token/token_manager.go#L42) refreshes the token once it has 20% of its lifetime remaining, with 10 seconds of jitter.

Experimentally, this doesn't seem to be enough to really blunt spikes of refresh requests that come from many workloads having a synced refresh.  This can occur when:

* A large deployment is scaled up.  All the new replicas will have synced-up refresh traffic.
* A kubelet restarts.  Because kubelet only holds the tokens in an in-memory cache, it will need to acquire new tokens for all pods on the node after it restarts.  After that, all of the pods will have synced-up refresh traffic.

As an example, here's serviceaccount/token API traffic from a real cluster.  Note how there is a repeating spike every 48 minutes, which is what we would expect from the "refresh after 80%" logic, and the default setting of token lifetime for 1 hour.  Effective jittering logic would be noticeably smoothing these peaks.

We should increase the `maxJitter` value.  Perhaps 5 minutes would be a good value.

![image](https://github.com/kubernetes/kubernetes/assets/1017202/133a6c5c-5a6b-4ad1-a1e8-ffb29fae3059)

/sig auth
/kind bug

### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124628 garbagecollector.GetDeletableResources logs json encoding error instead of actual error

- Issue é“¾æ¥ï¼š[#124628](https://github.com/kubernetes/kubernetes/issues/124628)

### Issue å†…å®¹

#### What happened?

In `garbagecollector.GetDeletableResources()` if `discoveryClient.ServerPreferredResources()` fails with a lookup error, we log something like the following due to json.Marshal() refusing to marshal a struct type.
```
garbagecollector.go:835] "failed to discover some groups" groups="<internal error: json: unsupported type: map[schema.GroupVersion]error>"
```

#### What did you expect to happen?

Get a usable error message, like:
```
"failed to discover some groups" groups="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1"
```


#### How can we reproduce it (as minimally and precisely as possible)?

Encounter a condition that causes a lookup failure. Mine was with a brand-new k3s install.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3-k3s1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.3-k3s1

```

</details>


#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="SLE Micro"
VERSION="5.5"
VERSION_ID="5.5"
PRETTY_NAME="SUSE Linux Enterprise Micro 5.5"
ID="sle-micro"
ID_LIKE="suse"
ANSI_COLOR="0;32"
CPE_NAME="cpe:/o:suse:sle-micro:5.5"

$ uname -a
Linux work-router.domain 5.14.21-150500.55.52-default #1 SMP PREEMPT_DYNAMIC Tue Mar 5 16:53:41 UTC 2024 (a62851f) x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124605 Server-side apply bumps resourceVersion when re-applying the same object

- Issue é“¾æ¥ï¼š[#124605](https://github.com/kubernetes/kubernetes/issues/124605)

### Issue å†…å®¹

#### What happened?

When continuously re-applying the same object via a controller or kubectl with SSA the resourceVersion increases every time.

#### What did you expect to happen?

I expected the resourceVersion to stay the same if the client always sends the same object via SSA, especially if the  resulting object doesn't change (apart from resourceVersion & managedFields, but these are managed by the apiserver).

#### How can we reproduce it (as minimally and precisely as possible)?

Apply this CRD: 

```bash
kubectl apply -f ./crd_machines.yaml
```

<details>
<summary>crd_machines.yaml</summary>

```yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: machines.cluster.x-k8s.io
spec:
  group: cluster.x-k8s.io
  names:
    kind: Machine
    listKind: MachineList
    plural: machines
    singular: machine
  scope: Namespaced
  versions:
  - name: v1beta1
    schema:
      openAPIV3Schema:
        properties:
          apiVersion:
            type: string
          kind:
            type: string
          metadata:
            type: object
          spec:
            properties:
             infrastructureRef:
               properties:
                 name:
                   type: string
                 namespace:
                   type: string
                   default: default-namespace
               type: object
               x-kubernetes-map-type: atomic
            type: object
        type: object
    served: true
    storage: true
```

</details>

Apply this CR:

```bash
kubectl apply -f ./cr_machine.yaml --server-side=true --field-manager=test-manager
```

<details>
<summary>cr_machine.yaml</summary>

```yaml
apiVersion: cluster.x-k8s.io/v1beta1
kind: Machine
metadata:
  name: machine-1
  namespace: default
spec:
  infrastructureRef:
    name: infrastructure-machine-1
```

</details>

Get state of the CR after first apply
```bash
k get machine -o yaml --show-managed-fields > /tmp/machine-1.yaml
```

Apply the CR again
```bash
kubectl apply -f ./cr_machine.yaml --server-side=true --field-manager=test-manager
```

Get state of the CR after second apply
```bash
k get machine -o yaml --show-managed-fields > /tmp/machine-2.yaml
```

Diff
```bash
diff /tmp/machine-1.yaml /tmp/machine-2.yaml
```

```bash
16c16
<       time: "2024-04-29T09:39:07Z"
---
>       time: "2024-04-29T09:39:40Z"
19c19
<     resourceVersion: "1167"
---
>     resourceVersion: "1209"
```

=> The resourceVersion and the "time" field of the managedField entry of the "test-manager" increases.




#### Anything else we need to know?

The use case we have is a lot more complex than this simple scenario. The tl;dr is that we want to implement caching for our SSA calls, so that we don't have SSA calls on every reconcile of our controller if nothing actually changes. This is relatively hard to do if the resourceVersion increases after every apply.


I took a closer look at the APIserver and ~ the following happens:

1. the "incoming" SSA patch is applied over the live object from etcd
    1. this leads to a change because `infrastructureRef` has `x-kubernetes-map-type: atomic` which means the `namespace` field will be dropped
    2. Because of that change the "time" field in the managedField entry gets set to "time.Now()" [here](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apimachinery/pkg/util/managedfields/internal/managedfieldsupdater.go#L76)
    3. later during admission the namespace field gets defaulted again (I also tried this with a mutating webhook, same result)

On a high-level:
* client always applies the same object
* defaulting always results in the same object
* resourceVersion and managedFields[].time continuously go up

I think there are many similar cases that would lead to this behavior. Basically whenever the admission chain mutates a field previously set via SSA.

Notes:
* This only happens if there is >1s between each apply, otherwise the managedField[].time is identical and then the resourceVersion stays the same (because we then don't write to etcd)

#### Kubernetes version

<details>

```console
$  kubectl version
Client Version: v1.29.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.2
```

I also tried this with client-go v0.30 and kube-apiserver v1.30. Same result.

</details>


#### OS version

<details>

Running this natively on Apple Silicon M2. But can't imagine this makes a difference. Also got the same results in Docker Desktop on Mac.

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124586 Restarted kubelet should not evict pods due to the node affinity

- Issue é“¾æ¥ï¼š[#124586](https://github.com/kubernetes/kubernetes/issues/124586)

### Issue å†…å®¹

#### What happened?

Currently, a running pod is being evicted if the assigned node, which does not meet the pod's node affinity requirements anymore, restarts its kubelet.

xref: https://github.com/kubernetes/kubernetes/issues/123980#issuecomment-2071554998

#### What did you expect to happen?

https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity

![Screenshot 2024-04-23 at 3 51 20â€¯PM](https://github.com/kubernetes/kubernetes/assets/19814047/5b15fcd0-3601-4b77-b863-6ca0970b107b)

According to the documentation, node affinity should only affect the scheduling phase. Therefore, the running pod should continue to run without interruption.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a deployment with node affinity.
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: test
  name: test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test
  template:
    metadata:
      labels:
        app: test
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: foo
                operator: In
                values:
                - bar
      containers:
      - image: nginx
        imagePullPolicy: Always
        name: nginx
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      terminationGracePeriodSeconds: 30
```
2. `kubectl label node TARGET_NODE foo=bar --overwrite` so that the pod is scheduled in the TARGET_NODE.
3. Waiting for the pod to be scheduled and running.
4. `kubectl label node TARGET_NODE foo=nonbar --overwrite`
5. Restart the kubelet of the TARGET_NODE.


#### Anything else we need to know?

previous discussion: https://github.com/kubernetes/kubernetes/pull/101218#discussion_r627495203

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.0-alpha.0.470+7d880fd489d1e5-dirty
```

</details>


#### Cloud provider

<details>
kind cluster
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124571 kubectl exec truncates stdout without reporting error

- Issue é“¾æ¥ï¼š[#124571](https://github.com/kubernetes/kubernetes/issues/124571)

### Issue å†…å®¹

#### What happened?

When running a command with `kubectl exec`, the output (data written to stdout) can be truncated without the command returning an error. This results in ambiguity around the command failing or network being torn down with an error (non-zero exit code) versus the command completing successfully and returning complete output (exit code 0).

Unsure what causes variation here, but probably related to network I/O speed and/or latency. Here's an example of drastic failures and inconsistencies on my machine:

```
% for i in 1 2 3 4 5; do kubectl exec "${POD:?}" -- sh -c 'seq 1 2000000' | tail -n1; done
1788552
1994888
809911
1862792
1214600
```

If it doesn't reproduce for you, try adding some network latency or bandwidth restrictions, or increasing the amount of data that goes across the pipe. Visiting fast.com in a browser while running the tests, running more tests, and transferring more output (larger second value in the `seq` command) will help reproduce.

#### What did you expect to happen?

I _either_:

- Get all of the commands output, OR
- Get truncated output and a non-zero exit code

#### How can we reproduce it (as minimally and precisely as possible)?

Run a command against a POD a number of times, see that the output always matches (it should):

```
for i in 1 2 3 4 5; do kubectl exec ${POD:?} -- sh -c 'seq 1 2000000' | tail -n1; done
```

If this doesn't reproduce try:
* using a larger value for the upper bounds to `seq`
* adding latency and/or bandwidth restrictions to the network link
* (wouldn't reproduce for a colleague, had him visit fast.com in a browser while testing, which trigged it right away)

#### Anything else we need to know?

This appears to have been a problem for a long time:

[Kubectl appears to be discarding standard output](https://stackoverflow.com/questions/74710112/kubectl-appears-to-be-discarding-standard-output)
["kubectl exec" sometimes incorrectly returns empty string causing tests to flake](https://github.com/kubernetes/kubernetes/issues/34256)
#34256] (closed w/ only workaround)

#### Kubernetes version

Also reproduces with (all permuatations of) client version 1.29, server version v1.27.11-eks-b9c9ed7:

```
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.27.11-gke.1062000
```

Also reproed with:

```
Client Version: v1.29.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.27.11-eks-b9c9ed7
```

#### Cloud provider

Reproduces against both AWS and GCP managed k8s.

#### OS version

I am on OS X.

```
Darwin hostname 23.3.0 Darwin Kernel Version 23.3.0: Wed Dec 20 21:30:44 PST 2023; root:xnu-10002.81.5~7/RELEASE_ARM64_T6000 arm64
```

Didn't have anyone handy not on OS X to try to repro.

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124547 The endpoint is lost when the APIServer is restored.

- Issue é“¾æ¥ï¼š[#124547](https://github.com/kubernetes/kubernetes/issues/124547)

### Issue å†…å®¹

#### What happened?

It happens when the apiserver goes down and after a few minutes when the apiserver comes back up, some endpoints have notReadyAddresses and do not recover.It's an accidental phenomenon.
The cause is that the endpoint obtained from the Informer is not the latest. In the syncService method of endpoint_controller.go,
`currentEndpoints, err := e.endpointsLister.Endpoints(service.Namespace).Get(service.Name)`
Because the obtained endpoint is not the latest, the system determines that the endpoints are the same. As a result, the endpoint is not updated.
I have added the log to print the endpoint and confirmed this section.
This is what the log shows.
I0425 08:25:57.715142      11 endpoints_controller.go:423] "About to update endpoints for service" service="manager/service-mchiroer"
I0425 08:25:57.715216      11 endpoints_controller.go:516] "endpoints are equal, skipping update" service="manager/service-mchiroer"
I0425 08:25:57.715225      11 endpoints_controller.go:389] "Finished syncing service endpoints" service="manager/service-mchiroer" startTime="83.332Âµs"
So I think the cache in informer is not caching the latest data, which is a bug.

#### What did you expect to happen?

The notReadyAddresses of the endpoint should be changed to addresses when the pod status is updated.

#### How can we reproduce it (as minimally and precisely as possible)?

1ã€Stop the apiserver service of the cluster.
2ã€Recover the apiserver service after a few minutes.
Repeat the preceding operations. The problem will recur.

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.28

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124542 ValidatingAdmissionPolicy:  Cost estimator for extended library was not registered

- Issue é“¾æ¥ï¼š[#124542](https://github.com/kubernetes/kubernetes/issues/124542)

### Issue å†…å®¹

#### What happened?

Thanks for @benluddy who raised the issue and investigated!

We didn't register the cost estimator for extended library hence the cost calculation is inaccurate for expression including extended library.


#### What did you expect to happen?

The cost should reflect the extended library

/cc @jpbetz @benluddy 
/sig api-machinery
/triage accepted

#### How can we reproduce it (as minimally and precisely as possible)?

An example would be:
```
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicy
metadata:
  name: costly-policy
spec:
  failurePolicy: Fail
  matchConstraints:
    resourceRules:
    - apiGroups:   [""]
      apiVersions: ["v1"]
      operations:  ["CREATE"]
      resources:   ["namespaces"]
  validations:
    - expression: "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10].all(x, authorizer.requestResource.check('create').allowed())"
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: "costly-binding"
spec:
  policyName: "costly-policy"
  validationActions: [Deny]
```

The cost of the above expression exceeds the per expression limit but didn't fail.


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124525 PersistentVolumeLabel admission plugin provides wrong Azure region

- Issue é“¾æ¥ï¼š[#124525](https://github.com/kubernetes/kubernetes/issues/124525)

### Issue å†…å®¹

#### What happened?

On Kubernetes 1.29 with PersistentVolumeLabel admission plugin enabled and no in-tree cloud provider configured in the kube-apiserver, the admission plugins adds label `topology.kubernetes.io/region: ""` to in-tree Azure Disk PVs.

The label is completely wrong - the Azure cloud provider is not initialized and [this](https://github.com/kubernetes/kubernetes/blob/d9d45306df052aa5824fd8b2dab4e61f5ecc481c/staging/src/k8s.io/legacy-cloud-providers/azure/azure_managedDiskController.go#L368) just provides empty string. This makes any pod that uses these PVs unschedulable.

As result, all in-tree PVs dynamically provisioned using CSI migration are not usable, because they have a wrong region.

#### What did you expect to happen?

When there is no cloud config provided, PersistentVolumeLabel does not add any labels to PVs. PVs created by the CSI driver + CSI migration already have correct `nodeAffinity`, the labels are useless.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Enable PersistentVolumeLabel admission in your cluster and disable any cloud providers. It does not need access to Azure, you can run it anywhere.

    Example for `local-up-cluster.sh`:

    ```
    ENABLE_ADMISSION_PLUGINS="NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,Priority,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,PersistentVolumeLabel"
    hack/local-up-cluster.sh
    ```

2. Create in-tree Azure Disk PV. Again, we're testing just the admission, we're not going to use the PV in a pod, so no access to Azure is needed. UUIDs were sanitized ;-)

    ```
    $ kubectl create -f <<EOF
    
    apiVersion: v1
    kind: PersistentVolume
    metadata:
      name: myvol
    spec:
      accessModes:
      - ReadWriteOnce
      capacity:
        storage: 5Gi
      azureDisk:
          cachingMode: ReadWrite
          diskName: pvc-ade30d58-5a51-4a62-a83a-142267b74f9e
          diskURI: /subscriptions/4347f140-6c74-4199-b4fc-1938eb6c16c0/resourceGroups/jsafrane-h-jsafrane-h-kxvpr/providers/Microsoft.Compute/disks/pvc-ade30d58-5a51-4a62-a83a-142267b74f9e
          fsType: ext4
          kind: Managed
          readOnly: false
    
    EOF
    ```

Actual result: wrong region label on the PV:

```
$ kubectl get pv --show-labels
LABELS
topology.kubernetes.io/region=
```

Expected result: no labels on the PV.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Server Version: v1.29.3
```

</details>


#### Cloud provider

<details>
None
</details>


#### OS version

_No response_

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---

## Issue #124504 PersistentVolumeLabel admission blocks PVs on Azure + vSphere

- Issue é“¾æ¥ï¼š[#124504](https://github.com/kubernetes/kubernetes/issues/124504)

### Issue å†…å®¹

#### What happened?

In 1.30, the PersistentVolumeLabel admission plugin blocks creation of in-tree AzureDisk and vSphere PVs:

```
persistentvolumes "pvc-2d9b3999-f60d-4797-923f-210f1f75025d" is forbidden:
error querying AzureDisk volume pvc-2d9b3999-f60d-4797-923f-210f1f75025d:
unable to build Azure cloud provider for AzureDisk
```

The reason is that the cloud provider was removed (https://github.com/kubernetes/kubernetes/pull/122857), but the admission plugin still calls GetCloudProvider there, which fails: https://github.com/kubernetes/kubernetes/blob/646fbe6d0a3fa1a100d90a429f039324de9c8138/plugin/pkg/admission/storage/persistentvolume/label/admission.go#L285


#### What did you expect to happen?

The PV should be admitted + created.

#### How can we reproduce it (as minimally and precisely as possible)?

Enable PersistentVolumeLabel admission plugin. For example, when running local-up-cluster.sh:
```
ALLOW_PRIVILEGED=true ENABLE_ADMISSION_PLUGINS="NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,Priority,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota,NodeRestriction,PersistentVolumeLabel" bash -x hack/local-up-cluster.sh
```

Create AzureDisk PV on _any_ 1.30 cluster, you don't need Azure cloud:

```
kubectl create -f <<EOF
apiVersion: v1
kind: PersistentVolume
metadata:
  name: myvol
spec:
  accessModes:
  - ReadWriteOnce
  capacity:
    storage: 5Gi
  azureDisk:
    diskName: test2.vhd
    diskURI: https://someacount.blob.core.windows.net/vhds/test2.vhd
EOF
```

#### Anything else we need to know?

/sig storage
/priority important-soon

#### Kubernetes version

<details>

Today's master
```console
$ kubectl version
Server Version: v1.31.0-alpha.0.24+9791f0d1f39f3f
```

</details>


#### Cloud provider

<details>
Azure
</details>


#### OS version

_No response_

#### Install tools

<details>
local-up-cluster.sh
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124495 K8s error message: unexpected error getting claim reference: selfLink was empty, can't make reference

- Issue é“¾æ¥ï¼š[#124495](https://github.com/kubernetes/kubernetes/issues/124495)

### Issue å†…å®¹

#### What happened?

I installed nfs provisioner and reported an error

#### What did you expect to happen?

How can I solve this problem

#### How can we reproduce it (as minimally and precisely as possible)?

k8s v1.28.2
nfs-client-provisioner:latest

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
v1.28.2
```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>
nfs-client-provisioner:latest
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124453 --hostname-override flag on kubelet is no longer used for Addresses.Hostname value in Kubernetes 1.29

- Issue é“¾æ¥ï¼š[#124453](https://github.com/kubernetes/kubernetes/issues/124453)

### Issue å†…å®¹

#### What happened?

It looks like there was a change that first appeared in Kubernetes 1.29, where in 1.29 the `--hostname-override` flag on kubelet is no longer being used to populate the Node's Addresses.Hostname value.  Here is the relevant output from a `kubectl describe node` for a 1.28 cluster with `hostname-override=10.X.X.X`:

```
Addresses:
  InternalIP:  10.X.X.X
  ExternalIP:  52.X.X.X
  Hostname:    10.X.X.X
```

And here's the output from a 1.29 cluster with the same configuration:

```
Addresses:
  InternalIP:  10.X.X.X
  ExternalIP:  52.X.X.X
```

Here is the full `kubelet` list of parameters in case that is helpful:

`/usr/local/bin/kubelet --config=/etc/kubernetes/kubelet-config.yaml --root-dir=/var/data/kubelet --node-labels=privateVLAN=XXXXXXX,publicVLAN=XXXXXXX --cloud-provider=external --v=2 --kubeconfig=/etc/kubernetes/kubelet-kubeconfig --hostname-override=10.X.X.X --enable-controller-attach-detach=false --version=v1.28.9+IKS --runtime-cgroups=/podruntime/runtime`

This is from an IBM Cloud Kubernetes Service 1.28 and 1.29 cluster.  We do not think there was any cloud-configuration-manager or any other related change that would cause this difference.  Is it possible it is a side effect of https://github.com/kubernetes/kubernetes/pull/121028 ?

#### What did you expect to happen?

I expected the behavior to be consistent between Kubernetes releases, since I didn't find any reference to this change in https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.29.md

#### How can we reproduce it (as minimally and precisely as possible)?

Create an IBM Cloud Kubernetes Service cluster at 1.28 and 1.29 and see the difference

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.4+IKS
$ 
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
root@test-coiu89n20065io30q4n0-bpvg1713759-default-00000251:/# cat /etc/os-release
NAME="Ubuntu"
VERSION="20.04.6 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.6 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal
root@test-coiu89n20065io30q4n0-bpvg1713759-default-00000251:/# uname -a
Linux test-coiu89n20065io30q4n0-bpvg1713759-default-00000251 5.4.0-176-generic #196-Ubuntu SMP Fri Mar 22 16:46:39 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
root@test-coiu89n20065io30q4n0-bpvg1713759-default-00000251:/# 

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124448 GracefulNodeShutdown fail to update Pod status for system critical pods.

- Issue é“¾æ¥ï¼š[#124448](https://github.com/kubernetes/kubernetes/issues/124448)

### Issue å†…å®¹

#### What happened?

Pods with a  `system-cluster-critical` priority class  set are not evicted by Graceful node shutdown when a node is slated to be shutdown. Instead they are evicted by the Taint Manager once the tolerationSeconds period expires. 

#### What did you expect to happen?

I'd expect the presence of priority classes to be orthogonal to whether or not Graceful node shutdown evicts the pods or not. If graceful node shutdown runs on node shutdown then pods should be evicted before Taint Manager runs. 

#### How can we reproduce it (as minimally and precisely as possible)?

Run a deployment with the below tolerations
```
priorityClass: system-cluster-critical
```

Schedule a node shutdown or trigger a shutdown event using gdbus
```
gdbus emit --system --object-path /org/freedesktop/login1 --signal 'org.freedesktop.login1.Manager.PrepareForShutdown' true
```

Observe that pods from the deployment enter a "Terminating" state and have conditions indicating they were evicted by the Taint manager
```
  - lastProbeTime: null
    lastTransitionTime: "2024-04-22T14:30:26Z"
    message: 'Taint manager: deleting due to NoExecute taint'
    reason: DeletionByTaintManager
    status: "True"
    type: DisruptionTarget
```

The pod status is never updated to show TerminationByKubelet and stays in Running state until the eviction by Taint Manager

#### Anything else we need to know?

Kubelet logs will indicate that the node shutdown manager finished terminating the pod but this doesn't get reflected in the API. 

```
shutdown_manager_linux.go:395] "Shutdown manager finished killing pod" pod="kube-system/coredns-5687d5bddb-fqlbh"
```

Pod in the API server
```
coredns-5687d5bddb-fqlbh                  1/1     Terminating   0          89m
```

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.3
```

</details>


#### Cloud provider

<details>
Akamai Connected Cloud (The cloud formerly known as Linode)
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 11 (bullseye)"
NAME="Debian GNU/Linux"
VERSION_ID="11"
VERSION="11 (bullseye)"
VERSION_CODENAME=bullseye
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"

$ uname -a
Linux lke-e2e-bcbc70-test-1a751f3d 6.1.0-0.deb11.17-cloud-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.1.69-1~bpo11+1 (2024-01-05) x86_64 GNU/Linux
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124440 kubelet failed to create containerd task if cgroupRoot defined cpuset and CPU Manager configured with static policy

- Issue é“¾æ¥ï¼š[#124440](https://github.com/kubernetes/kubernetes/issues/124440)

### Issue å†…å®¹

#### What happened?

If the kubelet is configured with `cgroupRoot` and `cpuManagerPolicy: static` and cpuset cgroup is defined with a specific vCPUs range, the kubelet fails to start containerd tasks or update container resources:
```
E0422 11:37:18.746817  109321 remote_runtime.go:343] "StartContainer from runtime service failed" err="rpc error: code = Unknown desc = failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: unable to apply cgroup configuration: failed to write \"0-39\": write /sys/fs/cgroup/cpuset/pods.slice/pods-kubepods.slice/pods-kubepods-burstable.slice/pods-kubepods-burstable-pode444cc90_8458_4d84_8319_a443fe6e975a.slice/cri-containerd-8e99221bf7eb3049f7afa35b3719f7870088c5191508f2bdf47959fe2a677385.scope/cpuset.cpus: permission denied: unknown" containerID="8e99221bf7eb3049f7afa35b3719f7870088c5191508f2bdf47959fe2a677385"
```

#### What did you expect to happen?

The CPU Manager respects the cpuset of the root cgroup and uses its value as the `defaultCpuSet`.

#### How can we reproduce it (as minimally and precisely as possible)?

use following `/var/lib/kubelet/config.yaml`:
```yaml
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 0s
    cacheUnauthorizedTTL: 0s
cgroupDriver: systemd
clusterDNS:
- fd00:10:245::a
clusterDomain: cluster.local
containerRuntimeEndpoint: ""
cpuManagerReconcilePeriod: 0s
evictionPressureTransitionPeriod: 0s
fileCheckFrequency: 0s
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 0s
imageMaximumGCAge: 0s
imageMinimumGCAge: 0s
kind: KubeletConfiguration
logging:
  flushFrequency: 0
  options:
    json:
      infoBufferSize: "0"
  verbosity: 0
memorySwap: {}
nodeStatusReportFrequency: 0s
nodeStatusUpdateFrequency: 0s
rotateCertificates: true
runtimeRequestTimeout: 0s
shutdownGracePeriod: 0s
shutdownGracePeriodCriticalPods: 0s
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
volumeStatsAggPeriod: 0s
cpuManagerPolicy: static
reservedSystemCPUs: 0-1,20-21
cgroupRoot: /pods
```

create cgroup with cpuset:
```sh
for DIR in hugetlb cpuset cpu,cpuacct memory systemd pids; do /bin/mkdir -p /sys/fs/cgroup/$DIR/pods.slice; done
echo 0-1 > /sys/fs/cgroup/cpuset/pods.slice/cpuset.mems
echo 0-1,6-39 > /sys/fs/cgroup/cpuset/pods.slice/cpuset.cpus
```

restart kubelet:
```sh
systemctl stop kubelet
rm /var/lib/kubelet/cpu_manager_state
systemctl start kubelet
cat /var/lib/kubelet/cpu_manager_state
{"policyName":"static","defaultCpuSet":"0-39","checksum":421241391}
```

check logs:
```
journalctl -u kubelet -f
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.4", GitCommit:"fa3d7990104d7c1f16943a67f11b154b71f6a132", GitTreeState:"clean", BuildDate:"2023-07-19T12:20:54Z", GoVersion:"go1.20.6", Compiler:"gc", Platform:"darwin/arm64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"29", GitVersion:"v1.29.1", GitCommit:"bc401b91f2782410b3fb3f9acf43a995c4de90d2", GitTreeState:"clean", BuildDate:"2024-01-17T15:41:12Z", GoVersion:"go1.21.6", Compiler:"gc", Platform:"linux/amd64"}
WARNING: version difference between client (1.27) and server (1.29) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

<details>
```
containerd --version
containerd github.com/containerd/containerd 1.7.2
```
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124433 Cannot delete PVC in terminating state

- Issue é“¾æ¥ï¼š[#124433](https://github.com/kubernetes/kubernetes/issues/124433)

### Issue å†…å®¹

#### What happened?

After deletion of pod, it PVC is getting stuck in Terminating state. Tried to force delete but its not working, even tried to remove finaliser but getting error that that its not allowed as the field is immutable and I'm not allowed to change `spec` field when I did not even change the spec field.

#### What did you expect to happen?

I'm expecting that by removing the finaliser the PVC will get deleted.

#### How can we reproduce it (as minimally and precisely as possible)?

Not sure how to reproduce the steps.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
verison: v1.22.15

```

</details>


#### Cloud provider

<details>
bare metal
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124418 Multiple PTR records being returned for a Pod backed by multiple Services

- Issue é“¾æ¥ï¼š[#124418](https://github.com/kubernetes/kubernetes/issues/124418)

### Issue å†…å®¹

#### What happened?

In the scenario where a Pod has multiple Services pointing at it, multiple PTR records are also created.


#### What did you expect to happen?

A single PTR record to be created.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a Deployment with at least 1 replica in it
2. Create a service that matches the labels of that Deployment
3. Create a second service that matches the labels of that Deployment
4. Query the PTR record for the pod's IP address

#### Anything else we need to know?

This was discovered in trying to understand the behaviour in https://github.com/kubernetes/kubernetes/issues/124207
There is more context here: https://kubernetes.slack.com/archives/C09QYUH5W/p1713551070931899?thread_ts=1713088975.540039&cid=C09QYUH5W

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.2```

</details>


#### Cloud provider

<details>
Discovered using kind
Reproduced on GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124411 Content-Type for RestClient Patch() is Overwritten By Body() Method 

- Issue é“¾æ¥ï¼š[#124411](https://github.com/kubernetes/kubernetes/issues/124411)

### Issue å†…å®¹

#### What happened?

I was writing a method to patch a CRD using the client-go rest client:

```
err := c.restClient.
		Patch(types.MergePatchType).
		Resource(MySuperAwesomeResource).
		SubResource("status").
		Name(name).
		Body(superAwesomeResourceObject).
		Do(ctx).
		Into(result)
```

and was surprised to receive an error:

```
the body of the request was in an unknown format - accepted media types include: application/json-patch+json, application/merge-patch+json, application/apply-patch+yaml
```



#### What did you expect to happen?

The patch to not fail due to an incorrect Content-Type, since I set it via `Patch(types.MergePatchType)`.

#### How can we reproduce it (as minimally and precisely as possible)?

Using the client-go rest client, try to patch a resource by passing a runtime.Object to the `Body()` method.

#### Anything else we need to know?

Here's what I think is happening:

The `Patch()` method [sets](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/rest/client.go#L186) the Content-Type header.

But passing a k8s runtime object to `Body()` [overwrites](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/client-go/rest/request.go#L478) the header with whatever Content-Type is set in the `ClientContentConfig` if (and only if) the passed object is a k8s runtime object. This defaults to `application/json`, and I think is correct for HTTP methods that aren't `Patch()`.

If one gates `Body()`'s setting of that header to not do that for the `PATCH` verb:

```
if r.verb != "PATCH" {
	r.SetHeader("Content-Type", r.c.content.ContentType)
}
```

then things work.

One can also call `SetHeader` after `Body()`:

```
err := c.restClient.
		Patch(types.MergePatchType).
		Resource(MySuperAwesomeResource).
		SubResource("status").
		Name(name).
		Body(superAwesomeResourceObject).
		SetHeader("Content-Type", string(types.MergePatchType)).
		Do(ctx).
		Into(result)
```

and things work.


IDK if I'm doing it wrong here, but this feels more like a bug and less like operator error?

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.3", GitCommit:"25b4e43193bcda6c7328a6d147b1fb73a33f1598", GitTreeState:"clean", BuildDate:"2023-06-14T09:53:42Z", GoVersion:"go1.20.5", Compiler:"gc", Platform:"linux/amd64"}
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.3", GitCommit:"25b4e43193bcda6c7328a6d147b1fb73a33f1598", GitTreeState:"clean", BuildDate:"2023-06-14T09:53:42Z", GoVersion:"go1.20.5", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.3", GitCommit:"25b4e43193bcda6c7328a6d147b1fb73a33f1598", GitTreeState:"clean", BuildDate:"2023-06-15T00:36:28Z", GoVersion:"go1.20.5", Compiler:"gc", Platform:"linux/amd64"}```

</details>


#### Cloud provider

<details>
self
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124407 Kubelet tries to get ContainerStatus of non-existent containers when initializing a cluster

- Issue é“¾æ¥ï¼š[#124407](https://github.com/kubernetes/kubernetes/issues/124407)

### Issue å†…å®¹

#### What happened?

When initializing a cluster using `kubeadm init --pod-network-cidr 10.112.0.0/12 --service-cidr 10.16.0.0/12 --apiserver-advertise-address 172.X.X.X --v=5`, during the `wait-control-plane` phase kubelet is launched and expected to launch essential pods for the control plane. However, kubeadm times out waiting for kubelet to be healthy:

```
[kubelet-start] Starting the kubelet
I0419 13:28:21.800518   83681 waitcontrolplane.go:83] [wait-control-plane] Waiting for the API server to be healthy
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.

Unfortunately, an error has occurred:
        timed out waiting for the condition

This error is likely caused by:
        - The kubelet is not running
        - The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
        - 'systemctl status kubelet'
        - 'journalctl -xeu kubelet'

Additionally, a control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI.
Here is one example how you may list all running Kubernetes containers by using crictl:
        - 'crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock ps -a | grep kube | grep -v pause'
        Once you have found the failing container, you can inspect its logs with:
        - 'crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock logs CONTAINERID'
couldn't initialize a Kubernetes cluster
...
```

Taking a look at kubelet journal logs: (Provided logs are for after the node is registered)
```
Apr 19 13:28:28 avije-master kubelet[83794]: I0419 13:28:28.049050   83794 kubelet_node_status.go:76] "Successfully registered node" node="avijeh-master"
Apr 19 13:28:28 avije-master kubelet[83794]: I0419 13:28:28.267975   83794 apiserver.go:52] "Watching apiserver" 
Apr 19 13:28:28 avije-master kubelet[83794]: I0419 13:28:28.290394   83794 desired_state_of_world_populator.go:159] "Finished populating initial desired state of world"
Apr 19 13:28:28 avije-master kubelet[83794]: E0419 13:28:28.400654   83794 kubelet.go:1921] "Failed creating a mirror pod for" err="pods \"kube-apiserver-avijeh-master\" is forbidden: no PriorityClass with name system-node-critical was found" pod="kube-system/kube-apiserver-avijeh-master"
Apr 19 13:28:28 avije-master kubelet[83794]: E0419 13:28:28.527008   83794 kubelet.go:1921] "Failed creating a mirror pod for" err="pods \"kube-controller-manager-avijeh-master\" is forbidden: no PriorityClass with name system-node-critical was found" pod="kube-system/kube-controller-manager-avijeh-master"
Apr 19 13:28:28 avije-master kubelet[83794]: E0419 13:28:28.551525   83794 kubelet.go:1921] "Failed creating a mirror pod for" err="pods \"kube-scheduler-avijeh-master\" is forbidden: no PriorityClass with name system-node-critical was found" pod="kube-system/kube-scheduler-avijeh-master"
Apr 19 13:28:36 avije-master kubelet[83794]: I0419 13:28:36.666828   83794 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/etcd-avijeh-master" podStartSLOduration=2.6666824719999997 podStartE2EDuration="2.666682472s" podCreationTimestamp="2024-04-19 13:28:34 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-04-19 13:28:34.456833141 +0000 UTC m=+12.642373986" watchObservedRunningTime="2024-04-19 13:28:36.666682472 +0000 UTC m=+14.852223314"
Apr 19 13:28:38 avije-master kubelet[83794]: I0419 13:28:38.545904   83794 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-apiserver-avijeh-master" podStartSLOduration=2.545820659 podStartE2EDuration="2.545820659s" podCreationTimestamp="2024-04-19 13:28:36 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-04-19 13:28:36.694198023 +0000 UTC m=+14.879738885" watchObservedRunningTime="2024-04-19 13:28:38.545820659 +0000 UTC m=+16.731361485"
Apr 19 13:28:43 avije-master kubelet[83794]: I0419 13:28:43.359153   83794 pod_startup_latency_tracker.go:102] "Observed pod startup duration" pod="kube-system/kube-controller-manager-avijeh-master" podStartSLOduration=5.358999714 podStartE2EDuration="5.358999714s" podCreationTimestamp="2024-04-19 13:28:38 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2024-04-19 13:28:43.358794059 +0000 UTC m=+21.544334918" watchObservedRunningTime="2024-04-19 13:28:43.358999714 +0000 UTC m=+21.544540543"
Apr 19 13:29:23 avije-master kubelet[83794]: E0419 13:29:23.304870   83794 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = NotFound desc = an error occurred when try to find container \"95685c1b9abef2e5cb5d7fc82874b1f5879244d16f12d4773ddc63b7d9e57dc2\": not found" containerID="95685c1b9abef2e5cb5d7fc82874b1f5879244d16f12d4773ddc63b7d9e57dc2"
Apr 19 13:29:23 avije-master kubelet[83794]: I0419 13:29:23.304969   83794 kuberuntime_gc.go:360] "Error getting ContainerStatus for containerID" containerID="95685c1b9abef2e5cb5d7fc82874b1f5879244d16f12d4773ddc63b7d9e57dc2" err="rpc error: code = NotFound desc = an error occurred when try to find container \"95685c1b9abef2e5cb5d7fc82874b1f5879244d16f12d4773ddc63b7d9e57dc2\": not found"
Apr 19 13:29:23 avije-master kubelet[83794]: E0419 13:29:23.306097   83794 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = NotFound desc = an error occurred when try to find container \"eb5090a7b6984cea307e3b89205512fc4df427d459ba77d3f4ad3c9609f710c5\": not found" containerID="eb5090a7b6984cea307e3b89205512fc4df427d459ba77d3f4ad3c9609f710c5"
Apr 19 13:29:23 avije-master kubelet[83794]: I0419 13:29:23.306185   83794 kuberuntime_gc.go:360] "Error getting ContainerStatus for containerID" containerID="eb5090a7b6984cea307e3b89205512fc4df427d459ba77d3f4ad3c9609f710c5" err="rpc error: code = NotFound desc = an error occurred when try to find container \"eb5090a7b6984cea307e3b89205512fc4df427d459ba77d3f4ad3c9609f710c5\": not found"
Apr 19 13:30:23 avije-master kubelet[83794]: E0419 13:30:23.361448   83794 kubelet_node_status.go:456] "Node not becoming ready in time after startup"
Apr 19 13:30:23 avije-master kubelet[83794]: E0419 13:30:23.420288   83794 kubelet.go:2892] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
Apr 19 13:30:28 avije-master kubelet[83794]: E0419 13:30:28.422517   83794 kubelet.go:2892] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
```

At this point the required containers have been started according to containerd:
```console
$ crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock ps -a
CONTAINER           IMAGE               CREATED              STATE               NAME                      ATTEMPT             POD ID              POD
cd75ded425c7e       3861cfcd7c04c       About a minute ago   Running             etcd                      82                  5d5eff6d06dac       etcd-master
251638dc7ae54       e444022412717       About a minute ago   Running             kube-scheduler            0                   463a0e09c6d7d       kube-scheduler-master
c232a00ad060c       48ad18e13fb4f       About a minute ago   Running             kube-controller-manager   0                   1da3b8f00b55b       kube-controller-manager-master
de2982e4c6a16       7ae3494460614       About a minute ago   Running             kube-apiserver            1                   42fbbede44751       kube-apiserver-master
```

However, the conatiners whose ContainerStatus kubelet is trying to get are not among the containers that containerd has created!
containerd logs confirm this as well:

```console
Apr 19 13:28:24 avije-master containerd[80846]: time="2024-04-19T13:28:24.205106219Z" level=info msg="CreateContainer within sandbox \"5d5eff6d06dacfd8b07d5f441cf5e76697a896f8d9f111ff07e716ffe1179c5d\" for container &ContainerMetadata{Name:etcd,Attempt:82,}"
Apr 19 13:28:24 avije-master containerd[80846]: time="2024-04-19T13:28:24.242328418Z" level=info msg="CreateContainer within sandbox \"5d5eff6d06dacfd8b07d5f441cf5e76697a896f8d9f111ff07e716ffe1179c5d\" for &ContainerMetadata{Name:etcd,Attempt:82,} returns container id \"cd75ded425c7e18e40958266e8e0abb551a8a9864f2d4c57b3a4547c26255ef1\""
Apr 19 13:28:24 avije-master containerd[80846]: time="2024-04-19T13:28:24.243302417Z" level=info msg="StartContainer for \"cd75ded425c7e18e40958266e8e0abb551a8a9864f2d4c57b3a4547c26255ef1\""
Apr 19 13:28:24 avije-master containerd[80846]: time="2024-04-19T13:28:24.395299613Z" level=info msg="StartContainer for \"de2982e4c6a161b4328e1f8545b51a9e4ee8b194a88bc8854beb4d1f21ff1ff5\" returns successfully"
Apr 19 13:28:24 avije-master containerd[80846]: time="2024-04-19T13:28:24.404490886Z" level=info msg="StartContainer for \"c232a00ad060cc4ba8837a51b0a3e70bdbbcc7e8066caaba01e1943204666e47\" returns successfully"
Apr 19 13:28:24 avije-master containerd[80846]: time="2024-04-19T13:28:24.459529956Z" level=info msg="StartContainer for \"251638dc7ae54cf85a671493cfb9f28eeb724c86a44480e1da8032295e8547fc\" returns successfully"
Apr 19 13:28:24 avije-master containerd[80846]: time="2024-04-19T13:28:24.522611185Z" level=info msg="StartContainer for \"cd75ded425c7e18e40958266e8e0abb551a8a9864f2d4c57b3a4547c26255ef1\" returns successfully"
Apr 19 13:29:23 avije-master containerd[80846]: time="2024-04-19T13:29:23.304289064Z" level=error msg="ContainerStatus for \"95685c1b9abef2e5cb5d7fc82874b1f5879244d16f12d4773ddc63b7d9e57dc2\" failed" error="rpc error: code = NotFound desc = an error occurred when try to find container \"95685c1b9abef2e5cb5d7fc82874b1f5879244d16f12d4773ddc63b7d9e57dc2\": not found"
Apr 19 13:29:23 avije-master containerd[80846]: time="2024-04-19T13:29:23.305689987Z" level=error msg="ContainerStatus for \"eb5090a7b6984cea307e3b89205512fc4df427d459ba77d3f4ad3c9609f710c5\" failed" error="rpc error: code = NotFound desc = an error occurred when try to find container \"eb5090a7b6984cea307e3b89205512fc4df427d459ba77d3f4ad3c9609f710c5\": not found"
```

This behavior results in failure when initializing the cluster.

#### What did you expect to happen?

For kubelet to verify the readiness of created containers properly and for kubeadm to verify kubelet's health and carry on with the rest of the initialization process.

#### How can we reproduce it (as minimally and precisely as possible)?

Run `kubeadm init` with standard appropriate flags and wait for it to reach the `wait-control-plane` phase.
Follow kubelet & containerd logs:
```console
$ journalctl -xeu kubelet -f
...
$ journalctl -xeu containerd -f
...
```

Also keep track of containerd containers.
```console
$ crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock ps -a
...
```

#### Anything else we need to know?

In case of concerns:
- swap is off.
- `SystemdCgroup` is set to `true` in `/etc/containerd/config.toml`.
- SELinux is turned off using `setenforce 0`.
- runc version: 1.1.12

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```

</details>


#### Cloud provider

<details>
Self-hosted
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.2 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.2 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux master 5.15.0-102-generic #112-Ubuntu SMP Tue Mar 5 16:50:32 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd

```console
$ containerd -v
containerd github.com/containerd/containerd v1.7.15 926c9586fe4a6236699318391cd44976a98e31f1
```
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
No CSI/CNI is installed.

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124406 forbidden message may include RBAC information

- Issue é“¾æ¥ï¼š[#124406](https://github.com/kubernetes/kubernetes/issues/124406)

### Issue å†…å®¹

#### What happened?

Kubernetes API server may include extra RBAC information on forbidden error messages. An authenticated user could gain unexpected knowledge of possible Kubernetes RBAC configuration problems.

#### What did you expect to happen?

Error message does not include RBAC information.

#### How can we reproduce it (as minimally and precisely as possible)?

**Test 1**

```
kubectl auth can-i create pods -n kube-system --as=system:serviceaccount:default:default
kubectl auth can-i create pods -n kube-system --as=system:anonymous
curl -k https://KUBERNETES_API_SERVER_HOST:KUBERNETES_API_SERVER_PORT/foo
```

```
kubectl apply -f - <<EOF
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: test
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: test
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: Group
  name: system:unauthenticated
EOF
```

```
kubectl auth can-i create pods -n kube-system --as=system:serviceaccount:default:default
kubectl auth can-i create pods -n kube-system --as=system:anonymous
curl -k https://KUBERNETES_API_SERVER_HOST:KUBERNETES_API_SERVER_PORT/foo
```

**Example Results for Test 1:**

```
vagrant@verify-cluster:~$ kubectl auth can-i create pods -n kube-system --as=system:serviceaccount:default:default
no
vagrant@verify-cluster:~$ kubectl auth can-i create pods -n kube-system --as=system:anonymous
Error from server (Forbidden): selfsubjectaccessreviews.authorization.k8s.io is forbidden: User "system:anonymous" cannot create resource "selfsubjectaccessreviews" in API group "authorization.k8s.io" at the cluster scope
vagrant@verify-cluster:~$ curl -k https://c105.containers.test.cloud.ibm.com:31573/foo
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {},
  "status": "Failure",
  "message": "forbidden: User \"system:anonymous\" cannot get path \"/foo\"",
  "reason": "Forbidden",
  "details": {},
  "code": 403
}vagrant@verify-cluster:~$ kubectl apply -f - <<EOF
> kind: ClusterRoleBinding
> apiVersion: rbac.authorization.k8s.io/v1
> metadata:
>   name: test
> roleRef:
>   apiGroup: rbac.authorization.k8s.io
>   kind: ClusterRole
>   name: test
> subjects:
> - apiGroup: rbac.authorization.k8s.io
>   kind: Group
>   name: system:unauthenticated
> EOF
clusterrolebinding.rbac.authorization.k8s.io/test created
vagrant@verify-cluster:~$ 
vagrant@verify-cluster:~$ kubectl auth can-i create pods -n kube-system --as=system:serviceaccount:default:default
no
vagrant@verify-cluster:~$ kubectl auth can-i create pods -n kube-system --as=system:anonymous
Error from server (Forbidden): selfsubjectaccessreviews.authorization.k8s.io is forbidden: User "system:anonymous" cannot create resource "selfsubjectaccessreviews" in API group "authorization.k8s.io" at the cluster scope: RBAC: clusterrole.rbac.authorization.k8s.io "test" not found
vagrant@verify-cluster:~$ curl -k https://c105.containers.test.cloud.ibm.com:31573/foo
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {},
  "status": "Failure",
  "message": "forbidden: User \"system:anonymous\" cannot get path \"/foo\": RBAC: clusterrole.rbac.authorization.k8s.io \"test\" not found",
  "reason": "Forbidden",
  "details": {},
  "code": 403
}vagrant@verify-cluster:~$ 
```

**Test 2 with Results:** Kubernetes API server updated with `--anonymous-auth=false`.

```
vagrant@verify-cluster:~$ kubectl auth can-i create pods -n kube-system --as=system:serviceaccount:default:default
no
vagrant@verify-cluster:~$ kubectl auth can-i create pods -n kube-system --as=system:anonymous
Error from server (Forbidden): selfsubjectaccessreviews.authorization.k8s.io is forbidden: User "system:anonymous" cannot create resource "selfsubjectaccessreviews" in API group "authorization.k8s.io" at the cluster scope
vagrant@verify-cluster:~$ curl -k https://c105.containers.test.cloud.ibm.com:31573/foo
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {},
  "status": "Failure",
  "message": "Unauthorized",
  "reason": "Unauthorized",
  "code": 401
}vagrant@verify-cluster:~$ 
vagrant@verify-cluster:~$ 
vagrant@verify-cluster:~$ 
vagrant@verify-cluster:~$ kubectl apply -f - <<EOF
> kind: ClusterRoleBinding
> apiVersion: rbac.authorization.k8s.io/v1
> metadata:
>   name: test
> roleRef:
>   apiGroup: rbac.authorization.k8s.io
>   kind: ClusterRole
>   name: test
> subjects:
> - apiGroup: rbac.authorization.k8s.io
>   kind: Group
>   name: system:serviceaccounts
> - apiGroup: rbac.authorization.k8s.io
>   kind: Group
>   name: system:unauthenticated
> EOF
clusterrolebinding.rbac.authorization.k8s.io/test created
vagrant@verify-cluster:~$ 
vagrant@verify-cluster:~$ 
vagrant@verify-cluster:~$ kubectl auth can-i create pods -n kube-system --as=system:serviceaccount:default:default
no - RBAC: clusterrole.rbac.authorization.k8s.io "test" not found
vagrant@verify-cluster:~$ kubectl auth can-i create pods -n kube-system --as=system:anonymous
Error from server (Forbidden): selfsubjectaccessreviews.authorization.k8s.io is forbidden: User "system:anonymous" cannot create resource "selfsubjectaccessreviews" in API group "authorization.k8s.io" at the cluster scope: RBAC: clusterrole.rbac.authorization.k8s.io "test" not found
vagrant@verify-cluster:~$ curl -k https://c105.containers.test.cloud.ibm.com:31573/foo
{
  "kind": "Status",
  "apiVersion": "v1",
  "metadata": {},
  "status": "Failure",
  "message": "Unauthorized",
  "reason": "Unauthorized",
  "code": 401
}vagrant@verify-cluster:~$
```

#### Anything else we need to know?

This issue was originally filed at https://hackerone.com but was closed. I was told to open an issue here.

#### Kubernetes version

<details>
Kubernetes versions 1.26.15 and 1.30.0-rc.1 were tested, but other versions likely impacted. Test was conducted against Kubernetes clusters deployed using IBM Cloud Kubernetes Service.
</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>
N/A
</details>


#### Install tools

<details>
IBM Cloud Kubernetes Service
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
N/A
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
N/A
</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124397 After restart Kubelet, node will become notReady in first kubelet update period.

- Issue é“¾æ¥ï¼š[#124397](https://github.com/kubernetes/kubernetes/issues/124397)

### Issue å†…å®¹

#### What happened?

After restart Kubelet, node will become notReady in first kubelet update period.

node condition is :
```
  - lastHeartbeatTime: "2024-04-19T06:40:55Z"
    lastTransitionTime: "2024-04-19T06:40:55Z"
    message: container runtime status check may not have completed yet
    reason: KubeletNotReady
    status: "False"
    type: Ready
```

in kubelet's logs:
```
I0419 14:40:55.510166 1902465 setters.go:577] "Node became not ready" node="mjq-k8stestarsenalwork-1" condition={Type:Ready Status:False LastHeartbeatTime:2024-04-19 14:40:55.510127183 +0800 CST m=+5.226928586 LastTransitionTime:2024-04-19 14:40:55.510127183 +0800 CST m=+5.226928586 Reason:KubeletNotReady Message:container runtime status check may not have completed yet}
```

when go in to next period (default 10s), node status resume to Ready.

#### What did you expect to happen?

Restart Kubelet should not make node status change to notReady.

#### How can we reproduce it (as minimally and precisely as possible)?

Restart Kubelet, and watch node status for second
```
watch -n 1 'kubectl get no <node-name>'
```


#### Anything else we need to know?

The root case maybe here:
When kubelet start, it will start two go routine, one detect runtime status, one check node status and update to APIServer;
detect runtime status:

in `updateRuntimeUp` it will update `lastBaseRuntimeSync`:
https://github.com/kubernetes/kubernetes/blob/bf07ef3950d80256cb366b035712b6d60b8a3f4c/pkg/kubelet/kubelet.go#L2909

in `kl.runtimeState.runtimeErrors` it will check runtimeStatus include `lastBaseRuntimeSync`:

https://github.com/kubernetes/kubernetes/blob/bf07ef3950d80256cb366b035712b6d60b8a3f4c/pkg/kubelet/runtime.go#L108

`kl.runtimeState.runtimeErrors` is one of the notReady setter func:
https://github.com/kubernetes/kubernetes/blob/bf07ef3950d80256cb366b035712b6d60b8a3f4c/pkg/kubelet/kubelet_node_status.go#L748

So if nodestatus execute before `updateRuntimeUp`, kubelet will update a node status with notReady condition:
https://github.com/kubernetes/kubernetes/blob/bf07ef3950d80256cb366b035712b6d60b8a3f4c/pkg/kubelet/kubelet.go#L1635-L1641
In kubelet starting,  both `syncNodeStatus` go routine and `fastStatusUpdateOnce` will be called before `updateRuntimeUp`,  so the first node status is notReady.

#### Kubernetes version

In our cluster is v1.22.17, but I belive it can reproduce in latest version.

<details>

```console
Client Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.17", GitCommit:"a7736eaf34d823d7652415337ac0ad06db9167fc", GitTreeState:"clean", BuildDate:"2022-12-08T11:47:36Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"22", GitVersion:"v1.22.17", GitCommit:"a7736eaf34d823d7652415337ac0ad06db9167fc", GitTreeState:"clean", BuildDate:"2022-12-08T11:42:04Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console

# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124392 scheduler: ```ImageLocality``` gives nodes in different situation the same score

- Issue é“¾æ¥ï¼š[#124392](https://github.com/kubernetes/kubernetes/issues/124392)

### Issue å†…å®¹

#### What happened?

I managed to reproduce [a bug that went stale](https://github.com/kubernetes/kubernetes/issues/112573) on the latest main branch. And also noticed a new situation that may trigger the bug.

The ```ImageLocality``` plugin may give a same score to nodes in different situations.

The related code:
```go
imageScores := sumImageScores(nodeInfo, pod, totalNumNodes, logger, nodeName)
score := calculatePriority(imageScores, len(pod.Spec.InitContainers)+len(pod.Spec.Containers), logger)

func calculatePriority(sumScores int64, numContainers int, logger klog.Logger) int64 {
	maxThreshold := maxContainerThreshold * int64(numContainers)
	if sumScores < minThreshold {
		sumScores = minThreshold
	} else if sumScores > maxThreshold {
		sumScores = maxThreshold
	}
	return framework.MaxNodeScore * (sumScores - minThreshold) / (maxThreshold - minThreshold)
}
```

**This will happen when the a node has a fully pulled image that is relatively small, another doesn't** (But still need time to pull, in this case image size is about 70MB). 

The node with fully pulled image has an original ```sumScores``` too close to ```minThreshold```, and the other node without that image has an original ```sumScores``` = 0. So after clip the difference of two nodes' ```sumScores``` is relatively small compared to ```maxThreshold - minThreshold```. 

Then the plugin calculate the score, and this will happen: For the first node's score ```= 100 * 0.0001 = 0```, for the second node' score ```= 100 * 0 = 0```. Resulting the same score even if the first node has fully pulled this image (about 70MB) and the second node hasn't.

---

**And I think this will also happen when a node has many relatively large image, another doesn't (the second also has some image but not as large as the first one).** Then the first node's ```sumScores``` is far more beyond ```maxThreshold```, and the second node's ```sumScores``` is just a little bit smaller than ```maxThreshold```. After clipping, the first node's ```sumScores```  becomes ```maxThreshold```.

Then the plugin calculate the score, and this will happen: For the first node's score ```= 100 * 1 = 100```, for the second node' score ```= 100 * 0.999 = 100```. Resulting the same score even if the first node has far more image than the second node.

#### What did you expect to happen?

ImageLocality gives two nodes a different score.

#### How can we reproduce it (as minimally and precisely as possible)?

I first create two nodes (worker1 with fully pulled image, and worker2 just start pulling), then apply a pod using the following specification. 
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod3
spec:
  containers:
    - name: test-container
      image: gke.gcr.io/calico/cni:v3.23.1-gke.0
```

and here is the log (added some log to see what happened in image_locality):
```bash
[image_locality.go:68] "ImageScores" logger="Score.ImageLocality" pod="default/nginx3" node="kind-worker" imageScores=25531886
[image_locality.go:93] "calculatePriority" logger="Score.ImageLocality" pod="default/nginx3" node="kind-worker" minThreshold=24117248 maxThreshold=1048576000 framework.MaxNodeScore=100
[image_locality.go:68] "ImageScores" logger="Score.ImageLocality" pod="default/nginx3" node="kind-worker2" imageScores=0
[image_locality.go:93] "calculatePriority" logger="Score.ImageLocality" pod="default/nginx3" node="kind-worker2" minThreshold=24117248 maxThreshold=1048576000 framework.MaxNodeScore=100
[image_locality.go:112] ImageName="gke.gcr.io/calico/cni:v3.23.1-gke.0" Size="76595659" node="kind-worker"
...
[schedule_one.go:784] "Plugin scored node for pod" pod="default/nginx3" plugin="ImageLocality" node="kind-worker" score=0
...
[schedule_one.go:784] "Plugin scored node for pod" pod="default/nginx3" plugin="ImageLocality" node="kind-worker2" score=0
[schedule_one.go:851] "Calculated node's final score for pod" pod="default/nginx3" node="kind-worker" score=492
[schedule_one.go:851] "Calculated node's final score for pod" pod="default/nginx3" node="kind-worker2" score=492
```


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version: v1.29.1
Server Version: v1.30.0-

Actually this is reproduced on the latest main branch.
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124391 errors running update-codegen.sh

- Issue é“¾æ¥ï¼š[#124391](https://github.com/kubernetes/kubernetes/issues/124391)

### Issue å†…å®¹

#### What happened?
Initially, running `hack/update-codegen.sh` returned:
```
hack/verify-codegen.sh
+++ [0418 21:16:48] Generating protobufs for 69 targets
go: -mod may only be set to readonly or vendor when in workspace mode, but it is set to "mod"
	Remove the -mod flag to use the default readonly value,
	or set GOWORK=off to disable workspace mode.
```
so I set `export GOWORK=off` in [`kube::golang::setup_env()`](https://github.com/kubernetes/kubernetes/blob/master/hack/lib/golang.sh#L590), which effectively disabled go workspace.

This fixed the error above, but led to a list of errors from `deepcopy-gen` even though my `kubernetes` repo is already under `$GOPATH/src/k8s.io`:
```
hack/verify-codegen.sh
+++ [0418 17:39:21] Generating protobufs for 69 targets
+++ [0418 17:40:04] Generating deepcopy code for 252 targets
F0418 17:40:09.204730   86904 main.go:107] Error: failed making a parser: error(s) in "./staging/src/k8s.io/api/admission/v1":
-: main module (k8s.io/kubernetes) does not contain package k8s.io/kubernetes/staging/src/k8s.io/api/admission/v1
error(s) in "./staging/src/k8s.io/api/admission/v1beta1":
-: main module (k8s.io/kubernetes) does not contain package k8s.io/kubernetes/staging/src/k8s.io/api/admission/v1beta1
error(s) in "./staging/src/k8s.io/api/admissionregistration/v1":
-: main module (k8s.io/kubernetes) does not contain package k8s.io/kubernetes/staging/src/k8s.io/api/admissionregistration/v1
error(s) in "./staging/src/k8s.io/api/admissionregistration/v1alpha1":
-: main module (k8s.io/kubernetes) does not contain package k8s.io/kubernetes/staging/src/k8s.io/api/admissionregistration/v1alpha1
...
```

#### What did you expect to happen?

`hack/update-codegen.sh` should work.

#### How can we reproduce it (as minimally and precisely as possible)?

* Change [this line](https://github.com/kubernetes/kubernetes/blob/master/hack/lib/golang.sh#L590) to `export GOWORK=off` and temporarily commit it (or else script will complain dirty working dir).
* Run `hack/update-codegen.sh`.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.24.13
```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124378 Garbage collector can create invalid virtual nodes

- Issue é“¾æ¥ï¼š[#124378](https://github.com/kubernetes/kubernetes/issues/124378)

### Issue å†…å®¹

#### What happened?

Disclaimer: I came across this when studying the GC code, but haven't observed the issue directly (or tried to).

In `GraphBuilder#addDependentToOwners`, a virtual node is created if the owner hasn't been observed in the graph yet. However, it blindly uses the dependent's namespace for this owner node. This will be incorrect if the owner is a non-namespaced resource.

See https://github.com/kubernetes/kubernetes/blob/183aca3cad427f28b746653075a6959cf4edf316/pkg/controller/garbagecollector/graph_builder.go#L411.

I'm reporting this as a kind of low-effort thing as I do not have capacity of interest in resolving this myself. Feel free to close if it doesn't meet community standards.

#### What did you expect to happen?

-

#### How can we reproduce it (as minimally and precisely as possible)?

-

#### Anything else we need to know?

-

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

æ ¹æ®æ‚¨æä¾›çš„ Issue å†…å®¹ï¼Œé—®é¢˜æè¿°æ˜¯åœ¨åƒåœ¾æ”¶é›†å™¨çš„ `GraphBuilder#addDependentToOwners` æ–¹æ³•ä¸­ï¼Œå½“æ‰€æœ‰è€…èŠ‚ç‚¹å°šæœªåœ¨å›¾ä¸­è§‚å¯Ÿåˆ°æ—¶ï¼Œä¼šåˆ›å»ºä¸€ä¸ªè™šæ‹ŸèŠ‚ç‚¹ã€‚ç„¶è€Œï¼Œå®ƒä½¿ç”¨äº†ä»å±èŠ‚ç‚¹çš„å‘½åç©ºé—´æ¥è®¾ç½®æ‰€æœ‰è€…èŠ‚ç‚¹çš„å‘½åç©ºé—´ï¼Œå¦‚æœæ‰€æœ‰è€…æ˜¯ä¸€ä¸ªéå‘½åç©ºé—´ï¼ˆcluster-scopedï¼‰çš„èµ„æºï¼Œè¿™å°†æ˜¯ä¸æ­£ç¡®çš„ã€‚

ç»è¿‡åˆ†æï¼Œæ­¤é—®é¢˜å¯èƒ½å¯¼è‡´åƒåœ¾æ”¶é›†å™¨çš„å†…éƒ¨å›¾ç»“æ„ä¸­å‡ºç°ä¸æ­£ç¡®çš„è™šæ‹ŸèŠ‚ç‚¹ï¼Œå¯èƒ½å½±å“åƒåœ¾æ”¶é›†å™¨å¯¹èµ„æºä¾èµ–å…³ç³»çš„è¿½è¸ªå’Œç®¡ç†ï¼Œå¯¼è‡´ä¸€äº›èµ„æºæœªèƒ½è¢«æ­£ç¡®åœ°åƒåœ¾å›æ”¶æˆ–äº§ç”Ÿå…¶ä»–éé¢„æœŸçš„è¡Œä¸ºã€‚

ç„¶è€Œï¼Œä»å®‰å…¨æ€§çš„è§’åº¦æ¥çœ‹ï¼Œæ­¤é—®é¢˜ä¸æ¶‰åŠæ½œåœ¨çš„å®‰å…¨é£é™©ï¼ŒåŸå› å¦‚ä¸‹ï¼š

1. **æ”»å‡»è€…æ— æ³•åˆ©ç”¨è¯¥é—®é¢˜è¿›è¡Œæ”»å‡»**ï¼šKubernetes API æœåŠ¡å™¨åœ¨å¤„ç†èµ„æºçš„ `ownerReferences` æ—¶ï¼Œä¼šè¿›è¡Œä¸¥æ ¼çš„æƒé™æ ¡éªŒã€‚åªæœ‰å…·å¤‡åˆ é™¤ç›®æ ‡æ‰€æœ‰è€…èµ„æºæƒé™çš„ç”¨æˆ·ï¼Œæ‰èƒ½åˆ›å»ºæŒ‡å‘è¯¥èµ„æºçš„ `ownerReference`ã€‚è¿™æ„å‘³ç€æ”»å‡»è€…æ— æ³•é€šè¿‡ä¼ªé€ æˆ–æ“çºµ `ownerReference` æ¥å½±å“åƒåœ¾æ”¶é›†å™¨çš„è¡Œä¸ºï¼Œä»¥åˆ é™¤ä»–ä»¬æ— æƒåˆ é™¤çš„èµ„æºã€‚å³ä½¿åƒåœ¾æ”¶é›†å™¨åœ¨å†…éƒ¨ç»´æŠ¤çš„å›¾ä¸­å­˜åœ¨ä¸æ­£ç¡®çš„è™šæ‹ŸèŠ‚ç‚¹ï¼Œæ”»å‡»è€…ä¹Ÿæ— æ³•åˆ©ç”¨è¿™ä¸€ç‚¹æ¥çªç ´æƒé™è¾¹ç•Œæˆ–æå‡æƒé™ã€‚

2. **è¯¥é—®é¢˜ä¸å¤ªå¯èƒ½æˆä¸ºä¸€ä¸ªé«˜å±æ¼æ´**ï¼šæ ¹æ® CVSS 3.1 è¯„åˆ†æ ‡å‡†ï¼Œæ­¤é—®é¢˜å¯¹æœºå¯†æ€§ã€å®Œæ•´æ€§å’Œå¯ç”¨æ€§æ²¡æœ‰ç›´æ¥çš„é«˜å½±å“ã€‚å®ƒä¸ä¼šå¯¼è‡´æœªç»æˆæƒçš„è®¿é—®ã€æ•°æ®æ³„éœ²æˆ–æœåŠ¡ä¸å¯ç”¨ç­‰é«˜é£é™©åæœã€‚å› æ­¤ï¼Œå³ä½¿è¢«è®¤å®šä¸ºæ¼æ´ï¼Œå…¶ä¸¥é‡æ€§è¯„çº§ä¹Ÿä¸ä¼šè¾¾åˆ° High æˆ–ä»¥ä¸Šçš„çº§åˆ«ï¼Œæ— æ³•æ»¡è¶³åˆ†é… CVE ç¼–å·çš„æ¡ä»¶ã€‚

å› æ­¤ï¼Œç»¼åˆä»¥ä¸Šåˆ†æï¼Œè¯¥ Issue ä¸æ¶‰åŠæ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

---

## Issue #124376 Kubelet CredentialProvider Fails to Connect to Local Docker Registry

- Issue é“¾æ¥ï¼š[#124376](https://github.com/kubernetes/kubernetes/issues/124376)

### Issue å†…å®¹

#### What happened?

I have configured the Kubelet CredentialProvider according to the documentation to connect to my local Docker registry. However, I am encountering issues when attempting to pull images from the registry.

I followed the documentation to set up the Kubelet CredentialProvider to connect to my local Docker registry. Despite following the steps outlined, I am facing difficulties when trying to pull images from the registry. Specifically, I receive a "403 Forbidden" status code when the Kubelet attempts to access the image repository on the local registry.

kubectl get pods -o wide 
`NAME                                  READY   STATUS             RESTARTS   AGE   IP               NODE                          NOMINATED NODE   READINESS GATES
example-deployment-7bc6984597-4jtbd   0/1     ImagePullBackOff   0          18m   10.250.172.164   node-worker   <none>           <none>`

when I go to the node and check the logs of the kubelet I saw this error
`Apr 18 16:05:46 node-worker kubelet[20981]: E0418 16:05:46.179931   20981 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"example-container\" with ErrImagePull: \"failed to pull and unpack image \\\"URL/angie-docker-dev/jarvis-monitor:latest\\\": failed to resolve reference \\\"URL/angie-docker-dev/jarvis-monitor:latest\\\": unexpected status from HEAD request to https://URL/v2/angie-docker-dev/jarvis-monitor/manifests/latest: 403 Forbidden\"" pod="jarvis/example-deployment-7bc6984597-4nc6k" podUID="5bc7f273-d1c2-4926-9282-97b3b56afff0"
Apr 18 16:05:46 node-worker kubelet[20981]: E0418 16:05:46.450655   20981 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"example-container\" with ImagePullBackOff: \"Back-off pulling image \\\"URL/angie-docker-dev/jarvis-monitor:latest\\\"\"" pod="jarvis/example-deployment-7bc6984597-4nc6k" podUID="5bc7f273-d1c2-4926-9282-97b3b56afff0"
Apr 18 16:05:47 node-worker kubelet[20981]: E0418 16:05:47.453020   20981 pod_workers.go:1300] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"example-container\" with ImagePullBackOff: \"Back-off pulling image \\\"URL/angie-docker-dev/jarvis-monitor:latest\\\"\"" pod="jarvis/example-deployment-7bc6984597-4nc6k" podUID="5bc7f273-d1c2-4926-9282-97b3b56afff0"`

 kubectl -n jarvis describe po example-deployment-7bc6984597-4nc6k
```
Events:
  Type     Reason     Age                    From               Message
  ----     ------     ----                   ----               -------
  Normal   Scheduled  6m                     default-scheduler  Successfully assigned jarvis/example-deployment-7bc6984597-4nc6k to algoobjd-test-1-k8sworker-1
  Warning  Failed     4m40s (x6 over 5m59s)  kubelet            Error: ImagePullBackOff
  Normal   Pulling    4m25s (x4 over 5m59s)  kubelet            Pulling image "URL/angie-docker-dev/jarvis-monitor:latest"
  Warning  Failed     4m25s (x4 over 5m59s)  kubelet            Failed to pull image "URL/angie-docker-dev/jarvis-monitor:latest": failed to pull and unpack image "URL/angie-docker-dev/jarvis-monitor:latest": failed to resolve reference "URL/angie-docker-dev/jarvis-monitor:latest": unexpected status from HEAD request to https://URL/v2/angie-docker-dev/jarvis-monitor/manifests/latest: 403 Forbidden
  Warning  Failed     4m25s (x4 over 5m59s)  kubelet            Error: ErrImagePull
  Normal   BackOff    48s (x21 over 5m59s)   kubelet            Back-off pulling image "URL/angie-docker-dev/jarvis-monitor:latest"
```

I create a few files in every nodes
URL.json

exist in every node in path:
/etc/kubernetes/registries/URL.json

the context of file is:
{"username":"YYY","password":"XXX"}

ecr-credential-provider : [https://raw.githubusercontent.com/JonTheNiceGuy/generic-credential-provider/main/generic-credential-provider](url)

file image-credential-provider-config.json:
`{
    "apiVersion": "kubelet.config.k8s.io/v1",
    "kind": "CredentialProviderConfig",
    "providers": [
      {
        "name": "generic-credential-provider",
        "matchImages": [
          "URL"
        ],
        "apiVersion": "credentialprovider.kubelet.k8s.io/v1"
      }
    ]
  }`

and kubelet extra args
`KUBELET_EXTRA_ARGS="--image-credential-provider-bin-dir /usr/local/bin/image-credential-provider --image-credential-provider-config /etc/kubernetes/image-credential-provider-config.json"`

I put it in file --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf.

I tested this code with the command
`echo '{
  "apiVersion": "credentialprovider.kubelet.k8s.io/v1",
  "kind": "CredentialProviderRequest",
  "image": "URL/docker-dev/jarvis-monitor:latest"
}' | /usr/local/bin/image-credential-provider/generic-credential-provider`

the output (like [https://kubernetes.io/docs/reference/config-api/kubelet-credentialprovider.v1/#credentialprovider-kubelet-k8s-io-v1-CredentialProviderResponse](url)
`{
    "kind": "CredentialProviderResponse",
    "apiVersion": "credentialprovider.kubelet.k8s.io/v1",
    "cacheKeyType": "Registry",
    "cacheDuration": "0",
    "auth": {
        "URL": {
            "username": "XXX",
            "password": "YYY"
        }
    }
}`

#### What did you expect to happen?

I expected that every node in the Kubernetes cluster would successfully pull Docker images from the local Docker registry without encountering any authentication or access issues. Specifically, I anticipated that the Kubelet CredentialProvider configuration would enable seamless access to the registry, allowing containers to start properly on each node.

#### How can we reproduce it (as minimally and precisely as possible)?

To reproduce the issue on an on-premises Kubernetes cluster, follow these steps:

1. Set up a Kubernetes cluster using your on-premises infrastructure.
2. Configure the Kubelet CredentialProvider to connect to your local Docker registry as per the documentation.
3. Deploy a pod that uses an image from the local Docker registry.
4. Monitor the Kubelet logs or events for any errors during the image pull process.
5. Verify if the pod starts successfully or if it encounters any issues related to image pulling.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.8
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.8
```

</details>


#### Cloud provider

<details>
on-prem
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124370 error while initializing kubeadm on ubuntu 22.04

- Issue é“¾æ¥ï¼š[#124370](https://github.com/kubernetes/kubernetes/issues/124370)

### Issue å†…å®¹

#### What happened?

sudo kubeadm init --control-plane-endpoint=master-node --upload-certs
I0418 16:55:58.530076   12569 version.go:256] remote version is much newer: v1.30.0; falling back to: stable-1.29
[init] Using Kubernetes version: v1.29.4
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
W0418 16:55:59.851437   12569 checks.go:835] detected that the sandbox image "registry.k8s.io/pause:3.8" of the container runtime is inconsistent with that used by kubeadm. It is recommended that using "registry.k8s.io/pause:3.9" as the CRI sandbox image.
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local master-node] and IPs [10.96.0.1 192.168.1.36]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [localhost master-node] and IPs [192.168.1.36 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [localhost master-node] and IPs [192.168.1.36 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "super-admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[kubelet-check] Initial timeout of 40s passed.

Unfortunately, an error has occurred:
	timed out waiting for the condition

This error is likely caused by:
	- The kubelet is not running
	- The kubelet is unhealthy due to a misconfiguration of the node in some way (required cgroups disabled)

If you are on a systemd-powered system, you can try to troubleshoot the error with the following commands:
	- 'systemctl status kubelet'
	- 'journalctl -xeu kubelet'

Additionally, a control plane component may have crashed or exited when started by the container runtime.
To troubleshoot, list all containers using your preferred container runtimes CLI.
Here is one example how you may list all running Kubernetes containers by using crictl:
	- 'crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock ps -a | grep kube | grep -v pause'
	Once you have found the failing container, you can inspect its logs with:
	- 'crictl --runtime-endpoint unix:///var/run/containerd/containerd.sock logs CONTAINERID'
error execution phase wait-control-plane: couldn't initialize a Kubernetes cluster

kubelet and docker status are active(running)
and i don't habe kubelet.log file inside /var/log was that the problem?
i am facing this error from past two weeks ,any solutions?

#### What did you expect to happen?

kubeadm initiated successfully

#### How can we reproduce it (as minimally and precisely as possible)?

no idea

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version 
# paste output here
```Client Version: v1.29.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
The connection to the server localhost:8080 was refused - did you specify the right host or port?


</details>


#### Cloud provider

<details>


sudo kubectl cluster-info
E0418 17:16:31.257788   13190 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E0418 17:16:31.258103   13190 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E0418 17:16:31.259393   13190 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E0418 17:16:31.259588   13190 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused
E0418 17:16:31.261055   13190 memcache.go:265] couldn't get current server API group list: Get "http://localhost:8080/api?timeout=32s": dial tcp 127.0.0.1:8080: connect: connection refused

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
The connection to the server localhost:8080 was refused - did you specify the right host or port?
</details>

#### OS version

<details>

```console
# On Linux: yes
$ cat /etc/os-release
# paste output here[sudo] password for rivertech: 
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy



$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>
kubeadm kublet kubectl

#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124368 exec /usr/local/bin/docker-entrypoint.sh argument list too long

- Issue é“¾æ¥ï¼š[#124368](https://github.com/kubernetes/kubernetes/issues/124368)

### Issue å†…å®¹

#### What happened?

My containers are in the state CrashLoopBackOff while deploying. when I check the logs I found that _**exec /usr/local/bin/docker-entrypoint.sh argument list too long**_

When I try to deploy new containers , same issue.

#### What did you expect to happen?

Every containers are running successfully for few months then suddenly all containers are crashed.

#### How can we reproduce it (as minimally and precisely as possible)?

I do kubectl apply -f deployment.yaml

#### Anything else we need to know?

_No response_

#### Kubernetes version

Client Version: v1.28.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.0

#### Cloud provider

Running on-premises

#### OS version


# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.3 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy


uname -a
Linux  5.15.0-102-generic

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

contained 
version : containerd://1.6.31

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124359 kubeadm uses and configures kube-log-runner

- Issue é“¾æ¥ï¼š[#124359](https://github.com/kubernetes/kubernetes/issues/124359)

### Issue å†…å®¹

#### What happened?

klog command line flags [are deprecated](https://github.com/kubernetes/enhancements/tree/master/keps/sig-instrumentation/2845-deprecate-klog-specific-flags-in-k8s-components) starting with Kubernetes v1.23 and removed in Kubernetes v1.26ï¼ŒI hope to use kube-log-runner to configure log output to a specified fileï¼ŒI deploy using kubeadm

#### What did you expect to happen?

I deploy using kubeadmï¼ŒI hope to use kube-log-runner to configure log output to a specified fileã€‚How should I edit the kube-apiserver.yaml file

/apps/k8s/bin/kube-log-runner -log-file=/apps/k8s/log/kube-apiserver.log --also-stdout=false /apps/k8s/bin/kube-apiserver $KUBE_APISERVER_OPTS



#### How can we reproduce it (as minimally and precisely as possible)?

Deploy kubernetes using kubeadm

#### Anything else we need to know?

_No response_

#### Kubernetes version

```
kubeadm version: &version.Info{Major:"1", Minor:"29", GitVersion:"v1.29.2", GitCommit:"4b8e819355d791d96b7e9d9efe4cbafae2311c88", GitTreeState:"clean", BuildDate:"2024-02-14T10:39:04Z", GoVersion:"go1.21.7", Compiler:"gc", Platform:"linux/amd64"}
```

#### Cloud provider

<details>
nothing
</details>


#### OS version

```
NAME="Rocky Linux"
VERSION="8.7 (Green Obsidian)"
ID="rocky"
ID_LIKE="rhel centos fedora"
VERSION_ID="8.7"
PLATFORM_ID="platform:el8"
PRETTY_NAME="Rocky Linux 8.7 (Green Obsidian)"
ANSI_COLOR="0;32"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:rocky:rocky:8:GA"
HOME_URL="https://rockylinux.org/"
BUG_REPORT_URL="https://bugs.rockylinux.org/"
ROCKY_SUPPORT_PRODUCT="Rocky-Linux-8"
ROCKY_SUPPORT_PRODUCT_VERSION="8.7"
REDHAT_SUPPORT_PRODUCT="Rocky Linux"
REDHAT_SUPPORT_PRODUCT_VERSION="8.7"
```



```
[root@k8s-master-01 manifests]# uname -rn
k8s-master-01 6.7.6-1.el8.elrepo.x86_64
```

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124347 [bug] StorageError: invalid object

- Issue é“¾æ¥ï¼š[#124347](https://github.com/kubernetes/kubernetes/issues/124347)

### Issue å†…å®¹

Copied from https://github.com/kubernetes-sigs/controller-runtime/issues/1881 where this was originally reported, as it is unlikely this has anything to do with controller-runtime. Unfortunately, the prow transfer plugin doesn't support cross-org transfer :(

/kind bug
/sig api-machinery
/cc @stijndehaes @pier-oliviert
Hi All,

We are seeing a lot of error logs when running our operator into production. And we can't figure out what is happening. The basic flow of what we do goes as follows:

```
func (r *SparkReconciler) createSparkSubmitterPod(ctx context.Context, spark *runtimev1.Spark) error {
	var pod corev1.Pod
	err := r.Get(ctx, client.ObjectKey{
		Name:      submitterPodName(spark),
		Namespace: spark.Namespace,
	}, &pod)
	if apierrors.IsNotFound(err) {
		r.Log.V(1).Info("Creating the spark submitter pod")
		return r.doCreateSparkSubmitterPod(ctx, spark)
	}
	return errors.Wrap(err, "Failed getting the pod")
}
```
As you can see in the code snippet. We create a pod only when it does not exist. Because we read from cache we know that the get call can return not found when the pod exists. We do the following in the doCreateSparkSubmitterPod:

```
if err := r.Client.Create(ctx, pod); err != nil {
    if apierrors.IsAlreadyExists(err) {
	return nil
    }
    return errors.Wrap(err, "Failed creating the pod")
}
```
So we capture an already exists error. However we tend to see a lot of errors like this when running into production:
```
"error":"Operation cannot be fulfilled on pods \"255f70af-5699-46c6-8002-a4df45af5209\": StorageError: invalid object, Code: 4, Key: /registry/pods/addatatest2/255f70af-5699-46c6-8002-a4df45af5209, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: f8104e60-1a9d-40ca-847e-52bc0f556844, UID in object meta: "
```

We have no clue on how to proceed on this. I think it might happen when the pod already exists. But not entirely sure. Perhaps someone can shed a light on this?

### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124345 Kubelet restart cause running pod restart with UnexpectedAdmissionError when pods have initContainers and external devices like GPU 

- Issue é“¾æ¥ï¼š[#124345](https://github.com/kubernetes/kubernetes/issues/124345)

### Issue å†…å®¹

#### What happened?

When restarting kubelet, it will restart the running pods with UnexpectedAdmissionError when pods' initContainers and containers both use external devices like GPU 

#### What did you expect to happen?

Restart kubelet should not cause running pods to restart

#### How can we reproduce it (as minimally and precisely as possible)?

1. create some pods with nvidia.com/gpu and initContainers which also use GPUs
2. wait until initContainers exit
3. restart kubelet

#### Anything else we need to know?

there was an issue and a fix for running pods with devices, but it looks like the initContainers is not counted as should skip containers.

the fix has cherry-picked to v1.25.16

- issue: https://github.com/kubernetes/kubernetes/issues/118559
- fix: https://github.com/kubernetes/kubernetes/pull/118635
- cherry-pick v1.25 since v1.25.14: https://github.com/kubernetes/kubernetes/pull/119707

Related Issues:
- Completed Pod will also be affected: https://github.com/kubernetes/kubernetes/issues/117955
- memory manager related kubelet restart cause UnexpectedAdmissionError: https://github.com/kubernetes/kubernetes/issues/123971

#### Kubernetes version

<details>

```console
$ kubectl version
# 1.25.16
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124336 CVE-2024-3177: Bypassing mountable secrets policy imposed by the ServiceAccount admission plugin

- Issue é“¾æ¥ï¼š[#124336](https://github.com/kubernetes/kubernetes/issues/124336)

### Issue å†…å®¹

CVSS Rating: [CVSS:3.1/AV:N/AC:L/PR:H/UI:N/S:U/C:L/I:N/A:N](https://www.first.org/cvss/calculator/3.1#CVSS:3.1/AV:N/AC:L/PR:H/UI:N/S:U/C:L/I:N/A:N) - **Low** (2.7)

A security issue was discovered in Kubernetes where users may be able to launch containers that bypass the mountable secrets policy enforced by the ServiceAccount admission plugin when using containers, init containers, and ephemeral containers with the envFrom field populated. The policy ensures pods running with a service account may only reference secrets specified in the service accountâ€™s secrets field. Kubernetes clusters are only affected if the ServiceAccount admission plugin and the `kubernetes.io/enforce-mountable-secrets` annotation are used together with containers, init containers, and ephemeral containers with the envFrom field populated. 

#### Am I vulnerable?

The ServiceAccount admission plugin is used. Most cluster should have this on by default as recommended in  https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#serviceaccount
The kubernetes.io/enforce-mountable-secrets annotation is used by a service account. This annotation is not added by default. Pods using containers, init containers, and ephemeral containers with the envFrom field populated.

##### Affected Versions

kube-apiserver v1.29.0 - v1.29.3
kube-apiserver v1.28.0 - v1.28.8
kube-apiserver <= v1.27.12

#### How do I mitigate this vulnerability?

This issue can be mitigated by applying the patch provided for the kube-apiserver component. The patch prevents containers, init containers, and ephemeral containers with the envFrom field populated from bypassing the mountable secrets policy enforced by the ServiceAccount admission plugin.

Outside of applying the provided patch, there are no known mitigations to this vulnerability.

##### Fixed Versions

- kube-apiserver master - fixed by #124322
- kube-apiserver v1.29.4 - fixed by #124325
- kube-apiserver v1.28.9 - fixed by #124326
- kube-apiserver v1.27.13 - fixed by #124327

To upgrade, refer to the documentation:
https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/ 

#### Detection

Pod update requests using a container, init container, or ephemeral container with the envFrom field populated that exploits this vulnerability with unintended secret will be captured in API audit logs. You can also use the following kubectl command to find active pods using the `kubernetes.io/enforce-mountable-secrets` annotation. 

`kubectl get serviceaccounts --all-namespaces -o jsonpath="{range .items[?(@.metadata.annotations['kubernetes\.io/enforce-mountable-secrets']=='true')]}{.metadata.namespace}{'\t'}{.metadata.name}{'\n'}{end}"` 

If you find evidence that this vulnerability has been exploited, please contact security@kubernetes.io

##### Acknowledgements

This vulnerability was reported by tha3e1vl. 

The issue was fixed and coordinated by the fix team: 

Rita Zhang @ritazh
Joel Smith @joelsmith
Mo Khan @enj

and release managers:
Sascha Grunert @saschagrunert
Jeremy Rickard @jeremyrickard

/triage accepted
/lifecycle frozen
/area security
/kind bug
/committee security-response

### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124332 failed to assign quota after kubelet was restarted

- Issue é“¾æ¥ï¼š[#124332](https://github.com/kubernetes/kubernetes/issues/124332)

### Issue å†…å®¹

#### What happened?

After kubelet was restarted, kubelet starts to output quota related errors and the total count of entries in `/etc/projects` and `/etc/projid` decreased.

```console
I0416 17:38:13.086019  157568 empty_dir.go:306] Set quota on /var/lib/kubelet/pods/e5149231-3a81-41e0-87be-579846f6caea/volumes/kubernetes.io~configmap/config failed assign quota FAILED exit status 1
I0416 17:38:13.086051  157568 operation_generator.go:838] "MountVolume.markVolumeErrorState leaving volume uncertain" volumeName=kubernetes.io/configmap/e5149231-3a81-41e0-87be-579846f6caea-config
E0416 17:38:13.086130  157568 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/configmap/e5149231-3a81-41e0-87be-579846f6caea-config podName:e5149231-3a81-41e0-87be-579846f6caea nodeName:}" failed. No retries permitted until 2024-04-16 17:40:15.086106798 +0800 CST m=+376.261198810 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "config" (UniqueName: "kubernetes.io/configmap/e5149231-3a81-41e0-87be-579846f6caea-config") pod "xxx-yyy" (UID: "e5149231-3a81-41e0-87be-579846f6caea") : assign quota FAILED exit status 1
I0416 17:38:13.086156  157568 event.go:294] "Event occurred" object="xxx/yyy" fieldPath="" kind="Pod" apiVersion="v1" type="Warning" reason="FailedMount" message="MountVolume.SetUp failed for volume \"config\" : assign quota FAILED exit status 1"
```

Similar issues: #115309

#### What did you expect to happen?

-

#### How can we reproduce it (as minimally and precisely as possible)?

1. ext4 fs with quota enabled and enforced `tune2fs -O project -Q prjquota /dev/xxx; mount -o prjquota ...`
2. enable featuregate `LocalStorageCapacityIsolationFSQuotaMonitoring` in kubelet config
3. register this node to k8s (start kubelet): works well
4. restart kubelet: reproduced

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.7", GitCommit:"84e1fc493a47446df2e155e70fca768d2653a398", GitTreeState:"clean", BuildDate:"2023-07-19T12:23:27Z", GoVersion:"go1.20.6", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.7", GitCommit:"84e1fc493a47446df2e155e70fca768d2653a398", GitTreeState:"clean", BuildDate:"2023-07-19T12:16:45Z", GoVersion:"go1.20.6", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
IDC
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="AlmaLinux"
VERSION="9.3 (Shamrock Pampas Cat)"
ID="almalinux"
ID_LIKE="rhel centos fedora"
VERSION_ID="9.3"
PLATFORM_ID="platform:el9"
PRETTY_NAME="AlmaLinux 9.3 (Shamrock Pampas Cat)"
ANSI_COLOR="0;34"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:almalinux:almalinux:9::baseos"
HOME_URL="https://almalinux.org/"
DOCUMENTATION_URL="https://wiki.almalinux.org/"
BUG_REPORT_URL="https://bugs.almalinux.org/"

ALMALINUX_MANTISBT_PROJECT="AlmaLinux-9"
ALMALINUX_MANTISBT_PROJECT_VERSION="9.3"
REDHAT_SUPPORT_PRODUCT="AlmaLinux"
REDHAT_SUPPORT_PRODUCT_VERSION="9.3"

$ uname -a
Linux xxx 5.14.0-362.24.2.el9_3.x86_64 #1 SMP PREEMPT_DYNAMIC Sat Mar 30 14:11:54 EDT 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd 1.6.31
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124310 Pod soft evicted is still ready in kubernetes 1.24+ .

- Issue é“¾æ¥ï¼š[#124310](https://github.com/kubernetes/kubernetes/issues/124310)

### Issue å†…å®¹

#### What happened?

If pod was soft evicted, it will keep ready until all container dead. Endpoints of the pod will not be removed.

When kubelet triggers soft eviction, the status manager will try to report the phase as failed.
And there are still running containers, the phase of newStatus will be overwritten by oldStatus.

https://github.com/kubernetes/kubernetes/blob/9791f0d1f39f3f1e0796add7833c1059325d5098/pkg/kubelet/status/status_manager.go#L1097-L1103

And then status manager will calculate pod condition by phase of oldStatus.
https://github.com/kubernetes/kubernetes/blob/9791f0d1f39f3f1e0796add7833c1059325d5098/pkg/kubelet/status/status_manager.go#L1109-L1117

This will lead to pod still ready, and the endpoint will not be removed until the container is killed, even if the Prestop Hook has been executed.

#### What did you expect to happen?

Kubelet will mark pod as not ready, if eviction triggered.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Start kubelet with args `--eviction-soft-grace-period=allocatableMemory.available=10s --eviction-soft=allocatableMemory.available<2Gi`
2. Create a pod and handle SIGTERM.
3. Run stress in pod cgroup.
4. Run `for c in `seq 1 1000`; do kubectl get pod -o yaml <pod-name> > `date +%T`.yaml; sleep 1; done

We'll see pod still ready until failed.

#### Anything else we need to know?

From https://github.com/kubernetes/kubernetes/pull/117822.

#### Kubernetes version

v1.26.1 or v1.28.3

#### Cloud provider

No matter.

#### OS version

No matter.

#### Install tools

No matter.

#### Container runtime (CRI) and version (if applicable)

No matter.

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

No matter.

### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124308 StartError (exit code 128) when pod tries to mount configmap that is being changed at same time

- Issue é“¾æ¥ï¼š[#124308](https://github.com/kubernetes/kubernetes/issues/124308)

### Issue å†…å®¹

#### What happened?

This is happening when deploying a pod with argo workflow and there is another job running which is editing the configmap
The workflow failed with following error 
`
StartError (exit code 128): failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error mounting "/var/lib/kubelet/pods/f22aa0b7-afdc-497c-beb3-ccb394e8428d/volume-subpaths/config/wait/9" to rootfs at "/mainctrfs/my.py": mount /var/lib/kubelet/pods/f22aa0b7-afdc-497c-beb3-ccb394e8428d/volume-subpaths/my-py/wait/9:/mainctrfs/my.py (via /proc/self/fd/6), flags: 0x5001, data: context="system_u:object_r:data_t:s0:c923,c1020": no such file or directory: unknown
`

#### What did you expect to happen?

I expected the pod to come up successfully with whatever version of configmap is available 

#### How can we reproduce it (as minimally and precisely as possible)?

This is little hard to reproduce, but to reproduce:
1. Run a script or job to replace/overwrite a configmap
2. at the same time try to create a pod which is mounting data from configmap with subpath

#### Anything else we need to know?

_No response_

#### Kubernetes version

client Version: v1.29.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.27.8-gke.1067004


#### Cloud provider

<details>
GKE
</details>


#### OS version

_No response_

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

Related Issues:
https://github.com/argoproj/argo-workflows/issues/11983

### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124307 Overlapping labels can lead to HPA matching incorrect pods

- Issue é“¾æ¥ï¼š[#124307](https://github.com/kubernetes/kubernetes/issues/124307)

### Issue å†…å®¹

#### What happened?

When an HPA targets a Deployment which has a label selector matching Pods that don't belong to it (overlapping labels, for example), those "other" Pods are considered by the HPA to be part of the targeted HPA.

#### What did you expect to happen?

I have always been lead to believe that this behaviour is correct and the user should have labels that are specific enough to not overlap other Deployments.
This is stated [here](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#creating-a-deployment):
> Do not overlap labels or selectors with other controllers (including other Deployments and StatefulSets). Kubernetes doesn't stop you from overlapping, and if multiple controllers have overlapping selectors those controllers might conflict and behave unexpectedly.

However, recently the VPA fixed the same behaviour in https://github.com/kubernetes/autoscaler/pull/6460

Which makes me wonder if I should expect the HPA to test the owner reference too, and only target that Deployment's pods.


#### How can we reproduce it (as minimally and precisely as possible)?

1. Create 2 deployments, both with the same labels/selectors
2. In 1 Deployment set resources
3. Deploy an HPA (and metrics-server)
4. Watch the events of the HPA throw error events related to Pods not under control of the HPA's deployment
   1. ie: `  Warning  FailedGetResourceMetric       87s (x3 over 117s)     horizontal-pod-autoscaler  failed to get cpu utilization: missing request for cpu in container ubuntu of Pod other-64886557cb-ldtnt`

<details>
  <summary>YAML files</summary>
  
  ```yaml
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: workload
  name: workload
spec:
  replicas: 1
  selector:
    matchLabels:
      app: other
  template:
    metadata:
      labels:
        app: other
    spec:
      containers:
      - command:
        - /bin/bash
        - -c
        - sha256sum /dev/zero
        image: ubuntu
        imagePullPolicy: Always
        name: nginx
        resources:
          requests:
            cpu: 10m
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: other
  name: other
spec:
  replicas: 1
  selector:
    matchLabels:
      app: other
  template:
    metadata:
      labels:
        app: other
    spec:
      containers:
      - command:
        - /bin/bash
        - -c
        - sleep infinity
        image: ubuntu
        imagePullPolicy: Always
        name: ubuntu
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: workload
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: workload
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
  ```
</details>


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.2
```

</details>


#### Cloud provider

<details>

- kind
- GKE
- kOps (AWS)

</details>


#### OS version

_No response_

#### Install tools

<details>
kind
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124286 Kubernetes not seeing existing containerd images

- Issue é“¾æ¥ï¼š[#124286](https://github.com/kubernetes/kubernetes/issues/124286)

### Issue å†…å®¹

#### What happened?

Hi, I'm using  ctr to export and import images because there's network issue which does not allow gcr.io images to be downloaded in my cluster. It worked well for a few images, I installed Knative and some other componenes in this way. However, when I install Tekton, I fond that somehow kubernetes is not using existing images.

The pods keeps pulling images:

```
$ kubectl get pods -n tekton-pipelines
NAME                                                READY   STATUS             RESTARTS   AGE
tekton-dashboard-84c6c97d8f-6vhbz                   0/1     ImagePullBackOff   0          13m

$ kubectl describe pod -n tekton-pipelines tekton-dashboard-84c6c97d8f-6vhbz
...
Events:
  Type     Reason     Age                   From               Message
  ----     ------     ----                  ----               -------
  Normal   Scheduled  14m                   default-scheduler  Successfully assigned tekton-pipelines/tekton-dashboard-84c6c97d8f-6vhbz to node02
  Warning  Failed     11m (x2 over 13m)     kubelet            Failed to pull image "gcr.io/tekton-releases/github.com/tektoncd/dashboard/cmd/dashboard:v0.45.0@sha256:5cd17db16f6b930e85d051a5669b032e5680ec80c2c87bc3a2f0134d55a53c38": rpc error: code = DeadlineExceeded desc = failed to pull and unpack image "gcr.io/tekton-releases/github.com/tektoncd/dashboard/cmd/dashboard@sha256:5cd17db16f6b930e85d051a5669b032e5680ec80c2c87bc3a2f0134d55a53c38": failed to resolve reference "gcr.io/tekton-releases/github.com/tektoncd/dashboard/cmd/dashboard@sha256:5cd17db16f6b930e85d051a5669b032e5680ec80c2c87bc3a2f0134d55a53c38": failed to do request: Head "https://gcr.io/v2/tekton-releases/github.com/tektoncd/dashboard/cmd/dashboard/manifests/sha256:5cd17db16f6b930e85d051a5669b032e5680ec80c2c87bc3a2f0134d55a53c38": dial tcp 108.177.125.82:443: i/o timeout
```

However , the images is already present in my cluter nodes:

```
$ ctr -n k8s.io images list | grep gcr.io/tekton-releases/github.com/tektoncd/dashboard/cmd/dashboard:v0.45.0@sha256:5cd17db16f6b930e85d051a5669b032e5680ec80c2c87bc3a2f0134d55a53c38
gcr.io/tekton-releases/github.com/tektoncd/dashboard/cmd/dashboard:v0.45.0@sha256:5cd17db16f6b930e85d051a5669b032e5680ec80c2c87bc3a2f0134d55a53c38                   application/vnd.oci.image.index.v1+json                   sha256:5cd17db16f6b930e85d051a5669b032e5680ec80c2c87bc3a2f0134d55a53c38 24.0 MiB  linux/amd64,linux/arm/v6,linux/arm/v7,linux/arm64,linux/ppc64le,linux/s390x                         io.cri-containerd.image=managed
```

And to verify that it's not a problem of ImagePullPolicy, I tried :
```
$ kubectl run tekton-dashboard-test --image=gcr.io/tekton-releases/github.com/tektoncd/dashboard/cmd/dashboard:v0.45.0@sha256:5cd17db16f6b930e85d051a5669b032e5680ec80c2c87bc3a2f0134d55a53c38 --image-pull-policy=Never

$ kubectl get pods
NAME                    READY   STATUS              RESTARTS   AGE
tekton-dashboard-test   0/1     ErrImageNeverPull   0          10m
```

Is this a bug?



#### What did you expect to happen?

Images imported via `ctr import` should be present in kubernetes

#### How can we reproduce it (as minimally and precisely as possible)?

1. A kubernetes cluster with containerd runtime
2. pull image in another machine and export it to a tar file:
```
sudo ctr images pull gcr.io/tekton-releases/github.com/tektoncd/dashboard/cmd/dashboard:v0.45.0@sha256:5cd17db16f6b930e85d051a5669b032e5680ec80c2c87bc3a2f0134d55a53c38 --all-platforms

sudo ctr images export dashboard-all-arch.tar gcr.io/tekton-releases/github.com/tektoncd/dashboard/cmd/dashboard:v0.45.0@sha256:5cd17db16f6b930e85d051a5669b032e5680ec80c2c87bc3a2f0134d55a53c38
```
3.  Import the tar file in all kubernetes nodes:
```
ctr -n k8s.io images import dashboard-all-arch.tar
```
4. Try to start a pod with the imported image:
```
kubectl run tekton-dashboard-test --image=gcr.io/tekton-releases/github.com/tektoncd/dashboard/cmd/dashboard:v0.45.0@sha256:5cd17db16f6b930e85d051a5669b032e5680ec80c2c87bc3a2f0134d55a53c38 --image-pull-policy=Never
```

#### Anything else we need to know?

This only happens to the tekton images somehow, I've installed other components in this way and worked before:

```
kubectl get pods --all-namespaces
NAMESPACE                    NAME                                                   READY   STATUS              RESTARTS   AGE
calico-apiserver             calico-apiserver-549fdf488f-6m84p                      1/1     Running             0          44h
calico-apiserver             calico-apiserver-549fdf488f-dwz7v                      1/1     Running             0          44h
calico-system                calico-kube-controllers-7bc6b5bb8-5dnsf                1/1     Running             0          44h
calico-system                calico-node-c4zxq                                      1/1     Running             0          27h
calico-system                calico-node-jdw2v                                      1/1     Running             0          44h
calico-system                calico-node-jw4kx                                      1/1     Running             0          27h
calico-system                calico-typha-5c754949c6-7zzdx                          1/1     Running             0          27h
calico-system                calico-typha-5c754949c6-qhfwz                          1/1     Running             0          44h
calico-system                csi-node-driver-9wmmd                                  2/2     Running             0          44h
calico-system                csi-node-driver-gpjqq                                  2/2     Running             0          27h
calico-system                csi-node-driver-sfqm7                                  2/2     Running             0          27h
default                      tekton-dashboard-test                                  0/1     ErrImageNeverPull   0          16m
knative-eventing             eventing-controller-68fb776c69-lvqjn                   1/1     Running             0          20h
knative-eventing             eventing-webhook-664866f89f-pvz6v                      1/1     Running             0          20h
knative-eventing             imc-controller-848cf74bc8-5bkxt                        1/1     Running             0          20h
knative-eventing             imc-dispatcher-57789765c6-7cmvz                        1/1     Running             0          20h
knative-eventing             kafka-broker-dispatcher-7c8b4cfc67-qxjqv               1/1     Running             0          20h
knative-eventing             kafka-broker-receiver-f8f488bd7-wsxnd                  1/1     Running             0          20h
knative-eventing             kafka-controller-b595dc6d-cg5dp                        1/1     Running             0          20h
knative-eventing             kafka-webhook-eventing-5d985c95d4-thg56                1/1     Running             0          20h
knative-eventing             mt-broker-controller-d8c869469-99g2q                   1/1     Running             0          20h
knative-eventing             mt-broker-filter-56d967db6-9q9hk                       1/1     Running             0          20h
knative-eventing             mt-broker-ingress-6c59758448-p2trg                     1/1     Running             0          20h
knative-serving              activator-58db57894b-ntvhb                             1/1     Running             0          21h
knative-serving              autoscaler-76f95fff78-f6r9q                            1/1     Running             0          21h
knative-serving              controller-7dd875844b-xw96w                            1/1     Running             0          21h
knative-serving              net-kourier-controller-6559c556d7-t54fk                1/1     Running             0          20h
knative-serving              webhook-d8674645d-ggfg9                                1/1     Running             0          21h
kourier-system               3scale-kourier-gateway-9bd7579-tt8sx                   1/1     Running             0          20h
kube-system                  coredns-76f75df574-f829g                               1/1     Running             0          45h
kube-system                  coredns-76f75df574-hg2sj                               1/1     Running             0          45h
kube-system                  csi-nfs-controller-646d4f97cb-s4k9k                    4/4     Running             0          24h
kube-system                  csi-nfs-node-dhklg                                     3/3     Running             0          24h
kube-system                  csi-nfs-node-h5w94                                     3/3     Running             0          24h
kube-system                  csi-nfs-node-prvp7                                     3/3     Running             0          24h
kube-system                  etcd-master                                            1/1     Running             0          45h
kube-system                  kube-apiserver-master                                  1/1     Running             0          45h
kube-system                  kube-controller-manager-master                         1/1     Running             0          45h
kube-system                  kube-proxy-g6llx                                       1/1     Running             0          27h
kube-system                  kube-proxy-tsp6d                                       1/1     Running             0          45h
kube-system                  kube-proxy-vzlbf                                       1/1     Running             0          27h
kube-system                  kube-scheduler-master                                  1/1     Running             0          45h
kube-system                  metrics-server-84989b68d9-fw5wt                        1/1     Running             0          18h
kubernetes-dashboard         kubernetes-dashboard-api-6c6b9896d4-gn82c              1/1     Running             0          18h
kubernetes-dashboard         kubernetes-dashboard-auth-8546cbf77b-qnph8             1/1     Running             0          18h
kubernetes-dashboard         kubernetes-dashboard-kong-75bb76dd5f-d5cr6             1/1     Running             0          18h
kubernetes-dashboard         kubernetes-dashboard-metrics-scraper-6485f66c7-5dxvj   1/1     Running             0          18h
kubernetes-dashboard         kubernetes-dashboard-web-6fc6cd548-bhf9j               1/1     Running             0          18h
metallb-system               controller-756c6b677-ggl74                             1/1     Running             0          134m
metallb-system               speaker-5m9p7                                          1/1     Running             0          134m
metallb-system               speaker-pmw7p                                          1/1     Running             0          134m
metallb-system               speaker-tmst5                                          1/1     Running             0          134m
tekton-pipelines-resolvers   tekton-pipelines-remote-resolvers-7ccf88788-6wxdk      0/1     ImagePullBackOff    0          83m
tekton-pipelines             tekton-dashboard-84c6c97d8f-6vhbz                      0/1     ImagePullBackOff    0          23m
tekton-pipelines             tekton-events-controller-7856d7f897-jg8p7              0/1     ImagePullBackOff    0          83m
tekton-pipelines             tekton-pipelines-controller-7cd6dd989f-hg2z6           0/1     ImagePullBackOff    0          83m
tekton-pipelines             tekton-pipelines-webhook-6779f85664-7qt7h              0/1     ImagePullBackOff    0          83m
tekton-pipelines             tekton-triggers-controller-5b6d5f54b7-psvfh            0/1     ImagePullBackOff    0          82m
tekton-pipelines             tekton-triggers-core-interceptors-f58696689-qk5bt      0/1     ImagePullBackOff    0          82m
tekton-pipelines             tekton-triggers-webhook-689688fc54-cfblg               0/1     ImagePullBackOff    0          82m
tigera-operator              tigera-operator-6bfc79cb9c-r585l                       1/1     Running             0          44h
```

#### Kubernetes version

<details>

```console
$ kubectl version
kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.3
```

</details>


#### Cloud provider

The cluster is created manually with 3  ubuntu vms. Here's my kubernetes information:
<details>

```
$ kubectl get nodes -o wide
NAME     STATUS   ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION       CONTAINER-RUNTIME
master   Ready    control-plane   45h   v1.29.3   10.19.30.61   <none>        Ubuntu 22.04.4 LTS   5.15.0-102-generic   containerd://1.6.28
node01   Ready    <none>          27h   v1.29.3   10.19.30.64   <none>        Ubuntu 22.04.4 LTS   5.15.0-102-generic   containerd://1.6.28
node02   Ready    <none>          27h   v1.29.3   10.19.30.13   <none>        Ubuntu 22.04.4 LTS   5.15.0-102-generic   containerd://1.6.28
```
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
uname -a
Linux master.agent-less.ksord.com 5.15.0-102-generic #112-Ubuntu SMP Tue Mar 5 16:50:32 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kubeadm init ...
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
ctr version
Client:
  Version:  1.6.28
  Revision: ae07eda36dd25f8a1b98dfbf587313b99c0190bb
  Go version: go1.21.8

Server:
  Version:  1.6.28
  Revision: ae07eda36dd25f8a1b98dfbf587313b99c0190bb
  UUID: 394309a2-cee9-40bd-a675-1f46af6cb45
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124282 Scheduler still counts InitContainer resource requests after a pod finishes initialization

- Issue é“¾æ¥ï¼š[#124282](https://github.com/kubernetes/kubernetes/issues/124282)

### Issue å†…å®¹

#### What happened?

I created a 1-node EKS cluster with that node having 4 CPU cores. I made sure at least 3.5 cores were allocatable after running all the daemonsets.

I created a deployment that has InitContainers, here is a summary of what that looked like (I replaced container names with dummy values and removed all the unnecessary container configuration).
```
initContainers:
    - image: 'test/initialization:v3.1.3'
      name: initialize
      resources:
        requests:
          cpu: 3
containers:
    - image: test/do-stuff
      resources:
        requests:
          cpu: 30m
```

I set the number of replicas to 1 and a pod successfully got scheduled, initialized and transitioned to the running phase.

I then bumped the number of replicas to 2, expecting the second pod to become scheduled since the first pod already finished initialization. However, it failed to schedule
```
Events:
  Type     Reason            Age                 From               Message
  ----     ------            ----                ----               -------
  Warning  FailedScheduling  57s (x2 over 6m3s)  default-scheduler  0/1 nodes are available: 1 Insufficient cpu. preemption: 0/1 nodes are available: 1 No preemption victims found for incoming pod.
```

#### What did you expect to happen?

I expected the second pod to go through since the first pod had already finished initialization and the CPU it requests for running would definitely leave more than 3 cores allocatable for the next pod.


From the documentation, I know the scheduler computes the "effective resource requests" on pods with InitContainers by getting the maximum requests across InitContainers and containers. I understand how this is essential when deciding where to place a pod.

However, if a pod finishes initialization and transitions to the running phase, I'd expect the scheduler to discard the requests specified on InitContainers when it's computing how many resources are already scheduled on a particular node.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Start with a minimal cluster with one node and keep note of how many CPU is allocatable.
2. Run 1 pod on the cluster that has an InitContainer comprising 75% of the allocatable CPU and a container with negligible CPU requests
3. Wait till that pod finishes initialization and then run another one. The other one will not be schedulable.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"26", GitVersion:"v1.26.0", GitCommit:"b46a3f887ca979b1a5d14fd39cb1af43e7e5d12d", GitTreeState:"clean", BuildDate:"2022-12-08T19:58:30Z", GoVersion:"go1.19.4", Compiler:"gc", Platform:"darwin/amd64"}
Kustomize Version: v4.5.7
Server Version: version.Info{Major:"1", Minor:"29+", GitVersion:"v1.29.1-eks-b9c9ed7", GitCommit:"07600c74de018baffb16c82771a48adcb843a932", GitTreeState:"clean", BuildDate:"2024-03-02T03:46:35Z", GoVersion:"go1.21.6", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
EKS (Amazon AWS)
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Amazon Linux"
VERSION="2"
ID="amzn"
ID_LIKE="centos rhel fedora"
VERSION_ID="2"
PRETTY_NAME="Amazon Linux 2"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2"
HOME_URL="https://amazonlinux.com/"
SUPPORT_END="2025-06-30"
$ uname -a
Linux <hostname trimmed> 5.10.205-195.807.amzn2.x86_64 #1 SMP Tue Jan 16 18:28:59 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>

- EKS Cluster was installed using terraform
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd 1.7.11 on EKS
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>


- coredns from Amazon EKS version: `v1.11.1-eksbuild.6`
- VPC CNI from Amazon EKS version: `v1.16.2-eksbuild.1`
- Kube Proxy from Amazon EKS version: `v1.29.0-eksbuild.2`
- Amazon EBS CSI Driver from Amazon EKS version: `v1.27.0-eksbuild.1`
</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124275 kube-scheduler: pod stuck in `PENDING` but added to active queue

- Issue é“¾æ¥ï¼š[#124275](https://github.com/kubernetes/kubernetes/issues/124275)

### Issue å†…å®¹

#### What happened?

We are writing a scheduler plugin using the scheduling framework. However, when creating a pod that uses out secondary scheduler e.g. by using

```
schedulerName: my-scheduler
```

The scheduler logs:
```
I0411 14:59:41.249623       1 eventhandlers.go:126] "Add event for unscheduled pod" pod="ns/pod1"
I0411 14:59:41.249678       1 scheduling_queue.go:575] "Pod moved to an internal scheduling queue" pod="ns/pod1" event="PodAdd" queue="Active"
```

but then nothing happens. The pod is stuck in `PENDING` state. Also, no events are created. 
We have the same scheduler running against a k8s v1.29 EKS cluster fine. It appears the pod is never retrieved from the active queue, e.g. [this line](https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/schedule_one.go#L68) is never hit.

#### What did you expect to happen?

The pod should get scheduled or fail unscheduled.

#### How can we reproduce it (as minimally and precisely as possible)?

Build a scheduler plugin using versions below and run on k8s v1.21.14.

#### Anything else we need to know?

Versions from `go.mod`:
<details>

```console
k8s.io/api => k8s.io/api v0.29.0
k8s.io/apiextensions-apiserver => k8s.io/apiextensions-apiserver v0.29.0
k8s.io/apimachinery => k8s.io/apimachinery v0.29.0
k8s.io/apiserver => k8s.io/apiserver v0.29.0
k8s.io/cli-runtime => k8s.io/cli-runtime v0.29.0
k8s.io/client-go => k8s.io/client-go v0.29.0
k8s.io/cloud-provider => k8s.io/cloud-provider v0.29.0
k8s.io/cluster-bootstrap => k8s.io/cluster-bootstrap v0.29.0
k8s.io/code-generator => k8s.io/code-generator v0.29.0
k8s.io/component-base => k8s.io/component-base v0.29.0
k8s.io/component-helpers => k8s.io/component-helpers v0.29.0
k8s.io/controller-manager => k8s.io/controller-manager v0.29.0
k8s.io/cri-api => k8s.io/cri-api v0.29.0
k8s.io/csi-translation-lib => k8s.io/csi-translation-lib v0.29.0
k8s.io/dynamic-resource-allocation => k8s.io/dynamic-resource-allocation v0.29.0
k8s.io/endpointslice => k8s.io/endpointslice v0.29.0
k8s.io/kms => k8s.io/kms v0.29.0
k8s.io/kube-aggregator => k8s.io/kube-aggregator v0.29.0
k8s.io/kube-controller-manager => k8s.io/kube-controller-manager v0.29.0
k8s.io/kube-proxy => k8s.io/kube-proxy v0.29.0
k8s.io/kube-scheduler => k8s.io/kube-scheduler v0.29.0
k8s.io/kubectl => k8s.io/kubectl v0.29.0
k8s.io/kubelet => k8s.io/kubelet v0.29.0
k8s.io/kubernetes => k8s.io/kubernetes v1.29.0
k8s.io/legacy-cloud-providers => k8s.io/legacy-cloud-providers v0.29.0
k8s.io/metrics => k8s.io/metrics v0.29.0
k8s.io/mount-utils => k8s.io/mount-utils v0.29.0
k8s.io/pod-security-admission => k8s.io/pod-security-admission v0.29.0
k8s.io/sample-apiserver => k8s.io/sample-apiserver v0.29.0
k8s.io/sample-cli-plugin => k8s.io/sample-cli-plugin v0.29.0
k8s.io/sample-controller => k8s.io/sample-controller v0.29.0
```
</details>

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"21", GitVersion:"v1.21.13", GitCommit:"80ec6572b15ee0ed2e6efa97a4dcd30f57e68224", GitTreeState:"clean", BuildDate:"2022-05-24T12:40:44Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"darwin/amd64"}
Server Version: version.Info{Major:"1", Minor:"21", GitVersion:"v1.21.14", GitCommit:"0f77da5bd4809927e15d1658fb4aa8f13ad890a5", GitTreeState:"clean", BuildDate:"2022-06-15T14:11:36Z", GoVersion:"go1.16.15", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124264 Deployment rollout status not reliable when using `status.conditions`

- Issue é“¾æ¥ï¼š[#124264](https://github.com/kubernetes/kubernetes/issues/124264)

### Issue å†…å®¹

#### What happened?

Currently I am working on a project where we implement a Kubernetes operator and we decided that for some flows we will need to wait for some deployments to complete the rollout before advancing to some other steps from our processes.
So, for an old deployment we only update an image in the pod template and then we wait for the updated deployment to finish the rollout.

The way the status of the Deployment is built/implemented is error prone and not reliable if we only take into account the `status.conditions` (as suggested in the documentation)

#### What did you expect to happen?

After updating the spec of a deployment, when the `status.ObservedGeneration` is changed to the latest `metadata.Generation`, the `status.conditions` should also be updated.

#### How can we reproduce it (as minimally and precisely as possible)?

We found out that if:
- we start with a Deployment having a `status.condition` of type `Progressing` with the reason `ProgressDeadlineExceeded`
- update the deployment spec (the only change would be an image from a container from the pod template)
- on a timer (every second), start retrieving the deployment using the API and check the status
Then, under some circumstances (so, not always; it's a concurrency/timing issue), the first change of the `status.observedGeneration` toward the new `metadata.generation` value, comes with *no change*(!) in the `status.conditions` (the old conditions are still there *without any single change*(!)). 
According to the Kubernetes documentation (and also the way the Operator SDK checks this rollout status), the interpretation of these would lead to say that the rollout failed, which is not accurate, because the actual rollout will only start a few seconds later...

This is an example from a log describing the issue (please observe that the two blocks are logged at a few milliseconds distance and the `status.conditions` are the same even the `status.ObservedGeneration` changed):
```
# 10:53:18.814 > Metadata.Generation: 2 | Status >> ObservedGeneration: 1, replicas: 1 , readyReplicas: , availableReplicas: , updatedReplicas: 1, unAvailableReplicas: 1
  Conditions:
    @10-Apr-24 10:53:02 | 10-Apr-24 10:53:02 | Available | False | MinimumReplicasUnavailable | Deployment does not have minimum availability.
    @10-Apr-24 10:53:18 | 10-Apr-24 10:53:18 | Progressing | False | ProgressDeadlineExceeded | ReplicaSet "my-deployment-5b8dc498c" has timed out progressing.
 => completed: False, failed: False

# 10:53:18.818 > Metadata.Generation: 2 | Status >> ObservedGeneration: 2, replicas: 1 , readyReplicas: , availableReplicas: , updatedReplicas: , unAvailableReplicas: 1
  Conditions:
    @10-Apr-24 10:53:02 | 10-Apr-24 10:53:02 | Available | False | MinimumReplicasUnavailable | Deployment does not have minimum availability.
    @10-Apr-24 10:53:18 | 10-Apr-24 10:53:18 | Progressing | False | ProgressDeadlineExceeded | ReplicaSet "my-deployment-5b8dc498c" has timed out progressing.
 => completed: False, failed: True
```


#### Anything else we need to know?

Please advise on how to reliably interpret the rollout status of a Deployment.


#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.8+k3s1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.8+k3s1
```

</details>


#### Cloud provider

<details>
no cloud
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="K3s v1.28.8+k3s1"


$ uname -a
 Linux 3432b46b9978 5.15.0-101-generic #111-Ubuntu SMP Tue Mar 5 20:16:58 UTC 2024 x86_64 GNU/Linux




```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124260 Broken connectivity while using IPVS, ExternalIP and externalTrafficPolicy: Local

- Issue é“¾æ¥ï¼š[#124260](https://github.com/kubernetes/kubernetes/issues/124260)

### Issue å†…å®¹

#### What happened?

First of all, I read the related issues (https://github.com/kubernetes-sigs/kubespray/issues/10572, https://github.com/k3s-io/k3s/issues/7183, https://github.com/kubernetes/kubernetes/issues/121272, https://github.com/kubernetes/kubernetes/issues/117613) and realized that the problem is still exists in mainstream.

I'm dealing with a Kubernetes cluster managed by Kubespray. It's set up in IPVS mode and uses Calico as a CNI. The cluster is tightly coupled with our network infrastructure: it announces all the types of Kubernetes IPs into ToR switches via BGP (so called IP Fabric Network Design).

In this scheme, the simplest way to assign a globally routed IP into a Service is to declare an ExternalIP. This doesn't require to interact with any kind of Kubernetes API using third-party software (like LoadBalancerIP do), Calico simply catches up the IPs like this and informs the switches about. That's why I can't consider ExternalIPs as an ancient concept (https://github.com/kubernetes/kubernetes/issues/121272#issuecomment-1770986681), they simplify the things.

All the ExternalIPs are automatically assigned to kube-ipvs0 which is great and all work fine with "externalTrafficPolicy: Cluster". But if I change the policy to Local, connections from outside a Kubernetes node to the corresponding ExternalIP become broken with "icmp-port-unreachable" error caused by iptables:

```
-A KUBE-IPVS-FILTER -m set --match-set KUBE-EXTERNAL-IP dst,dst -j RETURN
-A KUBE-IPVS-FILTER -m set --match-set KUBE-HEALTH-CHECK-NODE-PORT dst -j RETURN
-A KUBE-IPVS-FILTER -m conntrack --ctstate NEW -m set --match-set KUBE-IPVS-IPS dst -j REJECT --reject-with icmp-port-unreachable
```

#### What did you expect to happen?

It seems the only thing that the externalTrafficPolicy does in the network stack is moving ExternalIPs between KUBE-EXTERNAL-IP and KUBE-EXTERNAL-IP-LOCAL ipsets, but there's no equivalent RETURN rule for the -LOCAL one in the iptables. If I insert the rule manually:

```
-A KUBE-IPVS-FILTER -m set --match-set KUBE-EXTERNAL-IP dst,dst -j RETURN
-A KUBE-IPVS-FILTER -m set --match-set KUBE-EXTERNAL-IP-LOCAL dst,dst -j RETURN
-A KUBE-IPVS-FILTER -m set --match-set KUBE-HEALTH-CHECK-NODE-PORT dst -j RETURN
-A KUBE-IPVS-FILTER -m conntrack --ctstate NEW -m set --match-set KUBE-IPVS-IPS dst -j REJECT --reject-with icmp-port-unreachable
```
"externalTrafficPolicy: Local" works as expected. BTW, I have no idea why we should reject anything here at all, do you?

#### How can we reproduce it (as minimally and precisely as possible)?

Deploy a single node with IPVS mode. Declare a Service with ExternalIP and externalTrafficPolicy: Local. See the ExternalIP works locally, but filtered from outside the node.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```
# kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.5", GitCommit:"93e0d7146fb9c3e9f68aa41b2b4265b2fcdb0a4c", GitTreeState:"clean", BuildDate:"2023-08-24T00:48:26Z", GoVersion:"go1.20.7", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.5", GitCommit:"93e0d7146fb9c3e9f68aa41b2b4265b2fcdb0a4c", GitTreeState:"clean", BuildDate:"2023-08-24T00:42:11Z", GoVersion:"go1.20.7", Compiler:"gc", Platform:"linux/amd64"}
```

</details>

#### Cloud provider

Self-managed cluster via Kubespray v2.23.0

#### OS version

<details>

```
# cat /etc/os-release 
PRETTY_NAME="Ubuntu 22.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.3 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
```

</details>


#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

```
# calicoctl version
Client Version:    v3.25.2
Git commit:        978a0e4bc
Cluster Version:   v3.25.2
Cluster Type:      kubespray,bgp,kubeadm,kdd,k8s
```

</details>

### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124258 grandson ophan process started via kubectl exec not adopted by pid 1

- Issue é“¾æ¥ï¼š[#124258](https://github.com/kubernetes/kubernetes/issues/124258)

### Issue å†…å®¹

#### What happened?

```console
kubectl exec -- sh

sh-4.2# nohup  sh -c 'sleep 10000' &
sh-4.2# exit

kubectl exec -- sh
sh-4.2# ps -efH
UID         PID   PPID  C STIME TTY          TIME CMD
root        623      0  0 09:51 pts/0    00:00:00 sh -l
root        641    623  0 09:51 pts/0    00:00:00   ps -efH
root        621      0  0 09:50 ?        00:00:00 sleep 1000
root          1      0  0 09:07 ?        00:00:00 /usr/local/bin/dumb-init
```

sleep process ppid alway 0



#### What did you expect to happen?

 ppid should be 1 (sth like dumb-init)


#### How can we reproduce it (as minimally and precisely as possible)?

just run the command above

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.2", GitCommit:"7f6f68fdabc4df88cfea2dcf9a19b2b830f1e647", GitTreeState:"clean", BuildDate:"2023-05-17T14:20:07Z", GoVersion:"go1.20.4", Compiler:"gc", Platform:"darwin/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"23", GitVersion:"v1.23.0", GitCommit:"ab69524f795c42094a6630298ff53f3c3ebab7f4", GitTreeState:"clean", BuildDate:"2021-12-07T18:09:57Z", GoVersion:"go1.17.3", Compiler:"gc", Platform:"linux/amd64"}
WARNING: version difference between client (1.27) and server (1.23) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

no


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124255 removeMissingExtendedResources() did not remove unknown extension resources from InitContainer

- Issue é“¾æ¥ï¼š[#124255](https://github.com/kubernetes/kubernetes/issues/124255)

### Issue å†…å®¹

#### What happened?

The function removeMissingExtendedResources() located at pkg/kubelet/lifecycle/predicate.go:217, is designed to remove any extended resources from a containerâ€™s requests that are not found in nodeInfo.Allocatable before the pod is admitted. This is necessary to support cluster-level resources, which are extended resources that are unknown to nodes.
However, this function only removes missingExtendedResources from the containerâ€™s requests, and does not process the requests of initContainers. This could lead to issues, as initContainers might request resources that do not exist on the node, which could result in the pod failing to be admitted or other unforeseen issues. As such, it may be necessary to refine the removeMissingExtendedResources() function to also handle initContainerâ€™s requests.

#### What did you expect to happen?

Refine the removeMissingExtendedResources() function to also handle initContainerâ€™s requests.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a pod that requests cluster-level extended resources in both the container and InitContainer.

#### Anything else we need to know?

_No response_

#### Kubernetes version

The problem persists in the latest version.


#### Cloud provider

-

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124244 One pv with NFS can be mounted by two pods even with ReadWriteOncePod enabled

- Issue é“¾æ¥ï¼š[#124244](https://github.com/kubernetes/kubernetes/issues/124244)

### Issue å†…å®¹

#### What happened?

Cloud provider: openstack
Csi driver: NFS(Manila)

- First, I got one replicaSet with 1 replica and RWOP enabled so pod A comes up in node A. 
- Then, edit the nodeSelector to node B and forcibly delete the pod A which results in the pod A in terminate state and pod B is running in node B. 
- During this short period, pod A and pod B can both write data to the same pv even with RWOP enabled.
- After pod A is eliminated, pod B fully controls the volume.

#### What did you expect to happen?

The pv should always read-write by a single pod but not access by two pods when the old one is in grace period.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Apply a replicaSet with 1 replica and RWOP enabled like:
```
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: test
  template:
    metadata:
      labels:
        app: test
    spec:
      terminationGracePeriodSeconds: 120
      containers:
      - name: writer
        image: busybox
        env:
        - name: MY_POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        volumeMounts:
        - name: my-volume
          mountPath: "/mnt"
        command: ["/bin/sh"]
        args:
        - -c
        - |
          while true; do
            echo $MY_POD_NAME $(date) >> /mnt/myfile.txt;
            sleep 1;
          done
      volumes:
      - name: my-volume
        persistentVolumeClaim:
          claimName: pvc-rwop
     nodeSelector:
       xxx
```
3. After pod is running, change the nodeSelector in replicaSet to another node
4. Forcibly delete the pod with `kubectl delete pod test-5nffh --force `
5. A new node `test-g6vnt `should come up in a different node and the old pod should be terminated
6. At this time, pv is writable for these two pods and the file /mnt/myfile.txt is overlapped like this:
```
# cat /mnt/myfile.txt
test-5nffh Tue Apr 9 07:29:03 UTC 2024
test-5nffh Tue Apr 9 07:29:04 UTC 2024
test-5nffh Tue Apr 9 07:29:05 UTC 2024
test-g6vnt Tue Apr 9 07:29:06 UTC 2024
test-5nffh Tue Apr 9 07:29:06 UTC 2024
test-g6vnt Tue Apr 9 07:29:07 UTC 2024
test-5nffh Tue Apr 9 07:29:07 UTC 2024
test-g6vnt Tue Apr 9 07:29:08 UTC 2024
test-5nffh Tue Apr 9 07:29:08 UTC 2024
```
7. That means during the shutdown period, pv can be accessible for two pods simultaneously 



#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# 
Server Version: v1.28.6
```

</details>


#### Cloud provider

<details>
openstack
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
VERSION="15-SP3"
VERSION_ID="15.3"
PRETTY_NAME="SUSE Linux Enterprise Server 15 SP3"
$ uname -a
Debian 6.1.77-0gardenlinux1 (2024-02-12) x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
csidriver is: nfs.manila.csi.openstack.org
</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124237 ValidatingAdmissionPolicy using CRDs as paramKind can fail due to 30s discovery mechanism

- Issue é“¾æ¥ï¼š[#124237](https://github.com/kubernetes/kubernetes/issues/124237)

### Issue å†…å®¹

#### What happened?

When a ValidatingAdmissionPolicy is using CRDs as paramKind, it can result in `failed to find resource referenced by paramKind` error if the custom resource is created around the same time as the vap resource. This could result in new resources getting blocked by this vap because it thinks the custom type does not exist assuming fail policy is set to `fail`. 

As discussed with @alexzielenski on slack:
> It can take up to 30s for the discovery mechanism to pickup the change to the CRD list

#### What did you expect to happen?

Allow retries on CRD list to resync without breaking the vap. 

#### How can we reproduce it (as minimally and precisely as possible)?

A example of failed unit test https://github.com/open-policy-agent/gatekeeper/actions/runs/8070764571/job/22048871826?pr=3289 the test tries to create a custom resource after the vap resource is created and this new resource is blocked because it thinks the custom type referenced by paramKind does not exist

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
1.29
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124234 CEL cost budget exceeded when using messageExpression

- Issue é“¾æ¥ï¼š[#124234](https://github.com/kubernetes/kubernetes/issues/124234)

### Issue å†…å®¹

#### What happened?

I have a CRD with the following property within the top level spec schema:
```
config:
  type: object
  x-kubernetes-validations:
    - messageExpression: '''invalid attempts: '' + string(self.attempts)'
      rule: self.attempts >= 0
  properties:
    attempts:
      type: integer
      maximum: 2147483647
      minimum: -2147483648
```

The API server rejects the CRD with the following error:
```
* spec.validation.openAPIV3Schema.properties[spec].properties[config].x-kubernetes-validations[0].messageExpression: Forbidden: estimated messageExpression cost exceeds budget by factor of more than 100x (try simplifying the rule, or adding maxItems, maxProperties, and maxLength where arrays, maps, and strings are declared)
* spec.validation.openAPIV3Schema.properties[spec].properties[config].x-kubernetes-validations[0].messageExpression: Forbidden: contributed to estimated rule cost total exceeding cost limit for entire OpenAPIv3 schema
* spec.validation.openAPIV3Schema: Forbidden: x-kubernetes-validations estimated rule cost total for entire OpenAPIv3 schema exceeds budget by factor of more than 100x (try simplifying the rule, or adding maxItems, maxProperties, and maxLength where arrays, maps, and strings are declared)
```

This entire CRD contains a single CEL rule, and this is only a problem when using `messageExpression` in the validation rule. 

#### What did you expect to happen?

I would expect `string(intValue)` to not cause the cost budget of the validation rule to be exceeded.

#### How can we reproduce it (as minimally and precisely as possible)?

Use a similar schema as shown in the example.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.1", GitCommit:"4c9411232e10168d7b050c49a1b59f6df9d7ea4b", GitTreeState:"clean", BuildDate:"2023-04-14T13:21:19Z", GoVersion:"go1.20.3", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"29", GitVersion:"v1.29.2", GitCommit:"4b8e819355d791d96b7e9d9efe4cbafae2311c88", GitTreeState:"clean", BuildDate:"2024-02-14T22:24:00Z", GoVersion:"go1.21.7", Compiler:"gc", Platform:"linux/amd64"}
WARNING: version difference between client (1.27) and server (1.29) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

Kind cluster


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124233 Deployment does not apply multiplexed ports correctly to my Pods

- Issue é“¾æ¥ï¼š[#124233](https://github.com/kubernetes/kubernetes/issues/124233)

### Issue å†…å®¹

#### What happened?

I'm running a DNS server (dnsmasq) Deployment, which is exposed to the internet via a Traefik Reverse Proxy. The DNS protocol requires me to multiplex different protocols on the same port, as DNS queries can come in with either TCP or UDP protocol.

On both the dnsmasq and the Traefik deployments, I've run into the issue that the K8s API "swallows" one of the muliplexed ports, effectively rendering the DNS server useless (as either TCP or UDP traffic can't reach the Pod). 

To make it short: 
When I apply this port configuration to a container:
```
ports:
  - containerPort: 1053
    name: udp
    protocol: UDP
  - containerPort: 1053
    name: tcp
    protocol: TCP
```

only one of both ports is actually applied (and it seems to be random which of those ports is applied).

**The issue is not reproducible 100% of the time! In my short study, it occured about every 5th time I tried. The exact conditions that must be met to reproduce this issue are unknown to me.**

My current workaround is to manually add the missing port by editing the Deployment with kubectl/ArgoCD after it was initially applied. This has consistently resolved the issue, but is not a viable long-term solution for me.

#### What did you expect to happen?

I expected that both protocols are usable on the same port after the first `kubectl apply`.

#### How can we reproduce it (as minimally and precisely as possible)?

Keep in mind that this issue is occuring randomly, and you might need to try a few times until you are able to reproduce it.
You can reproduce the issue by `kubectl apply`ing this Deployment manifest:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: multiplexed-port
  namespace: multiplexed-port
spec:
  replicas: 1
  selector:
    matchLabels:
      app: multiplexed-port
  template:
    metadata:
      labels:
        app: multiplexed-port
    spec:
      containers:
        - name: multiplexed-port
          image: busybox:latest
          command: ["sh", "-c", "echo 'Hello, World!' && sleep infinity"]
          ports:
            - containerPort: 1053
              name: udp
              protocol: UDP
            - containerPort: 1053
              name: tcp
              protocol: TCP
```
After the manifest was applied to the cluster, run `kubectl describe` to see that only one of both ports was applied.

I did not test the issue with a naked Pod or other pod controllers like `DaemonSet` or `StatefulSet`.


#### Anything else we need to know?

I'm using ArgoCD (currently v2.9.6) to manage my Deployments. When one of the multiplexed ports is not correctly applied, ArgoCD does *not* show an OutOfSync status for my Deployment, indicating that the K8s API does not work correctly here.
The `kubectl.kubernetes.io/last-applied-configuration` annotation also shows both ports have been applied, while only one is displayed when I run `kubectl describe`.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.27.9
WARNING: version difference between client (1.29) and server (1.27) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
We're using an AKS cluster with AzureLinux.
</details>


#### OS version

<details>

I'm using MacOS Sonoma (14.4).
The cluster is always running on the latest version of AzureLinux.

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124230 Mounted volume size in pod is smaller than size declared in PVC.

- Issue é“¾æ¥ï¼š[#124230](https://github.com/kubernetes/kubernetes/issues/124230)

### Issue å†…å®¹

#### What happened?

Hello,

Mounted volume size in pod is smaller than size declared in PVC.
I don't know if this is the usual behavior or it is a bug.

Thanks for your help

#### What did you expect to happen?

I expect when I exec it the container and I hit the command df -h to see 20Gi for the path /mypath/ but I only see 4Gi.

#### How can we reproduce it (as minimally and precisely as possible)?


```
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: ex1
  namespace: ns1
spec:
  accessModes:
  - ReadWriteMany
  resources:
    requests:
      storage: 20Gi
  storageClassName: ibmc-s3fs-standard-perf-regional
  volumeMode: Filesystem
```
---
```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test
  namespace: ns01
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      volumeMode:
      - name: vol
        PersistentVolumeClaim:
          claimName: ex1
      containers:
      #
      #
        volumeMounts:
          - mountPath: /mypath/
            name: vol
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Client Version : 1.11
Server Version : 1.26
</details>


#### Cloud provider

<details>
IBM Cloud
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```
Ubuntu 20
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124224 Race condition in PV controller storeObjectUpdate()

- Issue é“¾æ¥ï¼š[#124224](https://github.com/kubernetes/kubernetes/issues/124224)

### Issue å†…å®¹

#### What happened?

PV controller (in KCM) has its own cache in addition to the informer cache, to store updates from itself. However, we've identified 2 possible race condition when invoking `storeObjectUpdate()` from multiple goroutines.

`storeObjectUpdate()` is currently invoked from `setClaimProvisioner()` and `syncUnboundClaim()`, which runs in 2 different goroutine. And I would expect more concurrent access for enhanced throughput in the future.

Race conditions:
1. internal update overwrites external delete. When a object is deleted from APIServer, an update request from controller itself may still in-flight. After the update returns, it tries to insert the object back to the cache, overwriting deletion propagated from informer. Resulting in the object staying in cache forever. Subsequently causing, for example, PV stay in Bound state forever and cannot be released.
2. read-compare-write in `storeObjectUpdate()`. When invoked concurrently, an old version (defined by ResourceVersion) may overwrite new version. Resulting in 15s cache stall (should be fixed by next sync).

#### What did you expect to happen?

No race conditions. Internal cache stays consistent on concurrent access.

#### How can we reproduce it (as minimally and precisely as possible)?

Hard to reproduce, this only happens with a small chance.

We observed condition 1 when we try to optimize the throughput of PV controller.

#### Anything else we need to know?

To fix race condition 2, I propose: add locks around `storeObjectUpdate()`

To fix race condition 1, I propose: explicitly differentiate update and add event, do not allow add for internal update response.

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124209 SCTP Server on agnhost image crashes when a client connects from the same pod

- Issue é“¾æ¥ï¼š[#124209](https://github.com/kubernetes/kubernetes/issues/124209)

### Issue å†…å®¹


How to repro:
1. Run an agnhost pod listening on sctp
```
spec:
  containers:
  - command:
    - /agnhost
    - netexec
    - --sctp-port
    - "8080"
    image: registry.k8s.io/e2e-test-images/agnhost:2.39
```

2. Exec into the pod and use the connect command to probe against the same pod
```
/agnhost connect --timeout=2s --protocol=sctp localhost:8080
bash-5.0# FATA[0014] execing command in container: command terminated with exit code 137
```

The stack trace 


```
I0406 18:21:17.102027       1 log.go:195] Started HTTP server on port 8080
I0406 18:21:17.102399       1 log.go:195] Started UDP server on port  8081
I0406 18:21:17.102465       1 log.go:195] Started SCTP server
I0406 18:22:07.943777       1 log.go:195] SCTP server exited
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x1 addr=0x20 pc=0x15097bb]

goroutine 77 [running]:
k8s.io/kubernetes/test/images/agnhost/netexec.startSCTPServer(0x0?)
        /go/src/k8s.io/kubernetes/test/images/agnhost/netexec/netexec.go:696 +0x3fb
created by k8s.io/kubernetes/test/images/agnhost/netexec.main
        /go/src/k8s.io/kubernetes/test/images/agnhost/netexec/netexec.go:217 +0x22f
root@kind-worker:/#

```

that points to 

https://github.com/kubernetes/kubernetes/blob/f8930f980d2986f9e486b04c14c3e93e57bdbe12/test/images/agnhost/netexec/netexec.go#L696

and this is from the vendored library vendor/github.com/ishidawataru/sctp/sctp.go , 

/kind bug
/sig network 
/help


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124208 2 sets of containers show after updating the deployment yaml.

- Issue é“¾æ¥ï¼š[#124208](https://github.com/kubernetes/kubernetes/issues/124208)

### Issue å†…å®¹

#### What happened?

There are 2 sets containers for a deployment. After remove a deployment, containers related to that deployment is not being controlled. And I cannot remove them.

#### What did you expect to happen?

The old version containers are deleted when I apply new configurations yaml. And containers with new version image start to work.

#### How can we reproduce it (as minimally and precisely as possible)?

I deployed deployment services. And I make another docker image and put them on my workers. I change the  image name in the deployment yaml file. And use kubectl apply to update the resources. The old version containers do not disappear but new version containers were created. When I delete the deployment, the containers created by  new version image are still running. Delete deployment again it showed me "Error from server (NotFound): deployments.apps "picdetecter-deployment" not found."

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.3

```

</details>


#### Cloud provider

<details>
oracle
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.3
ubuntu@master:~/k8s$ ^C
ubuntu@master:~/k8s$ ^C
ubuntu@master:~/k8s$ ^C
ubuntu@master:~/k8s$  cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

$ uname -a
Linux master 5.15.0-1051-oracle #57-Ubuntu SMP Wed Jan 24 18:29:02 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>
ordinary installation.
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Docker version 26.0.0, build 2ae903e
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
weavenet.
</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124207 Endpoint selected by multiple headless services only has 1 dns hostname

- Issue é“¾æ¥ï¼š[#124207](https://github.com/kubernetes/kubernetes/issues/124207)

### Issue å†…å®¹

#### What happened?

A pod in multiple endpoint slices selected by a headless service only resolves the hostname for one of the services.

Per the [spec](https://github.com/kubernetes/dns/blob/master/docs/specification.md#241---aaaaa-records):
> There must be an A record for each ready endpoint of the headless Service with IPv4 address <endpoint-ip> as shown below.

So I feel like I'm missing something?

#### What did you expect to happen?

Both names resolve (maybe?)

#### How can we reproduce it (as minimally and precisely as possible)?

- create 2x statefusets.
- add a shared selector to both
- add a unique selector to each
- create 3x headless services: one for each selector

now things get interesting:
- the shared service will show all endpoints from both pods, and the endpoint slices will show they as ready and serving
- DNS will only resolve for whatever you said SS serviceName to (perhaps deliberately?)
- SS subdomain is overwritten to match serviceName if specified?

end result is only `second-pod.second-service-name.ns.svc.cluster.local` resolves to the pod IP, even though `second-pod.shared-service.ns.svc.cluster.local` should too?

my read is that is due to this logic:
https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/endpointslice/util/controller_utils.go#L113-L117
https://github.com/coredns/coredns/blob/e3f83cb1fabb9b1cbaffb9df3c4b65476e92c39b/plugin/kubernetes/kubernetes.go#L476-L480

so who is wrong?

#### Anything else we need to know?

```bash
$ kubectl apply -f repro.yaml

# when using serviceName: shared
$ k exec -it first-0 -- dig second-0.shared.default.svc.cluster.local

; <<>> DiG 9.18.21 <<>> second-0.shared.default.svc.cluster.local
;; global options: +cmd
;; Got answer:
;; WARNING: .local is reserved for Multicast DNS
;; You are currently testing what happens when an mDNS query is leaked to DNS
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 10827
;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1
;; WARNING: recursion requested but not available

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 1232
; COOKIE: 5b180353a44f56c4 (echoed)
;; QUESTION SECTION:
;second-0.shared.default.svc.cluster.local. IN A

;; ANSWER SECTION:
second-0.shared.default.svc.cluster.local. 5 IN	A 10.7.225.77

;; Query time: 0 msec
;; SERVER: 10.16.0.10#53(10.16.0.10) (UDP)
;; WHEN: Sat Apr 06 15:01:31 UTC 2024
;; MSG SIZE  rcvd: 139
```

```bash
# same. just highlighting the 2nd subdomain doesn't resolve. perhaps expected
$ k exec -it first-0 -- dig second-0.second.default.svc.cluster.local

; <<>> DiG 9.18.21 <<>> second-0.second.default.svc.cluster.local
;; global options: +cmd
;; Got answer:
;; WARNING: .local is reserved for Multicast DNS
;; You are currently testing what happens when an mDNS query is leaked to DNS
;; ->>HEADER<<- opcode: QUERY, status: NXDOMAIN, id: 39439
;; flags: qr aa rd; QUERY: 1, ANSWER: 0, AUTHORITY: 1, ADDITIONAL: 1
;; WARNING: recursion requested but not available

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 1232
; COOKIE: d8f070b19ee2c55e (echoed)
;; QUESTION SECTION:
;second-0.second.default.svc.cluster.local. IN A

;; AUTHORITY SECTION:
cluster.local.		5	IN	SOA	ns.dns.cluster.local. hostmaster.cluster.local. 1712413987 7200 1800 86400 5

;; Query time: 0 msec
;; SERVER: 10.16.0.10#53(10.16.0.10) (UDP)
;; WHEN: Sat Apr 06 15:01:27 UTC 2024
;; MSG SIZE  rcvd: 175

```

```bash
# fine
$ k exec -it first-0 -- dig first-0.first.default.svc.cluster.local

; <<>> DiG 9.18.21 <<>> first-0.first.default.svc.cluster.local
;; global options: +cmd
;; Got answer:
;; WARNING: .local is reserved for Multicast DNS
;; You are currently testing what happens when an mDNS query is leaked to DNS
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 28555
;; flags: qr aa rd; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1
;; WARNING: recursion requested but not available

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 1232
; COOKIE: 474b9b921327388d (echoed)
;; QUESTION SECTION:
;first-0.first.default.svc.cluster.local. IN A

;; ANSWER SECTION:
first-0.first.default.svc.cluster.local. 5 IN A	10.7.225.73

;; Query time: 0 msec
;; SERVER: 10.16.0.10#53(10.16.0.10) (UDP)
;; WHEN: Sat Apr 06 15:01:39 UTC 2024
;; MSG SIZE  rcvd: 135
```

interestingly, on the endpoints with issue, I see in endpointslice yaml they don't have the hostname due to the k8s code above

<details>

<summary>long bash outputs of dig and endpoint slices full yaml output </summary>

```bash
$ k get ep -o yaml first second shared
apiVersion: v1
items:
- apiVersion: v1
  kind: Endpoints
  metadata:
    annotations:
      endpoints.kubernetes.io/last-change-trigger-time: "2024-04-06T14:31:49Z"
    creationTimestamp: "2024-04-06T14:27:11Z"
    labels:
      service.kubernetes.io/headless: ""
    name: first
    namespace: default
    resourceVersion: "4546341"
    uid: 884b42c2-1a3c-4f90-95e4-f82a8383c8f9
  subsets:
  - addresses:
    - hostname: first-0
      ip: 10.7.225.73
      nodeName: g1cb6d2
      targetRef:
        kind: Pod
        name: first-0
        namespace: default
        uid: 59d87cd3-5594-4ad8-b273-0969cd6c8d14
    ports:
    - name: rest
      port: 8501
      protocol: TCP
- apiVersion: v1
  kind: Endpoints
  metadata:
    annotations:
      endpoints.kubernetes.io/last-change-trigger-time: "2024-04-06T14:33:06Z"
    creationTimestamp: "2024-04-06T14:30:43Z"
    labels:
      service.kubernetes.io/headless: ""
    name: second
    namespace: default
    resourceVersion: "4546684"
    uid: 314a8f21-9133-4b6d-8f5e-5b25907cbf1a
  subsets:
  - addresses:
    - ip: 10.7.225.77
      nodeName: g1cb6d2
      targetRef:
        kind: Pod
        name: second-0
        namespace: default
        uid: 801102ab-874f-4ceb-ba5a-e49e34274c4d
    ports:
    - name: rest
      port: 8501
      protocol: TCP
- apiVersion: v1
  kind: Endpoints
  metadata:
    annotations:
      endpoints.kubernetes.io/last-change-trigger-time: "2024-04-06T14:33:06Z"
    creationTimestamp: "2024-04-06T14:27:11Z"
    labels:
      service.kubernetes.io/headless: ""
    name: shared
    namespace: default
    resourceVersion: "4546685"
    uid: 099d86cb-4418-44e7-b7d9-8fdb1ef0b3fb
  subsets:
  - addresses:
    - ip: 10.7.225.73
      nodeName: g1cb6d2
      targetRef:
        kind: Pod
        name: first-0
        namespace: default
        uid: 59d87cd3-5594-4ad8-b273-0969cd6c8d14
    - hostname: second-0
      ip: 10.7.225.77
      nodeName: g1cb6d2
      targetRef:
        kind: Pod
        name: second-0
        namespace: default
        uid: 801102ab-874f-4ceb-ba5a-e49e34274c4d
    ports:
    - name: rest
      port: 8080
      protocol: TCP
kind: List
metadata:
  resourceVersion: ""

$ k get endpointslice | rg '(first|second|shared)'
first-lr9rb                       IPv4          8501      10.7.225.73               21m
second-fhvbk                      IPv4          8501      10.7.225.77               17m
shared-4ph6n                      IPv4          8080      10.7.225.73,10.7.225.77   21m

$ k get endpointslice -o yaml first-lr9rb second-fhvbk shared-4ph6n
apiVersion: v1
items:
- addressType: IPv4
  apiVersion: discovery.k8s.io/v1
  endpoints:
  - addresses:
    - 10.7.225.73
    conditions:
      ready: true
      serving: true
      terminating: false
    hostname: first-0
    nodeName: g1cb6d2
    targetRef:
      kind: Pod
      name: first-0
      namespace: default
      uid: 59d87cd3-5594-4ad8-b273-0969cd6c8d14
    zone: "208"
  kind: EndpointSlice
  metadata:
    annotations:
      endpoints.kubernetes.io/last-change-trigger-time: "2024-04-06T14:31:49Z"
    creationTimestamp: "2024-04-06T14:27:11Z"
    generateName: first-
    generation: 6
    labels:
      endpointslice.kubernetes.io/managed-by: endpointslice-controller.k8s.io
      kubernetes.io/service-name: first
      service.kubernetes.io/headless: ""
    name: first-lr9rb
    namespace: default
    ownerReferences:
    - apiVersion: v1
      blockOwnerDeletion: true
      controller: true
      kind: Service
      name: first
      uid: 7d50bcea-2070-4484-a049-b63f8b67ff7f
    resourceVersion: "4546340"
    uid: b2da77c4-fa28-4be2-a562-e9d19cc0d849
  ports:
  - name: rest
    port: 8501
    protocol: TCP
- addressType: IPv4
  apiVersion: discovery.k8s.io/v1
  endpoints:
  - addresses:
    - 10.7.225.77
    conditions:
      ready: true
      serving: true
      terminating: false
    nodeName: g1cb6d2
    targetRef:
      kind: Pod
      name: second-0
      namespace: default
      uid: 801102ab-874f-4ceb-ba5a-e49e34274c4d
    zone: "208"
  kind: EndpointSlice
  metadata:
    annotations:
      endpoints.kubernetes.io/last-change-trigger-time: "2024-04-06T14:33:06Z"
    creationTimestamp: "2024-04-06T14:30:43Z"
    generateName: second-
    generation: 16
    labels:
      endpointslice.kubernetes.io/managed-by: endpointslice-controller.k8s.io
      kubernetes.io/service-name: second
      service.kubernetes.io/headless: ""
    name: second-fhvbk
    namespace: default
    ownerReferences:
    - apiVersion: v1
      blockOwnerDeletion: true
      controller: true
      kind: Service
      name: second
      uid: 28d521bc-1468-4e6a-a156-6f927494aead
    resourceVersion: "4546686"
    uid: 483b27b1-ba59-4ac2-9940-a465f43b8fc2
  ports:
  - name: rest
    port: 8501
    protocol: TCP
- addressType: IPv4
  apiVersion: discovery.k8s.io/v1
  endpoints:
  - addresses:
    - 10.7.225.73
    conditions:
      ready: true
      serving: true
      terminating: false
    nodeName: g1cb6d2
    targetRef:
      kind: Pod
      name: first-0
      namespace: default
      uid: 59d87cd3-5594-4ad8-b273-0969cd6c8d14
    zone: "208"
  - addresses:
    - 10.7.225.77
    conditions:
      ready: true
      serving: true
      terminating: false
    hostname: second-0
    nodeName: g1cb6d2
    targetRef:
      kind: Pod
      name: second-0
      namespace: default
      uid: 801102ab-874f-4ceb-ba5a-e49e34274c4d
    zone: "208"
  kind: EndpointSlice
  metadata:
    annotations:
      endpoints.kubernetes.io/last-change-trigger-time: "2024-04-06T14:33:06Z"
    creationTimestamp: "2024-04-06T14:27:11Z"
    generateName: shared-
    generation: 23
    labels:
      endpointslice.kubernetes.io/managed-by: endpointslice-controller.k8s.io
      kubernetes.io/service-name: shared
      service.kubernetes.io/headless: ""
    name: shared-4ph6n
    namespace: default
    ownerReferences:
    - apiVersion: v1
      blockOwnerDeletion: true
      controller: true
      kind: Service
      name: shared
      uid: a3c5d040-1905-447c-a457-6b72ba26f0b7
    resourceVersion: "4546683"
    uid: 0110bd21-b87c-4380-83bb-0c89f13098c2
  ports:
  - name: rest
    port: 8080
    protocol: TCP
kind: List
metadata:
  resourceVersion: ""
```

</details>


<details>
<summary>yamls</summary>

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    unique: &name first
    shared: shared
  name: *name
spec:
  persistentVolumeClaimRetentionPolicy:
    whenDeleted: Retain
    whenScaled: Retain
  podManagementPolicy: Parallel
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      unique: *name
  serviceName: *name
  template:
    metadata:
      labels:
        unique: *name
        shared: shared
    spec:
      containers:
      - command: ["sleep", "infinity"]
        image: docker.io/nicolaka/netshoot@sha256:b569665f0c32391b93f4de344f07bf6353ddff9d8c801ac3318d996db848a64c
        imagePullPolicy: IfNotPresent
        name: server
        ports:
        - containerPort: 8080
          name: rest
          protocol: TCP
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      subdomain: *name
      terminationGracePeriodSeconds: 1
      tolerations:
      - effect: NoSchedule
        key: is_cpu_compute
        operator: Exists
  updateStrategy:
    rollingUpdate:
      partition: 0
    type: RollingUpdate
---
apiVersion: v1
kind: Service
metadata:
  name: &name first
spec:
  clusterIP: None
  clusterIPs:
  - None
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: rest
    port: 8501
    protocol: TCP
    targetPort: 8501
  selector:
    unique: *name
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  labels:
    unique: &name second
    shared: shared
  name: *name
spec:
  persistentVolumeClaimRetentionPolicy:
    whenDeleted: Retain
    whenScaled: Retain
  podManagementPolicy: Parallel
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      unique: *name
  serviceName: shared
  template:
    metadata:
      labels:
        unique: *name
        shared: shared
    spec:
      containers:
      - command: ["sleep", "infinity"]
        image: docker.io/nicolaka/netshoot@sha256:b569665f0c32391b93f4de344f07bf6353ddff9d8c801ac3318d996db848a64c
        imagePullPolicy: IfNotPresent
        name: server
        ports:
        - containerPort: 8080
          name: rest
          protocol: TCP
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      subdomain: shared # <--- important to repro
      terminationGracePeriodSeconds: 1
      tolerations:
      - effect: NoSchedule
        key: is_cpu_compute
        operator: Exists
  updateStrategy:
    rollingUpdate:
      partition: 0
    type: RollingUpdate
---
apiVersion: v1
kind: Service
metadata:
  name: &name second
spec:
  clusterIP: None
  clusterIPs:
  - None
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: rest
    port: 8501
    protocol: TCP
    targetPort: 8501
  selector:
    unique: *name
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: v1
kind: Service
metadata:
  name: &name shared # <-- this one will have issues
spec:
  clusterIP: None
  clusterIPs:
  - None
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: rest
    port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    shared: *name
  sessionAffinity: None
  type: ClusterIP
---
```

</details>


#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.26.13
WARNING: version difference between client (1.29) and server (1.26) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>
tested across 3x providers so far with same behavior, multiple k8s versions but not latest master
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
...coredns?
</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124204 invalid bearer token, service account token has been invalidated

- Issue é“¾æ¥ï¼š[#124204](https://github.com/kubernetes/kubernetes/issues/124204)

### Issue å†…å®¹

#### What happened?

My K8s cluster kube apiserver has a large number of invalid bearer tokens, and the service account token has been invalidated
Phenomenon:
1. All are concentrated on one kube apiserver node
2. I did not find any functional damage in the cluster, including related business Pods
![image](https://github.com/kubernetes/kubernetes/assets/20591674/791b6289-8fec-4c19-a9d8-55794ff19dc2)

#### What did you expect to happen?

How do I know where this part of the request comes from and how do I handle it

#### How can we reproduce it (as minimally and precisely as possible)?

no

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
Server Version: version.Info{Major:"1", Minor:"25", GitVersion:"v1.25.6", GitCommit:"ff2c119726cc1f8926fb0585c74b25921e866a28", GitTreeState:"clean", BuildDate:"2023-01-18T19:15:26Z", GoVersion:"go1.19.5", Compiler:"gc", Platform:"linux/amd64"}
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```
Linux 5.4.86-1.el7.elrepo.x86_64 #1 SMP Tue Dec 29 10:39:46 EST 2020 x86_64 x86_64 x86_64 GNU/Linux
</details>


#### Install tools

<details>
kubeasz
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd github.com/containerd/containerd v1.6.19
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124201 api: v1.KubeSchedulerConfiguration Go type is not compatible with controller-gen

- Issue é“¾æ¥ï¼š[#124201](https://github.com/kubernetes/kubernetes/issues/124201)

### Issue å†…å®¹

## What would you like to be added?

I am using the `kubescheduler.config.k8s.io/v1.KubeSchedulerConfiguration` type as a field in my CustomResourceDefinition. However, when I try to generate CRD manifests from the Go type for the CRD using the command I get various errors.

## Why is this needed

I'm curious if this use case is supported. It can help projects like kubeadm, [Cluster API](https://cluster-api.sigs.k8s.io/) or [Kamaji](https://kamaji.clastix.io/) accept KubeSchedulerConfiguration objects as a field to customize the provisioned cluster's kube-scheduler config differently between clusters. The only other option to do so is to either deal with `runtime.RawExtensions` or use a YAML/JSON stored as `string`.

## Errors from `controller-gen crd`

When I use the `controller-gen crd` command to generate CustomResourceDefinition manifests for a Go type that has a struct member of type KubeSchedulerConfiguration, I get errors about float types (presumably QPS/Burst) in the `componentbaseconfigv1alpha1.ClientConnectionConfiguration`:

> `
/Users/abalkan/go-athens/pkg/mod/k8s.io/component-base@v0.29.1/config/v1alpha1/types.go:79:6: found float, the usage of which is highly discouraged, as support for them varies across languages. Please consider serializing your float as string instead. If you are really sure you want to use them, re-run with crd:allowDangerousTypes=true`

And a few errors about various [[]byte fields](https://github.com/kubernetes/kube-scheduler/blob/94ff27f8b9eb26561417aa148314397395e6faef/config/v1/types.go#L385-L396) that have `+listType=atomic`:

> `/Users/abalkan/go-athens/pkg/mod/k8s.io/kube-scheduler@v0.29.1/config/v1/types.go:388:2: must apply listType to an array, found string`

> `/Users/abalkan/go-athens/pkg/mod/k8s.io/kube-scheduler@v0.29.1/config/v1/types.go:392:2: must apply listType to an array, found string`

> `/Users/abalkan/go-athens/pkg/mod/k8s.io/kube-scheduler@v0.29.1/config/v1/types.go:396:2: must apply listType to an array, found string`

/sig scheduling
/sig api-machinery
/kind bug

### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124192 Import cycle while generating configuration with applyconfig-gen

- Issue é“¾æ¥ï¼š[#124192](https://github.com/kubernetes/kubernetes/issues/124192)

### Issue å†…å®¹

#### What happened?

`applyconfig-gen` code generator creates import cycle in case of multi-token group names (e.g. resource.io).

#### What did you expect to happen?

It should generate configuration files with valid go syntax.

#### How can we reproduce it (as minimally and precisely as possible)?

Checkout https://github.com/mtrqq/applygen-cycle-repro/tree/main for detailed reproduction. 

Generally the scenario should look like this - create a custom resource API with 1 resource referencing another one and name which includes as least 1 dot (like resource.io). Run `applyconfig-gen` on this configuration, as the result - we are getting import cycle in one of the configuration files

#### Anything else we need to know?

_No response_

#### Kubernetes version

https://github.com/kubernetes/kubernetes/commit/d9c54f69d4bb7ae1bb655e1a2a50297d615025b5

OR

https://github.com/kubernetes/code-generator/commit/670586590c4574bdb99793d8373d8e4431ee30db

#### Cloud provider

NONE

#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124188 kube-aggregator proxyHandler does not set traceparent

- Issue é“¾æ¥ï¼š[#124188](https://github.com/kubernetes/kubernetes/issues/124188)

### Issue å†…å®¹

#### What happened?

Extension API servers do not currently receive the `traceparent` header when requests are proxied through kube-aggregator, so spans are not linked properly.

#### What did you expect to happen?

Traces from the extension API sever should be linked to the parent in kube-aggregator.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Configure tracing for kube-apiserver
2. Configure tracing for sample-apiserver and aggregate it to kube-apiserver
3. Make a request to kube-apiserver for one of the resources from sample-apiserver
4. Relevant sample-apiserver traces will not be linked to the kube-apiserver traces

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

This problem currently exists on `master`.

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
Linux work 6.5.0-26-generic #26-Ubuntu SMP PREEMPT_DYNAMIC Tue Mar  5 21:19:28 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124176 ephemeral containers were repeatedly created and had inconsistent status after being manually deleted

- Issue é“¾æ¥ï¼š[#124176](https://github.com/kubernetes/kubernetes/issues/124176)

### Issue å†…å®¹

#### What happened?

ephemeral containers were repeatedly created and had inconsistent status after being manually deleted

#### What did you expect to happen?

https://github.com/kubernetes/kubernetes/blob/d9c54f69d4bb7ae1bb655e1a2a50297d615025b5/pkg/kubelet/kuberuntime/kuberuntime_manager.go#L893-L897

ebpemeral containers should never be restarted

#### How can we reproduce it (as minimally and precisely as possible)?

created a demo pod like this:
```
apiVersion: v1
kind: Pod
metadata:
  name: ephemeral-pod
spec:
  containers:
  - name: process
    image: nginx:latest
    ports:
    - containerPort: 80
    resources:
      requests:
        cpu: 0.1
        memory: 20Mi
      limits:
        cpu: 0.1
        memory: 20Mi
  # ephemeralContainers:
  # - name: debugger
  #   image: busybox
  #   command:
  #   - sleep
  #   - "1000"
```
kubectl apply -f ephemeral-pod.yaml
and then debug ephemeral-pod like this:
```
  # ephemeralContainers:
  # - name: debug
  #   image: busybox
  #   command:
  #   - sleep
  #   - "1000"
```

everything is normal now, the epemeral container is created and runs normally.
now go to the machine and stop the epemeral container:
```
crictl stop 9c684c6c31e2b
crictl rm 9c684c6c31e2b
```

After deletion, the debugger ephemeral container is created again, but the status is inconsistent with the ephemeral container in the pod.
![image](https://github.com/kubernetes/kubernetes/assets/30427474/a192142a-86fb-4299-be94-5af2247f4edc)

Delete again:
![image](https://github.com/kubernetes/kubernetes/assets/30427474/25fb5c94-d0d0-4a3f-8731-921bd0c7b879)

kubelet log:
![image](https://github.com/kubernetes/kubernetes/assets/30427474/8cd63376-a17c-4697-86a8-7121307c1034)
```
Apr 04 07:54:11 minikube kubelet[25225]: I0404 07:54:11.953549   25225 status_manager.go:877] "Failed to update status for pod" pod="default/ephemeral-pod" err="failed to patch status \"{\\\"metadata\\\":{\\\"uid\\\":\\\"841d260d-5737-4b84-a695-824d5ecb8918\\\"},\\\"status\\\":{\\\"ephemeralContainerStatuses\\\":[{\\\"containerID\\\":\\\"containerd://38843a8238d3b6d5aa781cf8ebb2201e956592d38e3fb7cfe2f319ef9b67260b\\\",\\\"image\\\":\\\"docker.io/library/busybox:latest\\\",\\\"imageID\\\":\\\"docker.io/library/busybox@sha256:c3839dd800b9eb7603340509769c43e146a74c63dca3045a8e7dc8ee07e53966\\\",\\\"lastState\\\":{},\\\"name\\\":\\\"debugger\\\",\\\"ready\\\":false,\\\"restartCount\\\":3,\\\"state\\\":{\\\"running\\\":{\\\"startedAt\\\":\\\"2024-04-04T07:53:01Z\\\"}}}]}}\" for pod \"default\"/\"ephemeral-pod\": Pod \"ephemeral-pod\" is invalid: status.ephemeralContainerStatuses[0].state: Forbidden: may not be transitioned to non-terminated state"
Apr 04 07:54:21 minikube kubelet[25225]: I0404 07:54:21.951540   25225 status_manager.go:877] "Failed to update status for pod" pod="default/ephemeral-pod" err="failed to patch status \"{\\\"metadata\\\":{\\\"uid\\\":\\\"841d260d-5737-4b84-a695-824d5ecb8918\\\"},\\\"status\\\":{\\\"ephemeralContainerStatuses\\\":[{\\\"containerID\\\":\\\"containerd://38843a8238d3b6d5aa781cf8ebb2201e956592d38e3fb7cfe2f319ef9b67260b\\\",\\\"image\\\":\\\"docker.io/library/busybox:latest\\\",\\\"imageID\\\":\\\"docker.io/library/busybox@sha256:c3839dd800b9eb7603340509769c43e146a74c63dca3045a8e7dc8ee07e53966\\\",\\\"lastState\\\":{},\\\"name\\\":\\\"debugger\\\",\\\"ready\\\":false,\\\"restartCount\\\":3,\\\"state\\\":{\\\"running\\\":{\\\"startedAt\\\":\\\"2024-04-04T07:53:01Z\\\"}}}]}}\" for pod \"default\"/\"ephemeral-pod\": Pod \"ephemeral-pod\" is invalid: status.ephemeralContainerStatuses[0].state: Forbidden: may not be transitioned to non-terminated state"
Apr 04 07:54:31 minikube kubelet[25225]: I0404 07:54:31.955727   25225 status_manager.go:877] "Failed to update status for pod" pod="default/ephemeral-pod" err="failed to patch status \"{\\\"metadata\\\":{\\\"uid\\\":\\\"841d260d-5737-4b84-a695-824d5ecb8918\\\"},\\\"status\\\":{\\\"ephemeralContainerStatuses\\\":[{\\\"containerID\\\":\\\"containerd://38843a8238d3b6d5aa781cf8ebb2201e956592d38e3fb7cfe2f319ef9b67260b\\\",\\\"image\\\":\\\"docker.io/library/busybox:latest\\\",\\\"imageID\\\":\\\"docker.io/library/busybox@sha256:c3839dd800b9eb7603340509769c43e146a74c63dca3045a8e7dc8ee07e53966\\\",\\\"lastState\\\":{},\\\"name\\\":\\\"debugger\\\",\\\"ready\\\":false,\\\"restartCount\\\":3,\\\"state\\\":{\\\"running\\\":{\\\"startedAt\\\":\\\"2024-04-04T07:53:01Z\\\"}}}]}}\" for pod \"default\"/\"ephemeral-pod\": Pod \"ephemeral-pod\" is invalid: status.ephemeralContainerStatuses[0].state: Forbidden: may not be transitioned to non-terminated state"
```


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.29.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.3
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
macos minikube
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124166 The setting "internalTrafficPolicy: Local" leads to an additional 1 second delay

- Issue é“¾æ¥ï¼š[#124166](https://github.com/kubernetes/kubernetes/issues/124166)

### Issue å†…å®¹

#### What happened?

In Kubernetes version 1.26.4, modifying the service parameter "internalTrafficPolicy: Local" results in a 1-second delay when accessing pod applications through the service.However, in Kubernetes version 1.25.5, it functions normally.

Kubernetes 1.26.4
![1712136070710](https://github.com/kubernetes/kubernetes/assets/74498996/fd7f0d69-cf00-4728-9e98-83aeea1f7009)

Kubernetes 1.25.5
![1712136118708](https://github.com/kubernetes/kubernetes/assets/74498996/a9219bae-c289-4d9f-a7a7-cf2f29d4b3af)


#### What did you expect to happen?

ã€‚

#### How can we reproduce it (as minimally and precisely as possible)?

ã€‚

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Kubernetes 1.26.4
Kubernetes 1.25.5
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠã€‚

---

## Issue #124164 failed to run `update-codegen.sh`

- Issue é“¾æ¥ï¼š[#124164](https://github.com/kubernetes/kubernetes/issues/124164)

### Issue å†…å®¹

#### What happened?

```
~# make update SILENT=false
Running in short-circuit mode; run with FORCE_ALL=true to force all scripts to run.
Running update-go-workspace
Running update-codegen
+++ [0403 16:52:58] Generating protobufs for 68 targets
unknown flag: --output-dir
Usage of go-to-protobuf:
      --add_dir_header                   If true, adds the file directory to the header of the log messages
      --alsologtostderr                  log to standard error as well as files (no effect when -logtostderr=true)
      --apimachinery-packages string     comma-separated list of directories to get apimachinery input types from which are needed by any API. Directories prefixed with '-' are not generated, directories prefixed with '+' only create types with explicit IDL instructions. (default "+k8s.io/apimachinery/pkg/util/intstr,+k8s.io/apimachinery/pkg/api/resource,+k8s.io/apimachinery/pkg/runtime/schema,+k8s.io/apimachinery/pkg/runtime,k8s.io/apimachinery/pkg/apis/meta/v1,k8s.io/apimachinery/pkg/apis/meta/v1beta1,k8s.io/apimachinery/pkg/apis/testapigroup/v1")
      --clean                            If true, remove all generated files for the specified Packages.
      --conditional string               An optional Golang build tag condition to add to the generated Go code
      --drop-embedded-fields string      Comma-delimited list of embedded Go types to omit from generated protobufs (default "k8s.io/apimachinery/pkg/apis/meta/v1.TypeMeta")
  -h, --go-header-file string            File containing boilerplate header text. The string YEAR will be replaced with the current 4-digit year.
      --keep-gogoproto                   If true, the generated IDL will contain gogoprotobuf extensions which are normally removed
      --log_backtrace_at traceLocation   when logging hits line file:N, emit a stack trace (default :0)
      --log_dir string                   If non-empty, write log files in this directory (no effect when -logtostderr=true)
      --log_file string                  If non-empty, use this log file (no effect when -logtostderr=true)
      --log_file_max_size uint           Defines the maximum size a log file can grow to (no effect when -logtostderr=true). Unit is megabytes. If the value is 0, the maximum file size is unlimited. (default 1800)
      --logtostderr                      log to standard error instead of files (default true)
      --one_output                       If true, only write logs to their native severity level (vs also writing to each lower severity level; no effect when -logtostderr=true)
      --only-idl                         If true, only generate the IDL for each package.
  -o, --output-base string               Output base; defaults to $GOPATH/src/ (default "/Users/cyclinder/Desktop/code/kubernetes/_output/local/go/src")
  -p, --packages string                  comma-separated list of directories to get input types from. Directories prefixed with '-' are not generated, directories prefixed with '+' only create types with explicit IDL instructions.
      --proto-import strings             The search path for the core protobuf .protos, required; defaults $GOPATH/src/k8s.io/kubernetes/vendor/github.com/gogo/protobuf/protobuf. (default [/Users/cyclinder/Desktop/code/kubernetes/_output/local/go/src/k8s.io/kubernetes/vendor/github.com/gogo/protobuf/protobuf])
      --skip-generated-rewrite           If true, skip fixing up the generated.pb.go file (debugging only).
      --skip_headers                     If true, avoid header prefixes in the log messages
      --skip_log_headers                 If true, avoid headers when opening log files (no effect when -logtostderr=true)
      --stderrthreshold severity         logs at or above this threshold go to stderr when writing to files and stderr (no effect when -logtostderr=true or -alsologtostderr=false) (default 2)
      --trim-path-prefix string          If set, trim the specified prefix from --output-package when generating files.
  -v, --v Level                          number for the log level verbosity (default 0)
      --vendor-output-base string        The vendor/ directory to look for packages in; defaults to $PWD/vendor/. (default "/Users/cyclinder/Desktop/code/kubernetes/vendor")
      --verify-only                      If true, only verify existing output, do not write anything.
      --vmodule moduleSpec               comma-separated list of pattern=N settings for file-filtered logging
unknown flag: --output-dir
!!! [0403 16:53:00] Call tree:
!!! [0403 16:53:00]  1: /Users/cyclinder/Desktop/code/kubernetes/hack/update-codegen.sh:885 codegen::protobuf(...)
Running update-codegen FAILED
make: *** [update] Error 1
~# protoc --version
libprotoc 23.4
```

#### What did you expect to happen?

`make update` works

#### How can we reproduce it (as minimally and precisely as possible)?

```
~# ./hack/install-protoc.sh
~# export PATH="/Users/cyclinder/Desktop/code/kubernetes/third_party/protoc:${PATH}"
~# make update SILENT=false
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124162 update-go-workspace.sh doesn't support for MacOS

- Issue é“¾æ¥ï¼š[#124162](https://github.com/kubernetes/kubernetes/issues/124162)

### Issue å†…å®¹

#### What happened?

```
make update
Running in silent mode, run with SILENT=false if you want to see script logs.
Running in short-circuit mode; run with FORCE_ALL=true to force all scripts to run.
Running update-go-workspace
dirname: illegal option -- z
usage: dirname path
```

#### What did you expect to happen?

`make update` can run on MacOS

#### How can we reproduce it (as minimally and precisely as possible)?

Run `make-update` on the MacOS

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
master

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124155 `hack/verify-file-sizes.sh` does not support Mac OS X

- Issue é“¾æ¥ï¼š[#124155](https://github.com/kubernetes/kubernetes/issues/124155)

### Issue å†…å®¹

#### What happened?

The `hack/verify-file-sizes.sh` script encounters an error when executed on Mac OS X because the `stat --printf=%s` option is not supported. The error message appears as follows:

```
ERROR: usage: stat [-FLnq] [-f format | -l | -r | -s | -x] [-t timefmt] [file ...]
```

#### What did you expect to happen?

I expected the `hack/verify-file-sizes.sh` script to execute successfully.

#### How can we reproduce it (as minimally and precisely as possible)?

Execute the `hack/verify-file-sizes.sh` command on macOS.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
$ uname -a
Darwin <name> 22.6.0 Darwin Kernel Version 22.6.0: Mon Feb 19 19:43:41 PST 2024; root:xnu-8796.141.3.704.6~1/RELEASE_ARM64_T8103 arm64
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

## Issue #124150 make test on darwin fails with error "build constraints exclude all Go files"

- Issue é“¾æ¥ï¼š[#124150](https://github.com/kubernetes/kubernetes/issues/124150)

### Issue å†…å®¹

#### What happened?

Tried running make test against local latest fork of kubernetes master

```
%  make test
+++ [0402 17:43:52] Set GOMAXPROCS automatically to 8
WARNING: ulimit -n (files) should be at least 1000, is 256, may cause test failure
+++ [0402 17:47:36] Running tests without code coverage and with -race
package k8s.io/kubernetes/pkg/proxy/iptables: build constraints exclude all Go files in /Users/karthikkn/karthik-k8-workspace/kubernetes/pkg/proxy/iptables
package k8s.io/kubernetes/pkg/proxy/ipvs/ipset: build constraints exclude all Go files in /Users/karthikkn/karthik-k8-workspace/kubernetes/pkg/proxy/ipvs/ipset
package k8s.io/kubernetes/pkg/proxy/ipvs/ipset/testing: build constraints exclude all Go files in /Users/karthikkn/karthik-k8-workspace/kubernetes/pkg/proxy/ipvs/ipset/testing
package k8s.io/kubernetes/pkg/proxy/ipvs/testing: build constraints exclude all Go files in /Users/karthikkn/karthik-k8-workspace/kubernetes/pkg/proxy/ipvs/testing
package k8s.io/kubernetes/pkg/proxy/ipvs/util: build constraints exclude all Go files in /Users/karthikkn/karthik-k8-workspace/kubernetes/pkg/proxy/ipvs/util
package k8s.io/kubernetes/pkg/proxy/ipvs/util/testing: build constraints exclude all Go files in /Users/karthikkn/karthik-k8-workspace/kubernetes/pkg/proxy/ipvs/util/testing
package k8s.io/kubernetes/pkg/proxy/nftables: build constraints exclude all Go files in /Users/karthikkn/karthik-k8-workspace/kubernetes/pkg/proxy/nftables
package k8s.io/kubernetes/pkg/util/iptables/testing: build constraints exclude all Go files in /Users/karthikkn/karthik-k8-workspace/kubernetes/pkg/util/iptables/testing
make: *** [test] Error 1

```

Go version
```
% go version
go version go1.22.0 darwin/amd64
```

Go env

```
% go env
GO111MODULE='on'
GOARCH='amd64'
GOBIN=''
GOCACHE='/Users/karthikkn/Library/Caches/go-build'
GOENV='/Users/karthikkn/Library/Application Support/go/env'
GOEXE=''
GOEXPERIMENT=''
GOFLAGS=''
GOHOSTARCH='amd64'
GOHOSTOS='darwin'
GOINSECURE=''
GOMODCACHE='/Users/karthikkn/go/pkg/mod'
GONOPROXY='github.com/PDeXchange'
GONOSUMDB='github.com/PDeXchange'
GOOS='darwin'
GOPATH='/Users/karthikkn/go'
GOPRIVATE='github.com/PDeXchange'
GOPROXY='https://proxy.golang.org,direct'
GOROOT='/Users/karthikkn/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.22.0.darwin-amd64'
GOSUMDB='sum.golang.org'
GOTMPDIR=''
GOTOOLCHAIN='auto'
GOTOOLDIR='/Users/karthikkn/go/pkg/mod/golang.org/toolchain@v0.0.1-go1.22.0.darwin-amd64/pkg/tool/darwin_amd64'
GOVCS=''
GOVERSION='go1.22.0'
GCCGO='gccgo'
GOAMD64='v1'
AR='ar'
CC='clang'
CXX='clang++'
CGO_ENABLED='1'
GOMOD='/Users/karthikkn/karthik-k8-workspace/kubernetes/go.mod'
GOWORK='/Users/karthikkn/karthik-k8-workspace/kubernetes/go.work'
CGO_CFLAGS='-O2 -g'
CGO_CPPFLAGS=''
CGO_CXXFLAGS='-O2 -g'
CGO_FFLAGS='-O2 -g'
CGO_LDFLAGS='-O2 -g'
PKG_CONFIG='pkg-config'
GOGCCFLAGS='-fPIC -arch x86_64 -m64 -pthread -fno-caret-diagnostics -Qunused-arguments -fmessage-length=0 -ffile-prefix-map=/var/folders/j0/f79y3_5d5svcgbm4_yfjg91w0000gn/T/go-build1055166348=/tmp/go-build -gno-record-gcc-switches -fno-common'
```

#### What did you expect to happen?

Successfully able to run the test

#### How can we reproduce it (as minimally and precisely as possible)?

Try running make test on darwin os.
```
make test
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>
Latest master

```
% git log --oneline
e3bb757129c (HEAD -> master, origin/master, origin/HEAD) Merge pull request #124143 from enj/enj/i/svm_update_crd_flake
f338ab5f959 svm test: wait after updating CRD to avoid flakes
79c61d5f030 Merge pull request #124124 from carlory/fix-124120
363fee59e4b fix panic with SIGSEGV in kubeadm certs check-expiration
3dedb8eb8c1 Merge pull request #124111 from liggitt/deflake-authz-test
```

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
Darwin Karthiks-MacBook-Pro.local 23.4.0 Darwin Kernel Version 23.4.0: Wed Feb 21 21:44:31 PST 2024; root:xnu-10063.101.15~2/RELEASE_X86_64 x86_64

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

ä¸æ¶‰åŠ

---

