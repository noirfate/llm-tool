# Issue å®‰å…¨åˆ†ææŠ¥å‘Š

# ğŸš¨ å­˜åœ¨é«˜é£é™©çš„ Issues (3 ä¸ª)

## Issue #129319 [Bug] Unexpected scheduling results due to mismatch between the inter-pod affinity rule implementation and the doc

- Issue é“¾æ¥ï¼š[#129319](https://github.com/kubernetes/kubernetes/issues/129319)

### Issue å†…å®¹

#### What happened?

There is a special rule in the scheduler's `pod affinity` plugin for scheduling a group of pods with inter-pod affinity to themselves. However, the current implementation does not match the doc and the comment, causing unexpected scheduling results.  
> [[doc]](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity)  
> ##### Scheduling a group of pods with inter-pod affinity to themselves  
> If the current pod being scheduled is the first in a series that have affinity to themselves, it is allowed to be scheduled if it passes all other affinity checks. This is determined by verifying that no other pod in the cluster matches the namespace and selector of this pod, that the pod matches its own terms, and the chosen node matches all requested topologies. This ensures that there will not be a deadlock even if all the pods have inter-pod affinity specified.

##### The Inconsistency

In the current version of the [documentation](https://github.com/kubernetes/website/blob/8a1bb4cf42fdd9c8065d66a1d2ebace4731ba414/content/en/docs/concepts/scheduling-eviction/assign-pod-node.md?plain=1#L292C16-L292C90) and the current version of the [code comment](https://github.com/kubernetes/kubernetes/blob/a4b8a3b2e33a3b591884f69b64f439e6b880dc40/pkg/scheduler/framework/plugins/interpodaffinity/filtering.go#L352C55-L353C67), both say that "**no other pod in the cluster** matches the namespace and selector of this pod," which implies that the scheduler will check **all pods**.

However, after investigating the implementation, the scheduler is actually checking **all pods on nodes with at least one topology key matched**, instead of **all pods**. (For more details, please see "Anything else we need to know?".)

As a result, the current implementation leads to unexpected scheduling results.


##### The Original Intent

We have investigated the history of this special rule, and it shows:

1. [At the very beginning](https://github.com/kubernetes/kubernetes/blob/587d164307de060d271f10f2386f39153360fba9/plugin/pkg/scheduler/algorithm/predicates/predicates.go#L836C4-L848C5), the code and the comment were consistent, both executing/stating that the scheduler would check **all pods in the cluster**.

2. [Later](https://github.com/kubernetes/kubernetes/blob/587d164307de060d271f10f2386f39153360fba9/plugin/pkg/scheduler/algorithm/predicates/predicates.go#L836C4-L848C5), previous developers introduced a mechanism for pre-calculating some data structures and using them to filter pod affinity. The newly added code became inconsistent with the comment:  
   - The code was checking **all pods on nodes with at least one topology key matched**.  
   - The comment, however, still stated **all pods in the cluster**.  
   At this point, the scheduler had fallback logic to the original code if the pre-calculated data didn't exist. Therefore, the scheduler have two routes simultaneouslyâ€”one consistent and the other inconsistent.

3. [Finally](https://github.com/kubernetes/kubernetes/blob/a4b8a3b2e33a3b591884f69b64f439e6b880dc40/pkg/scheduler/framework/plugins/interpodaffinity/filtering.go#L350C2-L360C3), previous developers removed both the fallback logic and the original code. The current implementation only uses the pre-calculated data structures, which are inconsistent with the comment.


#### What did you expect to happen?

According to the history of this rule, we assume the original intend was checking **all pods** in the cluster. It's because the new added data structure, the implementation became wrong as it checks **all pods on nodes with at least one topology key matched**. 

But we think this still need developers' help to check the original / ideal intent of this rule.
1. If the intent is "check all pods" -> the implementation should be fixed.
2. If the intent is "only check pods on nodes with at least one topology key matched", then the comment in kubernetes/kubernetes and documentation in kubernetes/website should be updated.


#### How can we reproduce it (as minimally and precisely as possible)?

##### Steps:
The incoming pod affinity's selector matches itself and also the existing pod.
The incoming pod's pod affinity has 2 terms with 2 different topology keys:
- node-0 has 2 of these topology keys
- node-1 has 1 of these topology keys
- node-2 has 0 of these topology keys

1. add 3 nodes using below yaml file.
`kubectl apply -f nodes.yaml`
2. add the existing pod first, it will land on node-2.
`kubectl apply -f existing_pod.yaml`
3. add the incoming pod, it can be scheduled, although at this time there is a pod that also matches these selectors in the cluster. Because that pod is on node-2, and node-2 doesn't match any topology key.
`kubectl apply -f incoming_pod.yaml`
4. delete all pods.
`kubectl delete pod --all`
5. change the existing pod's `nodeSelector` into `node-name: node-1`,  add the existing pod, it will land on node-1.
`(change the existing pod's nodeSelector)`
`kubectl apply -f existing_pod.yaml`
6. add the incoming pod, it will not be scheduled, this time the pod that also matches these selectors is on node-1, and node-1 matches 1 topology key.
`kubectl apply -f incoming_pod.yaml`
##### Nodes:
```yaml
apiVersion: v1
kind: Node
metadata:
  labels:
    label1: value1
    label2: value2
    node-name: node-0
  name: node-0
  namespace: default
status:
  allocatable:
    cpu: '10000'
    memory: 64T
    pods: '100000'
  capacity:
    cpu: '10000'
    memory: 64T
    pods: '10000'
---
apiVersion: v1
kind: Node
metadata:
  labels:
    label2: value2
    label3: value3
    node-name: node-1
  name: node-1
  namespace: default
status:
  allocatable:
    cpu: '10000'
    memory: 64T
    pods: '100000'
  capacity:
    cpu: '10000'
    memory: 64T
    pods: '10000'
---
apiVersion: v1
kind: Node
metadata:
  labels:
    label3: value3
    node-name: node-2
  name: node-2
  namespace: default
status:
  allocatable:
    cpu: '10000'
    memory: 64T
    pods: '100000'
  capacity:
    cpu: '10000'
    memory: 64T
    pods: '10000'
```
##### Existing pod:
```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    pod-label1: value1
  name: pod-0
  namespace: default
spec:
  containers:
  - image: nginx
    name: test-container
  nodeSelector:
    node-name: node-2
```
##### Incoming pod:
```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    pod-label1: value1
  name: test-pod
  namespace: default
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: pod-label1
            operator: In
            values:
            - value1
        topologyKey: label1
      - labelSelector:
          matchExpressions:
            - key: pod-label1
              operator: In
              values:
                - value1
        topologyKey: label2
  containers:
  - image: nginx
    name: test-container

```



#### Anything else we need to know?

##### Why the current implementation is checking pods on nodes with at least one topology key matched?
The `state.affinityCounts` is a map that maps "topology key-value pairs" to the "number of pods in the topology domain that match the namespace and selector." Below is the [code](https://github.com/kubernetes/kubernetes/blob/a4b8a3b2e33a3b591884f69b64f439e6b880dc40/pkg/scheduler/framework/plugins/interpodaffinity/filtering.go#L104C1-L125C1) related to this rule:
```go
# Check the special rule: no other pod in the cluster matches the namespace and selector of this pod.
if len(state.affinityCounts) == 0 &&...

# update state.affinityCounts
func (m topologyToMatchedTermCount) updateWithAffinityTerms(
	terms []framework.AffinityTerm, pod *v1.Pod, node *v1.Node, value int64) {
	if podMatchesAllAffinityTerms(terms, pod) {
		for _, t := range terms {
			m.update(node, t.TopologyKey, value)
		}
	}
}

func (m topologyToMatchedTermCount) update(node *v1.Node, tk string, value int64) {
	if tv, ok := node.Labels[tk]; ok {
		pair := topologyPair{key: tk, value: tv}
		m[pair] += value
		// value could be negative, hence we delete the entry if it is down to zero.
		if m[pair] == 0 {
			delete(m, pair)
		}
	}
}
```
In the last two function, we can see that only an existing pod on a node with at least one topology key required by the incoming pod will be counted in the `state.affinityCount`.
/sig scheduling

#### Kubernetes version

1.32.0


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†Kubernetesè°ƒåº¦å™¨ä¸­`pod affinity`æ’ä»¶çš„å®ç°ä¸å®˜æ–¹æ–‡æ¡£å’Œæ³¨é‡Šä¸ä¸€è‡´ï¼Œå¯¼è‡´äº†æ„å¤–çš„è°ƒåº¦ç»“æœã€‚å…·ä½“æ¥è¯´ï¼ŒæŒ‰ç…§æ–‡æ¡£å’Œæ³¨é‡Šçš„æè¿°ï¼Œå½“ä¸€ä¸ªPodæœ‰äº²å’Œæ€§è§„åˆ™è¦æ±‚æ—¶ï¼Œè°ƒåº¦å™¨åœ¨åˆ¤æ–­æ˜¯å¦å…è®¸è¯¥Podè¢«è°ƒåº¦æ—¶ï¼Œåº”è¯¥æ£€æŸ¥â€œé›†ç¾¤ä¸­æ‰€æœ‰Podâ€æ˜¯å¦åŒ¹é…è¯¥Podçš„å‘½åç©ºé—´å’Œé€‰æ‹©å™¨ã€‚ç„¶è€Œï¼Œå½“å‰çš„å®ç°ä»…æ£€æŸ¥äº†â€œåœ¨è‡³å°‘åŒ¹é…ä¸€ä¸ªæ‹“æ‰‘é”®çš„èŠ‚ç‚¹ä¸Šçš„æ‰€æœ‰Podâ€ï¼Œè€Œä¸æ˜¯é›†ç¾¤ä¸­çš„æ‰€æœ‰Podã€‚

è¿™ç§ä¸ä¸€è‡´å¯èƒ½å¯¼è‡´å®‰å…¨é£é™©ï¼š

- **è¿åéš”ç¦»ç­–ç•¥**ï¼šPodå¯èƒ½è¢«æ„å¤–åœ°è°ƒåº¦åˆ°ä¸ç¬¦åˆå®‰å…¨ç­–ç•¥çš„èŠ‚ç‚¹ä¸Šï¼Œå¯¼è‡´ä¸åŒç§Ÿæˆ·æˆ–åº”ç”¨ä¹‹é—´çš„éš”ç¦»è¢«æ‰“ç ´ã€‚
- **æ•°æ®æ³„éœ²é£é™©**ï¼šæ•æ„Ÿæ•°æ®å¯èƒ½è¢«æœªç»æˆæƒçš„Podè®¿é—®ã€‚
- **æƒé™æå‡**ï¼šä½æƒé™ç”¨æˆ·å¯èƒ½åˆ©ç”¨è¯¥æ¼æ´å°†Podè°ƒåº¦åˆ°é«˜æƒé™èŠ‚ç‚¹ï¼Œæå‡å…¶æƒé™ã€‚

æ ¹æ®CVSS 3.1è¯„åˆ†æ ‡å‡†ï¼Œæˆ‘ä»¬è¯„ä¼°è¯¥æ¼æ´çš„è¯„åˆ†å¦‚ä¸‹ï¼š

- **æ”»å‡»å‘é‡ï¼ˆAVï¼‰**ï¼šç½‘ç»œï¼ˆNï¼‰â€”â€”æ”»å‡»è€…å¯ä»¥é€šè¿‡ç½‘ç»œè®¿é—®Kubernetes APIæäº¤æ¶æ„Podã€‚
- **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰**ï¼šä½ï¼ˆLï¼‰â€”â€”æ”»å‡»ä¸éœ€è¦ç‰¹æ®Šçš„æ¡ä»¶æˆ–ç¯å¢ƒã€‚
- **ç‰¹æƒè¦æ±‚ï¼ˆPRï¼‰**ï¼šä½ï¼ˆLï¼‰â€”â€”éœ€è¦èƒ½å¤Ÿåˆ›å»ºPodçš„æƒé™ï¼Œé€šå¸¸åœ¨å¤šç§Ÿæˆ·ç¯å¢ƒä¸‹ï¼Œç”¨æˆ·å…·æœ‰åœ¨å…¶å‘½åç©ºé—´åˆ›å»ºPodçš„æƒé™ã€‚
- **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰**ï¼šæ— ï¼ˆNï¼‰â€”â€”ä¸éœ€è¦å…¶ä»–ç”¨æˆ·çš„äº¤äº’ã€‚
- **ä½œç”¨åŸŸï¼ˆSï¼‰**ï¼šæ”¹å˜ï¼ˆCï¼‰â€”â€”æ”»å‡»å¯èƒ½å½±å“åˆ°å…¶ä»–ç»„ä»¶æˆ–èµ„æºçš„å®‰å…¨æ€§ã€‚
- **æœºå¯†æ€§ï¼ˆCï¼‰**ï¼šé«˜ï¼ˆHï¼‰â€”â€”å¯èƒ½å¯¼è‡´æœªç»æˆæƒçš„è®¿é—®ï¼Œæ³„éœ²æ•æ„Ÿæ•°æ®ã€‚
- **å®Œæ•´æ€§ï¼ˆIï¼‰**ï¼šæ— ï¼ˆNï¼‰â€”â€”ä¸ä¼šç›´æ¥å¯¼è‡´æ•°æ®ç¯¡æ”¹ã€‚
- **å¯ç”¨æ€§ï¼ˆAï¼‰**ï¼šæ— ï¼ˆNï¼‰â€”â€”ä¸ä¼šç›´æ¥å¯¼è‡´æœåŠ¡ä¸å¯ç”¨ã€‚

æ ¹æ®ä»¥ä¸ŠæŒ‡æ ‡ï¼Œä½¿ç”¨CVSSè®¡ç®—å™¨è®¡ç®—å¾—åˆ†ï¼š

- **åŸºç¡€åˆ†æ•°ï¼ˆBase Scoreï¼‰**ï¼š**7.5ï¼ˆé«˜ï¼‰**

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# å¤ç°è„šæœ¬ï¼šæ¼”ç¤ºç”±äºPodäº²å’Œæ€§è§„åˆ™çš„å®ç°ä¸æ–‡æ¡£ä¸ä¸€è‡´ï¼Œå¯¼è‡´Podè¢«è°ƒåº¦åˆ°ä¸ç¬¦åˆé¢„æœŸçš„èŠ‚ç‚¹ä¸Šã€‚

import subprocess
import time

def run_cmd(cmd):
    print(f"æ‰§è¡Œå‘½ä»¤ï¼š{cmd}")
    subprocess.run(cmd, shell=True, check=True)

def apply_yaml(content, filename):
    with open(filename, 'w') as f:
        f.write(content)
    run_cmd(f"kubectl apply -f {filename}")

def delete_pods():
    run_cmd("kubectl delete pod --all --ignore-not-found")

# å®šä¹‰èŠ‚ç‚¹çš„YAML
nodes_yaml = """
apiVersion: v1
kind: Node
metadata:
  labels:
    label1: value1
    label2: value2
    node-name: node-0
  name: node-0
---
apiVersion: v1
kind: Node
metadata:
  labels:
    label2: value2
    label3: value3
    node-name: node-1
  name: node-1
---
apiVersion: v1
kind: Node
metadata:
  labels:
    label3: value3
    node-name: node-2
  name: node-2
"""

# å®šä¹‰ç°æœ‰çš„Podï¼Œåˆå§‹è°ƒåº¦åˆ°node-2
existing_pod_yaml = """
apiVersion: v1
kind: Pod
metadata:
  labels:
    pod-label1: value1
  name: pod-0
spec:
  containers:
  - image: nginx
    name: test-container
  nodeSelector:
    node-name: node-2
"""

# å®šä¹‰å¾…è°ƒåº¦çš„Podï¼Œå…·æœ‰Podäº²å’Œæ€§è¦æ±‚
incoming_pod_yaml = """
apiVersion: v1
kind: Pod
metadata:
  labels:
    pod-label1: value1
  name: test-pod
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: pod-label1
            operator: In
            values:
            - value1
        topologyKey: label1
      - labelSelector:
          matchExpressions:
            - key: pod-label1
              operator: In
              values:
                - value1
        topologyKey: label2
  containers:
  - image: nginx
    name: test-container
"""

def main():
    # åˆ é™¤æ‰€æœ‰Podï¼Œç¡®ä¿ç¯å¢ƒå¹²å‡€
    delete_pods()

    # åº”ç”¨èŠ‚ç‚¹é…ç½®
    apply_yaml(nodes_yaml, "nodes.yaml")
    time.sleep(2)

    # éƒ¨ç½²ç°æœ‰çš„Podåˆ°node-2
    apply_yaml(existing_pod_yaml, "existing_pod.yaml")
    time.sleep(2)

    # éƒ¨ç½²å¾…è°ƒåº¦çš„Pod
    apply_yaml(incoming_pod_yaml, "incoming_pod.yaml")
    time.sleep(5)

    # æŸ¥çœ‹Podçš„è°ƒåº¦æƒ…å†µ
    print("ç¬¬ä¸€æ¬¡è°ƒåº¦ç»“æœï¼š")
    run_cmd("kubectl get pods -o wide")

    # åˆ é™¤æ‰€æœ‰Pod
    delete_pods()
    time.sleep(2)

    # ä¿®æ”¹ç°æœ‰Podçš„nodeSelectorä¸ºnode-1
    modified_existing_pod_yaml = existing_pod_yaml.replace("node-2", "node-1")

    # éƒ¨ç½²ä¿®æ”¹åçš„ç°æœ‰Podåˆ°node-1
    apply_yaml(modified_existing_pod_yaml, "existing_pod.yaml")
    time.sleep(2)

    # å†æ¬¡éƒ¨ç½²å¾…è°ƒåº¦çš„Pod
    apply_yaml(incoming_pod_yaml, "incoming_pod.yaml")
    time.sleep(5)

    # æŸ¥çœ‹Podçš„è°ƒåº¦æƒ…å†µ
    print("ç¬¬äºŒæ¬¡è°ƒåº¦ç»“æœï¼š")
    run_cmd("kubectl get pods -o wide")

    print("å¤ç°å®Œæˆã€‚è¯·æŸ¥çœ‹è°ƒåº¦ç»“æœã€‚")

if __name__ == "__main__":
    main()
```


**è§£é‡Šè¯´æ˜ï¼š**

**è„šæœ¬åŠŸèƒ½**ï¼š

è¯¥è„šæœ¬æ—¨åœ¨å¤ç°Issueä¸­æè¿°çš„é—®é¢˜ï¼Œæ¼”ç¤ºç”±äºè°ƒåº¦å™¨å®ç°ä¸æ–‡æ¡£ä¸ä¸€è‡´ï¼Œå¯¼è‡´Podè°ƒåº¦ç»“æœä¸ç¬¦åˆé¢„æœŸçš„æƒ…å†µã€‚

**æ‰§è¡Œæ­¥éª¤**ï¼š

1. **æ¸…ç†ç¯å¢ƒ**ï¼šåˆ é™¤é›†ç¾¤ä¸­æ‰€æœ‰çš„Podï¼Œç¡®ä¿æµ‹è¯•ç¯å¢ƒå¹²å‡€ã€‚
   
2. **åˆ›å»ºèŠ‚ç‚¹**ï¼šåˆ›å»ºä¸‰ä¸ªèŠ‚ç‚¹`node-0`ã€`node-1`å’Œ`node-2`ï¼Œå¹¶åˆ†åˆ«èµ‹äºˆä¸åŒçš„æ ‡ç­¾ï¼Œä»¥ç”¨äºæµ‹è¯•æ‹“æ‰‘é”®ã€‚

3. **ç¬¬ä¸€æ¬¡æµ‹è¯•**ï¼š

   - **éƒ¨ç½²ç°æœ‰Podåˆ°`node-2`**ï¼šå°†`pod-0`éƒ¨ç½²åˆ°`node-2`ä¸Šï¼Œ`pod-0`çš„æ ‡ç­¾å°†åŒ¹é…å¾…è°ƒåº¦Podçš„äº²å’Œæ€§é€‰æ‹©å™¨ã€‚
   
   - **éƒ¨ç½²å¾…è°ƒåº¦Pod**ï¼šéƒ¨ç½²å…·æœ‰Podäº²å’Œæ€§è¦æ±‚çš„`test-pod`ã€‚æŒ‰ç…§æ–‡æ¡£ï¼Œ`test-pod`åº”è¯¥æ— æ³•è¢«è°ƒåº¦ï¼ˆå› ä¸ºé›†ç¾¤ä¸­å­˜åœ¨åŒ¹é…é€‰æ‹©å™¨çš„Podï¼‰ã€‚
   
   - **ç»“æœè§‚å¯Ÿ**ï¼šè„šæœ¬å°†æ˜¾ç¤º`test-pod`çš„å®é™…è°ƒåº¦ç»“æœã€‚ç”±äºå®ç°é—®é¢˜ï¼Œ`test-pod`å®é™…ä¸Šè¢«æˆåŠŸè°ƒåº¦ï¼Œè¿åäº†é¢„æœŸã€‚

4. **ç¬¬äºŒæ¬¡æµ‹è¯•**ï¼š

   - **æ¸…ç†ç¯å¢ƒ**ï¼šå†æ¬¡åˆ é™¤æ‰€æœ‰Podã€‚

   - **ä¿®æ”¹ç°æœ‰Podçš„èŠ‚ç‚¹**ï¼šå°†`pod-0`çš„`nodeSelector`ä¿®æ”¹ä¸º`node-1`ï¼Œä½¿å…¶éƒ¨ç½²åˆ°`node-1`ä¸Šã€‚`node-1`å…·æœ‰å¾…è°ƒåº¦Podäº²å’Œæ€§è¦æ±‚çš„æ‹“æ‰‘é”®ã€‚

   - **å†æ¬¡éƒ¨ç½²å¾…è°ƒåº¦Pod**ï¼šéƒ¨ç½²`test-pod`ã€‚æŒ‰ç…§å®ç°ï¼Œ`test-pod`æ­¤æ—¶æ— æ³•è¢«è°ƒåº¦ï¼Œå› ä¸ºè°ƒåº¦å™¨åœ¨`node-1`ä¸Šå‘ç°äº†åŒ¹é…çš„Podï¼Œä¸”`node-1`åŒ¹é…æ‹“æ‰‘é”®ã€‚

   - **ç»“æœè§‚å¯Ÿ**ï¼šè„šæœ¬å°†æ˜¾ç¤º`test-pod`çš„å®é™…è°ƒåº¦ç»“æœï¼Œæ­¤æ—¶`test-pod`æ— æ³•è¢«è°ƒåº¦ï¼Œç¬¦åˆé¢„æœŸã€‚

**ç»“è®º**ï¼š

é€šè¿‡è¯¥è„šæœ¬ï¼Œè¯æ˜äº†è°ƒåº¦å™¨åœ¨å¤„ç†Podäº²å’Œæ€§æ—¶ï¼Œä»…æ£€æŸ¥äº†éƒ¨åˆ†èŠ‚ç‚¹ï¼Œå¯¼è‡´äº†Podè¢«è°ƒåº¦åˆ°ä¸ç¬¦åˆå®‰å…¨ç­–ç•¥çš„èŠ‚ç‚¹ä¸Šã€‚è¿™å¯èƒ½è¢«æ¶æ„ç”¨æˆ·åˆ©ç”¨ï¼Œç»•è¿‡å®‰å…¨é™åˆ¶ï¼Œè®¿é—®æœªç»æˆæƒçš„èµ„æºï¼Œå¸¦æ¥é«˜é£é™©çš„å®‰å…¨é—®é¢˜ã€‚

---


## Issue #129249 devicemanager stuck in Allocate RPC and causing all new coming pods pending on admitting

- Issue é“¾æ¥ï¼š[#129249](https://github.com/kubernetes/kubernetes/issues/129249)

### Issue å†…å®¹

#### What happened?

After created pod with device request, its state always be `Pending`, even though it was scheduled.
And at the same time, new coming pod (without device request) on the same node is on `Pending` state too.

After a bit investigation with goroutines, devicemanager was stucking in `Allocate` RPC, and wait device plugin's response:
![image](https://github.com/user-attachments/assets/0a97e9ef-5341-4fd7-89bb-d4d210c3f00f)

Accroding to this stack, pod admitting is stucked obviously.

#### What did you expect to happen?

Device plugins' bug or out-of-service, should not lead kubelet pod admit procedure blocking.

#### How can we reproduce it (as minimally and precisely as possible)?

Reproduct it with https://github.com/kube-HPC/k8s-dummy-device-plugin. Add some sleep time in `Allocate` call:
![image](https://github.com/user-attachments/assets/9a1b021a-27a1-43a1-af73-0f6426e7f169)

![image](https://github.com/user-attachments/assets/501dd05e-10f8-4327-825b-16554896c0b6)

pod with dummy device and new coming pod without dummy device are in `Pending` state:
![image](https://github.com/user-attachments/assets/d73c63a5-ae7c-405c-9dc6-bce45076a01e)


After running out sleep time, all pod return to `Running` state:
![image](https://github.com/user-attachments/assets/212bcc69-df04-490f-8fad-24c8be5e4ce2)


#### Anything else we need to know?

NA

#### Kubernetes version

<details>

```console
v1.25
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
CentOS
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
cri-o
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
åœ¨è¯¥Issueä¸­ï¼Œæè¿°äº†ä¸€ä¸ªé—®é¢˜ï¼šåœ¨Kubernetesé›†ç¾¤ä¸­ï¼Œå½“åˆ›å»ºäº†ä¸€ä¸ªå¸¦æœ‰è®¾å¤‡è¯·æ±‚çš„Podæ—¶ï¼Œè¯¥Podä¸€ç›´å¤„äº`Pending`çŠ¶æ€ï¼Œå³ä½¿å®ƒå·²ç»è¢«è°ƒåº¦ã€‚åŒæ—¶ï¼Œåœ¨åŒä¸€èŠ‚ç‚¹ä¸Šæ–°åˆ›å»ºçš„Podï¼ˆå³ä½¿ä¸éœ€è¦è®¾å¤‡è¯·æ±‚ï¼‰ä¹Ÿå¤„äº`Pending`çŠ¶æ€ã€‚

ç»è¿‡è°ƒæŸ¥å‘ç°ï¼Œ`devicemanager`åœ¨å¤„ç†`Allocate` RPCæ—¶å¡ä½äº†ï¼Œç­‰å¾…è®¾å¤‡æ’ä»¶ï¼ˆdevice pluginï¼‰çš„å“åº”ï¼Œå¯¼è‡´Podçš„å‡†å…¥è¿‡ç¨‹è¢«é˜»å¡ã€‚è¿™æ„å‘³ç€ï¼Œå¦‚æœè®¾å¤‡æ’ä»¶å‡ºç°é—®é¢˜ï¼ˆä¾‹å¦‚bugæˆ–æ— å“åº”ï¼‰ï¼Œä¼šå½±å“åˆ°èŠ‚ç‚¹ä¸Šæ‰€æœ‰Podçš„è¿è¡Œï¼ŒåŒ…æ‹¬ä¸è¯·æ±‚è®¾å¤‡çš„Podã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼Œè¿™å¯èƒ½å¯¼è‡´æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚å¦‚æœæ”»å‡»è€…èƒ½å¤Ÿä½¿è®¾å¤‡æ’ä»¶çš„`Allocate`è°ƒç”¨å¡ä½ï¼Œå°±èƒ½é˜»å¡èŠ‚ç‚¹ä¸Šæ‰€æœ‰Podçš„è°ƒåº¦å’Œè¿è¡Œï¼Œå½±å“é›†ç¾¤çš„å¯ç”¨æ€§ã€‚ç‰¹åˆ«æ˜¯åœ¨å¤šç§Ÿæˆ·ç¯å¢ƒä¸­ï¼Œè¿™ç§è¡Œä¸ºå¯èƒ½è¢«ä½æƒé™ç”¨æˆ·åˆ©ç”¨æ¥å½±å“å…¶ä»–ç”¨æˆ·çš„æœåŠ¡ã€‚

æ ¹æ®CVSS 3.1æ ‡å‡†è¿›è¡Œè¯„åˆ†ï¼š

- æ”»å‡»å‘é‡ï¼ˆAVï¼‰ï¼šç½‘ç»œï¼ˆNï¼‰â€”â€”æ”»å‡»è€…å¯ä»¥é€šè¿‡æäº¤ç‰¹åˆ¶çš„Podè¿›è¡Œè¿œç¨‹æ”»å‡»
- æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰ï¼šä½ï¼ˆLï¼‰â€”â€”æ”»å‡»ä¸éœ€è¦ç‰¹æ®Šæ¡ä»¶
- æƒé™è¦æ±‚ï¼ˆPRï¼‰ï¼šä½ï¼ˆLï¼‰â€”â€”æ”»å‡»è€…éœ€è¦èƒ½å¤Ÿåˆ›å»ºPodçš„æƒé™
- ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰ï¼šæ— ï¼ˆNï¼‰â€”â€”ä¸éœ€è¦å…¶ä»–ç”¨æˆ·çš„äº¤äº’
- ä½œç”¨èŒƒå›´ï¼ˆSï¼‰ï¼šæ”¹å˜ï¼ˆCï¼‰â€”â€”æ”»å‡»å½±å“åˆ°äº†å…¶ä»–ç»„ä»¶ï¼ˆå…¶ä»–ç”¨æˆ·çš„Podï¼‰
- æœºå¯†æ€§ï¼ˆCï¼‰ï¼šæ— ï¼ˆNï¼‰
- å®Œæ•´æ€§ï¼ˆIï¼‰ï¼šæ— ï¼ˆNï¼‰
- å¯ç”¨æ€§ï¼ˆAï¼‰ï¼šé«˜ï¼ˆHï¼‰â€”â€”æœåŠ¡ä¸å¯ç”¨

CVSSå‘é‡ä¸ºï¼š`CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:C/C:N/I:N/A:H`ï¼ŒåŸºæœ¬åˆ†ä¸º7.5ï¼ˆé«˜ï¼‰

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import os
import time
import threading
import subprocess
import sys
import signal

# Function to simulate a hanging Allocate call in device plugin
def start_dummy_device_plugin():
    # Create the device plugin directory if it doesn't exist
    plugin_dir = '/var/lib/kubelet/device-plugins'
    os.makedirs(plugin_dir, exist_ok=True)
    
    # Create a Unix socket file for the dummy device plugin
    socket_path = os.path.join(plugin_dir, 'dummy-plugin.sock')
    
    # Remove the socket file if it already exists
    if os.path.exists(socket_path):
        os.remove(socket_path)
    
    # Start a Unix socket server to simulate the device plugin
    import socket
    server = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
    server.bind(socket_path)
    server.listen(1)
    print("Dummy device plugin started and waiting for connections...")
    
    while True:
        conn, addr = server.accept()
        print("Connection received. Simulating hanging Allocate call...")
        time.sleep(600)  # Simulate hang for 10 minutes

def register_device_plugin():
    # Register the dummy device plugin with kubelet
    registration_message = '''
    {
        "version": "v1beta1",
        "endpoint": "dummy-plugin.sock",
        "resourceName": "example.com/dummy",
        "options": {}
    }
    '''
    plugin_dir = '/var/lib/kubelet/device-plugins'
    kubelet_socket = os.path.join(plugin_dir, 'kubelet.sock')
    if not os.path.exists(kubelet_socket):
        print("Kubelet socket not found. Is kubelet running with device-plugins enabled?")
        sys.exit(1)
    
    # Send the registration message to kubelet via Unix socket
    import socket
    client = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)
    client.connect(kubelet_socket)
    client.sendall(bytes(registration_message, 'utf-8'))
    client.close()
    print("Dummy device plugin registered with kubelet.")

def create_pods():
    # Create a pod that requests the dummy device
    pod_with_device = '''
apiVersion: v1
kind: Pod
metadata:
  name: dummy-pod
spec:
  containers:
  - name: dummy-container
    image: busybox
    command: ["sh", "-c", "sleep 3600"]
    resources:
      limits:
        example.com/dummy: 1
    '''
    with open('dummy-pod.yaml', 'w') as f:
        f.write(pod_with_device)
    subprocess.run(['kubectl', 'apply', '-f', 'dummy-pod.yaml'])
    print("Pod with device request created.")
    
    # Create a pod that does not request any device
    pod_without_device = '''
apiVersion: v1
kind: Pod
metadata:
  name: normal-pod
spec:
  containers:
  - name: normal-container
    image: busybox
    command: ["sh", "-c", "sleep 3600"]
    '''
    with open('normal-pod.yaml', 'w') as f:
        f.write(pod_without_device)
    subprocess.run(['kubectl', 'apply', '-f', 'normal-pod.yaml'])
    print("Normal pod without device request created.")

def monitor_pods():
    print("Monitoring pod statuses...")
    while True:
        dummy_pod_status = subprocess.getoutput('kubectl get pod dummy-pod -o jsonpath="{.status.phase}"')
        normal_pod_status = subprocess.getoutput('kubectl get pod normal-pod -o jsonpath="{.status.phase}"')
        print(f"dummy-pod status: {dummy_pod_status}, normal-pod status: {normal_pod_status}")
        time.sleep(5)

if __name__ == '__main__':
    # Check if script is run as root
    if os.geteuid() != 0:
        print("Please run this script as root.")
        sys.exit(1)
    
    # Start dummy device plugin in a separate thread
    plugin_thread = threading.Thread(target=start_dummy_device_plugin, daemon=True)
    plugin_thread.start()
    
    # Give the plugin time to start
    time.sleep(5)
    
    # Register the device plugin with kubelet
    register_device_plugin()
    
    # Create pods
    create_pods()
    
    # Monitor pod statuses
    monitor_pods()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥è„šæœ¬æ¨¡æ‹Ÿäº†ä¸€ä¸ªè®¾å¤‡æ’ä»¶ï¼Œåœ¨`Allocate`è°ƒç”¨ä¸­äººä¸ºåœ°åŠ å…¥äº†é•¿æ—¶é—´çš„`sleep`ï¼Œä»¥æ¨¡æ‹Ÿè®¾å¤‡æ’ä»¶æŒ‚èµ·çš„æƒ…å†µã€‚

å…·ä½“æ­¥éª¤ï¼š

1. **å¯åŠ¨è®¾å¤‡æ’ä»¶ï¼š** ä½¿ç”¨Unix Socketæ¨¡æ‹Ÿè®¾å¤‡æ’ä»¶ï¼Œç­‰å¾…è¿æ¥åï¼Œåœ¨`Allocate`è°ƒç”¨ä¸­`sleep` 600ç§’ï¼ˆ10åˆ†é’Ÿï¼‰ã€‚

2. **æ³¨å†Œè®¾å¤‡æ’ä»¶ï¼š** å‘kubeletçš„`kubelet.sock`å‘é€æ³¨å†Œæ¶ˆæ¯ï¼Œæ³¨å†Œæ¨¡æ‹Ÿçš„è®¾å¤‡æ’ä»¶ã€‚

3. **åˆ›å»ºPodï¼š** åˆ›å»ºä¸¤ä¸ªPodï¼šä¸€ä¸ªè¯·æ±‚æ¨¡æ‹Ÿçš„è®¾å¤‡ï¼ˆ`dummy-pod`ï¼‰ï¼Œä¸€ä¸ªä¸è¯·æ±‚è®¾å¤‡ï¼ˆ`normal-pod`ï¼‰ã€‚

4. **ç›‘æ§PodçŠ¶æ€ï¼š** ä¸æ–­æŸ¥è¯¢å¹¶è¾“å‡ºä¸¤ä¸ªPodçš„çŠ¶æ€ã€‚

è¿è¡Œè¯¥è„šæœ¬éœ€è¦æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼š

- åœ¨Linuxç³»ç»Ÿä¸Šè¿è¡Œï¼Œå¹¶ä»¥`root`ç”¨æˆ·æƒé™æ‰§è¡Œï¼ˆå› ä¸ºéœ€è¦è®¿é—®`/var/lib/kubelet/device-plugins`ç›®å½•å’Œä¸kubeleté€šä¿¡ï¼‰ã€‚
- Kubernetesé›†ç¾¤å·²éƒ¨ç½²å¹¶è¿è¡Œï¼Œä¸”`kubectl`å·²æ­£ç¡®é…ç½®ã€‚
- kubeletå¯ç”¨äº†è®¾å¤‡æ’ä»¶åŠŸèƒ½ï¼ˆä½¿ç”¨`--feature-gates=DevicePlugins=true`å¯åŠ¨ï¼‰ã€‚

é€šè¿‡è¿è¡Œè¯¥è„šæœ¬ï¼Œå¯ä»¥è§‚å¯Ÿåˆ°ï¼š

- `dummy-pod`å› ä¸ºè®¾å¤‡æ’ä»¶çš„`Allocate`è°ƒç”¨è¢«é˜»å¡ï¼Œå¤„äº`Pending`çŠ¶æ€ã€‚
- `normal-pod`è™½ç„¶ä¸éœ€è¦è®¾å¤‡ï¼Œä½†ä¹Ÿä¼šå› ä¸ºè®¾å¤‡æ’ä»¶çš„é—®é¢˜è€Œå¤„äº`Pending`çŠ¶æ€ã€‚
- å½“è®¾å¤‡æ’ä»¶çš„`Allocate`è°ƒç”¨ç»“æŸåï¼ˆ10åˆ†é’Ÿåï¼‰ï¼Œä¸¤ä¸ªPodéƒ½ä¼šè¿›å…¥`Running`çŠ¶æ€ã€‚

è¿™è¯æ˜äº†è®¾å¤‡æ’ä»¶çš„å¼‚å¸¸ä¼šå½±å“åˆ°æ•´ä¸ªèŠ‚ç‚¹ä¸ŠPodçš„è°ƒåº¦å’Œè¿è¡Œï¼Œå¯èƒ½è¢«æ¶æ„åˆ©ç”¨æ¥å®æ–½æ‹’ç»æœåŠ¡æ”»å‡»ã€‚

---


## Issue #129199 Unable to verify signed images for 1.32 release

- Issue é“¾æ¥ï¼š[#129199](https://github.com/kubernetes/kubernetes/issues/129199)

### Issue å†…å®¹

#### What happened?


tried to verify using command mentioned in https://kubernetes.io/docs/tasks/administer-cluster/verify-signed-artifacts/#verifying-image-signatures 
```
cosign verify registry.k8s.io/kube-apiserver-amd64:v1.32.0 \
  --certificate-identity krel-trust@k8s-releng-prod.iam.gserviceaccount.com \
  --certificate-oidc-issuer https://accounts.google.com \
  | jq .
Error: no signatures found
main.go:69: error during command execution: no signatures found
```

#### What did you expect to happen?

Should have return along with signatures details 
```
The following checks were performed on each of these signatures:
  - The cosign claims were validated
  - Existence of the claims in the transparency log was verified offline
  - The code-signing certificate was verified using trusted certificate authority certificates
```

#### How can we reproduce it (as minimally and precisely as possible)?

Run 
```
cosign verify registry.k8s.io/kube-apiserver-amd64:v1.32.0 \
  --certificate-identity krel-trust@k8s-releng-prod.iam.gserviceaccount.com \
  --certificate-oidc-issuer https://accounts.google.com 
```

#### Anything else we need to know?

happing for other 1.32 patch images

- registry.k8s.io/kube-controller-manager:v1.32.0
- registry.k8s.io/kube-proxy:v1.32.0
- registry.k8s.io/kube-scheduler:v1.32.0
- registry.k8s.io/conformance:v1.32.0




#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
NA 
</details>


#### Cloud provider

<details>
NA
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
 cosign version
  ______   ______        _______. __    _______ .__   __.
 /      | /  __  \      /       ||  |  /  _____||  \ |  |
|  ,----'|  |  |  |    |   (----`|  | |  |  __  |   \|  |
|  |     |  |  |  |     \   \    |  | |  | |_ | |  . `  |
|  `----.|  `--'  | .----)   |   |  | |  |__| | |  |\   |
 \______| \______/  |_______/    |__|  \______| |__| \__|
cosign: A tool for Container Signing, Verification and Storage in an OCI registry.

GitVersion:    2.4.1
GitCommit:     9a4cfe1aae777984c07ce373d97a65428bbff734
GitTreeState:  "clean"
BuildDate:     2024-10-03T17:01:50Z
GoVersion:     go1.23.2
Compiler:      gc
Platform:      darwin/arm64
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨cosignå·¥å…·å¯¹Kubernetes 1.32ç‰ˆæœ¬çš„é•œåƒè¿›è¡Œç­¾åéªŒè¯æ—¶ï¼Œå‡ºç°äº†`Error: no signatures found`çš„é”™è¯¯ã€‚è¿™æ„å‘³ç€ç”¨æˆ·æ— æ³•éªŒè¯ä¸‹è½½çš„é•œåƒæ˜¯å¦ç»è¿‡å®˜æ–¹ç­¾åï¼Œå­˜åœ¨ä¾›åº”é“¾å®‰å…¨é£é™©ã€‚å¦‚æœæ”»å‡»è€…æ›¿æ¢äº†è¿™äº›é•œåƒï¼Œç”¨æˆ·æ— æ³•é€šè¿‡ç­¾åéªŒè¯å‘ç°ï¼Œå¯èƒ½å¯¼è‡´è¿è¡Œè¢«ç¯¡æ”¹æˆ–æ¶æ„çš„é•œåƒï¼Œé€ æˆä¸¥é‡çš„å®‰å…¨åæœã€‚

æ ¹æ®CVSS 3.1æ ‡å‡†è¿›è¡Œè¯„åˆ†ï¼š

- **æ”»å‡»å‘é‡ï¼ˆAVï¼‰**ï¼šç½‘ç»œï¼ˆNï¼‰
- **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰**ï¼šä½ï¼ˆLï¼‰
- **ç‰¹æƒè¦æ±‚ï¼ˆPRï¼‰**ï¼šæ— ï¼ˆNï¼‰
- **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰**ï¼šéœ€è¦ï¼ˆRï¼‰
- **èŒƒå›´ï¼ˆSï¼‰**ï¼šæœªæ”¹å˜ï¼ˆUï¼‰
- **æœºå¯†æ€§å½±å“ï¼ˆCï¼‰**ï¼šé«˜ï¼ˆHï¼‰
- **å®Œæ•´æ€§å½±å“ï¼ˆIï¼‰**ï¼šé«˜ï¼ˆHï¼‰
- **å¯ç”¨æ€§å½±å“ï¼ˆAï¼‰**ï¼šé«˜ï¼ˆHï¼‰

è®¡ç®—å¾—åˆ†ä¸º**8.8ï¼ˆé«˜ï¼‰**ï¼Œå› æ­¤é£é™©è¯„çº§ä¸ºé«˜é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import subprocess

def verify_image_signature():
    image = "registry.k8s.io/kube-apiserver-amd64:v1.32.0"
    identity = "krel-trust@k8s-releng-prod.iam.gserviceaccount.com"
    issuer = "https://accounts.google.com"
    command = [
        "cosign", "verify", image,
        "--certificate-identity", identity,
        "--certificate-oidc-issuer", issuer
    ]
    try:
        result = subprocess.run(command, capture_output=True, text=True, check=True)
        print("ç­¾åéªŒè¯æˆåŠŸï¼Œè¾“å‡ºå¦‚ä¸‹ï¼š")
        print(result.stdout)
    except subprocess.CalledProcessError as e:
        print("ç­¾åéªŒè¯å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯å¦‚ä¸‹ï¼š")
        print(e.stderr)

if __name__ == "__main__":
    verify_image_signature()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥è„šæœ¬ä½¿ç”¨Pythonçš„`subprocess`æ¨¡å—è°ƒç”¨`cosign`å‘½ä»¤ï¼Œå¯¹æŒ‡å®šçš„Kubernetesé•œåƒè¿›è¡Œç­¾åéªŒè¯ã€‚è„šæœ¬æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

1. å®šä¹‰è¦éªŒè¯çš„é•œåƒåç§°ã€è¯ä¹¦èº«ä»½å’ŒOIDCå‘è¡Œè€…ã€‚
2. æ„å»º`cosign verify`å‘½ä»¤çš„å‚æ•°åˆ—è¡¨ã€‚
3. ä½¿ç”¨`subprocess.run`æ‰§è¡Œå‘½ä»¤ï¼Œå¹¶æ•è·è¾“å‡ºã€‚
4. å¦‚æœéªŒè¯æˆåŠŸï¼Œæ‰“å°ç­¾åéªŒè¯çš„è¾“å‡ºä¿¡æ¯ã€‚
5. å¦‚æœéªŒè¯å¤±è´¥ï¼Œæ•è·å¼‚å¸¸å¹¶æ‰“å°é”™è¯¯ä¿¡æ¯ã€‚

è¯¥è„šæœ¬å¤ç°äº†Issueä¸­æåˆ°çš„é—®é¢˜ï¼Œå³åœ¨éªŒè¯`v1.32.0`ç‰ˆæœ¬çš„Kubernetesé•œåƒæ—¶ï¼Œæ— æ³•æ‰¾åˆ°ç­¾åï¼Œä»è€Œæç¤º`no signatures found`çš„é”™è¯¯ã€‚è¿™è¡¨æ˜è¯¥ç‰ˆæœ¬çš„é•œåƒç¼ºå°‘ç­¾åï¼Œå­˜åœ¨å®‰å…¨é£é™©ã€‚

---


# âš ï¸ å­˜åœ¨ä½é£é™©çš„ Issues (2 ä¸ª)

## Issue #129357 [FG:InPlacePodVerticalScaling] Pod CPU limit is not configured to cgroups as calculated if systemd cgroup driver is used

- Issue é“¾æ¥ï¼š[#129357](https://github.com/kubernetes/kubernetes/issues/129357)

### Issue å†…å®¹

#### What happened?

As a result of #124216, which was introduced in v.1.32, a pod CPU limit calculated in `ResourceConfigForPod()` is rounded up to the nearest 10ms in `libcontainer` at resizing the pod:
- Resize a pod:
  ```
  $ kubectl patch pod resize-pod --subresource=resize --patch '{"spec":{"containers":[{"name":"resize-container", "resources":{"limits":{"cpu":"417m"}}}]}}'
  pod/resize-pod patched
  ```
- The container cgroup value is set with 1ms precision:
  ```
  $ kubectl exec resize-pod -- cat /sys/fs/cgroup/cpu.max
  41700 100000
  ```
- The pod cgroup value is rounded up:
  ```
  $ cat /sys/fs/cgroup/kubelet.slice/kubelet-kubepods.slice/kubelet-kubepods-burstable.slice/kubelet-kubepods-burstable-pod68a17b59_0d31_40b2_ba86_ea43f3b2f05c.slice/cpu.max
  42000 100000
  ```

When `systemd` cgroup driver is used, `libcontainer` passes the CPU Quota to `systemd` with rounding up:
https://github.com/kubernetes/kubernetes/blob/a4b8a3b2e33a3b591884f69b64f439e6b880dc40/vendor/github.com/opencontainers/runc/libcontainer/cgroups/systemd/common.go#L304-L311

In addition, there seems to be a race in `libcontainer`. It directly writes values to the cgroup file without roundup after it passes the rounded value to `systemd`:
https://github.com/kubernetes/kubernetes/blob/a4b8a3b2e33a3b591884f69b64f439e6b880dc40/vendor/github.com/opencontainers/runc/libcontainer/cgroups/systemd/v2.go#L489-L493

So, there is also a case where the cgroup value is set as calculated. As far as I tried, decreasing CPU limits usually hits this case though Iâ€™m not sure why:
- Decrease the CPU limits:
  ```
  $ kubectl patch pod resize-pod --subresource=resize --patch '{"spec":{"containers":[{"name":"resize-container", "resources":{"limits":{"cpu":"365m"}}}]}}'
  pod/resize-pod patched
  ```

- Both the container and the pod cgroup values are set with 1ms precision:
  ```
  $ kubectl exec resize-pod -- cat /sys/fs/cgroup/cpu.max
  36500 100000
  $ cat /sys/fs/cgroup/kubelet.slice/kubelet-kubepods.slice/kubelet-kubepods-burstable.slice/kubelet-kubepods-burstable-pod68a17b59_0d31_40b2_ba86_ea43f3b2f05c.slice/cpu.max
  36500 100000
  ```


#### What did you expect to happen?
This roundup looks like the intended behavior of `systemd` cgroup driver because CPU quota is also rounded up when a pod is just created with 1ms precision CPU limits. However, I have the following concerns:
- We might need to confirm this tiny gap doesnâ€™t cause a similar issue to #128769 at resizing pods.
- We might need to clarify why the CPU quota of pod cgroup is sometimes not rounded up. This is especially necessary to complete #127192, which is going to add pod cgroup verification to resize tests.


#### How can we reproduce it (as minimally and precisely as possible)?

0. Use `systemd` cgroup driver and enable `InPlacePodVertialScaling`.
1. Resize CPU limits of a pod with 1ms precision.


#### Anything else we need to know?

_No response_

#### Kubernetes version

V1.32
<details>

```console
$ kubectl version
# paste output here
Client Version: v1.31.4
Kustomize Version: v5.4.2
Server Version: v1.32.0
```

</details>


#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†åœ¨å¯ç”¨`InPlacePodVerticalScaling`ç‰¹æ€§ï¼Œå¹¶ä½¿ç”¨`systemd` cgroupé©±åŠ¨æ—¶ï¼ŒPodçš„CPUé™åˆ¶åœ¨é…ç½®åˆ°cgroupsæ—¶ï¼Œä¸è®¡ç®—å€¼å­˜åœ¨åå·®ã€‚å…·ä½“è¡¨ç°ä¸ºï¼š

1. å½“è°ƒæ•´Podçš„CPUé™åˆ¶æ—¶ï¼ŒCPUé…é¢åœ¨ä¼ é€’ç»™`systemd`æ—¶ä¼šè¿›è¡Œå‘ä¸Šå–æ•´ï¼ˆä»¥10msä¸ºå•ä½ï¼‰ï¼Œå¯¼è‡´Podçš„cgroupä¸­é…ç½®çš„CPUé™é¢ç•¥é«˜äºé¢„æœŸå€¼ã€‚
2. æœ‰æ—¶ï¼Œå°¤å…¶æ˜¯åœ¨é™ä½CPUé™åˆ¶æ—¶ï¼ŒPodçš„cgroupä¸­çš„CPUé™é¢ä¸è®¡ç®—å€¼ä¸€è‡´ï¼Œæ²¡æœ‰å‘ä¸Šå–æ•´ï¼Œè¡¨ç°å‡ºä¸ä¸€è‡´æ€§ã€‚
3. è¿™ç§ä¸ä¸€è‡´å¯èƒ½å¯¼è‡´ç±»ä¼¼äºIssue [#128769](https://github.com/kubernetes/kubernetes/issues/128769)çš„é—®é¢˜ï¼Œå³å®é™…åˆ†é…çš„èµ„æºè¶…å‡ºé¢„æœŸï¼Œå¯èƒ½å½±å“ç³»ç»Ÿçš„èµ„æºç®¡ç†å’Œè°ƒåº¦ã€‚

æ½œåœ¨çš„å®‰å…¨é£é™©åˆ†æï¼š

- **èµ„æºè¶…é¢ä½¿ç”¨**ï¼šç”±äºCPUé™åˆ¶å€¼å‘ä¸Šå–æ•´ï¼ŒPodå¯èƒ½è·å¾—æ¯”é¢„æœŸæ›´å¤šçš„CPUèµ„æºã€‚åœ¨å¤šç§Ÿæˆ·ç¯å¢ƒä¸­ï¼Œè¿™å¯èƒ½å¯¼è‡´æŸä¸ªPodå ç”¨è¿‡å¤šçš„CPUèµ„æºï¼Œå½±å“å…¶ä»–Podçš„æ€§èƒ½ã€‚
- **æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰**ï¼šèµ„æºçš„è¿‡åº¦ä½¿ç”¨å¯èƒ½å¯¼è‡´å…¶ä»–æœåŠ¡æ€§èƒ½ä¸‹é™ï¼Œå­˜åœ¨æ½œåœ¨çš„DoSé£é™©ã€‚

ä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼š

- è¯¥é—®é¢˜çš„è§¦å‘éœ€è¦å…·å¤‡ä¿®æ”¹Podèµ„æºé™åˆ¶çš„æƒé™ï¼Œå³éœ€è¦å…·å¤‡åˆ›å»ºæˆ–ä¿®æ”¹Podçš„æƒé™ã€‚
- å‘ä¸Šå–æ•´çš„åå·®ç›¸å¯¹è¾ƒå°ï¼Œé€šå¸¸åœ¨å‡ ä¸ªæ¯«ç§’èŒƒå›´å†…ï¼Œå¯¹æ•´ä½“ç³»ç»Ÿå½±å“æœ‰é™ã€‚
- è¯¥é—®é¢˜æ›´å¤šåœ°å½±å“èµ„æºçš„ç²¾ç¡®åˆ†é…å’Œç®¡ç†ï¼Œå±äºæ€§èƒ½å’Œèµ„æºç®¡ç†é—®é¢˜ï¼Œè€Œéç›´æ¥çš„å®‰å…¨æ¼æ´ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

- **è§„åˆ™5**ï¼šå½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚
- **è§„åˆ™3**ï¼šæ ¹æ®CVSS 3.1è¯„åˆ†æ ‡å‡†ï¼Œè¯¥é—®é¢˜å¯èƒ½æ¶‰åŠåˆ°ç³»ç»Ÿå¯ç”¨æ€§çš„è½»å¾®å½±å“ï¼Œè¯„åˆ†åœ¨Highä»¥ä¸‹ã€‚
- **æ‹’ç»æœåŠ¡é£é™©**ï¼šè™½ç„¶å­˜åœ¨èµ„æºè¿‡åº¦ä½¿ç”¨çš„å¯èƒ½ï¼Œä½†åå·®è¾ƒå°ï¼Œä¸”éœ€è¦ç‰¹å®šæƒé™ã€‚

å› æ­¤ï¼Œç»¼åˆåˆ¤æ–­è¯¥Issueä¸º**ä½é£é™©**ï¼Œæ— éœ€æä¾›å¤ç°è„šæœ¬ã€‚

---


## Issue #129144 When a pod is removed, running execs stay open but frozen

- Issue é“¾æ¥ï¼š[#129144](https://github.com/kubernetes/kubernetes/issues/129144)

### Issue å†…å®¹

#### What happened?

When a pod is removed, running execs stay. Their standard streams are not closed.

#### What did you expect to happen?

The root processes of the execs are killed, so the running exec should be terminated by the kubeapi-server, or at least the stdout/stderr streams should be closed.

#### How can we reproduce it (as minimally and precisely as possible)?

- create a pod with a single container
- exec into that container with `kubectl exec` an interactive command (like a shell)
- remove that pod
- the `kubectl exec` is still running but is `frozen`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.31.0
```

</details>


#### Cloud provider

<details>
minikube or GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetesç¯å¢ƒä¸­ï¼Œå½“ä¸€ä¸ªPodè¢«åˆ é™¤åï¼Œé€šè¿‡`kubectl exec`å»ºç«‹çš„execä¼šè¯ä»ç„¶ä¿æŒæ‰“å¼€çŠ¶æ€ï¼Œä½†å¤„äºå†»ç»“çŠ¶æ€ï¼Œå…¶æ ‡å‡†è¾“å…¥è¾“å‡ºæµæœªè¢«å…³é—­ã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼š

1. **æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰é£é™©**ï¼šæ”»å‡»è€…å¯èƒ½é€šè¿‡é‡å¤æ­¤æ“ä½œï¼Œå ç”¨APIæœåŠ¡å™¨æˆ–èŠ‚ç‚¹çš„èµ„æºï¼Œå¯¼è‡´èµ„æºè€—å°½ï¼Œå½±å“é›†ç¾¤çš„å¯ç”¨æ€§ã€‚
2. **èµ„æºæ³„æ¼**ï¼šæœªå…³é—­çš„ä¼šè¯å¯èƒ½å¯¼è‡´ç³»ç»Ÿèµ„æºï¼ˆå¦‚æ–‡ä»¶æè¿°ç¬¦ã€å†…å­˜ç­‰ï¼‰æœªè¢«é‡Šæ”¾ã€‚

ç„¶è€Œï¼š

- **æƒé™è¦æ±‚é«˜**ï¼šæ‰§è¡Œæ­¤æ“ä½œéœ€è¦å…·å¤‡å¯¹é›†ç¾¤çš„è®¿é—®æƒé™ï¼Œèƒ½å¤Ÿåˆ›å»ºPodå¹¶æ‰§è¡Œ`kubectl exec`å‘½ä»¤ã€‚è¿™é€šå¸¸æ˜¯æˆæƒç”¨æˆ·æ‰å…·å¤‡çš„æƒé™ã€‚
- **å½±å“èŒƒå›´æœ‰é™**ï¼šç”±äºéœ€è¦é«˜æƒé™ï¼Œä¸”åªå½±å“è‡ªèº«æœ‰æƒé™çš„èµ„æºï¼Œå¯¹å…¶ä»–ç”¨æˆ·æˆ–ç³»ç»Ÿçš„å½±å“è¾ƒå°ã€‚

æ ¹æ®CVSS 3.1è¯„åˆ†æ ‡å‡†ï¼š

- æ”»å‡»å‘é‡ (AV)ï¼šç½‘ç»œ (N)
- æ”»å‡»å¤æ‚åº¦ (AC)ï¼šä½ (L)
- æƒé™è¦æ±‚ (PR)ï¼šé«˜ (H)
- ç”¨æˆ·äº¤äº’ (UI)ï¼šæ—  (N)
- å½±å“èŒƒå›´ (S)ï¼šæœªæ”¹å˜ (U)
- æœºå¯†æ€§ (C)ï¼šæ— å½±å“ (N)
- å®Œæ•´æ€§ (I)ï¼šæ— å½±å“ (N)
- å¯ç”¨æ€§ (A)ï¼šä½ (L)

ç»è®¡ç®—ï¼ŒåŸºç¡€è¯„åˆ†ä¸º3.2ï¼Œå±äºä½é£é™©èŒƒå›´ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

æ ¹æ®åˆ†æï¼Œè¯¥é—®é¢˜è¢«è¯„ä¼°ä¸ºä½é£é™©ï¼Œä¸éœ€è¦æä¾›å¤ç°è„šæœ¬ã€‚è™½ç„¶å­˜åœ¨èµ„æºæ³„æ¼å’Œæ½œåœ¨çš„DoSé£é™©ï¼Œä½†ç”±äºéœ€è¦é«˜æƒé™æ‰èƒ½å®æ–½æ”»å‡»ï¼Œä¸”å½±å“èŒƒå›´æœ‰é™ï¼Œä¸å±äºé«˜é£é™©èŒƒç•´ã€‚å»ºè®®åœ¨ç³»ç»Ÿä¸­æ·»åŠ æœºåˆ¶ï¼Œç¡®ä¿å½“Podè¢«åˆ é™¤æ—¶ï¼Œç›¸å…³çš„execä¼šè¯èƒ½è¢«æ­£å¸¸å…³é—­ï¼Œä»¥é‡Šæ”¾èµ„æºã€‚

---


# âœ… ä¸æ¶‰åŠå®‰å…¨é£é™©çš„ Issues (52 ä¸ª)

## Issue #129417 apiserver healthz should check etcd override endpoints

- Issue é“¾æ¥ï¼š[#129417](https://github.com/kubernetes/kubernetes/issues/129417)

### Issue å†…å®¹

#### What happened?

It seems the apiserver will fail bootstrap if the etcd override endpoint is not healthy.
But after the bootstrap completes, if the etcd override endpoint become unhealthy, the apiserver health check will still report OK while `kubectl get cs` will report etcd override endpoint is not healthy.

#### What did you expect to happen?

APIserver health check should report unhealthy when an etcd override endpoint is unhealthy.

#### How can we reproduce it (as minimally and precisely as possible)?

Run 2 etcd clusters: one for events and one for the other resources. Then configure apiserver to use the event etcd using the `--etcd-servers-overrides` flag. After the apiserver complete the bootstrap, kill the event etcd.
You will be able to see APIServer health check still reporting OK.

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.32. I think I have seen this issue in older version as well.

#### Cloud provider

N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†åœ¨APIServerä½¿ç”¨`--etcd-servers-overrides`å‚æ•°é…ç½®äº†é¢å¤–çš„etcdé›†ç¾¤ç”¨äºå­˜å‚¨äº‹ä»¶ç­‰èµ„æºçš„æƒ…å†µä¸‹ï¼Œå½“è¯¥etcdé›†ç¾¤å‡ºç°æ•…éšœï¼ˆä¾‹å¦‚è¢«å…³é—­ï¼‰åï¼ŒAPIServerçš„å¥åº·æ£€æŸ¥æ¥å£ï¼ˆhealthzï¼‰ä»ç„¶æŠ¥å‘ŠOKçŠ¶æ€ï¼Œè€Œé€šè¿‡`kubectl get cs`å¯ä»¥çœ‹åˆ°etcd override endpointçŠ¶æ€ä¸å¥åº·ã€‚

ä»é—®é¢˜æè¿°æ¥çœ‹ï¼Œè¿™æ˜¯APIServerå¥åº·æ£€æŸ¥åŠŸèƒ½çš„ç¼ºé™·ï¼Œå¯¼è‡´åœ¨etcd override endpointä¸å¥åº·æ—¶ï¼ŒAPIServerçš„å¥åº·æ£€æŸ¥æœªèƒ½å‡†ç¡®åæ˜ å®é™…çŠ¶æ€ã€‚è¿™å¯èƒ½ä¼šç»™é›†ç¾¤çš„è¿ç»´å’Œç›‘æ§å¸¦æ¥ä¸ä¾¿ï¼Œä½†å¹¶ä¸æ¶‰åŠä»»ä½•å®‰å…¨é£é™©ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. å¦‚æœIssueæè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

å› æ­¤ï¼Œè¯¥é—®é¢˜ä¸å±äºå®‰å…¨é—®é¢˜ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§é—®é¢˜ï¼Œå› æ­¤æ— éœ€æä¾›å¤ç°è„šæœ¬ã€‚

---


## Issue #129413 Link to Borg in readme is broken

- Issue é“¾æ¥ï¼š[#129413](https://github.com/kubernetes/kubernetes/issues/129413)

### Issue å†…å®¹

#### What happened?

I'm new to the Kubernetes repo and was looking through the docs and discovered a broken link to Borg in the readme file.

![image](https://github.com/user-attachments/assets/6acaa45b-fbd3-40d0-b08a-ee51d3f0cc9b)

#### What did you expect to happen?

See the page linked to Borg.

#### How can we reproduce it (as minimally and precisely as possible)?

Click on the link to Borg on the readme.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.2
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3```

</details>


#### Cloud provider

<details>

</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†åœ¨Kubernetesä»“åº“çš„READMEæ–‡ä»¶ä¸­ï¼ŒæŒ‡å‘Borgçš„é“¾æ¥å·²å¤±æ•ˆã€‚å½“ç”¨æˆ·å°è¯•ç‚¹å‡»è¯¥é“¾æ¥æ—¶ï¼Œæ— æ³•è®¿é—®åˆ°é¢„æœŸçš„é¡µé¢ã€‚Borgæ˜¯è°·æ­Œçš„å†…éƒ¨é›†ç¾¤ç®¡ç†ç³»ç»Ÿï¼ŒKuberneteså—å…¶å¯å‘è€Œå¼€å‘ã€‚

ä»Issueçš„æè¿°å’Œå†…å®¹æ¥çœ‹ï¼Œè¿™ä»…ä»…æ˜¯ä¸€ä¸ªæ–‡æ¡£ä¸­çš„é“¾æ¥å¤±æ•ˆé—®é¢˜ã€‚é“¾æ¥å¤±æ•ˆå¯èƒ½æ˜¯ç”±äºç›®æ ‡é¡µé¢å·²è¢«ç§»åŠ¨ã€åˆ é™¤ï¼Œæˆ–é“¾æ¥åœ°å€æ‹¼å†™é”™è¯¯å¯¼è‡´ã€‚è¿™ç§æƒ…å†µä¸ä¼šå¼•èµ·ä»»ä½•å®‰å…¨é£é™©ï¼Œä¸æ¶‰åŠä»£ç æˆ–ç³»ç»Ÿçš„æ¼æ´ï¼Œæ›´ä¸ä¼šå¯¼è‡´ä¿¡æ¯æ³„éœ²ã€æƒé™æå‡ã€è¿œç¨‹ä»£ç æ‰§è¡Œç­‰å®‰å…¨é—®é¢˜ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. Issueæè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Issueä»…æ¶‰åŠæ–‡æ¡£ä¸­ä¸€ä¸ªå¤±æ•ˆçš„é“¾æ¥ï¼Œå±äºä¸€èˆ¬çš„æ–‡æ¡£ç»´æŠ¤å’Œæ›´æ–°é—®é¢˜ï¼Œä¸æ¶‰åŠä»»ä½•å®‰å…¨é£é™©æˆ–æ¼æ´ã€‚å› æ­¤ï¼Œæ— éœ€æä¾›å¤ç°è„šæœ¬ã€‚

---


## Issue #129408 kubernetes 1.32: Informer with WatchClient fails to send events with Fakeclient

- Issue é“¾æ¥ï¼š[#129408](https://github.com/kubernetes/kubernetes/issues/129408)

### Issue å†…å®¹

#### What happened?

I tried to start an informer with WatchClient enabled in client go and i see this error

```
W1220 19:44:49.417766   26071 reflector.go:1044] k8s.io/client-go@v1.32/tools/cache/reflector.go:243: awaiting required bookmark event for initial events stream, no events received for 10.000196792s

```

and the above error keeps coming every 10 seconds.

The informer factory is stuck at this line

`xInformerFactory.WaitForCacheSync(ctx.Done())`





#### What did you expect to happen?

Watclist feature to work and Events to propagate when a resource is created.

#### How can we reproduce it (as minimally and precisely as possible)?

```
package main

import (
	"context"
	"flag"
	"fmt"
	"os"
    "time"

	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/runtime"
	utilruntime "k8s.io/apimachinery/pkg/util/runtime"
	"k8s.io/client-go/informers"
	"k8s.io/client-go/kubernetes/fake"
	"k8s.io/client-go/tools/cache"
	"k8s.io/klog/v2"
)

func init() {
	scheme := runtime.NewScheme()
	utilruntime.Must(corev1.AddToScheme(scheme))

}

func main() {
	os.Setenv("KUBE_FEATURE_WatchListClient", "true")
	client := fake.NewSimpleClientset()

	defer klog.Flush()
	flagSet := flag.NewFlagSet("test", flag.ExitOnError)
	klog.InitFlags(flagSet)
	_ = flagSet.Parse([]string{"--v", "6"})

	// Create an informer factory for the fake client
	informerFactory := informers.NewSharedInformerFactory(client, 0)

	// Get the Pod informer
	podInformer := informerFactory.Core().V1().Pods().Informer()

	// Add an event handler to the informer
	podInformer.AddEventHandler(&cache.ResourceEventHandlerFuncs{
		AddFunc: func(obj interface{}) {
			pod := obj.(*corev1.Pod)
			fmt.Printf("Pod added: %s/%s\n", pod.Namespace, pod.Name)
		},
		UpdateFunc: func(oldObj, newObj interface{}) {
			newPod := newObj.(*corev1.Pod)
			fmt.Printf("Pod updated: %s/%s\n", newPod.Namespace, newPod.Name)
		},
		DeleteFunc: func(obj interface{}) {
			pod := obj.(*corev1.Pod)
			fmt.Printf("Pod deleted: %s/%s\n", pod.Namespace, pod.Name)
		},
	})

	// Start the informer
	stopCh := make(chan struct{})
	defer close(stopCh)
	informerFactory.Start(stopCh)

	// Wait for cache sync
	if !cache.WaitForCacheSync(stopCh, podInformer.HasSynced) {
		fmt.Println("Failed to sync cache")
		return
	}
	// Use the client to create, update, or fetch resources
	pod := &corev1.Pod{
		ObjectMeta: metav1.ObjectMeta{
			Name:      "example-pod",
			Namespace: "default",
		},
		Spec: corev1.PodSpec{
			Containers: []corev1.Container{
				{
					Name:  "example-container",
					Image: "nginx",
				},
			},
		},
	}

	// Create a Pod
	_, err := client.CoreV1().Pods("default").Create(context.TODO(), pod, metav1.CreateOptions{})
	if err != nil {
		panic(err)
	}

	
	time.Sleep(time.Second * 5) 
	stopCh <- struct{}{}
}
```

go.mod file:

```
module myworks/watchlist-test

go 1.23.0

require (
	k8s.io/api v0.32.0
	k8s.io/apimachinery v0.32.0
	k8s.io/client-go v0.32.0
	k8s.io/klog/v2 v2.130.1
)

require (
	github.com/davecgh/go-spew v1.1.2-0.20180830191138-d8f796af33cc // indirect
	github.com/emicklei/go-restful/v3 v3.11.0 // indirect
	github.com/fxamacker/cbor/v2 v2.7.0 // indirect
	github.com/go-logr/logr v1.4.2 // indirect
	github.com/go-openapi/jsonpointer v0.21.0 // indirect
	github.com/go-openapi/jsonreference v0.20.2 // indirect
	github.com/go-openapi/swag v0.23.0 // indirect
	github.com/gogo/protobuf v1.3.2 // indirect
	github.com/golang/protobuf v1.5.4 // indirect
	github.com/google/gnostic-models v0.6.8 // indirect
	github.com/google/go-cmp v0.6.0 // indirect
	github.com/google/gofuzz v1.2.0 // indirect
	github.com/google/uuid v1.6.0 // indirect
	github.com/josharian/intern v1.0.0 // indirect
	github.com/json-iterator/go v1.1.12 // indirect
	github.com/mailru/easyjson v0.7.7 // indirect
	github.com/modern-go/concurrent v0.0.0-20180306012644-bacd9c7ef1dd // indirect
	github.com/modern-go/reflect2 v1.0.2 // indirect
	github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 // indirect
	github.com/pkg/errors v0.9.1 // indirect
	github.com/x448/float16 v0.8.4 // indirect
	golang.org/x/net v0.30.0 // indirect
	golang.org/x/oauth2 v0.23.0 // indirect
	golang.org/x/sys v0.26.0 // indirect
	golang.org/x/term v0.25.0 // indirect
	golang.org/x/text v0.19.0 // indirect
	golang.org/x/time v0.7.0 // indirect
	google.golang.org/protobuf v1.35.1 // indirect
	gopkg.in/evanphx/json-patch.v4 v4.12.0 // indirect
	gopkg.in/inf.v0 v0.9.1 // indirect
	gopkg.in/yaml.v3 v3.0.1 // indirect
	k8s.io/kube-openapi v0.0.0-20241105132330-32ad38e42d3f // indirect
	k8s.io/utils v0.0.0-20241104100929-3ea5e8cea738 // indirect
	sigs.k8s.io/json v0.0.0-20241010143419-9aa6b5e7a4b3 // indirect
	sigs.k8s.io/structured-merge-diff/v4 v4.4.2 // indirect
	sigs.k8s.io/yaml v1.4.0 // indirect
)
```



Error logs:
```
go run main.go
I1223 14:40:04.018354   35133 envvar.go:172] "Feature gate default state" feature="ClientsPreferCBOR" enabled=false
I1223 14:40:04.018890   35133 envvar.go:172] "Feature gate default state" feature="InformerResourceVersion" enabled=false
I1223 14:40:04.018897   35133 envvar.go:169] "Feature gate updated state" feature="WatchListClient" enabled=true
I1223 14:40:04.018901   35133 envvar.go:172] "Feature gate default state" feature="ClientsAllowCBOR" enabled=false
I1223 14:40:04.018930   35133 reflector.go:313] Starting reflector *v1.Pod (0s) from pkg/mod/k8s.io/client-go@v0.32.0/tools/cache/reflector.go:251
I1223 14:40:04.018968   35133 reflector.go:349] Listing and watching *v1.Pod from pkg/mod/k8s.io/client-go@v0.32.0/tools/cache/reflector.go:251
W1223 14:40:14.020097   35133 reflector.go:1052] pkg/mod/k8s.io/client-go@v0.32.0/tools/cache/reflector.go:251: awaiting required bookmark event for initial events stream, no events received for 10.000426333s
W1223 14:40:24.019566   35133 reflector.go:1052] pkg/mod/k8s.io/client-go@v0.32.0/tools/cache/reflector.go:251: awaiting required bookmark event for initial events stream, no events received for 20.000542791s
```




#### Anything else we need to know?


change the env var to false and you see things are working. 

```
os.Setenv("KUBE_FEATURE_WatchListClient", "false")
```

Logs:

```
go run main.go
I1223 14:45:47.374375   39780 envvar.go:172] "Feature gate default state" feature="ClientsAllowCBOR" enabled=false
I1223 14:45:47.374853   39780 envvar.go:172] "Feature gate default state" feature="ClientsPreferCBOR" enabled=false
I1223 14:45:47.374859   39780 envvar.go:172] "Feature gate default state" feature="InformerResourceVersion" enabled=false
I1223 14:45:47.374864   39780 envvar.go:169] "Feature gate updated state" feature="WatchListClient" enabled=false
I1223 14:45:47.374889   39780 reflector.go:313] Starting reflector *v1.Pod (0s) from pkg/mod/k8s.io/client-go@v0.32.0/tools/cache/reflector.go:251
I1223 14:45:47.374927   39780 reflector.go:349] Listing and watching *v1.Pod from pkg/mod/k8s.io/client-go@v0.32.0/tools/cache/reflector.go:251
I1223 14:45:47.375349   39780 reflector.go:376] Caches populated for *v1.Pod from pkg/mod/k8s.io/client-go@v0.32.0/tools/cache/reflector.go:251
Pod added: default/example-pod
```

#### Kubernetes version

Client Version: v1.32.0

#### Cloud provider

Local

#### OS version

MacOS
Darwin Kernel Version 23.6.0

#### Install tools

golang 

```
go version
go version go1.23.0 darwin/arm64
```

#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetes 1.32ç‰ˆæœ¬ä¸­ï¼Œä½¿ç”¨client-goåº“çš„Fake Clientè¿›è¡Œæµ‹è¯•æ—¶ï¼Œå½“å¯ç”¨äº†`WatchListClient`ç‰¹æ€§ï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡`KUBE_FEATURE_WatchListClient`è®¾ç½®ä¸º`true`ï¼‰åï¼ŒInformeræ— æ³•æ­£å¸¸å·¥ä½œï¼Œå‡ºç°äº†ç­‰å¾…bookmarkäº‹ä»¶çš„é”™è¯¯ï¼Œå¹¶ä¸”ç¼“å­˜åŒæ­¥å¡ä½ã€‚è¿™å¯¼è‡´äº†Informeræ— æ³•æ¥æ”¶åˆ°èµ„æºçš„äº‹ä»¶é€šçŸ¥ã€‚

ä»é—®é¢˜æè¿°å’Œæä¾›çš„ç¤ºä¾‹ä»£ç æ¥çœ‹ï¼Œè¿™æ˜¯ä¸€ä¸ªåœ¨å¯ç”¨`WatchListClient`ç‰¹æ€§åï¼ŒFake Clientä¸Informerä¹‹é—´çš„å…¼å®¹æ€§é—®é¢˜ï¼Œå¯¼è‡´æ— æ³•æ­£ç¡®å¤„ç†äº‹ä»¶æµã€‚

æ•´ä¸ªIssueä¸­æ²¡æœ‰æ¶‰åŠä»»ä½•å®‰å…¨æ–¹é¢çš„å†…å®¹ï¼Œä¾‹å¦‚ï¼šæƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œã€ä¿¡æ¯æ³„éœ²ã€æ‹’ç»æœåŠ¡æ”»å‡»ç­‰ã€‚å› æ­¤ï¼Œè¿™åªæ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ç¼ºé™·æˆ–Bugï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Issueçš„é—®é¢˜åœ¨äºå¯ç”¨äº†`WatchListClient`ç‰¹æ€§åï¼ŒFake Clientæ— æ³•ä¸Informeræ­£ç¡®åä½œï¼Œå¯¼è‡´Informeræ— æ³•æ¥æ”¶åˆ°èµ„æºäº‹ä»¶ã€‚è¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§çš„é—®é¢˜ï¼Œå¯èƒ½éœ€è¦åœ¨ä»£ç ä¸­ä¿®å¤Fake Clientä¸`WatchListClient`ç‰¹æ€§çš„å…¼å®¹æ€§ã€‚ç”±äºæ²¡æœ‰æ¶‰åŠä»»ä½•å®‰å…¨é£é™©ï¼Œä¸ä¼šå¯¹ç³»ç»Ÿçš„å®‰å…¨æ€§é€ æˆå½±å“ï¼Œå› æ­¤é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

---


## Issue #129405 Pending should not be handled as Error in PreFilter

- Issue é“¾æ¥ï¼š[#129405](https://github.com/kubernetes/kubernetes/issues/129405)

### Issue å†…å®¹

#### What happened?

https://github.com/kubernetes/kubernetes/blob/35f584187a6d1250191aa24b0dcf735350f57508/pkg/scheduler/framework/runtime/framework.go#L735-L750

Kube Scheduler will return framework.Error when any plugin return Pending in PreFilter

#### What did you expect to happen?

Kube Scheduler should return Pending if any plugin return Pending in PreFilter.

#### How can we reproduce it (as minimally and precisely as possible)?

Implement a scheduler plugin which return Pending in PreFilter.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.32
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†åœ¨Kubernetesè°ƒåº¦å™¨çš„PreFilteré˜¶æ®µï¼Œå½“ä»»ä½•æ’ä»¶è¿”å›`Pending`çŠ¶æ€æ—¶ï¼Œè°ƒåº¦å™¨ä¼šå°†å…¶é”™è¯¯åœ°å¤„ç†ä¸º`Error`çŠ¶æ€ã€‚æ ¹æ®é“¾æ¥çš„ä»£ç ï¼Œå¯ä»¥çœ‹åˆ°è°ƒåº¦å™¨åœ¨å¤„ç†æ’ä»¶è¿”å›çš„çŠ¶æ€æ—¶ï¼Œæ²¡æœ‰æ­£ç¡®åŒºåˆ†`Pending`å’Œ`Error`ï¼Œå¯¼è‡´é¢„æœŸè¿”å›`Pending`çŠ¶æ€çš„æ’ä»¶è¢«è®¤ä¸ºæ˜¯å‘ç”Ÿäº†é”™è¯¯ã€‚

ä»é—®é¢˜æè¿°æ¥çœ‹ï¼Œè¿™æ˜¯ä¸€ä¸ªè°ƒåº¦å™¨é€»è¾‘å¤„ç†ä¸Šçš„é—®é¢˜ï¼Œå±äºåŠŸèƒ½æ€§é”™è¯¯ã€‚è¿™ä¸ªé—®é¢˜å¯èƒ½ä¼šå¯¼è‡´è°ƒåº¦å™¨çš„è¡Œä¸ºä¸æ’ä»¶çš„è®¾è®¡é¢„æœŸä¸ä¸€è‡´ï¼Œä»è€Œå½±å“è°ƒåº¦å†³ç­–ã€‚ä½†æ•´ä¸ªè¿‡ç¨‹ä¸­ï¼Œå¹¶æœªæ¶‰åŠä»»ä½•å®‰å…¨æ–¹é¢çš„æ¼æ´æˆ–é£é™©ï¼š

- **ä¿¡æ¯æ³„éœ²**ï¼šä¸å­˜åœ¨ï¼Œé—®é¢˜ä¸æ¶‰åŠæ•æ„Ÿä¿¡æ¯çš„å¤„ç†æˆ–ä¼ è¾“ã€‚
- **æƒé™æå‡**ï¼šä¸å­˜åœ¨ï¼Œé—®é¢˜ä¸æ¶‰åŠæƒé™æ§åˆ¶æˆ–è¶Šæƒæ“ä½œã€‚
- **ä»£ç /å‘½ä»¤æ‰§è¡Œ**ï¼šä¸å­˜åœ¨ï¼Œé—®é¢˜ä¸æ¶‰åŠæ‰§è¡Œä»»æ„ä»£ç æˆ–å‘½ä»¤çš„é£é™©ã€‚
- **æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰**ï¼šè™½ç„¶è°ƒåº¦å™¨çš„é”™è¯¯å¤„ç†å¯èƒ½å½±å“åˆ°æœåŠ¡çš„æ­£å¸¸è°ƒåº¦ï¼Œä½†ç”±äºå¿…é¡»ç¼–å†™ç‰¹å®šçš„æ’ä»¶è¿”å›`Pending`çŠ¶æ€æ‰èƒ½è§¦å‘ï¼Œä¸”éœ€è¦ç®¡ç†å‘˜æƒé™æ¥éƒ¨ç½²æ’ä»¶ï¼Œå› æ­¤ä¸å±äºé«˜é£é™©çš„DoSæ”»å‡»ã€‚

æ ¹æ®ä»¥ä¸Šåˆ†æï¼Œé—®é¢˜å±äºåŠŸèƒ½æ€§é”™è¯¯ï¼Œå¯¹ç³»ç»Ÿçš„å®‰å…¨æ€§æ²¡æœ‰ç›´æ¥å½±å“ã€‚

---


## Issue #129403 killPodOptions not showing up properly

- Issue é“¾æ¥ï¼š[#129403](https://github.com/kubernetes/kubernetes/issues/129403)

### Issue å†…å®¹

#### What happened?

``` shell
I1226 15:13:00.040437 4101130 kubelet_pods.go:473] "Clean up probes for terminated pods"
I1226 15:13:00.040470 4101130 kubelet_pods.go:545] "Clean up containers for orphaned pod we had not seen before" podUID="2a07603d-dc01-4897-84f4-716127ffe399" killPodOptions="<internal error: json: unsupported type: chan<- struct {}>"
I1226 15:13:00.040502 4101130 pod_workers.go:727] "Pod is being synced for the first time" pod="kube-system/kindnet-d8bcd" podUID="2a07603d-dc01-4897-84f4-716127ffe399" updateType="kill"
I1226 15:13:00.040524 4101130 pod_workers.go:808] "Pod is orphaned and must be torn down" pod="kube-system/kindnet-d8bcd" podUID="2a07603d-dc01-4897-84f4-716127ffe399" updateType="kill"
I1226 15:13:00.040551 4101130 pod_workers.go:913] "Notifying pod of pending update" pod="kube-system/kindnet-d8bcd" podUID="2a07603d-dc01-4897-84f4-716127ffe399" workType="terminating"
```

#### What did you expect to happen?

`killPodOptions` correctly displays parameter information

#### How can we reproduce it (as minimally and precisely as possible)?

N/A

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
v1.32.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®Issueçš„æè¿°ï¼Œé—®é¢˜å‡ºç°åœ¨`killPodOptions`å‚æ•°åœ¨æ—¥å¿—ä¸­æœªæ­£ç¡®æ˜¾ç¤ºï¼Œè¾“å‡ºäº†`<internal error: json: unsupported type: chan<- struct {}>`çš„é”™è¯¯ä¿¡æ¯ã€‚è¿™è¡¨æ˜åœ¨åºåˆ—åŒ–`killPodOptions`å‚æ•°ä¸ºJSONæ ¼å¼æ—¶ï¼Œé‡åˆ°äº†ä¸æ”¯æŒçš„ç±»å‹`chan<- struct {}`ï¼Œå¯¼è‡´åºåˆ—åŒ–å¤±è´¥ã€‚

ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼š

- **ä¿¡æ¯æ³„éœ²**ï¼šæ—¥å¿—ä¸­æ³„éœ²çš„ä¿¡æ¯ä»…ä¸ºå†…éƒ¨é”™è¯¯å’Œæ•°æ®ç±»å‹æè¿°ï¼Œæ²¡æœ‰åŒ…å«ä»»ä½•æ•æ„Ÿæ•°æ®æˆ–å‡­æ®ä¿¡æ¯ã€‚
- **æ”»å‡»é¢**ï¼šè¯¥é”™è¯¯å±äºå†…éƒ¨æ—¥å¿—è®°å½•é—®é¢˜ï¼Œå¤–éƒ¨æ”»å‡»è€…æ— æ³•é€šè¿‡æ­¤é”™è¯¯ç›´æ¥è¿›è¡Œæ”»å‡»ï¼Œä¹Ÿæ— æ³•åˆ©ç”¨æ­¤ä¿¡æ¯è¿›è¡Œè¿›ä¸€æ­¥çš„æ¸—é€æˆ–æ”»å‡»ã€‚
- **å½±å“èŒƒå›´**ï¼šé—®é¢˜ä»…å½±å“æ—¥å¿—çš„æ­£ç¡®æ€§ï¼Œå¯¹ç³»ç»ŸåŠŸèƒ½å’Œå®‰å…¨æ€§æ²¡æœ‰ç›´æ¥å½±å“ã€‚

æ ¹æ®ä»¥ä¸Šåˆ†æï¼Œè¯¥é—®é¢˜å¹¶æœªå¼•å…¥æ–°çš„å®‰å…¨é£é™©ï¼Œå±äºåŠŸèƒ½æ€§ç¼ºé™·æˆ–æ—¥å¿—è®°å½•é—®é¢˜ã€‚

---


## Issue #129402 DRA: Pod termination is stuck when DRA Driver is stopped

- Issue é“¾æ¥ï¼š[#129402](https://github.com/kubernetes/kubernetes/issues/129402)

### Issue å†…å®¹

#### What happened?

A status of Pod allocated some device remains as terminating when the DRA Driver is stopped.
I don't know this is intentional or a bug.

#### What did you expect to happen?

A Pod is completely terminated.

#### How can we reproduce it (as minimally and precisely as possible)?

We can reproduce it using [dra-example-driver](https://github.com/kubernetes-sigs/dra-example-driver).

#### Summary
1. Install the `DRA Driver`(dra-example-driver) and create a `DeviceClass`.
1. Create a `ResourceClaimTemplate`.
1. Deploy a Pod allocated some device via the `ResourceClaimTemplate`.
1. Stop the DRA Driver.
1. Delete the Pod.
1. The Pod remains as terminating.

#### Procedure

<details>

Install the `DRA Driver` and create a `DeviceClass` by following [dra-example-driver demo](https://github.com/kubernetes-sigs/dra-example-driver?tab=readme-ov-file#demo).

Create a `ResourceClaimTemplate`.

`resource-claim-template-0.yaml`
```yaml
apiVersion: resource.k8s.io/v1beta1
kind: ResourceClaimTemplate
metadata:
  name: single-gpu
spec:
  spec:
    devices:
      requests:
      - name: gpu
        deviceClassName: gpu.example.com
```
```bash
$ kubectl apply -f resource-claim-template-0.yaml
```

Deploy a Pod allocated some device via the `ResourceClaimTemplate`.

`sample-pod-0.yaml`
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sample-pod-0
  labels:
    app: sample-pod-0
spec:
  containers:
  - name: ctr0
    image: ubuntu:22.04
    command: ["bash", "-c"]
    args: ["export; trap 'exit 0' TERM; sleep 9999 & wait"]
    resources:
      claims:
      - name: gpu
  resourceClaims:
  - name: gpu
    resourceClaimTemplateName: single-gpu
```

```bash
$ kubectl apply -f sample-pod-0.yaml
```


The current status of cluster is as follows.
```bash
$ kubectl get deviceclasses,resourceclaimtemplates,resourceclaim,pods
NAME                                          AGE
deviceclass.resource.k8s.io/gpu.example.com   54m

NAME                                               AGE
resourceclaimtemplate.resource.k8s.io/single-gpu   85s

NAME                                                   STATE                AGE
resourceclaim.resource.k8s.io/sample-pod-0-gpu-hxsn7   allocated,reserved   77s

NAME               READY   STATUS    RESTARTS   AGE
pod/sample-pod-0   1/1     Running   0          77s
```

Stop the DRA Driver.
In this case, we can uninstall the `dra-example-driver` via helm.
```bash
$ helm -n dra-example-driver uninstall dra-example-driver
```

Delete the Pod and the status remains as terminating.
```bash
$ kubectl delete po sample-pod-0
pod "sample-pod-0" deleted
(stucking...)

$ kubectl get pod
NAME           READY   STATUS        RESTARTS   AGE
sample-pod-0   0/1     Terminating   0          17m
```

</details>

#### Anything else we need to know?

The kubelet log shows the following error. 
```bash
# journalctl -xu kubelet
...
Dec 26 05:35:41 kind-v1.32.0-worker kubelet[231]: I1226 05:35:41.323475     231 kubelet.go:2490] "SyncLoop DELETE" source="api" pods=["default/sample-pod-0"]
Dec 26 05:35:41 kind-v1.32.0-worker kubelet[231]: I1226 05:35:41.323611     231 kuberuntime_container.go:809] "Killing container with a grace period" pod="default/sample-pod-0" podUID="5598e74f-08ff-40ab-aba3-fa811874f9dc" containerName="ctr0" containerID="containerd://f42f59f5684ff9be1a0ee57d70231456530e40e87e652ddf0b72c7639a544a05" gracePeriod=30
Dec 26 05:35:41 kind-v1.32.0-worker kubelet[231]: E1226 05:35:41.415637     231 pod_workers.go:1301] "Error syncing pod, skipping" err="get gRPC client for DRA driver gpu.example.com: plugin name gpu.example.com not found in the list of registered DRA plugins" pod="default/sample-pod-0" podUID="5598e74f-08ff-40ab-aba3-fa811874f9dc"
Dec 26 05:35:42 kind-v1.32.0-worker kubelet[231]: I1226 05:35:42.032843     231 generic.go:358] "Generic (PLEG): container finished" podID="5598e74f-08ff-40ab-aba3-fa811874f9dc" containerID="f42f59f5684ff9be1a0ee57d70231456530e40e87e652ddf0b72c7639a544a05" exitCode=0
Dec 26 05:35:42 kind-v1.32.0-worker kubelet[231]: I1226 05:35:42.032885     231 kubelet.go:2506] "SyncLoop (PLEG): event for pod" pod="default/sample-pod-0" event={"ID":"5598e74f-08ff-40ab-aba3-fa811874f9dc","Type":"ContainerDied","Data":"f42f59f5684ff9be1a0ee57d70231456530e40e87e652ddf0b72c7639a544a05"}
Dec 26 05:35:42 kind-v1.32.0-worker kubelet[231]: I1226 05:35:42.032901     231 kubelet.go:2506] "SyncLoop (PLEG): event for pod" pod="default/sample-pod-0" event={"ID":"5598e74f-08ff-40ab-aba3-fa811874f9dc","Type":"ContainerDied","Data":"220eab5b9bc2277f4ac7777dabb82a92bee9d3d17bfa42f92d204cb9d85c936d"}
Dec 26 05:35:42 kind-v1.32.0-worker kubelet[231]: I1226 05:35:42.032907     231 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="220eab5b9bc2277f4ac7777dabb82a92bee9d3d17bfa42f92d204cb9d85c936d"
Dec 26 05:35:42 kind-v1.32.0-worker kubelet[231]: I1226 05:35:42.032934     231 util.go:48] "No ready sandbox for pod can be found. Need to start a new one" pod="default/sample-pod-0"
Dec 26 05:35:42 kind-v1.32.0-worker kubelet[231]: E1226 05:35:42.044794     231 pod_workers.go:1301] "Error syncing pod, skipping" err="get gRPC client for DRA driver gpu.example.com: plugin name gpu.example.com not found in the list of registered DRA plugins" pod="default/sample-pod-0" podUID="5598e74f-08ff-40ab-aba3-fa811874f9dc"
Dec 26 05:35:53 kind-v1.32.0-worker kubelet[231]: I1226 05:35:53.469847     231 util.go:48] "No ready sandbox for pod can be found. Need to start a new one" pod="default/sample-pod-0"
Dec 26 05:35:53 kind-v1.32.0-worker kubelet[231]: E1226 05:35:53.482748     231 pod_workers.go:1301] "Error syncing pod, skipping" err="get gRPC client for DRA driver gpu.example.com: plugin name gpu.example.com not found in the list of registered DRA plugins" pod="default/sample-pod-0" podUID="5598e74f-08ff-40ab-aba3-fa811874f9dc"
Dec 26 05:36:08 kind-v1.32.0-worker kubelet[231]: I1226 05:36:08.468746     231 util.go:48] "No ready sandbox for pod can be found. Need to start a new one" pod="default/sample-pod-0"
Dec 26 05:36:08 kind-v1.32.0-worker kubelet[231]: E1226 05:36:08.480012     231 pod_workers.go:1301] "Error syncing pod, skipping" err="get gRPC client for DRA driver gpu.example.com: plugin name gpu.example.com not found in the list of registered DRA plugins" pod="default/sample-pod-0" podUID="5598e74f-08ff-40ab-aba3-fa811874f9dc"
```

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.32.0
```

</details>


#### Cloud provider

<details>
none
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

```console
$ kind version
kind v0.26.0 go1.23.4 linux/amd64
```

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨DRAï¼ˆDynamic Resource Allocationï¼‰æ—¶ï¼Œå½“DRA Driverè¢«åœæ­¢åï¼Œå·²åˆ†é…è®¾å¤‡çš„Podåœ¨åˆ é™¤æ—¶ä¼šå¡åœ¨"Terminating"çŠ¶æ€ï¼Œæ— æ³•æ­£å¸¸ç»ˆæ­¢ã€‚æ ¹æ®æä¾›çš„æ—¥å¿—ä¿¡æ¯ï¼Œkubeletæ— æ³•æ‰¾åˆ°å·²æ³¨å†Œçš„DRAæ’ä»¶ï¼Œå¯¼è‡´åœ¨åŒæ­¥PodçŠ¶æ€æ—¶å‡ºç°é”™è¯¯ï¼š

```
Error syncing pod, skipping" err="get gRPC client for DRA driver gpu.example.com: plugin name gpu.example.com not found in the list of registered DRA plugins"
```

è¿™æ˜¯å› ä¸ºkubeletåœ¨åˆ é™¤Podæ—¶éœ€è¦ä¸DRA Driveré€šä¿¡ä»¥é‡Šæ”¾è®¾å¤‡èµ„æºï¼Œä½†ç”±äºDRA Driverå·²è¢«åœæ­¢ï¼Œkubeletæ— æ³•å®Œæˆè¯¥æ“ä½œï¼Œä»è€Œå¯¼è‡´Podä¸€ç›´å¤„äº"Terminating"çŠ¶æ€ã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼Œè¿™ä¸ªé—®é¢˜å±äºç³»ç»ŸåŠŸèƒ½æ€§ç¼ºé™·æˆ–Bugï¼Œæœªæ¶‰åŠä»»ä½•å®‰å…¨æ¼æ´ã€‚è¯¥é—®é¢˜ä¸ä¼šå¯¼è‡´æœªæˆæƒè®¿é—®ã€æƒé™æå‡ã€æ•°æ®æ³„éœ²ã€è¿œç¨‹ä»£ç æ‰§è¡Œç­‰å®‰å…¨é£é™©ã€‚

---


## Issue #129392 Creation fails when the CRD property is ServiceAccount

- Issue é“¾æ¥ï¼š[#129392](https://github.com/kubernetes/kubernetes/issues/129392)

### Issue å†…å®¹

#### What happened?

My CRD property use the ServiceAccount type, When I use `kubelet apply -f <crds>` to create CRDs generated by controller-gen, get an error message: 
```text
Required value: this property is in x-kubernetes-list-map-keys, so it must have a default or be a required property.
```

This is because ServiceAccount.Secret.Name property is in x-kubernetes-list-map-keys, but it is not have a default and not be a required property.

Structure definition:
```go
// +genclient
// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object
// +kubebuilder:...
type Foo struct {
    ServiceAccount corev1.ServiceAccount `json:"serviceAccount,omitempty"`
    ....
}
```

#### What did you expect to happen?

I expect to generate CRDs that can be created successfully.

Solutions:
- Set ObjectReference.Name to required. I think the name is required in most scenarios.
- Add default value annotations  (`//+default=""` and `//+kubebuilder:default=""`) for ObjectReference.Name.
- Modify the ServiceAccount.Secrets type to []SecretReference and set SecretReference.name to required.

#### How can we reproduce it (as minimally and precisely as possible)?

Refer to "What happened"

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

v1.30+

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨controller-genç”ŸæˆCRDæ—¶ï¼ŒCRDä¸­çš„å±æ€§ä½¿ç”¨äº†`corev1.ServiceAccount`ç±»å‹ï¼Œå¯¼è‡´åœ¨ä½¿ç”¨`kubectl apply -f <crds>`åˆ›å»ºCRDæ—¶å‡ºç°é”™è¯¯ã€‚é”™è¯¯ä¿¡æ¯ä¸ºï¼š
```
Required value: this property is in x-kubernetes-list-map-keys, so it must have a default or be a required property.
```
è¿™æ˜¯å› ä¸º`ServiceAccount.Secret.Name`å±æ€§åœ¨`x-kubernetes-list-map-keys`ä¸­ï¼Œä½†æ²¡æœ‰é»˜è®¤å€¼ä¹Ÿä¸æ˜¯å¿…éœ€å±æ€§ï¼Œå¯¼è‡´CRDçš„SchemaéªŒè¯å¤±è´¥ã€‚

è¯¥é—®é¢˜æœ¬è´¨ä¸Šæ˜¯CRDå®šä¹‰å’Œç”Ÿæˆè¿‡ç¨‹ä¸­çš„æŠ€æœ¯é—®é¢˜ï¼Œæ¶‰åŠåˆ°Kubernetes CRDçš„Schemaçº¦æŸå’Œcontroller-genå·¥å…·çš„ä½¿ç”¨ï¼Œå¹¶æœªæ¶‰åŠä»»ä½•å®‰å…¨æ¼æ´æˆ–é£é™©ï¼Œä¾‹å¦‚å‘½ä»¤æ‰§è¡Œã€æƒé™æå‡ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ç­‰ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1. **æ ‡å‡†2**ï¼šå¦‚æœè¯¥issueæè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

å› æ­¤ï¼Œè¯¥Issueä¸å±äºå®‰å…¨é—®é¢˜ï¼Œä¸å­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

---


## Issue #129382 Swap stats is not shown as part of the metrics/resource endpoint

- Issue é“¾æ¥ï¼š[#129382](https://github.com/kubernetes/kubernetes/issues/129382)

### Issue å†…å®¹

#### What happened?

The following:
```
> kubectl get --raw "/api/v1/nodes/k8s-dev-worker/proxy/metrics/resource"
# HELP container_cpu_usage_seconds_total [STABLE] Cumulative cpu time consumed by the container in core-seconds
# TYPE container_cpu_usage_seconds_total counter
container_cpu_usage_seconds_total{container="kindnet-cni",namespace="kube-system",pod="kindnet-ndczz"} 1.121325 1735032838055
container_cpu_usage_seconds_total{container="kube-proxy",namespace="kube-system",pod="kube-proxy-l5jhs"} 1.100665 1735032838936
container_cpu_usage_seconds_total{container="metrics-server",namespace="kube-system",pod="metrics-server-8598789fdb-nw6cq"} 7.333964 1735032837430
# HELP container_memory_working_set_bytes [STABLE] Current working set of the container in bytes
# TYPE container_memory_working_set_bytes gauge
container_memory_working_set_bytes{container="kindnet-cni",namespace="kube-system",pod="kindnet-ndczz"} 3.2923648e+07 1735032838055
container_memory_working_set_bytes{container="kube-proxy",namespace="kube-system",pod="kube-proxy-l5jhs"} 4.0628224e+07 1735032838936
container_memory_working_set_bytes{container="metrics-server",namespace="kube-system",pod="metrics-server-8598789fdb-nw6cq"} 4.7026176e+07 1735032837430
# HELP container_start_time_seconds [STABLE] Start time of the container since unix epoch in seconds
# TYPE container_start_time_seconds gauge
container_start_time_seconds{container="kindnet-cni",namespace="kube-system",pod="kindnet-ndczz"} 1.7350309825441425e+09
container_start_time_seconds{container="kube-proxy",namespace="kube-system",pod="kube-proxy-l5jhs"} 1.7350309819809804e+09
container_start_time_seconds{container="metrics-server",namespace="kube-system",pod="metrics-server-8598789fdb-nw6cq"} 1.7350309993126562e+09
# HELP node_cpu_usage_seconds_total [STABLE] Cumulative cpu time consumed by the node in core-seconds
# TYPE node_cpu_usage_seconds_total counter
node_cpu_usage_seconds_total 71.41304 1735032832343
# HELP node_memory_working_set_bytes [STABLE] Current working set of the node in bytes
# TYPE node_memory_working_set_bytes gauge
node_memory_working_set_bytes 2.134016e+08 1735032832343
# HELP pod_cpu_usage_seconds_total [STABLE] Cumulative cpu time consumed by the pod in core-seconds
# TYPE pod_cpu_usage_seconds_total counter
pod_cpu_usage_seconds_total{namespace="kube-system",pod="kindnet-ndczz"} 1.145182 1735032830497
pod_cpu_usage_seconds_total{namespace="kube-system",pod="kube-proxy-l5jhs"} 1.108676 1735032837395
pod_cpu_usage_seconds_total{namespace="kube-system",pod="metrics-server-8598789fdb-nw6cq"} 7.336168 1735032831254
# HELP pod_memory_working_set_bytes [STABLE] Current working set of the pod in bytes
# TYPE pod_memory_working_set_bytes gauge
pod_memory_working_set_bytes{namespace="kube-system",pod="kindnet-ndczz"} 3.3222656e+07 1735032830497
pod_memory_working_set_bytes{namespace="kube-system",pod="kube-proxy-l5jhs"} 4.0914944e+07 1735032837395
pod_memory_working_set_bytes{namespace="kube-system",pod="metrics-server-8598789fdb-nw6cq"} 4.732928e+07 1735032831254
# HELP resource_scrape_error [STABLE] 1 if there was an error while getting container metrics, 0 otherwise
# TYPE resource_scrape_error gauge
resource_scrape_error 0
```

As can be seen, swap stats is not shown here:
```
> kubectl get --raw "/api/v1/nodes/k8s-dev-worker/proxy/metrics/resource" | grep -i swap
> 
```

#### What did you expect to happen?

Swap to be included in metrics/resource endpoint stats.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Bring up a cluster
2. Install metrics server
3. Run `kubectl get --raw "/api/v1/nodes/<NODE-NAME>/proxy/metrics/resource" | grep -i swap`

#### Anything else we need to know?

Swap stats were introduced in this PR: https://github.com/kubernetes/kubernetes/pull/118865.
It also shows the expected output.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.32.0

```

</details>


#### Cloud provider

<details>
Using a kind development cluster
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Fedora Linux"
VERSION="40 (Forty)"
ID=fedora
VERSION_ID=40
VERSION_CODENAME=""
PLATFORM_ID="platform:f40"
PRETTY_NAME="Fedora Linux 40 (Forty)"
ANSI_COLOR="0;38;2;60;110;180"
LOGO=fedora-logo-icon
CPE_NAME="cpe:/o:fedoraproject:fedora:40"
DEFAULT_HOSTNAME="fedora"
HOME_URL="https://fedoraproject.org/"
DOCUMENTATION_URL="https://docs.fedoraproject.org/en-US/fedora/f40/system-administrators-guide/"
SUPPORT_URL="https://ask.fedoraproject.org/"
BUG_REPORT_URL="https://bugzilla.redhat.com/"
REDHAT_BUGZILLA_PRODUCT="Fedora"
REDHAT_BUGZILLA_PRODUCT_VERSION=40
REDHAT_SUPPORT_PRODUCT="Fedora"
REDHAT_SUPPORT_PRODUCT_VERSION=40
SUPPORT_END=2025-05-13

$ uname -a
Linux fedora40-eve 6.12.5-100.fc40.x86_64 #1 SMP PREEMPT_DYNAMIC Mon Dec 16 15:00:58 UTC 2024 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kind
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
> crio version
Version:        1.31.3
GitCommit:      5865976a62cd3abd1b56ec21ea4fde19a730fe87
GitCommitDate:  2024-12-02T10:48:49Z
GitTreeState:   dirty
BuildDate:      1970-01-01T00:00:00Z
GoVersion:      go1.22.5
Compiler:       gc
Platform:       linux/amd64
Linkmode:       static
BuildTags:
  static
  netgo
  osusergo
  exclude_graphdriver_btrfs
  seccomp
  apparmor
  selinux
  exclude_graphdriver_devicemapper
LDFlags:          unknown
SeccompEnabled:   true
AppArmorEnabled:  false

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†åœ¨ä½¿ç”¨`kubectl get --raw "/api/v1/nodes/<NODE-NAME>/proxy/metrics/resource"`å‘½ä»¤è·å–èŠ‚ç‚¹èµ„æºæŒ‡æ ‡æ—¶ï¼Œæ²¡æœ‰æ˜¾ç¤ºswapï¼ˆäº¤æ¢åˆ†åŒºï¼‰çš„ç»Ÿè®¡ä¿¡æ¯ã€‚æäº¤è€…æœŸæœ›åœ¨è¯¥æŒ‡æ ‡ç«¯ç‚¹ä¸­åŒ…å«swapçš„ç»Ÿè®¡æ•°æ®ï¼Œä»¥ä¾¿æ›´å…¨é¢åœ°ç›‘æ§èŠ‚ç‚¹çš„èµ„æºä½¿ç”¨æƒ…å†µã€‚

æ ¹æ®é—®é¢˜æè¿°ï¼Œè¿™å±äºåŠŸèƒ½ç¼ºå¤±æˆ–æ”¹è¿›çš„è¯·æ±‚ï¼Œä¸ç³»ç»Ÿæ€§èƒ½ç›‘æ§å’Œèµ„æºç»Ÿè®¡æœ‰å…³ã€‚ä½†ä»å®‰å…¨é£é™©çš„è§’åº¦æ¥çœ‹ï¼Œç¼ºå°‘swapç»Ÿè®¡ä¿¡æ¯å¹¶ä¸ä¼šå¯¼è‡´ç³»ç»Ÿå­˜åœ¨å®‰å…¨æ¼æ´æˆ–è¢«æ¶æ„åˆ©ç”¨ã€‚å®ƒä¸ä¼šå¼•èµ·è¯¸å¦‚æƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œã€ä¿¡æ¯æ³„éœ²ç­‰å®‰å…¨é—®é¢˜ã€‚

æŒ‰ç…§é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. è¯¥Issueæè¿°çš„é—®é¢˜å¹¶éå®‰å…¨é—®é¢˜ï¼Œè€Œæ˜¯åŠŸèƒ½æ€§çš„é—®é¢˜ã€‚
2. ä¸æ¶‰åŠæ”»å‡»è€…ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²æˆ–ç³»ç»Ÿè¢«æ”»ç ´çš„æƒ…å†µã€‚

---


## Issue #129374 kube-controller-manager: leases.coordination.k8s.io "kube-controller-manager" is forbidden: User "system:anonymous"

- Issue é“¾æ¥ï¼š[#129374](https://github.com/kubernetes/kubernetes/issues/129374)

### Issue å†…å®¹

#### What happened?

The first time you run the 1.30.8 controller, it will report insufficient permissions. Running 1.30.7 again will succeed, and then running 1.30.8 will succeed again.
```
[root@localhost ssl]# kube-controller-manager --version
Kubernetes v1.30.8
[root@localhost ssl]# kube-controller-manager \
> --allocate-node-cidrs=true \
> --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf \
> --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf \
> --bind-address=0.0.0.0 \
> --client-ca-file=/etc/kubernetes/ssl/ca.pem \
> --cluster-cidr=10.244.0.0/16 \
> --cluster-name=kubernetes   \
> --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem   \
> --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem   \
> --controllers=*,bootstrapsigner,tokencleaner \
> --feature-gates=RotateKubeletServerCertificate=true \
> --kubeconfig=/etc/kubernetes/controller-manager.conf \
> --leader-elect=true \
> --node-cidr-mask-size=24 \
> --requestheader-client-ca-file=/etc/kubernetes/ssl/ca.pem \
> --root-ca-file=/etc/kubernetes/ssl/ca.pem \
> --service-account-private-key-file=/etc/kubernetes/ssl/sa-key.pem  \
> --service-cluster-ip-range=10.96.0.0/16 \
> --use-service-account-credentials=true
I1224 10:11:54.211385    2940 serving.go:380] Generated self-signed cert in-memory
I1224 10:11:54.590485    2940 controllermanager.go:189] "Starting" version="v1.30.8"
I1224 10:11:54.590556    2940 controllermanager.go:191] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1224 10:11:54.592919    2940 secure_serving.go:213] Serving securely on [::]:10257
I1224 10:11:54.592977    2940 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/etc/kubernetes/ssl/ca.pem"
I1224 10:11:54.593023    2940 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1224 10:11:54.592977    2940 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/etc/kubernetes/ssl/ca.pem"
I1224 10:11:54.593268    2940 leaderelection.go:250] attempting to acquire leader lease kube-system/kube-controller-manager...
E1224 10:11:54.599548    2940 leaderelection.go:347] error retrieving resource lock kube-system/kube-controller-manager: leases.coordination.k8s.io "kube-controller-manager" is forbidden: User "system:anonymous" cannot get resource "leases" in API group "coordination.k8s.io" in the namespace "kube-system"
E1224 10:11:56.832867    2940 leaderelection.go:347] error retrieving resource lock kube-system/kube-controller-manager: leases.coordination.k8s.io "kube-controller-manager" is forbidden: User "system:anonymous" cannot get resource "leases" in API group "coordination.k8s.io" in the namespace "kube-system"
```

```
[root@localhost bin]# ./kube-controller-manager --version
Kubernetes v1.30.7
[root@localhost bin]# ./kube-controller-manager \
> --allocate-node-cidrs=true \
> --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf \
> --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf \
> --bind-address=0.0.0.0 \
> --client-ca-file=/etc/kubernetes/ssl/ca.pem \
> --cluster-cidr=10.244.0.0/16 \
> --cluster-name=kubernetes   \
> --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem   \
> --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem   \
> --controllers=*,bootstrapsigner,tokencleaner \
> --feature-gates=RotateKubeletServerCertificate=true \
> --kubeconfig=/etc/kubernetes/controller-manager.conf \
> --leader-elect=true \
> --node-cidr-mask-size=24 \
> --requestheader-client-ca-file=/etc/kubernetes/ssl/ca.pem \
> --root-ca-file=/etc/kubernetes/ssl/ca.pem \
> --service-account-private-key-file=/etc/kubernetes/ssl/sa-key.pem  \
> --service-cluster-ip-range=10.96.0.0/16 \
> --use-service-account-credentials=true
I1224 10:19:45.752973    2999 serving.go:380] Generated self-signed cert in-memory
I1224 10:19:46.257829    2999 controllermanager.go:189] "Starting" version="v1.30.7"
I1224 10:19:46.257881    2999 controllermanager.go:191] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1224 10:19:46.260377    2999 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/etc/kubernetes/ssl/ca.pem"
I1224 10:19:46.260526    2999 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/etc/kubernetes/ssl/ca.pem"
I1224 10:19:46.260698    2999 secure_serving.go:213] Serving securely on [::]:10257
I1224 10:19:46.260773    2999 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1224 10:19:46.260957    2999 leaderelection.go:250] attempting to acquire leader lease kube-system/kube-controller-manager...
E1224 10:19:51.261438    2999 leaderelection.go:347] error retrieving resource lock kube-system/kube-controller-manager: Get "https://192.168.58.162:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
```

```
[root@localhost bin]# kube-controller-manager --version
Kubernetes v1.30.8
[root@localhost bin]# kube-controller-manager \
> --allocate-node-cidrs=true \
> --authentication-kubeconfig=/etc/kubernetes/controller-manager.conf \
> --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf \
> --bind-address=0.0.0.0 \
> --client-ca-file=/etc/kubernetes/ssl/ca.pem \
> --cluster-cidr=10.244.0.0/16 \
> --cluster-name=kubernetes   \
> --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem   \
> --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem   \
> --controllers=*,bootstrapsigner,tokencleaner \
> --feature-gates=RotateKubeletServerCertificate=true \
> --kubeconfig=/etc/kubernetes/controller-manager.conf \
> --leader-elect=true \
> --node-cidr-mask-size=24 \
> --requestheader-client-ca-file=/etc/kubernetes/ssl/ca.pem \
> --root-ca-file=/etc/kubernetes/ssl/ca.pem \
> --service-account-private-key-file=/etc/kubernetes/ssl/sa-key.pem  \
> --service-cluster-ip-range=10.96.0.0/16 \
> --use-service-account-credentials=true
I1224 10:20:27.397464    3019 serving.go:380] Generated self-signed cert in-memory
I1224 10:20:28.175486    3019 controllermanager.go:189] "Starting" version="v1.30.8"
I1224 10:20:28.175526    3019 controllermanager.go:191] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1224 10:20:28.177207    3019 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/etc/kubernetes/ssl/ca.pem"
I1224 10:20:28.177235    3019 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/etc/kubernetes/ssl/ca.pem"
I1224 10:20:28.177628    3019 secure_serving.go:213] Serving securely on [::]:10257
I1224 10:20:28.177668    3019 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I1224 10:20:28.177829    3019 leaderelection.go:250] attempting to acquire leader lease kube-system/kube-controller-manager...
E1224 10:20:33.178872    3019 leaderelection.go:347] error retrieving resource lock kube-system/kube-controller-manager: Get "https://192.168.58.162:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
```



#### What did you expect to happen?

successful

#### How can we reproduce it (as minimally and precisely as possible)?

kube-apiserver.service
```
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target

[Service]
ExecStart=/usr/local/bin/kube-apiserver \
--allow-privileged=true \
--authorization-mode=Node,RBAC \
--bind-address=0.0.0.0 \
--client-ca-file=/etc/kubernetes/ssl/ca.pem \
--enable-admission-plugins=NodeRestriction \
--enable-bootstrap-token-auth=true \
--etcd-cafile=/etc/kubernetes/ssl/ca.pem   \
--etcd-certfile=/etc/kubernetes/ssl/etcd.pem   \
--etcd-keyfile=/etc/kubernetes/ssl/etcd-key.pem   \
--etcd-servers=https://127.0.0.1:2379 \
--feature-gates=RotateKubeletServerCertificate=true \
--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \
--proxy-client-cert-file=/etc/kubernetes/ssl/kube-proxy.pem \
--proxy-client-key-file=/etc/kubernetes/ssl/kube-proxy-key.pem \
--requestheader-allowed-names=front-proxy-client \
--requestheader-client-ca-file=/etc/kubernetes/ssl/ca.pem \
--requestheader-extra-headers-prefix=X-Remote-Extra- \
--requestheader-group-headers=X-Remote-Group \
--requestheader-username-headers=X-Remote-User \
--secure-port=6443 \
--service-account-issuer=https://kubernetes.default.svc.cluster.local \
--service-account-key-file=/etc/kubernetes/ssl/sa.pem \
--service-account-signing-key-file=/etc/kubernetes/ssl/sa-key.pem \
--service-cluster-ip-range=10.96.0.0/16 \
--tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem   \
--tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem   
Restart=on-failure
RestartSec=5
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target

```


```
export K8S_API_URL=${master01ip}

/usr/local/bin/kubectl config set-cluster kubernetes \
--certificate-authority=/etc/kubernetes/ssl/ca.pem \
--embed-certs=true \
--server=https://${K8S_API_URL}:6443 \
--kubeconfig=controller-manager.conf


/usr/local/bin/kubectl  config set-credentials system:controller-manager \
--client-certificate=/etc/kubernetes/ssl/kubernetes.pem \
--client-key=/etc/kubernetes/ssl/kubernetes-key.pem \
--embed-certs=true \
--kubeconfig=controller-manager.conf


/usr/local/bin/kubectl  config set-context system:kube-controller-manager@kubernetes \
--cluster=kubernetes \
--user=system:kube-controller-manager \
--kubeconfig=controller-manager.conf

# è®¾ç½®é»˜è®¤ä¸Šä¸‹æ–‡
/usr/local/bin/kubectl  config use-context system:kube-controller-manager@kubernetes --kubeconfig=controller-manager.conf
```



#### Anything else we need to know?

_No response_

#### Kubernetes version


```console
$ kubectl version 1.30.8

[root@localhost bin]# etcd --version
etcd Version: 3.5.17
Git SHA: 507c0de
Go Version: go1.22.9
Go OS/Arch: linux/amd64

# paste output here 
```

</details>


#### Cloud provider

vmware virtual machine kylin operating system

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
[root@localhost setup]# cat /etc/os-release 
NAME="Kylin Linux Advanced Server"
VERSION="V10 (Lance)"
ID="kylin"
VERSION_ID="V10"
PRETTY_NAME="Kylin Linux Advanced Server V10 (Lance)"
ANSI_COLOR="0;31"

# paste output here
$ uname -a
Linux k8s01.example.com 4.19.90-52.15.v2207.ky10.x86_64 #1 SMP Thu Nov 24 21:50:03 CST 2022 x86_64 x86_64 x86_64 GNU/Linux
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»æä¾›çš„Issueå†…å®¹æ¥çœ‹ï¼Œç”¨æˆ·åœ¨è¿è¡Œ`kube-controller-manager`ç‰ˆæœ¬`v1.30.8`æ—¶ï¼Œé¦–æ¬¡å¯åŠ¨å‡ºç°äº†æƒé™ä¸è¶³çš„é”™è¯¯ï¼Œé”™è¯¯ä¿¡æ¯ä¸ºï¼š

```
leases.coordination.k8s.io "kube-controller-manager" is forbidden: User "system:anonymous" cannot get resource "leases" in API group "coordination.k8s.io" in the namespace "kube-system"
```

éšåç”¨æˆ·è¿è¡Œäº†`v1.30.7`ç‰ˆæœ¬ï¼ŒæˆåŠŸå¯åŠ¨ï¼Œå†æ¬¡è¿è¡Œ`v1.30.8`ç‰ˆæœ¬ä¹ŸæˆåŠŸäº†ã€‚

ä»é”™è¯¯ä¿¡æ¯åˆ†æï¼Œ`kube-controller-manager`åœ¨è¿›è¡Œé¢†å¯¼è€…é€‰ä¸¾ï¼ˆLeader Electionï¼‰æ—¶ï¼Œéœ€è¦è®¿é—®`leases`èµ„æºï¼Œä½†è¯·æ±‚ä»¥`system:anonymous`ç”¨æˆ·èº«ä»½å‘èµ·ï¼Œç¼ºå°‘å¿…è¦çš„æƒé™ï¼Œå¯¼è‡´è®¿é—®è¢«æ‹’ç»ã€‚

å‡ºç°è¯¥é—®é¢˜çš„åŸå› å¯èƒ½æ˜¯`kube-controller-manager`çš„é…ç½®æœ‰è¯¯ï¼Œæ¯”å¦‚`--kubeconfig`å‚æ•°æŒ‡å‘çš„é…ç½®æ–‡ä»¶ä¸æ­£ç¡®ï¼Œå¯¼è‡´è®¤è¯å¤±è´¥ï¼Œç»„ä»¶ä»¥åŒ¿åç”¨æˆ·èº«ä»½è¯·æ±‚API Serverã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

- ç¬¬2æ¡ï¼šå¦‚æœè¯¥issueæè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚
- ç¬¬4æ¡ï¼šissueæäº¤è€…åœ¨æäº¤å†…å®¹ä¸­æš´éœ²çš„æ•æ„Ÿä¿¡æ¯ã€ä¸å½“æ“ä½œã€ä¸å½“é…ç½®ç­‰é—®é¢˜ï¼Œä¸å±äºå®‰å…¨é£é™©ï¼Œåº”å½“åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

å› æ­¤ï¼Œè¯¥Issueåæ˜ çš„æ˜¯ç”¨æˆ·çš„é…ç½®æˆ–æ“ä½œé—®é¢˜ï¼Œä¸æ¶‰åŠé¡¹ç›®çš„å®‰å…¨é£é™©ã€‚

---


## Issue #129362 bug(dra): when deleting resourceclaimtemplate, pod can't running again

- Issue é“¾æ¥ï¼š[#129362](https://github.com/kubernetes/kubernetes/issues/129362)

### Issue å†…å®¹

#### What happened?

```yaml
root@VM-0-6-ubuntu:/home/ubuntu/k8s-dra-driver/demo/specs/quickstart# kubectl apply -f gpu-test2-dep.yaml
namespace/gpu-test2 created
resourceclaimtemplate.resource.k8s.io/single-gpu created
deployment.apps/gpu-deployment created
root@VM-0-6-ubuntu:/home/ubuntu/k8s-dra-driver/demo/specs/quickstart# kubectl get all -ngpu-test2
NAME                                  READY   STATUS    RESTARTS   AGE
pod/gpu-deployment-6965899554-zmq5j   2/2     Running   0          16s

NAME                             READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/gpu-deployment   1/1     1            1           16s

NAME                                        DESIRED   CURRENT   READY   AGE
replicaset.apps/gpu-deployment-6965899554   1         1         1       16s
root@VM-0-6-ubuntu:/home/ubuntu/k8s-dra-driver/demo/specs/quickstart# kubectl get resourceclaimtemplates -ngpu-test2
NAME         AGE
single-gpu   47s
root@VM-0-6-ubuntu:/home/ubuntu/k8s-dra-driver/demo/specs/quickstart#
```
1. delete resourceclaimtemplates
2. delete pod
```yaml
root@VM-0-6-ubuntu:/home/ubuntu/k8s-dra-driver/demo/specs/quickstart# kubectl get resourceclaimtemplates -ngpu-test2
NAME         AGE
single-gpu   2m33s
root@VM-0-6-ubuntu:/home/ubuntu/k8s-dra-driver/demo/specs/quickstart# kubectl delete resourceclaimtemplates single-gpu -ngpu-test2
resourceclaimtemplate.resource.k8s.io "single-gpu" deleted
root@VM-0-6-ubuntu:/home/ubuntu/k8s-dra-driver/demo/specs/quickstart# kubectl get pods -ngpu-test2
NAME                              READY   STATUS    RESTARTS   AGE
gpu-deployment-6965899554-zmq5j   2/2     Running   0          2m50s
root@VM-0-6-ubuntu:/home/ubuntu/k8s-dra-driver/demo/specs/quickstart# kubectl delete pods gpu-deployment-6965899554-zmq5j -ngpu-test2
pod "gpu-deployment-6965899554-zmq5j" deleted
root@VM-0-6-ubuntu:/home/ubuntu/k8s-dra-driver/demo/specs/quickstart# kubectl get pods -ngpu-test2
NAME                              READY   STATUS    RESTARTS   AGE
gpu-deployment-6965899554-wl2dj   0/2     Pending   0          3s
root@VM-0-6-ubuntu:/home/ubuntu/k8s-dra-driver/demo/specs/quickstart#
```
pod is always Pending
```yaml
root@VM-0-6-ubuntu:/home/ubuntu/k8s-dra-driver/demo/specs/quickstart# kubectl describe pods gpu-deployment-6965899554-wl2dj -ngpu-test2
Name:             gpu-deployment-6965899554-wl2dj
Namespace:        gpu-test2
Priority:         0
Service Account:  default
Node:             <none>
Labels:           app=gpu-app
                  pod-template-hash=6965899554
Annotations:      <none>
Status:           Pending
IP:
IPs:              <none>
Controlled By:    ReplicaSet/gpu-deployment-6965899554
Containers:
  ctr0:
    Image:      ubuntu:22.04
    Port:       <none>
    Host Port:  <none>
    Command:
      bash
      -c
    Args:
      nvidia-smi -L; trap 'exit 0' TERM; sleep 9999 & wait
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-m26j7 (ro)
  ctr1:
    Image:      ubuntu:22.04
    Port:       <none>
    Host Port:  <none>
    Command:
      bash
      -c
    Args:
      nvidia-smi -L; trap 'exit 0' TERM; sleep 9999 & wait
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-m26j7 (ro)
Volumes:
  kube-api-access-m26j7:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              nvidia.com/gpu.present=true
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
                             nvidia.com/gpu:NoSchedule op=Exists
Events:
  Type     Reason                       Age                 From            Message
  ----     ------                       ----                ----            -------
  Warning  FailedResourceClaimCreation  11s (x13 over 31s)  resource_claim  PodResourceClaim shared-gpu: resource claim template "single-gpu": resourceclaimtemplate.resource.k8s.io "single-gpu" not found
```

#### What did you expect to happen?

When resourceclaimtemplate is deleted, the pod can still run successfully after restart. Or resourceclaimtemplate cannot be deleted when it is in use.

#### How can we reproduce it (as minimally and precisely as possible)?

```yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: gpu-test2

---
apiVersion: resource.k8s.io/v1beta1
kind: ResourceClaimTemplate
metadata:
  namespace: gpu-test2
  name: single-gpu
spec:
  spec:
    devices:
      requests:
      - name: gpu
        deviceClassName: gpu.nvidia.com

---
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: gpu-test2
  name: gpu-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: gpu-app
  template:
    metadata:
      labels:
        app: gpu-app
    spec:
      containers:
      - name: ctr0
        image: ubuntu:22.04
        command: ["bash", "-c"]
        args: ["nvidia-smi -L; trap 'exit 0' TERM; sleep 9999 & wait"]
        resources:
          claims:
          - name: shared-gpu
      - name: ctr1
        image: ubuntu:22.04
        command: ["bash", "-c"]
        args: ["nvidia-smi -L; trap 'exit 0' TERM; sleep 9999 & wait"]
        resources:
          claims:
          - name: shared-gpu
      resourceClaims:
      - name: shared-gpu
        resourceClaimTemplateName: single-gpu
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
      nodeSelector:
        nvidia.com/gpu.present: "true"
      restartPolicy: Always
```
use this sample yaml, and delete pod.

#### Anything else we need to know?

I'm not sure if this is by design. But according to common sense, there is a possibility that resourceclaimtemplate was accidentally deleted. If the training task is restarted, the above problem may occur. ğŸ¤” 

#### Kubernetes version



```console
root@VM-0-6-ubuntu:/home/ubuntu# kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.32.0
```




#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†åœ¨Kubernetesç¯å¢ƒä¸­ï¼Œå½“åˆ é™¤`ResourceClaimTemplate`åï¼Œå†åˆ é™¤ç›¸å…³çš„Podï¼ŒPodæ— æ³•é‡æ–°è¿è¡Œï¼Œä¸€ç›´å¤„äºPendingçŠ¶æ€ã€‚é€šè¿‡æŸ¥çœ‹äº‹ä»¶æ—¥å¿—ï¼Œå‘ç°Podå› ä¸ºæ— æ³•æ‰¾åˆ°`ResourceClaimTemplate`è€Œæ— æ³•åˆ›å»ºæ‰€éœ€çš„èµ„æºå£°æ˜ï¼Œå¯¼è‡´Podæ— æ³•è°ƒåº¦è¿è¡Œã€‚

è¿™ä¸ªé—®é¢˜æ¶‰åŠåˆ°èµ„æºç®¡ç†å’Œä¾èµ–å…³ç³»çš„é—®é¢˜ã€‚å½“åˆ é™¤äº†`ResourceClaimTemplate`åï¼Œä¾èµ–äºè¯¥æ¨¡æ¿çš„èµ„æºå£°æ˜æ— æ³•åˆ›å»ºï¼Œè¿™æ˜¯é¢„æœŸçš„è¡Œä¸ºã€‚è™½ç„¶è¿™å¯èƒ½ç»™ç”¨æˆ·å¸¦æ¥ä¸ä¾¿ï¼Œä½†è¿™å±äºåŠŸèƒ½æ€§é—®é¢˜æˆ–è®¾è®¡ç¼ºé™·ï¼Œå¹¶ä¸æ¶‰åŠä»»ä½•å®‰å…¨æ¼æ´æˆ–é£é™©ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **å¦‚æœè¯¥issueæè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚**

å› æ­¤ï¼Œè¯¥Issueæè¿°çš„é—®é¢˜ä¸å±äºå®‰å…¨é—®é¢˜ï¼Œè€Œæ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§Bugæˆ–è®¾è®¡é—®é¢˜ã€‚

---


## Issue #129356 TestServerRunWithSNI Unit Test Fails Intermittently

- Issue é“¾æ¥ï¼š[#129356](https://github.com/kubernetes/kubernetes/issues/129356)

### Issue å†…å®¹

#### What happened?

The `TestServerRunWithSNI` unit test is failing intermittently. 

```
=== NAME  TestServerRunWithSNI/loopback:_bind_to_0.0.0.0_=>_loopback_uses_localhost
    serving_test.go:339: Dialing localhost:43713 as ""
    serving_test.go:372: failed to connect with loopback client: Get "https://0.0.0.0:43713/version?timeout=32s": net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
--- FAIL: TestServerRunWithSNI (0.00s)
    --- PASS: TestServerRunWithSNI/one_SNI_and_the_default_cert_with_the_same_name (0.23s)
    --- PASS: TestServerRunWithSNI/cert_with_multiple_alternate_names (0.28s)
    --- PASS: TestServerRunWithSNI/only_one_cert (0.30s)
    --- PASS: TestServerRunWithSNI/loopback:_LoopbackClientServerNameOverride_on_server_cert (0.37s)
    --- PASS: TestServerRunWithSNI/loopback:_LoopbackClientServerNameOverride_not_on_any_cert (0.38s)
    --- PASS: TestServerRunWithSNI/loopback:_LoopbackClientServerNameOverride_on_SNI_cert (0.55s)
    --- PASS: TestServerRunWithSNI/matching_IP_in_SNI_cert_and_the_server_cert (0.55s)
    --- PASS: TestServerRunWithSNI/matching_SNI_cert (0.57s)
    --- PASS: TestServerRunWithSNI/wildcards (0.62s)
    --- FAIL: TestServerRunWithSNI/loopback:_bind_to_0.0.0.0_=>_loopback_uses_localhost (32.69s)
FAIL
I1221 18:33:43.081497   65589 dynamic_serving_content.go:149] "Shutting down controller" name="serving-cert::testdata/localhost__/cert::testdata/localhost__/key"
I1221 18:33:43.081523   65589 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I1221 18:33:43.081523   65589 object_count_tracker.go:151] "StorageObjectCountTracker pruner is exiting"
I1221 18:33:43.081567   65589 secure_serving.go:258] Stopped listening on 127.0.0.1:43713
FAIL	k8s.io/apiserver/pkg/server/options	32.722s
FAIL

```


#### What did you expect to happen?

The TestServerRunWithSNI test should pass consistently without any errors

#### How can we reproduce it (as minimally and precisely as possible)?

go test -v k8s.io/apiserver/pkg/server/options -run TestServerRunWithSNI

#### Anything else we need to know?

_No response_

#### Kubernetes version

1.31

#### Cloud provider

<details>

</details>


#### OS version

windows 11



#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueå†…å®¹ï¼Œ`TestServerRunWithSNI`å•å…ƒæµ‹è¯•åœ¨æŸäº›æƒ…å†µä¸‹ä¼šé—´æ­‡æ€§å¤±è´¥ã€‚å…·ä½“é”™è¯¯ä¿¡æ¯æ˜¾ç¤ºï¼Œåœ¨å°è¯•ä½¿ç”¨å›ç¯å®¢æˆ·ç«¯è¿æ¥`localhost:43713`æ—¶ï¼Œå‘ç”Ÿäº†è¶…æ—¶é”™è¯¯ï¼Œæœªèƒ½æˆåŠŸå»ºç«‹è¿æ¥ã€‚

ä»æè¿°æ¥çœ‹ï¼Œè¿™æ›´åƒæ˜¯ä¸€ä¸ªå•å…ƒæµ‹è¯•çš„ç¨³å®šæ€§æˆ–å¯é æ€§é—®é¢˜ï¼Œå¯èƒ½ä¸æµ‹è¯•ç¯å¢ƒã€ç½‘ç»œé…ç½®æˆ–ä»£ç æœ¬èº«çš„é€»è¾‘æœ‰å…³ã€‚æ²¡æœ‰è¿¹è±¡è¡¨æ˜è¯¥é—®é¢˜æ¶‰åŠå®‰å…¨æ–¹é¢çš„é£é™©ï¼Œä¾‹å¦‚æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€æƒé™æå‡ã€ä»£ç æ‰§è¡Œç­‰ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. å¦‚æœè¯¥Issueæè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

---


## Issue #129320 `kubectl describe pod` probe host is empty

- Issue é“¾æ¥ï¼š[#129320](https://github.com/kubernetes/kubernetes/issues/129320)

### Issue å†…å®¹

#### What happened?

Using `kubectl describe po xxx` to show pod probe info, when the probe host is empty, there is no default value showed in the output, like below, so this PR add the default value.
```
    State:          Running
      Started:      Thu, 19 Dec 2024 15:25:42 +0800
    Ready:          False
    Restart Count:  0
    ...
    Liveness:       http-get http://:8080/healthz delay=0s timeout=5s period=10s #success=1 #failure=3
```

#### What did you expect to happen?

When the host not set, add the default value `localhost`.
```
    Liveness:       http-get http://:8080/healthz delay=0s timeout=5s period=10s #success=1 #failure=3
    ======== to below
    Liveness:       http-get http://localhost:8080/healthz delay=0s timeout=5s period=10s #success=1 #failure=3
```

#### How can we reproduce it (as minimally and precisely as possible)?

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          timeoutSeconds: 5

```
`kubectl apply -f xxx.yaml && kubectl describe po xxx` will reproduce this issue.

#### Anything else we need to know?

Issue tested in 1.31.

#### Kubernetes version

1.31

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueï¼Œé—®é¢˜æ˜¯åœ¨ä½¿ç”¨`kubectl describe pod xxx`å‘½ä»¤æ˜¾ç¤ºPodçš„æ¢é’ˆä¿¡æ¯æ—¶ï¼Œå½“æ¢é’ˆçš„`host`å­—æ®µä¸ºç©ºæ—¶ï¼Œè¾“å‡ºä¸­æœªæ˜¾ç¤ºé»˜è®¤å€¼`localhost`ã€‚å› æ­¤ï¼Œæå‡ºäº†ä¸€ä¸ªPRæ¥åœ¨è¾“å‡ºä¸­æ·»åŠ é»˜è®¤å€¼`localhost`ã€‚

è¯¥é—®é¢˜æ¶‰åŠçš„æ˜¯å‘½ä»¤è¡Œè¾“å‡ºçš„ä¿¡æ¯å±•ç¤ºé—®é¢˜ï¼Œå±äºç”¨æˆ·ä½“éªŒå’Œå‹å¥½åº¦çš„ä¼˜åŒ–ï¼Œå¹¶ä¸æ¶‰åŠä»»ä½•å®‰å…¨é£é™©ã€‚å³ä½¿`host`å­—æ®µä¸ºç©ºï¼Œæ¢é’ˆå®é™…ä¹Ÿä¼šé»˜è®¤ä½¿ç”¨`localhost`ï¼ŒåŠŸèƒ½ä¸Šæ²¡æœ‰é—®é¢˜ï¼Œä»…ä»…æ˜¯è¾“å‡ºä¿¡æ¯ä¸­æœªæ˜¾ç¤ºé»˜è®¤å€¼ï¼Œå¯èƒ½ä¼šå¯¼è‡´ç”¨æˆ·å›°æƒ‘ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
- å¦‚æœIssueæè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

---


## Issue #129310 DRA: Using All allocation mode will schedule to nodes with zero devices

- Issue é“¾æ¥ï¼š[#129310](https://github.com/kubernetes/kubernetes/issues/129310)

### Issue å†…å®¹

#### What happened?

I created a resource claim template to get "All" GPUs on a node:

```yaml
apiVersion: resource.k8s.io/v1beta1
kind: ResourceClaimTemplate
metadata:
  name: all-gpus
spec:
  spec:
    devices:
      requests:
      - name: gpu
        deviceClassName: gpu.nvidia.com
        allocationMode: All
```

I then created a deployment that had a Pod that used that claim. The Pod was scheduled to a node. However, my DRA driver on that node was not running, so there were no resource slices for that node.

#### What did you expect to happen?

I expected the pod to not schedule, since there were no available devices meeting the request. "All" should mean "at least one".

#### How can we reproduce it (as minimally and precisely as possible)?

Create the resource claim template as shown and a deployment, with no DRA driver running. The pod will still schedule.

#### Anything else we need to know?

/wg device-management

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.32.0-gke.1358000
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux rodete"
NAME="Debian GNU/Linux rodete"
VERSION_CODENAME=rodete
ID=debian
HOME_URL="https://go/glinux"
SUPPORT_URL="https://go/techstop"
BUG_REPORT_URL="https://go/techstop"
$ uname -a
Linux jbelamaric.c.googlers.com 6.10.11-1rodete2-amd64 #1 SMP PREEMPT_DYNAMIC Debian 6.10.11-1rodete2 (2024-10-16) x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
Issueæè¿°äº†åœ¨Kubernetesä¸­ä½¿ç”¨åŠ¨æ€èµ„æºåˆ†é…ï¼ˆDRAï¼‰æ—¶ï¼Œå½“è®¾ç½®äº†`allocationMode: All`æ¥è·å–èŠ‚ç‚¹ä¸Šçš„æ‰€æœ‰GPUè®¾å¤‡æ—¶ï¼Œå³ä½¿èŠ‚ç‚¹ä¸Šæ²¡æœ‰è¿è¡ŒDRAé©±åŠ¨ï¼ˆå› æ­¤æ²¡æœ‰å¯ç”¨çš„GPUè®¾å¤‡ï¼‰ï¼ŒPodä»ç„¶ä¼šè¢«è°ƒåº¦åˆ°è¯¥èŠ‚ç‚¹ä¸Šã€‚é¢„æœŸè¡Œä¸ºæ˜¯å¦‚æœæ²¡æœ‰å¯ç”¨çš„è®¾å¤‡ï¼ŒPodä¸åº”è¯¥è¢«è°ƒåº¦åˆ°è¯¥èŠ‚ç‚¹ä¸Šã€‚

ä»é—®é¢˜çš„æè¿°æ¥çœ‹ï¼Œè¿™æ˜¯ä¸€ä¸ªè°ƒåº¦å™¨åœ¨èµ„æºä¸å¯ç”¨æƒ…å†µä¸‹çš„é”™è¯¯è°ƒåº¦é—®é¢˜ï¼Œå¯èƒ½å¯¼è‡´Podæ— æ³•æ­£å¸¸è¿è¡Œï¼Œå½±å“åº”ç”¨ç¨‹åºçš„æ€§èƒ½æˆ–åŠŸèƒ½ã€‚

ç„¶è€Œï¼Œä»å®‰å…¨é£é™©çš„è§’åº¦åˆ†æï¼š

1. è¯¥é—®é¢˜ä¸ä¼šå¯¼è‡´æœªæˆæƒçš„è®¿é—®æˆ–æƒé™æå‡ã€‚æ”»å‡»è€…æ— æ³•åˆ©ç”¨è¯¥æ¼æ´è·å–æ›´é«˜çš„æƒé™æˆ–è®¿é—®å—é™èµ„æºã€‚

2. è¯¥é—®é¢˜ä¸ä¼šå¯¼è‡´æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€‚æ²¡æœ‰ä»»ä½•å‡­æ®æˆ–æœºå¯†æ•°æ®å› æ­¤è¢«æš´éœ²ã€‚

3. è¯¥é—®é¢˜ä¸ä¼šå¯¼è‡´ä»£ç æ‰§è¡Œã€å®¹å™¨é€ƒé€¸æˆ–ææƒç­‰é«˜é£é™©å®‰å…¨é—®é¢˜ã€‚

4. å³ä½¿å­˜åœ¨æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰çš„å¯èƒ½æ€§ï¼Œä½†éœ€è¦å…·æœ‰åˆ›å»ºæˆ–ä¿®æ”¹Podçš„æƒé™æ‰èƒ½è§¦å‘ï¼Œæ­¤ç±»æƒé™é€šå¸¸ä»…æˆäºˆå—ä¿¡ä»»çš„ç”¨æˆ·ã€‚

åŸºäºä»¥ä¸Šåˆ†æï¼ŒæŒ‰ç…§ç»™å®šçš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥Issueæè¿°çš„é—®é¢˜ä¸å±äºå®‰å…¨é—®é¢˜ã€‚

---


## Issue #129309 MutatingAdmissionPolicy mutation ordering issue

- Issue é“¾æ¥ï¼š[#129309](https://github.com/kubernetes/kubernetes/issues/129309)

### Issue å†…å®¹

#### What happened?

This works:
```
    - patchType: "JSONPatch"
      jsonPatch:
        expression: >
          [
            JSONPatch{
              op: "add", path: "/spec/initContainers",
              value: []
            },
            JSONPatch{
              op: "add", path: "/spec/initContainers/-",
              value: Object.spec.initContainers{
                name: "mesh-proxy",
                image: "mesh-proxy/v1.0.0",
                restartPolicy: "Always"
              }
            }
          ]
```

But, this fails for a pod:
```
    - patchType: "JSONPatch"
      jsonPatch:
        expression: >
          [
            JSONPatch{
              op: "add", path: "/spec/initContainers",
              value: []
            }
          ]
    - patchType: "JSONPatch"
      jsonPatch:
        expression: >
          [
            JSONPatch{
              op: "add", path: "/spec/initContainers/-",
              value: Object.spec.initContainers{
                name: "mesh-proxy",
                image: "mesh-proxy/v1.0.0",
                restartPolicy: "Always"
              }
            }
          ]
```
with:
```
denied request: JSON Patch: add operation does not apply: doc is missing path: "/spec/initContainers/-": missing value
```

Seems like the ordering is wrong or the input of one isn't fed into the next

#### What did you expect to happen?

Both should work

#### How can we reproduce it (as minimally and precisely as possible)?

Try chaining the mutations together in the same object, one relying on the next as above

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.32.0
```

</details>


#### Cloud provider

NA

#### OS version

minikube

#### Install tools

minikube

#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨Kubernetesçš„MutatingAdmissionPolicyè¿›è¡Œèµ„æºçš„å˜æ›´æ—¶ï¼Œé‡åˆ°äº†mutationæ“ä½œçš„æ‰§è¡Œé¡ºåºé—®é¢˜ã€‚å…·ä½“è¡¨ç°ä¸ºï¼š

- å½“å°†ä¸¤ä¸ªJSONPatchæ“ä½œæ”¾åœ¨åŒä¸€ä¸ªè¡¨è¾¾å¼ä¸­æ—¶ï¼Œå˜æ›´èƒ½æˆåŠŸåº”ç”¨ã€‚
- å½“å°†ä¸¤ä¸ªJSONPatchæ“ä½œåˆ†å¼€ä¸ºä¸¤ä¸ªç‹¬ç«‹çš„mutationæ—¶ï¼Œç¬¬äºŒä¸ªmutationæœªèƒ½è¯†åˆ«åˆ°ç¬¬ä¸€ä¸ªmutationçš„ç»“æœï¼Œå¯¼è‡´æ“ä½œå¤±è´¥ï¼Œå‡ºç°é”™è¯¯ä¿¡æ¯ï¼šâ€œdenied request: JSON Patch: add operation does not apply: doc is missing path: "/spec/initContainers/-": missing valueâ€ã€‚

è¿™ä¸ªé—®é¢˜çš„æ ¸å¿ƒåœ¨äºMutatingAdmissionPolicyåœ¨å¤„ç†å¤šä¸ªmutationæ“ä½œæ—¶ï¼Œæœªèƒ½æ­£ç¡®åœ°æŒ‰é¡ºåºæ‰§è¡Œï¼Œæˆ–è€…æœªèƒ½å°†å‰ä¸€ä¸ªmutationçš„ç»“æœä½œä¸ºåä¸€ä¸ªmutationçš„è¾“å…¥ã€‚è¿™å¯èƒ½æ˜¯MutatingAdmissionPolicyçš„å®ç°ç¼ºé™·æˆ–ä½¿ç”¨æ–¹å¼ä¸å½“å¯¼è‡´çš„ã€‚

ä»Issueä¸­æä¾›çš„ä¿¡æ¯æ¥çœ‹ï¼Œè¿™å±äºåŠŸèƒ½æ€§é—®é¢˜ï¼Œå¹¶æœªæ¶‰åŠå®‰å…¨é£é™©ã€‚è¯¥é—®é¢˜ä¸ä¼šå¯¼è‡´æƒé™æå‡ã€æ•°æ®æ³„éœ²ã€è¶Šæƒè®¿é—®ç­‰å®‰å…¨é—®é¢˜ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

- ç¬¬2æ¡ï¼šâ€œå¦‚æœè¯¥issueæè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠâ€ã€‚

å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---


## Issue #129294 services have not yet been read at least once, cannot construct envvars

- Issue é“¾æ¥ï¼š[#129294](https://github.com/kubernetes/kubernetes/issues/129294)

### Issue å†…å®¹

#### What happened?

å½“æˆ‘ä½¿ç”¨äºŒè¿›åˆ¶éƒ¨ç½²kubernetesé‡åˆ°ä¸€ä¸ªé—®é¢˜ï¼Œç¯å¢ƒå¦‚ä¸‹ 
root@192:~/generic_architecture# kube-apiserver --version
Kubernetes v1.28.12
root@192:~/generic_architecture# containerd --version
containerd containerd.io 1.7.24 88bf19b2105c8b17560993bee28a01ddc2f97182

root@192:/etc/kubernetes# cat /etc/os-release 
PRETTY_NAME="Ubuntu 24.04.1 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04.1 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo


é…ç½®æ–‡ä»¶å¦‚ä¸‹ï¼š
```
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
    cacheTTL: "2m"
  x509:
    clientCAFile: "/data/kubernetes/pki/ca-cert.pem"
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: "5m"
    cacheUnauthorizedTTL: "30s"
# DNS é…ç½®
clusterDNS:
- "169.254.123.123"  # ä½¿ç”¨ NodeLocal DNSCache çš„ IP
clusterDomain: "cluster.local"
podCIDR: "10.244.0.0/16"
resolvConf: "/run/systemd/resolve/resolv.conf"
runtimeRequestTimeout: "15m"
rotateCertificates: true
serverTLSBootstrap: false
tlsCertFile: "/data/kubernetes/pki/system:node:192.168.102.230-cert.pem"
tlsPrivateKeyFile: "/data/kubernetes/pki/system:node:192.168.102.230-key.pem"
failSwapOn: false
enableSystemLogHandler: true
enableDebuggingHandlers: true
# æŒ‡å®š cgroup é©±åŠ¨ä¸º systemd
cgroupDriver: systemd

# ç¡¬é©±é€é˜ˆå€¼é…ç½®ï¼šå½“è¾¾åˆ°è¿™äº›é˜ˆå€¼æ—¶ï¼Œpod å°†è¢«ç«‹å³é©±é€
# nodefs: æ ¹æ–‡ä»¶ç³»ç»Ÿ (kubelet æ ¹ç›®å½•å’Œæ—¥å¿—ç›®å½•æ‰€åœ¨çš„æ–‡ä»¶ç³»ç»Ÿ)
# imagefs: é•œåƒæ–‡ä»¶ç³»ç»Ÿ (å®¹å™¨è¿è¡Œæ—¶ç”¨äºå­˜å‚¨å®¹å™¨é•œåƒå’Œå®¹å™¨å¯å†™å±‚çš„æ–‡ä»¶ç³»ç»Ÿ)
evictionHard:
  memory.available: "100Mi"     # å½“å¯ç”¨å†…å­˜å°äº 100Mi æ—¶è§¦å‘ç¡¬é©±é€
  nodefs.available: "5%"        # å½“æ ¹æ–‡ä»¶ç³»ç»Ÿå¯ç”¨ç©ºé—´å°äº 1% æ—¶è§¦å‘ç¡¬é©±é€
  nodefs.inodesFree: "4%"       # å½“æ ¹æ–‡ä»¶ç³»ç»Ÿå¯ç”¨ inode å°äº 4% æ—¶è§¦å‘ç¡¬é©±é€
  imagefs.available: "5%"      # å½“é•œåƒæ–‡ä»¶ç³»ç»Ÿå¯ç”¨ç©ºé—´å°äº 10% æ—¶è§¦å‘ç¡¬é©±é€

# è½¯é©±é€é˜ˆå€¼é…ç½®ï¼šå½“è¾¾åˆ°è¿™äº›é˜ˆå€¼æ—¶ï¼Œpod å°†åœ¨å®½é™æœŸåè¢«é©±é€
# è½¯é©±é€æä¾›äº†ä¸€ä¸ªç¼“å†²åŒºï¼Œè®©ç³»ç»Ÿæœ‰æ—¶é—´å°è¯•è‡ªåŠ¨æ¢å¤
evictionSoft:
  memory.available: "200Mi"     # å½“å¯ç”¨å†…å­˜å°äº 200Mi æ—¶è§¦å‘è½¯é©±é€
  nodefs.available: "5%"       # å½“æ ¹æ–‡ä»¶ç³»ç»Ÿå¯ç”¨ç©ºé—´å°äº 10% æ—¶è§¦å‘è½¯é©±é€
  nodefs.inodesFree: "5%"       # å½“æ ¹æ–‡ä»¶ç³»ç»Ÿå¯ç”¨ inode å°äº 1% æ—¶è§¦å‘è½¯é©±é€
  imagefs.available: "5%"      # å½“é•œåƒæ–‡ä»¶ç³»ç»Ÿå¯ç”¨ç©ºé—´å°äº 1% æ—¶è§¦å‘è½¯é©±é€

# è½¯é©±é€å®½é™æœŸï¼šåœ¨è§¦å‘è½¯é©±é€åï¼Œç­‰å¾…å¤šé•¿æ—¶é—´æ‰å¼€å§‹é©±é€ pod
# è¿™æ®µæ—¶é—´å†…ï¼Œç³»ç»Ÿä¼šå°è¯•è‡ªåŠ¨æ¢å¤ï¼Œå¦‚æœæ¢å¤å¤±è´¥æ‰ä¼šé©±é€ pod
evictionSoftGracePeriod:
  memory.available: "2m"        # å†…å­˜ä¸è¶³æ—¶çš„å®½é™æœŸä¸º 2 åˆ†é’Ÿ
  nodefs.available: "2m"        # ç£ç›˜ç©ºé—´ä¸è¶³æ—¶çš„å®½é™æœŸä¸º 2 åˆ†é’Ÿ
  nodefs.inodesFree: "2m"      # inode ä¸è¶³æ—¶çš„å®½é™æœŸä¸º 2 åˆ†é’Ÿ
  imagefs.available: "2m"       # é•œåƒå­˜å‚¨ç©ºé—´ä¸è¶³æ—¶çš„å®½é™æœŸä¸º 2 åˆ†é’Ÿ

# é©±é€å‹åŠ›è½¬æ¢å‘¨æœŸï¼šç³»ç»Ÿå¿…é¡»åœ¨ç­‰å¾…å¤šé•¿æ—¶é—´åæ‰èƒ½è½¬æ¢å‹åŠ›çŠ¶æ€
# è¿™å¯ä»¥é˜²æ­¢ç³»ç»Ÿåœ¨å‹åŠ›ä¸´ç•Œå€¼é™„è¿‘é¢‘ç¹éœ‡è¡
evictionPressureTransitionPeriod: "5m0s"

# æœ€å°å›æ”¶é‡ï¼šå½“è§¦å‘é©±é€æ—¶ï¼Œå¿…é¡»è¦å›æ”¶çš„æœ€å°èµ„æºé‡
# è¿™ç¡®ä¿æ¯æ¬¡é©±é€éƒ½èƒ½é‡Šæ”¾è¶³å¤Ÿçš„èµ„æºï¼Œé¿å…é¢‘ç¹é©±é€
evictionMinimumReclaim:
  memory.available: "0Mi"       # ä¸å¼ºåˆ¶å†…å­˜æœ€å°å›æ”¶é‡
  nodefs.available: "500Mi"     # æ¯æ¬¡è‡³å°‘å›æ”¶ 500Mi ç£ç›˜ç©ºé—´
  imagefs.available: "500Mi"    # æ¯æ¬¡è‡³å°‘å›æ”¶ 500Mi é•œåƒå­˜å‚¨ç©ºé—´

# Kubernetes ç³»ç»Ÿç»„ä»¶é¢„ç•™èµ„æºï¼šç¡®ä¿ kubernetes ç»„ä»¶ï¼ˆå¦‚ kubeletï¼‰æœ‰è¶³å¤Ÿèµ„æºè¿è¡Œ
kubeReserved:
  memory: "512Mi"              # ä¸º kubernetes ç»„ä»¶é¢„ç•™ 512Mi å†…å­˜
  ephemeral-storage: "1Gi"     # ä¸º kubernetes ç»„ä»¶é¢„ç•™ 1Gi ä¸´æ—¶å­˜å‚¨

# ç³»ç»Ÿé¢„ç•™èµ„æºï¼šç¡®ä¿ç³»ç»Ÿå®ˆæŠ¤è¿›ç¨‹ï¼ˆå¦‚ sshdã€udev ç­‰ï¼‰æœ‰è¶³å¤Ÿèµ„æºè¿è¡Œ
systemReserved:
  memory: "512Mi"              # ä¸ºç³»ç»Ÿå®ˆæŠ¤è¿›ç¨‹é¢„ç•™ 512Mi å†…å­˜
  ephemeral-storage: "1Gi"     # ä¸ºç³»ç»Ÿå®ˆæŠ¤è¿›ç¨‹é¢„ç•™ 1Gi ä¸´æ—¶å­˜å‚¨

```

kube-proxy
```
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: "/etc/kubernetes/kube-proxy.kubeconfig"
mode: "ipvs"
clusterCIDR: "10.244.0.0/16"
ipvs:
  strictARP: true
```




#### What did you expect to happen?

podçŠ¶æ€å¼‚å¸¸
```
root@192:~/generic_architecture# k get pod -A
NAMESPACE         NAME                               READY   STATUS                       RESTARTS   AGE
tigera-operator   tigera-operator-56b74f76df-n2vpx   0/1     CreateContainerConfigError   0          13m


root@192:~/generic_architecture# k describe pod -n tigera-operator tigera-operator-56b74f76df-n2vpx 
Name:             tigera-operator-56b74f76df-n2vpx
Namespace:        tigera-operator
Priority:         0
Service Account:  tigera-operator
Node:             192.168.102.230/192.168.102.230
Start Time:       Thu, 19 Dec 2024 05:41:55 +0000
Labels:           k8s-app=tigera-operator
                  name=tigera-operator
                  pod-template-hash=56b74f76df
Annotations:      <none>
Status:           Pending
IP:               192.168.102.230
IPs:
  IP:           192.168.102.230
Controlled By:  ReplicaSet/tigera-operator-56b74f76df
Containers:
  tigera-operator:
    Container ID:  
    Image:         quay.io/tigera/operator:v1.36.0
    Image ID:      
    Port:          <none>
    Host Port:     <none>
    Command:
      operator
    State:          Waiting
      Reason:       CreateContainerConfigError
    Ready:          False
    Restart Count:  0
    Environment Variables from:
      kubernetes-services-endpoint  ConfigMap  Optional: true
    Environment:
      WATCH_NAMESPACE:                     
      POD_NAME:                            tigera-operator-56b74f76df-n2vpx (v1:metadata.name)
      OPERATOR_NAME:                       tigera-operator
      TIGERA_OPERATOR_INIT_IMAGE_VERSION:  v1.36.0
    Mounts:
      /var/lib/calico from var-lib-calico (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bpbk7 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True 
  Initialized                 True 
  Ready                       False 
  ContainersReady             False 
  PodScheduled                True 
Volumes:
  var-lib-calico:
    Type:          HostPath (bare host directory volume)
    Path:          /var/lib/calico
    HostPathType:  
  kube-api-access-bpbk7:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              kubernetes.io/os=linux
Tolerations:                 :NoExecute op=Exists
                             :NoSchedule op=Exists
Events:
  Type     Reason     Age                From               Message
  ----     ------     ----               ----               -------
  Normal   Scheduled  13m                default-scheduler  Successfully assigned tigera-operator/tigera-operator-56b74f76df-n2vpx to 192.168.102.230
  Normal   Pulled     13m (x3 over 13m)  kubelet            Container image "quay.io/tigera/operator:v1.36.0" already present on machine
  Warning  Failed     13m (x3 over 13m)  kubelet            Error: services have not yet been read at least once, cannot construct envvars


```


kubeletè¾“å‡º
```
ent="NodeHasSufficientPID"
I1219 05:56:58.059704   37131 kubelet_node_status.go:72] "Attempting to register node" node="192.168.102.230"
I1219 05:56:58.059860   37131 event.go:389] "Event occurred" object="192.168.102.230" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientMemory" message="Node 192.168.102.230 status is now: NodeHasSufficientMemory"
I1219 05:56:58.059892   37131 event.go:389] "Event occurred" object="192.168.102.230" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasNoDiskPressure" message="Node 192.168.102.230 status is now: NodeHasNoDiskPressure"
I1219 05:56:58.059900   37131 event.go:389] "Event occurred" object="192.168.102.230" fieldPath="" kind="Node" apiVersion="" type="Normal" reason="NodeHasSufficientPID" message="Node 192.168.102.230 status is now: NodeHasSufficientPID"
I1219 05:56:58.077561   37131 kubelet_node_status.go:111] "Node was previously registered" node="192.168.102.230"
I1219 05:56:58.077816   37131 kubelet_node_status.go:75] "Successfully registered node" node="192.168.102.230"
I1219 05:56:58.097306   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:58.098564   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:58.202836   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:58.203206   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:58.307148   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:58.307318   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:58.409691   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:58.410029   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:58.513072   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:58.513152   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:58.617537   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:58.617663   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:58.720189   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:58.720245   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:58.822460   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:58.822514   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:58.833937   37131 apiserver.go:50] "node sync has not completed yet"
I1219 05:56:58.833992   37131 apiserver.go:46] "node sync completed"
I1219 05:56:58.833995   37131 apiserver.go:52] "Watching apiserver"
I1219 05:56:58.834027   37131 reflector.go:305] Starting reflector *v1.Pod (0s) from pkg/kubelet/config/apiserver.go:66
I1219 05:56:58.834064   37131 reflector.go:341] Listing and watching *v1.Pod from pkg/kubelet/config/apiserver.go:66
I1219 05:56:58.836294   37131 reflector.go:368] Caches populated for *v1.Pod from pkg/kubelet/config/apiserver.go:66
I1219 05:56:58.836362   37131 config.go:292] "Setting pods for source" source="api"
I1219 05:56:58.836600   37131 config.go:397] "Receiving a new pod" pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:58.836670   37131 kubelet.go:2407] "SyncLoop ADD" source="api" pods=["tigera-operator/tigera-operator-56b74f76df-kb242"]
I1219 05:56:58.836769   37131 pod_workers.go:768] "Pod is being synced for the first time" pod="tigera-operator/tigera-operator-56b74f76df-kb242" podUID="04be5275-9b1c-465a-b701-67af82dbc671" updateType="create"
I1219 05:56:58.836789   37131 pod_workers.go:963] "Notifying pod of pending update" pod="tigera-operator/tigera-operator-56b74f76df-kb242" podUID="04be5275-9b1c-465a-b701-67af82dbc671" workType="sync"
I1219 05:56:58.836807   37131 pod_workers.go:1233] "Processing pod event" pod="tigera-operator/tigera-operator-56b74f76df-kb242" podUID="04be5275-9b1c-465a-b701-67af82dbc671" updateType="sync"
I1219 05:56:58.836823   37131 kubelet.go:1758] "SyncPod enter" pod="tigera-operator/tigera-operator-56b74f76df-kb242" podUID="04be5275-9b1c-465a-b701-67af82dbc671"
I1219 05:56:58.836846   37131 kubelet_pods.go:1774] "Generating pod status" podIsTerminal=false pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:58.836867   37131 kubelet_pods.go:1787] "Got phase for pod" pod="tigera-operator/tigera-operator-56b74f76df-kb242" oldPhase="Pending" phase="Pending"
I1219 05:56:58.836986   37131 status_manager.go:227] "Syncing updated statuses"
I1219 05:56:58.837068   37131 volume_manager.go:404] "Waiting for volumes to attach and mount for pod" pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:58.837089   37131 reflector.go:305] Starting reflector *v1.ConfigMap (0s) from object-"tigera-operator"/"kube-root-ca.crt"
I1219 05:56:58.837096   37131 reflector.go:341] Listing and watching *v1.ConfigMap from object-"tigera-operator"/"kube-root-ca.crt"
I1219 05:56:58.837145   37131 reflector.go:305] Starting reflector *v1.ConfigMap (0s) from object-"tigera-operator"/"kubernetes-services-endpoint"
I1219 05:56:58.837154   37131 reflector.go:341] Listing and watching *v1.ConfigMap from object-"tigera-operator"/"kubernetes-services-endpoint"
I1219 05:56:58.838431   37131 reflector.go:368] Caches populated for *v1.ConfigMap from object-"tigera-operator"/"kube-root-ca.crt"
I1219 05:56:58.838461   37131 reflector.go:368] Caches populated for *v1.ConfigMap from object-"tigera-operator"/"kubernetes-services-endpoint"
W1219 05:56:58.850981   37131 warnings.go:70] unknown field "status.containerStatuses[0].volumeMounts"
I1219 05:56:58.851284   37131 status_manager.go:872] "Patch status for pod" pod="tigera-operator/tigera-operator-56b74f76df-kb242" podUID="04be5275-9b1c-465a-b701-67af82dbc671" patch="{\"metadata\":{\"uid\":\"04be5275-9b1c-465a-b701-67af82dbc671\"},\"status\":{\"containerStatuses\":[{\"image\":\"quay.io/tigera/operator:v1.36.0\",\"imageID\":\"\",\"lastState\":{},\"name\":\"tigera-operator\",\"ready\":false,\"restartCount\":0,\"started\":false,\"state\":{\"waiting\":{\"reason\":\"ContainerCreating\"}},\"volumeMounts\":[{\"mountPath\":\"/var/lib/calico\",\"name\":\"var-lib-calico\",\"readOnly\":true,\"recursiveReadOnly\":\"Disabled\"},{\"mountPath\":\"/var/run/secrets/kubernetes.io/serviceaccount\",\"name\":\"kube-api-access-qx6bh\",\"readOnly\":true,\"recursiveReadOnly\":\"Disabled\"}]}]}}"
I1219 05:56:58.851321   37131 status_manager.go:881] "Status for pod updated successfully" pod="tigera-operator/tigera-operator-56b74f76df-kb242" statusVersion=1 status={"phase":"Pending","conditions":[{"type":"PodReadyToStartContainers","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-12-19T05:56:49Z"},{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-12-19T05:56:48Z"},{"type":"Ready","status":"False","lastProbeTime":null,"lastTransitionTime":"2024-12-19T05:56:48Z","reason":"ContainersNotReady","message":"containers with unready status: [tigera-operator]"},{"type":"ContainersReady","status":"False","lastProbeTime":null,"lastTransitionTime":"2024-12-19T05:56:48Z","reason":"ContainersNotReady","message":"containers with unready status: [tigera-operator]"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2024-12-19T05:56:48Z"}],"hostIP":"192.168.102.230","hostIPs":[{"ip":"192.168.102.230"}],"podIP":"192.168.102.230","podIPs":[{"ip":"192.168.102.230"}],"startTime":"2024-12-19T05:56:48Z","containerStatuses":[{"name":"tigera-operator","state":{"waiting":{"reason":"ContainerCreating"}},"lastState":{},"ready":false,"restartCount":0,"image":"quay.io/tigera/operator:v1.36.0","imageID":"","started":false,"volumeMounts":[{"name":"var-lib-calico","mountPath":"/var/lib/calico","readOnly":true,"recursiveReadOnly":"Disabled"},{"name":"kube-api-access-qx6bh","mountPath":"/var/run/secrets/kubernetes.io/serviceaccount","readOnly":true,"recursiveReadOnly":"Disabled"}]}],"qosClass":"BestEffort"}
I1219 05:56:58.862724   37131 desired_state_of_world.go:308] "expected volume SELinux label context" volume="var-lib-calico" label=""
I1219 05:56:58.862781   37131 desired_state_of_world.go:328] "volume does not support SELinux context mount, clearing the expected label" volume="var-lib-calico"
I1219 05:56:58.862803   37131 desired_state_of_world_populator.go:326] "Added volume to desired state" pod="tigera-operator/tigera-operator-56b74f76df-kb242" volumeName="var-lib-calico" volumeSpecName="var-lib-calico"
I1219 05:56:58.862823   37131 desired_state_of_world.go:308] "expected volume SELinux label context" volume="kube-api-access-qx6bh" label=""
I1219 05:56:58.862830   37131 desired_state_of_world.go:328] "volume does not support SELinux context mount, clearing the expected label" volume="kube-api-access-qx6bh"
I1219 05:56:58.862839   37131 desired_state_of_world_populator.go:326] "Added volume to desired state" pod="tigera-operator/tigera-operator-56b74f76df-kb242" volumeName="kube-api-access-qx6bh" volumeSpecName="kube-api-access-qx6bh"
I1219 05:56:58.862858   37131 desired_state_of_world_populator.go:154] "Finished populating initial desired state of world"
I1219 05:56:58.900157   37131 reconciler_common.go:207] "Starting operationExecutor.MountVolume for volume \"kube-api-access-qx6bh\" (UniqueName: \"kubernetes.io/projected/04be5275-9b1c-465a-b701-67af82dbc671-kube-api-access-qx6bh\") pod \"tigera-operator-56b74f76df-kb242\" (UID: \"04be5275-9b1c-465a-b701-67af82dbc671\") " pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:58.900277   37131 reconciler_common.go:218] "operationExecutor.MountVolume started for volume \"kube-api-access-qx6bh\" (UniqueName: \"kubernetes.io/projected/04be5275-9b1c-465a-b701-67af82dbc671-kube-api-access-qx6bh\") pod \"tigera-operator-56b74f76df-kb242\" (UID: \"04be5275-9b1c-465a-b701-67af82dbc671\") " pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:58.900346   37131 reconciler_common.go:245] "operationExecutor.VerifyControllerAttachedVolume started for volume \"var-lib-calico\" (UniqueName: \"kubernetes.io/host-path/04be5275-9b1c-465a-b701-67af82dbc671-var-lib-calico\") pod \"tigera-operator-56b74f76df-kb242\" (UID: \"04be5275-9b1c-465a-b701-67af82dbc671\") " pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:58.900320   37131 projected.go:185] Setting up volume kube-api-access-qx6bh for pod 04be5275-9b1c-465a-b701-67af82dbc671 at /var/lib/kubelet/pods/04be5275-9b1c-465a-b701-67af82dbc671/volumes/kubernetes.io~projected/kube-api-access-qx6bh
I1219 05:56:58.906641   37131 atomic_writer.go:185] pod tigera-operator/tigera-operator-56b74f76df-kb242 volume kube-api-access-qx6bh: write required for target directory /var/lib/kubelet/pods/04be5275-9b1c-465a-b701-67af82dbc671/volumes/kubernetes.io~projected/kube-api-access-qx6bh
I1219 05:56:58.907199   37131 atomic_writer.go:203] pod tigera-operator/tigera-operator-56b74f76df-kb242 volume kube-api-access-qx6bh: performed write of new data to ts data directory: /var/lib/kubelet/pods/04be5275-9b1c-465a-b701-67af82dbc671/volumes/kubernetes.io~projected/kube-api-access-qx6bh/..2024_12_19_05_56_58.2665344692
I1219 05:56:58.907547   37131 operation_generator.go:637] "MountVolume.SetUp succeeded for volume \"kube-api-access-qx6bh\" (UniqueName: \"kubernetes.io/projected/04be5275-9b1c-465a-b701-67af82dbc671-kube-api-access-qx6bh\") pod \"tigera-operator-56b74f76df-kb242\" (UID: \"04be5275-9b1c-465a-b701-67af82dbc671\") " pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:58.922048   37131 kuberuntime_manager.go:432] "Retrieved pods from runtime" all=true
I1219 05:56:58.924256   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:58.924332   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:58.943971   37131 reflector.go:341] Listing and watching *v1.Service from k8s.io/client-go/informers/factory.go:160
W1219 05:56:58.945919   37131 reflector.go:561] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: "spec.clusterIP" is not a known field selector: only "metadata.name", "metadata.namespace"
E1219 05:56:58.946134   37131 reflector.go:158] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: \"spec.clusterIP\" is not a known field selector: only \"metadata.name\", \"metadata.namespace\"" logger="UnhandledError"
I1219 05:56:59.000972   37131 reconciler_common.go:207] "Starting operationExecutor.MountVolume for volume \"var-lib-calico\" (UniqueName: \"kubernetes.io/host-path/04be5275-9b1c-465a-b701-67af82dbc671-var-lib-calico\") pod \"tigera-operator-56b74f76df-kb242\" (UID: \"04be5275-9b1c-465a-b701-67af82dbc671\") " pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:59.001410   37131 reconciler_common.go:218] "operationExecutor.MountVolume started for volume \"var-lib-calico\" (UniqueName: \"kubernetes.io/host-path/04be5275-9b1c-465a-b701-67af82dbc671-var-lib-calico\") pod \"tigera-operator-56b74f76df-kb242\" (UID: \"04be5275-9b1c-465a-b701-67af82dbc671\") " pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:59.001482   37131 operation_generator.go:637] "MountVolume.SetUp succeeded for volume \"var-lib-calico\" (UniqueName: \"kubernetes.io/host-path/04be5275-9b1c-465a-b701-67af82dbc671-var-lib-calico\") pod \"tigera-operator-56b74f76df-kb242\" (UID: \"04be5275-9b1c-465a-b701-67af82dbc671\") " pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:59.027542   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:59.027917   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:59.129801   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:59.130218   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:59.137519   37131 volume_manager.go:440] "All volumes are attached and mounted for pod" pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:59.137624   37131 kuberuntime_manager.go:970] "Container of pod is not in the desired state and shall be started" containerName="tigera-operator" pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:59.137637   37131 kuberuntime_manager.go:1055] "computePodActions got for pod" podActions="KillPod: false, CreateSandbox: false, UpdatePodResources: false, Attempt: 0, InitContainersToStart: [], ContainersToStart: [0], EphemeralContainersToStart: [],ContainersToUpdate: map[], ContainersToKill: map[]" pod="tigera-operator/tigera-operator-56b74f76df-kb242"
I1219 05:56:59.137784   37131 kuberuntime_manager.go:1251] "Creating container in pod" containerType="container" container="&Container{Name:tigera-operator,Image:quay.io/tigera/operator:v1.36.0,Command:[operator],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:WATCH_NAMESPACE,Value:,ValueFrom:nil,},EnvVar{Name:POD_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:OPERATOR_NAME,Value:tigera-operator,ValueFrom:nil,},EnvVar{Name:TIGERA_OPERATOR_INIT_IMAGE_VERSION,Value:v1.36.0,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:var-lib-calico,ReadOnly:true,MountPath:/var/lib/calico,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-qx6bh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:kubernetes-services-endpoint,},Optional:*true,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,}" pod="tigera-operator/tigera-operator-56b74f76df-kb242"
E1219 05:56:59.138638   37131 kuberuntime_manager.go:1274] "Unhandled Error" err="container &Container{Name:tigera-operator,Image:quay.io/tigera/operator:v1.36.0,Command:[operator],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{EnvVar{Name:WATCH_NAMESPACE,Value:,ValueFrom:nil,},EnvVar{Name:POD_NAME,Value:,ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,},},EnvVar{Name:OPERATOR_NAME,Value:tigera-operator,ValueFrom:nil,},EnvVar{Name:TIGERA_OPERATOR_INIT_IMAGE_VERSION,Value:v1.36.0,ValueFrom:nil,},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:var-lib-calico,ReadOnly:true,MountPath:/var/lib/calico,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},VolumeMount{Name:kube-api-access-qx6bh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{EnvFromSource{Prefix:,ConfigMapRef:&ConfigMapEnvSource{LocalObjectReference:LocalObjectReference{Name:kubernetes-services-endpoint,},Optional:*true,},SecretRef:nil,},},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod tigera-operator-56b74f76df-kb242_tigera-operator(04be5275-9b1c-465a-b701-67af82dbc671): CreateContainerConfigError: services have not yet been read at least once, cannot construct envvars" logger="UnhandledError"
I1219 05:56:59.138838   37131 event.go:389] "Event occurred" object="tigera-operator/tigera-operator-56b74f76df-kb242" fieldPath="spec.containers{tigera-operator}" kind="Pod" apiVersion="v1" type="Normal" reason="Pulled" message="Container image \"quay.io/tigera/operator:v1.36.0\" already present on machine"
I1219 05:56:59.140134   37131 event.go:389] "Event occurred" object="tigera-operator/tigera-operator-56b74f76df-kb242" fieldPath="spec.containers{tigera-operator}" kind="Pod" apiVersion="v1" type="Warning" reason="Failed" message="Error: services have not yet been read at least once, cannot construct envvars"
I1219 05:56:59.141473   37131 kubelet.go:1764] "SyncPod exit" pod="tigera-operator/tigera-operator-56b74f76df-kb242" podUID="04be5275-9b1c-465a-b701-67af82dbc671" isTerminal=false
E1219 05:56:59.141512   37131 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"tigera-operator\" with CreateContainerConfigError: \"services have not yet been read at least once, cannot construct envvars\"" pod="tigera-operator/tigera-operator-56b74f76df-kb242" podUID="04be5275-9b1c-465a-b701-67af82dbc671"
I1219 05:56:59.142136   37131 pod_workers.go:1338] "Processing pod event done" pod="tigera-operator/tigera-operator-56b74f76df-kb242" podUID="04be5275-9b1c-465a-b701-67af82dbc671" updateType="sync"
I1219 05:56:59.234456   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:59.234485   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:59.337851   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:59.337946   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:59.440760   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:59.440798   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:59.542854   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:59.542989   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
I1219 05:56:59.644427   37131 kubelet.go:2895] "Container runtime status" status="Runtime Conditions: RuntimeReady=true reason: message:, NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized, ContainerdHasNoDeprecationWarnings=true reason: message:; Handlers: , Features: nil"
E1219 05:56:59.644553   37131 kubelet.go:2902] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"

```


containerd è¾“å‡º
```

UnprivilegedPorts:false EnableUnprivilegedICMP:false EnableCDI:false CDISpecDirs:[/etc/cdi /var/run/cdi] ImagePullProgressTimeout:15m0s DrainExecSyncIOTimeout:0s ImagePullWithSyncFs:false IgnoreDeprecationWarnings:[]} ContainerdRootDir:/var/lib/containerd ContainerdEndpoint:/run/containerd/containerd.sock RootDir:/var/lib/containerd/io.containerd.grpc.v1.cri StateDir:/run/containerd/io.containerd.grpc.v1.cri}"
(reverse-i-search)`vim': ^Cm /etc/kibana/config/kibana.yml 
root@192:~/generic_architecture# vim /etc/containerd/config.toml
root@192:~/generic_architecture# systemctl restart containerd
root@192:~/generic_architecture# journalctl -fu containerd
Dec 19 05:56:48 192.168.102.230 containerd[36825]: time="2024-12-19T05:56:48.521223723Z" level=info msg="TearDown network for sandbox \"a27e45dc9375af4ab103632c054008ca94453ce43ae64be1630b3ac440be9e0a\" successfully"
Dec 19 05:56:48 192.168.102.230 containerd[36825]: time="2024-12-19T05:56:48.521297276Z" level=info msg="StopPodSandbox for \"a27e45dc9375af4ab103632c054008ca94453ce43ae64be1630b3ac440be9e0a\" returns successfully"
Dec 19 05:56:48 192.168.102.230 containerd[36825]: time="2024-12-19T05:56:48.749025102Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:tigera-operator-56b74f76df-kb242,Uid:04be5275-9b1c-465a-b701-67af82dbc671,Namespace:tigera-operator,Attempt:0,}"
Dec 19 05:56:48 192.168.102.230 containerd[36825]: time="2024-12-19T05:56:48.768922260Z" level=info msg="loading plugin \"io.containerd.event.v1.publisher\"..." runtime=io.containerd.runc.v2 type=io.containerd.event.v1
Dec 19 05:56:48 192.168.102.230 containerd[36825]: time="2024-12-19T05:56:48.768981468Z" level=info msg="loading plugin \"io.containerd.internal.v1.shutdown\"..." runtime=io.containerd.runc.v2 type=io.containerd.internal.v1
Dec 19 05:56:48 192.168.102.230 containerd[36825]: time="2024-12-19T05:56:48.768992923Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.task\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 19 05:56:48 192.168.102.230 containerd[36825]: time="2024-12-19T05:56:48.769194324Z" level=info msg="loading plugin \"io.containerd.ttrpc.v1.pause\"..." runtime=io.containerd.runc.v2 type=io.containerd.ttrpc.v1
Dec 19 05:56:48 192.168.102.230 containerd[36825]: time="2024-12-19T05:56:48.840085232Z" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:tigera-operator-56b74f76df-kb242,Uid:04be5275-9b1c-465a-b701-67af82dbc671,Namespace:tigera-operator,Attempt:0,} returns sandbox id \"8553992ad21317e7a995a33eacef59442a66080abcb42b206ff143cb23557472\""
Dec 19 05:56:53 192.168.102.230 containerd[36825]: time="2024-12-19T05:56:53.892818421Z" level=info msg="No cni config template is specified, wait for other system components to drop the config."
Dec 19 05:56:57 192.168.102.230 containerd[36825]: time="2024-12-19T05:56:57.835999208Z" level=info msg="No cni config template is specified, wait for other system components to drop the config."

```



containerdé…ç½®
```
gins = []
imports = []
oom_score = 0
plugin_dir = ""
required_plugins = []
root = "/var/lib/containerd" 
state = "/run/containerd"
temp = ""
version = 2

[cgroup]
  path = ""

[debug]
  address = ""
  format = ""
  gid = 0
  level = ""
  uid = 0

[grpc]
  address = "/run/containerd/containerd.sock"
  gid = 0
  max_recv_message_size = 16777216
  max_send_message_size = 16777216
  tcp_address = ""
  tcp_tls_ca = ""
  tcp_tls_cert = ""
  tcp_tls_key = ""
  uid = 0

[metrics]
  address = ""
  grpc_histogram = false

[plugins]

  [plugins."io.containerd.gc.v1.scheduler"]
    deletion_threshold = 0
    mutation_threshold = 100
    pause_threshold = 0.02
    schedule_delay = "0s"
    startup_delay = "100ms"

  [plugins."io.containerd.grpc.v1.cri"]
    cdi_spec_dirs = ["/etc/cdi", "/var/run/cdi"]
    device_ownership_from_security_context = false
    disable_apparmor = false
    disable_cgroup = false
    disable_hugetlb_controller = true
    disable_proc_mount = false
    disable_tcp_service = true
    drain_exec_sync_io_timeout = "0s"
    enable_cdi = false
    enable_selinux = false
    enable_tls_streaming = false
    enable_unprivileged_icmp = false
    enable_unprivileged_ports = false
    ignore_image_defined_volumes = false
    image_pull_progress_timeout = "15m0s"
    max_concurrent_downloads = 3
    max_container_log_line_size = 16384
    netns_mounts_under_state_dir = false
    restrict_oom_score_adj = false
    sandbox_image = "registry.aliyuncs.com/google_containers/pause:3.8"
    selinux_category_range = 1024
    stats_collect_period = 10
    stream_idle_timeout = "4h0m0s"
    stream_server_address = "127.0.0.1"
    stream_server_port = "0"
    systemd_cgroup = false
    tolerate_missing_hugetlb_controller = true
    unset_seccomp_profile = ""

    [plugins."io.containerd.grpc.v1.cri".cni]
      bin_dir = "/opt/cni/bin"
      conf_dir = "/etc/cni/net.d"
      #conf_template = "/etc/cni/net.d/10-default.conf"
      ip_pref = ""
      max_conf_num = 1
      setup_serially = false

    [plugins."io.containerd.grpc.v1.cri".containerd]
      default_runtime_name = "runc"
      disable_snapshot_annotations = true
      discard_unpacked_layers = false
      ignore_blockio_not_enabled_errors = false
      ignore_rdt_not_enabled_errors = false
      no_pivot = false
      snapshotter = "overlayfs"

      [plugins."io.containerd.grpc.v1.cri".containerd.default_runtime]
        base_runtime_spec = ""
        cni_conf_dir = ""
        cni_max_conf_num = 0
        container_annotations = []
        pod_annotations = []
        privileged_without_host_devices = false
        privileged_without_host_devices_all_devices_allowed = false
        runtime_engine = ""
        runtime_path = ""
        runtime_root = ""
        runtime_type = ""
        sandbox_mode = ""
        snapshotter = ""

        [plugins."io.containerd.grpc.v1.cri".containerd.default_runtime.options]

      [plugins."io.containerd.grpc.v1.cri".containerd.runtimes]

        [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc]
          base_runtime_spec = ""
          cni_conf_dir = ""
          cni_max_conf_num = 0
          container_annotations = []
          pod_annotations = []
          privileged_without_host_devices = false
          privileged_without_host_devices_all_devices_allowed = false
          runtime_engine = ""
          runtime_path = ""
          runtime_root = ""
          runtime_type = "io.containerd.runc.v2"
          sandbox_mode = "podsandbox"
          snapshotter = ""

          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            BinaryName = ""
            CriuImagePath = ""
            CriuPath = ""
            CriuWorkPath = ""
            IoGid = 0
            IoUid = 0
            NoNewKeyring = false
            NoPivotRoot = false
            Root = ""
            ShimCgroup = ""
            SystemdCgroup = true

      [plugins."io.containerd.grpc.v1.cri".containerd.untrusted_workload_runtime]
        base_runtime_spec = ""
        cni_conf_dir = ""
        cni_max_conf_num = 0
        container_annotations = []
        pod_annotations = []
        privileged_without_host_devices = false
        privileged_without_host_devices_all_devices_allowed = false
        runtime_engine = ""
        runtime_path = ""
        runtime_root = ""
        runtime_type = ""
        sandbox_mode = ""
        snapshotter = ""

        [plugins."io.containerd.grpc.v1.cri".containerd.untrusted_workload_runtime.options]

    [plugins."io.containerd.grpc.v1.cri".image_decryption]
      key_model = "node"

    [plugins."io.containerd.grpc.v1.cri".registry]
      config_path = "/etc/containerd/certs.d/"

      [plugins."io.containerd.grpc.v1.cri".registry.auths]

      [plugins."io.containerd.grpc.v1.cri".registry.configs]

      [plugins."io.containerd.grpc.v1.cri".registry.headers]

      [plugins."io.containerd.grpc.v1.cri".registry.mirrors]
    [plugins."io.containerd.grpc.v1.cri".x509_key_pair_streaming]
      tls_cert_file = ""
      tls_key_file = ""

  [plugins."io.containerd.internal.v1.opt"]
    path = "/opt/containerd"

  [plugins."io.containerd.internal.v1.restart"]
    interval = "10s"


  [plugins."io.containerd.metadata.v1.bolt"]
    content_sharing_policy = "shared"

  [plugins."io.containerd.monitor.v1.cgroups"]
    no_prometheus = false

  [plugins."io.containerd.nri.v1.nri"]
    disable = true
    disable_connections = false
    plugin_config_path = "/etc/nri/conf.d"
    plugin_path = "/opt/nri/plugins"
    plugin_registration_timeout = "5s"
    plugin_request_timeout = "2s"
    socket_path = "/var/run/nri/nri.sock"

  [plugins."io.containerd.runtime.v1.linux"]
    no_shim = false
    runtime = "runc"
    runtime_root = ""
    shim = "containerd-shim"
    shim_debug = false

  [plugins."io.containerd.runtime.v2.task"]
    sched_core = false

  [plugins."io.containerd.service.v1.diff-service"]
    default = ["walking"]

  [plugins."io.containerd.service.v1.tasks-service"]
    blockio_config_file = ""
    rdt_config_file = ""

  [plugins."io.containerd.snapshotter.v1.aufs"]
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.blockfile"]
    fs_type = ""
    mount_options = []
    root_path = ""
    scratch_file = ""

  [plugins."io.containerd.snapshotter.v1.btrfs"]
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.devmapper"]
    async_remove = false
    base_image_size = ""
    discard_blocks = false
    fs_options = ""
    fs_type = ""
    pool_name = ""
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.native"]
    root_path = ""

  [plugins."io.containerd.snapshotter.v1.overlayfs"]
    mount_options = []
    root_path = ""
    sync_remove = false
    upperdir_label = false

  [plugins."io.containerd.snapshotter.v1.zfs"]
    root_path = ""

  [plugins."io.containerd.tracing.processor.v1.otlp"]
    endpoint = ""
    insecure = false
    protocol = ""

  [plugins."io.containerd.transfer.v1.local"]
    config_path = ""
    max_concurrent_downloads = 3
    max_concurrent_uploaded_layers = 3

    [[plugins."io.containerd.transfer.v1.local".unpack_config]]
      differ = ""
      snapshotter = "overlayfs"

[proxy_plugins]

[stream_processors]

  [stream_processors."io.containerd.ocicrypt.decoder.v1.tar"]
    accepts = ["application/vnd.oci.image.layer.v1.tar+encrypted"]
    args = ["--decryption-keys-path", "/etc/containerd/ocicrypt/keys"]
    env = ["OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf"]
    path = "ctd-decoder"
    returns = "application/vnd.oci.image.layer.v1.tar"

  [stream_processors."io.containerd.ocicrypt.decoder.v1.tar.gzip"]
    accepts = ["application/vnd.oci.image.layer.v1.tar+gzip+encrypted"]
    args = ["--decryption-keys-path", "/etc/containerd/ocicrypt/keys"]
    env = ["OCICRYPT_KEYPROVIDER_CONFIG=/etc/containerd/ocicrypt/ocicrypt_keyprovider.conf"]
    path = "ctd-decoder"
    returns = "application/vnd.oci.image.layer.v1.tar+gzip"

[timeouts]
  "io.containerd.timeout.bolt.open" = "0s"
  "io.containerd.timeout.metrics.shimstats" = "2s"
  "io.containerd.timeout.shim.cleanup" = "5s"
  "io.containerd.timeout.shim.load" = "5s"
  "io.containerd.timeout.shim.shutdown" = "3s"
  "io.containerd.timeout.task.state" = "2s"

[ttrpc]
  address = ""
  gid = 0
  uid = 0

```

#### How can we reproduce it (as minimally and precisely as possible)?

éƒ¨ç½²è„šæœ¬
```
https://gitee.com/slcnx/generic_architecture/

ansible-playbook playbooks/docker/install.yml
ansible-playbook playbooks/cert/etcd.yml
cd playbooks/cert
docker compose up # å‡†å¤‡etcdé›†ç¾¤
ansible-playbook playbooks/cert/kubernetes.yml
ansible-playbook playbooks/cert/worker.yml
ansible-playbook playbooks/kubernetes/deploy-master.yml
ansible-playbook playbooks/kubernetes/deploy-worker.yml
ansible-playbook playbooks/kubernetes/deploy-cni.yml
```


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.12
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.12

```

</details>


#### Cloud provider

<details>
vmwareè™šæ‹Ÿæœº
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 24.04.1 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04.1 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo

$ uname -a
Linux 192.168.102.230 6.8.0-49-generic #49-Ubuntu SMP PREEMPT_DYNAMIC Mon Nov  4 02:06:24 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
calico 3.29.0
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®Issueçš„æè¿°ï¼Œç”¨æˆ·åœ¨ä½¿ç”¨äºŒè¿›åˆ¶éƒ¨ç½²Kubernetesæ—¶é‡åˆ°äº†é—®é¢˜ï¼ŒPodæ— æ³•æ­£å¸¸å¯åŠ¨ï¼Œå‡ºç°é”™è¯¯ä¿¡æ¯ï¼šâ€œservices have not yet been read at least once, cannot construct envvarsâ€ã€‚ä»æä¾›çš„æ—¥å¿—å’Œé…ç½®æ–‡ä»¶å¯ä»¥çœ‹å‡ºï¼Œä¸»è¦é—®é¢˜åœ¨äºCNIæ’ä»¶æœªåˆå§‹åŒ–ï¼Œå¯¼è‡´ç½‘ç»œæœªå°±ç»ªã€‚å…·ä½“è¡¨ç°ä¸ºï¼š

- kubeletæ—¥å¿—ä¸­å¤šæ¬¡å‡ºç°`Network plugin returns error: cni plugin not initialized`çš„é”™è¯¯ã€‚
- containerdæ—¥å¿—ä¸­æç¤º`No cni config template is specified, wait for other system components to drop the config`ï¼Œè¯´æ˜CNIé…ç½®ç¼ºå¤±æˆ–æœªæ­£ç¡®åŠ è½½ã€‚

è¿™ä¸€é—®é¢˜çš„æ ¹æœ¬åŸå› æ˜¯ç½‘ç»œæ’ä»¶ï¼ˆå¦‚Calicoï¼‰æœªæ­£ç¡®å®‰è£…æˆ–é…ç½®ï¼Œå¯¼è‡´Kubernetesçš„ç½‘ç»œåŠŸèƒ½ä¸å¯ç”¨ï¼Œè¿›è€Œå½±å“Podçš„åˆ›å»ºå’Œå¯åŠ¨ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¿™æ˜¯ä¸€ä¸ªé…ç½®é—®é¢˜æˆ–éƒ¨ç½²é—®é¢˜ï¼Œä¸æ¶‰åŠå®‰å…¨é—®é¢˜ã€‚

---


## Issue #129268 RBAC `cluster/rolebinding` created without `roleRef` and `SA` existence check

- Issue é“¾æ¥ï¼š[#129268](https://github.com/kubernetes/kubernetes/issues/129268)

### Issue å†…å®¹

#### What happened?

When creating RBAC `rolebinding` and `clusterrolebinding` with subjects `ServiceAccount`, there is no `roleRef` (`Role/ClusterRole`) and `SA` existence check.

#### What did you expect to happen?

Shoud check both `roleRef` (`Role/ClusterRole`) and `SA` existence.

#### How can we reproduce it (as minimally and precisely as possible)?

rolebinding-test.yaml
```
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: rb-test
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: role-not-exist # should check Role existence
subjects:
- kind: ServiceAccount
  name: sa-not-exist # should check SA existence
  namespace: default
```

clusterrolebinding-test.yaml
```
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: crb-test
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-role-not-exist # should check ClusterRole existence
subjects:
- kind: ServiceAccount
  name: sa-not-exist # should check SA existence
  namespace: default
```

`kubectl apply -f xxx.yaml` will reproduce this issue.

#### Anything else we need to know?

Issue tested in 1.31.

#### Kubernetes version

1.31

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŒ‡å‡ºï¼Œåœ¨åˆ›å»ºKubernetesçš„`RoleBinding`å’Œ`ClusterRoleBinding`æ—¶ï¼Œå¦‚æœæŒ‡å®šçš„`roleRef`ï¼ˆå³`Role`æˆ–`ClusterRole`ï¼‰æˆ–`ServiceAccount`ï¼ˆå³`SA`ï¼‰ä¸å­˜åœ¨ï¼Œç³»ç»Ÿä¸ä¼šè¿›è¡Œå­˜åœ¨æ€§æ£€æŸ¥ï¼Œä»ç„¶å…è®¸åˆ›å»ºç»‘å®šå¯¹è±¡ã€‚è¿™å¯èƒ½å¯¼è‡´ç®¡ç†å‘˜åœ¨é…ç½®RBACæƒé™æ—¶å‡ºç°é”™è¯¯é…ç½®ï¼Œç»‘å®šäº†ä¸å­˜åœ¨çš„è§’è‰²æˆ–æœåŠ¡è´¦æˆ·ï¼Œä»è€Œå¯¼è‡´é¢„æœŸçš„æƒé™æœªèƒ½æ­£ç¡®èµ‹äºˆã€‚

ä»å®‰å…¨é£é™©çš„è§’åº¦æ¥çœ‹ï¼Œæ­¤é—®é¢˜ä¸»è¦æ¶‰åŠåˆ°é…ç½®çš„æ­£ç¡®æ€§å’Œå®Œæ•´æ€§ï¼Œè€Œä¸å±äºç›´æ¥çš„å®‰å…¨æ¼æ´ã€‚å› ä¸ºå³ä½¿ç»‘å®šäº†ä¸å­˜åœ¨çš„`Role`æˆ–`ServiceAccount`ï¼Œä¹Ÿä¸ä¼šèµ‹äºˆä»»ä½•å®é™…æƒé™ï¼Œä¸ä¼šé€ æˆæƒé™æå‡æˆ–ä¿¡æ¯æ³„éœ²ç­‰å®‰å…¨é—®é¢˜ã€‚æ­¤å¤–ï¼Œæ”»å‡»è€…æ— æ³•åˆ©ç”¨è¯¥é—®é¢˜ç›´æ¥è¿›è¡Œæ”»å‡»æˆ–è·å–æ›´é«˜æƒé™ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤Issueæè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œé£é™©è¯„çº§åˆ¤æ–­ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

---


## Issue #129266 informer list and watch keep str error log:too old resource version

- Issue é“¾æ¥ï¼š[#129266](https://github.com/kubernetes/kubernetes/issues/129266)

### Issue å†…å®¹

#### What happened?

informer list and watch keep str error log:too old resource version
resourceVersion stay at the same value. 

#### What did you expect to happen?

when watch have error:too old resource version,resourceVersion will change other value.
informer recovery available.


#### How can we reproduce it (as minimally and precisely as possible)?

maybe etcd leader changed.

#### Anything else we need to know?


if a list request have resourceVersion and have limit or continue,apiserver will query it from etcd and return request resourceVersion.
for example 
https://qqqq6443/api/v1/nodes?resourceVersion=1111&limit=10
response resourseVersion will is 1111.


informer
1.use page.list() to list resource from apiserver.
2.get resourceVersion from 1 result.
3.use resourceVersion to watch from apiserver.


then informer page.list will always return 1111.
watch will always std err too old resource version.
#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
åœ¨è¿™ä¸ªIssueä¸­ï¼Œæå‡ºè€…æè¿°äº†Kubernetesä¸­Informeråœ¨è¿›è¡Œèµ„æºçš„åˆ—è¡¨ï¼ˆListï¼‰å’Œç›‘è§†ï¼ˆWatchï¼‰æ“ä½œæ—¶ï¼ŒæŒç»­è¾“å‡ºé”™è¯¯æ—¥å¿—ï¼šâ€œtoo old resource versionâ€ï¼Œå¹¶ä¸”`resourceVersion`ä¿æŒä¸å˜ã€‚å½“Watchæ“ä½œå‡ºç°â€œtoo old resource versionâ€é”™è¯¯æ—¶ï¼Œä»–ä»¬æœŸæœ›`resourceVersion`èƒ½å¤Ÿæ›´æ–°ä¸ºå…¶ä»–å€¼ï¼Œä½¿Informerèƒ½å¤Ÿæ¢å¤æ­£å¸¸å·¥ä½œã€‚

ä»–ä»¬æ¨æµ‹å¯èƒ½æ˜¯ç”±äºetcdçš„leaderå˜æ›´å¯¼è‡´çš„ã€‚æ­¤å¤–ï¼Œä»–ä»¬æåˆ°å½“Listè¯·æ±‚åŒ…å«`resourceVersion`å’Œ`limit`æˆ–`continue`å‚æ•°æ—¶ï¼ŒAPI Serverä¼šä»etcdä¸­æŸ¥è¯¢å¹¶è¿”å›è¯·æ±‚çš„`resourceVersion`ï¼Œè¿™å¯èƒ½å¯¼è‡´`resourceVersion`åœç•™åœ¨æ—§å€¼ï¼Œè¿›è€Œå¼•å‘Watchæ“ä½œçš„æŒç»­é”™è¯¯ã€‚

ä»ä¸Šè¿°æè¿°å¯ä»¥çœ‹å‡ºï¼Œè¿™æ˜¯ä¸€ä¸ªå…³äºKubernetesé›†ç¾¤ä¸­Informeræ¨¡å—åœ¨ç‰¹å®šæƒ…å†µä¸‹å¤„ç†`resourceVersion`é€»è¾‘å¯èƒ½å­˜åœ¨çš„é—®é¢˜ï¼Œå¯¼è‡´Informeræ— æ³•æ­£å¸¸æ›´æ–°èµ„æºç‰ˆæœ¬ï¼Œé™·å…¥é”™è¯¯å¾ªç¯ã€‚

ä»å®‰å…¨æ€§çš„è§’åº¦è€ƒè™‘ï¼š

- **æœºå¯†æ€§**ï¼šæ²¡æœ‰æ¶‰åŠä»»ä½•æ•æ„Ÿä¿¡æ¯çš„æ³„éœ²ã€‚
- **å®Œæ•´æ€§**ï¼šæ²¡æœ‰å‡ºç°æ•°æ®è¢«æœªæˆæƒç¯¡æ”¹çš„æƒ…å†µã€‚
- **å¯ç”¨æ€§**ï¼šè™½ç„¶Informerçš„å¼‚å¸¸å¯èƒ½å½±å“åˆ°é›†ç¾¤çš„éƒ¨åˆ†åŠŸèƒ½ï¼Œä½†è¿™æ›´åå‘äºå¯ç”¨æ€§é—®é¢˜ã€‚
- **æ”»å‡»å¯èƒ½æ€§**ï¼šæ²¡æœ‰æåˆ°ä»»ä½•å¯èƒ½è¢«æ¶æ„ç”¨æˆ·åˆ©ç”¨çš„æ¼æ´ï¼Œå¦‚æœªç»æˆæƒçš„è®¿é—®ã€ææƒç­‰ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤é—®é¢˜å¹¶éå®‰å…¨é—®é¢˜ï¼Œè€Œæ˜¯åŠŸèƒ½æ€§ç¼ºé™·æˆ–ç¨³å®šæ€§é—®é¢˜ã€‚

---


## Issue #129265 RBAC `cluster/rolebinding` created without subjects and `SA` namespace check

- Issue é“¾æ¥ï¼š[#129265](https://github.com/kubernetes/kubernetes/issues/129265)

### Issue å†…å®¹

#### What happened?

When creating RBAC `rolebinding` and `clusterrolebinding` with subjects `ServiceAccount`, there is no subjects empty check and namespace empty check.

#### What did you expect to happen?

Shoud check both subjects empty and namespace empty.

#### How can we reproduce it (as minimally and precisely as possible)?

rolebinding-test.yaml
```
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: rb-test
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: role-test
#subjects: # subjects is required
#- kind: ServiceAccount
#  name: sa-test
#  namespace: default # namespace is required
```

clusterrolebinding-test.yaml
```
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: crb-test
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-role-test
#subjects: # subjects is required
#- kind: ServiceAccount
#  name: sa-test
#  namespace: default # namespace is required
```

`kubectl apply -f xxx.yaml` will reproduce this issue.

#### Anything else we need to know?
Issue tested in 1.31.

#### Kubernetes version
1.31

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
åœ¨ Kubernetes ä¸­ï¼ŒRoleBinding å’Œ ClusterRoleBinding ç”¨äºå°†è§’è‰²ï¼ˆRole æˆ– ClusterRoleï¼‰ç»‘å®šåˆ°ç‰¹å®šçš„ä¸»ä½“ï¼ˆSubjectsï¼‰ï¼Œè¿™äº›ä¸»ä½“å¯ä»¥æ˜¯ç”¨æˆ·ã€ç”¨æˆ·ç»„æˆ–æœåŠ¡è´¦æˆ·ï¼ˆServiceAccountï¼‰ã€‚åœ¨åˆ›å»º RoleBinding æˆ– ClusterRoleBinding æ—¶ï¼Œsubjects å­—æ®µæ˜¯å¯é€‰çš„ï¼Œè¿™æ„å‘³ç€å¯ä»¥åˆ›å»ºä¸€ä¸ªæ²¡æœ‰ä»»ä½•ä¸»ä½“çš„ç»‘å®šå¯¹è±¡ã€‚

å¯¹äºæ²¡æœ‰ subjects çš„ç»‘å®šå¯¹è±¡ï¼Œå…¶å®å¹¶ä¸ä¼šèµ‹äºˆä»»ä½•ä¸»ä½“æƒé™ï¼Œå› æ­¤ä¸ä¼šå¯¼è‡´æƒé™æ³„æ¼æˆ–æœªæˆæƒè®¿é—®ã€‚ç›¸åï¼Œè¿™å¯èƒ½æ˜¯ç”¨æˆ·é…ç½®å¤±è¯¯ï¼Œå¯¼è‡´é¢„æœŸçš„æƒé™æœªèƒ½æ­£ç¡®åˆ†é…ã€‚ä½†è¿™å±äºåŠŸèƒ½ä½¿ç”¨ä¸Šçš„é—®é¢˜ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚

åŒæ ·åœ°ï¼Œå…³äº ServiceAccount çš„ namespace æœªæŒ‡å®šçš„é—®é¢˜ï¼Œå¦‚æœåœ¨ subjects ä¸­å¼•ç”¨äº† ServiceAccount ä½†æœªæŒ‡å®šå…¶å‘½åç©ºé—´ï¼ŒKubernetes é»˜è®¤ä¼šå°†å…¶è§†ä¸ºç»‘å®šåˆ°æ‰€åœ¨å‘½åç©ºé—´ä¸‹çš„ ServiceAccountã€‚å¦‚æœæ˜¯åœ¨ ClusterRoleBinding ä¸­å¼•ç”¨æœªæŒ‡å®šå‘½åç©ºé—´çš„ ServiceAccountï¼Œå¯èƒ½ä¼šäº§ç”Ÿé¢„æœŸä»¥å¤–çš„è¡Œä¸ºï¼Œä½†ä»ç„¶å±äºé…ç½®é—®é¢˜ï¼Œä¸ä¼šå¯¼è‡´æœªæˆæƒçš„æƒé™æå‡æˆ–æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥ Issue æè¿°çš„é—®é¢˜ä¸»è¦æ˜¯é…ç½®éªŒè¯ä¸å®Œå–„ï¼Œå¯èƒ½å¯¼è‡´ç”¨æˆ·è¯¯æ“ä½œï¼Œä½†å¹¶ä¸ç›´æ¥é€ æˆå®‰å…¨é£é™©ã€‚

---


## Issue #129261 Kubernetes appears to use a lot of memory for its own components (â‰…80GiB)

- Issue é“¾æ¥ï¼š[#129261](https://github.com/kubernetes/kubernetes/issues/129261)

### Issue å†…å®¹

#### What happened?

I recently installed Kubernetes on an Ubuntu 22.04 system environment. I set up the Kubernetes environment as follows: as you can see, I created one control node and two worker nodes.

```
-----------+---------------------------+--------------------------+------------
           |                           |                          |
       eth0|10.0.0.10              eth0|10.0.0.11             eth0|10.0.0.22
+----------+-----------+   +-----------+-----------+   +-----------+-----------+
|  [ ctrl.myk8s     ]  |   |  [ node01.myk8s    ]  |   |  [ node02.myk8s    ]  |
|     Control Node     |   |       Worker Node     |   |      Worker Node      |
+----------------------+   +-----------------------+   +-----------------------+
```

I started the Kubernetes service daemons and ran some pods. After a few hours, every time I ran the `free -h` command, I noticed that 80GB of memory was being used. Most of this memory usage is attributed to Kubernetes processes. 

I am curious why Kubernetes uses so much physical memory, even when running just one or two pods. I welcome any comments, hints, or information.

#### Memory usage: 

```bash
$ free -h 
    total          used        free       shared   buff/cache   available
Mem:           124Gi       85.6Gi       38Gi        21Mi       452Mi          38Gi
Swap:             0B              0B          0B

$  ps -eo uid,euser,wchan,comm,rss,vsize,%cpu,nlwp | awk '$5 != 0' | sort -k5,5rn

uid   euser    wchan  comm            rss   vsize   %cpu nlwp 
----------------------------------------------------------------------
    0 root     futex_ kubelet         117576 6850960 1.1   76
    0 root     futex_ containerd      100948 7420620 0.6   82
    0 root     futex_ calico-node     93024  5417080 0.4   56
    0 root     futex_ calico-node     83600  3494160 0.0   30
    0 root     futex_ calico-node     83192  3715612 0.0   33
    0 root     futex_ calico-node     80348  3641880 0.0   32
    0 root     futex_ calico-node     79912  3494416 0.0   30
    0 root     futex_ calico-node     74932  3493392 0.0   30
    0 root     futex_ kube-proxy      72944  1295544 0.0   32
    0 root     do_pol python3         31636   42076  0.0    1
    0 root     futex_ node-driver-reg 29436  1248032 0.0   32
    0 root     futex_ tuned           26628  257916  0.0    4
  997 polkitd  do_pol polkitd         22804  2983876 0.0   12
    0 root     futex_ csi-driver      21824  1237992 0.0   24
    0 root     do_pol NetworkManager  21480  259196  0.0    3
    0 root     futex_ containerd-shim 17492  1238184 0.0   14
    0 root     futex_ containerd-shim 16600  1238184 0.0   13
    0 root     ep_pol systemd         15192  175176  0.0    1
    0 root     futex_ containerd-shim 13688  1238184 0.0   13
    0 root     do_sel snmpd           13056   25748  0.0    1
    0 root     ep_pol systemd-journal 13056   27820  0.0    1
 1000 codegrok ep_pol systemd         13056   23928  0.0    1
    0 root     futex_ containerd-shim 12704  1238184 0.0   13

```


![k8s-85gb-mem-hog](https://github.com/user-attachments/assets/c4f4919f-db8f-467e-9f0d-73d81460d0dc)



#### What did you expect to happen?

I expected Kubernetes to use less than 8GB of memory when running just one or two pods.

#### How can we reproduce it (as minimally and precisely as possible)?

#### [1] 	Install Containerd and apply some requirements on all Nodes.
```
root@ctrl:~# apt -y install containerd
root@ctrl:~# cat > /etc/sysctl.d/99-k8s-cri.conf <<EOF
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward=1
EOF
root@ctrl:~# sysctl --system
root@ctrl:~# modprobe overlay; modprobe br_netfilter

root@ctrl:~# echo -e overlay\\nbr_netfilter > /etc/modules-load.d/k8s.conf
# needs [iptables-legacy] for iptables backend
# if nftables is enabled, change to [iptables-legacy]

root@ctrl:~# update-alternatives --config iptables

There are 2 choices for the alternative iptables (providing /usr/sbin/iptables).

  Selection    Path                       Priority   Status
------------------------------------------------------------
* 0            /usr/sbin/iptables-nft      20        auto mode
  1            /usr/sbin/iptables-legacy   10        manual mode
  2            /usr/sbin/iptables-nft      20        manual mode

Press <enter> to keep the current choice[*], or type selection number: 1
update-alternatives: using /usr/sbin/iptables-legacy to provide /usr/sbin/iptables (iptables) in manual mode

# disable swap

root@ctrl:~# swapoff -a

root@ctrl:~# vi /etc/fstab

# comment out
#/swap.img      none    swap    sw      0       0

# switch to Cgroup v1 (default is v2)

root@ctrl:~# vi /etc/default/grub
# line 11 : add

GRUB_CMDLINE_LINUX="systemd.unified_cgroup_hierarchy=0
"
root@ctrl:~# update-grub
```



#### [2] 	Install Kubeadm, Kubelet, Kubectl on all Nodes.
```
root@ctrl:~# curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg -o /etc/apt/keyrings/kubernetes-keyring.gpg

root@ctrl:~# echo "deb [signed-by=/etc/apt/keyrings/kubernetes-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | tee /etc/apt/sources.list.d/kubernetes.list

root@ctrl:~# apt update

root@ctrl:~# apt -y install kubeadm kubelet kubectl
root@ctrl:~# vi /etc/default/kubelet
# create new

KUBELET_EXTRA_ARGS=--cgroup-driver=systemd --container-runtime=remote --container-runtime-endpoint=unix:///run/containerd/containerd.sock
root@ctrl:~# systemctl edit containerd.service
# add follows

[Service]
KillMode=
KillMode=mixed
root@ctrl:~# systemctl restart containerd.service 
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
# kubectl version
Client Version: v1.31.3
Kustomize Version: v5.4.2
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:

$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.4 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.4 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

$ uname -a
Linux mymate 6.5.0-41-generic #41~22.04.2-Ubuntu SMP PREEMPT_DYNAMIC Mon Jun  3 11:32:55 UTC 2 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®Issueçš„æè¿°ï¼Œç”¨æˆ·åœ¨Ubuntu 22.04ç³»ç»Ÿä¸Šå®‰è£…äº†Kubernetesé›†ç¾¤ï¼ˆä¸€ä¸ªæ§åˆ¶èŠ‚ç‚¹ï¼Œä¸¤ä¸ªå·¥ä½œèŠ‚ç‚¹ï¼‰ï¼Œå‘ç°Kubernetesè‡ªèº«ç»„ä»¶ä½¿ç”¨äº†å¤§é‡çš„å†…å­˜ï¼ˆçº¦80GiBï¼‰ï¼Œå³ä½¿åªè¿è¡Œäº†ä¸€ä¸¤ä¸ªpodã€‚ç”¨æˆ·å¯¹ä¸ºä»€ä¹ˆä¼šå‡ºç°å¦‚æ­¤é«˜çš„å†…å­˜å ç”¨æ„Ÿåˆ°ç–‘æƒ‘ï¼Œå¹¶å¸Œæœ›è·å¾—è§£é‡Šã€‚

ä»Issueå†…å®¹æ¥çœ‹ï¼Œè¿™æ˜¯ä¸€ä¸ªå…³äºKuberneteså†…å­˜æ¶ˆè€—çš„æ€§èƒ½é—®é¢˜ï¼Œæ¶‰åŠKubernetesç»„ä»¶ï¼ˆå¦‚kubeletã€containerdã€calico-nodeç­‰ï¼‰å ç”¨å†…å­˜è¿‡é«˜çš„é—®é¢˜ã€‚ç”¨æˆ·æä¾›äº†å†…å­˜ä½¿ç”¨æƒ…å†µçš„è¾“å‡ºï¼Œä»¥åŠå®‰è£…å’Œé…ç½®Kubernetesçš„æ­¥éª¤ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. Issueæè¿°çš„é—®é¢˜æ˜¯Kubernetesç»„ä»¶çš„é«˜å†…å­˜ä½¿ç”¨ï¼Œå±äºæ€§èƒ½ä¼˜åŒ–æˆ–èµ„æºç®¡ç†çš„é—®é¢˜ï¼Œè€Œéå®‰å…¨é—®é¢˜ã€‚
2. æ²¡æœ‰æ¶‰åŠä»»ä½•å®‰å…¨æ¼æ´æˆ–å®‰å…¨é£é™©çš„æè¿°ï¼Œä¾‹å¦‚ä»£ç æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ã€ææƒç­‰ã€‚
3. æ²¡æœ‰æåŠä»»ä½•æ•æ„Ÿä¿¡æ¯çš„æ³„éœ²ï¼Œæˆ–å¯èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨çš„æ¼æ´ã€‚

å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---


## Issue #129260  add a non-existent owner reference to a resource can cause it to be silently deleted

- Issue é“¾æ¥ï¼š[#129260](https://github.com/kubernetes/kubernetes/issues/129260)

### Issue å†…å®¹

#### What happened?

 if we add a non-existent owner reference to a resource, the API **does not return an error when doing the modification**. However, the resource is deleted **silently** in the background.

#### What did you expect to happen?

1. Do we need to return an error when the owner dose not exist ?
2. I am aware of an issue that one can delete a resource only with update permissions by hacking a non-existent owner reference to it. This should be regarded as an authorization issue.

#### How can we reproduce it (as minimally and precisely as possible)?

1. create an arbitary resource
2. `kubectl edit` the resource, add a non-existent owner reference to a resource.
3. `kubectl get` the resource. the resource will be not found.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"27", GitVersion:"v1.27.4", GitCommit:"fa3d7990104d7c1f16943a67f11b154b71f6a132", GitTreeState:"clean", BuildDate:"2023-07-19T12:20:54Z", GoVersion:"go1.20.6", Compiler:"gc", Platform:"linux/amd64"}
Kustomize Version: v5.0.1
Server Version: version.Info{Major:"1", Minor:"30", GitVersion:"v1.30.5-gke.1443001", GitCommit:"d16b6cc50798c61dc2740487abdaa656af180538", GitTreeState:"clean", BuildDate:"2024-10-07T04:15:31Z", GoVersion:"go1.22.6 X:boringcrypto", Compiler:"gc", Platform:"linux/amd64"}
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Ubuntu"
VERSION="20.04.6 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.6 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal
$ uname -a
Linux 5.15.0-1062-gcp #70~20.04.1-Ubuntu SMP Fri May 24 20:12:18 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
**é—®é¢˜æè¿°**ï¼š

åœ¨ Kubernetes ä¸­ï¼Œå¦‚æœå¯¹æŸä¸ªèµ„æºæ·»åŠ äº†ä¸€ä¸ªä¸å­˜åœ¨çš„æ‰€æœ‰è€…å¼•ç”¨ï¼ˆOwner Referenceï¼‰ï¼ŒAPI åœ¨è¿›è¡Œä¿®æ”¹æ“ä½œæ—¶å¹¶ä¸ä¼šè¿”å›é”™è¯¯æç¤ºã€‚ç„¶è€Œï¼Œè¯¥èµ„æºä¼šåœ¨åå°è¢«â€œé™é»˜â€åœ°åˆ é™¤ã€‚

**å®‰å…¨é£é™©åˆ†æ**ï¼š

1. **æƒé™ç»•è¿‡**ï¼šç”¨æˆ·åªéœ€è¦æ‹¥æœ‰å¯¹èµ„æºçš„æ›´æ–°æƒé™ï¼ˆupdateï¼‰ï¼Œå°±å¯ä»¥é€šè¿‡æ·»åŠ ä¸€ä¸ªä¸å­˜åœ¨çš„æ‰€æœ‰è€…å¼•ç”¨æ¥åˆ é™¤èµ„æºï¼Œè€Œä¸éœ€è¦å®é™…çš„åˆ é™¤æƒé™ï¼ˆdeleteï¼‰ã€‚è¿™å¯èƒ½å¯¼è‡´æƒé™æå‡çš„é—®é¢˜ã€‚

2. **å½±å“èŒƒå›´**ï¼šè¯¥é—®é¢˜å¯èƒ½å¯¼è‡´éé¢„æœŸçš„èµ„æºåˆ é™¤ï¼Œå½±å“ç³»ç»Ÿçš„å®Œæ•´æ€§å’Œå¯ç”¨æ€§ã€‚

3. **å¤ç°éš¾åº¦**ï¼šæ”»å‡»è€…åªéœ€ç¼–è¾‘èµ„æºå¹¶æ·»åŠ ä¸å­˜åœ¨çš„æ‰€æœ‰è€…å¼•ç”¨å³å¯ï¼Œæ“ä½œç®€å•ï¼Œæ”»å‡»å¤æ‚åº¦ä½ã€‚

**CVSS 3.1 é£é™©è¯„åˆ†**ï¼š

- **æ”»å‡»å‘é‡ï¼ˆAVï¼‰**ï¼šç½‘ç»œï¼ˆNï¼‰â€”â€”æ”»å‡»è€…é€šè¿‡ç½‘ç»œå¯¹ Kubernetes API å‘èµ·è¯·æ±‚ã€‚
- **æ”»å‡»å¤æ‚åº¦ï¼ˆACï¼‰**ï¼šä½ï¼ˆLï¼‰â€”â€”æ”»å‡»ä¸éœ€è¦ç‰¹æ®Šæ¡ä»¶æˆ–ç¯å¢ƒã€‚
- **æƒé™è¦æ±‚ï¼ˆPRï¼‰**ï¼šä½ï¼ˆLï¼‰â€”â€”éœ€è¦å…·å¤‡å¯¹èµ„æºçš„æ›´æ–°æƒé™ã€‚
- **ç”¨æˆ·äº¤äº’ï¼ˆUIï¼‰**ï¼šæ— ï¼ˆNï¼‰â€”â€”ä¸éœ€è¦å…¶ä»–ç”¨æˆ·çš„äº¤äº’ã€‚
- **ä½œç”¨èŒƒå›´ï¼ˆSï¼‰**ï¼šæœªæ”¹å˜ï¼ˆUï¼‰â€”â€”æ”»å‡»ä»…å½±å“å—å®³è€…æ‰€åœ¨çš„æƒé™åŸŸã€‚
- **æœºå¯†æ€§ï¼ˆCï¼‰**ï¼šæ— ï¼ˆNï¼‰â€”â€”ä¸å½±å“æ•°æ®çš„æœºå¯†æ€§ã€‚
- **å®Œæ•´æ€§ï¼ˆIï¼‰**ï¼šä½ï¼ˆLï¼‰â€”â€”å¯¹æ•°æ®å®Œæ•´æ€§æœ‰ä¸€å®šå½±å“ã€‚
- **å¯ç”¨æ€§ï¼ˆAï¼‰**ï¼šé«˜ï¼ˆHï¼‰â€”â€”å¯¼è‡´èµ„æºè¢«åˆ é™¤ï¼Œä¸¥é‡å½±å“å¯ç”¨æ€§ã€‚

**ç»¼åˆè¯„åˆ†**ï¼š6.1ï¼ˆä¸­ç­‰ï¼‰

è®¡ç®—ç»“æœå¯å‚è€ƒ CVSS 3.1 è®¡ç®—å™¨ã€‚

---


## Issue #129259 Recreate strategy doesn't create new replicaset on its own

- Issue é“¾æ¥ï¼š[#129259](https://github.com/kubernetes/kubernetes/issues/129259)

### Issue å†…å®¹

#### What happened?

We've switched some of our deployments to the recreate strategy and as a result we're seeing long delays between a replicaset scaling down and a new one scaling up when a new version is rolled out (10+ minutes between events). This can be due to a number of things but it seemed to only impact our workloads on the recreate strategy. After digging into the source code it appears that the recreate logic doesn't actually create the new replicaset even though it seems like it should. Instead it looks like it exits out early and relies on the cluster/replicaset controller to naturally resolve the out of sync state when it gets around to it (which in our case can be pretty slow). This can result in very long deploy cycles for workloads using the recreate strategy. The issue I believe is stemming from [here](https://github.com/kubernetes/kubernetes/blob/v1.31.2/pkg/controller/deployment/recreate.go#L45) where we return early after updating the deployment status if the previous replicaset has scaled down regardless of the new replicaset. Checking the history of the code this has been around for [9 years](https://github.com/kubernetes/kubernetes/commit/92798408af90569f492be3a1a4d8de02538a6787#diff-c553184997fa4c9f63e697af2f39eb2e6fb43eeff64a0ef66c9e955f61347b02R428) so either I am not interpreting what should be happening correctly or just no one has noticed but I would expect that the code doesn't return there but instead continues to spin up the new replicaset like the logic after that return statement does in that method.

#### What did you expect to happen?

I expected my deployments that use the recreate strategy to deploy their new replicasets shortly after the old ones have been scaled down.

#### How can we reproduce it (as minimally and precisely as possible)?

A unit test could probably reproduce this but I'm more curious about an explanation of the code to make sure I understand it correctly.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
v1.31.2

```console
$ kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
```

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetesä¸­ä½¿ç”¨`recreate`ç­–ç•¥è¿›è¡Œéƒ¨ç½²æ—¶ï¼Œå‘ç°æ—§çš„ReplicaSetç¼©å®¹åï¼Œæ–°çš„ReplicaSetåœ¨å¯åŠ¨æ—¶å­˜åœ¨è¾ƒé•¿çš„å»¶è¿Ÿï¼ˆè¶…è¿‡10åˆ†é’Ÿï¼‰ã€‚ç»è¿‡å¯¹æºç çš„åˆ†æï¼Œå‘ç°å¯èƒ½æ˜¯ç”±äºåœ¨ç¼©å®¹æ—§çš„ReplicaSetåï¼Œä»£ç åœ¨æ›´æ–°DeploymentçŠ¶æ€åè¿‡æ—©åœ°è¿”å›ï¼Œå¯¼è‡´æ–°ReplicaSetçš„åˆ›å»ºä¾èµ–äºé›†ç¾¤/ReplicaSetæ§åˆ¶å™¨çš„è‡ªç„¶çŠ¶æ€åŒæ­¥ï¼Œè€Œéç«‹å³åˆ›å»ºæ–°çš„ReplicaSetã€‚è¿™å¯èƒ½å¯¼è‡´éƒ¨ç½²è¿‡ç¨‹çš„å»¶è¿Ÿã€‚

ä»é—®é¢˜çš„æè¿°æ¥çœ‹ï¼Œè¿™æ˜¯Kuberneteséƒ¨ç½²ç­–ç•¥å®ç°ä¸Šçš„åŠŸèƒ½æ€§é—®é¢˜ï¼Œå½±å“äº†éƒ¨ç½²çš„æ•ˆç‡å’Œæ€§èƒ½ï¼Œä½†å¹¶æœªæ¶‰åŠä»»ä½•å®‰å…¨é£é™©ã€‚å…·ä½“è€Œè¨€ï¼š

- **æœªæ¶‰åŠæœªç»æˆæƒçš„è®¿é—®æˆ–æƒé™æå‡**ï¼šæ²¡æœ‰æåˆ°ä½æƒé™ç”¨æˆ·å¯ä»¥åˆ©ç”¨è¯¥é—®é¢˜è·å–æ›´é«˜æƒé™æˆ–è®¿é—®æœªç»æˆæƒçš„èµ„æºã€‚
- **æœªæ¶‰åŠæ•æ„Ÿä¿¡æ¯æ³„éœ²**ï¼šæ²¡æœ‰æ¶‰åŠä»»ä½•æ•æ„Ÿä¿¡æ¯çš„æŠ«éœ²ï¼Œå¦‚å‡­æ®ã€å¯†é’¥ç­‰ã€‚
- **æœªæ¶‰åŠè¿œç¨‹ä»£ç æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰é«˜å±å®‰å…¨é£é™©**ï¼šè¯¥é—®é¢˜ä¸ä¼šå¯¼è‡´æ”»å‡»è€…åœ¨ç³»ç»Ÿä¸Šæ‰§è¡Œä»»æ„ä»£ç æˆ–çªç ´å®¹å™¨éš”ç¦»ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

- **ç¬¬2æ¡**ï¼šå¦‚æœè¯¥Issueæè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œåªæ˜¯Kubernetesåœ¨ç‰¹å®šéƒ¨ç½²ç­–ç•¥ä¸‹çš„åŠŸèƒ½å®ç°é—®é¢˜ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œå› æ­¤æ— éœ€æä¾›å¤ç°è„šæœ¬ã€‚

---


## Issue #129253 The container_memory_working_set_bytes indicator corresponding to a container has three records.

- Issue é“¾æ¥ï¼š[#129253](https://github.com/kubernetes/kubernetes/issues/129253)

### Issue å†…å®¹

#### What happened?

When the container_memory_working_set_bytes indicator is queried through the Cadvisor interface, three data records exist in the same container, and the corresponding IDs are nested. Why?
```
[root@master1 log]# kubectl get --raw /api/v1/nodes/master1/proxy/metrics/cadvisor | grep container_memory_working_set_bytes | grep 'container="qk-sandbox-elf-service"'
container_memory_working_set_bytes{container="qk-sandbox-elf-service",id="/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-poddc35e999_4467_45c5_92ee_4dd4ae8baa2c.slice/cri-containerd-104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873.scope",image="registry.caas.local/default/sandbox-detect-base-image-aarch64:24.010.0829",name="104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873",namespace="nce",pod="qk-sandbox-elf-service-7b5f8d98c6-m5xjn"} 1.470353408e+09 1734442795448
container_memory_working_set_bytes{container="qk-sandbox-elf-service",id="/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-poddc35e999_4467_45c5_92ee_4dd4ae8baa2c.slice/cri-containerd-104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873.scope/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-poddc35e999_4467_45c5_92ee_4dd4ae8baa2c.slice/cri-containerd-104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873.scope",image="registry.caas.local/default/sandbox-detect-base-image-aarch64:24.010.0829",name="104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873",namespace="nce",pod="qk-sandbox-elf-service-7b5f8d98c6-m5xjn"} 1.055014912e+09 1734442803008
container_memory_working_set_bytes{container="qk-sandbox-elf-service",id="/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-poddc35e999_4467_45c5_92ee_4dd4ae8baa2c.slice/cri-containerd-104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873.scope/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-poddc35e999_4467_45c5_92ee_4dd4ae8baa2c.slice/cri-containerd-104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873.scope/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-poddc35e999_4467_45c5_92ee_4dd4ae8baa2c.slice/cri-containerd-104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873.scope",image="registry.caas.local/default/sandbox-detect-base-image-aarch64:24.010.0829",name="104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873",namespace="nce",pod="qk-sandbox-elf-service-7b5f8d98c6-m5xjn"} 1.055014912e+09 1734442797894
```

In addition, the corresponding directory is found on the node.
```
[root@master1 cri-containerd-104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873.scope]# pwd
/sys/fs/cgroup/memory/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-poddc35e999_4467_45c5_92ee_4dd4ae8baa2c.slice/cri-containerd-104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873.scope/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-poddc35e999_4467_45c5_92ee_4dd4ae8baa2c.slice/cri-containerd-104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873.scope/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-poddc35e999_4467_45c5_92ee_4dd4ae8baa2c.slice/cri-containerd-104d1007f522b9efb6ab12384082efc8ff218652b1898d9db208a6ea2712c873.scope
```

#### What did you expect to happen?

One container corresponds to one piece of data.

#### How can we reproduce it (as minimally and precisely as possible)?

N/A

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.31
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueä¸­ï¼Œç”¨æˆ·å‘ç°é€šè¿‡Cadvisoræ¥å£æŸ¥è¯¢å®¹å™¨çš„`container_memory_working_set_bytes`æŒ‡æ ‡æ—¶ï¼ŒåŒä¸€å®¹å™¨å­˜åœ¨ä¸‰æ¡æ•°æ®è®°å½•ï¼Œä¸”å¯¹åº”çš„IDæ˜¯åµŒå¥—çš„ï¼Œå¯¹æ­¤æ„Ÿåˆ°ç–‘æƒ‘å¹¶è¯¢é—®åŸå› ã€‚ä»–æœŸæœ›æ¯ä¸ªå®¹å™¨åªå¯¹åº”ä¸€æ¡æ•°æ®ã€‚

ä»é—®é¢˜æè¿°æ¥çœ‹ï¼Œè¿™æ˜¯å…³äºCadvisorç›‘æ§æ•°æ®é‡‡é›†æ–¹å¼çš„æŠ€æœ¯é—®é¢˜ï¼Œå¯èƒ½ä¸å®¹å™¨çš„cgroupå±‚çº§é…ç½®ã€Cadvisorçš„å®ç°æ–¹å¼æˆ–Kubernetesçš„èµ„æºç®¡ç†æœ‰å…³ã€‚è¿™ç§æƒ…å†µå¯èƒ½æ˜¯ç”±äºcgroupå±‚çº§çš„åµŒå¥—å¯¼è‡´çš„ï¼Œå±äºè¿ç»´æˆ–ç›‘æ§é…ç½®æ–¹é¢çš„é—®é¢˜ã€‚

è¯¥Issueæ²¡æœ‰æ¶‰åŠä»»ä½•å®‰å…¨é£é™©ï¼Œæ²¡æœ‰æåŠæ•æ„Ÿä¿¡æ¯æ³„éœ²ã€æƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰å®‰å…¨é—®é¢˜ã€‚ä¹Ÿæ²¡æœ‰è¿¹è±¡è¡¨æ˜å­˜åœ¨å¯èƒ½è¢«æ”»å‡»è€…åˆ©ç”¨çš„æ¼æ´ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤é—®é¢˜éå®‰å…¨é—®é¢˜ã€‚

---


## Issue #129229 fix: noderesources plugin flaw

- Issue é“¾æ¥ï¼š[#129229](https://github.com/kubernetes/kubernetes/issues/129229)

### Issue å†…å®¹

#### What happened?

The score function of resourceAllocationScorer should not iterate over args.ScoringStrategy.Resources. For example: the strategy parameters are set to [gpu:2,cpu:1,mem:1]. At this time, the pod only applies for cpu and mem, but because k8s traverses parameters instead of traversing pod application, gpu becomes the final The key factor in scoring, but in fact, the pod only needs to consider resource application issues of CPU and MEM, and does not need to consider GPU. After all, it has not applied for GPU. If it is the MostRequestedPriority policy at this time, after consideration, the CPU strength will be scheduled to the GPU. On the machine, there is a fragmentation problem


#### What did you expect to happen?

Which resources will be considered for scoring based on which resources the pod has applied for,Instead of scoring all the resources configured in the configuration file, which may be counterproductive in real scenarios.


#### How can we reproduce it (as minimally and precisely as possible)?

The MostRequestedPriority policy parameters are set to [gpu:2,cpu:1,mem:1]. At this time, the pod only applies for cpu and mem. It is found that the pod is scheduled to a machine with concentrated GPUs. As a result, when there are large model GPU tasks that need to be scheduled, The machine's GPU is satisfied but the CPU is not.


#### Anything else we need to know?

_No response_

#### Kubernetes version

latest version


#### Cloud provider

only k8s

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†Kubernetesè°ƒåº¦å™¨åœ¨èµ„æºåˆ†é…è¯„åˆ†ï¼ˆ`resourceAllocationScorer`ï¼‰æ—¶çš„é—®é¢˜ã€‚å½“è°ƒåº¦ç­–ç•¥å‚æ•°è®¾ç½®ä¸ºåŒ…å«GPUèµ„æºï¼ˆä¾‹å¦‚`[gpu:2,cpu:1,mem:1]`ï¼‰ï¼Œè€ŒPodåªç”³è¯·äº†CPUå’Œå†…å­˜èµ„æºæ—¶ï¼Œè°ƒåº¦å™¨ä»ä¼šéå†æ‰€æœ‰ç­–ç•¥å‚æ•°ï¼ŒåŒ…æ‹¬GPUèµ„æºã€‚è¿™å¯èƒ½å¯¼è‡´Podè¢«è°ƒåº¦åˆ°æ‹¥æœ‰GPUçš„èŠ‚ç‚¹ä¸Šï¼Œå³ä½¿Podå¹¶ä¸éœ€è¦GPUï¼Œå¯¼è‡´GPUèµ„æºè¢«å ç”¨ï¼Œé€ æˆèµ„æºç¢ç‰‡åŒ–ã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼š

- **èµ„æºæµªè´¹é—®é¢˜**ï¼šè¿™æ˜¯ä¸€ä¸ªèµ„æºè°ƒåº¦ä¼˜åŒ–é—®é¢˜ï¼Œæ¶‰åŠèµ„æºåˆ©ç”¨ç‡å’Œè°ƒåº¦æ•ˆç‡ï¼Œä½†å¹¶ä¸ç›´æ¥æ„æˆå®‰å…¨é£é™©ã€‚
- **æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰é£é™©**ï¼šè™½ç„¶èµ„æºç¢ç‰‡åŒ–å¯èƒ½é—´æ¥å¯¼è‡´GPUèµ„æºä¸è¶³ï¼Œå½±å“å…¶ä»–éœ€è¦GPUçš„Podçš„è°ƒåº¦ï¼Œä½†éœ€è¦å…·å¤‡åˆ›å»ºPodçš„æƒé™ï¼Œä¸”å½±å“æ˜¯ç”±äºè°ƒåº¦ç­–ç•¥ä¸ä½³å¼•èµ·çš„ï¼Œä¸å±äºå…¸å‹çš„DoSæ”»å‡»ã€‚
- **å¤šç§Ÿæˆ·å½±å“**ï¼šåœ¨å¤šç§Ÿæˆ·ç¯å¢ƒä¸‹ï¼Œä½æƒé™ç”¨æˆ·æäº¤çš„Podå¯èƒ½å½±å“åˆ°å…¶ä»–ç”¨æˆ·çš„èµ„æºä½¿ç”¨ï¼Œä½†è¿™åœ¨Kubernetesä¸­æ˜¯é€šè¿‡èµ„æºé…é¢å’Œè°ƒåº¦ç­–ç•¥ç®¡ç†çš„ï¼Œä¸å±äºå®‰å…¨æ¼æ´ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueåæ˜ çš„æ˜¯Kubernetesè°ƒåº¦ç­–ç•¥çš„æ”¹è¿›éœ€æ±‚ï¼Œæ—¨åœ¨ä¼˜åŒ–èµ„æºåˆ†é…ï¼Œæé«˜ç³»ç»Ÿæ€§èƒ½ï¼Œä¸æ¶‰åŠå®‰å…¨æ¼æ´ã€‚

---


## Issue #129228 HPA scales up despite utilization being under target

- Issue é“¾æ¥ï¼š[#129228](https://github.com/kubernetes/kubernetes/issues/129228)

### Issue å†…å®¹

#### What happened?

We use Argo Rollouts to perform canary deployments of our services. During a canary deployment, new pods are brought up (the canary pods) which are included in the [Status](https://github.com/kubernetes/kubernetes/blob/5ba2b78eae18645744b51d94d279582bdcccec23/pkg/apis/autoscaling/types.go#L51) of the Rollout's scale subresource. When HPA is configured to scale on a metric with a high utilization ratio (generally memory), this results in HPA scaling out, despite the fact that the utilization is under the target. 

This seems to be a result of the behaviour of the [replica calculator](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/podautoscaler/replica_calculator.go#L65) where:
- the recommended replicas is a simple function of utilizationRatio and the total number of pods returned by the selector
- missing pods with a utilization ratio of below 1 are assumed to be because of a "scale down" and are treated as if they are [consuming 100% of resources](https://github.com/kubernetes/kubernetes/blob/5ba2b78eae18645744b51d94d279582bdcccec23/pkg/controller/podautoscaler/replica_calculator.go#L108-L113) (whereas in this case we are scaling out during a rollout)

In addition, the utilization ratio is never checked prior to updating prior to HPA increasing the desired replicas leading to [incorrect/misleading log messages](https://github.com/kubernetes/kubernetes/blob/5ba2b78eae18645744b51d94d279582bdcccec23/pkg/controller/podautoscaler/horizontal.go#L857-L859) like "New size: X; reason: memory resource utilization (percentage of request) above target". 

Further details can be found in issues logged for Argo Rollouts [here](https://github.com/argoproj/argo-rollouts/issues/2857) and [here](https://github.com/argoproj/argo-rollouts/issues/3849).

#### What did you expect to happen?

HPA should not scale up when utilization is below target (I.e. when `utilizationRatio < 1`)

#### How can we reproduce it (as minimally and precisely as possible)?

The issue can be reproduced by simulating the behaviour of Argo Rollouts and creating a 2nd `ReplicaSet` with selectors that match an existing `Deployment`. For example:

Create base `Deployment` and `HorizontalPodAutoscaler` resources:  
``` yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  namespace: test
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - image: nginx
          name: nginx
          resources:
            requests:
              memory: "10Mi"
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx
  namespace: test
spec:
  maxReplicas: 50
  minReplicas: 3
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx
  metrics:
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 50
```

Wait to ensure that metrics are available and HPA is stable at 3 pods.
``` bash
$ kubectl get pods,hpa
NAME                         READY   STATUS    RESTARTS   AGE
pod/nginx-7fb4f6d65f-jgdzm   1/1     Running   0          30s
pod/nginx-7fb4f6d65f-mtnxv   1/1     Running   0          30s
pod/nginx-7fb4f6d65f-tc6c8   1/1     Running   0          30s

NAME                                        REFERENCE          TARGETS           MINPODS   MAXPODS   REPLICAS   AGE
horizontalpodautoscaler.autoscaling/nginx   Deployment/nginx   memory: 42%/50%   3         50        3          30s
```

Create a 2nd replica set: 
``` yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-canary
  namespace: test
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
      role: canary
  template:
    metadata:
      labels:
        app: nginx
        role: canary
    spec:
      containers:
        - image: nginx
          name: nginx
          resources:
            requests:
              memory: "10Mi"
```

Observe that HPA will scale the existing `Deployment`:
``` bash
$ kubectl get pods,hpa                                                                                     
NAME                         READY   STATUS    RESTARTS   AGE
pod/nginx-7fb4f6d65f-2hh7f   1/1     Running   0          2m36s
pod/nginx-7fb4f6d65f-6lfr5   1/1     Running   0          2m6s
pod/nginx-7fb4f6d65f-9wgfn   1/1     Running   0          6s
pod/nginx-7fb4f6d65f-gpzp9   1/1     Running   0          2m6s
pod/nginx-7fb4f6d65f-hdsff   1/1     Running   0          3m6s
pod/nginx-7fb4f6d65f-j24kp   1/1     Running   0          3m6s
pod/nginx-7fb4f6d65f-jgdzm   1/1     Running   0          4m36s
pod/nginx-7fb4f6d65f-kxbvz   1/1     Running   0          66s
pod/nginx-7fb4f6d65f-l4rzr   1/1     Running   0          2m36s
pod/nginx-7fb4f6d65f-mtnxv   1/1     Running   0          4m36s
pod/nginx-7fb4f6d65f-qvpqx   1/1     Running   0          3m6s
pod/nginx-7fb4f6d65f-rq2r8   1/1     Running   0          96s
pod/nginx-7fb4f6d65f-s52hj   1/1     Running   0          66s
pod/nginx-7fb4f6d65f-tc6c8   1/1     Running   0          4m36s
pod/nginx-7fb4f6d65f-zwpfc   1/1     Running   0          36s
pod/nginx-canary-2prb6       1/1     Running   0          3m41s
pod/nginx-canary-9l5rc       1/1     Running   0          3m41s
pod/nginx-canary-g79tk       1/1     Running   0          3m41s

NAME                                        REFERENCE          TARGETS           MINPODS   MAXPODS   REPLICAS   AGE
horizontalpodautoscaler.autoscaling/nginx   Deployment/nginx   memory: 42%/50%   3         50        14         4m36s
```

Check events on HPA to see details on why scaling occurred:
```
$ kubectl describe hpa nginx
Name:                                                     nginx
Namespace:                                                test
Labels:                                                   <none>
Annotations:                                              <none>
CreationTimestamp:                                        Mon, 16 Dec 2024 10:20:04 +0100
Reference:                                                Deployment/nginx
Metrics:                                                  ( current / target )
  resource memory on pods  (as a percentage of request):  42% (4503324444m) / 50%
Min replicas:                                             3
Max replicas:                                             50
Deployment pods:                                          8 current / 8 desired
Conditions:
  Type            Status  Reason              Message
  ----            ------  ------              -------
  AbleToScale     True    ReadyForNewScale    recommended size matches current size
  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from memory resource utilization (percentage of request)
  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range
Events:
  Type     Reason                   Age                    From                       Message
  ----     ------                   ----                   ----                       -------
  Warning  FailedGetResourceMetric  4m35s (x3 over 4m46s)  horizontal-pod-autoscaler  unable to get metric memory: no metrics returned from resource metrics API
  Normal   SuccessfulRescale        3m31s                  horizontal-pod-autoscaler  New size: 6; reason: memory resource utilization (percentage of request) above target
  Normal   SuccessfulRescale        3m1s                   horizontal-pod-autoscaler  New size: 8; reason: memory resource utilization (percentage of request) above target
  Normal   SuccessfulRescale        2m31s                  horizontal-pod-autoscaler  New size: 10; reason: memory resource utilization (percentage of request) above target
  Normal   SuccessfulRescale        2m1s                   horizontal-pod-autoscaler  New size: 11; reason: memory resource utilization (percentage of request) above target
  Normal   SuccessfulRescale        91s                    horizontal-pod-autoscaler  New size: 13; reason: memory resource utilization (percentage of request) above target
  Normal   SuccessfulRescale        61s                    horizontal-pod-autoscaler  New size: 14; reason: memory resource utilization (percentage of request) above target
  Normal   SuccessfulRescale        31s                    horizontal-pod-autoscaler  New size: 15; reason: memory resource utilization (percentage of request) above target
  Normal   SuccessfulRescale        1s                     horizontal-pod-autoscaler  New size: 16; reason: memory resource utilization (percentage of request) above target
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.3
Kustomize Version: v5.4.2
Server Version: v1.30.5-gke.1443001
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueå†…å®¹ï¼Œé—®é¢˜æè¿°äº†åœ¨ä½¿ç”¨Argo Rolloutsè¿›è¡Œé‡‘ä¸é›€éƒ¨ç½²æ—¶ï¼ŒHPAï¼ˆHorizontal Pod Autoscalerï¼‰åœ¨èµ„æºåˆ©ç”¨ç‡ä½äºç›®æ ‡å€¼çš„æƒ…å†µä¸‹ï¼Œä»ç„¶ä¼šæ‰©å®¹ã€‚åŸå› åœ¨äºHPAçš„å‰¯æœ¬è®¡ç®—å™¨åœ¨å¤„ç†ç¼ºå¤±çš„podsæ—¶ï¼Œå°†å…¶è§†ä¸ºæ­£åœ¨ç¼©å®¹ï¼Œå¹¶å‡è®¾è¿™äº›podsæ­£åœ¨æ¶ˆè€—100%çš„èµ„æºï¼Œä»è€Œå¯¼è‡´æ‰©å®¹å†³ç­–ã€‚

ä»å®‰å…¨é£é™©çš„è§’åº¦åˆ†æï¼š

1. **é—®é¢˜æ€§è´¨**ï¼šè¯¥é—®é¢˜å±äºHPAåœ¨ç‰¹å®šæƒ…å†µä¸‹çš„è¡Œä¸ºå¼‚å¸¸ï¼Œå¯¼è‡´èµ„æºçš„ä¸å¿…è¦æ¶ˆè€—ã€‚è¿™æ›´å€¾å‘äºåŠŸèƒ½æ€§é”™è¯¯æˆ–æ€§èƒ½é—®é¢˜ã€‚

2. **æ”»å‡»å¯èƒ½æ€§**ï¼šè¦åˆ©ç”¨æ­¤é—®é¢˜ï¼Œæ”»å‡»è€…éœ€è¦å…·å¤‡åœ¨é›†ç¾¤ä¸­åˆ›å»ºReplicaSetæˆ–ä¿®æ”¹éƒ¨ç½²çš„æƒé™ã€‚è¿™å±äºè¾ƒé«˜çš„æƒé™èŒƒç•´ã€‚

3. **é£é™©è¯„ä¼°æ ‡å‡†**ï¼š

   - æ ¹æ®ç¬¬2æ¡ï¼Œå¦‚æœIssueæè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚
   - æ ¹æ®ç¬¬5æ¡ï¼Œåœ¨DoSæ”»å‡»é£é™©ä¸­ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚

ç»¼ä¸Šï¼Œè¯¥Issueæè¿°çš„é—®é¢˜ä¸å±äºå®‰å…¨é£é™©ï¼Œè€Œæ˜¯éœ€è¦ä¿®å¤çš„åŠŸèƒ½æ€§é—®é¢˜ã€‚

---


## Issue #129220 kube-apiserver memory leak

- Issue é“¾æ¥ï¼š[#129220](https://github.com/kubernetes/kubernetes/issues/129220)

### Issue å†…å®¹

#### What happened?

I have upgraded to kubernetes 1.31.1 using Kubespray, after the update i can observe several issues, if we observe the graph for mem usage its like 45 degrees, from the start rising up untill it crashes, not much can be seen from the logs except some GRCP errors, i doubt this to cause the crashes, but this is everything i have

#### What did you expect to happen?

Api not leak memory and crashes

#### How can we reproduce it (as minimally and precisely as possible)?

Not sure

#### Anything else we need to know?

W1214 15:25:17.351529       1 logging.go:55] [core] [Channel #1029556 SubChannel #1029558]grpc: addrConn.createTransport failed to connect to {Addr: "REDACTED:REDACTED", ServerName: "REDACTED:REDACTED", }. Err: connection error: desc = "error reading server preface: read tcp REDACTED:REDACTED->REDACTED:REDACTED: use of closed network connection"


Those erros are filling the logs, nothing else can be observed

#### Kubernetes version

<details>

```console
$ kubectl version 
# paste output here
```
1.31.1

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# Ubuntu 24.04
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
Kubespray
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
Containerd
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
Weave
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®æä¾›çš„Issueå†…å®¹ï¼Œç”¨æˆ·åœ¨ä½¿ç”¨Kubesprayå‡çº§åˆ°Kubernetes 1.31.1åï¼Œå‘ç°`kube-apiserver`å­˜åœ¨å†…å­˜æ³„æ¼å¹¶æœ€ç»ˆå¯¼è‡´å´©æºƒã€‚æ—¥å¿—ä¸­ä¸»è¦æ˜¾ç¤ºäº†ä¸€äº›GRPCé”™è¯¯ï¼Œå…·ä½“ä¸ºï¼š

```
W1214 15:25:17.351529       1 logging.go:55] [core] [Channel #1029556 SubChannel #1029558]grpc: addrConn.createTransport failed to connect to {Addr: "REDACTED:REDACTED", ServerName: "REDACTED:REDACTED", }. Err: connection error: desc = "error reading server preface: read tcp REDACTED:REDACTED->REDACTED:REDACTED: use of closed network connection"
```

ä»æè¿°æ¥çœ‹ï¼Œè¿™æ˜¯ä¸€æ¬¡æ­£å¸¸çš„å‡çº§æ“ä½œåå‡ºç°çš„æœåŠ¡å¼‚å¸¸ï¼Œå¯¼è‡´`kube-apiserver`å†…å­˜é€æ¸å¢å¤§å¹¶æœ€ç»ˆå´©æºƒã€‚è¿™æ›´å¯èƒ½æ˜¯ç”±äºè½¯ä»¶Bugã€é…ç½®ä¸å½“æˆ–è€…å…¼å®¹æ€§é—®é¢˜å¼•èµ·çš„æ€§èƒ½æˆ–ç¨³å®šæ€§é—®é¢˜ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠï¼š** å¦‚æœè¯¥Issueæè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚
2. **æœªæ¶‰åŠå®‰å…¨æ¼æ´æˆ–æ•æ„Ÿä¿¡æ¯ï¼š** Issueä¸­æ²¡æœ‰æåŠä»»ä½•å¯è¢«åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€æ”»å‡»æ‰‹æ®µæˆ–æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€‚
3. **ç¼ºä¹å¤ç°æ­¥éª¤ï¼š** Issueä¸­ä¹Ÿæ²¡æœ‰æä¾›ä»»ä½•å¯ç”¨äºæ¶æ„åˆ©ç”¨çš„é—®é¢˜ç»†èŠ‚æˆ–å¤ç°æ­¥éª¤ã€‚

---


## Issue #129197 watcher always terminal

- Issue é“¾æ¥ï¼š[#129197](https://github.com/kubernetes/kubernetes/issues/129197)

### Issue å†…å®¹

#### What happened?

when is use 
https://xxxxxx/api/v1/pods?timeoutSeconds=10000&watch=true

watcher will terminal

we have 100000 pods

i see #13969

this param will return all pods event

(c *cacheWatcher) processInterval  will exec process func when initEvents send to result success,but is took 5s-6s.
if process func not be exec,watcher  input chan will not have comsumer.

then watcher will be blockedWatcher,when etcd have changed
the func watcher.add(event, timer) will timeout and kill my watcher.

#### What did you expect to happen?

watcher keep to timeoutSecond

#### How can we reproduce it (as minimally and precisely as possible)?

https://xxxxxx/api/v1/pods?timeoutSeconds=10000&watch=true
100000 pods

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>
1.30

#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetesé›†ç¾¤ä¸­ï¼Œå½“å­˜åœ¨100,000ä¸ªPodæ—¶ï¼Œä½¿ç”¨Watcherï¼ˆé€šè¿‡è®¿é—®`https://xxxxxx/api/v1/pods?timeoutSeconds=10000&watch=true`ï¼‰è¿›è¡Œç›‘å¬æ—¶ï¼ŒWatcherä¼šæå‰ç»ˆæ­¢ã€‚Issueä¸­æåˆ°ï¼š

- åˆå§‹åŒ–äº‹ä»¶å‘é€åˆ°ç»“æœé€šé“æ—¶è€—æ—¶è¾ƒé•¿ï¼ˆ5-6ç§’ï¼‰ï¼Œå¯¼è‡´Watcherçš„è¾“å…¥é€šé“æ²¡æœ‰æ¶ˆè´¹è€…ï¼Œå‡ºç°é˜»å¡ã€‚
- å½“etcdæœ‰å˜åŒ–æ—¶ï¼Œ`watcher.add(event, timer)`å‡½æ•°ä¼šè¶…æ—¶ï¼Œè¿›è€Œç»ˆæ­¢Watcherã€‚

ä»ä¸Šè¿°æè¿°æ¥çœ‹ï¼Œè¿™ä¸ªé—®é¢˜ä¸»è¦æ˜¯ç”±äºåœ¨å¤§è§„æ¨¡Podæ•°é‡ä¸‹ï¼ŒWatcheråœ¨å¤„ç†åˆå§‹äº‹ä»¶æ—¶å­˜åœ¨æ€§èƒ½é—®é¢˜ï¼Œå¯¼è‡´Watcheræ— æ³•æŒç»­å·¥ä½œåˆ°`timeoutSeconds`æŒ‡å®šçš„æ—¶é—´ã€‚

åœ¨å®‰å…¨é£é™©è¯„ä¼°ä¸­ï¼Œè¯¥é—®é¢˜å±äºåŠŸèƒ½æ€§æˆ–æ€§èƒ½é—®é¢˜ï¼Œå¹¶æœªæ¶‰åŠåˆ°ä»»ä½•æ½œåœ¨çš„å®‰å…¨æ¼æ´ã€‚ä¾‹å¦‚ï¼š

- **æœªæ¶‰åŠæœªæˆæƒè®¿é—®**ï¼šæ²¡æœ‰æåˆ°ä»»ä½•æœªç»æˆæƒè®¿é—®æˆ–æ•°æ®æ³„éœ²çš„æƒ…å†µã€‚
- **æœªæ¶‰åŠæƒé™æå‡**ï¼šæ²¡æœ‰æåˆ°ä½æƒé™ç”¨æˆ·æå‡æƒé™çš„æƒ…å†µã€‚
- **æœªæ¶‰åŠä»£ç æ‰§è¡Œæˆ–å®¹å™¨é€ƒé€¸**ï¼šæ²¡æœ‰ä»»ä½•å…³äºå‘½ä»¤æ‰§è¡Œã€ä»£ç æ³¨å…¥æˆ–å®¹å™¨é€ƒé€¸çš„æè¿°ã€‚
- **æœªæ¶‰åŠæ‹’ç»æœåŠ¡æ”»å‡»ï¼ˆDoSï¼‰**ï¼šè™½ç„¶Watcherç»ˆæ­¢ï¼Œä½†è¿™æ˜¯ç”±äºè‡ªèº«å¤„ç†æ€§èƒ½é—®é¢˜å¼•èµ·çš„ï¼Œå¹¶éå¤–éƒ¨æ”»å‡»å¯¼è‡´çš„èµ„æºè€—å°½æˆ–æœåŠ¡ä¸å¯ç”¨ã€‚

---


## Issue #129196 'requiredDuringSchedulingIgnoredDuringExecution' evicts Daemonset when node label removed

- Issue é“¾æ¥ï¼š[#129196](https://github.com/kubernetes/kubernetes/issues/129196)

### Issue å†…å®¹

#### What happened?

The node affinity requires that the Daemonset be deployed on a node with a specified label. Affinity type is requiredDuringSchedulingIgnoredDuringExecution.
When Daemonset has been running for a while, remove the label.
Daemonset will be destroyed.

However, according to [the official doc,](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity-per-scheduling-profile) the Daemonset type is IgnoredDuringExecution, and the pod should not be destroyed after the tag is removed.
```
Note: 
In the preceding types, IgnoredDuringExecution means that if the node labels change after Kubernetes schedules the Pod, the Pod continues to run.
```

#### What did you expect to happen?

After the label is removed, Daemonset should continue to run (consistent with the IgnoredDuringExecution described in the doc).

#### How can we reproduce it (as minimally and precisely as possible)?

apply the following yaml and modify the image path. [full yaml link](https://gitee.com/OpenCloudOS/xuanwan/blob/master/os-update-operator/configs/deploy/operator/up-proxy.yaml)
```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: up-proxy
  namespace: kube-system
  labels:
    control-plane: up-proxy
spec:
  selector:
    matchLabels:
      control-plane: up-proxy
  template:
    metadata:
      labels:
        control-plane: up-proxy
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: kubernetes.io/os
                    operator: In
                    values:
                      - linux
                  - key: xuanwan.opencloudos.org/updater-interface-version
                    operator: In
                    values:
                      - 1.0.0
                  - key: kubernetes.io/arch
                    operator: In
                    values:
                      - amd64
                      - arm64
      containers:
      - name: up-proxy
        image: <up-proxy-container-path>
        imagePullPolicy: IfNotPresent
```
On the node label xuanwan.opencloudos.org/updater-interface-version= "1.0.0" and wait for Daemonset deployed and run.
Remove the node label and observe whether Daemonset continues to run

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
v1.27.4
```

</details>


#### Cloud provider

<details>
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="OpenCloudOS"
VERSION="9.2"
ID="opencloudos"
ID_LIKE="opencloudos"
VERSION_ID="9.2"
PLATFORM_ID="platform:oc9"
PRETTY_NAME="OpenCloudOS 9.2"
$ uname -a
Linux kube-master 6.6.47-12.oc9.x86_64 #1 SMP PREEMPT_DYNAMIC Tue Sep 24 16:15:42 CST 2024 x86_64 GNU/Linux



```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†å½“ä½¿ç”¨ `requiredDuringSchedulingIgnoredDuringExecution` ç±»å‹çš„ `nodeAffinity` æ—¶ï¼ŒDaemonSet åœ¨èŠ‚ç‚¹æ ‡ç­¾è¢«ç§»é™¤åï¼ŒPod è¢«é©±é€çš„ç°è±¡ã€‚æ ¹æ® Kubernetes å®˜æ–¹æ–‡æ¡£ï¼Œ`IgnoredDuringExecution` è¡¨ç¤ºå³ä½¿åœ¨ Pod è°ƒåº¦åèŠ‚ç‚¹æ ‡ç­¾å‘ç”Ÿå˜åŒ–ï¼ŒPod ä¹Ÿåº”ç»§ç»­è¿è¡Œã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼š

1. **å¯èƒ½çš„å®‰å…¨å½±å“**ï¼šå¦‚æœæ”»å‡»è€…èƒ½å¤Ÿç§»é™¤èŠ‚ç‚¹ä¸Šçš„ç‰¹å®šæ ‡ç­¾ï¼Œå¯¼è‡´å…³é”®çš„ DaemonSet Pod è¢«é©±é€ï¼Œå¯èƒ½ä¼šé€ æˆæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ï¼Œå½±å“é›†ç¾¤ä¸­æœåŠ¡çš„å¯ç”¨æ€§ã€‚

2. **å®æ–½æ¡ä»¶**ï¼šè¦ç§»é™¤èŠ‚ç‚¹æ ‡ç­¾ï¼Œæ”»å‡»è€…éœ€è¦å…·æœ‰ä¿®æ”¹èŠ‚ç‚¹å¯¹è±¡çš„æƒé™ã€‚è¿™é€šå¸¸éœ€è¦è¾ƒé«˜çš„æƒé™çº§åˆ«ï¼ˆå¦‚é›†ç¾¤ç®¡ç†å‘˜æˆ–å…·æœ‰èŠ‚ç‚¹ç®¡ç†æƒé™çš„ç”¨æˆ·ï¼‰ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

- **æ ‡å‡†5**ï¼šåœ¨æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»çš„é£é™©ç±»å‹ä¸­ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™ï¼ˆå¦‚åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™ï¼‰æ‰èƒ½å®æ–½æ”»å‡»ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSS è¯„çº§åœ¨ high ä»¥ä¸‹ã€‚

å› æ­¤ï¼Œå°½ç®¡è¯¥é—®é¢˜æ¶‰åŠå®‰å…¨é£é™©ï¼Œä½†ç”±äºéœ€è¦é«˜æƒé™æ‰èƒ½å®æ–½æ”»å‡»ï¼Œé£é™©è¯„çº§åº”ä¸º**ä½é£é™©**ã€‚

---


## Issue #129193 "Error syncing pod, skipping" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: connection refused\"" pod="" podUID=""

- Issue é“¾æ¥ï¼š[#129193](https://github.com/kubernetes/kubernetes/issues/129193)

### Issue å†…å®¹

#### What happened?

Pod has been terminating, error: "Error syncing pod, skipping" err="rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial unix /run/containerd/containerd.sock: connect: connection refused\"" pod="" podUID=""ï¼Œ Actually, containerd is normal

#### What did you expect to happen?

Pod pulled up normally

#### How can we reproduce it (as minimally and precisely as possible)?

Dec  5 23: Upgrade caas from 53 onwards
Dec  5 23: 56 Node Restart
Dec 5 23:56:52 containerd starts after power on
Since the start of the kubelet process on Dec 6 00:00:23, it has not been restarted yet. It should be at this point in time that the kubelet was upgraded
Dec 6 00:00:06 Crittl Check the start and run time of the container process for nbidataserver (cinit process inside the container)
Dec 6 00:01:16 ps Check the startup time of the business process. The probe may not be ready for a short period of time before starting, and the kubelet may only sync after reaching certain conditions 
Dec 6 00:01:37 nbidataservice updataPodContainerResources first reported failure containerdsock connection rejected and kept reporting errors afterwards
Dec 6 00:01:38 containerd upgraded and started up until now



Latest progress: This issue is caused by enabling the InPlacePodVerticalScaling feature
Event replay:
Event 1:2024-12-24 22:41:33 Machine Power On/Off
Event 2:2024-12-24 22:47:53 kubelet starts to start, Pod starts to be pulled up
Event 3:2024-12-24 22:49:03 containerd starts
Event 4:2024-12-24 22:49:04 kubelet started pulling up smauthservice, but received an error message "server is not initialized yet" when accessing containerd. After containerd started, smauthservice started normally.
Event 5:2024-12-24 22:54:12: The upgrade of smauthservice started, and the Kubernetes control plane set the pod smauthservice to Terminating state, which has been consistent until now.
Cause analysis:
1. The kubelet caches real pod information (also known as pod cache), which is synchronized from containerd by PLEG (PodLifecycle Event Generator) coroutine timing. PLEG itself also caches the real pod state (known as PLEG cache). If PLEG detects a difference between the real container state and its own cache, it will synchronize the real pod information to the pod cache.
2. After enabling the InPlacePodVerticalScaling feature, during the pod startup phase, if the pod is expanding, kubelet will query the real pod status from containerd and update the pod cache directly without PLEG. At this time, because containerd cannot access it, unknown status will be saved to the pod cache (Event 4). However, PLEG will not update the pod cache again because the actual pod status is no different from its own cache.
3. Later on, when upgrading the pod, the coroutine for handling pod deletion needed to retrieve the actual pod state from the pod cache for tuning. However, an unknown state was obtained, so the deletion of the pod was stopped and the pod remained in Terminating (Event 5).


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.28.1
</details>


#### Cloud provider

<details>
na
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æ ‡é¢˜æè¿°äº†åœ¨åŒæ­¥ Pod æ—¶å‡ºç°é”™è¯¯ï¼Œæ— æ³•è¿æ¥ `/run/containerd/containerd.sock`ï¼Œæç¤º "connection refused"ã€‚Issue å†…å®¹è¿›ä¸€æ­¥åˆ†æäº†é—®é¢˜å‘ç”Ÿçš„åŸå› ï¼š

1. å¯ç”¨äº† `InPlacePodVerticalScaling` ç‰¹æ€§åï¼Œkubelet åœ¨ Pod å¯åŠ¨é˜¶æ®µï¼Œå¦‚æœ Pod å¤„äºæ‰©å®¹çŠ¶æ€ï¼Œä¼šç›´æ¥ä» containerd æŸ¥è¯¢å®é™…çš„ Pod çŠ¶æ€å¹¶æ›´æ–°ç¼“å­˜ï¼Œè€Œä¸æ˜¯é€šè¿‡ PLEGï¼ˆPod Lifecycle Event Generatorï¼‰ã€‚
2. å¦‚æœæ­¤æ—¶ containerd å°šæœªå¯åŠ¨ï¼Œkubelet æ— æ³•è®¿é—® containerdï¼Œå¯¼è‡´ä¿å­˜äº†æœªçŸ¥çš„ Pod çŠ¶æ€åˆ°ç¼“å­˜ä¸­ã€‚
3. å› ä¸º PLEG ä¸ä¼šæ›´æ–°ç¼“å­˜ï¼ˆå› ä¸ºå®ƒè®¤ä¸ºçŠ¶æ€æ²¡æœ‰å˜åŒ–ï¼‰ï¼Œåœ¨åç»­çš„ Pod åˆ é™¤è¿‡ç¨‹ä¸­ï¼Œkubelet è·å–åˆ°æœªçŸ¥çš„ Pod çŠ¶æ€ï¼Œæ— æ³•æ­£å¸¸åˆ é™¤ Podï¼Œå¯¼è‡´ Pod ä¸€ç›´å¤„äº Terminating çŠ¶æ€ã€‚

ä»ä»¥ä¸Šåˆ†æå¯ä»¥çœ‹å‡ºï¼Œè¿™ä¸ªé—®é¢˜å¯¼è‡´äº† Pod æ— æ³•æ­£å¸¸åˆ é™¤ï¼ŒæœåŠ¡é™·å…¥ä¸å¯ç”¨çŠ¶æ€ï¼Œå±äºæ½œåœ¨çš„æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰é£é™©ã€‚

ç„¶è€Œï¼Œè¦è§¦å‘è¿™ä¸ªé—®é¢˜ï¼Œéœ€è¦æ»¡è¶³ä»¥ä¸‹æ¡ä»¶ï¼š

- å¯ç”¨äº† `InPlacePodVerticalScaling` ç‰¹æ€§ã€‚
- kubelet å’Œ containerd çš„å¯åŠ¨é¡ºåºç‰¹å®šï¼ˆcontainerd å¯åŠ¨è¾ƒæ…¢ï¼Œkubelet å…ˆå°è¯•è®¿é—®ï¼‰ã€‚
- æ”»å‡»è€…éœ€è¦èƒ½å¤Ÿå½±å“èŠ‚ç‚¹ä¸Š kubelet å’Œ containerd çš„å¯åŠ¨é¡ºåºæˆ–æœåŠ¡çŠ¶æ€ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

- ç¬¬5æ¡æŒ‡å‡ºï¼Œå½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSS è¯„çº§åœ¨ high ä»¥ä¸‹ã€‚
- æ­¤é—®é¢˜éœ€è¦æ”»å‡»è€…å…·æœ‰å¯¹èŠ‚ç‚¹çš„ç®¡ç†æƒé™ï¼Œæ‰èƒ½å½±å“ kubelet å’Œ containerd çš„å¯åŠ¨å’Œé…ç½®ã€‚

å› æ­¤ï¼Œè¯¥é—®é¢˜è™½å±äºå®‰å…¨é—®é¢˜ï¼Œä½†é£é™©è¯„çº§åº”ä¸ºä½é£é™©ã€‚

---


## Issue #129187 Failed to get rootfs info if the host machine uses bcachefs raid array

- Issue é“¾æ¥ï¼š[#129187](https://github.com/kubernetes/kubernetes/issues/129187)

### Issue å†…å®¹

#### What happened?

If you create a host machine with bcachefs file system with multiple drives:

```bash
â¯ bcachefs format /dev/nvme0n1p2 /dev/nvme1n1p2 --replicas=2
â¯ mount -t bcachefs /dev/nvme0n1p2:/dev/nvme1n1p2 /mnt
```

The filesystem device is in every linux tool defined as `/dev/nvme0n1p2:/dev/nvme1n1p2`.

```bash
â¯ mount
/dev/nvme0n1p2:/dev/nvme1n1p2 on / type bcachefs (rw,relatime,metadata_replicas=2,data_replicas=2,compression=lz4,background_compression=lz4,nopromote_whole_extents,fix_errors=yes,nojournal_transaction_names)

â¯ df /
Filesystem                     1K-blocks       Used  Available Use% Mounted on
/dev/nvme0n1p2:/dev/nvme1n1p2 3593522709 1260627309 2297004701  36% /
```

As you might know, docker displays this file system label if you add any unnamed volumes to your container. And this brings us to the k8s issue. If you create a cluster e.g. with kind or minikube, it starts a kubelet service in the container. Kubelet tries to read all mounts, and fails to parse the bcachefs filesystem:

```
kubelet.go:1566] "Failed to start ContainerManager" err="failed to get rootfs info: failed to get mount point for device \"/dev/nvme0n1p2:/dev/nvme1n1p2\": no partition info for device \"/dev/nvme0n1p2:/dev/nvme1n1p2\""
```

What you _can_ do is to just take the first partition from the list and use that. But here we have kubelet just crashing, rendering systems such as kind or minikube not working.

#### What did you expect to happen?

Kubelet should just not care the filesystem name format. The system should start even when using bcachefs.

#### How can we reproduce it (as minimally and precisely as possible)?

Spin up a virtual machine with two disks. It's probably easiest to run on NixOS:

https://nixos.wiki/wiki/Bcachefs

Create the root volume with

```bash
â¯ bcachefs format /dev/nvme0n1p2 /dev/nvme1n1p2 --replicas=2
â¯ mount -t bcachefs /dev/nvme0n1p2:/dev/nvme1n1p2 /mnt
```

Continue the installation as instructed.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
```

</details>


#### Cloud provider

<details>
Local machine.
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"
$ uname -a
# paste output here
Linux docker-container 6.12.3-cachyos #1-NixOS SMP PREEMPT_DYNAMIC Fri Dec  6 06:20:46 UTC 2024 x86_64 GNU/Linux
```

</details>


#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æŠ¥å‘Šäº†åœ¨ä¸»æœºä½¿ç”¨å¤šä¸ªé©±åŠ¨å™¨å¹¶é‡‡ç”¨ bcachefs æ–‡ä»¶ç³»ç»Ÿæ—¶ï¼Œ`kubelet` æœåŠ¡æ— æ³•å¯åŠ¨çš„é—®é¢˜ã€‚å…·ä½“è¡¨ç°ä¸º `kubelet` åœ¨å°è¯•è·å– rootfs ä¿¡æ¯æ—¶ï¼Œæ— æ³•è§£æè®¾å¤‡åç§° `/dev/nvme0n1p2:/dev/nvme1n1p2`ï¼Œå¯¼è‡´æŠ¥é”™å¹¶åœæ­¢è¿è¡Œã€‚è¿™ä½¿å¾—åŸºäº Kubernetes çš„ç³»ç»Ÿï¼ˆå¦‚ kind æˆ– minikubeï¼‰æ— æ³•æ­£å¸¸å·¥ä½œã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œéœ€è¦è¯„ä¼°è¯¥é—®é¢˜æ˜¯å¦å­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ã€‚

é¦–å…ˆï¼Œè¯¥é—®é¢˜å¯¼è‡´çš„æ˜¯ `kubelet` æœåŠ¡æ— æ³•å¯åŠ¨ï¼Œå½±å“äº†ç³»ç»Ÿçš„æ­£å¸¸è¿è¡Œï¼Œè¿™æ˜¯ä¸€ä¸ªå¯ç”¨æ€§æ–¹é¢çš„é—®é¢˜ï¼Œå±äºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰çš„èŒƒç•´ã€‚

æ ¹æ®æ ‡å‡†ç¬¬5æ¡ï¼š

> åœ¨é£é™©ç±»å‹ä¸ºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»æ—¶ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å¤Ÿå®æ–½è¯¥æ”»å‡»ï¼Œåˆ™è§†æƒ…å†µéœ€è¦é™çº§å¤„ç†ï¼Œå½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ï¼ŒCVSSè¯„çº§åœ¨highä»¥ä¸‹ã€‚

åœ¨æ­¤æƒ…å¢ƒä¸­ï¼Œåªæœ‰å…·æœ‰ä¸»æœºç®¡ç†å‘˜æƒé™çš„ç”¨æˆ·æ‰èƒ½é…ç½®æ–‡ä»¶ç³»ç»Ÿä¸º bcachefs å¹¶ä½¿ç”¨å¤šä¸ªé©±åŠ¨å™¨ã€‚è¿™æ„å‘³ç€æ™®é€šç”¨æˆ·æˆ–å¤–éƒ¨æ”»å‡»è€…æ— æ³•é€šè¿‡æ­¤æ–¹å¼æ¥å¯¼è‡´ `kubelet` æ— æ³•å¯åŠ¨ã€‚

å› æ­¤ï¼Œè¯¥é—®é¢˜å¹¶éå®‰å…¨æ¼æ´ï¼Œè€Œæ˜¯ä¸€ä¸ªç”±äºç‰¹æ®Šé…ç½®å¯¼è‡´çš„å…¼å®¹æ€§é—®é¢˜ï¼Œä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---


## Issue #129177 The actual pod call to the readNamespacedPod returned a 404 NotFound error.

- Issue é“¾æ¥ï¼š[#129177](https://github.com/kubernetes/kubernetes/issues/129177)

### Issue å†…å®¹

#### What happened?

This is the response from the call:
header:{audit-id=[22e1f76c-4364-41d6-a826-16f26b0df14b], cache-control=[no-cache, private], content-length=[228], content-type=[application/json], date=[Thu, 12 Dec 2024 12:53:25 GMT], x-kubernetes-pf-flowschema-uid=[38efc94f-5c70-40e9-8e97-167a23b9942a], x-kubernetes-pf-prioritylevel-uid=[09099773-afa1-4ec2-b873-29bc7bc42106]}, body:{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"pods \"iac-pod-xxx\" not found","reason":"NotFound","details":{"name":"iac-pod-xxx","kind":"pods"},"code":404}

#### What did you expect to happen?

the pod is existedï¼Œbut it returned a 404 error

#### How can we reproduce it (as minimally and precisely as possible)?

This is an intermittent issue.



#### Anything else we need to know?

_No response_

#### Kubernetes version

        <dependency>
            <groupId>io.kubernetes</groupId>
            <artifactId>client-java</artifactId>
            <version>16.0.0</version>
        </dependency>

#### Cloud provider

alibaba cloud

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®Issueçš„æè¿°ï¼Œç”¨æˆ·åœ¨è°ƒç”¨Kubernetesçš„`readNamespacedPod` APIæ—¶ï¼Œå°½ç®¡Podå®é™…å­˜åœ¨ï¼Œä½†æœ‰æ—¶ä¼šè¿”å›404 NotFoundé”™è¯¯ã€‚è¯¥é—®é¢˜æ˜¯é—´æ­‡æ€§å‘ç”Ÿçš„ã€‚

ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¿™æ›´åƒæ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§æˆ–ç¨³å®šæ€§é—®é¢˜ï¼Œå¯èƒ½ä¸ä»¥ä¸‹å› ç´ æœ‰å…³ï¼š

- **ç½‘ç»œå»¶è¿Ÿæˆ–ä¸ç¨³å®š**ï¼šç½‘ç»œé—®é¢˜å¯èƒ½å¯¼è‡´APIè¯·æ±‚æœªèƒ½åŠæ—¶è·å–æœ€æ–°çš„Podä¿¡æ¯ã€‚
- **Kubernetes APIæœåŠ¡å™¨çš„å»¶è¿Ÿæˆ–ä¸€è‡´æ€§**ï¼šåœ¨é›†ç¾¤ä¸­ï¼Œæ•°æ®çš„åŒæ­¥å¯èƒ½å­˜åœ¨çŸ­æš‚çš„ä¸ä¸€è‡´æ€§ï¼Œå¯¼è‡´å¶å°”æ— æ³•æ‰¾åˆ°Podã€‚
- **Podçš„çŠ¶æ€å˜åŒ–**ï¼šå¦‚æœPodæ­£åœ¨è¢«é‡å¯ã€è¿ç§»æˆ–çŸ­æš‚åœ°è¢«åˆ é™¤å†åˆ›å»ºï¼Œå¯èƒ½ä¼šåœ¨æŸäº›æ—¶åˆ»æ— æ³•è®¿é—®ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **é£é™©è¯„çº§åˆ†ä¸ºä¸‰ç±»ï¼Œä¸æ¶‰åŠï¼Œä½é£é™©å’Œé«˜é£é™©**ã€‚
2. **å¦‚æœè¯¥Issueæè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠ**ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueåæ˜ çš„æ˜¯ç³»ç»ŸåŠŸèƒ½æ€§é—®é¢˜ï¼Œå¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---


## Issue #129176 Empty annotation removes all existing and valid annotations

- Issue é“¾æ¥ï¼š[#129176](https://github.com/kubernetes/kubernetes/issues/129176)

### Issue å†…å®¹

#### What happened?

When creating a resource (e.g. a Service) with annotations the annotations are all removed when one of them has an empty value. The same happens on update.
It happens in our production environment as well as locally on minikube so I don't expect anything we deployed there to interfere with it.

#### What did you expect to happen?

Applying the resource should fail on empty values

#### How can we reproduce it (as minimally and precisely as possible)?

Create a Service (notice the empty `baz` annotation)

```
apiVersion: v1
kind: Service
metadata:
  name: my-service
  annotations:
    foo: bar
    baz:
spec:
  selector:
    app.kubernetes.io/name: MyApp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376
```

Apply it

```
kubectl apply -f my-service.yaml
```

Retrieve it
```
kubectl get service my-service -o yaml
```

The service also lacks the `foo` annotation
```
apiVersion: v1
kind: Service
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"my-service","namespace":"default"},"spec":{"ports":[{"port":80,"protocol":"TCP","targetPort":9376}],"selector":{"app.kubernetes.io/name":"MyApp"}}}
  creationTimestamp: "2024-12-12T12:16:24Z"
  name: my-service
  namespace: default
  resourceVersion: "1776057"
  uid: a947a42c-a922-4f67-8103-c88dbb814004
spec:
  clusterIP: 169.254.131.120
  clusterIPs:
  - 169.254.131.120
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - port: 80
    protocol: TCP
    targetPort: 9376
  selector:
    app.kubernetes.io/name: MyApp
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```
  

#### Anything else we need to know?

This happens with `kubectl apply` but also with `helm install` 

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.4
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.25.5
WARNING: version difference between client (1.28) and server (1.25) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

MacOs 15.1.1

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨`kubectl apply`æˆ–`helm install`åˆ›å»ºKubernetesèµ„æºï¼ˆä¾‹å¦‚Serviceï¼‰æ—¶ï¼Œå¦‚æœ`annotations`ä¸­åŒ…å«ä¸€ä¸ªç©ºå€¼çš„é”®ï¼ˆå¦‚`baz`ï¼‰ï¼Œåˆ™æ‰€æœ‰å·²æœ‰ä¸”æœ‰æ•ˆçš„`annotations`éƒ½ä¼šè¢«ç§»é™¤ã€‚è¿™æ„å‘³ç€ï¼Œå³ä½¿å…¶ä»–`annotations`å…·æœ‰æœ‰æ•ˆçš„å€¼ï¼Œä¹Ÿä¼šå› ä¸ºå­˜åœ¨ä¸€ä¸ªç©ºçš„`annotation`è€Œå¯¼è‡´å…¨éƒ¨è¢«åˆ é™¤ã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼š

1. `annotations`é€šå¸¸ç”¨äºå­˜å‚¨å…ƒæ•°æ®ï¼Œå¹¶å¯èƒ½ç”¨äºé…ç½®æŸäº›åŠŸèƒ½æˆ–ç­–ç•¥ã€‚ä½†åœ¨æ­¤æ¡ˆä¾‹ä¸­ï¼Œé—®é¢˜çš„æ ¹æºæ˜¯ç”¨æˆ·æä¾›äº†å«æœ‰ç©ºå€¼çš„`annotation`ï¼Œå¯¼è‡´æ‰€æœ‰`annotations`è¢«ç§»é™¤ã€‚è¿™æ›´åƒæ˜¯ä¸€ä¸ªé…ç½®æˆ–ä½¿ç”¨ä¸Šçš„é”™è¯¯ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚

2. è¯¥é—®é¢˜ä¸ä¼šå¯¼è‡´æƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œã€æ•°æ®æ³„éœ²ç­‰å®‰å…¨é£é™©ã€‚

3. æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬2æ¡ï¼šâ€œå¦‚æœè¯¥issueæè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚â€

---


## Issue #129169 Data race in cbor serializer

- Issue é“¾æ¥ï¼š[#129169](https://github.com/kubernetes/kubernetes/issues/129169)

### Issue å†…å®¹

Seen on 2f9fb220ba69fe0d35fd151de05b769c5cb05e01 when running `TestFrontProxyConfig` with the race detector enabled and the following patch applied:

```diff
diff --git a/test/integration/examples/apiserver_test.go b/test/integration/examples/apiserver_test.go
index a50a8a846a5..837122a1847 100644
--- a/test/integration/examples/apiserver_test.go
+++ b/test/integration/examples/apiserver_test.go
@@ -45,7 +45,6 @@ import (
 	"k8s.io/apiserver/pkg/authentication/serviceaccount"
 	"k8s.io/apiserver/pkg/authentication/user"
 	genericapirequest "k8s.io/apiserver/pkg/endpoints/request"
-	"k8s.io/apiserver/pkg/features"
 	"k8s.io/apiserver/pkg/server/dynamiccertificates"
 	genericapiserveroptions "k8s.io/apiserver/pkg/server/options"
 	utilfeature "k8s.io/apiserver/pkg/util/feature"
@@ -263,7 +262,7 @@ func TestFrontProxyConfig(t *testing.T) {
 		testFrontProxyConfig(t, false)
 	})
 	t.Run("WithUID", func(t *testing.T) {
-		featuregatetesting.SetFeatureGateDuringTest(t, utilfeature.DefaultFeatureGate, features.RemoteRequestHeaderUID, true)
+		featuregatetesting.SetFeatureGateDuringTest(t, utilfeature.DefaultFeatureGate, "AllAlpha", true)
 		testFrontProxyConfig(t, true)
 	})
 }
```

```
WARNING: DATA RACE
Write at 0x00c005915e30 by goroutine 8814:
  k8s.io/apimachinery/pkg/runtime/serializer/cbor/internal/modes.(*checkers).getCheckerInternal()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/serializer/cbor/internal/modes/custom.go:310 +0x1744
  k8s.io/apimachinery/pkg/runtime/serializer/cbor/internal/modes.(*checkers).getChecker()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/serializer/cbor/internal/modes/custom.go:120 +0xc8
  k8s.io/apimachinery/pkg/runtime/serializer/cbor/internal/modes.RejectCustomMarshalers()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/serializer/cbor/internal/modes/custom.go:43 +0x90
  k8s.io/apimachinery/pkg/runtime/serializer/cbor.(*serializer).encode()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/serializer/cbor/cbor.go:155 +0xc4
  k8s.io/apimachinery/pkg/runtime/serializer/cbor.(*serializer).Encode()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/serializer/cbor/cbor.go:142 +0x64
  k8s.io/apimachinery/pkg/runtime.WithVersionEncoder.Encode()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/helper.go:246 +0x2fc
  k8s.io/apimachinery/pkg/runtime.(*WithVersionEncoder).Encode()
      <autogenerated>:1 +0x94
  k8s.io/apimachinery/pkg/runtime.Encode()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/codec.go:49 +0x90
  k8s.io/client-go/rest.(*Request).Body()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/client-go/rest/request.go:503 +0x584
  k8s.io/apiserver/plugin/pkg/authorizer/webhook.(*subjectAccessReviewV1Client).Create()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/plugin/pkg/authorizer/webhook/webhook.go:490 +0x1b8
  k8s.io/apiserver/plugin/pkg/authorizer/webhook.(*WebhookAuthorizer).Authorize.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/plugin/pkg/authorizer/webhook/webhook.go:240 +0xe0
  k8s.io/apiserver/pkg/util/webhook.WithExponentialBackoff.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/util/webhook/webhook.go:125 +0x40
  k8s.io/apimachinery/pkg/util/wait.runConditionWithCrashProtectionWithContext()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:154 +0x60
  k8s.io/apimachinery/pkg/util/wait.ExponentialBackoffWithContext()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:485 +0x7c
  k8s.io/apiserver/pkg/util/webhook.WithExponentialBackoff()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/util/webhook/webhook.go:124 +0xd0
  k8s.io/apiserver/plugin/pkg/authorizer/webhook.(*WebhookAuthorizer).Authorize()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/plugin/pkg/authorizer/webhook/webhook.go:235 +0x504
  k8s.io/apiserver/pkg/authorization/union.unionAuthzHandler.Authorize()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/authorization/union/union.go:52 +0xf4
  k8s.io/apiserver/pkg/authorization/union.(*unionAuthzHandler).Authorize()
      <autogenerated>:1 +0x74
  k8s.io/apiserver/pkg/endpoints/filters.withAuthorization.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/authorization.go:71 +0x100
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x1c0
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func23.deferwrap1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6c
  runtime.deferreturn()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/runtime/panic.go:605 +0x5c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle.func10()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:298 +0x108
  k8s.io/apiserver/pkg/util/flowcontrol.(*configController).Handle.func2()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/apf_filter.go:192 +0x2ac
  k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset.(*request).Finish.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go:391 +0x88
  k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset.(*request).Finish()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go:392 +0x44
  k8s.io/apiserver/pkg/util/flowcontrol.(*configController).Handle()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/apf_filter.go:179 +0x9f8
  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle.func11()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:304 +0xe0
  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:305 +0xecc
  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle-fm()
      <autogenerated>:1 +0x4c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x1c0
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func24.deferwrap1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6c
  runtime.deferreturn()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/runtime/panic.go:605 +0x5c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithImpersonation.func4()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/impersonation.go:50 +0x178
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x1c0
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func25.deferwrap1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6c
  runtime.deferreturn()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/runtime/panic.go:605 +0x5c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x1c0
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func27.deferwrap1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6c
  runtime.deferreturn()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/runtime/panic.go:605 +0x5c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filters.withAuthentication.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/authentication.go:123 +0x8dc
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:94 +0x3a8
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithWarningRecorder.func11()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/warning.go:35 +0x10c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/timeout.go:115 +0xb8

Previous read at 0x00c005915e30 by goroutine 8816:
  k8s.io/apimachinery/pkg/runtime/serializer/cbor/internal/modes.(*checkers).getChecker()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/serializer/cbor/internal/modes/custom.go:117 +0x88
  k8s.io/apimachinery/pkg/runtime/serializer/cbor/internal/modes.RejectCustomMarshalers()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/serializer/cbor/internal/modes/custom.go:43 +0x90
  k8s.io/apimachinery/pkg/runtime/serializer/cbor.(*serializer).encode()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/serializer/cbor/cbor.go:155 +0xc4
  k8s.io/apimachinery/pkg/runtime/serializer/cbor.(*serializer).Encode()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/serializer/cbor/cbor.go:142 +0x64
  k8s.io/apimachinery/pkg/runtime.WithVersionEncoder.Encode()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/helper.go:246 +0x2fc
  k8s.io/apimachinery/pkg/runtime.(*WithVersionEncoder).Encode()
      <autogenerated>:1 +0x94
  k8s.io/apimachinery/pkg/runtime.Encode()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/runtime/codec.go:49 +0x90
  k8s.io/client-go/rest.(*Request).Body()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/client-go/rest/request.go:503 +0x584
  k8s.io/apiserver/plugin/pkg/authorizer/webhook.(*subjectAccessReviewV1Client).Create()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/plugin/pkg/authorizer/webhook/webhook.go:490 +0x1b8
  k8s.io/apiserver/plugin/pkg/authorizer/webhook.(*WebhookAuthorizer).Authorize.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/plugin/pkg/authorizer/webhook/webhook.go:240 +0xe0
  k8s.io/apiserver/pkg/util/webhook.WithExponentialBackoff.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/util/webhook/webhook.go:125 +0x40
  k8s.io/apimachinery/pkg/util/wait.runConditionWithCrashProtectionWithContext()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:154 +0x60
  k8s.io/apimachinery/pkg/util/wait.ExponentialBackoffWithContext()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:485 +0x7c
  k8s.io/apiserver/pkg/util/webhook.WithExponentialBackoff()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/util/webhook/webhook.go:124 +0xd0
  k8s.io/apiserver/plugin/pkg/authorizer/webhook.(*WebhookAuthorizer).Authorize()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/plugin/pkg/authorizer/webhook/webhook.go:235 +0x504
  k8s.io/apiserver/pkg/authorization/union.unionAuthzHandler.Authorize()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/authorization/union/union.go:52 +0xf4
  k8s.io/apiserver/pkg/authorization/union.(*unionAuthzHandler).Authorize()
      <autogenerated>:1 +0x74
  k8s.io/apiserver/pkg/endpoints/filters.withAuthorization.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/authorization.go:71 +0x100
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x1c0
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func23.deferwrap1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6c
  runtime.deferreturn()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/runtime/panic.go:605 +0x5c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle.func10()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:298 +0x108
  k8s.io/apiserver/pkg/util/flowcontrol.(*configController).Handle.func2()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/apf_filter.go:192 +0x2ac
  k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset.(*request).Finish.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go:391 +0x88
  k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset.(*request).Finish()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go:392 +0x44
  k8s.io/apiserver/pkg/util/flowcontrol.(*configController).Handle()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/apf_filter.go:179 +0x9f8
  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle.func11()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:304 +0xe0
  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:305 +0xecc
  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle-fm()
      <autogenerated>:1 +0x4c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x1c0
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func24.deferwrap1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6c
  runtime.deferreturn()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/runtime/panic.go:605 +0x5c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithImpersonation.func4()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/impersonation.go:50 +0x178
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x1c0
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func25.deferwrap1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6c
  runtime.deferreturn()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/runtime/panic.go:605 +0x5c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x1c0
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func27.deferwrap1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6c
  runtime.deferreturn()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/runtime/panic.go:605 +0x5c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filters.withAuthentication.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/authentication.go:123 +0x8dc
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:94 +0x3a8
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithWarningRecorder.func11()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/warning.go:35 +0x10c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP.func1()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/timeout.go:115 +0xb8

Goroutine 8814 (running) created at:
  k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/timeout.go:101 +0x250
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestDeadline.withRequestDeadline.func28()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/request_deadline.go:100 +0x1b0
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithWaitGroup.withWaitGroup.func29()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/waitgroup.go:86 +0x170
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithCacheControl.func14()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/cachecontrol.go:31 +0xbc
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithHTTPLogging.WithLogging.withLogging.func35()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/httplog/httplog.go:139 +0x1e8
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filters.WithTracing.func2()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/traces.go:51 +0x1cc
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*middleware).serveHTTP()
      /go/src/github.com/kubernetes/kubernetes/vendor/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/handler.go:218 +0x13e0
  go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.NewMiddleware.func1.1()
      /go/src/github.com/kubernetes/kubernetes/vendor/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/handler.go:74 +0x5c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithLatencyTrackers.func16()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/webhook_duration.go:56 +0x194
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRoutine.func17()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/routine/routine.go:65 +0x94
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestInfo.func18()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/requestinfo.go:39 +0x138
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestReceivedTimestamp.withRequestReceivedTimestampWithClock.func32()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/request_received_time.go:38 +0xb0
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithMuxAndDiscoveryComplete.func19()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/mux_discovery_complete.go:52 +0xb8
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithPanicRecovery.withPanicRecovery.func33()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/wrap.go:73 +0x17c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithAuditInit.withAuditInit.func34()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/audit_init.go:63 +0x124
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.(*APIServerHandler).ServeHTTP()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/handler.go:188 +0x5c
  net/http.serverHandler.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:3210 +0x2a8
  net/http.initALPNRequest.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:3819 +0x268
  net/http.(*initALPNRequest).ServeHTTP()
      <autogenerated>:1 +0x74
  net/http.Handler.ServeHTTP-fm()
      <autogenerated>:1 +0x60
  golang.org/x/net/http2.(*serverConn).runHandler()
      /go/src/github.com/kubernetes/kubernetes/vendor/golang.org/x/net/http2/server.go:2439 +0x194
  golang.org/x/net/http2.(*serverConn).scheduleHandler.gowrap1()
      /go/src/github.com/kubernetes/kubernetes/vendor/golang.org/x/net/http2/server.go:2373 +0x58

Goroutine 8816 (running) created at:
  k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/timeout.go:101 +0x250
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestDeadline.withRequestDeadline.func28()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/request_deadline.go:100 +0x1b0
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithWaitGroup.withWaitGroup.func29()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/waitgroup.go:86 +0x170
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithCacheControl.func14()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/cachecontrol.go:31 +0xbc
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithHTTPLogging.WithLogging.withLogging.func35()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/httplog/httplog.go:139 +0x1e8
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/endpoints/filters.WithTracing.func2()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/traces.go:51 +0x1cc
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*middleware).serveHTTP()
      /go/src/github.com/kubernetes/kubernetes/vendor/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/handler.go:218 +0x13e0
  go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.NewMiddleware.func1.1()
      /go/src/github.com/kubernetes/kubernetes/vendor/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/handler.go:74 +0x5c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithLatencyTrackers.func16()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/webhook_duration.go:56 +0x194
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRoutine.func17()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/routine/routine.go:65 +0x94
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestInfo.func18()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/requestinfo.go:39 +0x138
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestReceivedTimestamp.withRequestReceivedTimestampWithClock.func32()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/request_received_time.go:38 +0xb0
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithMuxAndDiscoveryComplete.func19()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/mux_discovery_complete.go:52 +0xb8
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithPanicRecovery.withPanicRecovery.func33()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/wrap.go:73 +0x17c
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithAuditInit.withAuditInit.func34()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/audit_init.go:63 +0x124
  net/http.HandlerFunc.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:2220 +0x48
  k8s.io/apiserver/pkg/server.(*APIServerHandler).ServeHTTP()
      /go/src/github.com/kubernetes/kubernetes/staging/src/k8s.io/apiserver/pkg/server/handler.go:188 +0x5c
  net/http.serverHandler.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:3210 +0x2a8
  net/http.initALPNRequest.ServeHTTP()
      /go/pkg/mod/golang.org/toolchain@v0.0.1-go1.23.1.darwin-arm64/src/net/http/server.go:3819 +0x268
  net/http.(*initALPNRequest).ServeHTTP()
      <autogenerated>:1 +0x74
  net/http.Handler.ServeHTTP-fm()
      <autogenerated>:1 +0x60
  golang.org/x/net/http2.(*serverConn).runHandler()
      /go/src/github.com/kubernetes/kubernetes/vendor/golang.org/x/net/http2/server.go:2439 +0x194
  golang.org/x/net/http2.(*serverConn).scheduleHandler.gowrap1()
      /go/src/github.com/kubernetes/kubernetes/vendor/golang.org/x/net/http2/server.go:2373 +0x58
```

/kind bug
/sig api-machinery 

@kubernetes/sig-api-machinery-bugs @deads2k @jpbetz @benluddy 

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨è¿è¡Œ`TestFrontProxyConfig`æµ‹è¯•å¹¶å¯ç”¨ç«æ€æ£€æµ‹å™¨ï¼ˆrace detectorï¼‰æ—¶ï¼Œå‘ç°åœ¨CBORåºåˆ—åŒ–å™¨ä¸­å­˜åœ¨æ•°æ®ç«äº‰ï¼ˆdata raceï¼‰é—®é¢˜ã€‚å…·ä½“è¡¨ç°ä¸ºåœ¨å¹¶å‘ç¯å¢ƒä¸‹ï¼Œå¯¹`checkers`å¯¹è±¡çš„è®¿é—®æ²¡æœ‰è¿›è¡Œé€‚å½“çš„åŒæ­¥ï¼Œå¯¼è‡´è¯»å†™å†²çªã€‚

æ•°æ®ç«äº‰å¯èƒ½å¯¼è‡´ç¨‹åºçš„ä¸ç¡®å®šè¡Œä¸ºï¼Œå¦‚å´©æºƒã€æ•°æ®æŸåç­‰ã€‚åœ¨æŸäº›æç«¯æƒ…å†µä¸‹ï¼Œæ•°æ®ç«äº‰å¯èƒ½è¢«åˆ©ç”¨æ¥è¿›è¡Œæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚ç„¶è€Œï¼Œä»Issueæä¾›çš„å †æ ˆä¿¡æ¯å’Œé—®é¢˜æè¿°æ¥çœ‹ï¼Œè¿™ä¸ªæ•°æ®ç«äº‰ä¸»è¦å½±å“çš„æ˜¯åºåˆ—åŒ–å™¨çš„å†…éƒ¨çŠ¶æ€ï¼Œæ²¡æœ‰ç›´æ¥çš„é€”å¾„è¢«å¤–éƒ¨æ”»å‡»è€…åˆ©ç”¨æ¥æ‰§è¡Œä»»æ„ä»£ç ã€æå‡æƒé™æˆ–è¿›è¡Œå…¶ä»–é«˜å±æ“ä½œã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. æ•°æ®ç«äº‰å±äºä»£ç å®ç°ä¸­çš„å¹¶å‘é—®é¢˜ï¼Œå¯èƒ½å¯¼è‡´ç¨‹åºå¼‚å¸¸æˆ–å´©æºƒã€‚
2. è¯¥é—®é¢˜æ²¡æœ‰æåˆ°æ”»å‡»è€…å¯ä»¥åˆ©ç”¨çš„æ•°æ®è¾“å…¥æˆ–ç‰¹å®šæ“ä½œæ¥è§¦å‘ï¼Œä¸”éœ€è¦å…·å¤‡ä¸€å®šçš„æƒé™æ‰èƒ½å½±å“åºåˆ—åŒ–è¿‡ç¨‹ã€‚
3. æŒ‰ç…§CVSS 3.1æ ‡å‡†è¯„ä¼°ï¼Œè¯¥é—®é¢˜çš„å½±å“ä¸»è¦åœ¨äºå¯ç”¨æ€§æ–¹é¢ï¼Œä¸”åˆ©ç”¨å¤æ‚åº¦è¾ƒé«˜ï¼Œæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™ï¼Œæ•…å¾—åˆ†ä¸ä¼šè¾¾åˆ°é«˜å±ç­‰çº§ã€‚

---


## Issue #129157 `1.31` requires `privileged` to create a tun device, `1.30` only required `NET_ADMIN`

- Issue é“¾æ¥ï¼š[#129157](https://github.com/kubernetes/kubernetes/issues/129157)

### Issue å†…å®¹

#### What happened?

Upgraded to `1.31.3` (k3s) and all of my pods failed to start with `permission denied` errors. On start up of the pod, it creates a tun device at `/dev/net/tun`. On version 1.30, this worked fine with the `NET_ADMIN` permissions. Now I have updated to `1.31` and the pods won't start unless I add `privileged` permission to the pods. 

#### What did you expect to happen?

v1.30 to be the same as v1.31 in terms of the permissions required to create tun devices.

#### How can we reproduce it (as minimally and precisely as possible)?

I am creating the tun device in go using the following:

```
err = unix.Mknod("/dev/net/tun", unix.S_IFCHR|0600, int(unix.Mkdev(10, 200)))
if err != nil {
	return nil, fmt.Errorf("failed to create /dev/net/tun: %w", err)
}

fd, err = unix.Open("/dev/net/tun", os.O_RDWR, 0)
if err != nil {
	return nil, fmt.Errorf("created /dev/net/tun, but still failed: %w", err) // <-- failing here
}
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.4
Kustomize Version: v5.4.2
Server Version: v1.31.3+k3s1
```

</details>


#### Cloud provider

<details>
Self hosted
</details>


#### OS version

OpenSUSE MicroOS

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)
containerd

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

Flannel

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨å°†Kubernetesä»`v1.30`å‡çº§åˆ°`v1.31`åï¼Œåˆ›å»ºtunè®¾å¤‡éœ€è¦çš„æƒé™ä»`NET_ADMIN`æå‡åˆ°äº†`privileged`ï¼Œå¯¼è‡´Podæ— æ³•å¯åŠ¨ã€‚å…·ä½“æ¥è¯´ï¼Œåº”ç”¨åœ¨å®¹å™¨å†…åˆ›å»º`/dev/net/tun`è®¾å¤‡æ—¶ï¼Œåœ¨`v1.30`ç‰ˆæœ¬ä¸­ä»…éœ€è¦æˆäºˆ`NET_ADMIN`æƒé™å³å¯ï¼Œè€Œåœ¨`v1.31`ç‰ˆæœ¬ä¸­åˆ™éœ€è¦æˆäºˆ`privileged`æƒé™ã€‚

æ ¹æ®Issueå†…å®¹ï¼Œè¿™æ˜¯ç”±äºKubernetesåœ¨æ–°ç‰ˆæœ¬ä¸­å¯¹æƒé™ç®¡ç†è¿›è¡Œäº†è°ƒæ•´ï¼Œå¯¼è‡´äº†æƒé™è¦æ±‚çš„å˜åŒ–ã€‚è¯¥é—®é¢˜ä¸»è¦æ¶‰åŠæƒé™é…ç½®çš„å˜åŒ–ï¼Œå¯¼è‡´åº”ç”¨è¿è¡Œå—åˆ°å½±å“ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

æ­¤Issueåæ˜ çš„æ˜¯Kubernetesç‰ˆæœ¬å‡çº§åæƒé™è¦æ±‚çš„å˜åŒ–ï¼Œå¯¼è‡´éœ€è¦ä¸ºPodæˆäºˆæ›´é«˜çš„æƒé™æ‰èƒ½æ­£å¸¸è¿è¡Œã€‚è¿™å¹¶ä¸æ˜¯ä¸€ä¸ªå®‰å…¨æ¼æ´æˆ–å®‰å…¨é—®é¢˜ï¼Œè€Œæ˜¯è½¯ä»¶å‡çº§å¸¦æ¥çš„é…ç½®å˜æ›´é—®é¢˜ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

- **æ ‡å‡†2**ï¼šå¦‚æœè¯¥issueæè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

å› æ­¤ï¼Œé£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ï¼Œä¸éœ€è¦æä¾›å¤ç°è„šæœ¬ã€‚

---


## Issue #129152 [FG:InPlacePodVerticalScaling] Inconsistent handling of memory limit decrease

- Issue é“¾æ¥ï¼š[#129152](https://github.com/kubernetes/kubernetes/issues/129152)

### Issue å†…å®¹

/kind bug

When shrinking the pod-level memory limits (sum of container limits iff all containers have limits), the Kubelet checks the current pod memory usage, and doesn't apply the new limits if the new limits < current usage. However, the Kubelet doesn't place the same restriction on containers, and we don't require container runtimes to make the same check.

In practice, this means that for a single container pod, decreasing the memory limit below usage will result in the resize being indefinitely in progress, but for a multiple container pod, if some containers have sufficient free memory, you can shrink the memory limits below usage for a container in the pod, resulting in that container being OOM-killed.

What is the desired behavior here?

https://docs.google.com/document/d/1cEFLXKwNOSNLAkzyhoJUgkBW0OiX-9bXB_aJV7OAypw/edit?tab=t.0 provides more background information and several options for how this should be handled.

/sig node
/priority important-soon

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŒ‡å‡ºï¼Œåœ¨Kubernetesä¸­ï¼Œå½“ç¼©å‡Podçº§åˆ«çš„å†…å­˜é™åˆ¶ï¼ˆå³æ‰€æœ‰å®¹å™¨éƒ½æœ‰å†…å­˜é™åˆ¶æ—¶ï¼Œå®¹å™¨é™åˆ¶ä¹‹å’Œï¼‰ï¼Œ`Kubelet`ä¼šæ£€æŸ¥å½“å‰Podçš„å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œå¦‚æœæ–°çš„é™åˆ¶å°äºå½“å‰ä½¿ç”¨é‡ï¼Œåˆ™ä¸ä¼šåº”ç”¨æ–°çš„é™åˆ¶ã€‚ç„¶è€Œï¼Œ`Kubelet`å¯¹å®¹å™¨çº§åˆ«æ²¡æœ‰è¿›è¡ŒåŒæ ·çš„æ£€æŸ¥ï¼Œä¸”æˆ‘ä»¬ä¹Ÿä¸è¦æ±‚å®¹å™¨è¿è¡Œæ—¶æ‰§è¡Œæ­¤æ£€æŸ¥ã€‚

åœ¨å®é™…æ“ä½œä¸­ï¼Œè¿™æ„å‘³ç€ï¼š

- **å•å®¹å™¨Pod**ï¼šå¦‚æœå°†å†…å­˜é™åˆ¶å‡å°‘åˆ°ä½äºå½“å‰ä½¿ç”¨é‡ï¼ŒPodçš„è°ƒæ•´å°†ä¸€ç›´å¤„äºè¿›è¡Œä¸­çŠ¶æ€ï¼Œæ–°é™åˆ¶ä¸ä¼šç”Ÿæ•ˆã€‚
- **å¤šå®¹å™¨Pod**ï¼šå¦‚æœæŸäº›å®¹å™¨æœ‰è¶³å¤Ÿçš„ç©ºé—²å†…å­˜ï¼Œå¯ä»¥å°†å†…å­˜é™åˆ¶ç¼©å‡åˆ°ä½äºæŸä¸ªå®¹å™¨çš„å½“å‰ä½¿ç”¨é‡ï¼Œå¯¼è‡´è¯¥å®¹å™¨è¢«OOMï¼ˆOut Of Memoryï¼‰æ€æ­»ã€‚

è¿™ç§ä¸ä¸€è‡´çš„è¡Œä¸ºå¯èƒ½å¯¼è‡´ç‰¹å®šå®¹å™¨è¢«æ„å¤–åœ°OOMæ€æ­»ï¼Œé€ æˆæœåŠ¡ä¸­æ–­ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. è¿™æ˜¯ä¸€ä¸ªå®‰å…¨é—®é¢˜ï¼Œæ¶‰åŠåˆ°å®¹å™¨å¯èƒ½è¢«æ„å¤–ç»ˆæ­¢ï¼Œå±äºå¯ç”¨æ€§æ–¹é¢çš„å®‰å…¨é£é™©ã€‚
2. æ ¹æ®ç¬¬äº”æ¡ï¼Œæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»å¦‚æœéœ€è¦ä¸€å®šæƒé™æ‰èƒ½å®æ–½æ”»å‡»ï¼Œä¸”éœ€è¦åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™ï¼Œåˆ™CVSSè¯„çº§åœ¨Highä»¥ä¸‹ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚
3. åœ¨æ­¤æƒ…å†µä¸‹ï¼Œæ”»å‡»è€…éœ€è¦æœ‰æƒé™ä¿®æ”¹Podçš„è§„æ ¼ï¼ˆå†…å­˜é™åˆ¶ï¼‰ï¼Œè¿™éœ€è¦è¾ƒé«˜çš„æƒé™ï¼Œä¸æ˜¯ä»»æ„ä½æƒé™ç”¨æˆ·éƒ½å¯ä»¥æ‰§è¡Œçš„æ“ä½œã€‚

å› æ­¤ï¼Œç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥é—®é¢˜å±äºä½é£é™©å®‰å…¨é—®é¢˜ã€‚

---


## Issue #129146 kube-proxy: net.InterfaceAddrs may return error due to a race condition, causing the Nodeport Service to be inaccessible

- Issue é“¾æ¥ï¼š[#129146](https://github.com/kubernetes/kubernetes/issues/129146)

### Issue å†…å®¹

#### What happened?

In our production environment, which verson is 1.24.4
1. kube-proxy's error log: 
2024-12-09T07:35:24.325300135+08:00 E1209 07:35:24.325193       1 proxier.go:1131] "Failed to get node IP address matching nodeport cidr" err="error listing all interfaceAddrs from host, error: route ip+net: no such network interface"
2. Then the nodeport service's backend pod on this node cannot be accessible by the Nodeport Service.
3. On the save time, in this node's /var/log/message, the log is:
Dec  9 07:35:24 VM-96-121-tencentos containerd: 2024-12-09T07:35:24+08:00 [info] cmdDel: {containerId c915c615cb43c0f7711d125e39e722581384bc45b3453e3c3c79ebe5ed994e71, netNs /var/run/netns/cni-278204fe-6400-6354-982f-ae2299bfae17, ifName eth0, args K8S_POD_INFRA_CONTAINER_ID=c915c615cb43c0f7711d125e39e722581384bc45b3453e3c3c79ebe5ed994e71;K8S_POD_UID=0a3c7874-54ae-4ec6-b4a5-b5c3b9d63a12;IgnoreUnknown=1;K8S_POD_NAMESPACE=bkmonitor-operator;K8S_POD_NAME=bcs-blackbox-job-073518-f8n4c, path /opt/cni/bin, stdinData {"capabilities":{"bandwidth":true,"portMappings":true},"cniVersion":"0.3.1","defaultDelegates":"tke-bridge","kubeconfig":"/etc/kubernetes/tke-cni-kubeconfig","logLevel":"info","name":"multus-cni","type":"multus"}}, <nil>, <nil>


#### What did you expect to happen?

The Nodeport Service should be accessible.

#### How can we reproduce it (as minimally and precisely as possible)?

In the latest version, it also have the same problem.

1, syncProxyRules->GetNodeIPs->net.InterfaceAddrs()->interfaceAddrTable(which is in interface_linux.go for Linux)->syscall.NetlinkRIB(syscall.RTM_GETADDR, syscall.AF_UNSPEC) is called.
2. Then an interface is delete by CNI.
3. Then interfaceAddrTable(which is in interface_linux.go) call interfaceTable(which is in interface_linux.go)->syscall.NetlinkRIB(syscall.RTM_GETLINK, syscall.AF_UNSPEC)
   The key point is that, RTM_GETADDR's return have the delete interface, but the RTM_GETLINK's return donot have the deleted interface.
4.  Then in interfaceAddrTable->addrTable will return err.
<img width="805" alt="Clipboard_Screenshot_1733841188" src="https://github.com/user-attachments/assets/d84160a2-2b10-4a0a-9078-faad3a010076">
5. Then the syncProxyRules will not write correct forward configuration because the GetNodeIPs's return donot have any ip.



#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
This production environment cluster version is v1.24.4

But any version have the same problem.

</details>


#### Cloud provider

<details>
TKE
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```
Centos8
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨kube-proxyä¸­ï¼Œç”±äºnet.InterfaceAddrs()å‡½æ•°å¯èƒ½ä¼šå› ä¸ºä¸CNIæ’ä»¶åˆ é™¤ç½‘ç»œæ¥å£çš„ç«äº‰æ¡ä»¶ï¼ˆrace conditionï¼‰ï¼Œå¯¼è‡´è·å–èŠ‚ç‚¹IPåœ°å€å¤±è´¥ï¼Œä»è€Œå¯¼è‡´NodePortæœåŠ¡æ— æ³•è®¿é—®ã€‚è¿™æ˜¯ä¸€ä¸ªå› ä¸ºç«äº‰æ¡ä»¶å¯¼è‡´çš„ç³»ç»Ÿå¯é æ€§é—®é¢˜ï¼Œä¼šå½±å“æœåŠ¡çš„æ­£å¸¸è®¿é—®ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. è¯¥é—®é¢˜å±äºç³»ç»Ÿç¨³å®šæ€§å’Œå¯ç”¨æ€§é—®é¢˜ï¼Œå¹¶éå®‰å…¨é—®é¢˜ã€‚
2. æ²¡æœ‰æåŠä»»ä½•æ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ­¤æ¼æ´è¿›è¡Œæ¶æ„æ“ä½œçš„æƒ…å½¢ã€‚
3. ä¸æ¶‰åŠä¿¡æ¯æ³„éœ²ã€æƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œç­‰å®‰å…¨é£é™©ã€‚

å› æ­¤ï¼Œè¯¥Issueæè¿°çš„é—®é¢˜ä¸å±äºå®‰å…¨é—®é¢˜ã€‚

---


## Issue #129143 Kube scheduler has a confusing error message when scheduling pods that use claims with `ReadWriteOncePod` access mode

- Issue é“¾æ¥ï¼š[#129143](https://github.com/kubernetes/kubernetes/issues/129143)

### Issue å†…å®¹

#### What happened?

When we deploy a claim like:
```yaml
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ebs-claim
spec:
  accessModes:
  - ReadWriteOncePod
  resources:
    requests:
      storage: 4Gi
```

And then deploy two pods:
```yaml
---
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  containers:
  - name: app
    image: centos
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo $(date -u) >> /data/out.txt; sleep 5; done"]
    volumeMounts:
    - name: persistent-storage
      mountPath: /data
  volumes:
  - name: persistent-storage
    persistentVolumeClaim:
      claimName: ebs-claim
---
apiVersion: v1
kind: Pod
metadata:
  name: app2
spec:
  containers:
  - name: app
    image: centos
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo $(date -u) >> /data/out.txt; sleep 5; done"]
    volumeMounts:
    - name: persistent-storage
      mountPath: /data
  volumes:
  - name: persistent-storage
    persistentVolumeClaim:
      claimName: ebs-claim
```
Second pod is rightfully not scheduled.
It has the following error message:
```
 k describe po app2

Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  5s    default-scheduler  0/5 nodes are available: 5 node has pod using PersistentVolumeClaim with the same name and ReadWriteOncePod access mode. preemption: 0/5 nodes are available: 5 No preemption victims found for incoming pod.
  Warning  FailedScheduling  2s    default-scheduler  0/5 nodes are available: 5 node has pod using PersistentVolumeClaim with the same name and ReadWriteOncePod access mode. preemption: 0/5 nodes are available: 5 No preemption victims found for incoming pod.
```
This error message is misleading - there are no 5 nodes that have pods using this PersistentVolumeClaim.

Error message comes from https://github.com/kubernetes/kubernetes/blob/v1.30.5/pkg/scheduler/framework/plugins/volumerestrictions/volume_restrictions.go#L55.
It's set in the following method - https://github.com/kubernetes/kubernetes/blob/v1.30.5/pkg/scheduler/framework/plugins/volumerestrictions/volume_restrictions.go#L280
It's raised upon calling the `Filter` method - https://github.com/kubernetes/kubernetes/blob/v1.30.5/pkg/scheduler/framework/plugins/volumerestrictions/volume_restrictions.go#L314

#### What did you expect to happen?

I expect that we get an error message that reads something like:
```
  Warning  FailedScheduling  5s    default-scheduler  0/5 nodes are available: 5 nodes were not able to schedule a pod using PersistentVolumeClaim <PVC-Name> name, ReadWriteOncePod access mode. preemption: 0/5 nodes are available: 5 No preemption victims found for incoming pod.
```

or similar message, that points the operator to the actual reason for not scheduling the pod.


#### How can we reproduce it (as minimally and precisely as possible)?

Build a cluster with 3 or more nodes and apply the following manifests:
```ymal
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ebs-claim
spec:
  accessModes:
  - ReadWriteOncePod
  resources:
    requests:
      storage: 4Gi
---
apiVersion: v1
kind: Pod
metadata:
  name: app
spec:
  containers:
  - name: app
    image: centos
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo $(date -u) >> /data/out.txt; sleep 5; done"]
    volumeMounts:
    - name: persistent-storage
      mountPath: /data
  volumes:
  - name: persistent-storage
    persistentVolumeClaim:
      claimName: ebs-claim
---
apiVersion: v1
kind: Pod
metadata:
  name: app2
spec:
  containers:
  - name: app
    image: centos
    command: ["/bin/sh"]
    args: ["-c", "while true; do echo $(date -u) >> /data/out.txt; sleep 5; done"]
    volumeMounts:
    - name: persistent-storage
      mountPath: /data
  volumes:
  - name: persistent-storage
    persistentVolumeClaim:
      claimName: ebs-claim
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
k version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.30.5
```

</details>

but I believe it's applicable for other versions as well.

#### Cloud provider

Message is the same, regardless of the provider.

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†åœ¨ä½¿ç”¨`ReadWriteOncePod`è®¿é—®æ¨¡å¼çš„PersistentVolumeClaimæ—¶ï¼Œå½“å°è¯•è°ƒåº¦å¤šä¸ªä½¿ç”¨ç›¸åŒPVCçš„Podæ—¶ï¼Œç¬¬äºŒä¸ªPodæ— æ³•è¢«è°ƒåº¦ï¼Œè¿™æ˜¯é¢„æœŸçš„è¡Œä¸ºã€‚ç„¶è€Œï¼Œè°ƒåº¦å™¨è¿”å›çš„é”™è¯¯ä¿¡æ¯å…·æœ‰è¯¯å¯¼æ€§ï¼Œæ˜¾ç¤ºâ€œ5ä¸ªèŠ‚ç‚¹ä¸Šæœ‰Podä½¿ç”¨äº†ç›¸åŒåç§°å’Œ`ReadWriteOncePod`è®¿é—®æ¨¡å¼çš„PersistentVolumeClaimâ€ï¼Œè¿™ä¸å®é™…æƒ…å†µä¸ç¬¦ï¼Œå¯èƒ½ä¼šå¯¼è‡´è¿ç»´äººå‘˜çš„å›°æƒ‘ã€‚

ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¯¥é—®é¢˜å±äºé”™è¯¯ä¿¡æ¯æè¿°ä¸å‡†ç¡®ï¼Œå±äºç”¨æˆ·ä½“éªŒå’Œå¯ç”¨æ€§çš„é—®é¢˜ã€‚å®ƒå¹¶æœªå¯¼è‡´ä»»ä½•å®‰å…¨æ¼æ´æˆ–é£é™©ï¼Œä¾‹å¦‚æƒé™æå‡ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€å‘½ä»¤æ‰§è¡Œç­‰ã€‚å› æ­¤ï¼Œè¿™ä¸ªIssueå¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---


## Issue #129138 NodeResourcesBalancedAllocation cause too many pods scheduled to the same node

- Issue é“¾æ¥ï¼š[#129138](https://github.com/kubernetes/kubernetes/issues/129138)

### Issue å†…å®¹

#### What happened?

NodeResourcesBalancedAllocation will return different score if pod request is empty.

```
I1210 06:42:54.701779       1 resource_allocation.go:70] "Listing internal info for allocatable resources, requested resources and score" pod="tuyaco-k8s/task-worker-9" node="10.20.96.50" resourceAllocationScorer="NodeResourcesBalancedAllocation" allocatableResource=map[cpu:15600 memory:28727588291] requestedResource=map[cpu:14580 memory:24593301504] resourceScore=96
I1210 06:42:54.701793       1 resource_allocation.go:70] "Listing internal info for allocatable resources, requested resources and score" pod="tuyaco-k8s/task-worker-9" node="10.20.96.8" resourceAllocationScorer="NodeResourcesBalancedAllocation" allocatableResource=map[cpu:15600 memory:28727600579] requestedResource=map[cpu:15350 memory:24235737088] resourceScore=92
I1210 06:42:54.701807       1 resource_allocation.go:70] "Listing internal info for allocatable resources, requested resources and score" pod="tuyaco-k8s/task-worker-9" node="10.20.96.9" resourceAllocationScorer="NodeResourcesBalancedAllocation" allocatableResource=map[cpu:15600 memory:28727600579] requestedResource=map[cpu:13480 memory:19474153472] resourceScore=90
```


#### What did you expect to happen?

NodeResourcesBalancedAllocation score should return zero if podRequest is zero. 

https://github.com/kubernetes/kubernetes/blob/a499facee693a1a83daadb82d88f7b51d324ffc5/pkg/scheduler/framework/plugins/noderesources/resource_allocation.go#L85-L114

The code below use `IsPrefixedNativeResource`, which ignore `cpu` and `memory`. 
https://github.com/kubernetes/kubernetes/blob/a499facee693a1a83daadb82d88f7b51d324ffc5/pkg/scheduler/util/utils.go#L158-L162

I think we should use below function.

https://github.com/kubernetes/kubernetes/blob/a499facee693a1a83daadb82d88f7b51d324ffc5/pkg/apis/core/v1/helper/helpers.go#L54-L60

#### How can we reproduce it (as minimally and precisely as possible)?

create pod whcih request is empty and enable NodeResourcesBalancedAllocation.

scheduler config:
```yaml
- name: NodeResourcesBalancedAllocation
  args:
    resources:
      - name: cpu
      - name: memory
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Server Version: v1.24.15
```

</details>


#### Cloud provider

<details>
vanilla
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨Kubernetesè°ƒåº¦å™¨æ’ä»¶NodeResourcesBalancedAllocationæ—¶ï¼Œå¦‚æœPodçš„èµ„æºè¯·æ±‚ï¼ˆresource requestï¼‰ä¸ºç©ºï¼Œè°ƒåº¦å™¨ä¼šè¿”å›ä¸åŒçš„è¯„åˆ†ï¼Œå¯èƒ½å¯¼è‡´è¿‡å¤šçš„Podè¢«è°ƒåº¦åˆ°åŒä¸€ä¸ªèŠ‚ç‚¹ä¸Šã€‚è¿™æ˜¯ç”±äºåœ¨è®¡ç®—èµ„æºåˆ†é…å¹³è¡¡æ—¶ï¼Œå¯¹èµ„æºè¯·æ±‚ä¸ºç©ºçš„Podå¤„ç†ä¸å½“ï¼Œå¯¼è‡´èµ„æºä½¿ç”¨ä¸å¹³è¡¡ã€‚

ä»å®‰å…¨é£é™©çš„è§’åº¦æ¥çœ‹ï¼Œè¿™ä¸ªé—®é¢˜ä¸»è¦æ¶‰åŠåˆ°èµ„æºè°ƒåº¦ç­–ç•¥çš„ä¼˜åŒ–å’ŒèŠ‚ç‚¹èµ„æºçš„æœ‰æ•ˆåˆ©ç”¨ï¼Œå¹¶ä¸æ¶‰åŠæ”»å‡»è€…åˆ©ç”¨æ¼æ´è¿›è¡Œæ”»å‡»æˆ–å¯¼è‡´ç³»ç»Ÿå®‰å…¨æ€§çš„ä¸‹é™ã€‚å³ä½¿å­˜åœ¨èµ„æºä½¿ç”¨ä¸å‡è¡¡çš„æƒ…å†µï¼Œä¹Ÿéœ€è¦å…·å¤‡åˆ›å»ºPodçš„æƒé™æ‰èƒ½æäº¤èµ„æºè¯·æ±‚ä¸ºç©ºçš„Podã€‚åœ¨Kubernetesä¸­ï¼Œåˆ›å»ºPodçš„æ“ä½œé€šå¸¸å—åˆ°RBACæƒé™æ§åˆ¶ï¼Œæ™®é€šç”¨æˆ·æ— æ³•éšæ„åˆ›å»ºPodã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

- **æ ‡å‡†2**ï¼šå¦‚æœè¯¥issueæè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚
- **æ ‡å‡†5**ï¼šåœ¨é£é™©ç±»å‹ä¸ºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»æ—¶ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å¤Ÿå®æ–½è¯¥æ”»å‡»ï¼Œåˆ™è§†æƒ…å†µéœ€è¦é™çº§å¤„ç†ã€‚å½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚

å› æ­¤ï¼Œè¯¥Issueä¸å±äºå®‰å…¨é£é™©é—®é¢˜ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

ç”±äºè¯¥é—®é¢˜ä¸å±äºå®‰å…¨é£é™©ï¼Œå› æ­¤æ— éœ€æä¾›å¤ç°è„šæœ¬ã€‚

---


## Issue #129136 Preemption picks wrong victim node with higher priority pod on it after #128307

- Issue é“¾æ¥ï¼š[#129136](https://github.com/kubernetes/kubernetes/issues/129136)

### Issue å†…å®¹

#### What happened?

related: #128307

After #128307 has been merged, preemption logic picks wrong victim node with higher priority pod on it.
In the following situation, `high` pod on `worker1` not `mid` on `worker2` is evicted when `very-high` pod(Priority=10000) attempts to schedule.

- `worker1`
  - `high` pod(Priority=1000)
  - `low` pod(Priority=0, preempting it will violate PDB)
- `worker2`
    - `mid` pod(Priority=100, preempting it will violate PDB)

#### What did you expect to happen?

According to here, `mid` pod on `worker2` seems to be picked as a victim.

https://github.com/kubernetes/kubernetes/blob/1148e5ee5fd95117db6c2fb92194272df574cc38/pkg/scheduler/framework/preemption/preemption.go#L411-L424

#### How can we reproduce it (as minimally and precisely as possible)?

We can reproduce it by the following steps using kind.

Preparation
<details>

Use kind.
```bash
$ kind version
kind v0.25.0 go1.22.9 linux/amd64
```

Build Node image of v1.31.3.
```bash
$ kind build node-image v1.31.3 --image ndest/node:1.31.3-build
```

Prepare kind cluster config and create cluster.

`kind-config-1.31.3.yaml`
```yaml
kind: Cluster
apiVersion: "kind.x-k8s.io/v1alpha4"
name: kind-v1.31.3
nodes:
- role: control-plane
  image: ndest/node:1.31.3-build
- role: worker
  image: ndest/node:1.31.3-build
- role: worker
  image: ndest/node:1.31.3-build
```

```bash
$ kind create cluster --config kind-config-1.31.3.yaml

$ kubectl get no
NAME                         STATUS   ROLES           AGE   VERSION
kind-v1.31.3-control-plane   Ready    control-plane   33s   v1.31.3
kind-v1.31.3-worker          Ready    <none>          18s   v1.31.3
kind-v1.31.3-worker2         Ready    <none>          18s   v1.31.3
```

Add PriorityClass to DaemonSet/kindnet to prevent it from being preempted.
```bash
$ kubectl -n kube-system patch ds kindnet --patch '{"spec": {"template": {"spec": {"priorityClassName": "system-node-critical"}}}}'
```

Modify `maxPods` of `kubelet` config on `worker` and `worker2` to trigger preemption.
```bash
# worker
$ docker exec -it kind-v1.31.3-worker /bin/bash
root@kind-v1:/# echo "maxPods: 4" >> /var/lib/kubelet/config.yaml
root@kind-v1:/# systemctl restart kubelet
root@kind-v1:/# exit

# worker2
$ docker exec -it kind-v1.31.3-worker2 /bin/bash
root@kind-v1:/# echo "maxPods: 3" >> /var/lib/kubelet/config.yaml
root@kind-v1:/# systemctl restart kubelet
root@kind-v1:/# exit
```

Now, we can schedule 2 pods on `worker` and 1 pod on `worker2`.
```bash
$ k get no -A -o='custom-columns=NAME:.metadata.name,MAXPOD:.status.capacity.pods,VERSION:.status.nodeInfo.kubeletVersion'
NAME                         MAXPOD   VERSION
kind-v1.31.3-control-plane   110      v1.31.3
kind-v1.31.3-worker          4        v1.31.3
kind-v1.31.3-worker2         3        v1.31.3

$ k get po -A -owide | grep -w kind-v1.31.3-worker | wc -l
2
$ k get po -A -owide | grep -w kind-v1.31.3-worker2 | wc -l
2
```

</details>

Create `PriorityClass`, `PodDisruptionBudget`, and `Pod`.
Applying these manifests, we can see the following situation.

<details>

`high-priority.yaml`
```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000
globalDefault: false
description: "This priority class should be used for high priority pods."
```

`high.yaml`
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: high
  labels:
    app: high
spec:
  containers:
  - name: nginx
    image: nginx
  priorityClassName: high-priority
  nodeSelector:
    kubernetes.io/hostname: kind-v1.31.3-worker
```

`mid-priority.yaml`
```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: mid-priority
value: 100
globalDefault: false
description: "This priority class should be used for mid priority pods."
```

`mid.yaml`
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: mid
  labels:
    app: mid
spec:
  containers:
  - name: nginx
    image: nginx
  priorityClassName: mid-priority
  nodeSelector:
    kubernetes.io/hostname: kind-v1.31.3-worker2
```

`low-priority.yaml`
```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: low-priority
value: 0
globalDefault: false
description: "This priority class should be used for low priority pods."
```

`low.yaml`
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: low
  labels:
    app: low
spec:
  containers:
  - name: nginx
    image: nginx
  priorityClassName: low-priority
  nodeSelector:
    kubernetes.io/hostname: kind-v1.31.3-worker
```


`mid-pdb.yaml`
```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: mid-pdb
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: mid
```

`low-pdb.yaml`
```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: mid-pdb
spec:
  minAvailable: 1
  selector:
    matchLabels:
      app: mid
```

`very-high-priority.yaml`
```yaml
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: very-high-priority
value: 10000
globalDefault: false
description: "This priority class should be used for very high priority pods."
```

</details>

`worker` and `worker2` habe reached their `maxPods` limit and evicting `low` or `mid` pod violates PDB.
```bash
$ k get po -o='custom-columns=NAME:.metadata.name,STATUS:.status.phase,PRIORITY:.spec.priority,NODE:.spec.nodeName'
NAME   STATUS    PRIORITY   NODE
high   Running   1000       kind-v1.31.3-worker
low    Running   0          kind-v1.31.3-worker
mid    Running   100        kind-v1.31.3-worker2

$ k get priorityclasses
NAME                      VALUE        GLOBAL-DEFAULT   AGE
high-priority             1000         false            26s
low-priority              0            false            26s
mid-priority              100          false            26s
system-cluster-critical   2000000000   false            17m
system-node-critical      2000001000   false            17m
very-high-priority        10000        false            26s

$ k get pdb
NAME      MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
low-pdb   1               N/A               0                     47s
mid-pdb   1               N/A               0                     47s

$ k get po -A -owide | grep -w kind-v1.31.3-worker | wc -l
4
ubuntu-user@ubuntu-server01 ~/work/kubernetes/preemption $ k get po -A -owide | grep -w kind-v1.31.3-worker2 | wc -l
3
```


Now, attempt to schedule `very-high` pod.

`very-high.yaml`
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: very-high
  labels:
    app: very-high
spec:
  containers:
  - name: nginx
    image: nginx
  priorityClassName: very-high-priority
```

```bash
$ k apply -f very-high.yaml
```

We can see that `high` pod on `worker` is evicted instead of `mid` pod on `worker2`.
```bash
$ k get po -owide -w
NAME   READY   STATUS    RESTARTS   AGE   IP           NODE                   NOMINATED NODE   READINESS GATES
high   1/1     Running   0          6m    10.244.1.2   kind-v1.31.3-worker    <none>           <none>
low    1/1     Running   0          6m    10.244.1.3   kind-v1.31.3-worker    <none>           <none>
mid    1/1     Running   0          6m    10.244.2.2   kind-v1.31.3-worker2   <none>           <none>


very-high   0/1     Pending   0          0s    <none>       <none>                 <none>           <none>
high        1/1     Running   0          6m9s   10.244.1.2   kind-v1.31.3-worker    <none>           <none>
high        1/1     Terminating   0          6m9s   10.244.1.2   kind-v1.31.3-worker    <none>           <none>
very-high   0/1     Pending       0          0s     <none>       <none>                 kind-v1.31.3-worker   <none>
high        1/1     Terminating   0          6m9s   10.244.1.2   kind-v1.31.3-worker    <none>                <none>
high        0/1     Completed     0          6m9s   10.244.1.2   kind-v1.31.3-worker    <none>                <none>
high        0/1     Completed     0          6m10s   10.244.1.2   kind-v1.31.3-worker    <none>                <none>
high        0/1     Completed     0          6m10s   10.244.1.2   kind-v1.31.3-worker    <none>                <none>
very-high   0/1     Pending       0          2s      <none>       kind-v1.31.3-worker    kind-v1.31.3-worker   <none>
very-high   0/1     ContainerCreating   0          2s      <none>       kind-v1.31.3-worker    <none>                <none>
very-high   1/1     Running             0          3s      10.244.1.4   kind-v1.31.3-worker    <none>                <none>

$ k get po -owide
NAME        READY   STATUS    RESTARTS   AGE     IP           NODE                   NOMINATED NODE   READINESS GATES
low         1/1     Running   0          7m23s   10.244.1.3   kind-v1.31.3-worker    <none>           <none>
mid         1/1     Running   0          7m23s   10.244.2.2   kind-v1.31.3-worker2   <none>           <none>
very-high   1/1     Running   0          74s     10.244.1.4   kind-v1.31.3-worker    <none>           <none>
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.31.3
```

</details>

#### Cloud provider

<details>
none
</details>

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
$ kind version
kind v0.25.0 go1.22.9 linux/amd64

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetesé›†ç¾¤ä¸­ï¼Œè°ƒåº¦å™¨çš„é¢„å é€»è¾‘åœ¨ç‰¹å®šæƒ…å†µä¸‹é€‰æ‹©äº†é”™è¯¯çš„å¾…é©±é€Podï¼Œå¯¼è‡´å…·æœ‰æ›´é«˜ä¼˜å…ˆçº§çš„Podè¢«é©±é€ï¼Œè€Œä¸æ˜¯é¢„æœŸçš„æ›´ä½ä¼˜å…ˆçº§Podã€‚å…·ä½“è€Œè¨€ï¼Œå½“ä¸€ä¸ªéå¸¸é«˜ä¼˜å…ˆçº§ï¼ˆPriority=10000ï¼‰çš„Podå°è¯•è°ƒåº¦æ—¶ï¼Œé¢„å é€»è¾‘é”™è¯¯åœ°é€‰æ‹©äº†åœ¨`worker1`èŠ‚ç‚¹ä¸Šçš„`high` Podï¼ˆPriority=1000ï¼‰ä½œä¸ºé©±é€ç›®æ ‡ï¼Œè€Œä¸æ˜¯åœ¨`worker2`èŠ‚ç‚¹ä¸Šçš„`mid` Podï¼ˆPriority=100ï¼‰ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæˆ‘ä»¬éœ€è¦è¯„ä¼°è¯¥é—®é¢˜æ˜¯å¦æ„æˆå®‰å…¨é£é™©ï¼Œä»¥åŠå…¶é£é™©ç­‰çº§ã€‚

1. **æƒé™è¦æ±‚**ï¼šä¸ºäº†è§¦å‘è¯¥é—®é¢˜ï¼Œæ”»å‡»è€…éœ€è¦èƒ½å¤Ÿåˆ›å»ºå…·æœ‰é«˜ä¼˜å…ˆçº§çš„Podï¼ˆPriority=10000ï¼‰ã€‚åœ¨Kubernetesä¸­ï¼Œè®¾ç½®Podçš„ä¼˜å…ˆçº§éœ€è¦ä½¿ç”¨å·²å­˜åœ¨çš„PriorityClassï¼Œæˆ–è€…åˆ›å»ºæ–°çš„PriorityClassã€‚ç„¶è€Œï¼Œåˆ›å»ºæˆ–ä¿®æ”¹PriorityClassé€šå¸¸éœ€è¦é›†ç¾¤ç®¡ç†å‘˜æƒé™ï¼Œæ™®é€šç”¨æˆ·æ— æ³•ä»»æ„è®¾ç½®é«˜ä¼˜å…ˆçº§ã€‚å› æ­¤ï¼Œæ”»å‡»è€…éœ€è¦è¾ƒé«˜æƒé™æ‰èƒ½åˆ©ç”¨è¯¥é—®é¢˜ã€‚

2. **å¤šç§Ÿæˆ·å½±å“**ï¼šå¦‚æœæ”»å‡»è€…èƒ½å¤Ÿåˆ›å»ºé«˜ä¼˜å…ˆçº§Podï¼Œå¹¶å¯¼è‡´å…¶ä»–ç”¨æˆ·çš„é«˜ä¼˜å…ˆçº§Podè¢«é©±é€ï¼Œå¯èƒ½ä¼šå¯¹å…¶ä»–ç”¨æˆ·é€ æˆå½±å“ã€‚ç„¶è€Œï¼Œç”±äºéœ€è¦é«˜æƒé™æ‰èƒ½åˆ›å»ºé«˜ä¼˜å…ˆçº§çš„PriorityClassï¼Œä¸”åœ¨å®‰å…¨é…ç½®çš„å¤šç§Ÿæˆ·ç¯å¢ƒä¸­ï¼Œç”¨æˆ·æƒé™é€šå¸¸å—åˆ°ä¸¥æ ¼æ§åˆ¶ï¼Œå› æ­¤è¯¥é£é™©å¯æ§ã€‚

3. **æ‹’ç»æœåŠ¡é£é™©**ï¼šæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬ï¼ˆ5ï¼‰æ¡ï¼Œåœ¨æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»çš„æƒ…å†µä¸‹ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å®æ–½æ”»å‡»ï¼Œåˆ™åº”é€‚å½“é™ä½é£é™©è¯„çº§ã€‚å½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueæè¿°çš„é—®é¢˜æ˜¯è°ƒåº¦å™¨é€»è¾‘ä¸Šçš„é”™è¯¯ï¼Œå¯èƒ½ä¼šå¯¼è‡´èµ„æºè°ƒåº¦çš„ä¸åˆç†ï¼Œä½†å¹¶æœªæ¶‰åŠå®‰å…¨æ¼æ´ã€‚æ”»å‡»è€…éœ€è¦é«˜æƒé™æ‰èƒ½åˆ©ç”¨è¯¥é—®é¢˜ï¼Œä¸”å…¶å½±å“èŒƒå›´æœ‰é™ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

æ— 

---


## Issue #129130 Unrealistic high cpu usage in metrics report after upgrading kubelet to 1.31 during pod restart

- Issue é“¾æ¥ï¼š[#129130](https://github.com/kubernetes/kubernetes/issues/129130)

### Issue å†…å®¹

#### What happened?

We are performing the kubenetes cluster upgrade from 1.29 to 1.31

First we upgrade cluster api server to 1.31, then we upgrade all the nodes to kubelet 1.31
metrics-server version: 0.7.1
kube-state-metric version: 2.13.0
We found lots of our metric has a very high cpu usage and cpu/memory request during the pod restart phase

For example:
![image](https://github.com/user-attachments/assets/678f681b-b066-4adc-93d3-dc7e4b3f34a4)
Regularly, cpu usage should be under 100, but it's like 100M to 700M during the restart

Also, the allocatable resource:
![image](https://github.com/user-attachments/assets/84290587-350f-458c-8ced-78c8dbd709d8)




#### What did you expect to happen?

Correct metric

#### How can we reproduce it (as minimally and precisely as possible)?

upgrade kubelet from 1.29 to 1.31 not sure if 1.30 to 1.31 has this issue

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: v1.29.8
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.2-eks-dfa7665
```

</details>


#### Cloud provider

<details>
AWS EKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥é—®é¢˜æè¿°äº†åœ¨å°† kubelet ä» 1.29 å‡çº§åˆ° 1.31 åï¼Œpod é‡å¯æœŸé—´çš„æŒ‡æ ‡æŠ¥å‘Šä¸­å‡ºç°äº†ä¸åˆç†çš„é«˜ CPU ä½¿ç”¨ç‡å’Œ CPU/å†…å­˜è¯·æ±‚å€¼ã€‚ä¾‹å¦‚ï¼Œé€šå¸¸ CPU ä½¿ç”¨ç‡åº”ä½äº 100ï¼Œä½†åœ¨é‡å¯æœŸé—´å´è¾¾åˆ°äº† 100M åˆ° 700Mã€‚

ä»æè¿°æ¥çœ‹ï¼Œè¿™æ˜¯ç”±äºå‡çº§ kubelet åï¼Œå¯¼è‡´æŒ‡æ ‡é‡‡é›†å‡ºç°å¼‚å¸¸ï¼Œmetrics-server æˆ– kube-state-metric æ”¶åˆ°äº†é”™è¯¯çš„æ•°æ®ï¼Œå¯¼è‡´æŒ‡æ ‡æ˜¾ç¤ºå¼‚å¸¸é«˜çš„èµ„æºä½¿ç”¨æƒ…å†µã€‚

è¿™ä¸€é—®é¢˜ä¸»è¦æ˜¯å…³äºåœ¨å‡çº§è¿‡ç¨‹ä¸­å‡ºç°çš„æ€§èƒ½æŒ‡æ ‡ä¸å‡†ç¡®çš„é—®é¢˜ï¼Œå±äºåŠŸèƒ½æ€§ç¼ºé™·æˆ–å…¼å®¹æ€§é—®é¢˜ï¼Œå¹¶ä¸æ¶‰åŠä»»ä½•æ½œåœ¨çš„å®‰å…¨é£é™©ï¼Œä¹Ÿæ²¡æœ‰æåˆ°å¯èƒ½å¯¼è‡´çš„å®‰å…¨åæœã€‚

---


## Issue #129122 draplugin.PublishResources() only supports enumerating resources in a single slice

- Issue é“¾æ¥ï¼š[#129122](https://github.com/kubernetes/kubernetes/issues/129122)

### Issue å†…å®¹

#### What happened?

I wanted to publish multiple resource slices within the same pool, but there was no easy way to do this with the recommended helper library.

#### What did you expect to happen?

I expected the API to `draplugin.PublishResources()` to take a list of ResourceSlices, similar to the call to `resourceslice.Update()` that it wraps. 

#### How can we reproduce it (as minimally and precisely as possible)?

Examine the code here:
https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/dynamic-resource-allocation/kubeletplugin/draplugin.go#L427-L459
https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/dynamic-resource-allocation/kubeletplugin/draplugin.go#L410

#### Anything else we need to know?

/wg device-management

#### Kubernetes version

`v1.32.0`

#### Cloud provider

None

#### OS version

Irrelevant

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨`draplugin.PublishResources()`æ–¹æ³•æ—¶ï¼Œåªèƒ½å‘å¸ƒå•ä¸ªèµ„æºç‰‡æ®µï¼ˆResourceSliceï¼‰ï¼Œæ— æ³•æ–¹ä¾¿åœ°åœ¨åŒä¸€èµ„æºæ± ä¸­å‘å¸ƒå¤šä¸ªèµ„æºç‰‡æ®µã€‚æé—®è€…å¸Œæœ›è¯¥APIèƒ½å¤Ÿæ¥æ”¶ä¸€ä¸ªResourceSliceåˆ—è¡¨ï¼Œç±»ä¼¼äºå®ƒæ‰€å°è£…çš„`resourceslice.Update()`è°ƒç”¨ã€‚

è¿™ä¸ªé—®é¢˜å±äºåŠŸèƒ½æ”¹è¿›æˆ–æ–°ç‰¹æ€§è¯·æ±‚ï¼Œæ¶‰åŠAPIçš„è®¾è®¡å’Œå®ç°ï¼Œå¹¶ä¸æ¶‰åŠä»»ä½•å®‰å…¨é£é™©ã€‚å› æ­¤ï¼Œæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¯¥Issueéå®‰å…¨é—®é¢˜ã€‚

---


## Issue #129115 can't find way to check streams is closed 

- Issue é“¾æ¥ï¼š[#129115](https://github.com/kubernetes/kubernetes/issues/129115)

### Issue å†…å®¹

#### What happened?

https://github.com/kubernetes/kubernetes/blame/v1.31.3/staging/src/k8s.io/kubelet/pkg/cri/streaming/remotecommand/attach.go#L43

#### What did you expect to happen?

can find way to check streams is closed

#### How can we reproduce it (as minimally and precisely as possible)?

no method

#### Anything else we need to know?

no method

#### Kubernetes version

no method

#### Cloud provider

null

#### OS version

null

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueçš„æ ‡é¢˜ä¸ºâ€œcan't find way to check streams is closedâ€ï¼Œå†…å®¹æåˆ°åœ¨Kubernetesä»£ç åº“ä¸­ `staging/src/k8s.io/kubelet/pkg/cri/streaming/remotecommand/attach.go` æ–‡ä»¶çš„ç¬¬43è¡Œï¼Œæ— æ³•æ‰¾åˆ°æ£€æŸ¥æµï¼ˆstreamsï¼‰æ˜¯å¦å…³é—­çš„æ–¹æ³•ã€‚æäº¤è€…å¸Œæœ›èƒ½å¤Ÿæ‰¾åˆ°ä¸€ç§æ–¹æ³•æ¥æ£€æµ‹æµçš„å…³é—­çŠ¶æ€ã€‚

ä»é—®é¢˜æè¿°æ¥çœ‹ï¼Œè¿™å±äºåŠŸèƒ½ä¸Šçš„æ”¹è¿›éœ€æ±‚æˆ–ç¼ºé™·æŠ¥å‘Šï¼Œæ¶‰åŠåˆ°ä»£ç çš„å¯ç”¨æ€§å’Œå¥å£®æ€§ï¼Œä½†å¹¶ä¸ç›´æ¥æ¶‰åŠå®‰å…¨é—®é¢˜ã€‚æ— æ³•æ£€æµ‹æµæ˜¯å¦å…³é—­ï¼Œå¯èƒ½ä¼šå¯¼è‡´èµ„æºæœªåŠæ—¶é‡Šæ”¾ï¼Œä»è€Œå½±å“ç³»ç»Ÿæ€§èƒ½ï¼Œä½†è¿™å±äºå¯é æ€§é—®é¢˜ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. è¯¥Issueæè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œåº”å½“åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

---


## Issue #129104 CoreDNS service is running but IPs are not assigned

- Issue é“¾æ¥ï¼š[#129104](https://github.com/kubernetes/kubernetes/issues/129104)

### Issue å†…å®¹

/sig Network
#### What happened?

I started a new cluster with 1 control plane node and 1 worker node. None of the default pods are getting IPs and it seems like there is no associated node with core dns pods. Some logs are as below:

$ kubectl get services -A
NAMESPACE     NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
default       kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP                  9m21s
kube-system   kube-dns     ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   8m48s

$ kubectl get pods -o wide -A
NAMESPACE     NAME                                                       READY   STATUS    RESTARTS   AGE     IP       NODE                               NOMINATED NODE   READINESS GATES
kube-system   coredns-55cb58b774-h4vbk                                   0/1     Pending   0          3m59s   <none>   <none>                             <none>           <none>
kube-system   coredns-55cb58b774-zfxmn                                   0/1     Pending   0          3m59s   <none>   <none>                             <none>           <none>
kube-system   etcd-bsocialtoday-control-plane-s7n99                      1/1     Running   0          3m58s   <none>   bsocialtoday-control-plane-s7n99   <none>           <none>
kube-system   kube-apiserver-bsocialtoday-control-plane-s7n99            1/1     Running   0          3m58s   <none>   bsocialtoday-control-plane-s7n99   <none>           <none>
kube-system   kube-controller-manager-bsocialtoday-control-plane-s7n99   1/1     Running   0          3m58s   <none>   bsocialtoday-control-plane-s7n99   <none>           <none>
kube-system   kube-proxy-64tgt                                           1/1     Running   0          81s     <none>   bsocialtoday-md-0-dwwnv-8r6sj      <none>           <none>
kube-system   kube-proxy-q7l7s                                           1/1     Running   0          3m59s   <none>   bsocialtoday-control-plane-s7n99   <none>           <none>
kube-system   kube-scheduler-bsocialtoday-control-plane-s7n99            1/1     Running   0          3m58s   <none>   bsocialtoday-control-plane-s7n99   <none>           <none>

#### What did you expect to happen?

I expected core dns to work fine with default instructions as followed in [here](https://syself.com/docs/caph/getting-started/quickstart/creating-a-workload-cluster).

#### How can we reproduce it (as minimally and precisely as possible)?

Follow instructions [here](https://syself.com/docs/caph/getting-started/quickstart/creating-a-workload-cluster).

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
1.30.7
```

</details>


#### Cloud provider

<details>
Hetzner
</details>


#### OS version

Ubuntu 22.04

#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetesé›†ç¾¤ä¸­ï¼ŒCoreDNSæœåŠ¡æ­£åœ¨è¿è¡Œï¼Œä½†Podsæœªè¢«åˆ†é…IPåœ°å€ï¼Œä¸”æœªå…³è”åˆ°ä»»ä½•èŠ‚ç‚¹ã€‚è¿™å¯¼è‡´CoreDNSç­‰Podså¤„äºPendingçŠ¶æ€ã€‚Issueä¸­æä¾›çš„æ—¥å¿—æ˜¾ç¤ºï¼ŒCoreDNS Podsçš„`NODE`å­—æ®µä¸º`<none>`ï¼Œ`IP`å­—æ®µä¸º`<none>`ï¼Œè¿™é€šå¸¸æ˜¯ç”±äºç½‘ç»œæ’ä»¶ï¼ˆå¦‚CNIæ’ä»¶ï¼‰æœªæ­£ç¡®å®‰è£…æˆ–é…ç½®æ‰€å¯¼è‡´çš„ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠ**ï¼šè¯¥Issueåæ˜ çš„æ˜¯Kubernetesé›†ç¾¤çš„é…ç½®é—®é¢˜ï¼Œå°¤å…¶æ˜¯ç½‘ç»œé…ç½®æ–¹é¢çš„é—®é¢˜ï¼Œå¦‚CNIæ’ä»¶æœªæ­£ç¡®å®‰è£…ã€‚è¿™å±äºåŠŸèƒ½æ€§é”™è¯¯æˆ–é…ç½®é”™è¯¯ã€‚

2. **Issueæäº¤è€…çš„é—®é¢˜**ï¼šIssueä¸­æ²¡æœ‰æåŠä»»ä½•å®‰å…¨æ¼æ´ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²æˆ–å…¶ä»–å¯èƒ½å¯¼è‡´å®‰å…¨é£é™©çš„å› ç´ ã€‚

3. **ä¸æ¶‰åŠå®‰å…¨é£é™©**ï¼šæ²¡æœ‰è¿¹è±¡è¡¨æ˜è¯¥é—®é¢˜ä¼šå¯¼è‡´æƒé™æå‡ã€ä»»æ„ä»£ç æ‰§è¡Œã€æ‹’ç»æœåŠ¡æ”»å‡»ç­‰å®‰å…¨é£é™©ã€‚

å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œé£é™©è¯„çº§åˆ¤æ–­ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

---


## Issue #129102 Loss of Kind and APIVersion in Errors from Request.Stream

- Issue é“¾æ¥ï¼š[#129102](https://github.com/kubernetes/kubernetes/issues/129102)

### Issue å†…å®¹

#### What happened?

When invoking the func (r *Request) Stream(ctx context.Context) (io.ReadCloser, error) function in the Kubernetes client-go library, the error returned by the apiserver originally includes fields such as kind and apiVersion (e.g., "kind":"Status","apiVersion":"v1"). However, after passing through the rest-clientâ€™s processing logic, these fields (Kind and APIVersion) are stripped from the error object.

This leads to the following issues:

Users lose structured metadata (Kind, APIVersion) from the original server response, which is essential for identifying and categorizing the error type.
It becomes challenging to programmatically distinguish between different error types or versions without additional error handling logic.


#### What did you expect to happen?

same as apiserver

#### How can we reproduce it (as minimally and precisely as possible)?

Make a request using the Kubernetes client-go library that results in an error, e.g., an invalid resource request.
Observe the response from the apiserver (e.g., using a network debugging tool like kubectl or other HTTP debugging methods) to verify that the Kind and APIVersion fields are present in the error payload.
Check the error returned by the Request.Stream function to observe the absence of Kind and APIVersion.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨ Kubernetes client-go åº“çš„ `Request.Stream` å‡½æ•°æ—¶ï¼Œæ¥è‡ª apiserver çš„é”™è¯¯å“åº”ä¸­åŸæœ¬åŒ…å«çš„ `kind` å’Œ `apiVersion` å­—æ®µåœ¨ç»è¿‡ rest-client çš„å¤„ç†åè¢«å»é™¤äº†ã€‚è¿™æ ·å¯¼è‡´ç”¨æˆ·æ— æ³•è·å–åŸå§‹æœåŠ¡å™¨å“åº”ä¸­çš„ç»“æ„åŒ–å…ƒæ•°æ®ï¼Œå½±å“å¯¹é”™è¯¯ç±»å‹çš„è¯†åˆ«å’Œåˆ†ç±»ï¼Œå¢åŠ äº†å¤„ç†é”™è¯¯çš„éš¾åº¦ã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼š

- **ä¿¡æ¯å®Œæ•´æ€§**ï¼šé”™è¯¯ä¿¡æ¯çš„å®Œæ•´æ€§å—åˆ°å½±å“ï¼Œä½†è¿™ä¸»è¦æ˜¯å¯¹å¼€å‘å’Œè°ƒè¯•é€ æˆä¸ä¾¿ï¼Œä¸ä¼šç›´æ¥å¯¼è‡´å®‰å…¨é£é™©ã€‚
- **æ”»å‡»é¢**ï¼šæ”»å‡»è€…æ— æ³•é€šè¿‡è¯¥é—®é¢˜è·å–æ•æ„Ÿä¿¡æ¯ã€æå‡æƒé™æˆ–æ‰§è¡Œæœªæˆæƒæ“ä½œã€‚
- **å½±å“èŒƒå›´**ï¼šè¯¥é—®é¢˜å½±å“çš„æ˜¯å®¢æˆ·ç«¯å¯¹é”™è¯¯çš„å¤„ç†æ–¹å¼ï¼Œå¯¹ç³»ç»Ÿçš„å®‰å…¨æ€§æ²¡æœ‰å®è´¨æ€§å½±å“ã€‚

æ ¹æ®ä»¥ä¸Šåˆ†æï¼Œè¯¥é—®é¢˜ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---


## Issue #129097 Namespace deletion does not trigger container lifecycle hooks

- Issue é“¾æ¥ï¼š[#129097](https://github.com/kubernetes/kubernetes/issues/129097)

### Issue å†…å®¹

#### What happened?

I want to ensure that certain commands are run on graceful deletion of a pod, so I have set up a container lifecycle prestop hook to do so. Deleting the pod does trigger the prestop hook as expected; however, when deleting the namespace that the pod is in, I would expect this to also be a graceful shutdown event, which should trigger the prestop hook, but this does not happen. In addition, helm uninstalling our helm chart with the relevant statefulset has the same issue. See the test example below where we have setup RBAC for kubectl access within the container:

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: <>
  namespace: <>
  labels:
    component: <>
    app: <>
spec:
  ...
  template:
    metadata:
      labels:
    spec:
      containers:
        - name: testContainer
          image: <>
          lifecycle:
           preStop:
             exec:
               command:
                  - /bin/bash
                  - -c
                  - "kubectl label cm test-configmap -n test-prestop-hook testlabel=8 --overwrite"
             
```

#### What did you expect to happen?

Prestop hook should be triggered by namespace deletion. 

#### How can we reproduce it (as minimally and precisely as possible)?

1. Bring up a pod with a container that has a prestop hook configured
2. Exec into the pod and label the ConfigMap to see that it works (no RBAC or image issues)
3. Delete the namespace that the pod is in (expected to trigger the prestop hook)
4. Wait 30 seconds and see that the prestop hook did not run, configmap label did not change to `testlabel: "8"`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.28.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.29.10-eks-7f9249a
```

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

NA

</details>


#### Install tools

<details>
Using helm to install the helm chart with statefulset:

$ helm version
version.BuildInfo{Version:"v3.12.3", GitCommit:"3a31588ad33fe3b89af5a2a54ee1d25bfe6eaa5e", GitTreeState:"clean", GoVersion:"go1.20.7"}

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetesç¯å¢ƒä¸­ï¼Œå½“åˆ é™¤Podæ‰€åœ¨çš„å‘½åç©ºé—´æ—¶ï¼Œå®¹å™¨çš„ç”Ÿå‘½å‘¨æœŸé’©å­ï¼ˆpreStopï¼‰æœªè¢«è§¦å‘çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå•ç‹¬åˆ é™¤Podä¼šå¦‚é¢„æœŸè§¦å‘preStopé’©å­ï¼Œä½†åˆ é™¤å‘½åç©ºé—´æˆ–é€šè¿‡Helmå¸è½½StatefulSetæ—¶ï¼ŒpreStopé’©å­æœªè¢«æ‰§è¡Œã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼š

1. **è¡Œä¸ºæè¿°**ï¼šå®¹å™¨çš„preStopé’©å­ç”¨äºåœ¨å®¹å™¨ç»ˆæ­¢å‰æ‰§è¡Œç‰¹å®šæ“ä½œã€‚æœªè§¦å‘preStopé’©å­å¯èƒ½å¯¼è‡´ä¸€äº›æ¸…ç†æˆ–çŠ¶æ€æ›´æ–°æ“ä½œæœªæ‰§è¡Œã€‚

2. **æ½œåœ¨å½±å“**ï¼šæœªæ‰§è¡ŒpreStopé’©å­å¯èƒ½ä¼šå¯¼è‡´èµ„æºæœªåŠæ—¶é‡Šæ”¾ã€æœªå®Œæˆçš„äº‹åŠ¡æˆ–æ—¥å¿—æœªè®°å½•ç­‰é—®é¢˜ï¼Œä½†è¿™äº›éƒ½å±äºåº”ç”¨ç¨‹åºçš„ç”Ÿå‘½å‘¨æœŸç®¡ç†é—®é¢˜ã€‚

3. **å®‰å…¨æ€§è€ƒè™‘**ï¼šè¯¥é—®é¢˜ä¸ä¼šå¯¼è‡´æœªç»æˆæƒçš„è®¿é—®ã€æƒé™æå‡ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ç­‰å®‰å…¨é£é™©ã€‚æ”»å‡»è€…æ— æ³•åˆ©ç”¨è¯¥è¡Œä¸ºè¿›è¡Œæ¶æ„æ“ä½œï¼Œå› ä¸ºè¯¥è¿‡ç¨‹éœ€è¦å¯¹å‘½åç©ºé—´æˆ–èµ„æºå…·æœ‰åˆ é™¤æƒé™ï¼Œå±äºé«˜æƒé™æ“ä½œã€‚

å› æ­¤ï¼Œè¯¥Issueæè¿°çš„æ˜¯Kubernetesåœ¨ç‰¹å®šæƒ…å†µä¸‹çš„é¢„æœŸè¡Œä¸ºä¸å®é™…è¡Œä¸ºä¸ä¸€è‡´çš„é—®é¢˜ï¼Œå±äºåŠŸèƒ½æ€§ç¼ºé™·æˆ–æ”¹è¿›éœ€æ±‚ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚

---


## Issue #129093 Link for cluster bootstrap is broken

- Issue é“¾æ¥ï¼š[#129093](https://github.com/kubernetes/kubernetes/issues/129093)

### Issue å†…å®¹

#### What happened?

Link for Cluster-bootstrap is broken in https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/cluster-bootstrap/README.md

#### What did you expect to happen?

Link should be working

#### How can we reproduce it (as minimally and precisely as possible)?

clicking on link

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.31.0



```
</details>


#### Cloud provider

<details>
baremetal
</details>


#### OS version

<details>

```console
# On Linux:
  $ cat /etc/os-release
# paste output here
PRETTY_NAME="Ubuntu 24.04 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian

$ uname -a
# paste output here
Linux XXXXXXXXXXXXX 6.8.0-45-generic #45-Ubuntu SMP PREEMPT_DYNAMIC Fri Aug 30 12:02:04 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®Issueçš„æè¿°ï¼ŒæŠ¥å‘Šè€…åœ¨Kubernetesé¡¹ç›®çš„READMEæ–‡æ¡£ä¸­å‘ç°äº†ä¸€ä¸ªå¤±æ•ˆçš„é“¾æ¥ï¼ˆbroken linkï¼‰ã€‚å…·ä½“åœ°è¯´ï¼Œé“¾æ¥æŒ‡å‘`cluster-bootstrap`ï¼Œä½†æ— æ³•æ­£å¸¸è®¿é—®ã€‚è¿™æ˜¯ä¸€ä¸ªæ–‡æ¡£ç»´æŠ¤é—®é¢˜ï¼Œä¸é¡¹ç›®çš„æºä»£ç æˆ–éƒ¨ç½²æ— å…³ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. **é£é™©è¯„çº§åˆ†ä¸ºä¸‰ç±»ï¼Œä¸æ¶‰åŠï¼Œä½é£é™©å’Œé«˜é£é™©**ã€‚
2. **å¦‚æœè¯¥Issueæè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠ**ã€‚

å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠä»»ä½•å®‰å…¨é£é™©ï¼Œåªæ˜¯ä¸€ä¸ªæ–‡æ¡£é“¾æ¥é”™è¯¯çš„é—®é¢˜ã€‚

---


## Issue #129086 flowcontrol: maxCL is unreachable

- Issue é“¾æ¥ï¼š[#129086](https://github.com/kubernetes/kubernetes/issues/129086)

### Issue å†…å®¹

#### What happened?

With the current implementation of `maxCL`, it is possible that the value reported by `apiserver_flowcontrol_upper_limit_seats` exceeds the total concurrency limit. Since the concurrency limit of a given priority level is bound by the total concurrency limit, this effectively means the value reported by the metric is too loose.

Moreover, in practice a priority level of type `Limited` cannot borrow more than the total number of seats that other priority levels are configured to lend. This means that the total concurrency limit is generally speaking a loose upper bound that can be restricted even further.

Here below are the values of `apiserver_flowcontrol_upper_limit_seats` for an apiserver running with the default total concurrency limit of 600.

```bash
$ kubectl get --raw /metrics | grep apiserver_flowcontrol_upper_limit_seats
# HELP apiserver_flowcontrol_upper_limit_seats [ALPHA] Configured upper bound on number of execution seats available to each priority level
# TYPE apiserver_flowcontrol_upper_limit_seats gauge
apiserver_flowcontrol_upper_limit_seats{priority_level="catch-all"} 613
apiserver_flowcontrol_upper_limit_seats{priority_level="exempt"} 600
apiserver_flowcontrol_upper_limit_seats{priority_level="global-default"} 649
apiserver_flowcontrol_upper_limit_seats{priority_level="leader-election"} 625
apiserver_flowcontrol_upper_limit_seats{priority_level="node-high"} 698
apiserver_flowcontrol_upper_limit_seats{priority_level="system"} 674
apiserver_flowcontrol_upper_limit_seats{priority_level="workload-high"} 698
apiserver_flowcontrol_upper_limit_seats{priority_level="workload-low"} 845
```

#### What did you expect to happen?

With the default priority levels and default values, that is total concurrency limit of 600 and 343 borrowable seats in total, we expect the following upper limits

```console
$ kubectl get --raw /metrics | grep apiserver_flowcontrol_upper_limit_seats
# HELP apiserver_flowcontrol_upper_limit_seats [ALPHA] Configured upper bound on number of execution seats available to each priority level
# TYPE apiserver_flowcontrol_upper_limit_seats gauge
apiserver_flowcontrol_upper_limit_seats{priority_level="catch-all"} 356        # = lower limit ( 13 ) + borrowable ( 343 )
apiserver_flowcontrol_upper_limit_seats{priority_level="exempt"} 600
apiserver_flowcontrol_upper_limit_seats{priority_level="global-default"} 367   # = lower limit ( 24 ) + borrowable ( 343 )
apiserver_flowcontrol_upper_limit_seats{priority_level="leader-election"} 368  # = lower limit ( 25 ) + borrowable ( 343 )
apiserver_flowcontrol_upper_limit_seats{priority_level="node-high"} 416        # = lower limit ( 73 ) + borrowable ( 343 )
apiserver_flowcontrol_upper_limit_seats{priority_level="system"} 393           # = lower limit ( 50 ) + borrowable ( 343 )
apiserver_flowcontrol_upper_limit_seats{priority_level="workload-high"} 392    # = lower limit ( 49 ) + borrowable ( 343 )
apiserver_flowcontrol_upper_limit_seats{priority_level="workload-low"} 367     # = lower limit ( 24 ) + borrowable ( 343 )
```

#### How can we reproduce it (as minimally and precisely as possible)?

This behavior can be reproduced with a kind cluster using the default configuration
The total number of lendable seats sums up to 343, this is the maximum number of seats any limited priority level can borrow

```console
$ kind create cluster
$ kubectl get --raw /metrics | grep apiserver_flowcontrol_nominal_limit_seats
# HELP apiserver_flowcontrol_nominal_limit_seats [BETA] Nominal number of execution seats configured for each priority level
# TYPE apiserver_flowcontrol_nominal_limit_seats gauge
apiserver_flowcontrol_nominal_limit_seats{priority_level="catch-all"} 13        #  0% lendable =   0 lendable seats
apiserver_flowcontrol_nominal_limit_seats{priority_level="exempt"} 0
apiserver_flowcontrol_nominal_limit_seats{priority_level="global-default"} 49   # 50% lendable =  25 lendable seats
apiserver_flowcontrol_nominal_limit_seats{priority_level="leader-election"} 25  #  0% lendable =   0 lendable seats
apiserver_flowcontrol_nominal_limit_seats{priority_level="node-high"} 98        # 25% lendable =  25 lendable seats
apiserver_flowcontrol_nominal_limit_seats{priority_level="system"} 74           # 33% lendable =  24 lendable seats
apiserver_flowcontrol_nominal_limit_seats{priority_level="workload-high"} 98    # 50% lendable =  49 lendable seats
apiserver_flowcontrol_nominal_limit_seats{priority_level="workload-low"} 245    # 90% lendable = 220 lendable seats
```

#### Anything else we need to know?

An accurate `apiserver_flowcontrol_upper_limit_seats` is useful to compare against `apiserver_flowcontrol_current_limit_seats`. If they are equal, operators can infer that the priority level cannot borrow any more seats, helping debug faster throttling issues.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.3
Kustomize Version: v5.4.2
Server Version: v1.31.2
```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
$ cat /etc/os-release
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"
NAME="Debian GNU/Linux"
VERSION_ID="12"
VERSION="12 (bookworm)"
VERSION_CODENAME=bookworm
ID=debian
HOME_URL="https://www.debian.org/"
SUPPORT_URL="https://www.debian.org/support"
BUG_REPORT_URL="https://bugs.debian.org/"
$ uname -a
Linux kind-control-plane 6.10.14-linuxkit #1 SMP Thu Oct 24 19:28:55 UTC 2024 aarch64 GNU/Linux
```

</details>


#### Install tools

<details>
https://kind.sigs.k8s.io/#installation-and-usage
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
N/A
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
N/A
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
**ä¸æ¶‰åŠ**

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†Kubernetes APIæœåŠ¡å™¨ä¸­çš„æµæ§å®ç°é—®é¢˜ï¼Œå…·ä½“æ¥è¯´ï¼Œ`apiserver_flowcontrol_upper_limit_seats`æŒ‡æ ‡å¯èƒ½æŠ¥å‘Šè¶…è¿‡æ€»å¹¶å‘é™åˆ¶çš„å€¼ã€‚è¿™æ„å‘³ç€è¯¥æŒ‡æ ‡å¯èƒ½è¿‡äºæ¾æ•£ï¼Œæ— æ³•å‡†ç¡®åæ˜ å®é™…çš„å¹¶å‘é™åˆ¶ã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´è¿ç»´äººå‘˜åœ¨ç›‘æ§å’Œè°ƒè¯•æµæ§é—®é¢˜æ—¶äº§ç”Ÿè¯¯è§£ã€‚

ä»Issueçš„æè¿°æ¥çœ‹ï¼Œè¯¥é—®é¢˜æ¶‰åŠä»¥ä¸‹æ–¹é¢ï¼š

- **æŒ‡æ ‡ä¸å‡†ç¡®**ï¼š`apiserver_flowcontrol_upper_limit_seats`æŒ‡æ ‡å€¼è¶…è¿‡äº†å®é™…çš„æ€»å¹¶å‘é™åˆ¶ï¼Œæ— æ³•å‡†ç¡®åæ˜ æ¯ä¸ªä¼˜å…ˆçº§çº§åˆ«å¯ç”¨çš„æœ€å¤§æ‰§è¡Œå¸­ä½æ•°ã€‚
- **è¿ç»´å½±å“**ï¼šä¸å‡†ç¡®çš„æŒ‡æ ‡å¯èƒ½ä¼šå¯¼è‡´è¿ç»´äººå‘˜åœ¨å¤„ç†è¯·æ±‚è¢«é™æµç­‰é—®é¢˜æ—¶æ— æ³•å¿«é€Ÿå®šä½å’Œè§£å†³é—®é¢˜ã€‚

ç„¶è€Œï¼Œè¯¥é—®é¢˜å¹¶æœªæ¶‰åŠä»»ä½•å®‰å…¨æ–¹é¢çš„æ¼æ´ï¼Œä¾‹å¦‚æƒé™æå‡ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€æ‹’ç»æœåŠ¡æ”»å‡»ç­‰ã€‚å®ƒä¸»è¦æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§å’Œå¯è§‚æµ‹æ€§çš„é—®é¢˜ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬2æ¡ï¼šâ€œå¦‚æœè¯¥Issueæè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚â€è¯¥Issueè®¨è®ºçš„æ˜¯æµæ§æŒ‡æ ‡çš„å‡†ç¡®æ€§é—®é¢˜ï¼Œå¯¹ç³»ç»Ÿçš„å®‰å…¨æ€§æ²¡æœ‰ç›´æ¥å½±å“ï¼Œå› æ­¤é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚æ— éœ€æä¾›å¤ç°è„šæœ¬ã€‚

---


## Issue #129078 cpumanager:staticpolicy:smtalign: pod admission failed after kubelet restart

- Issue é“¾æ¥ï¼š[#129078](https://github.com/kubernetes/kubernetes/issues/129078)

### Issue å†…å®¹

#### What happened?

When Kubelet is configured with `static` CPUManager policy + `full-pcpus-only` option, after kubelet restart, pod is not get admitted and  I'm getting the following error under kubelet logs:

```
Dec 04 12:42:25 kubenswrapper[2410667]: I1204 12:42:25.173086 2410667 kubelet.go:2320] "Pod admission denied" podUID="4355ee04-54b0-4755-b23e-d05ce12e54c1" pod="my-app-namespace/app-deployment" reason="SMTAlignmentError" message="SMT Alignment Error: not enough free physical CPUs: available physical CPUs = 1, requested CPUs = 2, CPUs per core = 1"
```

After manually deleting and recreating the pod, it gets admitted, so the claim that there are node enough free physical CPUs is wrong.


Pod spec:

```yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2024-12-04T12:50:20Z"
  name: app-deployment
  namespace: my-app-namespace
  resourceVersion: "11109192"
  uid: 16737fd6-45e1-4371-b10e-3eff1377c224
spec:
  containers:
  - args:
    - while true; do sleep 10000; done;
    command:
    - /bin/sh
    - -c
    image: quay.io/jitesoft/alpine
    imagePullPolicy: Always
    name: app-container2
    resources:
      limits:
        cpu: "2"
        memory: 100Mi
      requests:
        cpu: "2"
        memory: 100Mi
  qosClass: Guaranteed

```

#### What did you expect to happen?

After kubelet restart pod should readmitted 

#### How can we reproduce it (as minimally and precisely as possible)?
On a node with 4 CPUs:
 `1` reserved (`reserverSystemCPUs`)
 `3` allocatable

1. Configure CPUManager to `static` and `full-pcpus-only` option to `true`
2. Create a Guaranteed QoS class pod requesting 2  exclusive CPUs:
``` yaml
kind: Pod
...
spec:
  containers:
...
    resources:
      limits:
        cpu: "2"
        memory: 100Mi
      requests:
        cpu: "2"
        memory: 100Mi
```
3. wait for pod to start
4. restart kubelet
5. Pod will failed with SMTAlignment error

NOTE:
It's possible to reproduce the issue on a system with any size of CPUs.
A general formula would be:
On a node with `N` CPUs:
 `1` reserved 
 `N-1` allocatable
Create a Guaranteed QoS class pod requesting `N / 2`  exclusive CPUs:

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.29.7
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.2
WARNING: version difference between client (1.29) and server (1.31) exceeds the supported minor version skew of +/-1

```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨è®¾ç½®Kubeletçš„CPUManagerç­–ç•¥ä¸º`static`å¹¶å¯ç”¨`full-pcpus-only`é€‰é¡¹çš„æƒ…å†µä¸‹ï¼Œé‡å¯Kubeletåï¼ŒPodæ— æ³•è¢«é‡æ–°æ¥çº³ï¼Œå‡ºç°äº†â€œSMT Alignment Errorâ€çš„é”™è¯¯ä¿¡æ¯ã€‚æ‰‹åŠ¨åˆ é™¤å¹¶é‡æ–°åˆ›å»ºPodåï¼ŒPodå¯ä»¥æ­£å¸¸è¿è¡Œã€‚è¿™è¡¨æ˜åœ¨Kubeleté‡å¯åï¼ŒCPUManagerå¯¹äºCPUèµ„æºçš„åˆ†é…å‡ºç°äº†é—®é¢˜ã€‚

ä»é—®é¢˜æè¿°æ¥çœ‹ï¼Œè¿™æ˜¯ä¸€ä¸ªç”±äºKubeletåœ¨ç‰¹å®šé…ç½®ä¸‹é‡å¯å¯¼è‡´çš„èµ„æºè°ƒåº¦é”™è¯¯ï¼Œå±äºåŠŸèƒ½æ€§ç¼ºé™·æˆ–Bugï¼Œå¹¶æœªæ¶‰åŠä»»ä½•å®‰å…¨æ¼æ´ã€‚æ²¡æœ‰æåŠæ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ­¤æ¼æ´è¿›è¡Œæœªæˆæƒçš„è®¿é—®ã€æå‡æƒé™ã€è¿œç¨‹ä»£ç æ‰§è¡Œæˆ–å…¶ä»–å®‰å…¨é£é™©ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

- **ç¬¬2æ¡**ï¼šå¦‚æœè¯¥issueæè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

å› æ­¤ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

---


## Issue #129069 Is it necessary for the kube-apiserver to periodically list services?

- Issue é“¾æ¥ï¼š[#129069](https://github.com/kubernetes/kubernetes/issues/129069)

### Issue å†…å®¹

#### What happened?

Is it necessary for the kube-apiserver to periodically list services?
https://github.com/kubernetes/kubernetes/blob/810e9e212ec5372d16b655f57b9231d8654a2179/pkg/registry/core/service/ipallocator/controller/repair.go#L125-L131
https://github.com/kubernetes/kubernetes/blob/810e9e212ec5372d16b655f57b9231d8654a2179/pkg/registry/core/service/ipallocator/controller/repair.go#L211

#### What did you expect to happen?

The kube-apiserver does not periodically list services.

#### How can we reproduce it (as minimally and precisely as possible)?

Currently, metrics statistics show that the kube-apiserver frequently performs list services operations.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# 1.31.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®Issueçš„æè¿°ï¼Œæé—®è€…å…³æ³¨çš„æ˜¯kube-apiserveræ˜¯å¦æœ‰å¿…è¦å®šæœŸåˆ—å‡ºæœåŠ¡ï¼ˆservicesï¼‰ã€‚ä»–å‘ç°kube-apiserveré¢‘ç¹æ‰§è¡Œäº†List servicesçš„æ“ä½œï¼Œå¹¶è´¨ç–‘è¿™ç§è¡Œä¸ºæ˜¯å¦å¿…è¦ã€‚

ä»æä¾›çš„é“¾æ¥å¯ä»¥çœ‹åˆ°ï¼Œkube-apiserveråœ¨æ‰§è¡ŒService IPåˆ†é…çš„ä¿®å¤æ“ä½œæ—¶ï¼Œä¼šå®šæœŸåˆ—å‡ºæ‰€æœ‰çš„æœåŠ¡ã€‚è¿™å¯èƒ½ä¼šå¼•èµ·å¯¹æ€§èƒ½çš„æ‹…å¿§ï¼Œä½†ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œæ²¡æœ‰ç›´æ¥çš„å®‰å…¨é£é™©ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. å¦‚æœè¯¥issueæè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚

è¯¥Issueå¹¶æœªæåˆ°ä»»ä½•ä¸å®‰å…¨ç›¸å…³çš„é—®é¢˜ï¼Œå¦‚æƒé™æå‡ã€ä¿¡æ¯æ³„éœ²ã€è¿œç¨‹ä»£ç æ‰§è¡Œç­‰ã€‚è€Œæ˜¯å…³æ³¨kube-apiserverçš„æ“ä½œè¡Œä¸ºæ˜¯å¦åˆç†ï¼Œå±äºæ€§èƒ½ä¼˜åŒ–æˆ–åŠŸèƒ½æ”¹è¿›çš„èŒƒç•´ã€‚

---


## Issue #129065 Fake client does not validate if the namespace exists when creating resources

- Issue é“¾æ¥ï¼š[#129065](https://github.com/kubernetes/kubernetes/issues/129065)

### Issue å†…å®¹

#### What happened?

When using the fake client provided by the Kubernetes client-go library, I observed that it does not validate whether the namespace exists before creating resources.

For example, in the test case below, I attempted to create a Deployment in a non-existent namespace (test-namespace). However, no error was returned, and the operation succeeded without the namespace being created or verified:

```go
import (
	clientFake "k8s.io/client-go/kubernetes/fake"
	v1 "k8s.io/api/apps/v1"
        "testing"
)

func Test1(t *testing.T) {
    k8sClient := clientFake.NewSimpleClientset()
    _, err := k8sClient.AppsV1().Deployments("test-namespace").Create(context.Background(), &v1.Deployment{
        ObjectMeta: metav1.ObjectMeta{Name: "test-deployment"},
    }, metav1.CreateOptions{})
    fmt.Println(err)
}
```

Running this test produces the following output:
```bash
<nil>
PASS
ok      portrait/offline/pkg/prometheus 0.018s
```

I expected the fake client to return an error indicating that the namespace does not exist. However, it silently allowed the creation of the resource.


#### What did you expect to happen?

The fake client should validate whether the specified namespace exists before allowing the creation of a resource within it. If the namespace does not exist, it should return an error similar to the behavior of a real Kubernetes API server.

#### How can we reproduce it (as minimally and precisely as possible)?

* Create a fake client using clientFake.NewSimpleClientset().
* Attempt to create a resource (e.g., Deployment) in a non-existent namespace.
* Observe that no error is returned, and the resource is created without namespace validation.

#### Anything else we need to know?

* The fake client is commonly used for unit testing in Kubernetes-related projects. If it does not mimic the real Kubernetes API server's behavior accurately, it can lead to tests passing incorrectly and bugs being introduced into production code.
* I understand that the fake client is not a full Kubernetes API server implementation. However, basic validations like namespace existence checks are critical for accurate testing.
* I am happy to submit a PR to address this issue if it is agreed that this is a bug or missing feature. Please let me know if this behavior is intentional or if I should proceed with the fix.
* If the maintainers decide this is not a bug, I would appreciate clarification on whether this behavior is by design or if there are alternative approaches to handle namespace validation in unit tests.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.21.1
WARNING: version difference between client (1.31) and server (1.21) exceeds the supported minor version skew of +/-1
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.3 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.3 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

$ uname -a
Linux MI-20240719YHKN 5.15.153.1-microsoft-standard-WSL2 #1 SMP Fri Mar 29 23:14:13 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŒ‡å‡ºï¼Œå½“ä½¿ç”¨Kubernetes client-goåº“æä¾›çš„fake clientæ—¶ï¼Œåœ¨åˆ›å»ºèµ„æºæ—¶ä¸ä¼šéªŒè¯å‘½åç©ºé—´æ˜¯å¦å­˜åœ¨ã€‚å…·ä½“æ¥è¯´ï¼Œå³ä½¿å°è¯•åœ¨ä¸€ä¸ªä¸å­˜åœ¨çš„å‘½åç©ºé—´ä¸­åˆ›å»ºDeploymentï¼Œä¹Ÿä¸ä¼šè¿”å›é”™è¯¯ï¼Œæ“ä½œä¼šæ— é”™è¯¯åœ°æˆåŠŸæ‰§è¡Œã€‚è¿™ä¸çœŸå®çš„Kubernetes APIæœåŠ¡å™¨çš„è¡Œä¸ºä¸ä¸€è‡´ï¼Œåè€…ä¼šåœ¨å‘½åç©ºé—´ä¸å­˜åœ¨æ—¶è¿”å›é”™è¯¯ã€‚

è¿™ä¸ªé—®é¢˜çš„ä¸»è¦å½±å“æ˜¯å¯èƒ½å¯¼è‡´å•å…ƒæµ‹è¯•é€šè¿‡ï¼Œä½†åœ¨çœŸå®ç¯å¢ƒä¸­ä¼šå¤±è´¥ï¼Œä»è€Œå¼•å…¥æ½œåœ¨çš„bugã€‚å¼€å‘è€…å¯èƒ½è¯¯ä»¥ä¸ºä»£ç åœ¨å¤„ç†ä¸å­˜åœ¨çš„å‘½åç©ºé—´æ—¶èƒ½å¤Ÿæ­£å¸¸å·¥ä½œï¼Œä½†å®é™…ä¸Šåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä¼šé‡åˆ°é”™è¯¯ã€‚

ç„¶è€Œï¼Œè¿™ä¸€é—®é¢˜å±äºæµ‹è¯•å‡†ç¡®æ€§å’Œä»£ç è´¨é‡çš„é—®é¢˜ï¼Œå¹¶ä¸æ¶‰åŠç›´æ¥çš„å®‰å…¨é£é™©ã€‚å®ƒä¸ä¼šå¯¼è‡´æœªç»æˆæƒçš„è®¿é—®ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€æƒé™æå‡æˆ–è¿œç¨‹ä»£ç æ‰§è¡Œç­‰å®‰å…¨é—®é¢˜ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

- **æ ‡å‡†2**ï¼šå¦‚æœè¯¥issueæè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠã€‚
- **æ ‡å‡†9**ï¼šå¦‚æœissueä¸­æä¾›çš„å†…å®¹ä¸å¤Ÿå……åˆ†ï¼Œåˆ™æ ¹æ®issueå¯èƒ½å¯¼è‡´çš„åæœåˆ¤æ–­é£é™©è¯„çº§ã€‚

ç»¼åˆä»¥ä¸Šåˆ†æï¼Œè®¤ä¸ºè¯¥é—®é¢˜ä¸å±äºå®‰å…¨é£é™©ï¼Œè€Œæ˜¯åŠŸèƒ½æ€§é—®é¢˜ã€‚

---


## Issue #129060 Compatibility Versions E2E tests failing for v1.33 with "emulation version 1.31 is not between [1.32, 1.33.0-alpha.0.1+0e1abc4d18e353]"

- Issue é“¾æ¥ï¼š[#129060](https://github.com/kubernetes/kubernetes/issues/129060)

### Issue å†…å®¹

#### What happened?

Currently the Compatibility Versions E2E tests are failing when run against v1.33 with the below error:
```
> 2024-11-27T02:30:24.881212795Z stderr F E1127 02:30:24.881143       1 run.go:72] "command failed" err="emulation version 1.31 is not between [1.32, 1.33.0-alpha.0.1+0e1abc4d18e353]"
```
This test is catching a bug where currently it seems that there is some logic in k8s with the validation for --emulated-version where it is not allowing n-2 versions even though Compatibility Versions supports up to n-3

Related kube-apiserver logs link:
https://storage.googleapis.com/kubernetes-ci-logs/logs/ci-kubernetes-e2e-kind-compatibility-versions/1861594576398061568/artifacts/kind-control-plane/containers/kube-apiserver-kind-control-plane_kube-system_kube-apiserver-842342e203722dea42f37c832c5b10193a08bca734ad290c3a1ee3c32921e560.log


Associated TestGrid Alert Information Below + Additional Links:
New TestGrid alerts at [sig-testing-kind#compatibility-version-test](https://testgrid.k8s.io/sig-testing-kind#compatibility-version-test) since commit/0e1abc4d1.

Found 1 failing rows in compatibility-version-test: 1 new, 0 other 1 new Test Failures in compatibility-version-test.
Test [ci-kubernetes-e2e-kind-compatibility-versions.Overall](https://testgrid.k8s.io/sig-testing-kind#compatibility-version-test) has failed 3 times since it last passed at commit/8770bd58d. It started failing between commit/8770bd58d (Tue Nov 26 12:16 PST) and commit/0e1abc4d1 (Tue Nov 26 18:16 PST): .

TestGrid Link: https://testgrid.k8s.io/sig-testing-kind#compatibility-version-test

Prow CI Link For First Failing Test:
https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-e2e-kind-compatibility-versions/1861594576398061568a

Commits that caused test breakage:
https://github.com/kubernetes/kubernetes/compare/8770bd58d...0e1abc4d1
^ this is associated with creating a new git tag for the v1.32 RC which modifies the k8s version output by `git describe`/`hack/print-workspace-status.sh`.  Eg: master is now v1.33.0-alpha.0.1+0e1abc4d18e353 with this commit

#### What did you expect to happen?

K8s v1.33 should work with --emulated-version=1.31

#### How can we reproduce it (as minimally and precisely as possible)?

See test-grid test above.  I am not sure why but this doesn't repro locally using hack/local-up-cluster.sh (perhaps the current version of k8s is calculated differently using this script?)

#### Anything else we need to know?

/assign
/sig api-machinery

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨Kubernetes v1.33ç‰ˆæœ¬ä¸­ï¼ŒCompatibility Versions E2Eæµ‹è¯•å¤±è´¥ï¼Œé”™è¯¯ä¿¡æ¯ä¸ºï¼š
```
emulation version 1.31 is not between [1.32, 1.33.0-alpha.0.1+0e1abc4d18e353]
```
è¿™è¡¨æ˜åœ¨ä½¿ç”¨`--emulated-version`å‚æ•°è¿›è¡Œç‰ˆæœ¬æ¨¡æ‹Ÿæ—¶ï¼Œç‰ˆæœ¬éªŒè¯é€»è¾‘å­˜åœ¨é—®é¢˜ï¼Œå¯¼è‡´æ— æ³•æ”¯æŒé¢„æœŸçš„n-2ç‰ˆæœ¬å…¼å®¹æ€§ã€‚Issueä¸­æåˆ°Compatibility Versionsåº”è¯¥æ”¯æŒåˆ°n-3ç‰ˆæœ¬ï¼Œä½†å½“å‰çš„éªŒè¯é€»è¾‘ä¸å…è®¸ã€‚è¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§é”™è¯¯ï¼Œæ¶‰åŠç‰ˆæœ¬å…¼å®¹æ€§å’Œç‰ˆæœ¬éªŒè¯é€»è¾‘çš„é—®é¢˜ã€‚

ä»å®‰å…¨é£é™©çš„è§’åº¦æ¥çœ‹ï¼Œæ­¤é—®é¢˜å¹¶ä¸æ¶‰åŠä»»ä½•å®‰å…¨æ¼æ´æˆ–å®‰å…¨é£é™©ã€‚å®ƒä¸ä¼šå¯¼è‡´æƒé™æå‡ã€ä¿¡æ¯æ³„éœ²ã€å‘½ä»¤æ‰§è¡Œã€æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ç­‰å®‰å…¨é—®é¢˜ã€‚è¯¥é—®é¢˜ä»…å½±å“åˆ°æµ‹è¯•æµç¨‹ï¼Œå±äºè½¯ä»¶çš„åŠŸèƒ½æ€§ç¼ºé™·ã€‚

---


## Issue #129056 kubelet: PodRejectionStatus Kubelet should reject pod when the node didn't have enough resource test error

- Issue é“¾æ¥ï¼š[#129056](https://github.com/kubernetes/kubernetes/issues/129056)

### Issue å†…å®¹

#### What happened?

https://github.com/kubernetes/kubernetes/pull/128403 merged to move PodRejectionStatus into e2e/node from e2e/common/node. In certain cases this test fails because the test is comparing the entire status object. The test needs to be changed to validate the fields we care about.

```
Expected
    <v1.PodStatus>: 
        conditions:
        - lastProbeTime: null
          lastTransitionTime: "2024-11-29T14:10:06Z"
          status: "False"
          type: PodReadyToStartContainers
        - lastProbeTime: null
          lastTransitionTime: "2024-11-29T14:10:06Z"
          status: "True"
          type: Initialized
        - lastProbeTime: null
          lastTransitionTime: "2024-11-29T14:10:06Z"
          reason: PodFailed
          status: "False"
          type: Ready
        - lastProbeTime: null
          lastTransitionTime: "2024-11-29T14:10:06Z"
          reason: PodFailed
          status: "False"
          type: ContainersReady
        - lastProbeTime: null
          lastTransitionTime: "2024-11-29T14:10:06Z"
          status: "True"
          type: PodScheduled
        containerStatuses:
        - image: registry.k8s.io/pause:3.10
          imageID: ""
          lastState: {}
          name: pod-out-of-cpu
          ready: false
          restartCount: 0
          started: false
          state:
            waiting:
              reason: ContainerCreating
          volumeMounts:
          - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
            name: kube-api-access-4dd6q
            readOnly: true
            recursiveReadOnly: Disabled
        hostIP: 10.0.128.3
        hostIPs:
        - ip: 10.0.128.3
        message: 'Pod was rejected: Node didn''t have enough resource: cpu, requested: 1000000000000000,
          used: 491, capacity: 3500'
        phase: Failed
        qosClass: Burstable
        reason: OutOfcpu
        startTime: "2024-11-29T14:10:06Z"
to equal
    <v1.PodStatus>: 
        message: 'Pod was rejected: Node didn''t have enough resource: cpu, requested: 1000000000000000,
          used: 491, capacity: 3500'
        phase: Failed
        qosClass: Burstable
        reason: OutOfcpu
        startTime: "2024-11-29T14:10:06Z"
```



#### What did you expect to happen?

Expect the test case to run successfully.

#### How can we reproduce it (as minimally and precisely as possible)?

See reproduced output.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
1.31+
</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>
Linux
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°çš„æ˜¯åœ¨åˆå¹¶äº†PR #128403åï¼Œ`PodRejectionStatus`æµ‹è¯•ä»`e2e/common/node`ç§»åŠ¨åˆ°äº†`e2e/node`ä¸­ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¯¥æµ‹è¯•å¤±è´¥äº†ï¼ŒåŸå› æ˜¯æµ‹è¯•åœ¨æ¯”è¾ƒæ•´ä¸ª`PodStatus`å¯¹è±¡ï¼Œè€Œå®é™…ä¸Šåº”è¯¥åªéªŒè¯æˆ‘ä»¬å…³å¿ƒçš„ç‰¹å®šå­—æ®µã€‚Issueä¸­æä¾›äº†é¢„æœŸçš„`PodStatus`å’Œå®é™…çš„`PodStatus`ï¼Œå¯ä»¥çœ‹å‡ºå®é™…çš„`PodStatus`åŒ…å«äº†æ›´å¤šçš„å­—æ®µä¿¡æ¯ï¼Œå¯¼è‡´æµ‹è¯•å¤±è´¥ã€‚

è¿™ä¸ªé—®é¢˜æ¶‰åŠåˆ°æµ‹è¯•ç”¨ä¾‹çš„ç¼–å†™å’ŒéªŒè¯é—®é¢˜ï¼Œéœ€è¦ä¿®æ”¹æµ‹è¯•ä»£ç ï¼Œä»¥ç¡®ä¿åªæ¯”è¾ƒå…³å¿ƒçš„å­—æ®µï¼Œè€Œä¸æ˜¯æ•´ä¸ªå¯¹è±¡ã€‚è¿™æ˜¯ä¸€ä¸ªä»£ç è´¨é‡å’Œæµ‹è¯•å¯é æ€§çš„é—®é¢˜ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š

1. è¯¥Issueæè¿°çš„é—®é¢˜å¹¶éå®‰å…¨é—®é¢˜ï¼Œå¦‚æ¼æ´ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ç­‰ã€‚
2. æ²¡æœ‰æ¶‰åŠæ”»å‡»è€…å¯ä»¥åˆ©ç”¨çš„å®‰å…¨æ¼æ´æˆ–é£é™©ã€‚

---


## Issue #129050 failed to create patch: unable to find api field in struct Probe for the json field "grpc"

- Issue é“¾æ¥ï¼š[#129050](https://github.com/kubernetes/kubernetes/issues/129050)

### Issue å†…å®¹

#### What happened?

When running `helm upgrade ...` for a certain release, the following error has occurs:
```
$ helm upgrade --debug --install <REDACTED>
history.go:53: [debug] getting history for release <REDACTED>
upgrade.go:121: [debug] preparing upgrade for <REDACTED>
upgrade.go:428: [debug] reusing the old release's values
upgrade.go:129: [debug] performing update for <REDACTED>
upgrade.go:296: [debug] creating upgraded release for <REDACTED>
client.go:173: [debug] checking 6 resources for changes
client.go:436: [debug] Looks like there are no changes for Service "<REDACTED>"
client.go:205: [debug] error updating the resource "<REDACTED>":
 	 failed to create patch: unable to find api field in struct Probe for the json field "grpc"
 upgrade.go:355: [debug] warning: Upgrade "<REDACTED>" failed: failed to create patch: unable to find api field in struct Probe for the json field "grpc"
 ##[error]history.go:53: [debug] getting history for release <REDACTED>
upgrade.go:121: [debug] preparing upgrade for <REDACTED>
upgrade.go:428: [debug] reusing the old release's values
upgrade.go:129: [debug] performing update for <REDACTED>
upgrade.go:296: [debug] creating upgraded release for <REDACTED>
client.go:173: [debug] checking 6 resources for changes
client.go:436: [debug] Looks like there are no changes for Service "<REDACTED>"
client.go:205: [debug] error updating the resource "<REDACTED>":
	 failed to create patch: unable to find api field in struct Probe for the json field "grpc"
upgrade.go:355: [debug] warning: Upgrade "<REDACTED>" failed: failed to create patch: unable to find api field in struct Probe for the json field "grpc"
Error: UPGRADE FAILED: failed to create patch: unable to find api field in struct Probe for the json field "grpc"
helm.go:94: [debug] failed to create patch: unable to find api field in struct Probe for the json field "grpc"
helm.sh/helm/v3/pkg/kube.(*Client).Update
	/home/circleci/helm.sh/helm/pkg/kube/client.go:218
helm.sh/helm/v3/pkg/action.(*Upgrade).performUpgrade
	/home/circleci/helm.sh/helm/pkg/action/upgrade.go:310
helm.sh/helm/v3/pkg/action.(*Upgrade).Run
	/home/circleci/helm.sh/helm/pkg/action/upgrade.go:130
main.newUpgradeCmd.func2
	/home/circleci/helm.sh/helm/cmd/helm/upgrade.go:154
github.com/spf13/cobra.(*Command).execute
	/go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:842
github.com/spf13/cobra.(*Command).ExecuteC
	/go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:950
github.com/spf13/cobra.(*Command).Execute
	/go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:887
main.main
	/home/circleci/helm.sh/helm/cmd/helm/helm.go:93
runtime.main
	/usr/local/go/src/runtime/proc.go:203
runtime.goexit
	/usr/local/go/src/runtime/asm_amd64.s:1373
UPGRADE FAILED
main.newUpgradeCmd.func2
	/home/circleci/helm.sh/helm/cmd/helm/upgrade.go:156
github.com/spf13/cobra.(*Command).execute
	/go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:842
github.com/spf13/cobra.(*Command).ExecuteC
	/go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:950
github.com/spf13/cobra.(*Command).Execute
	/go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:887
main.main
	/home/circleci/helm.sh/helm/cmd/helm/helm.go:93
runtime.main
	/usr/local/go/src/runtime/proc.go:203
runtime.goexit
	/usr/local/go/src/runtime/asm_amd64.s:1373

2024-12-01T16:36:53.8872455Z Error: UPGRADE FAILED: failed to create patch: unable to find api field in struct Probe for the json field "grpc"
2024-12-01T16:36:53.8882936Z helm.go:94: [debug] failed to create patch: unable to find api field in struct Probe for the json field "grpc"
2024-12-01T16:36:53.8883982Z helm.sh/helm/v3/pkg/kube.(*Client).Update
2024-12-01T16:36:53.8884509Z 	/home/circleci/helm.sh/helm/pkg/kube/client.go:218
2024-12-01T16:36:53.8884968Z helm.sh/helm/v3/pkg/action.(*Upgrade).performUpgrade
2024-12-01T16:36:53.8885425Z 	/home/circleci/helm.sh/helm/pkg/action/upgrade.go:310
2024-12-01T16:36:53.8885874Z helm.sh/helm/v3/pkg/action.(*Upgrade).Run
2024-12-01T16:36:53.8886315Z 	/home/circleci/helm.sh/helm/pkg/action/upgrade.go:130
2024-12-01T16:36:53.8886733Z main.newUpgradeCmd.func2
2024-12-01T16:36:53.8887154Z 	/home/circleci/helm.sh/helm/cmd/helm/upgrade.go:154
2024-12-01T16:36:53.8887599Z github.com/spf13/cobra.(*Command).execute
2024-12-01T16:36:53.8888049Z 	/go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:842
2024-12-01T16:36:53.8888497Z github.com/spf13/cobra.(*Command).ExecuteC
2024-12-01T16:36:53.8888948Z 	/go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:950
2024-12-01T16:36:53.8889382Z github.com/spf13/cobra.(*Command).Execute
2024-12-01T16:36:53.8889825Z 	/go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:887
2024-12-01T16:36:53.8890231Z main.main
2024-12-01T16:36:53.8890638Z 	/home/circleci/helm.sh/helm/cmd/helm/helm.go:93
2024-12-01T16:36:53.8891035Z runtime.main
2024-12-01T16:36:53.8891429Z 	/usr/local/go/src/runtime/proc.go:203
2024-12-01T16:36:53.8891825Z runtime.goexit
2024-12-01T16:36:53.8892223Z 	/usr/local/go/src/runtime/asm_amd64.s:1373
2024-12-01T16:36:53.8892633Z UPGRADE FAILED
2024-12-01T16:36:53.8893026Z main.newUpgradeCmd.func2
2024-12-01T16:36:53.8893464Z 	/home/circleci/helm.sh/helm/cmd/helm/upgrade.go:156
2024-12-01T16:36:53.8893917Z github.com/spf13/cobra.(*Command).execute
2024-12-01T16:36:53.8894386Z 	/go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:842
2024-12-01T16:36:53.8894933Z github.com/spf13/cobra.(*Command).ExecuteC
2024-12-01T16:36:53.8895407Z 	/go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:950
2024-12-01T16:36:53.8895846Z github.com/spf13/cobra.(*Command).Execute
2024-12-01T16:36:53.8896265Z 	/go/pkg/mod/github.com/spf13/cobra@v1.0.0/command.go:887
2024-12-01T16:36:53.8896669Z main.main
2024-12-01T16:36:53.8897065Z 	/home/circleci/helm.sh/helm/cmd/helm/helm.go:93
2024-12-01T16:36:53.8897469Z runtime.main
2024-12-01T16:36:53.8897823Z 	/usr/local/go/src/runtime/proc.go:203
2024-12-01T16:36:53.8898178Z runtime.goexit
2024-12-01T16:36:53.8898541Z 	/usr/local/go/src/runtime/asm_amd64.s:1373
2024-12-01T16:36:53.8976539Z ##[section]Finishing: helm upgrade
```

I suspect the issue is coming from the `kubectl patch` command which helm uses, as you can see in the error message:
`failed to create patch`

This only happens when there's a deployment's pod spec with a *Probe (startupProbe, readinessProbe, etc...), with the `grpc` attribute, but the `grpc.service` attribute is an empty string.
When I put something in the service, this works fine.
e.g:
This works:
```yaml
readinessProbe:
  grpc:
    port: 8080
    service: "readiness"
```
These don't work:
```yaml
readinessProbe:
  grpc:
    port: 8080
    service: ""
```
```yaml
readinessProbe:
  grpc:
    port: 8080
    service:
```

#### What did you expect to happen?

The *Probe struct should be able to receive an "empty" value for the `grpc.service` endpoint.

#### How can we reproduce it (as minimally and precisely as possible)?

 - Create a deployment with these values (both should behave the same imo):
```yaml
readinessProbe:
  grpc:
    port: 8080
    service: ""
```
```yaml
readinessProbe:
  grpc:
    port: 8080
    service:
```
 - Apply the deployment to a cluster
 - Make a change to something unrelated to the `grpc` struct
 - Try to `kubectl patch` it

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.5
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.28.9
```

</details>


#### Cloud provider

<details>
Azure AKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
BuildNumber  Caption                          OSArchitecture  Version
26100        Microsoft Windows 11 Enterprise  64-bit          10.0.26100
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨ä½¿ç”¨Helmæ‰§è¡Œå‡çº§æ“ä½œæ—¶é‡åˆ°çš„é”™è¯¯ï¼š

```
failed to create patch: unable to find api field in struct Probe for the json field "grpc"
```

å…·ä½“æƒ…å†µæ˜¯ï¼Œå½“Deploymentçš„Podè§„èŒƒä¸­åŒ…å«äº†*Probeï¼ˆä¾‹å¦‚`readinessProbe`ï¼‰ï¼Œå¹¶ä¸”ä½¿ç”¨äº†`grpc`å±æ€§ï¼Œä½†`grpc.service`å­—æ®µä¸ºç©ºå­—ç¬¦ä¸²æˆ–æœªè®¾ç½®æ—¶ï¼Œæ‰§è¡Œ`helm upgrade`ä¼šå¤±è´¥ã€‚è€Œå½“`grpc.service`å­—æ®µæœ‰æœ‰æ•ˆå€¼æ—¶ï¼Œå‡çº§æ“ä½œæ­£å¸¸ã€‚

é—®é¢˜å¯èƒ½å‡ºåœ¨Helmå†…éƒ¨è°ƒç”¨çš„`kubectl patch`å‘½ä»¤ï¼Œå› ä¸ºé”™è¯¯ä¿¡æ¯ä¸­æåˆ°äº†`failed to create patch`ã€‚è¿™è¡¨æ˜åœ¨ç”Ÿæˆè¡¥ä¸æ—¶ï¼Œæ— æ³•æ‰¾åˆ°å¯¹åº”çš„APIå­—æ®µã€‚

ä»æè¿°æ¥çœ‹ï¼Œè¿™æ˜¯ç”±äºKubernetesçš„APIåœ¨å¤„ç†Probeç»“æ„æ—¶ï¼Œå¯¹`grpc.service`å­—æ®µä¸ºç©ºçš„æƒ…å†µå¤„ç†ä¸å½“ï¼Œå¯¼è‡´æ— æ³•ç”Ÿæˆæ­£ç¡®çš„è¡¥ä¸æ–‡ä»¶ã€‚è¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§é—®é¢˜ï¼Œæ¶‰åŠåˆ°Kubernetesèµ„æºçš„é…ç½®å’Œæ›´æ–°æµç¨‹ã€‚

æ ¹æ®æä¾›çš„ä¿¡æ¯ï¼Œè¯¥é—®é¢˜å¹¶æœªæ¶‰åŠä»»ä½•å®‰å…¨æ¼æ´ï¼Œä¾‹å¦‚æœªæˆæƒçš„è®¿é—®ã€æƒé™æå‡ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ç­‰ã€‚

---


